This document is downloaded from DR‑NTU ( 
Nanyang Technological University, Singapore.
Mining actionlet ensemble for action recognition
with depth cameras
Wang, Jiang; Liu, Zicheng; Wu, Ying; Yuan, Junsong
Wang, J., Liu, Z., Wu, Y., & Yuan, J.  . Mining actionlet ensemble for action recognition
with depth cameras. 2012 IEEE Conference on Computer Vision and Pattern Recognition,
1290‑1297.
 
 
© 2012 IEEE. Personal use of this material is permitted. Permission from IEEE must be
obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new
collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted
component of this work in other works. The published version is available at:
[ 
Downloaded on 26 Mar 2025 17:54:54 SGT
Mining Actionlet Ensemble for Action Recognition with Depth Cameras
Jiang Wang1
Zicheng Liu2
Junsong Yuan3
 
 
 
 
1Northwestern University
2Microsoft Research
3Nanyang Technological University
Human action recognition is an important yet challenging task. The recently developed commodity depth sensors
open up new possibilities of dealing with this problem but
also present some unique challenges. The depth maps captured by the depth cameras are very noisy and the 3D positions of the tracked joints may be completely wrong if serious occlusions occur, which increases the intra-class variations in the actions. In this paper, an actionlet ensemble model is learnt to represent each action and to capture
the intra-class variance. In addition, novel features that
are suitable for depth data are proposed. They are robust
to noise, invariant to translational and temporal misalignments, and capable of characterizing both the human motion and the human-object interactions. The proposed approach is evaluated on two challenging action recognition
datasets captured by commodity depth cameras, and another dataset captured by a MoCap system. The experimental evaluations show that the proposed approach achieves
superior performance to the state of the art algorithms.
1. Introduction
Recognizing human actions can have many potential applications including video surveillance, human computer
interfaces, sports video analysis and video retrieval. Despite
the research efforts in the past decade and many encouraging advances, accurate recognition of the human actions is
still a quite challenging task. There are two related major
issues for human action recognition. The ﬁrst one is the
sensory input, and the other is the modeling of the human
actions that are dynamic, ambiguous and interactive with
other objects.
The human motion is articulated, and capturing such
highly articulated motion from monocular video sensors is
a very difﬁcult task. This difﬁculty largely limits the performance of video-based human action recognition, as indicated in the studies in the past decade. The recent introduction of the cost-effective depth cameras may change
the picture by providing 3D depth data of the scene, which
YQKRKZUTLKGZ[XK
256LKGZ[XK
,U[XOKX:KSVUXGR6_XGSOJ
,KGZ[XK+^ZXGIZOUT
LKGZ[XKYGZ
GRRZNKPUOTZY
'IZOUTRKZY
3[RZOVRK1KXTKR2KGXTOTM
'IZOUT2GHKRY
YQKRKZUTLKGZ[XK
256LKGZ[XK
YQKRKZUTLKGZ[XK
256LKGZ[XK
Figure 1. The general framework of the proposed approach.
largely eases the task of object segmentation. Moreover, it
has facilitated a rather powerful human motion capturing
technique that outputs the 3D joint positions of the human skeleton.
Although the depth cameras in general produce better quality 3D motion than those estimated from monocular video sensors, simply using such 3D motion sensory
data and the estimated 3D joint positions for human action
recognition is not plausible. One reason is that the estimated
3D joint positions are noisy and may have signiﬁcant errors
when there are occlusions such as one leg being in front
of the other, a hand touching another body part, two hands
crossing, etc. In addition, the 3D skeleton motion alone
is not sufﬁcient to distinguish some actions. For example,
“drinking” and “eating snacks” give very similar motion for
the human skeleton. Extra inputs need to be included and
exploited for better recognition.
This paper presents a novel human action recognition approach using a depth camera. The basic idea is illustrated in
Fig. 1. Based on the depth data and the estimated 3D joint
positions, we propose a new feature called local occupancy
pattern or LOP feature. Each 3D joint is associated with
a LOP feature, which can be treated as the “depth appearance” of this 3D joint. Translational invariant and highly
discriminative, this new feature is also able to capture the
relations between the human body parts and the environmental objects in the interaction. In addition, to represent
the temporal structure of an individual joint in an action,
we propose a new temporal pattern representation called
Fourier Temporal Pyramid. This representation is insensitive to temporal sequence misalignment and is robust to
More importantly, we propose a new model for human
actions, called the Actionlet Ensemble Model. The articulated human body has a large number of kinematic joints,
but a certain action is usually only associated with and characterized by the interactions and combinations of a subset
of them. For example, the joints “right wrist” and “head”
are discriminative for the action “drinking”. Therefore, we
introduce the concept of actionlet. An actionlet is a particular conjunction of the features for a subset of the joints,
indicating a structure of the features. As there are an enormous number of possible actionlets, we propose a novel
data mining solution to discover discriminative actionlets.
Then an action is represented as an Actionlet Ensemble,
which is a linear combination of the actionlets, and their
discriminative weights are learnt via a multiple kernel learning method. This new action model is more robust to the
errors in the features, and it can better characterize the intraclass variations in the actions. For example, for the action
“call cellphone”, some people use their right hands while
others use their left hands. This variation can be characterized by the proposed actionlet ensemble model.
Our main contributions include the following three aspects.
First, this paper proposes the actionlet ensemble
model as a new way of characterizing and recognizing
human actions.
Second, our extensive experiments have
shown that the proposed features are well suitable for the
depth data-based action recognition task. Third, the proposed Fourier temporal pyramid is a new representation of
temporal patterns, and it is shown to be robust to temporal
misalignment and noise.
The proposed features and models are evaluated on
three benchmark datasets: CMU MoCap dataset , MSR-
Action3D dataset and DailyActivity3D dataset. The
ﬁrst dataset contains 3D joint positions captured by a multicamera motion capturing system, and the other two datasets
are captured with commodity depth cameras. Our extensive
experimental results show that the proposed method is able
to achieve signiﬁcantly better recognition accuracy than the
state-of-the-art methods.
2. Related Work
Actions are spatio-temporal patterns. There are two important issues in action recognition: the representation of
suitable spatio-temporal features, and the modeling of dynamical patterns.
Features can be sensor-dependent. In video-based methods, it is a common practice to locate spatio-temporal interest points like STIP , and use the distributions of the
local features like HOF or HOG to represent such
local spatio-temporal pattern. When we want to use depth
data, however, because there is no texture in the depth map,
these local features are not discriminative enough for classiﬁcation.
It is generally agreed that knowing the 3D joint position
is helpful for action recognition. Multi-camera motion capture (MoCap) systems have been used for human action
recognition, but such special equipment is marker-based
and expensive. It is still a challenging problem for markerfree motion capturing via regular video sensors.
Costeffective depth cameras have been used for motion capturing, and produced reasonable results, despite of the noise
when occlusion occurs. Because of the different quality of
the motion data, the action recognition methods designed
for MoCap data may not be suitable for depth camera.
In the literature, there have been many different approaches for temporal modeling. One way to model the
human actions is to employ generative models, such as a
Hidden Markov model (HMM) for a number of pre-deﬁned
relative position features from 3D joint positions , or
a conditional random ﬁeld (CRF) for the 3D joint positions . Similar approaches are also proposed to model
human actions in normal videos . However, the 3D
joint positions that are generated via skeleton tracking from
the depth map sequences are generally more noisy than that
of the MoCap data. When the difference between the actions is subtle, it is usually difﬁcult to determine the accurate states from the observation without careful selection
of the features, which undermines the performance of such
generative models. Moreover, with limited amount of training data, training a complex generative model is easy to
Another generative approach to dynamical patterns can
also be modeled by linear dynamical systems, and the states
of the system can be used for MoCap action categorization . In addition, the complex and nonlinear dynamics can also be characterized by a Recurrent Neural Network . Although these two approaches are good models for time series data and are robust to temporal misalignment, it is generally difﬁcult to learn these models from limited amount of training data.
Another method for modeling actions is dynamic temporal warping (DTW), which matches the 3D joint positions to
a template , and action recognition can be done through
a nearest-neighbor classiﬁcation method. Its performance
heavily depends on a good metric to measure the similarity
of frames. Moreover, for periodic actions (such as “waving”), DTW is likely to produce large temporal misalignment which may ruin action classiﬁcation .
Different from these approaches, we propose to employ
Fourier Temporal Pyramid to represent the temporal patterns. The Fourier temporal pyramid is a descriptive model.
It does not involve complicated learning as in the generative models (e.g., HMM, CRF and dynamical systems), and
it is much more robust than DTW to noise and temporal
misalignment.
For the action of a complex articulated structure, the motion of the individual parts are correlated. The relationship
among these parts (or high-order features) may be more discriminative than the individual ones. Such combinatorial
features can be represented by stochastic AND/OR structures. This idea has been pursued for face detection ,
human body parsing , object recognition , and human object interaction recognition . This paper presents
an initial attempt of using the AND/OR ensemble approach
to action recognition. We propose a novel data mining solution to discover the discriminative conjunction rules, and
apply multiple kernel learning to learn the ensemble.
3. Spatio-Temporal Features
This section gives a detailed description of two types of
features that we utilize to represent the actions: the 3D joint
position feature and the Local Occupancy Pattern (LOP).
These features can characterize the human motions as well
as the interactions between the objects and the human. In
addition, the Fourier Temporal Pyramid is proposed to represent the temporal dynamics. The proposed features are
invariant to the translation of the human and robust to noise
and temporal misalignment.
3.1. Invariant Features for 3D Joint Positions
The 3D joint positions are employed to shape the motion
of the human body. Our key observation is that representing
the human movement as the pairwise relative positions of
the joints results in more discriminative features.
For a human subject, 20 joint positions are tracked (the
Motion Capture system captures 30 joints) by the skeleton tracker and each joint i has 3 coordinates pi(t) =
(xi(t), yi(t), zi(t)) at a frame t. The coordinates are normalized so that the motion is invariant to the absolute body
position, the initial body orientation and the body size.
For each joint i, we extract the pairwise relative position
features by taking the difference between the position of
joint i and that of each other joint j:
pij = pi −pj,
The 3D joint feature for joint i is deﬁned as:
pi = {pij|i ̸= j}.
Although enumerating all the joint pairs introduces some
information that is irrelevant to our classiﬁcation task, our
approach is able to select the joints that are most relevant to
our recognition task. The selection will be handled by the
Actionlet mining as discussed in Section 4.
Representing the human motion as the relative joint positions results in more discriminative and intuitive features.
For example, the action “waving” is generally interpreted
as “arms above the shoulder and move left and right”. This
can be better characterized through the pairwise relative positions.
3.2. Local Occupancy Patterns
It is insufﬁcient to only use the 3D joint positions to fully
model an action, especially when the action includes the
interactions between the subject and other objects. Therefore, it is necessary to design a feature to describe the local
“depth appearance” for the joints. In this paper, the interaction between the human subject and the environmental
objects is characterized by the Local Occupancy Patterns
or LOP at each joint. For example, suppose a person is
drinking a cup of water. When the person fetches the cup,
the space around his/her hand is occupied by the cup. Afterwards, when the person lifts the cup to his/her mouth,
the space around both the hand and the head is occupied.
This information can be useful to characterize this interaction and to differentiate the drinking action from other actions.
In each frame, as described below, an LOP feature computes the local occupancy information based on the 3D
point cloud around a particular joint, so that the temporal
dynamics of all such occupancy patterns can roughly discriminate different types of interactions.
At frame t, we have the point cloud generated from the
depth map of this frame. For each joint j, its local region
is partitioned into Nx × Ny × Nz spatial grid. Each bin
of the grid is of size (Sx, Sy, Sz) pixels. For example, if
(Nx, Ny, Nz) = (12, 12, 4) and (Sx, Sy, Sz) = (6, 6, 80),
the local (96, 96, 320) region around a joint is partitioned
into 12 × 12 × 4 bins, and the size of each bin is (6,6,80).
The number of points at the current frame that fall into
each bin bxyz of the grid is counted, and a sigmoid normalization function is applied to obtain the feature oxyz for this
bin. In this way, the local occupancy information of this bin
where Iq = 1 if the point cloud has a point in the location
q and Iq = 0 otherwise. δ(.) is a sigmoid normalization
9NUXZ:OSK,U[XOKX:XGTYLUXS
9NUXZ:OSK,U[XOKX:XGTYLUXS
9NUXZ:OSK,U[XOKX:XGTYLUXS
Figure 2. A Illustration of the Fourier Temporal Pyramid.
function: δ(x) =
1+e−βx . The LOP feature of a joint i is
a vector consisting of the feature oxyz of all the bins in the
spatial grid around the joint, denoted by oi.
3.3. Fourier Temporal Pyramid
Two types of features are extracted from each frame t :
the 3D joint position features pi[t], and the LOP features
oi[t]. In this subsection, we propose the Fourier temporal
pyramid to represent the temporal dynamics of these framelevel features.
When using the current cost-effective depth camera, we
always experience noisy depth data and temporal misalignment. We aim to design temporal representations that are
robust to both the data noise and the temporal misalignment.
We also want such temporal features to be a good representation of the temporal structure of the actions. For example,
one action may contain two consecutive sub-actions: “bend
the body” and “pick up”. The proposed Fourier Temporal
Pyramid is a descriptive representation that satisﬁes these
properties.
Inspired by the Spatial Pyramid approach , in order
to capture the temporal structure of the action, in addition
to the global Fourier coefﬁcients, we recursively partition
the action into a pyramid, and use the short time Fourier
transform for all the segments, as illustrated in Fig. 2. The
ﬁnal feature is the concatenation of the Fourier coefﬁcients
from all the segments.
For each joint i, let gi = (pi, oi) denote its overall feature vector where pi is its 3D pairwise position vector and
oi is its LOP vector. Let Ni denote the dimension of gi, i.e.,
gi = (g1, . . . , gNi). Note that each element gj is a function
of time and we can write it as gj[t]. For each time segment
at each pyramid level, we apply Short Fourier Transform
 to element gj[t] and obtain its Fourier coefﬁcients, and
we utilize its low-frequency coefﬁcients as features. The
Fourier Temporal Pyramid feature at joint i is deﬁned as the
low-frequency coefﬁcients at all levels of the pyramid, and
is denoted as Gi.
The proposed Fourier Temporal Pyramid feature has
several beneﬁts.
First, by discarding the high-frequency
Fourier coefﬁcients, the proposed feature is robust to noise.
Second, this feature is insensitive to temporal misalignment,
because time series with temporal translation have the same
Fourier coefﬁcient magnitude. Finally, the temporal structure of the actions can be characterized by the pyramid
structure.
4. Actionlet Ensemble
Although the proposed feature is robust to noise, to deal
with the errors of the skeleton tracking and better characterize the intra-class variations, an actionlet ensemble approach is proposed in this section as a representation of the
An actionlet is deﬁned as a conjunctive (or AND) structure on the base features. One base feature is deﬁned as
a Fourier Pyramid feature of one joint. A discriminative
actionlet should be highly representative of one action and
highly discriminative compared to other actions. A novel
data mining algorithm is proposed to discover the discriminative actionlets.
Once we have mined a set of discriminative actionlets, a
multiple kernel learning approach is employed to learn
an actionlet ensemble structure that combines these discriminative actionlets.
4.1. Mining Discriminative Actionlets
An actionlet is denoted as a subset of joints S
{1, 2, · · · , Nj}, where Nj is the total number of joints.
Suppose we have training pairs (x(j), t(j)).
to determine how discriminative each individual joint is, a
SVM model is trained on feature Gi of each joint i. For
each training sample x(j) and the SVM model on the joint
i, the probability that its classiﬁcation label y(j) is equal to
an action class c is denoted as Pi(y(j) = c|x(j)), which
can be estimated from the pairwise probabilities by using
pairwise coupling approach .
Since an actionlet takes a conjunctive operation, it predicts y(j) = c if and only if every joint i ∈S predicts
y(j) = c. Thus, assuming the joints are independent, the
probability that the predicted label y(j) is equal to an action class c given an example x(j) for an actionlet S can be
computed as:
PS(y(j) = c|x(j)) =
Pi(y(j) = c|x(j))
Deﬁne Xc as {j : t(j) = c}. For an actionlet to be discriminative, the probability PS(y(j) = c|x(j)) should be
large for some data in Xc, and be small for all the data that
does not belong to Xc. Deﬁne the conﬁdence for actionlet
ConfS = max
j∈Xc log PS(y(j) = c|x(j))
and the ambiguity for actionlet S as
log PS(y(j) = c|x(j))
We would like a discriminative actionlet to have large con-
ﬁdence ConfS and small ambiguity AmbS. An actionlet
S is called an l-actionlet if its cardinality |S| = l. One
important property is that if we add a joint i /∈S to an
(l −1)-actionlet S to generate an l-actionlet S ∪{i}, we
have ConfS∪{i} ≤ConfS, i.e., adding a new joint into one
actionlet will always reduce the conﬁdence. As a result, the
Aprior mining process can be applied to select the actionlets with large ConfS and small AmbS. If ConfS is less
than the threshold, we do not need to consider any S′ with
S′ ⊃S. The outline of the mining process is shown in Alg.
1. For each class c, the mining algorithm outputs a discriminative actionlet pool Pc which contains the actionlets that
meet our criteria: AmbS ≤Tamb and ConfS ≥Tconf.
1 Take the set of joints, the feature Gi on each joint i,
the number of the classes C, thresholds Tconf and Tamb.
2 Train the base classiﬁer on the features Gi of each
3 for Class c = 1 to C do
Set Pc, the discriminative actionlet pool for class c
to be empty : Pc = {}. Set l = 1.
Generate the l-actionlets by adding one joint
into each (l −1)-actionlet in the
discriminative actionlet pool Pc.
Add the l-actionlets whose conﬁdences are
larger than Tconf to the pool Pc.
until no discriminative actionlet is added to Pc in
this iteration;
remove the actionlets whose ambiguities are larger
than Tamb in the pool Pc.
12 return the discriminative actionlet pool for all the
Algorithm 1: Discriminative Actionlet Mining
4.2. Learning Actionlet Ensemble
For each actionlet Sk in the discriminative actionlet pool,
an SVM model on it deﬁnes a joint feature map Φk(x, y) on
data X and labels Y as a linear output function fk(x, y) =
⟨wk, Φk(x, y)⟩+ bk, parameterized with the hyperplane
normal wk and bias bk. The predicted class y for x is chosen to maximize the output fk(x, y).
Multiclass-MKL considers a convex combination of p
kernels, K(xi, xj) = ∑p
k=1 βkKk(xi, xj), where each
kernel corresponds to an actionlet. Equivalently, we consider the following output function:
fﬁnal(x, y) =
[βk⟨wk, Φk(x, y)⟩+ bk]
We aim at choosing w = (wk) , b = (bk) , β =
(βk) , k = 1, . . . , p, such that given any training data pair
(x(i), y(i)), fﬁnal(x(i), y(i)) ≥fﬁnal(x(i), u) for all u ∈
Y −{y(i)}. The resulting optimization problem becomes:
s.t. ∀i : ξi = max
u̸=yi l(fﬁnal(x(i), y(i)) −fﬁnal(x(i), u))
where C is the regularization parameter and l is a convex
loss function, and Ω(β) is a regularization parameter on the
β. Following the approach in , we choose Ω(β) = ∥β∥2
to encourage a sparse β, so that an ensemble of a small number of actionlets is learned.
This problem can be solved by iteratively optimizing β
with ﬁxed w and b through linear programming, and optimizing w and b with ﬁxed β through a generic SVM solver
such as LIBSVM.
5. Experimental Results
We choose CMU MoCap dataset , MSR-Action3D
dataset and MSRDailyActivity3D dataset to evaluate
the proposed action recognition approach. In all the experiments, we use three-level Fourier temporal pyramid, with
1/4 length of each segment as low-frequency coefﬁcients.
The empirical results show that the proposed framework
outperforms the state of the art methods.
5.1. MSR-Action3D Dataset
MSR-Action3D dataset is an action dataset of depth
sequences captured by a depth camera. This dataset contains twenty actions: high arm wave, horizontal arm wave,
hammer, hand catch, forward punch, high throw, draw x,
draw tick, draw circle, hand clap, two hand wave, sideboxing, bend, forward kick, side kick, jogging, tennis swing,
tennis serve, golf swing, pick up & throw. Each action was
performed by ten subjects for three times. The frame rate is
15 frames per second and resolution 640×480. Altogether,
the dataset has 23797 frames of depth map for 402 action
Figure 3. Sample frames of the MSR-Action3D dataset.
Recurrent Neural Network 
Dynamic Temporal Warping 
Hidden Markov Model 
Action Graph on Bag of 3D Points 
Proposed Method
Table 1. Recognition Accuracy Comparison for MSR-Action3D
samples. Some examples of the depth sequences are shown
in Fig. 3.
Those actions were chosen to cover various movement
of arms, legs, torso and their combinations, and the subjects
were advised to use their right arm or leg if an action is
performed by a single arm or leg. Although the background
of this dataset is clean, this dataset is challenging because
many of the actions in the dataset are highly similar to each
The 3D joint positions are extracted from the depth sequence by using the real time skeleton tracking algorithm
proposed in . Since there is no human-object interaction in this dataset, we only extract the 3D joint position
We compare our method with the state-of-the-art methods on the cross-subject test setting , where the samples
of half of the subjects are used as training data, and the rest
of the samples are used as test data. The recognition accuracy of the dynamic temporal warping is only 54%, because some of actions in the dataset are very similar to each
other, and there are typical large temporal misalignment in
the dataset. The accuracy of recurrent neural network is
42.5%. The accuracy of Hidden Markov Model is 63%.
The proposed method achieves an accuracy of 88.2%. This
is a very good performance considering that the skeleton
tracker sometimes fails and the tracked joint positions are
quite noisy. The confusion matrix is illustrated in Fig. 4.
For most of the actions, our method works very well. The
classiﬁcation errors occur if two actions are too similar to
each other, such as “hand catch” and “high throw”, or if
the occlusion is so large that the skeleton tracker fails frequently, such as the action “pick up and throw”.
The comparison between the robustness of the Fourier
highArmWave
horizontalArmWave
forwardPunch
drawCircle
twoHandWave
sideBoxing
forwardKick
tennisSwing
tennisServe
pickUpThrow
highArmWave
horizontalArmWave
forwardPunch
drawCircle
twoHandWave
sideBoxing
forwardKick
tennisSwing
tennisServe
pickUpThrow
Figure 4. The confusion matrix for MSR-Action3D dataset.
Fourier Temporal
Hidden Markov Model
The standard vara!on of the added noise
Rela!ve Accuracy
Fourier Temporal
Hidden Markov Model
The number of frames shi!ed
Rela"ve Accuracy
Figure 5. The relationship between the relative accuracy and the
noise or temporal shift.
Temporal Pyramid features and that of Hidden Markov
Model is shown in Fig. 5(a). In this experiment, we add
white Gaussian noise to the 3D joint positions of all samples, and compare the relative accuracies of the two methods. For each method, its relative accuracy is deﬁned as
the accuracy under the noisy environment divided by the
accuracy under the environment without noise. We can see
that the proposed Fourier Temporal Pyramid feature is much
more robust to noise than the Hidden Markov Model.
The robustness of the proposed method and the Hidden Markov model to temporal shift is also compared. In
Figure 6. Sample frames of the DailyActivity3D dataset.
this experiment, we circularly shift all the training data,
and keep the test data unchanged. The relative accuracy
is shown in Fig. 5(b). It can be seen that both methods are
robust to the temporal shift of the depth sequences, though
the Fourier Temporal Pyramid is slightly more sensitive to
temporal shift than the Hidden Markov Model.
5.2. MSRDailyActivity3D Dataset
DailyActivity3D dataset1 is a daily activity dataset captured by a Kinect device. There are 16 activity types: drink,
eat, read book, call cellphone, write on a paper, use laptop, use vacuum cleaner, cheer up, sit still, toss paper, play
game, lay down on sofa, walk, play guitar, stand up, sit
down. If possible, each subject performs an activity in two
different poses: “sitting on sofa” and “standing”. The total number of the activity samples is 320. Some example
activities are shown in Fig. 6.
This dataset is designed to cover human’s daily activities
in the living room. When the performer stands close to the
sofa or sits on the sofa, the 3D joint positions extracted by
the skeleton tracker are very noisy. Moreover, most of the
activities involve the humans-object interactions. Thus this
dataset is more challenging.
Table 2 shows the accuracies of different methods. By
employing an actionlet ensemble model, we obtain a recognition accuracy of 85.75%. This is a decent result considering the difﬁculties in this dataset. If we directly train a
SVM on the Fourier Temporal Pyramid features, the accuracy is 78%. When only the LOP feature is employed, the
recognition accuracy drops to 42.5%. If we only use 3D
joint position features without using LOP, the recognition
accuracy is 68%.
Fig. 7 shows the confusion matrix of the proposed
method. Fig. 8 compares the accuracy of the actionlet ensemble method and that of the support vector machine on
the Fourier Temporal Pyramid features. We can observe
that for the activities where the hand gets too close to the
body, the proposed actionlet ensemble method can signiﬁcantly improve the accuracy. Fig. 9 illustrates the actionlets
with high weights discovered by our mining algorithm.
5.3. CMU MoCap Dataset
We also evaluate the proposed method on the 3D joint
positions extracted by a motion capture system. The dataset
1 
callCellphone
vaccumCleaner
playGuitar
callCellphone
vaccumCleaner
playGuitar
Figure 7. The confusion matrix of the proposed method on Daily-
Activity3D dataset.
callCellphone
vaccumCleaner
playGuitar
SVM on Fourier Features
Ac onlet Ensemble
Figure 8. The comparison between the accuracy of the proposed
actionlet ensemble method and that of the support vector machine
on the Fourier Temporal Pyramid features.
Figure 9. Examples of the mined actionlets. The joints contained
in each actionlet are marked as red.
(a), (b) are actionlets for
“drink” (c), (d) are actionlets for “call”. (e), (f) are actionlets for
we use is the CMU Motion Capture (MoCap) dataset.
Five subtle actions are chosen from CMU MoCap
datasets following the conﬁguration in .
The ﬁve actions differ from each other only in the motion of one or
two limbs. The actions in this dataset include: walking,
marching, dribbling, walking with stiff arms, walking with
wild legs. The 3D joint positions in CMU MoCap dataset
are relatively clean because they are captured with high-
Dynamic Temporal Warping 
Only LOP features
Only Joint Position features
SVM on Fourier Temporal Pyramid Features
Actionlet Ensemble
Table 2. Recognition Accuracy Comparison for DailyActivity3D
CRF with learned manifold space 
Proposed Method
Table 3. Recognition Accuracy Comparison for CMU MoCap
precision camera array and markers. This dataset is employed to evaluate the performance of the proposed 3D joint
position-based features on 3D joint positions captured by
Motion Capture system.
The comparison of the performance is shown in Table 3.
Since only the 3D joint positions are available, the propose
method only utilizes the 3D joint position features. It can be
seen that the proposed method achieves comparable results
with the state of the art methods on the MoCap dataset.
6. Conclusion
We have proposed novel features and an actionlet ensemble model for human action recognition with depth cameras.
The proposed features are discriminative enough to classify
human actions with subtle differences as well as humanobject interactions and robust to noise and temporal misalignment. The actionlet ensemble model is capable of better capturing the intra-class variations and is more robust to
the noises and errors in the depth maps and joint positions.
The experiments demonstrated the superior performance of
the proposed approach to the state of the art methods. In the
future, we aim to exploit the effectiveness of the proposed
technique for the understanding of more complex activities.
7. Acknowledgements
This work was supported in part by National Science
Foundation grant IIS-0347877, IIS-0916607, US Army Research Laboratory and the US Army Research Ofﬁce under grant ARO W911NF-08-1-0504, and DARPA Award
FA 8650-11-1-7149. This work is partially supported by
Microsoft Research.