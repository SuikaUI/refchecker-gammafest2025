Efﬁcient Architecture Search
by Network Transformation
Han Cai,1 Tianyao Chen,1 Weinan Zhang,1∗Yong Yu,1 Jun Wang2
1Shanghai Jiao Tong University, 2University College London
{hcai,tychen,wnzhang,yyu}@apex.sjtu.edu.cn, 
Techniques for automatically designing deep neural network architectures such as reinforcement learning based approaches have recently shown promising results. However,
their success is based on vast computational resources (e.g.
hundreds of GPUs), making them difﬁcult to be widely used.
A noticeable limitation is that they still design and train each
network from scratch during the exploration of the architecture space, which is highly inefﬁcient. In this paper, we propose a new framework toward efﬁcient architecture search by
exploring the architecture space based on the current network
and reusing its weights. We employ a reinforcement learning agent as the meta-controller, whose action is to grow the
network depth or layer width with function-preserving transformations. As such, the previously validated networks can
be reused for further exploration, thus saves a large amount
of computational cost. We apply our method to explore the
architecture space of the plain convolutional neural networks
(no skip-connections, branching etc.) on image benchmark
datasets (CIFAR-10, SVHN) with restricted computational
resources (5 GPUs). Our method can design highly competitive networks that outperform existing networks using
the same design scheme. On CIFAR-10, our model without skip-connections achieves 4.23% test error rate, exceeding a vast majority of modern architectures and approaching
DenseNet. Furthermore, by applying our method to explore
the DenseNet architecture space, we are able to achieve more
accurate networks with fewer parameters.
Introduction
The great success of deep neural networks in various challenging applications 
has led to a paradigm shift from feature designing to architecture designing, which still remains a laborious task and
requires human expertise. In recent years, many techniques
for automating the architecture design process have been
proposed , and promising results of designing competitive models against humandesigned models are reported on some benchmark datasets
∗Correspondence to Weinan Zhang.
Copyright c⃝2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
 . Despite the promising
results as reported, their success is based on vast computational resources (e.g. hundreds of GPUs), making them dif-
ﬁcult to be used in practice for individual researchers, small
sized companies, or university research teams. Another key
drawback is that they still design and train each network
from scratch during exploring the architecture space without
any leverage of previously explored networks, which results
in high computational resources waste.
In fact, during the architecture design process, many
slightly different networks are trained for the same task.
Apart from their ﬁnal validation performances that are used
to guide exploration, we should also have access to their
architectures, weights, training curves etc., which contain
abundant knowledge and can be leveraged to accelerate the
architecture design process just like human experts . Furthermore, there are typically many well-designed architectures,
by human or automatic architecture designing methods, that
have achieved good performances at the target task. Under
restricted computational resources limits, instead of totally
neglecting these existing networks and exploring the architecture space from scratch (which does not guarantee to result in better performance architectures), a more economical
and efﬁcient alternative could be exploring the architecture
space based on these successful networks and reusing their
In this paper, we propose a new framework, called EAS,
Efﬁcient Architecture Search, where the meta-controller explores the architecture space by network transformation operations such as widening a certain layer (more units or ﬁlters), inserting a layer, adding skip-connections etc., given
an existing network trained on the same task. To reuse
weights, we consider the class of function-preserving transformations that allow
to initialize the new network to represent the same function
as the given network but use different parameterization to
be further trained to improve the performance, which can
signiﬁcantly accelerate the training of the new network especially for large networks. Furthermore, we combine our
framework with recent advances of reinforcement learning (RL) based automatic architecture designing methods
 , and employ a RL
based agent as the meta-controller.
The Thirty-Second AAAI Conference
on Artificial Intelligence (AAAI-18)
Our experiments of exploring the architecture space of
the plain convolutional neural networks (CNNs), which
purely consists of convolutional, fully-connected and pooling layers without skip-connections, branching etc., on image benchmark datasets (CIFAR-10, SVHN), show that EAS
with limited computational resources (5 GPUs) can design
competitive architectures. The best plain model designed
by EAS on CIFAR-10 with standard data augmentation
achieves 4.23% test error rate, even better than many modern
architectures that use skip-connections. We further apply our
method to explore the DenseNet architecture space, and achieve 4.66% test error rate on CIFAR-
10 without data augmentation and 3.44% on CIFAR-10 with
standard data augmentation, surpassing the best results given
by the original DenseNet while still maintaining fewer parameters.
Related Work and Background
Automatic Architecture Designing
There is a long standing study on automatic architecture designing. Neuroevolution algorithms which mimic the evolution processes
in the nature, are one of the earliest automatic architecture designing methods . Authors in used neuro-evolution algorithms to explore a large
CNN architecture space and achieved networks which can
match performances of human-designed models. In parallel, automatic architecture designing has also been studied in the context of Bayesian optimization . Recently, reinforcement learning is introduced in automatic architecture designing and has shown
strong empirical results. Authors in presented a Q-learning agent to sequentially pick CNN layers;
authors in used an auto-regressive recurrent network to generate a variable-length string that speci-
ﬁes the architecture of a neural network and trained the recurrent network with policy gradient.
As the above solutions rely on designing or training
networks from scratch, signiﬁcant computational resources
have been wasted during the construction. In this paper,
we aim to address the efﬁciency problem. Technically, we
allow to reuse the existing networks trained on the same
task and take network transformation actions. Both functionpreserving transformations and an alternative RL based
meta-controller are used to explore the architecture space.
Moreover, we notice that there are some complementary
techniques, such as learning curve prediction , for improving the efﬁciency, which can be combined
with our method.
Transformation
Generally, any modiﬁcation to a given network can be
viewed as a network transformation operation. In this paper, since our aim is to utilize knowledge stored in previously trained networks, we focus on identifying the kind
of network transformation operations that would be able to
reuse pre-existing models. The idea of reusing pre-existing
models or knowledge transfer between neural networks
has been studied before. Net2Net technique introduced in
 describes two speciﬁc
function-preserving transformations, namely Net2WiderNet
and Net2DeeperNet, which respectively initialize a wider or
deeper student network to represent the same functionality
of the given teacher network and have proved to signiﬁcantly
accelerate the training of the student network especially for
large networks. Similar function-preserving schemes have
also been proposed in ResNet particularly for training very
deep architectures . Additionally, the network compression technique presented in 
prunes less important connections (low-weight connections)
in order to shrink the size of neural networks without reducing their accuracy.
In this paper, instead, we focus on utilizing such network
transformations to reuse pre-existing models to efﬁciently
and economically explore the architecture space for automatic architecture designing.
Reinforcement
Background
metacontroller in this work is based on RL , techniques for training the agent to maximize
cumulative
interacting
an environment . We use the REIN-
FORCE algorithm similar to for updating the meta-controller, while
other advanced policy gradient methods can be applied analogously. Our
action space is, however, different with that of or any other RL based approach ,
as our actions are the network transformation operations
like adding, deleting, widening, etc., while others are
speciﬁc conﬁgurations of a newly created network layer
on the top of preceding layers. Speciﬁcally, we model the
automatic architecture design procedure as a sequential
decision making process, where the state is the current
network architecture and the action is the corresponding
network transformation operation. After T steps of network
transformations, the ﬁnal network architecture, along with
its weights transferred from the initial input network, is then
trained in the real data to get the validation performance
to calculate the reward signal, which is further used to
update the meta-controller via policy gradient algorithms
to maximize the expected validation performances of the
designed networks by the meta-controller.
Architecture Search by Net Transformation
In this section, we ﬁrst introduce the overall framework
of our meta-controller, and then show how each speciﬁc
network transformation decision is made under it. We
later extend the function-preserving transformations to the
DenseNet architecture space where directly applying the original Net2Net operations can be problematic since the output of a layer will be fed to all subsequent layers.
We consider learning a meta-controller to generate network transformation actions given the current network architecture, which is speciﬁed with a variable-length string
 . To be able to generate various types
Network Transformation
CONV(32,3,1)
CONV(64,5,1)
Actor Network
Layer Embedding
Update the network
Actor Networks
Network Transformation
Layer Embedding
Layer Embedding
Layer Embedding
Layer Embedding
Net2Deeper
Actor Network
Figure 1: Overview of the RL based meta-controller in EAS,
which consists of an encoder network for encoding the architecture and multiple separate actor networks for taking
network transformation actions.
of network transformation actions while keeping the metacontroller simple, we use an encoder network to learn a lowdimensional representation of the given architecture, which
is then fed into each separate actor network to generate
a certain type of network transformation actions. Furthermore, to handle variable-length network architectures as input and take the whole input architecture into consideration when making decisions, the encoder network is implemented with a bidirectional recurrent network with an input embedding layer. The overall
framework is illustrated in Figure 1, which is an analogue
of end-to-end sequence to sequence learning .
Actor Networks
Given the low dimensional representation of the input architecture, each actor network makes necessary decisions for
taking a certain type of network transformation actions. In
this work, we introduce two speciﬁc actor networks, namely
Net2Wider actor and Net2Deeper actor which correspond to
Net2WiderNet and Net2DeeperNet respectively.
Net2Wider Actor
Net2WiderNet operation allows to replace a layer with a wider layer, meaning more units for
fully-connected layers, or more ﬁlters for convolutional layers, while preserving the functionality. For example, consider a convolutional layer with kernel Kl whose shape is
o) where kl
h denote the ﬁlter width and
height, while f l
o denote the number of input and output channels. To replace this layer with a wider layer that
o) output channels, we should ﬁrst introduce a
random remapping function Gl, which is deﬁned as
random sample from {1, · · · , f l
o <j ≤ˆf l
With the remapping function Gl, we have the new kernel ˆKl
for the wider layer with shape (kl
ˆKl[x, y, i, j] = Kl[x, y, i, Gl(j)].
As such, the ﬁrst f l
o entries in the output channel dimension
of ˆKl are directly copied from Kl while the remaining ˆf l
Probability of
Widening the Layer
Classifier
Classifier
Classifier
Decision of
Widening the Layer
Figure 2: Net2Wider actor, which uses a shared sigmoid
classiﬁer to simultaneously determine whether to widen
each layer based on its hidden state given by the encoder
o entries are created by choosing randomly as deﬁned in
Gl. Accordingly, the new output of the wider layer is ˆOl
with ˆOl(j) = Ol(Gl(j)), where Ol is the output of the
original layer and we only show the channel dimension to
make the notation simpler.
To preserve the functionality, the kernel Kl+1 of the next
layer should also be modiﬁed due to the replication in its
input. The new kernel ˆKl+1 with shape (kl+1
) is given as
ˆKl+1[x, y, j, k] = Kl+1[x, y, Gl(j), k]
{z|Gl(z) = Gl(j)}
For further details, we refer to the original Net2Net work
 .
In our work, to be ﬂexible and efﬁcient, the Net2Wider
actor simultaneously determines whether each layer should
be extended. Speciﬁcally, for each layer, this decision is
carried out by a shared sigmoid classiﬁer given the hidden state of the layer learned by the bidirectional encoder
network. Moreover, we follow previous work and search
the number of ﬁlters for convolutional layers and units for
fully-connected layers in a discrete space. Therefore, if the
Net2Wider actor decides to widen a layer, the number of
ﬁlters or units of the layer increases to the next discrete
level, e.g. from 32 to 64. The structure of Net2Wider actor
is shown in Figure 2.
Net2Deeper Actor
Net2DeeperNet operation allows to
insert a new layer that is initialized as adding an identity
mapping between two layers so as to preserve the functionality. For a new convolutional layer, the kernel is set to be identity ﬁlters while for a new fully-connected layer, the weight
matrix is set to be identity matrix. Thus the new layer is set
with the same number of ﬁlters or units as the layer below
at ﬁrst, and could further get wider when Net2WiderNet operation is performed on it. To fully preserve the functionality, Net2DeeperNet operation has a constraint on the activation function φ, i.e. φ must satisfy φ(Iφ(v)) = φ(v) for all
vectors v. This property holds for rectiﬁed linear activation
(ReLU) but fails for sigmoid and tanh activation. However,
we can still reuse weights of existing networks with sigmoid
Parameters of New Layer
if Applicable (CNN as example)
Initial State
Net2Deeper
Figure 3: Net2Deeper actor, which uses a recurrent network
to sequentially determine where to insert the new layer and
corresponding parameters for the new layer based on the ﬁnal hidden state of the encoder network given the input architecture.
or tanh activation, which could be useful compared to random initialization. Additionally, when using batch normalization , we need to set output scale
and output bias of the batch normalization layer to undo the
normalization, rather than initialize them as ones and zeros.
Further details about the Net2DeeperNet operation is provided in the original paper , we allow the Net2Deeper actor to insert one new layer at each
step. Speciﬁcally, we divide a CNN architecture into several blocks according to the pooling layers and Net2Deeper
actor sequentially determines which block to insert the new
layer, a speciﬁc index within the block and parameters of the
new layer. For a new convolutional layer, the agent needs to
determine the ﬁlter size and the stride while for a new fullyconnected layer, no parameter prediction is needed. In CNN
architectures, any fully-connected layer should be on the top
of all convolutional and pooling layers. To avoid resulting in
unreasonable architectures, if the Net2Deeper actor decides
to insert a new layer after a fully-connected layer or the ﬁnal
global average pooling layer, the new layer is restricted to be
a fully-connected layer, otherwise it must be a convolutional
Function-preserving Transformation for DenseNet
The original Net2Net operations proposed in are discussed under the scenarios
where the network is arranged layer-by-layer, i.e. the output
of a layer is only fed to its next layer. As such, in some modern CNN architectures where the output of a layer would be
fed to multiple subsequent layers, such as DenseNet , directly applying the original Net2Net operations can be problematic. In this section, we introduce several extensions to the original Net2Net operations to enable
function-preserving transformations for DenseNet.
Different from the plain CNN, in DenseNet, the lth layer
would receive the outputs of all preceding layers as input,
which are concatenated on the channel dimension, denoted
as [O0, O1, · · · , Ol−1], while its output Ol would be fed to
all subsequent layers.
Denote the kernel of the lth layer as Kl with shape
o). To replace the lth layer with a wider layer
that has ˆf l
o output channels while preserving the functionality, the creation of the new kernel ˆKl in the lth layer
is the same as the original Net2WiderNet operation (see
Eq. (1) and Eq. (2)). As such, the new output of the wider
layer is ˆOl with ˆOl(j) = Ol(Gl(j)), where Gl is the
random remapping function as deﬁned in Eq. (1). Since
the output of the lth layer will be fed to all subsequent
layers in DenseNet, the replication in ˆOl will result in
replication in the inputs of all layers after the lth layer.
As such, instead of only modifying the kernel of the next
layer as done in the original Net2WiderNet operation, we
need to modify the kernels of all subsequent layers in
DenseNet. For the mth layer where m > l, its input becomes [O0, · · · , Ol−1, ˆOl, Ol+1, · · · , Om−1] after widening the lth layer, thus from the perspective of mth layer, the
equivalent random remapping function ˆGm can be written
1 ≤j ≤f 0:l
o <j ≤f 0:l
o < j ≤f 0:m
where f 0:l
o is the number of input channels for
the lth layer, the ﬁrst part corresponds to [O0, · · · , Ol−1],
the second part corresponds to [ ˆOl], and the last part corresponds to [Ol+1, · · · , Om−1]. A simple example of ˆGm is
ˆGm : {1, · · · , 5,
6, 7, 8, 9, 10, 11} →{1, · · · , 5,
6, 7, 6, 6, 8, 9}
where Gl : {1, 2, 3, 4} →{1, 2, 1, 1}.
Accordingly the new kernel of mth layer can be given by
Eq. (3) with Gl replaced with ˆGm.
To insert a new layer in DenseNet, suppose the new
layer is inserted after the lth layer. Denote the output of
the new layer as Onew, and its input is [O0, O1, · · · , Ol].
Therefore, for the mth (m > l) layer, its new input after
the insertion is [O0, O1, · · · , Ol, Onew, Ol+1, · · · , Om−1].
To preserve the functionality, similar to the Net2WiderNet
case, Onew should be the replication of some entries in
[O0, O1, · · · , Ol]. It is possible, since the input of the new
layer is [O0, O1, · · · , Ol]. Each ﬁlter in the new layer can
be represented with a tensor, denoted as ˆF with shape
), where knew
denote the
width and height of the ﬁlter, and f new
is the number of input channels. To make the output of ˆF to be a replication
of the nth entry in [O0, O1, · · · , Ol], we can set ˆF (using
the special case that knew
= 3 for illustration) as the
ˆF [x, y, n] =
while all other values in ˆF are set to be 0. Note that n can be
chosen randomly from {1, · · · , f 0:l+1
} for each ﬁlter. After
all ﬁlters in the new layer are set, we can form an equivalent
random remapping function for all subsequent layers as is
done in Eq. (4) and modify their kernels accordingly.
Experiments and Results
In line with the previous work , we apply the proposed EAS on
image benchmark datasets (CIFAR-10 and SVHN) to explore high performance CNN architectures for the image
classiﬁcation task1. Notice that the performances of the ﬁnal
designed models largely depend on the architecture space
and the computational resources. In our experiments, we
evaluate EAS in two different settings. In all cases, we use
restricted computational resources (5 GPUs) compared to
the previous work such as that used
800 GPUs. In the ﬁrst setting, we apply EAS to explore the
plain CNN architecture space, which purely consists of convolutional, pooling and fully-connected layers. While in the
second setting, we apply EAS to explore the DenseNet architecture space.
Image Datasets
The CIFAR-10 dataset consists of 50,000 training images and 10,000
test images. We use a standard data augmentation scheme
that is widely used for CIFAR-10 , and
denote the augmented dataset as C10+ while the original
dataset is denoted as C10. For preprocessing, we normalized the images using the channel means and standard deviations. Following the previous work , we randomly sample 5,000 images from
the training set to form a validation set while using the remaining 45,000 images for training during exploring the architecture space.
The Street View House Numbers (SVHN) dataset
 contains 73,257 images in the original
training set, 26,032 images in the test set, and 531,131 additional images in the extra training set. For preprocessing, we
divide the pixel values by 255 and do not perform any data
augmentation, as is done in . We follow
 and use the original training set during
the architecture search phase with 5,000 randomly sampled
images as the validation set, while training the ﬁnal discovered architectures using all the training data, including the
original training set and extra training set.
1Experiment code and discovered top architectures along with
weights: 
Figure 4: Progress of two stages architecture search on C10+
in the plain CNN architecture space.
Training Details
For the meta-controller, we use a one-layer bidirectional
LSTM with 50 hidden units as the encoder network (Figure 1) with an embedding size of 16, and train it with the
ADAM optimizer .
At each step, the meta-controller samples 10 networks by
taking network transformation actions. Since the sampled
networks are not trained from scratch but we reuse weights
of the given network in our scenario, they are then trained for
20 epochs, a relative small number compared to 50 epochs in
 . Besides, we use a smaller initial learning rate for this reason. Other settings for training networks
on CIFAR-10 and SVHN, are similar to . Speciﬁcally, we use the SGD with a
Nesterov momentum of 0.9, a weight
decay of 0.0001, a batch size of 64. The initial learning rate
is 0.02 and is further annealed with a cosine learning rate
decay . The accuracy in the held-out validation set is used to compute the reward signal for each sampled network. Since the gain of improving the accuracy from
90% to 91% should be much larger than from 60% to 61%,
instead of directly using the validation accuracy accv as the
reward, as done in , we perform a nonlinear transformation on accv, i.e. tan(accv × π/2), and use
the transformed value as the reward. Additionally, we use
an exponential moving average of previous rewards, with a
decay of 0.95 as the baseline function to reduce the variance.
Explore Plain CNN Architecture Space
We start applying EAS to explore the plain CNN architecture space. Following the previous automatic architecture designing methods , EAS searches layer parameters in a discrete and limited space. For every convolutional layer, the ﬁlter size is
chosen from {1, 3, 5} and the number of ﬁlters is chosen from {16, 32, 64, 96, 128, 192, 256, 320, 384, 448, 512},
while the stride is ﬁxed to be 1 . For every
fully-connected layer, the number of units is chosen from
{64, 128, 256, 384, 512, 640, 768, 896, 1024}. Additionally,
Table 1: Simple start point network. C(n, f, l) denotes a convolutional layer with n ﬁlters, ﬁlter size f and stride l; P(f, l, MAX)
and P(f, l, AVG) denote a max and an average pooling layer with ﬁlter size f and stride l respectively; FC(n) denotes a fullyconnected layer with n units; SM(n) denotes a softmax layer with n output units.
Model Architecture
Validation Accuracy (%)
C(16, 3, 1), P(2, 2, MAX), C(32, 3, 1), P(2, 2, MAX), C(64, 3, 1),
P(2, 2, MAX), C(128, 3, 1), P(4, 4, AVG), FC(256), SM(10)
we use ReLU and batch normalization for each convolutional or fully-connected layer. For SVHN, we add a dropout
layer after each convolutional layer (except the ﬁrst layer)
and use a dropout rate of 0.2 .
Start with Small Network
We begin the exploration on
C10+, using a small network (see Table 1), which achieves
87.07% accuracy in the held-out validation set, as the start
point. Different from ,
EAS is not restricted to start from empty and can ﬂexibly
use any discovered architecture as the new start point. As
such, to take the advantage of such ﬂexibility and also reduce the search space for saving the computational resources
and time, we divide the whole architecture search process
into two stages where we allow the meta-controller to take 5
steps of Net2Deeper action and 4 steps of Net2Wider action
in the ﬁrst stage. After 300 networks are sampled, we take
the network which performs best currently and train it with
a longer period of time (100 epochs) to be used as the start
point for the second stage. Similarly, in the second stage, we
also allow the meta-controller to take 5 steps of Net2Deeper
action and 4 steps of Net2Wider action and stop exploration
after 150 networks are sampled.
The progress of the two stages architecture search is
shown in Figure 4, where we can ﬁnd that EAS gradually learns to pick high performance architectures at each
stage. As EAS takes function-preserving transformations to
explore the architecture space, we can also ﬁnd that the
sampled architectures consistently perform better than the
start point network at each stage. Thus it is usually “safe”
to explore the architecture space with EAS. We take the
top networks discovered during the second stage and further train the networks with 300 epochs using the full training set. Finally, the best model achieves 95.11% test accuracy (i.e. 4.89% test error rate). Furthermore, to justify
the transferability of the discovered networks, we train the
top architecture (95.11% test accuracy) on SVHN from random initialization with 40 epochs using the full training
set and achieves 98.17% test accuracy (i.e. 1.83% test error rate), better than both human-designed and automatically
designed architectures that are in the plain CNN architecture
space (see Table 2).
We would like to emphasize that the required computational resources to achieve this result is much smaller than
those required in .
Speciﬁcally, it takes less than 2 days on 5 GeForce GTX
1080 GPUs with totally 450 networks trained to achieve
4.89% test error rate on C10+ starting from a small network.
Further Explore Larger Architecture Space
To further
search better architectures in the plain CNN architecture
Table 2: Test error rate (%) comparison with CNNs that use
convolutional, fully-connected and pooling layers alone.
Maxout 
NIN 
All-CNN 
VGGnet 
MetaQNN (depth=7)
MetaQNN (ensemble)
EAS (plain CNN, depth=16)
EAS (plain CNN, depth=20)
space, in the second experiment, we use the top architectures discovered in the ﬁrst experiment, as the start points
to explore a larger architecture space on C10+ and SVHN.
This experiment on each dataset takes around 2 days on 5
The summarized results of comparing with humandesigned and automatically designed architectures that use
a similar design scheme (plain CNN), are reported in Table
2, where we can ﬁnd that the top model designed by EAS
on the plain CNN architecture space outperforms all similar
models by a large margin. Speciﬁcally, comparing to humandesigned models, the test error rate drops from 7.25% to
4.23% on C10+ and from 2.35% to 1.73% on SVHN. While
comparing to MetaQNN, the Q-learning based automatic architecture designing method, EAS achieves a relative test error rate reduction of 38.9% on C10+ and 16.0% on SVHN.
We also notice that the best model designed by MetaQNN
on C10+ only has a depth of 7, though the maximum is set
to be 18 in the original paper . We suppose maybe they trained each designed network from scratch
and used an aggressive training strategy to accelerate training, which resulted in many networks under performed, especially for deep networks. Since we reuse the weights of
pre-existing networks, the deep networks are validated more
accurately in EAS, and we can thus design deeper and more
accurate networks than MetaQNN.
We also report the comparison with state-of-the-art architectures that use advanced techniques such as skipconnections, branching etc., on C10+ in Table 3. Though it
is not a fair comparison since we do not incorporate such
advanced techniques into the search space in this experiment, we still ﬁnd that the top model designed by EAS is
highly competitive even comparing to these state-of-the-art
modern architectures. Speciﬁcally, the 20-layers plain CNN
with 23.4M parameters outperforms ResNet, its stochastic depth variant and its pre-activation variant. It also approaches the best result given by DenseNet. When comparing to automatic architecture designing methods that in-
Table 3: Test error rate (%) comparison with state-of-the-art architectures.
ResNet 
ResNet (stochastic depth) 
Wide ResNet 
Wide ResNet 
ResNet (pre-activation) 
DenseNet (L = 40, k = 12) 
DenseNet-BC (L = 100, k = 12) 
DenseNet-BC (L = 190, k = 40) 
Large-Scale Evolution (250 GPUs) 
NAS (predicting strides, 800 GPUs) 
NAS (max pooling, 800 GPUs) 
NAS (post-processing, 800 GPUs) 
EAS (plain CNN, 5 GPUs)
Figure 5: Comparison between RL based meta-controller
and random search on C10+.
Table 4: Test error rate (%) results of exploring DenseNet
architecture space with EAS.
DenseNet (L = 100, k = 24)
DenseNet-BC (L = 250, k = 24)
DenseNet-BC (L = 190, k = 40)
NAS (post-processing)
EAS (DenseNet on C10)
EAS (DenseNet on C10+)
corporate skip-connections into their search space, our 20layers plain model beats most of them except NAS with
post-processing, that is much deeper and has more parameters than our model. Moreover, we only use 5 GPUs and
train hundreds of networks while they use 800 GPUs and
train tens of thousands of networks.
Comparison Between RL and Random Search
framework is not restricted to use the RL based metacontroller. Beside RL, one can also take network transformation actions to explore the architecture space by random
search, which can be effective in some cases . In this experiment, we compare the performances of the RL based meta-controller and the random
search meta-controller in the architecture space that is used
in the above experiments. Speciﬁcally, we use the network
in Table 1 as the start point and let the meta-controller to
take 5 steps of Net2Deeper action and 4 steps of Net2Wider
action. The result is reported in Figure 5, which shows that
the RL based meta-controller can effectively focus on the
right search direction, while the random search cannot (left
plot), and thus ﬁnd high performance architectures more ef-
ﬁciently than random search.
Explore DenseNet Architecture Space
We also apply EAS to explore the DenseNet architecture
space. We use the DenseNet-BC (L = 40, k = 40) as
the start point. The growth rate, i.e. the width of the nonbottleneck layer is chosen from {40, 44, 48, 52, 56, 60, 64},
and the result is reported in Table 4. We ﬁnd that by applying EAS to explore the DenseNet architecture space, we
achieve a test error rate of 4.66% on C10, better than the
best result, i.e. 5.19% given by the original DenseNet while
having 43.79% less parameters. On C10+, we achieve a test
error rate of 3.44%, also outperforming the best result, i.e.
3.46% given by the original DenseNet while having 58.20%
less parameters.
Conclusion
In this paper, we presented EAS, a new framework toward economical and efﬁcient architecture search, where
the meta-controller is implemented as a RL agent. It learns
to take actions for network transformation to explore the
architecture space. By starting from an existing network
and reusing its weights via the class of function-preserving
transformation operations, EAS is able to utilize knowledge
stored in previously trained networks and take advantage
of the existing successful architectures in the target task to
explore the architecture space efﬁciently. Our experiments
have demonstrated EAS’s outstanding performance and ef-
ﬁciency compared with several strong baselines. For future
work, we would like to explore more network transformation operations and apply EAS for different purposes such
as searching networks that not only have high accuracy but
also keep a balance between the size and the performance.
Acknowledgments
This research was sponsored by Huawei Innovation Research Program, NSFC (61702327) and Shanghai Sailing
Program (17YF1428200).