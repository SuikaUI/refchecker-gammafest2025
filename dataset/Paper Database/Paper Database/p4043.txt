Unsupervised Cross-Dataset Transfer Learning for Person Re-identiﬁcation
Peixi Peng1,5, Tao Xiang2, Yaowei Wang3∗, Massimiliano Pontil4,
Shaogang Gong2, Tiejun Huang1,Yonghong Tian1,5∗
1National Engineering Laboratory for Video Technology, Peking University, Beijing, China
2School of Electronic Engineering and Computer Science, Queen Mary University of London, UK
3Department of Electronic Engineering, Beijing Institute of Technology, China
4Italian Institute of Technology, Italy
5Cooperative Medianet Innovation Center, China
{pxpeng, yhtian, tjhuang}@pku.edu.cn, ,
{t.xiang, s.gong}@qmul.ac.uk, 
Most existing person re-identiﬁcation (Re-ID) approaches follow a supervised learning framework, in which
a large number of labelled matching pairs are required
for training. This severely limits their scalability in realworld applications.
To overcome this limitation, we develop a novel cross-dataset transfer learning approach to
learn a discriminative representation. It is unsupervised in
the sense that the target dataset is completely unlabelled.
Speciﬁcally, we present an multi-task dictionary learning
method which is able to learn a dataset-shared but targetdata-biased representation.
Experimental results on ﬁve
benchmark datasets demonstrate that the method signiﬁcantly outperforms the state-of-the-art.
1. Introduction
Person re-identiﬁcation (Re-ID) is the problem of matching people across non-overlapping camera views. It has become one of the most studied problems in video surveillance due to its great potentials for security and safety management applications.
Despite the best efforts from the
computer vision researchers , it remains an unsolved
This is because a person’s appearance often
changes dramatically across camera views due to changes
in body pose, view angle, occlusion and illumination conditions.
To address these challenges, most existing research efforts on Re-ID 
are based on supervised learning. Speciﬁcally, they require
a large number of labelled matching pairs across each two
∗Corresponding
(email: and ).
camera views to learn a representation or matching function
that is invariant to the appearance changes. However, relying on manually labelled data for each camera-pair leads
to poor scalability. This is due to two reasons: (1) For
each pair of cameras, eye-balling the two views to annotate
correctly matching pairs among hundreds of imposters is a
tough job even for humans. (2) Given a camera network of
even a moderate size, e.g. one installed in an underground
station, there can be easily over one hundred cameras and
thousands of camera pairs. Since hundreds of labelled image pairs are typically needed from each camera pair for supervised learning, the labelling cost would be prohibitively
high. This scalability issue thus severely limits the applicability of the existing methods.
In order to make a person Re-ID model scalable, one solution is to utilise the unlabelled data, which are abundant in
the context of Re-ID – in a busy public space, thousands of
people pass by in each camera view everyday. There are a
few existing efforts on exploiting unlabelled data for unsupervised Re-ID modelling . However, compared
to supervised learning approaches, the matching performance of unsupervised models are typically much weaker,
rendering them less effective. The reason is that without
labelled matching pairs across camera views, existing unsupervised models are unable to learn what makes a person
recognisable under severe appearance changes.
Different from existing unsupervised Re-ID methods, we
propose to solve the Re-ID problem without any labelled
matching pairs of target data using cross-dataset transfer
learning. The idea is that labelling matching pairs for a set
of given target camera views is tedious for practical applications. However, there already exist labelled datasets collected elsewhere from other camera networks; it is therefore possible to learn a representation that captures the
view-invariant aspects of a person’s appearance and transfer it to the target dataset for matching.
Since the target views/dataset contains no label, this is an unsupervised
learning problem . It is thus an extremely challenging problem because not only the source and target domains
are different (different camera views), more critically they
also have different recognition tasks (different sets of person identities to be matched in each domain), in contrast to
most existing transfer learning assumptions.
To solve the above unsupervised cross-dataset transfer
learning problem, we propose a novel asymmetric multitask learning approach which is able to transfer a viewinvariant representation from a number of existing labelled
source datasets, each consisting of camera pairs with different viewing conditions, to an unlabelled target dataset containing people who never appeared in the source datasets.
Our method is based on dictionary learning, that is, we assume that a person’s appearance can be represented as a
linear combination of latent factors each corresponding to
a dictionary atom. Furthermore, we assume that some of
the atoms are view/dataset-independent, thus shared across
different datasets/tasks, whilst others are unique to each
dataset and may or may not be useful for Re-ID in a new
unlabelled target dataset. This results in three types of dictionaries being jointly learned using all datasets.
The key strength of our method, which also distinguishes
it from existing multi-task learning methods , is that it
is able to learn from unlabelled target data. This is precisely
why a dictionary learning model is adopted – it is originally
designed for unsupervised learning and can thus be naturally reformulated for unsupervised transfer learning. To
this end, graph Laplacian regularisation terms with iterative
updating are introduced in our formulation in order to learn
from both the labelled information from the source data and
the unlabelled data from the target data. In addition, to make
the learned dictionary biased towards the target dataset, different decompositions of dictionaries are introduced for the
source and target datasets respectively to reﬂect the fact that
our multi-task learning model is asymmetric, i.e. the multitask joint learning only aims to beneﬁt the target task.
2. Related Work
Most existing person Re-ID models are supervised,
based on either distance metric learning , discriminative subspace learning , learning to rank , or deep learning . These models are
thus unscalable as they need a large number of labelled data
(cross-view matched image pairs) to train for each given
camera pair. In particular, each learned model is camerapair-speciﬁc thus cannot be directly applied to another new
camera pair due to the view condition changes, as veriﬁed
by our experiments (Sec. 4).
To address the scalability issue, there have been a number of unsupervised Re-ID methods proposed in the literature, including two types: those designing hand-crafted appearance features and those modelling localised
saliency statistics .
However, compared to supervised learning approaches, both approaches yield much
weaker performance, since without pairwise identity labels
they cannot exploit cross-view identity-discriminative information that is critical for matching.
To strike a balance between scalability and matching accuracy, a semisupervised approach is proposed. Nevertheless, it still
requires a fairly substantial amount of pairwise labelling
which is not possible for large scale deployment in realworld applications.
cross-dataset transfer learning has been
adopted for Re-ID in the hope that labelled data from other
camera views/datasets can provide transferable identitydiscriminative information for a given target dataset. Note
that this cross-dataset transfer learning problem is very different from the same-dataset cross-identity or same-dataset
cross-view problems tackled in some early transfer Re-ID
works .
When both the dataset/domain and the
identities are different, the transfer learning problem considered in this work is much harder. Among the existing
cross-dataset transfer learning works, adopted an SVM
multi-kernel learning transfer strategy, and both and
 employed multi-task metric learning models. All of
theses works are supervised and they need labelled data in
the target dataset.
As far as we know, the only existing unsupervised crossdataset transfer learning model for Re-ID is the work in
The model proposed in utilises cross-domain
ranking SVMs. Unlike the dictionary learning model employed in this work, an SVM-based model does not naturally learns from completely unlabelled data. As a result,
their target dataset is not exactly unlabelled: it is assumed
that negative image pairs are given for the target dataset.
Therefore, strictly speaking, the model in is a weaklysupervised rather than unsupervised model. In contrast, our
model is completely unsupervised without requiring any labelled data from the target dataset. Our experiments show
that our method signiﬁcantly outperforms that of , even
with less supervision.
Beyond person Re-ID, dictionary learning for sparse
coding has been extensively studied . Graph Laplacian regularisation has also been explored in a sparse coding
formulation before, for problems such as unsupervised clustering , or supervised face veriﬁcation/recognition
 . Our model differs in that (1) dictionary learning is
performed under an asymmetric multi-task learning framework, hence the unique design of different decompositions
of dictionaries for the source and target tasks; and (2)
the Laplacian regularisation terms are updated iteratively
to adapt transferable knowledge learned from the labelled
source data to the unlabelled target data.
Note that a number of works have exploited domain adaptation for cross-view classiﬁcation or
veriﬁcation of faces/actions, based on dictionary learning
and/or sparse representation models.
They are thus related to our work. But there are signiﬁcant differences. In
particular, some of them are supervised and require labelled training data from the target domains. The
work in is unsupervised and based on unsupervised
domain adaptation . Nevertheless they tackle a
within-dataset cross-camera view domain adaptation problem. This is fundamentally different to our cross-dataset
transfer learning problem:
the domain change is much
greater across datasets, and importantly the images from
cross-domain/view but same dataset contain people of the
same identities, whilst a completely different set of people are captured in different datasets. In our experiments,
we demonstrate that these unsupervised domain adaptation
methods do not work on our cross-dataset transfer learning
task because the target dataset contains different classes.
Contributions
The main contributions of this work are:
(1) We formulate the Re-ID problem as an unsupervised
cross-dataset transfer learning problem and do not require
any labelled data from the target dataset, and (2) a novel
asymmetric multi-task dictionary learning model is proposed to learn view-invariant and identity-discriminative information from unlabelled target data. Extensive experiments are carried out on ﬁve widely used Re-ID benchmark
datasets. The results show that the proposed model significantly outperforms the state-of-the-art Re-ID approaches,
as well as existing transfer learning models, under both unsupervised and semi-supervised settings.
3. Methodology
3.1. Problem Deﬁnition
Assume a number of source datasets are collected from
different camera networks each consisting of two camera views1. The images in the source datasets (domains)
are paired across camera views by the person’s identity, i.e. they are pairwise labelled. An unlabelled target
dataset is captured from an entirely different domain (camera view/location) and contains a completely different set
of identities/classes. Therefore, the unsupervised transfer
learning for Re-ID problem is deﬁned as the problem of
learning the optimal representation/matching function for
the target dataset/domain using knowledge transferred from
the labelled source datasets.
3.2. Formulation
Taking a multi-task learning approach, we consider
learning a Re-ID model for each dataset as a task.
1 This is for simpliﬁcation of notations; datasets of more than two views
can be easily modelled by our model.
Figure 1: Some samples of latent attributes discovered by
the proposed Unsupervised Multi-task Dictionary Learning
(UMDL) model. It is clear that the latent attributes are visually and semantically meaningful. (a) Upper body red. (b)
Lower body black. (c) Short trousers. (d) Jeans.
wish to learn all tasks jointly so that they can beneﬁt each
other. Importantly, since we are only concerned with the
target task, the multi-task model is asymmetric and biased
towards the target task. Formally, assume Xt ∈RM×Nt is
a feature matrix with each column xt,i corresponding to an
M-dimensional feature vector representing the i-th person’s
appearance in the dataset t (t = 1, ..., T) consisting of Nt
Assume task T is the target task and the others are
source tasks.
Adopting a dictionary learning model, for
each task/dataset, the goal is to learn a shared dictionary
D ∈RM×k using all datasets {X1, ..., XT }.
dictionary, each M-dimensional feature vector, regardless
which view it comes from, is projected into a lower kdimensional subspace spanned by the k dictionary atoms
(columns of D) so that the corresponding coefﬁcients (code
vectors) can be matched by the cosine distance in this subspace. The idea is that each atom or the dimension of the
subspace corresponds to a latent appearance attribute which
is invariant to the camera view changes, thus useful for
cross-view matching. Figure 1 shows some examples of
latent attributes learned by the proposed model.
In a multi-task dictionary learning model, it is necessary
to decompose the dictionary into two parts: the one shared
between the tasks, which captures latent attributes that are
invariant against any view changes, and a task-speciﬁc one
that captures dataset-unique aspects of human appearance
In addition, it is important to note that apart from
the latent attributes that can contribute to Re-ID, there are
also other aspects of appearance that are variant to view
changes. These appearance aspects must be modelled as
part of the dictionary as well. Furthermore, the decomposition should be different for the source and target datasets as
we only care about the target one. Based on these consider-
ations, three types of dictionaries are introduced in our Unsupervised Multi-task Dictionary Learning (UMDL) model:
(1) Task-shared dictionary Ds which is used to encode the
dataset/view invariant latent attributes, and is shared by all
tasks, (2) dictionary unique to the target task Du
view-invariant , and (3) task-speciﬁc residual dictionary Dr
(t = 1, ..., T) which is task-speciﬁc and used to encode the
residual parts of features which cannot be modelled by Ds
(source tasks) or Ds and Du
T (target task). It is clear that the
source and target tasks are treated differently: for the target task, an additional third dictionary Du
T is needed to account for view-invariant but dataset-variant latent attributes
unique to the target views.
Now we can formulate our UMDL method as:
1, · · · , Dr
T ] = arg min
t {∥Xt −DsAs
F + ∥Xt −DsAs
F + ∥XT −DsAs
2 ≤1, ∀i, t
where matrices As
T are codes corresponding to
dictionaries Ds, Dr
T respectively; ds
t,i and du
are the ith column of Ds, Dr
T respectively; as
the ith column of As
T,i is the ith column of Au
and λ are weights of various cost function terms; and Wt is
the afﬁnity matrix for the task t indicating the relationships
among different training samples. Speciﬁcally, for the labelled source datasets, wt,i,j = 1 if xt,i and xt,j are of the
same person across views and wt,i,j = 0 otherwise. For the
target task, WT is initialised as a zero matrix because the
target task are unlabelled.
There are seven terms in this cost function and they
fall into two categories: the ﬁrst ﬁve are reconstruction
error terms that make sure that the learned dictionaries
can encode the feature matrices well, and the last two are
graph Laplacian regularisation terms that enforce that similar codes are obtained for instances of the same person
across camera views.
Note that these two regularisation
terms are put on the codes obtained using Ds and Du
As for those obtained using Dr
t , they are not subject to the
graph Laplacian regularisation because they are either untransferrable to the target task or are view-variant thus useless for Re-ID. Note that since WT is a zero matrix, it seems
to make no sense to have the seventh term for the target task
T. However, we shall see later that WT will be updated iteratively once a better representation for Re-ID is learned.
The last two terms can be rewritten using the Laplacian matrix as
where Lt = Dt −Wt and Dt is a diagonal matrix whose
diagonal elements are the sums of the row elements of Wt.
Now we explain how the ﬁrst ﬁve reconstruction error
terms are designed.
First, we note that the reconstruction error terms are formulated stepwise by the priority of
different dictionaries. Let us consider the ﬁrst two terms
F + ∥Xt −DsAs
F for the source
task t. The minimisation of the ﬁrst reconstruction error
term enables Ds to encode Xt as much as possible while
the minimisation of the second reconstruction error term enables Dr
t to encode and align the residual part of Xt that
cannot be coded using Ds. This stepwise reconstruction
formulation is also applied to the target task T resulting in
terms 3–5. However, as an asymmetric multi-task learning model, the target task is biased with three dictionaries
rather than two, hence the three terms. We shall see in our
experiments that both the stepwise reconstruction terms and
asymmetric design contribute positively to the ﬁnal performance of the model.
Note that unlike conventional dictionary learning for
sparse coding models, in our model, there is no L1 sparsity
penalty term. This is because (1) empirically, we ﬁnd that
less-sparse codes contain richer information for Re-ID, and
(2) removing these L1 terms leads to a simpler optimisation
3.3. Optimisation
Next we describe how the optimisation problem in (1)
can be solved. This optimisation problem is divided into
the following subproblems:
(1) Computing As
Given ﬁxed Ds, Dr
T , the coding problem of the task t (t = 1, ..., T) becomes:
Xt −˜D ˜At
F + λTr( ˜AtLt ˜A′
where, for the target task:
and for the source tasks:
ηt (Xt −Dr
Let the derivative of (2) equals to 0 and the analytical solution of ˜at,i (the ith column of ˜At) can be obtained as:
˜D′ ˜D + 2λlt,i,iI
˜D′˜xt,i −2λ
˜xt,klt,k,i
where lt,k,i is the (k, i) element of Lt. I is the identity
matrix and ˜xt,i is the ith column of ˜Xt.
(2) Computing Ar
Fix other terms and Ar
t is solved as:
For the target task:
min ∥XT −DsAs
and for the source tasks:
min ∥Xt −DsAs
Let the derivative of (3) equals to 0 and the analytical solution of Ar
t can be obtained as:
For the target task:
′ (XT −DsAs
and for the source tasks:
′ (Xt −DsAs
(3) Updating dictionaries
1, ..., T), Du
T are given, Ds is optimised as:
min ∥X −DsA∥2
F , s.t. ∥ds
2 ≤1, (∀i),
X = [η1X1, ..., ηT −1XT −1, η1(X1 −Dr
1), ..., ηT −1(XT −1−
T −1), XT , XT −Du
T , XT −Du
1, ..., ηT −1As
T −1, η1As
1, ..., ηT −1As
(4) can be optimised by the Lagrange dual and the analytical solution of Ds can be computed as:
(XA′) (AA′ + Λ)−1, where Λ is a diagonal matrix constructed from all the dual variables.
Then, for the target task, ﬁx the dictionaries Ds, Dr
T , then Du
T can be updated by:
2 ≤1, (∀i),
T = [XT , XT −DsAs
T , XT −DsAs
At last, ﬁx Ds, Du
t, the objective function to solve Dr
t is (t = 1, ..., T) :
For the target task:
T = XT −DsAs
and for the source tasks:
t = Xt −DsAs
Algorithm 1: Unsupervised Multi-task learning
Input: Xt; initialise Ds, Dr
T randomly; Ar
Output: Ds, Du
t; (t = 1, ..., T).
while Non-convergence do
for t = 1 →T do
if Source tasks then
t , and Ar
t, then calculate As
t , and As
t, then calculate Ar
if Target task then
T , then calculate As
T , then calculate
Fix other terms, update Ds by (4).
for t = 1 →T do
if Source tasks then
t with ﬁxed Ds, As
if Target task then
T with ﬁxed Ds,Dr
T with ﬁxed Ds,Du
(6) and (8) can be solved similarly as (4):
Alg. 1 summarises our optimisation algorithm. It converges after a few (< 30) iterations in our experiments.
Iterative Updating WT
After running Alg. 1, each training sample xT,i from the target task will be coded by (10)
(detailed below) and the code is aT,i =
With this code, we can measure the similarity between
each pair of target data samples across views and recompute WT . This matrix now captures the soft relationships
among the training samples from the target tasks which we
aim to preserve in the lower dimensional space spanned
by the dictionary columns. Speciﬁcally, if aT,j is among
the k-nearest-neighbours of aT,i and aT,i is among the knearest-neighbours of aT,j, wT,i,j =
∥aT,i∥∥aT,j∥, otherwise, wT,i,j = 0. With the updated WT , we re-run Alg. 1
to enter the next iteration. The iterations terminate when
a stopping criterion is met, and the number of iterations is
typically < 5 in our experiments.
3.4. Application to Re-ID
Re-ID for the Target Task
After training the model,
each test sample xT,i from the target task T can be encoded
via Ds, Du
by solving the fol-
lowing problem:
xT,i −Dsas
which can be solved easily by a linear system. With this
new representation, Re-ID is done simply by computing the
cosine distance between the code vectors of a probe and a
gallery sample.
Extension to Semi-Supervised Re-ID
If the target task
are partially labelled, our model can be readily extended
with minimal modiﬁcation. Speciﬁcally, for the labelled
data, wT,i,j will be set to 1 if xT,i and xT,j are from same
individual, otherwise wT,i,j = 0. For the unlabelled data,
the corresponding part of WT is initialised and iteratively
updated as in the unsupervised setting.
4. Experiments
4.1. Datasets and Settings
Five widely used benchmark datasets are chosen in our experiments.
The VIPeR dataset contains 1,264 images of 632 individuals from two distinct
camera views (two images per individual) featured with
large viewpoint changes and varying illumination conditions (Fig. 2(a)). The PRID dataset consists of images extracted from multiple person trajectories recorded
from two surveillance static cameras (Fig. 2(b)).
Camera view A contains 385 individuals, camera view B contains 749 individuals, with 200 of them appearing in both
views. The CUHK01 dataset contains 971 individuals captured from two camera views in a campus environment (Fig. 2(c)). Each person has two images in each
camera view. We follow the single-shot setting, that is, we
randomly select one image for each individual in each camera view for both training and testing in our experiments.
The iLIDS dataset has 476 images of 119 individuals captured in an airport terminal from three cameras with
different viewpoints (Fig. 2(d)). It contains large occlusions
caused by people and luggage. The CAVIAR dataset 
includes 1,220 images of 72 individuals from two cameras
in a shopping mall (Fig. 2(e)). Each person has 10 to 20
images. The image sizes of this dataset vary signiﬁcantly
(from 141 × 72 to 39 × 17). By examining Fig. 2, it is
clear that the obvious variations of visual scenes and crossview conditions between the ﬁve benchmark datasets make
the transfer learning task extremely challenging.
A single-shot experiment setting is adopted similar to . In each experiment, one dataset is chosen as
the target dataset and the other four are used as the source
datasets. All the individuals in the source data are labelled
Figure 2: Image samples of the ﬁve datasets. Images in the
same column are from the same person across two views.
Better viewed in colour.
and used for model training, while the individuals in the target dataset are randomly divided into two equal-sized subsets as the training and test sets, with no overlapping on
person identities. This process is repeated 10 times, and the
averaged performance is reported as the ﬁnal results. For
datasets with only two camera views (VIPeR, PRID and
CHUK01), we randomly select one view as probe and the
other as gallery. While for the multi-view dataset (iLIDS),
one image of each individual in the test set is randomly selected as the gallery image and the rest of the images are
used as probe images. Results are obtained by averaging
with 10 trials. For the CAVIAR dataset, the same setting
as iLIDS is used in the unsupervised setting.
for fair comparison with published results under the semisupervised setting, we follow and randomly choose 14
of the 50 individuals appearing in two cameras as the labelled training data, and the remaining 36 individuals as
testing data. The 22 people appearing in one camera are
used as the unlabelled training data.
Also, ﬁnal results
are obtained by averaging with 10 trials. All images are
normalized to 128 × 48 pixels and the colour+HOG+LBP
histogram-based 5138-D feature representation in is
used. As for the number of dictionary atoms, the size of
the task-shared dictionary Ds is the same as the residual
dictionary Dr
t (t = 1, 2..., T) , which is half of the size of
the unique dictionary Du
T . The size of Ds is set to 150 for
all experiments. We found that the model’s performance is
insensitive to the different dictionary sizes. Other parameters (ηt and λ in Eq. (1)) in our model are set automatically
using four-fold cross-validation with one of the four source
datasets as the validation set and the other three as training
4.2. Unsupervised Re-ID Results
Under this setting, the target dataset is unlabelled. The
compared methods can be categorised into two groups:
(1) Single-task methods.
Without transfer learning,
the training data of these unsupervised methods are
downloaded
 
SA DA+kLFDA
Table 1: Results on unsupervised Re-ID. ‘-’ means no implementation code or reported result is available.
Fea AdaRSVM
Table 2: More detailed comparisons with AdaRSVM.
only the unlabelled data from the target dataset.
state-of-the-art unsupervised Re-ID methods are selected
for comparison, including the hand-crafted-feature-based
method SDALF , the saliency-learning-based eSDC
 , the graphical-model-based GTS and the sparserepresentation-classiﬁcation-based ISR .
report results of the single-task version of proposed model
by removing all source data related terms in Eq. (1),
denoted as Ours S.
(2) Multi-task methods. There are few multi-task learning
methods, or unsupervised transfer learning methods in
general, available for the unsupervised setting. AdaRSVM
 is the only unsupervised cross-data transfer learning
work that we are aware of, and it is also designed for person
Re-ID. As discussed in Sec. 2, the main difference is that
they assume the availability of negative pairs in the target
dataset, thus using more supervision than our method.
We also use the subspace alignment based unsupervised
domain adaptation method SA DA to align the data
distributions of the source and target datasets ﬁrst. Then
a supervised Re-ID model, kLFDA , is trained on the
labelled source datasets and applied to the aligned target
dataset. This method is denoted as SA DA+kFLDA. Note
that as an unsupervised domain adaptation method, SA DA
assumes that the source and target domains have the same
classes, which is invalid for cross-dataset transfer learning.
In addition, we compare with a naive transfer approach, that
is, learning kFLDA on source datasets ﬁrst, and applying it
directly to the target dataset without any model adaptation.
This is denoted as kLFDA N.
Table 1 reports the results measured with the Rank 1
matching accuracy (%)3. From these results, it is evident
that: (1) Compared with existing unsupervised methods including SDALF, eSDC, GTS and ISR, our model is signiﬁcantly better. This shows that transfer learning indeed
helps for unsupervised Re-ID. (2) The difference in performance between “Ours S” and “Ours” models shows exactly how much the target dataset has beneﬁted from the
source datasets using our unsupervised asymmetric multitask transfer learning. (3) The results of kLFDA N is very
poor, showing that the knowledge learned from the labelled
source datasets cannot be directly used to help match target data. This is due to the drastically different viewing
conditions and condition changes across views in the target dataset compared to those in the source (see Fig. 2). A
naive transfer learning approach such as kLFDA N would
not be able to cope with the domain shift/difference of this
magnitude. (4) Importantly it is noted that when an existing unsupervised domain-adaptation based transfer learning model is applied to alleviate the domain shift problem
(SA DA+kLFDA), the result is even worse. This is not surprising as existing unsupervised domain adaptation methods are designed under the assumption that the source and
target domains have the same recognition tasks (i.e. having the same set of classes) – an invalid assumption for our
unsupervised Re-ID problem as different datasets contain
different person identities. (5) The results of the only existing cross-dataset unsupervised Re-ID method AdaRSVM is
actually the worst. Note that since the code is not available,
these are the reported results in . Since different feature representation and two instead of four source datasets
were used, this comparison is only indicative. However, by
examining some additional results in Table 2, we can still
conclude that AdaRSVM is able to transfer very little useful information from the source datasets even when they use
more supervision on the target dataset than our model. More
speciﬁcally, in Table 2, Fea AdaRSVM (Fea Ours) means
the matching accuracy by L1-Norm distance of the features
used in AdaRSVM (Ours). The results in Table 2 show that
our transfer model can improve 8%-15% matching accuracy
over non-learning based L1-Norm distance. In contrast, the
increase for AdaRSVM is 1%-2%. (6) It is noted that on
three of ﬁve datasets (PRID,CAVIAR and iLIDS), our unsupervised results is close or surpasses the best reported results using existing supervised methods . This
shows the clear advantage of our unsupervised transfer
learning model over existing models (supervised and unsupervised) on both scalability and accuracy.
4.3. Semi-supervised Re-ID Results
In this experiment, one third of the training set from the
target dataset is labelled as in . Again, we compare
3The CMC curves of the proposed method can be found at
 
with two groups of methods:
(1) Single-task methods.
SSCDL is the most relevant semi-supervised Re-ID method because it is also
based on dictionary learning. In addition, with the target
data partially labelled, we can now compare with the
existing fully-supervised models by training them using
the labelled target data only. These include kLFDA 
and KCCA .
The same features are used for fair
comparison.
(2) Multi-task methods. cAMT is the latest multi-task
learning method for person Re-ID to our knowledge.
Based on a constrained asymmetric multi-task discriminant
component analysis model, it also attempts to transfer
knowledge from source tasks to target task. However the
key difference is that it needs labelled data in both source
datasets and the target dataset; it thus can only be compared
under this semi-supervised setting. We also compare with a
naive transfer learning method denoted as kLFDA N, that
is, we learn kFLDA using a mix of the labelled source data
and the labelled target data. Again, the same features are
used for fair comparison.
Table 3: Semi-supervised Re-ID results
The results are shown in Table 3, from which we note
that: (1) Compared to our results in Table 1, all results improve, albeit by a moderate margin. This means on one
hand, our model does beneﬁt from additional labelled data
from the target task; on the other hand, they are the ice
on the cake as the transferred knowledge from the source
task is already very discriminative for the target task. (2)
Compared to SSCDL, our result is much better on VIPeR
and slightly worse on CAVIAR. Overall, our model is better because as a transfer learning model it can take advantage more labelled data from the source datasets. (3)
The results of supervised models (kLFDA and KCCA) are
much weaker than ours indicating that they require much
more labelled data than the one-third to function properly4.
(4) The naive transfer model kLFDA N failed miserably.
Again this is due to the untreated domain shift problem. (5)
The existing multi-task transfer Re-ID method cAMT fares
even worse. This shows that a dictionary learning based
4We note that when trained using fully labelled target set, their results
are close to ours under the same setting showing the advantage of being a
transfer model diminishes when labels are abundant.
multi-task model is more appropriate. This is because being originally designed for unsupervised learning, dictionary learning can exploit the unlabelled target data more
naturally than the discriminant component analysis model
in which is originally designed for supervised learning.
4.4. Further Evaluations
Contributions of Model Components
The two key
model design components are evaluated: (1) the asymmetric
treatment of the target task by including an additional dictionary Du
T ; and (2) the stepwise reconstruction error formulation. For the former, we remove Du
T in Eq. (1), and for
the latter, we remove Terms 1, 3 and 4 in Eq. (1). Table 4
shows clearly that both components contribute positively to
the ﬁnal performance of the model.
Without Du
Without stepwise
Our Full Model
Evaluation under unsupervised setting on the
model components
Running Cost
On a desktop PC with two 3.20 GHz
CPUs and 4G RAM running in Matlab, our model takes
12 minutes to train and 0.78 seconds to match 312 images against 312 images when VIPeR is used as the target
dataset. It is thus extremely efﬁcient during testing as a linear model.
5. Conclusion
We have developed a novel unsupervised cross-dataset
transfer learning approach based on asymmetric multi-task
dictionary learning.
It differs signiﬁcantly from existing
methods in that it can exploit labelled datasets collected
elsewhere whilst requiring no labelling on a target dataset.
Extensive experiments show that our model is superior to
existing Re-ID methods with or without transfer learning
and has great potentials for real-world applications due to
its high scalability, low running cost, and high matching accuracy.
Acknowledgements
This work was partially supported by the National
Basic Research Program of China under grant No.
2015CB351806, the National Natural Science Foundation
of China under contract No. 61425025, No. 61390515, No.
61471042, and No. 61421062, the National Key Technology Research and Development Program under contract No.
2014BAK10B02, and the Shenzhen Peacock Plan.