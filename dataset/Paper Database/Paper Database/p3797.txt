MITSUBISHI ELECTRIC RESEARCH LABORATORIES
 
Scalable Active Learning for Multi-Class
Image ClassiÔ¨Åcation
Joshi, A.J.; Porikli, F.; Papanikolopoulos, N.
TR2012-026
January 2012
Machine learning techniques for computer vision applications like object recognition, scene classiÔ¨Åcation, etc. require a large number of training samples for satisfactory performance. Especially when classiÔ¨Åcation is to be performed over many categories, providing enough training
samples for each category is infeasible. This paper describes new ideas in multi-class active
learning to deal with the training bottleneck, making it easier to train large multi-class image
classiÔ¨Åcation systems. First we propose a new interaction modality for training which requires
only yes-no type binary feedback instead of a precise category label. The modality is especially
powerful in the presence of hundreds of categories. For the proposed modality, we develop a
Value-of-Information (VOI) algorithm that chooses informative queries while also considering
user annotation cost. Second, we propose an active selection measure that works with many
categories and is extremely fast to compute. This measure is employed to perform a fast seed
search before computing VOI, resulting in an algorithm that scales linearly with data-set size.
Third, we use locality sensitive hashing to provide a very fast approximation to active learning,
which gives sub-linear time scaling allowing application to very large data-sets. The approximation provides up to two orders of magnitude speedups with little loss in accuracy. Thorough
empirical evaluation of classiÔ¨Åcation accuracy, noise sensitivity, imbalanced data, and computational performance on a diverse set of image data-sets demonstrates the strengths of the proposed
algorithms.
IEEE Transactions on Pattern Analysis and Machine Intelligence
This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part
without payment of fee is granted for nonproÔ¨Åt educational and research purposes provided that all such whole or partial copies include
the following: a notice that such copying is by permission of Mitsubishi Electric Research Laboratories, Inc.; an acknowledgment of
the authors and individual contributions to the work; and all applicable portions of the copyright notice. Copying, reproduction, or
republishing for any other purpose shall require a license with payment of fee to Mitsubishi Electric Research Laboratories, Inc. All
rights reserved.
Copyright c‚ÉùMitsubishi Electric Research Laboratories, Inc., 2012
201 Broadway, Cambridge, Massachusetts 02139
MERLCoverPageSide2
Scalable Active Learning for Multi-Class Image
ClassiÔ¨Åcation
Ajay J. Joshi, Student Member, IEEE, Fatih Porikli, Senior Member, IEEE
and Nikolaos Papanikolopoulos, Fellow, IEEE
Abstract‚ÄîMachine learning techniques for computer vision
applications like object recognition, scene classiÔ¨Åcation, etc.
require a large number of training samples for satisfactory
performance. Especially when classiÔ¨Åcation is to be performed
over many categories, providing enough training samples for
each category is infeasible. This paper describes new ideas in
multi-class active learning to deal with the training bottleneck,
making it easier to train large multi-class image classiÔ¨Åcation
systems. First we propose a new interaction modality for training
which requires only yes-no type binary feedback instead of a
precise category label. The modality is especially powerful in the
presence of hundreds of categories. For the proposed modality,
we develop a Value-of-Information (VOI) algorithm that chooses
informative queries while also considering user annotation cost.
Second, we propose an active selection measure that works with
many categories and is extremely fast to compute. This measure
is employed to perform a fast seed search before computing
VOI, resulting in an algorithm that scales linearly with dataset
size. Third, we use locality sensitive hashing to provide a very
fast approximation to active learning, which gives sublinear
time scaling allowing application to very large datasets. The
approximation provides up to two orders of magnitude speedups
with little loss in accuracy. Thorough empirical evaluation of
classiÔ¨Åcation accuracy, noise sensitivity, imbalanced data, and
computational performance on a diverse set of image datasets
demonstrates the strengths of the proposed algorithms.
Index Terms‚ÄîActive Learning, Scalable Machine Learning,
Multi-Class ClassiÔ¨Åcation, Object Recognition
I. INTRODUCTION
Real world classiÔ¨Åcation applications such as object recognition and classiÔ¨Åcation typically require large amounts of
annotated training data due to the tremendous amount of
variation in image appearance. Considering the variety and
scale of images on the web, training satisfactory classiÔ¨Åers
is increasingly difÔ¨Åcult using traditional supervised learning
techniques. At the same time, in most image classiÔ¨Åcation
problems, we typically have a large number of unlabeled
data. Intelligently exploiting the large amounts of data is a
challenging problem. To this end, there has been recent interest
in active learning, wherein, classiÔ¨Åers are trained interactively
‚Äì the system queries the user for annotations on ‚Äúinformative
samples‚Äù, instead of accepting annotation passively. Previous
work on binary classiÔ¨Åcation , and more recently even
multi-class classiÔ¨Åcation , , , has shown that
such an active learning approach can reduce the amount of
training data required compared to supervised passive learning.
A. J. Joshi and N. Papanikolopoulos are with the Department of Computer
Science and Engineering, University of Minnesota, Twin Cities.
E-mail: {ajay,npapas}@cs.umn.edu
F. Porikli is with Mitsubishi Electric Research Labs, Cambridge, MA.
E-mail: 
 

Top row: sample interaction in traditional multi-class active learning
approaches. The user needs to input a category name/number for the query
image from a large dataset possibly consisting of hundreds of categories.
Bottom row: the binary interaction model we propose: the user only needs to
say whether or not the query image and the sample image belong to the same
Even though multi-class active learning methods successfully reduce the amount of training data required, they can
be labor intensive from a user interaction standpoint for the
following reasons: (i) for each unlabeled image queried for
annotation, the user has to sift through many categories to
input the precise one. Especially for images, providing input
in this form can be difÔ¨Åcult, and sometimes impossible when
a huge (or unknown) number of categories are present; (ii) the
time and effort required increase with an increase in the
number of categories; (iii) the interaction is prone to mistakes
in annotation, and (iv) it is not easily amenable to distributed
annotation as all users need to be consistent in labeling.
The distributed annotation aspect is of increasing importance
in exploiting distributed labeling resources such as Amazon
Mechanical Turk , .
Apart from the above, current active learning algorithms are
computationally intensive which limits their applicability to
datasets of hundreds or a few thousand samples. At the same
time, image datasets are ever increasing in their size and the
image variety - it is not uncommon to have tens of thousands
of image classes , . In order to design systems that are
practical at larger scales, it is essential to allow easier modes of
annotation and interaction for the user, along with algorithms
that are scalable. Motivated by this, our contributions in this
paper are the following:
‚Ä¢ We develop a multi-class active learning setup that requires only binary user feedback (yes/no). Our system
generalizes interaction since it can also accept precise
category annotation as in the traditional setting, if available for any images. We propose a Value-of-Information
(VOI) based active selection framework for the binary
interaction modality.
‚Ä¢ We propose an efÔ¨Åcient measure to compute uncertainty
(uncertainty sampling) of examples for query selection.
Unlike previous work, the selection measure works directly in the multi-class setting, instead of actively selecting samples from various binary classiÔ¨Åers separately.
‚Ä¢ We propose extremely efÔ¨Åcient approximations to active
learning that scale sublinearly with dataset size. Scaling
is of utmost importance towards using active learning for
current applications which are at a substantially larger
scale than what most algorithms are designed for.
‚Ä¢ Unlike most previous methods, the proposed system is
designed to handle and incorporate unseen categories as
it learns (i.e., we do not assume a training set containing
samples from all classes to begin with). This aspect is
particularly important in real systems, where it is unlikely
to have training examples from all categories at the outset.
‚Ä¢ The proposed system is empirically shown to handle
many frequent issues that plague real data: class population imbalance owing to largely varying number of
examples across categories, label noise occurring due to
human training errors or noisy acquisition processes.
Due to the ease of interaction of the proposed system, easy
scalability, allowing the incorporation of unseen categories,
and dealing with noise and imbalance, we believe this paper
demonstrably shows the effectiveness of active learning for
training very large-scale image classiÔ¨Åcation systems.
A. Ease of interaction
In order to quantitatively compare the two interaction modalities, we conducted experiments on 20 users with 50-class
and 100-class data, obtained from the Caltech-101 object
categories dataset . Each user was asked to interact with
two modalities: i) giving category labels (out of a given set
of labels) to randomly queried images, as is typically used for
training, and ii) giving yes/no responses to two images based
on whether they came from the same class. We measured
interaction time and the number of errors made in both
modalities by each user, along with an overall satisfaction
score from 1 through 5, indicating the ease of interaction
experienced (1 being the easiest). Table I summarizes the
Response time (s)
Satisfaction
BF ‚Äì 50 classes
1.6 (¬±0.2)
MCF ‚Äì 50 classes
11.7 (¬±3.1)
BF ‚Äì 100 classes
1.7 (¬±0.2)
MCF ‚Äì 100 classes
28.8 (¬±5.3)
COMPARING THE TWO INTERACTION MODALITIES.
First, it can be seen that binary feedback (BF) requires
far lesser user time than giving multi-class feedback (MCF).
Although BF in principle also provides lesser information
than MCF, we demonstrate in our experiments that the BF
interaction model still achieves superior classiÔ¨Åcation accuracy
than MCF with the same expenditure of user time. Second, as
seen in the table, MCF has much more noise associated ‚Äì
users make many more errors when sifting through potential
categories and Ô¨Ånding the correct one. In contrast, BF is
much cleaner since it is much easier to simply look at two
images and determine whether they belong to the same class
or not. Third, the interaction time and annotation errors in
MCF increase with the number of categories. This is expected
as annotation requires browsing over all possible classes.
In contrast, in the BF model, there is no observed increase
in user time with increasing number of categories. This aspect
is particularly appealing, as the main objective is to scale well
to larger problems with potentially thousands of classes. Four,
as seen from the satisfaction scores, users are much more
satisÔ¨Åed with the overall interaction in BF, since it does not
need browsing through many images, and can be done quickly.
Apart from the above advantages, distributed annotation across
many trainers is easily possible in the BF model. Also, it is
straightforward to allow exploration of the data when new
categories continuously appear (as opposed to a setting often
used previously, wherein the initial training set is created by
including examples from all classes ), or when notions
of categories change with time. In summary, binary feedback
provides an extremely appealing interaction model for large
problems with many classes.
II. RELATED WORK
Many problems in computer vision suffer from the fact
that they require substantial amounts of training data for
performing accurate classiÔ¨Åcation. As such, active learning has
received increasing interest in the computer vision community.
In the following, we review relevant work on object recognition and active learning.
Tong and Chang propose active learning for SVM in
a relevance feedback framework for image retrieval. Their
approach relies on the margins for unlabeled examples for
binary classiÔ¨Åcation. Tong et al. use an active learning
method to minimize the version space1 at each iteration.
However, both these approaches target binary classiÔ¨Åcation.
Gaussian processes (GP) have been used for object categorization by Kapoor et al. . They demonstrate an active
learning approach through uncertainty estimation based on
GP regression, which requires O(N 3) computations, cubic
in the number of training examples. They use one-versus-all
SVM formulation for multi-class classiÔ¨Åcation, and select one
example per classiÔ¨Åer at each iteration of active learning.
Holub et al. propose an entropy-based active learning
method for object recognition. Their method selects examples
from the active pool, whose addition to the training set
minimizes the expected entropy of the system. On the other
hand, our VOI method computes the expected improvement
in classiÔ¨Åcation accuracy, while also attempting to minimize
the expected user annotation cost. The entropy-based approach
proposed in requires O(k3N 3) computations, where N
is the number of examples in the active pool and k is the
number of classes. Qi et al. demonstrate a multi-label
classiÔ¨Åcation method that employs active selection along two
1Version space is the subset consisting of all hypotheses that are consistent
with the training data .
dimensions ‚Äì examples and their labels. Label correlations are
exploited for selecting the examples and labels to query the
Kapoor et al. propose a VOI type method for semisupervised learning which is similar to the one proposed here.
Our approach however proposes a simpler binary interaction
model for multi-class problems, along with an associated
efÔ¨Åcient means to compute VOI on the binary model.
For handling multiple image selection at each iteration, Hoi
et al. introduced batch mode active learning with SVMs.
Since their method is targeted towards image retrieval, the
primary classiÔ¨Åcation task is binary ‚Äì to determine whether an
image belongs to the class of the query image. Active learning
with uncertainty sampling has been demonstrated by Li and
Sethi , in which they use conditional error as a metric of
uncertainty, and work with binary classiÔ¨Åcation.
For a comprehensive survey on various algorithms and applications of active learning, see . Although there has been
a lot of work on reducing the number of training examples for
classiÔ¨Åcation, the interaction and computational complexity of
active learning has been more or less overlooked, especially
for classiÔ¨Åcation tasks involving large number of categories.
As mentioned previously, addressing this is the primary contribution of the paper.

Block schematic of the active learning setting. Our focus in this
paper is on the query and sample selection algorithms ‚Äì depicted in white
boxes with red borders (see text for details).
Learning setup
Figure 2 shows a block schematic of the proposed active
learning setup. The active pool consists of a large number
of unlabeled images from which the active learning algorithm
can select images to query the user. The training set consists
of images for which category labels are known and can be
used for training the classiÔ¨Åer. Throughout the paper, we use
Support Vector Machines (SVM) as the underlying classiÔ¨Åcation algorithm, since it provides state-of-the-art performance
on the datasets used for evaluation. For the multi-class case,
one-vs-one SVM (classiÔ¨Åers trained for each pair of classes)
In the traditional multi-class active learning setting, an
unlabeled image (query image) needs to be selected for user
annotation. In our case, however, since user input is only
binary, we also require an image from a known category
to show the user for comparison. Selecting this image from
the training set is a new aspect of active selection that our
framework requires. We refer to this comparison image from
a known category as the ‚Äúsample image.‚Äù We focus on query
and sample selection algorithms in this paper ‚Äì denoted by
white boxes with red borders in Figure 2.
Our approach for query as well as sample selection is
probabilistic, i.e., based on the current training set, class
membership probability estimates are obtained for the images
in the active pool. We use Platt‚Äôs method , to estimate
binary probabilities based on the SVM margins, combined
with pairwise coupling with one-vs-one SVM for multiclass probability estimation on the unlabeled images. Probability estimation details are given in Section IV-A.
In Figure 2, the query selection algorithm selects a query
image from the active pool using the estimated class membership probabilities. Based on the estimated membership probabilities for the query image, the sample selection algorithm
selects a sample image from the current training set. The
query-sample pair is shown to the user for feedback. If a
‚Äúmatch‚Äù response is obtained, indicating that the query and
sample images belong to the same category, the query image is
added to the current training set along with its category label.
If a ‚Äúno-match‚Äù response is obtained, the sample selection
algorithm is again invoked to ask for a different sample image.
This process goes on until either the label for the query
image is obtained (with a ‚Äúmatch‚Äù response), or until the query
image does not match any of the categories in the training
set. In the latter case, a new category label is initiated and
assigned to the query image2. Through such a mechanism, the
learning process can be started with very few training images
initially chosen at random (seed set). As the process continues,
the active selection algorithm requires far fewer queries than
random selection to achieve similar classiÔ¨Åcation rate on a
separate test set. Note that the system is also able to exploit
feedback in terms of precise category annotation (as in the
typical setting), if available. Binary feedback however generalizes the applicability and allows learning in new unknown
environments for exploration.
Binary input has been employed previously in the context
of clustering data, by asking the user for pairwise mustlink and cannot-link constraints . This approach can be
adapted to the active learning framework by choosing even
the sample images from unlabeled data and performing a (unsupervised) clustering step before user annotation. However, in
our observation, such an approach was prone to noise due to
unsupervised clustering, which can lead to an entire cluster
of incorrectly labeled training data. Noise reduction in the
preclustering approach is an interesting future work direction.
On the other hand, in this paper (and the preliminary version
2Initiating a new category can require many user responses when many
classes are present ‚Äì we later discuss how to overcome this through a fast
new class initialization step along with cluster merging.
 ), we demonstrate empirically that the setup we employ is
robust to labeling noise.
III. THE ACTIVE LEARNING METHOD
There are two parts to binary feedback active learning: (i) to
select a query image from the active pool, and (ii) to select a
sample image from a known category to be shown to the user
along with the query image.
A. Query selection
The goal here is to query informative images, i.e., images
that are likely lead to an improvement in future classiÔ¨Åcation
accuracy. We use the Value of Information framework ,
 , employed in decision theory for query selection in
this paper. The broad idea is to select examples based on an
objective function that combines the misclassiÔ¨Åcation risk and
the cost of user annotation. Consider a risk matrix M ‚ààRk√ók
for a k-class problem. The entry Mij in the matrix indicates
the risk associated with misclassifying an image having true
label i as belonging to class j. Correct classiÔ¨Åcation incurs no
risk and hence the diagonal of M is zero, Mii = 0, ‚àÄi.
Denote the estimated class membership distribution for an
unlabeled image x as px = {p1
x, . . . , pk
x}. Note that since
the true class membership distribution for x is unknown, the
actual misclassiÔ¨Åcation risk cannot be computed ‚Äì we instead
Ô¨Ånd the expected misclassiÔ¨Åcation risk for x as
x|L) ¬∑ (pj
where L is the set of labeled examples based on which the
probabilities are estimated. Consider that the test set T consists
of N images x1, . . . , xN. The total expected risk over the test
set (normalized by size) is
x|L) ¬∑ (pj
Note that the above expression requires that the test set be
available while computing the total risk. Typically, the test
set is not available beforehand, and we can use the images
in the active pool A for computing the expected risk. Indeed,
most work on classiÔ¨Åcation uses surrogates to estimate the
misclassiÔ¨Åcation risk in the absence of the test set. In many
scenarios, the entire available set of unlabeled images is used
as the active pool and is typically very large, thus an estimate
of risk on the active pool is fairly reliable.
Now, if y ‚ààA is added to the labeled training set by
acquiring its label from the user, the expected reduction in
risk on the active pool can be computed as
x|L) ¬∑ (pj
x|L‚Ä≤) ¬∑ (pj
where L‚Ä≤ = L‚à™{y}, and A‚Ä≤ = A\{y}. The above expression
captures the value of querying y and adding it to the labeled
set. However, we also need consider the cost associated with
obtaining feedback from the user for y. Assume that the
cost of obtaining user annotation on y is given by C(y). In
our framework, we wish to actively choose the image that
reduces the cost incurred while maximizing the reduction in
misclassiÔ¨Åcation risk. Assuming risk reduction and annotation
cost are measured in the same units, the joint objective that
represents the value of information (VOI) for a query y is
V (y) = RL ‚àíRL‚Ä≤ ‚àíC(y).
The term RL in the above equation is independent of y, the
example to be selected for query. Therefore, active selection
for maximizing VOI can be expressed as a minimization
y‚àó= argmin
RL‚Ä≤ + C(y).
Note that the above framework can utilize any notions of
risk and annotation cost that are speciÔ¨Åc to the domain. For
instance, we can capture the fact that misclassifying examples
belonging to certain classes can be more expensive than
others. Such a notion could be extremely useful for classifying
medical images so as to determine whether they contain a
potentially dangerous tumor. Misclassifying a ‚Äòclean‚Äô image
as having a tumor only incurs the cost of the doctor verifying
the classiÔ¨Åcation. However, misclassifying a ‚Äòtumor image‚Äô as
clean could be potentially fatal in a large dataset wherein the
doctor cannot manually look at all the data. In such scenarios,
the different misclassiÔ¨Åcation risks could be suitably encoded
in the matrix M.
As in most work on active learning, our evaluation is
based on classiÔ¨Åcation accuracy. As such we employ equal
misclassiÔ¨Åcation cost, so that Mij = 1, for i Ã∏= j.
B. Sample selection
Given a query image, the sample selection algorithm should
select sample images so as to minimize the number of responses the user has to provide. In our framework, the sample
images belong to a known category; the problem of selecting
a sample image then reduces to the problem of Ô¨Ånding a likely
category for the query image from which a representative
image can be chosen as the sample image. When presented
with a query image and a sample image, note that a ‚Äúmatch‚Äù
response from the user actually gives us the category label
of the query image itself! A ‚Äúno match‚Äù response does not
provide much information. Suppose that the dataset consists
of 100 categories. A ‚Äúno match‚Äù response from the user to
a certain query-sample image pair still leaves 99 potential
categories to which the query image can belong. Based on
this understanding, the goal of selecting a sample image is to
maximize the likelihood of a ‚Äúmatch‚Äù response from the user.
Selecting a sample image (category) can be accomplished
by again using the estimated class membership probabilities
for the selected query image. For notational simplicity, assume
that the query image distribution {p1, . . . , pk} is in sorted
order such that p1 ‚â•p2 ‚â•. . . ‚â•pk. The algorithm proceeds
as follows. Select a representative sample image from class 1
and obtain user response. As long as a ‚Äúno match‚Äù response
is obtained for class i ‚àí1, select a sample image from class i
to present the user. This is continued until a ‚Äúmatch‚Äù response
is obtained. Through such a scheme, sample images from the
more likely categories are selected earlier in the process, in an
attempt to minimize the number of user responses required.
1) Annotation cost: In the binary feedback setting, our
experiments indicated that it is reasonable to assume that
each binary comparison requires a constant cost (time) for
annotation. Thus, for each query image, the cost incurred
to obtain the class label is equal to the number of binary
comparisons required. Since this number is unknown, we compute its expectation based on the estimated class membership
distribution instead. If the distribution is assumed to be in
sorted order as above, the expected number of user responses
to get a ‚Äúmatch‚Äù response is
1) . . . (1 ‚àípx
which is also the user annotation cost. We can scale the
misclassiÔ¨Åcation risk (by scaling M) with the real-world cost
incurred to Ô¨Ånd the true risk, which is in the same units as
annotation cost. Here we choose the true risk as the expected
number of misclassiÔ¨Åcations in the active pool, and compute it
by scaling M with the active pool size. Along with our choice
of C(x), this amounts to equating the cost of each binary input
from the user to every misclassiÔ¨Åcation, i.e., we can trade
one binary input from the user for correctly classifying one
unlabeled image.
C. Stopping criterion
The above VOI-based objective function leads to an appealing stopping criterion ‚Äì we can stop whenever the maximum expected VOI for any unlabeled image is negative,
i.e., argmaxx‚ààA V (x) < 0. With our deÔ¨Åned notions of
risk and cost, negative values of VOI indicate that a single
binary input from the user is not expected to reduce the
number of misclassiÔ¨Åcations by even one, hence querying
is not worth the information obtained. It should be noted
that different notions of real-world risk and annotation cost
could be employed instead if speciÔ¨Åc domain knowledge is
available. The selection and stopping criteria directly capture
the particular quantities used.
D. Initiating new classes
Many active learning methods make the restrictive assumption that the initial training set contains examples from all
categories . This assumption is unrealistic for most real
problems, since the user has to explicitly construct a training
set with all classes, defeating our goal of reducing supervision.
Also, if a system is expected to operate over long periods of
time, handling new classes is essential. Thus, we start with
small seed sets, and allow dynamic addition of new classes.
In the sample selection method described above, the user is
queried by showing sample images until a ‚Äúmatch‚Äù response is
obtained. However, if the query image belongs to a category
that is not present in the current training set, many queries
will be needed to initiate a new class.
Instead, we initiate a new class when a Ô¨Åxed small number
(say 5) of ‚Äúno-match‚Äù responses are obtained. With good category models, the expected distributions correctly capture the
categories of unlabeled images ‚Äì hence, ‚Äúno-match‚Äù responses
to the few most likely classes often indicates the presence of a
previously unseen category. However, it may happen that the
Input: Labeled set L, active pool A, cost matrix M
L0 := L; A0 := A
for round r = 0 to n ‚àí1 do
foreach image xi ‚ààA(r) do
for class yi = 1 to k do
Train multi-class classiÔ¨Åer with
L(r) ‚à™{xi, yi}
Estimate class membership probabilities
for images in the active pool A(r)
Compute risk on the active pool R(xi,yi)
Compute expected risk (L‚Ä≤r = Lr ‚à™{xi})
l P(yi = l) ¬∑ R(xi,l)
Compute expected annotation cost C(xi)
Find image x‚àó= argminxi‚ààA(r) RL‚Ä≤r + C(xi)
Find V (x‚àó) using Eqn. (4)
if V (x‚àó) > 0 then
Query user with query image x‚àóand likely
sample images until true label k‚àóis obtained
Set L(r+1) := L(r) ‚à™{x‚àó, k‚àó}; and
A(r+1) := A(r) \ {x‚àó}
else return L(n) = L(r)
Output: The new labeled set L(n)
Multi-class active learning with binary feedback.
unlabeled image belongs to a category present in the training
data. In such cases, creating a new class and assigning it to the
unlabeled image results in overclustering. This is dealt with
by agglomerative clustering (cluster merging), following the
min-max cut algorithm , along with user input.
The basic idea in agglomerative clustering is to iteratively
merge two clusters that have the highest similarity (linkage
value) l(Ci, Cj). For min-max clustering the linkage function is given by l(Ci, Cj) = s(Ci, Cj)/(s(Ci, Ci)s(Cj, Cj)),
where s indicates a cluster similarity score: s(Ci, Cj) =
y‚ààCj K(x, y). Here K is the kernel function that
captures similarity between two objects x and y (the same
kernel function is also used for classiÔ¨Åcation with SVM).
In our algorithm, we evaluate cluster linkage values after
each iteration of user feedback. If the maximum linkage value
(indicating cluster overlap) is for clusters Ci and Cj, and
is above a threshold of 0.5, we query the user by showing
two images from Ci and Cj. A ‚Äúmatch‚Äù response results in
merging of the two clusters. Note that our setting is much
simpler than the unsupervised clustering setting since we have
user feedback available. As such, the method is relatively
insensitive to the particular threshold used, and lesser noise
is encountered. Also, note that we do not need to compute
the linkage values from scratch at each iteration ‚Äì only a
simple incremental computation is required. In summary, new
classes are initiated quickly, and erroneous ones are corrected
by cluster merging with little user feedback.
E. Computational considerations
The computational complexity of each query iteration in
our algorithm (Figure 3) is O(N 2k3), with an active pool
of size N and k classes. Although it works well for small
problems, the cost can be impractical at larger scales. In the
following, we propose a new uncertainty measure for active
selection in multi-class scenarios that allows extremely fast
computation. The measure can be seen as one way to deÔ¨Åne
margins in the multi-class case. We will then use the proposed
selection measure to seed the search by restricting the number
of examples over which VOI has to be computed.
IV. MULTI-CLASS ACTIVE SELECTION MEASURE
Our approach follows the idea of uncertainty sampling ,
 , wherein examples on which the current classiÔ¨Åer is
uncertain are selected to query the user. Distance from the
hyperplane for margin-based classiÔ¨Åers has been used as a
notion of uncertainty in previous work. However, this does not
easily extend to multi-class classiÔ¨Åcation due to the presence
of multiple hyperplanes. We use a different notion of uncertainty that is easily applicable to a large number of classes.
The uncertainty can be obtained from the class membership
probability estimates for the unlabeled examples as output by
the multi-class classiÔ¨Åer. In the case of a probabilistic model,
these values are directly available. For other classiÔ¨Åers such as
SVM, we need to Ô¨Årst estimate class membership probabilities
of the unlabeled examples. In the following, we outline our
approach for estimating the probability values for multi-class
SVM. However, such an approach for estimating probabilities
can be used with many other non-probabilistic classiÔ¨Åcation
techniques also.
A. Probability estimation
In order to obtain class membership probability estimates
for unlabeled examples in the active pool, we follow the
approach proposed by , which is a modiÔ¨Åed version of
Platt‚Äôs method to extract probabilistic outputs from SVM .
The basic idea is to approximate the class probability
using a sigmoid function. Suppose that xi ‚ààRn are the
feature vectors, yi ‚àà{‚àí1, 1} are their corresponding labels,
and f(x) is the decision function of the SVM which can
be used to Ô¨Ånd the class prediction by thresholding. The
conditional probability of class membership P(y = 1|x) can
be approximated using
p(y = 1|x) =
1 + exp(Af(x) + B),
where A and B are parameters to be estimated. Maximum
likelihood estimation is used to solve for the parameters. In
order to generate probability estimates from the binary classi-
Ô¨Åers described above, pairwise coupling was used. Please
see for details on the probability estimation method.
We used the toolbox LIBSVM that implements the SVM
for classiÔ¨Åcation and probability estimation in the multi-class
B. Pairwise classiÔ¨Åcation
As shown above, we use pairwise SVM for classiÔ¨Åcation.
Consequently, O(k2) classiÔ¨Åers are required for a k class
problem. Even though training in the pairwise classiÔ¨Åers
setting appears to be computationally inefÔ¨Åcient compared to
the one-versus-all setting, which requires k classiÔ¨Åers, pairwise
classiÔ¨Åers can in fact be equally efÔ¨Åcient, or occasionally
even more so than the one-versus all setting for the following
reasons. In the one-vs-all setting, we need to train k classiÔ¨Åers with N (training set sample size) data samples each,
assuming no sampling approximations. On the other hand,
assuming relatively equal distribution of samples across all
the classes, each classiÔ¨Åer in the pairwise setting is trained
with about 2N/k samples. Further noting that SVM training
scales approximately quadratically with the training set size,
the pairwise setting often results in faster training on the
entire dataset. Along with faster training, pairwise classiÔ¨Åers
result in better prediction in our experiments. Computational
efÔ¨Åciency of pairwise classiÔ¨Åcation has also been demonstrated
previously in , and its superior classiÔ¨Åcation performance
was noted by Duan and Keerthi .
C. Entropy (EP) as uncertainty
Each labeled training example belongs to a certain class
denoted by y ‚àà{1, . . . , k}. However, we do not know true
class labels for examples in the active pool. For each unlabeled
example, we can consider the class membership variable to be
a random variable denoted by Y . We have a distribution p for
Y of estimated class membership probabilities computed in
the way described above. Entropy is a measure of uncertainty
of a random variable. Since we are looking for measures that
indicate uncertainty in class membership Y, its discrete entropy
is a natural choice. The discrete entropy of Y can be estimated
pi log(pi).
Higher values of entropy imply more uncertainty in the
distribution; this can be used as an indicator of uncertainty
of an example. If an example has a distribution with high
entropy, the classiÔ¨Åer is uncertain about its class membership.
The algorithm proceeds in the following way. At each round
of active learning, we compute class membership probabilities
for all examples in the active pool. Examples with the highest
estimated value of discrete entropy are selected to query
the user. User labels are obtained and the corresponding
examples are incorporated in the training set and the classiÔ¨Åer
is retrained. As will be seen in Section IV-H, active learning
through entropy (EP)-based selection outperforms random
selection in some cases.
D. Best-versus-Second Best (BvSB)
Even though EP-based active learning is often better than
random selection, it has a drawback. A problem of the EP
measure is that its value is heavily inÔ¨Çuenced by probability
values of unimportant classes. See Figure 4 for a simple
illustration. The Ô¨Ågure shows estimated probability values for
two examples on a 10-class problem. The example on the left
has a smaller entropy than the one on the right. However, from
a classiÔ¨Åcation perspective, the classiÔ¨Åer is more confused
about about the former since it assigns close probability
values to two classes. For the example in Figure 4(b), small
probability values of unimportant classes contribute to the
high entropy score, even though the classiÔ¨Åer is much more
conÔ¨Ådent about the classiÔ¨Åcation of the example. This problem
becomes even more acute when a large number of classes are
present. Although entropy is a true indicator of uncertainty of
a random variable, we are interested in a more speciÔ¨Åc type
of uncertainty relating only to classiÔ¨Åcation amongst the most
confused classes (the example is virtually guaranteed to not
belong to classes having a small probability estimate).
Instead of relying on the entropy score, we take a more
greedy approach to account for the problem mentioned. We
consider the difference between the probability values of the
two classes having the highest estimated probability value as
a measure of uncertainty. Since it is a comparison of the best
guess and the second best guess, we refer to it as the Bestversus-Second-Best (BvSB) approach . Such a measure
is a more direct way of estimating confusion about class
membership from a classiÔ¨Åcation standpoint. Using the BvSB
measure, the example on the left in Figure 4 will be selected to
query the user. As mentioned previously, conÔ¨Ådence estimates


An illustration of why entropy can be a poor estimate of classiÔ¨Åcation
uncertainty. The plots show estimated probability distributions for two unlabeled examples in a 10 class problem. In (a), the classiÔ¨Åer is highly confused
between classes 4 and 5. In (b), the classiÔ¨Åer is relatively more conÔ¨Ådent that
the example belongs to class 4, but is assigned higher entropy. The entropy
measure is inÔ¨Çuenced by probability values of unimportant classes.
are reliable in the sense that classes assigned low probabilities
are very rarely the true classes of the examples. However,
this is only true if the initial training set size is large enough
for good probability estimation. In our experiments, we start
from as few as 2 examples for training in a 100 class problem.
In such cases, initially the probability estimates are not very
reliable, and random example selection gives similar results.
As the number of examples in the training set grows, active
learning through BvSB quickly dominates random selection
by a signiÔ¨Åcant margin.
E. Another perspective
One way to see why active selection works is to consider
the BvSB measure as a greedy approximation to entropy
for estimating classiÔ¨Åcation uncertainty. We describe another
perspective that explains why selecting examples in this way
is beneÔ¨Åcial. The understanding crucially relies on our use
of one-versus-one approach for multi-class classiÔ¨Åcation. Suppose that we wish to estimate the value of a certain example


  
Illustration of one-vs-one classiÔ¨Åcation (classes that each classiÔ¨Åer
separates are noted). Assuming that the estimated distribution for the unlabeled
example (shown as a blue disk) peaks at ‚ÄòClass 4‚Äô, the set of classiÔ¨Åers in
contention is shown as red lines. BvSB estimates the highest uncertainty in
this set ‚Äì uncertainty of other classiÔ¨Åers is irrelevant.
for active selection. Say its true class label is l (note that this
is unknown when selecting the example). We wish to Ô¨Ånd
whether the example is informative, i.e., if it will modify the
classiÔ¨Åcation boundary of any of the classiÔ¨Åers, once its label
is known. Since its true label is l, it can only modify the
boundary of the classiÔ¨Åers that separate class l from the other
classes. We call these classiÔ¨Åers as those in contention, and
denote them by Cl = {C(l,i) | i = 1, . . . , k, i Ã∏= l}, where
C(i,j) indicates the binary classiÔ¨Åer that separates class i from
class j. Furthermore, in order to be informative at all, the
selected example needs to modify the current boundary (be a
good candidate for a new support vector ‚Äì as indicated by its
uncertainty). Therefore, one way to look at multi-class active
selection for one-versus-one SVMs is the task of Ô¨Ånding an
example that is likely to be a support vector for one of the
classiÔ¨Åers in contention, without knowing which classiÔ¨Åers are
in contention. See Figure 5 for an illustration.
Say that our estimated probability distribution for a certain
example is denoted by p, where pi denotes the membership
probability for class i. Also suppose that the distribution p has
a maximum value for class h. Based on current knowledge, the
most likely set of classiÔ¨Åers in contention is Ch. The classi-
Ô¨Åcation conÔ¨Ådence for the classiÔ¨Åers in this set is indicated
by the difference in the estimated class probability values,
ph ‚àípi. This difference is an indicator of how informative
the particular example is to a certain classiÔ¨Åer. Minimizing the
difference ph ‚àípi, or equivalently, maximizing the confusion
(uncertainty), we obtain the BvSB measure. This perspective
shows that our intuition behind choosing the difference in
the top two probability values of the estimated distribution
has a valid underlying interpretation ‚Äì it is a measure of
uncertainty for the most likely classiÔ¨Åer in contention. Also,
the BvSB measure can then be considered to be an efÔ¨Åcient
approximation for selecting examples that are likely to be
informative, in terms of changing classiÔ¨Åcation boundaries.
F. Binary classiÔ¨Åcation
For binary classiÔ¨Åcation problems, our method reduces to
selecting examples closest to the classiÔ¨Åcation boundary, i.e.,
examples having the smallest margin. In binary problems, the
BvSB measure Ô¨Ånds the difference in class membership probability estimates between the two classes. The probabilities are
Caltech-101
DATASET DETAILS. # POOL = ACTIVE POOL SIZE, # TEST = TEST SET SIZE.
estimated using Equation 7, that relies on the function value
f(x) of each unlabeled example. Furthermore, the sigmoid
Ô¨Åt is monotonic with the function value ‚Äì the difference in
class probability estimates is larger for examples away from
the margin. Therefore, our active learning method can be
considered to be a generalization of binary active learning
schemes that select examples having the smallest margin.
G. Computational cost
There are two aspects to the cost of active selection. One
is the cost of training the SVM on the training set at each
iteration. Second is probability estimation on the active pool,
and selecting examples with the highest BvSB score. Since
we use one-vs-one SVM, we need to train O(k2) classiÔ¨Åers
for k classes. As the essence of active learning is to minimize
training set sizes through intelligent example selection, it is
also important to consider the cost of probability estimation
and example selection on the relatively much larger active
pool. The Ô¨Årst cost comes from probability estimation in
binary SVM classiÔ¨Åers. The estimation is efÔ¨Åcient since it
is performed using Newton‚Äôs method with backtracking line
search that guarantees quadratic rate of convergence. Given
class probability values for binary SVMs, multi-class probability estimates can be obtained in O(k) time per example
 . With N examples in the active pool, the entire BvSB
computation scales as O(Nk2).
H. Experiments with BvSB
In this section, we show experiments demonstrating the
ability of the BvSB measure to select informative examples
for query. Note that this section reports results only using this
uncertainty measure and not VOI. Later, we incorporate BvSB
as an approximation to reduce the computational expense of
VOI computation. We demonstrate results on standard image
datasets available from the UCI repository , the Caltech-
101 dataset of object categories, and a dataset of 13 natural
scene categories. All the results show signiÔ¨Åcant improvement
owing to active example selection. Table II shows a summary
of datasets used and their details. For choosing the kernel, we
ran supervised learning experiments with linear, polynomial,
and Radial Basis Function (RBF) kernels on a randomly
chosen training set, and picked the kernel that gave the best
classiÔ¨Åcation accuracy averaging over multiple runs.
1) Reduction in training required: In this section, we
perform experiments to quantify the reduction in the number
of training examples required for BvSB to obtain similar
classiÔ¨Åcation accuracy as random example selection. For each
round of active learning, we Ô¨Ånd the number of rounds of
random selection to achieve the same classiÔ¨Åcation accuracy.
In other words, Ô¨Åxing the classiÔ¨Åcation accuracy achieved,
we measure the difference in the training set size of both
methods and report the corresponding training rounds in Table
III. The table shows that active learning achieves a reduction
BvSB selection
Random selection
% Reduction in #
training examples
PERCENTAGE REDUCTION IN THE NUMBER OF TRAINING EXAMPLES
PROVIDED TO THE ACTIVE LEARNING ALGORITHM TO ACHIEVE
CLASSIFICATION ACCURACY EQUAL TO OR MORE THAN RANDOM
EXAMPLE SELECTION ON THE USPS DATASET.
of about 50% in the number of training examples required, i.e.,
it can reach near optimal performance with 50% fewer training
examples. Table III reports results for the USPS dataset,
however, similar results were obtained for the Pendigits dataset
and the Letter dataset.
An important point to note from Table III is that active
learning does not provide a large beneÔ¨Åt in the initial rounds.
One reason for this is that all methods start with the same
seed set initially. In the Ô¨Årst few rounds, the number of
examples actively selected are far fewer compared to the seed
set size (100 examples). Actively selected examples thus form
a small fraction of the total training examples, explaining the
small difference in classiÔ¨Åcation accuracy of both methods
in the initial rounds. As the number of rounds increase, the
importance of active selection becomes clear, explained by
the reduction in the amount of training required to reach nearoptimal performance.
I. Exploring the space
Space exploration of active selection ‚Äì BvSB-based selection is
almost as good as random exploration, while the former achieves much higher
classiÔ¨Åcation accuracy than random.
In many applications, the number of categories to be classiÔ¨Åed is extremely large, and we start with only a few labeled
images. In such scenarios, active learning has to balance two
often conÔ¨Çicting objectives ‚Äì exploration and exploitation.
Exploration in this context means the the ability to obtain
labeled images from classes not seen before. Exploitation
refers to classiÔ¨Åcation accuracy on the classes seen so far.
Exploitation can conÔ¨Çict with exploration, since in order to
achieve high classiÔ¨Åcation accuracy on the seen classes, more
training images from those classes might be required, while
sacriÔ¨Åcing labeled images from new classes. In the results so
far, we show classiÔ¨Åcation accuracy on the entire test data
consisting of all classes ‚Äì thus good performance requires
a good balance between exploration and exploitation. Here
we explicitly demonstrate how the different example selection
mechanisms explore the space for the Caltech-101 dataset that
has 102 categories. Figure 6 shows that the BvSB measure
Ô¨Ånds newer classes almost as fast as random selection, while
achieving signiÔ¨Åcantly higher classiÔ¨Åcation accuracy than random selection. Fast exploration of BvSB implies that learning
can be started with labeled images from very few classes and
the selection mechanism will soon obtain images from the
unseen classes. Interestingly, EP-based selection explores the
space poorly.
   

 & &
& &
& &&
Active learning on the 13 natural scene categories dataset.
1) Scene recognition: Further, we performed experiments
for the application of classifying natural scene categories on
the 13 scene categories dataset . GIST image features 
that provide a global representation were used. Results are
shown in Figure 7. The Ô¨Ågure shows accuracy improvement
(active selection accuracy - random selection accuracy) per
class after 30 BvSB-based active learning rounds. Note that
although we do not explicitly minimize redundancy amongst
images, active selection leads to signiÔ¨Åcant improvements even
when as many as 20 images are selected at each active
learning round.
Top row shows images on which the classiÔ¨Åer is uncertain using the
BvSB score. Bottom row shows images on which the classiÔ¨Åer is conÔ¨Ådent.
True labels are noted below the corresponding images. We can see that the
top row has more confusing images, indicating that the active learning method
chooses harder examples.
2) Which examples are selected?: In Figure 8, we show
example images from the USPS dataset and their true labels.
The top row images were confusing for the classiÔ¨Åer (indicated
by their BvSB score) and were therefore selected for active
learning at a certain iteration. The bottom row shows images
on which the classiÔ¨Åer was most conÔ¨Ådent. The top row has
more confusing images even for the human eye, and ones that
do not represent their true label well. We noticed that the most
conÔ¨Ådent images (bottom row) consisted mainly of the digits
‚Äò1‚Äô and ‚Äò7‚Äô, which were clearly drawn. The results indicate that
the active learning method selects hard examples for query.
Y-axis: # examples correctly classiÔ¨Åed by random example selection
for a given class. X-axis: # examples of the corresponding class chosen by
active selection. The negative correlation shows that active learning chooses
more examples from harder classes.
One of the reasons active learning algorithms perform well
is the imbalanced selection of examples across classes. In
our case, the method chooses more examples for the classes
which are hard to classify (based on how the random example
selection algorithm performs on them). Figure 9 demonstrates
the imbalanced example selection across different classes on
the Caltech-101 dataset. On the y-axis, we plot the number of
examples correctly classiÔ¨Åed by the random example selection
algorithm for each class, as an indicator of hardness of the
class. Note that the test set used in this case is balanced with
15 images per class. On the x-axis, we plot the number of
examples selected by the active selection algorithm for the
corresponding class from the active pool. The data shows a
distinct negative correlation, indicating that more examples
are selected from the harder classes, conÔ¨Årming our intuition.
Notice the empty region on the bottom left of the Ô¨Ågure,
showing that active learning selected more images from all
classes that were hard to classify.
J. Approximations to VOI
In the previous section, we showed how the proposed
uncertainty sampling measure can efÔ¨Åciently select informative examples for active learning. Here we discuss some
approximations that substantially improve the running time
of the proposed VOI algorithm using the BvSB measure.
The VOI algorithm described previously in Figure 3 is the
original algorithm on which the following approximations are
performed (line numbers refer to the algorithm).
1) Seed sampling: Since VOI computation is relatively
expensive, Ô¨Ånding the scores for all examples in the active
pool is costly (line 3). Instead, we use the BvSB measure to



Active learning in the BF model requires far lesser user training time compared to active selection in the MCF model. US: uncertainty sampling,
RND: random. (a) USPS, (b) Pendigits, (c) Caltech-101 datasets.
sample uncertain examples from the active pool on which VOI
computation is performed. Typically, a sample of 50 examples
is obtained from active pools of thousands of examples. We
observed that even though BvSB and VOI do not correlate
perfectly, the the top 50 examples chosen by BvSB almost
always contain the examples that would have been the highest
ranked using VOI alone. Quantitatively, the results differ only
2% of the time, and the difference in classiÔ¨Åcation accuracy
is negligible. On the other hand, the computational speedups
achieved are substantial.
2) Expected value computation: In the VOI algorithm,
estimating expected risk is expensive. For each unlabeled
image, we need to train classiÔ¨Åers assuming that the image
can belong to any of the possible categories (line 4). This
can be slow when many classes are present. To overcome
this, we make the following observation: given the estimated
probability distribution of an unlabeled image, it is unlikely to
belong to the classes that are assigned low probability values,
i.e., the image most likely belongs to the classes that have the
highest estimated probabilities. As such, instead of looping
over all possible classes, we can only loop over the most likely
ones. In particular, we loop over only the top 2 most likely
classes, as they contain most of the discriminative information,
as utilized in the BvSB measure. Such an approximation
relies to some extent on the correctness of the estimated
model, which implies an optimistic assumption often made for
computational tractability . Further, we can use the same
‚Äútop-2‚Äù approximation, for computing the expected risk (line
9) on unlabeled images, as an approximation to Eqn. (1).
3) Clustering for estimating risk: In the above algorithm,
the risk needs to be estimated on the entire active pool.
Instead, we Ô¨Årst cluster the unlabeled images in the active pool
using the kernel k-means algorithm [?]. Then we form a new
unlabeled image set by choosing one representative (closest to
the centroid) image from each cluster, and estimate risk on this
reduced set. The clustering needs to be performed only once
initially, and not in every query iteration. In our implementation, we Ô¨Åx the number of clusters as 1/100 fraction of the
active pool size. Experiments showed that this approximation
rarely (less than 5% of the time) changes the images selected
actively, and makes a negligible difference in the estimated
risk value, and the future classiÔ¨Åcation accuracy.
With the above approximations, the complexity of each
query iteration is O(Nk2), a large improvement over the
original version. In Section VI, we propose a sublinear time
approximation for scaling to very large datasets.
V. EXPERIMENTAL RESULTS
In this section, we evaluate the proposed VOI algorithm on
various datasets described in Table II. Scene-13 is a dataset of
13 natural scene categories , for which we employ GIST
features . Precomputed pyramid match kernel matrices
 were used as features for the Caltech-101 dataset.
For implementation we used Matlab along with the LIB-
SVM toolbox (written in C, interfaced with Matlab for
SVM and probability estimation). With an active pool size
of 5000 images for a 10-class problem (USPS) each query
iteration on average takes about 0.9 seconds on a 2.67 Ghz
Xeon machine. For the Caltech dataset with an active pool
of size 1515 images with 101 classes, a query iteration takes
about 1.3 seconds.
A. User interaction time
We have previously demonstrated the beneÔ¨Åts of the BF
model as compared to MCF from the ease of interaction
standpoint. Here we compare the total user annotation time
required with various methods to achieve similar classiÔ¨Åcation
rates. The comparison shows the following methods: our proposed VOI method with binary feedback (VOI+BF), VOI with
Multi-class feedback (MCF), active learning using only the
BvSB measure (US+MCF), where US stands for uncertainty
sampling, and random selection with both BF and MCF. Figure
10 shows the substantial reduction in user training time with
the proposed method. For all the datasets, the proposed VOIbased algorithm beats all others (including active selection
with MCF), indicating that the advantages come from both our
active selection algorithm, as well as the binary feedback
model. Further, note that the relative improvement is larger for
the Caltech dataset, as it has a larger number of categories. As
such, we can train classiÔ¨Åers in a fraction of the time typically
required, demonstrating the strength of our approach for multiclass problems.
B. Importance of considering annotation cost
As mentioned before, we use uncertainty sampling(US)based active selection to form a smaller set from which the
most informative images are selected using VOI computation.
Here we demonstrate that the good results are not due to
uncertainty sampling alone. Figure 11 compares the number of
binary comparisons the user has to provide in our algorithm

 !"#
$

 !"#
$
VOI-based active selection and uncertainty sampling (both
with BF) during the initial phases of active learning.
Confusion matrices with (a) active (VOI), and (b) random selection
(max. trace = 1515). VOI leads to much lower confusion.
along with the BvSB uncertainty sampling method (also in the
BF model) in the initial stages of active learning. The Ô¨Ågure
shows two plots with 50 and 70 class problems, obtained from
the Caltech-101 dataset. Our method signiÔ¨Åcantly outperforms
US in both cases, and the relative improvement increases with
problem size. As the number of classes increases, considering
user annotation cost for each query image becomes increasingly important. The VOI framework captures annotation cost
unlike US, explaining the better performance for the 70 class
C. Active selection (VOI) v/s random selection
Figure 12 shows the confusion matrices for active selection
with VOI as well as random selection on the Caltech 101 class
problem. Active selection results in much lesser confusion,
also indicated by the trace of the two matrices. This demonstrates that the algorithm offers large advantages for many
category problems. Figure 14 shows per-class classiÔ¨Åcation
accuracy of both VOI and random selection methods on the
Scene-13 dataset. VOI achieves higher accuracy for 9 of the
13 classes, and comprehensively beats random selection in the
overall accuracy.
D. Noise sensitivity
In many real-world learning tasks, the labels are noisy, either
due to errors in the gathering apparatus, or even because of
human annotation mistakes. It is therefore important for the
learning algorithm to be robust to a reasonable amount of
labeling noise. In this section, we perform experiments to
quantify the noise sensitivity of the methods. We artiÔ¨Åcially
impart stochastic labeling noise to the training images. For
example, 5% noise implies that training images are randomly
given an incorrect label with a probability of 0.05. The
algorithms are then run on the noisy as well as clean data
‚Äì results for the USPS dataset are shown in Figure 13.
The Ô¨Ågure shows both active and random selection on clean
as well as noisy data (10% and 20% noise). Expectedly, there
is a reduction in classiÔ¨Åcation accuracy for both algorithms
when noise is introduced. Interestingly, however, even with
as much as 10% label noise, the active learning method
still outperforms random selection on clean data, whereas
with about 20% noise, active learning still matches random
selection on clean data. This result shows that active selection
can tolerate a signiÔ¨Åcant amount of noise while giving a high
classiÔ¨Åcation rate.
One reason why active selection can be robust to noise arises
from the fact that the algorithm selects ‚Äúhard‚Äù examples for

 !"#
$%&!#
'(!#

 !"#
$%&!#
'(!#
Sensitivity to label noise, (a) 10%, (b) 20%. VOI with noisy data
outperforms the random selection with clean data.
query. In most cases, these examples lie close to the separating
boundaries of the corresponding classiÔ¨Åers. Intuitively, we
expect noise in these examples to have a smaller effect, since
they change the classiÔ¨Åcation boundary marginally. In contrast,
a misclassiÔ¨Åed example deep inside the region associated with
a certain class can be much more harmful. In essence, through
its example selection mechanism, active learning encounters
noise that has a relatively smaller impact on the classiÔ¨Åcation
boundary, and thus the future classiÔ¨Åcation rate.
E. Population imbalance
Real-world data often exhibits class population imbalance,
with vastly varying number of examples belonging different
classes . For example, in the Caltech-101 dataset, the
category ‚Äòairplanes‚Äô has over 800 images, while the category
‚Äòwrench‚Äô has only 39 images.
 
Fig. 14. Per-class accuracy of VOI
v/s random on the scene-13 dataset.
Population imbalance: VOI
selects many images even for classes
with small populations (see text for
We demonstrate here that active selection can effectively
counter population imbalances in order to generalize better.
The experiment is conducted as follows. The active pool (from
which unlabeled images are selected for query) consisting of
vastly varying number of examples of each class is generated
for the Pendigits dataset. However, the test set is kept unmodiÔ¨Åed. In this scenario, random example selection suffers since
it obtains fewer examples from the less populated classes.
Active selection, on the other hand, counters the imbalance
by selecting a relatively higher number of examples even
from the less populated classes. Figure 15 demonstrates the
results. The three bars show (normalized) number of examples
per class in the unlabeled pool, and in the training sets with
active and random selection. Random selection does poorly ‚Äì
for instance, it does not obtain even a single training image
from class ‚Äò9‚Äô due to its low population in the unlabeled
pool. Active selection overcomes population imbalance and
selects many images from class ‚Äò9‚Äô. This is further reinforced
by computing the variance in the normalized population. The
standard deviation in the (normalized) number of examples
selected per class with active and random selection is 0.036
and 0.058 respectively. The signiÔ¨Åcantly smaller deviation
shows that active selection overcomes population imbalance
to a large extent.
F. Fast initiation of new classes
W/ clustering
Caltech-101
USER TRAINING TIME REQUIRED TO ENCOUNTER ALL 101 CLASSES.
In Section III-D, we described our method of quickly
initiating new classes and then merging the erroneous ones
using agglomerative clustering and user feedback. Table IV
summarizes the advantages of the approach (i.e., w/ clustering)
compared to simple category initiation when a new image
does not match any training image (naive). We start with a
small seed set of 20 images, and run the experiment until both
methods encounter all the 101 categories in the data. Note the
large reduction in user training time with clustering, due to the
fewer number of binary comparisons requested. This aspect is
increasingly important as the number of classes increases.
VI. SPEEDING UP ACTIVE LEARNING
There has been some recent work on scaling up active learning to work with large datasets. In , a graph-regularization
approach is proposed to maximize the expected information
gain for scalable active learning. Segal and Markowitz 
propose an approximate uncertainty sampling approach in
which only a subset of samples are evaluated at each iteration
for active learning. Their approach provides speedups for the
application of labeling email corpora. A hierarchical sampling
approach along with feature space indexing was proposed for
scaling active learning to large datasets by Panda et al. .
In this section, we show initial results with a different
approach to speeding up active learning via locality sensitive
hashing (LSH) . As opposed to previous work, our method
does not modify the active selection criteria and can work
with any classiÔ¨Åers. Instead of performing an exhaustive search
with a linear scan (LS) of the entire unlabeled pool, the main
idea is to Ô¨Årst Ô¨Ånd representative samples that are informative
(for seeding the search) according to our active selection
measure. Using locality sensitive hashing on these samples,
informative samples from the unlabeled pool are obtained (in
time scaling sublinearly with the pool size). This approach
provides up to 2 orders of magnitude speed-up on the linear
scan active learning version, while making little difference in
classiÔ¨Åcation accuracy. We can thus scale the algorithm to
datasets with hundreds of thousands of samples. With a pool
size of 50000 images represented in 384-dimensional space,
the LSH-based approximation provides a 91-fold speedup on
average with negligible reduction in classiÔ¨Åcation accuracy. In
the following, we provide a brief introduction to LSH using
p-stable distributions followed by its application in our
active learning algorithm.
A. LSH with p-Stable Distributions
DeÔ¨Ånition 1:
 A LSH family H = {h : S ‚ÜíU} is
called (r1, r2, p1, p2)-sensitive for distance d if for any u, v ‚àà
‚Ä¢ if u ‚ààB(v, r1), then PrH[h(u) = h(v)] ‚â•p1,
‚Ä¢ if u /‚ààB(v, r2), then PrH[h(u) = h(v)] ‚â§p2,
where B(q, r) indicates a ball of radius r centered at q. If
p1 > p2 and r1 < r2, the family H can be used for the
(R,c)-NN problem , wherein one has to retrieve points p
such that d(p, q) ‚â§cR, if there exists a point in P within
distance R from q. The basic idea is that the hash functions
evaluate to the same values with high probability for points
that are close to each other, whereas for distant points the
probability of matching (collision) is low. The probability gap
can be increased by concatenation of multiple hash functions
chosen randomly from the family H.
B. Sublinear time Active Learning
Here we propose a simple way to speed up active learning
using LSH. During preprocessing, we Ô¨Årst hash all the points
in the database3 to the respective buckets using the chosen
hash functions. At each iteration, we pick the samples from
our training data that give the highest VOI assuming they
are unlabeled. These samples are treated as informative seed
samples that will be used as queries to retrieve the nearest
neighbors from the active pool, in the hope that they will
also be informative. Since the training set is usually orders
of magnitude smaller than the unlabeled pool, a linear scan to
choose best samples from it does not slow down the algorithm.
Also, other seeding strategies that do not require a scan could
easily be employed instead.
Assuming that the VOI function is spatially smooth, the
rationale behind choosing the nearest neighbors of the points
with high VOI is to Ô¨Ånd other unlabeled points with high VOI.
Intuitively, many functions that capture informativeness of
samples such as distance to hyperplane etc. can be reasonably
assumed to be smooth, so that such a search will lead to
useful samples for active learning. Furthermore, note that
the proposed strategy does not depend on the choice of the
classiÔ¨Åer or the active selection measure used. It can be employed for other classiÔ¨Åers as well as other selection measures
seamlessly. The hashing method as proposed requires the
3It is shown in that the hash functions ha,b(x) = ‚åäa¬∑x+b
‚åã, where each
element of a is sampled from N(0, 1), and b chosen uniformly from [0, r]
represents a (R, cR, p1, p2)-sensitive LSH family for the Euclidean distance
  
(a) Speedup achieved with LSH over LS for the approximate near neighbor problem on the Cifar-10 dataset. c = 1 + «´ denotes the approximation
factor. (b),(c) Active learning with the LSH approximation gives little difference in accuracy compared to the LS. (b) USPS dataset, (c) Cifar-10 dataset. On
average, the speedup for USPS was 17-fold, while that for Cifar-10 was 91-fold.
explicit feature vectors of the data samples, and as such cannot
be used directly for kernel matrices. Extending to kernels using
kernelized LSH is an interesting direction for future work.
C. Experiments with hashing
Experiments are performed on two datasets: the USPS
dataset used previously, and the Cifar-10 dataset , which is
a collection of 50000 training images and 10000 test images
obtained from the 80 million tiny images dataset . For
Cifar-10, 384-d GIST descriptors are used as per .
Our algorithm relies on LSH retrieving points that are
close to the query points with high probability. Here we Ô¨Årst
perform an experiment with the Cifar-10 dataset to analyze
how efÔ¨Åciently nearest neighbors are retrieved by LSH. The
setup is as follows. For each iteration, a random point was
selected as the query. The LSH and LS were run to Ô¨Ånd the
near neighbors of the query, while noting the time required
for both along with the distance to the nearest neighbor found
(LS Ô¨Ånds the true nearest neighbor). The distance to the nearest
neighbor found by LSH is normalized by the distance to the
true neighbor to Ô¨Ånd the approximation factor c = 1 + «´. We
ran 1000 such iterations and the resulting speedup values were
put into 5 bins.
Figure 16(a) shows a plot of the approximation factor
achieved versus speedup. As expected, we see that a higher
speedup gives worse approximation. The speedups however
are large across the entire spectrum of approximation values, achieving a 400-fold speedup for a 0.2-approximation
(c = 1.2). Note that the approximation guarantees for LSH are
conservative, and we observe signiÔ¨Åcantly better performance
in practice. Furthermore, since the LSH algorithm scales
sublinearly with data size, we expect the speedups to be even
larger for bigger datasets.
It is important to note that even a crude approximation
to nearest neighbor does not necessarily hurt active learning.
Active selection measures are typically based on computations
of potential informativeness of the data sample which are often
approximate, and are heavily dependent on the current model.
As such, even points that are not the nearest neighbors to
informative queries might have very close (and sometimes
even better) informativeness scores than the true nearest neighbors. Our experiment below demonstrates that this is indeed
the case: an approximate nearest neighbor often makes no
difference in the informativeness values of the chosen samples
as well as in the Ô¨Ånal classiÔ¨Åcation accuracy achieved.
Figures 16 (b),(c) show classiÔ¨Åcation accuracy comparisons
between LS and LSH active learning algorithms. In both plots,
the difference in accuracy due to the approximation is very
small, whereas the LSH-based active learning algorithms run
about 1 and 2 orders of magnitude faster respectively on
USPS (‚àº5000 samples) and Cifar-10 (‚àº50000 samples). As
mentioned before, the speedup is expected to increase with the
dataset size, since the linear scan takes O(N) time whereas
LSH-based active learning runs in expected time O(N œÅ) with
œÅ < 1. This demonstrates the powerful scaling ability of the
locality sensitive hashing approach to active learning.
VII. CONCLUSION AND FUTURE WORK
In this paper, we proposed a multi-class active learning
framework using only binary feedback. A Value of Information
algorithm was developed for active learning in this framework,
along with a simple and efÔ¨Åcient active selection measure.
The feedback modality allows very efÔ¨Åcient annotation in
multi-class problems and thereby substantially reduces training
time and effort. Further, we presented results using locality
sensitive hashing to speed up active learning so as to achieve
sublinear time scaling (w.r.t. dataset size) for choosing a query.
The proposed modiÔ¨Åcation achieved two orders of magnitude
speedup with little difference in classiÔ¨Åcation accuracy. Future
work will focus on batch-mode sampling and further improving scaling to allow thousands of data categories along with
millions of samples.
ACKNOWLEDGMENT
Foundation
#IIP-0443945,
#CNS-0708344,
#CNS-0821474,
#IIP-0934327,
#CNS-1039741, #IIS-1017344, #IIP-1032018, and #SMA-
1028076, the Minnesota Department of Transportation, and
the ITS Institute at the University of Minnesota. We thank
Professor Kristen Grauman for providing kernel matrices for
Caltech-101 data, and the anonymous reviewers for their
helpful suggestions.