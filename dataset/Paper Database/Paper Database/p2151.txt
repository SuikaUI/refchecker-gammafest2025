Attention Augmented Convolutional Networks
Irwan Bello
Barret Zoph
Ashish Vaswani
Jonathon Shlens
Quoc V. Le
Google Brain
{ibello,barretzoph,avaswani,shlens,qvl}@google.com
Convolutional networks have been the paradigm of
choice in many computer vision applications. The convolution operation however has a signiﬁcant weakness in that it
only operates on a local neighborhood, thus missing global
information. Self-attention, on the other hand, has emerged
as a recent advance to capture long range interactions, but
has mostly been applied to sequence modeling and generative modeling tasks. In this paper, we consider the use of
self-attention for discriminative visual tasks as an alternative to convolutions. We introduce a novel two-dimensional
relative self-attention mechanism that proves competitive
in replacing convolutions as a stand-alone computational
primitive for image classiﬁcation. We ﬁnd in control experiments that the best results are obtained when combining
both convolutions and self-attention. We therefore propose
to augment convolutional operators with this self-attention
mechanism by concatenating convolutional feature maps
with a set of feature maps produced via self-attention. Extensive experiments show that Attention Augmentation leads
to consistent improvements in image classiﬁcation on ImageNet and object detection on COCO across many different models and scales, including ResNets and a stateof-the art mobile constrained network, while keeping the
number of parameters similar. In particular, our method
achieves a 1.3% top-1 accuracy improvement on ImageNet
classiﬁcation over a ResNet50 baseline and outperforms
other attention mechanisms for images such as Squeezeand-Excitation . It also achieves an improvement of
1.4 mAP in COCO Object Detection on top of a RetinaNet
1. Introduction
Convolutional Neural Networks have enjoyed tremendous success in many computer vision applications, especially in image classiﬁcation . The design of the
convolutional layer imposes 1) locality via a limited receptive ﬁeld and 2) translation equivariance via weight sharing.
number of parameters (millions)
top-1 accuracy
AA-ResNet (ours)
Figure 1. Attention Augmentation systematically improves image classiﬁcation across a large variety of networks of different
scales. ImageNet classiﬁcation accuracy versus the number of
parameters for baseline models (ResNet) , models augmented
with channel-wise attention (SE-ResNet) and our proposed
architecture (AA-ResNet).
Both these properties prove to be crucial inductive biases
when designing models that operate over images. However,
the local nature of the convolutional kernel prevents it from
capturing global contexts in an image, often necessary for
better recognition of objects in images .
Self-attention , on the other hand, has emerged as a
recent advance to capture long range interactions, but has
mostly been applied to sequence modeling and generative
modeling tasks. The key idea behind self-attention is to
produce a weighted average of values computed from hidden units. Unlike the pooling or the convolutional operator,
the weights used in the weighted average operation are produced dynamically via a similarity function between hidden units. As a result, the interaction between input signals
depends on the signals themselves rather than being predetermined by their relative location like in convolutions. In
particular, this allows self-attention to capture long range
 
Attention maps
Weighted average of the values
Standard convolution
Figure 2. Attention-augmented convolution: For each spatial location (h, w), Nh attention maps over the image are computed from
queries and keys. These attention maps are used to compute Nh weighted averages of the values V. The results are then concatenated,
reshaped to match the original volume’s spatial dimensions and mixed with a pointwise convolution. Multi-head attention is applied in
parallel to a standard convolution operation and the outputs are concatenated.
interactions without increasing the number of parameters.
In this paper, we consider the use of self-attention for
discriminative visual tasks as an alternative to convolutions. We develop a novel two-dimensional relative selfattention mechanism that maintains translation equivariance while being infused with relative position information, making it well suited for images. Our self-attention
formulation proves competitive for replacing convolutions
entirely, however we ﬁnd in control experiments that the
best results are obtained when combining both. We therefore do not completely abandon the idea of convolutions,
but instead propose to augment convolutions with this selfattention mechanism.
This is achieved by concatenating
convolutional feature maps, which enforce locality, to selfattentional feature maps capable of modeling longer range
dependencies (see Figure 2).
We test our method on the CIFAR-100 and ImageNet
classiﬁcation and the COCO object detection 
tasks, across a wide range of architectures at different computational budgets, including a state-of-the art resource
constrained architecture .
Attention Augmentation
yields systematic improvements with minimal additional
computational burden and notably outperforms the popular Squeeze-and-Excitation channelwise attention approach in all experiments. In particular, Attention Augmentation achieves a 1.3% top-1 accuracy ImageNet on top of
a ResNet50 baseline and 1.4 mAP increase in COCO object detection on top of a RetinaNet baseline. Suprisingly,
experiments also reveal that fully self-attentional models,
a special case of Attention Augmentation, only perform
slightly worse than their fully convolutional counterparts on
ImageNet, indicating that self-attention is a powerful standalone computational primitive for image classiﬁcation.
2. Related Work
2.1. Convolutional networks
Modern computer vision has been built on powerful image featurizers learned on image classiﬁcation tasks such
as CIFAR-10 and ImageNet . These datasets have
been used as benchmarks for delineating better image featurizations and network architectures across a broad range
of tasks . For example, improving the “backbone” network typically leads to improvements in object detection
 and image segmentation . These observations have
inspired the research and design of new architectures, which
are typically derived from the composition of convolution
operations across an array of spatial scales and skip connections . Indeed, automated
search strategies for designing architectures based on convolutional primitives result in state-of-the-art accuracy on
large-scale image classiﬁcation tasks that translate across a
range of tasks .
2.2. Attention mechanisms in networks
Attention has enjoyed widespread adoption as a computational module for modeling sequences because of its
ability to capture long distance interactions .
Most notably, Bahdanau et al. ﬁrst proposed to combine attention with a Recurrent Neural Network for
alignment in Machine Translation. Attention was further
extended by Vaswani et al. , where the self-attentional
Transformer architecture achieved state-of-the-art results in
Machine Translation. Using self-attention in cooperation
with convolutions is a theme shared by recent work in Natural Language Processing and Reinforcement Learning . For example, the QANet and Evolved Transformer architectures alternate between self-attention
layers and convolution layers for Question Answering applications and Machine Translation respectively.
Additionally, multiple attention mechanisms have been proposed for visual tasks to address the weaknesses of convolutions . For instance, Squeezeand-Excitation and Gather-Excite reweigh feature
channels using signals aggregated from entire feature maps,
while BAM and CBAM reﬁne convolutional features independently in the channel and spatial dimensions.
In non-local neural networks , improvements are shown
in video classiﬁcation and object detection via the additive use of a few non-local residual blocks that employ
self-attention in convolutional architectures. However, nonlocal blocks are only added to the architecture after ImageNet pretraining and are initialized in such a way that they
do not break pretraining.
In contrast, our attention augmented networks do not rely
on pretraining of their fully convolutional counterparts and
employ self-attention along the entire architecture. The use
of multi-head attention allows the model to attend jointly
to both spatial and feature subspaces. Additionally, we enhance the representational power of self-attention over images by extending relative self-attention to two dimensional inputs allowing us to model translation equivariance in a principled way. Finally our method produces additional feature maps, rather than recalibrating convolutional
features via addition or gating . This
property allows us to ﬂexibly adjust the fraction of attentional channels and consider a spectrum of architectures,
ranging from fully convolutional to fully attentional models.
3. Methods
We now formally describe our proposed Attention Augmentation method. We use the following naming conventions: H, W and Fin refer to the height, width and number
of input ﬁlters of an activation map. Nh, dv and dk respectively refer the number of heads, the depth of values and the
depth of queries and keys in multihead-attention (MHA).
We further assume that Nh divides dv and dk evenly and
k the depth of values and queries/keys per
attention head.
3.1. Self-attention over images
Given an input tensor of shape (H, W, Fin),1 we ﬂatten
it to a matrix X ∈RHW ×Fin and perform multihead attention as proposed in the Transformer architecture . The
output of the self-attention mechanism for a single head h
1We omit the batch dimension for simplicity.
can be formulated as:
Oh = Softmax
(XWq)(XWk)T
where Wq, Wk ∈RFin×dh
k and Wv ∈RFin×dh
v are learned
linear transformations that map the input X to queries Q =
XWq, keys K = XWk and values V = XWv. The outputs
of all heads are then concatenated and projected again as
MHA(X) = Concat
O1, . . . , ONh
where W O ∈Rdv×dv is a learned linear transformation.
MHA(X) is then reshaped into a tensor of shape (H, W, dv)
to match the original spatial dimensions.
We note that
multi-head attention incurs a complexity of O((HW)2dk)
and a memory cost of O((HW)2Nh) as it requires to store
attention maps for each head.
Two-dimensional Positional Embeddings
Without explicit information about positions, self-attention
is permutation equivariant:
MHA(π(X)) = π(MHA(X))
for any permutation π of the pixel locations, making it ineffective for modeling highly structured data such as images. Multiple positional encodings that augment activation
maps with explicit spatial information have been proposed
to alleviate related issues. In particular, the Image Transformer extends the sinusoidal waves ﬁrst introduced in
the original Transformer to 2 dimensional inputs and
CoordConv concatenates positional channels to an activation map.
However these encodings did not help in our experiments on image classiﬁcation and object detection (see Section 4.5). We hypothesize that this is because such positional encodings, while not permutation equivariant, do not
satisfy translation equivariance, which is a desirable property when dealing with images. As a solution, we propose
to extend the use of relative position encodings to two
dimensions and present a memory efﬁcient implementation
based on the Music Transformer .
Relative positional embeddings:
Introduced in for
the purpose of language modeling, relative self-attention
augments self-attention with relative position embeddings
and enables translation equivariance while preventing permutation equivariance. We implement two-dimensional relative self-attention by independently adding relative height
information and relative width information. The attention
logit for how much pixel i = (ix, iy) attends to pixel
j = (jx, jy) is computed as:
jx−ix + rH
where qi is the query vector for pixel i (the i-th row of Q),
kj is the key vector for pixel j (the j-th row of K) and rW
jy−iy are learned embeddings for relative width jx−ix
and relative height jy −iy, respectively. The output of head
h now becomes:
Oh = Softmax
QKT + Srel
where Srel
W ∈RHW ×HW are matrices of relative position logits along height and width dimensions that satisfy
H [i, j] = qT
jy−iy and Srel
W [i, j] = qT
The relative attention algorithm in explicitly
stores all relative embeddings rij in a tensor of shape
(HW, HW, dh
k), thus incurring an additional memory cost
of O((HW)2dh
k). This compares to O((HW)2Nh) for the
position-unaware version self-attention that does not use
position encodings. As we typically have Nh < dh
k, such an
implementation can prove extremely prohibitive and restrict
the number of images that can ﬁt in a minibatch. Instead, we
extend the memory efﬁcient relative masked attention algorithm presented in to unmasked relative self-attention
over 2 dimensional inputs. Our implementation has a memory cost of O(HWdh
k). We leave the Tensorﬂow code of
the algorithm in the Appendix.
The relative positional embeeddings rH and rW are
learned and shared across heads but not layers. For each
layer, we add (2(H + W) −2)dh
k parameters to model relative distances along height and width.
3.2. Attention Augmented Convolution
Multiple previously proposed attention mechanisms over
images suggest that the convolution operator is limited by its locality and lack of understanding
of global contexts. These methods capture long-range dependencies by recalibrating convolutional feature maps. In
particular, Squeeze-and-Excitation (SE) and Gather-
Excite (GE) perform channelwise reweighing while
BAM and CBAM reweigh both channels and
spatial positions independently.
In contrast to these approaches, we 1) use an attention mechanism that can attend
jointly to spatial and feature subspaces (each head corresponding to a feature subspace) and 2) introduce additional
feature maps rather than reﬁning them. Figure 2 summarizes our proposed augmented convolution.
Concatenating convolutional and attentional feature
Formally, consider an original convolution operator with kernel size k, Fin input ﬁlters and Fout output
ﬁlters. The corresponding attention augmented convolution
can be written as
AAConv(X) = Concat
Conv(X), MHA(X)
We denote υ =
Fout the ratio of attentional channels to
number of original output ﬁlters and κ =
Fout the ratio of
key depth to number of original output ﬁlters. Similarly to
the convolution, the proposed attention augmented convolution 1) is equivariant to translation and 2) can readily operate on inputs of different spatial dimensions. We include
Tensorﬂow code for the proposed attention augmented convolution in the Appendix A.3.
Effect on number of parameters:
Multihead attention
introduces a 1x1 convolution with Fin input ﬁlters and
(2dk+dv) = Fout(2κ+υ) output ﬁlters to compute queries,
keys and values and an additional 1x1 convolution with
dv = Foutυ input and output ﬁlters to mix the contribution of different heads. Considering the decrease in ﬁlters
in the convolutional part, this leads to the following change
in parameters:
∆params ∼FinFout(2κ + (1 −k2)υ + Fout
where we ignore the parameters introduced by relative position embeddings for simplicity as these are negligible. In
practice, this causes a slight decrease in parameters when
replacing 3x3 convolutions and a slight increase in parameters when replacing 1x1 convolutions. Interestingly, we ﬁnd
in experiments that attention augmented networks still signiﬁcantly outperform their fully convolutional counterparts
while using less parameters.
Attention Augmented Convolutional Architectures:
all our experiments, the augmented convolution is followed
by a batch normalization layer which can learn to scale
the contribution of the convolution feature maps and the attention feature maps. We apply our augmented convolution
once per residual block similarly to other visual attention
mechanisms and along the entire architecture as memory permits (see Section 4 for more details).
Since the memory cost O((Nh(HW)2) can be prohibitive for large spatial dimensions, we augment convolutions with attention starting from the last layer (with smallest spatial dimension) until we hit memory constraints. To
reduce the memory footprint of augmented networks, we
typically resort to a smaller batch size and sometimes additionally downsample the inputs to self-attention in the layers with the largest spatial dimensions where it is applied.
Downsampling is performed by applying 3x3 average pooling with stride 2 while the following upsampling (required
for the concatenation) is obtained via bilinear interpolation.
4. Experiments
In the subsequent experiments, we test Attention Augmentation on standard computer vision architectures such
as ResNets , and MnasNet on the CIFAR-
100 , ImageNet and COCO datasets. Our experiments show that Attention Augmentation leads to systematic improvements on both image classiﬁcation and object detection tasks across a broad array of architectures and
computational demands. We validate the utility of the proposed two-dimensional relative attention mechanism in ablation experiments. In all experiments, we substitute convolutional feature maps with self-attention feature maps as
it makes for an easier comparison against the baseline models. Unless speciﬁed otherwise, all results correspond to our
two-dimensional relative self-attention mechanism. Experimental details can be found in the Appendix.
4.1. CIFAR-100 image classiﬁcation
We ﬁrst investigate how Attention Augmentation performs on CIFAR-100 , a standard benchmark for lowresolution imagery, using a Wide ResNet architecture .
The Wide-ResNet-28-10 architecture is comprised of 3
stages of 4 residual blocks each using two 3 × 3 convolutions. We augment the Wide-ResNet-28-10 by augmenting
the ﬁrst convolution of all residual blocks with relative attention using Nh=8 heads and κ=2υ=0.2 and a minimum of
20 dimensions per head for the keys. We compare Attention
Augmentation (AA) against other forms of attention including Squeeze-and-Excitation (SE) and the parameterfree formulation of Gather-Excite (GE) . Table 1 shows
that Attention Augmentation improves performance both
over the baseline network and Squeeze-and-Excitation at a
similar parameter and complexity cost.
Architecture
Wide-ResNet 
GE-Wide-ResNet 
SE-Wide-ResNet 
AA-Wide-ResNet (ours)
Table 1. Image classiﬁcation on the CIFAR-100 dataset using
the Wide-ResNet 28-10 architecture .
4.2. ImageNet image classiﬁcation with ResNet
We next examine how Attention Augmentation performs
on ImageNet , a standard large-scale dataset for high
resolution imagery, across an array of architectures. We
start with the ResNet architecture because of its
widespread use and its ability to easily scale across several
computational budgets. The building block in ResNet-34
comprises two 3x3 convolutions with the same number of
output ﬁlters. ResNet-50 and its larger counterparts use a
bottleneck block comprising of 1x1, 3x3, 1x1 convolutions
where the last pointwise convolution expands the number
of ﬁlters and the ﬁrst one contracts the number of ﬁlters.
We modify all ResNets by augmenting the 3x3 convolutions as this decreases number of parameters.2 We apply
Attention Augmentation in each residual block of the last 3
stages of the architecture – when the spatial dimensions of
the activation maps are 28x28, 14x14 and 7x7 – and downsample only during the ﬁrst stage. All attention augmented
networks use κ=2υ=0.2, except for ResNet-34 which uses
κ=υ=0.25. The number of attention heads is ﬁxed to Nh=8.
Architecture
Params (M)
77.5 (77.0)
77.4 (77.4)
77.5 (77.3)
AA (υ = 0.25)
Table 2. Image classiﬁcation performance of different attention
mechanisms on the ImageNet dataset. ∆refers to the increase
in latency times compared to the ResNet50 on a single Tesla V100
GPU with Tensorﬂow using a batch size of 128. For fair comparison, we also include top-1 results (in parentheses) when scaling
networks in width to match ∼25.6M parameters as the ResNet50
Architecture
ResNet-34 
SE-ResNet-34 
AA-ResNet-34 (ours)
ResNet-50 
SE-ResNet-50 
AA-ResNet-50 (ours)
ResNet-101 
SE-ResNet-101 
AA-ResNet-101 (ours)
ResNet-152 
SE-ResNet-152 
AA-ResNet-152 (ours)
Table 3. Image classiﬁcation on the ImageNet dataset across
a range of ResNet architectures: ResNet-34, ResNet-50, Resnet-
101, and ResNet-152 .
Table 2 benchmarks Attention Augmentation against
channel and spatial attention mechanisms BAM ,
CBAM and GALA with channel reduction ratio σ = 16 on the ResNet50 architecture.
Despite the
2We found that augmenting the pointwise expansions works just as well
but does not save parameters or computations.
Architecture
MnasNet-0.75
AA-MnasNet-0.75
MnasNet-1.0
AA-MnasNet-1.0
MnasNet-1.25
AA-MnasNet-1.25
MnasNet-1.4
AA-MnasNet-1.4
Table 4. Baseline and attention augmented MnasNet accuracies with width multipliers 0.75, 1.0, 1.25 and 1.4.
lack of specialized kernels (See Appendix A.3), Attention
Augmentation offers a competitive accuracy/computational
trade-off compared to previously proposed attention mechanisms. Table 3 compares the non-augmented networks and
Squeeze-and-Excitation (SE) across different network
In all experiments, Attention Augmentation signiﬁcantly increases performance over the non-augmented
baseline and notably outperforms Squeeze-and-Excitation
(SE) while being more parameter efﬁcient (Figure 1).
Remarkably, our AA-ResNet-50 performs comparably to
the baseline ResNet-101 and our AA-ResNet-101 outperforms the baseline ResNet-152. These results suggest that
attention augmentation is preferable to simply making networks deeper. We include and discuss attention maps visualizations from different pixel positions in the appendix.
4.3. ImageNet classiﬁcation with MnasNet
In this section, we inspect the use of Attention Augmentation in a resource constrained setting by conducting
ImageNet experiments with the MnasNet architecture ,
which is an extremely parameter-efﬁcient architecture. In
particular, the MnasNet was found by neural architecture search , using only the highly optimized mobile inverted bottleneck block and the Squeeze-and-
Excitation operation as the primitives in its search
We apply Attention Augmentation to the mobile
inverted bottleneck by replacing convolutional channels in
the expansion pointwise convolution using κ=2υ=0.1 and
Nh=4 heads. Our augmented MnasNets use augmented inverted bottlenecks in the last 13 blocks out of 18 in the
MnasNet architecture, starting when the spatial dimension
is 28x28. We downsample only in the ﬁrst stage where Attention Augmentation is applied. We leave the ﬁnal pointwise convolution, also referred to as the “head”, unchanged.
In Table 4, we report ImageNet accuracies for the baseline MnasNet and its attention augmented variants at different width multipliers. Our experiments show that Attention Augmentation yields accuracy improvements across
all width multipliers. Augmenting MnasNets with relative
self-attention incurs a slight parameter increase, however
number of parameters (millions)
top-1 accuracy
AA-MnasNet (ours)
Figure 3. ImageNet top-1 accuracy as a function of number of parameters for MnasNet (black) and Attention-Augmented-MnasNet
(red) with width multipliers 0.75, 1.0, 1.25 and 1.4.
we verify in Figure 3 that the accuracy improvements are
not just explained by the parameter increase. Additionally,
we note that the MnasNet architecture employs Squeezeand-Excitation at multiple locations that were optimally selected via architecture search, further suggesting the bene-
ﬁts of our method.
4.4. Object Detection with COCO dataset
We next investigate the use of Attention Augmentation
on the task of object detection on the COCO dataset .
We employ the RetinaNet architecture with a ResNet-50
and ResNet-101 backbone as done in , using the opensourced RetinaNet codebase.3
We apply Attention Augmentation uniquely on the ResNet backbone, modifying
them similarly as in our ImageNet classiﬁcation experiments.
Our relative self-attention mechanism improves the performance of the RetinaNet on both ResNet-50 and ResNet-
101 as shown in Table 5. Most notably, Attention Augmentation yields a 1.4% mAP improvement over a strong
RetinaNet baseline from . In contrast to the success
of Squeeze-and-Excitation in image classiﬁcation with ImageNet, our experiments show that adding Squeeze-and-
Excitation operators in the backbone network of the RetinaNet signiﬁcantly hurts performance, in spite of grid
searching over the squeeze ratio σ ∈{4, 8, 16}. We hypothesize that localization requires precise spatial information which SE discards during the spatial pooling operation,
thereby negatively affecting performance. Self-attention on
the other hand maintains spatial information and is likely to
3 
models/official/retinanet
Backbone architecture
ResNet-50 
SE-ResNet-50 
AA-ResNet-50 (ours)
ResNet-101 
SE-ResNet-101 
AA-ResNet-101 (ours)
Table 5. Object detection on the COCO dataset using the RetinaNet architecture with different backbone architectures. We report
mean Average Precision at three different IoU values.
be able to identify object boundaries successfully. Visualizations of attention maps (See Figures 9 and 10 in the Appendix) reveal that some heads are indeed delineating objects from their background which might be important for
localization.
4.5. Ablation Study
Fully-attentional vision models:
In this section, we investigate the performance of Attention Augmentation as a
function of the fraction of attentional channels. As we increase this fraction to 100%, we begin to replace a ConvNet with a fully attentional model, only leaving pointwise
convolutions and the stem unchanged. Table 6 presents the
performance of Attention Augmentation on the ResNet-50
architecture for varying ratios κ=υ ∈{0.25, 0.5, 0.75, 1.0}.
Performance slightly degrades as the ratio of attentional
channels increases, which we hypothesize is partly explained by the average pooling operation for downsampling
at the ﬁrst stage where Attention Augmentation is applied.
Attention Augmentation proves however quite robust to the
fraction of attentional channels. For instance, AA-ResNet-
50 with κ=υ=0.75 outperforms its ResNet-50 counterpart,
while being more parameter and ﬂops efﬁcient, indicating
that mostly employing attentional channels is readily competitive.
Perhaps surprisingly, these experiments also reveal that
our proposed self-attention mechanism is a powerful standalone computational primitive for image classiﬁcation and
that fully attentional models are viable for discriminative visual tasks. In particular, AA-ResNet-50 with κ=υ=1, which
uses exclusively attentional channels, is only 2.5% worse
in accuracy than its fully convolutional counterpart, in spite
of downsampling with average pooling and having 25% less
parameters. Notably, this fully attentional architecture4 also
outperforms ResNet-34 while being more parameter and
ﬂops efﬁcient (see Table 6).
4We consider pointwise convolutions as dense layers. This architecture
employs 4 non-pointwise convolutions in the stem and the ﬁrst stage of the
architecture, but we believe such operations can be replaced by attention
Architecture
ResNet-34 
ResNet-50 
κ = υ = 0.25
κ = υ = 0.5
κ = υ = 0.75
κ = υ = 1.0
Table 6. Attention Augmented ResNet-50 with varying ratios of
attentional channels.
fraction of attention versus convolutional channels
top-1 accuracy
no position
with position
Figure 4. Effect of relative position embeddings as the ratio
of attentional channels increases on our Attention-Augmented
Importance of position encodings:
In Figure 4, we show
the effect of our proposed two-dimensional relative position encodings as a function of the fraction of attentional
channels. As expected, experiments demonstrate that our
relative position encodings become increasingly more important as the architecture employs more attentional channels. In particular, the fully self-attentional ResNet-50 gains
2.8% top-1 ImageNet accuracy when using relative position
encodings, which indicates the necessity of maintaining position information for fully self-attentional vision models.
We additionally compare our proposed two-dimensional
Architecture
Position Encodings
AA-ResNet-34
AA-ResNet-34
AA-ResNet-34
AA-ResNet-34
Relative (ours)
AA-ResNet-50
AA-ResNet-50
AA-ResNet-50
AA-ResNet-50
Relative (ours)
Table 7. Effects of different position encodings in Attention Augmentation on ImageNet classiﬁcation.
Position Encodings
CoordConv 
Relative (ours)
Table 8. Effects of different position encodings in Attention Augmentation on the COCO object detection task using a RetinaNet
AA-ResNet-50 backbone.
relative position encodings to other position encoding
schemes. We apply Attention Augmentation using the same
hyperparameters as 4.2 with the following different position encoding schemes: 1) The position-unaware version of
self-attention (referred to as None), 2) a two-dimensional
implementation of the sinusoidal positional waves (referred
to as 2d Sine) as used in , 3) CoordConv for which
we concatenate (x,y,r) coordinate channels to the inputs of
the attention function, and 4) our proposed two-dimensional
relative position encodings (referred to as Relative).
In Table 7 and 8, we present the results on ImageNet
classiﬁcation and the COCO object detection task respectively. On both tasks, Attention Augmentation without position encodings already yields improvements over the fully
convolutional non-augmented variants.
Our experiments
also reveal that the sinusoidal encodings and the coordinate
convolution do not provide improvements over the positionunaware version of Attention Augmentation. We obtain additional improvements when using our two-dimensional relative attention, demonstrating the utility of preserving translation equivariance while preventing permutation equivariance.
5. Discussion and future work
In this work, we consider the use of self-attention for vision models as an alternative to convolutions. We introduce
a novel two-dimensional relative self-attention mechanism
for images that enables training of competitive fully selfattentional vision models on image classiﬁcation for the ﬁrst
time. We propose to augment convolutional operators with
this self-attention mechanism and validate the superiority of
this approach over other attention schemes. Extensive experiments show that Attention Augmentation leads to systematic improvements on both image classiﬁcation and object detection tasks across a wide range of architectures and
computational settings.
Several open questions from this work remain. In future work, we will focus on the fully attentional regime
and explore how different attention mechanisms trade off
computational efﬁciency versus representational power. For
instance, identifying a local attention mechanism may result in an efﬁcient and scalable computational mechanism
that could prevent the need for downsampling with average
pooling . Additionally, it is plausible that architectural
design choices that are well suited when exclusively relying
on convolutions are suboptimal when using self-attention
mechanisms. As such, it would be interesting to see if using Attention Augmentation as a primitive in automated architecture search procedures proves useful to ﬁnd even better models than those previously found in image classiﬁcation , object detection , image segmentation and
other domains . Finally, one can ask to which
degree fully attentional models can replace convolutional
networks for visual tasks.
Acknowledgements
The authors would like to thank Tsung-Yi Lin, Prajit Ramachandran, Mingxing Tan, Yanping Huang and the
Google Brain team for insightful comments and discussions.