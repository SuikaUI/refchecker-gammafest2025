Semantic Web 1 1–5
DBpedia – A Large-scale, Multilingual
Knowledge Base Extracted from Wikipedia
Editor(s): Name Surname, University, Country
Solicited review(s): Name Surname, University, Country
Open review(s): Name Surname, University, Country
Jens Lehmann a,∗, Robert Isele g, Max Jakob e, Anja Jentzsch d, Dimitris Kontokostas a,
Pablo N. Mendes f, Sebastian Hellmann a, Mohamed Morsey a, Patrick van Kleef c, S¨oren Auer a,
Christian Bizer b
a University of Leipzig, Institute of Computer Science, AKSW Group, Augustusplatz 10, D-04009 Leipzig, Germany
E-mail: {lastname}@informatik.uni-leipzig.de
b University of Mannheim, Research Group Data and Web Science, B6-26, D-68159 Mannheim
E-mail: 
c OpenLink Software, 10 Burlington Mall Road, Suite 265, Burlington, MA 01803, U.S.A.
E-mail: 
d Hasso-Plattner-Institute for IT-Systems Engineering, Prof.-Dr.- Helmert-Str. 2-3, D-14482 Potsdam, Germany
E-mail: 
e Neofonie GmbH, Robert-Koch-Platz 4, D-10115 Berlin, Germany
E-mail: 
f Kno.e.sis - Ohio Center of Excellence in Knowledge-enabled Computing, Wright State University, Dayton, USA.
E-Mail: 
g Brox IT-Solutions GmbH, An der Breiten Wiese 9, D-30625 Hannover, Germany
E-Mail: 
Abstract. The DBpedia community project extracts structured, multilingual knowledge from Wikipedia and makes it freely
available on the Web using Semantic Web and Linked Data technologies. The project extracts knowledge from 111 different
language editions of Wikipedia. The largest DBpedia knowledge base which is extracted from the English edition of Wikipedia
consists of over 400 million facts that describe 3.7 million things. The DBpedia knowledge bases that are extracted from the other
110 Wikipedia editions together consist of 1.46 billion facts and describe 10 million additional things. The DBpedia project maps
Wikipedia infoboxes from 27 different language editions to a single shared ontology consisting of 320 classes and 1,650 properties.
The mappings are created via a world-wide crowd-sourcing effort and enable knowledge from the different Wikipedia editions to
be combined. The project publishes regular releases of all DBpedia knowledge bases for download and provides SPARQL query
access to 14 out of the 111 language editions via a global network of local DBpedia chapters. In addition to the regular releases,
the project maintains a live knowledge base which is updated whenever a page in Wikipedia changes. DBpedia sets 27 million
RDF links pointing into over 30 external data sources and thus enables data from these sources to be used together with DBpedia
data. Several hundred data sets on the Web publish RDF links pointing to DBpedia themselves and thus make DBpedia one of
the central interlinking hubs in the Linked Open Data (LOD) cloud. In this system report, we give an overview of the DBpedia
community project, including its architecture, technical implementation, maintenance, internationalisation, usage statistics and
applications.
Keywords: Knowledge Extraction, Wikipedia, Multilingual Knowledge Bases, Linked Data, RDF
1570-0844/12/$27.50 c⃝2012 – IOS Press and the authors. All rights reserved
Lehmann et al. / DBpedia
1. Introduction
Wikipedia is the 6th most popular website1, the most
widely used encyclopedia, and one of the ﬁnest examples of truly collaboratively created content. There are
ofﬁcial Wikipedia editions in 287 different languages
which range in size from a couple of hundred articles
up to 3.8 million articles (English edition)2. Besides of
free text, Wikipedia articles consist of different types of
structured data such as infoboxes, tables, lists, and categorization data. Wikipedia currently offers only freetext search capabilities to its users. Using Wikipedia
search, it is thus very difﬁcult to ﬁnd all rivers that ﬂow
into the Rhine and are longer than 100 miles, or all
Italian composers that were born in the 18th century.
The DBpedia project builds a large-scale, multilingual knowledge base by extracting structured data from
Wikipedia editions in 111 languages. This knowledge
base can be used to answer expressive queries such as
the ones outlined above. Being multilingual and covering an wide range of topics, the DBpedia knowledge
base is also useful within further application domains
such as data integration, named entity recognition, topic
detection, and document ranking.
The DBpedia knowledge base is widely used as a
testbed in the research community and numerous applications, algorithms and tools have been built around or
applied to DBpedia. DBpedia is served as Linked Data
on the Web. Since it covers a wide variety of topics
and sets RDF links pointing into various external data
sources, many Linked Data publishers have decided
to set RDF links pointing to DBpedia from their data
sets. Thus, DBpedia has developed into a central interlinking hub in the Web of Linked Data and has been
a key factor for the success of the Linked Open Data
initiative.
The structure of the DBpedia knowledge base is
maintained by the DBpedia user community. Most
importantly, the community creates mappings from
Wikipedia information representation structures to the
DBpedia ontology. This ontology – which will be explained in detail in Section 3 – uniﬁes different template structures, both within single Wikipedia language
editions and across currently 27 different languages.
*Corresponding
 .
1 Retrieved in October 2013.
2 
Wikipedias
The maintenance of different language editions of DBpedia is spread across a number of organisations. Each
organisation is responsible for the support of a certain
language. The local DBpedia chapters are coordinated
by the DBpedia Internationalisation Committee.
The aim of this system report is to provide a description of the DBpedia community project, including the
architecture of the DBpedia extraction framework, its
technical implementation, maintenance, internationalisation, usage statistics as well as presenting some popular DBpedia applications. This system report is a comprehensive update and extension of previous project descriptions in and . The main advances compared
to these articles are:
– The concept and implementation of the extraction
based on a community-curated DBpedia ontology.
– The wide internationalisation of DBpedia.
– A live synchronisation module which processes
updates in Wikipedia as well as the DBpedia ontology and allows third parties to keep their copies
of DBpedia up-to-date.
– A description of the maintenance of public DBpedia services and statistics about their usage.
– An increased number of interlinked data sets
which can be used to further enrich the content of
– The discussion and summary of novel third party
applications of DBpedia.
Overall, DBpedia has undergone 7 years of continuous evolution. Table 17 provides an overview of the
project’s timeline.
The system report is structured as follows: In the next
section, we describe the DBpedia extraction framework,
which forms the technical core of DBpedia. This is
followed by an explanation of the community-curated
DBpedia ontology with a focus on multilingual support. In Section 4, we explicate how DBpedia is synchronised with Wikipedia with just very short delays
and how updates are propagated to DBpedia mirrors
employing the DBpedia Live system. Subsequently, we
give an overview of the external data sets that are interlinked from DBpedia or that set RDF links pointing
to DBpedia themselves (Section 5). In Section 6, we
provide statistics on the usage of DBpedia and describe
the maintenance of a large scale public data set. Within
Section 7, we brieﬂy describe several use cases and
applications of DBpedia in a variety of different areas.
Finally, we report on related work in Section 8 and give
an outlook on the further development of DBpedia in
Section 9.
Lehmann et al. / DBpedia
Fig. 1. Overview of DBpedia extraction framework.
2. Extraction Framework
Wikipedia articles consist mostly of free text, but
also comprise of various types of structured information
in the form of wiki markup. Such information includes
infobox templates, categorisation information, images,
geo-coordinates, links to external web pages, disambiguation pages, redirects between pages, and links
across different language editions of Wikipedia. The
DBpedia extraction framework extracts this structured
information from Wikipedia and turns it into a rich
knowledge base. In this section, we give an overview
of the DBpedia knowledge extraction framework.
2.1. General Architecture
Figure 1 shows an overview of the technical framework. The DBpedia extraction is structured into four
Input: Wikipedia pages are read from an external
source. Pages can either be read from a Wikipedia
dump or directly fetched from a MediaWiki installation using the MediaWiki API.
Parsing: Each Wikipedia page is parsed by the wiki
parser. The wiki parser transforms the source code
of a Wikipedia page into an Abstract Syntax Tree.
Extraction: The Abstract Syntax Tree of each Wikipedia
page is forwarded to the extractors. DBpedia offers extractors for many different purposes, for instance, to extract labels, abstracts or geographical
coordinates. Each extractor consumes an Abstract
Syntax Tree and yields a set of RDF statements.
Output: The collected RDF statements are written to
a sink. Different formats, such as N-Triples, are
supported.
2.2. Extractors
The DBpedia extraction framework employs various
extractors for translating different parts of Wikipedia
pages to RDF statements. A list of all available extractors is shown in Table 1. DBpedia extractors can be
divided into four categories:
Mapping-Based Infobox Extraction: The mappingbased infobox extraction uses manually written
mappings that relate infoboxes in Wikipedia to
terms in the DBpedia ontology. The mappings also
specify a datatype for each infobox property and
thus help the extraction framework to produce high
quality data. The mapping-based extraction will
be described in detail in Section 2.4.
Raw Infobox Extraction: The raw infobox extraction
provides a direct mapping from infoboxes in
Wikipedia to RDF. As the raw infobox extraction
does not rely on explicit extraction knowledge in
the form of mappings, the quality of the extracted
data is lower. The raw infobox data is useful if a
speciﬁc infobox has not been mapped yet and thus
is not available in the mapping-based extraction.
Feature Extraction: The feature extraction uses a
number of extractors that are specialized in extracting a single feature from an article, such as a
label or geographic coordinates.
Lehmann et al. / DBpedia
Statistical Extraction: Some NLP related extractors
aggregate data from all Wikipedia pages in order to
provide data that is based on statistical measures
of page links or word counts, as further described
in Section 2.6.
2.3. Raw Infobox Extraction
The type of Wikipedia content that is most valuable
for the DBpedia extraction are infoboxes. Infoboxes are
frequently used to list an article’s most relevant facts
as a table of attribute-value pairs on the top right-hand
side of the Wikipedia page (for right-to-left languages
on the top left-hand side). Infoboxes that appear in a
Wikipedia article are based on a template that speciﬁes
a list of attributes that can form the infobox. A wide
range of infobox templates are used in Wikipedia. Common examples are templates for infoboxes that describe
persons, organisations or automobiles. As Wikipedia’s
infobox template system has evolved over time, different communities of Wikipedia editors use different templates to describe the same type of things (e.g.
Infobox city japan,
Infobox swiss town
and Infobox town de). In addition, different templates use different names for the same attribute
(e.g. birthplace and placeofbirth). As many
Wikipedia editors do not strictly follow the recommendations given on the page that describes a template,
attribute values are expressed using a wide range of
different formats and units of measurement. An excerpt
of an infobox that is based on a template for describing
automobiles is shown below:
{{Infobox automobile
= Ford GT40
| manufacturer = [[Ford Advanced Vehicles]]
| production
= 1964-1969
In this infobox, the ﬁrst line speciﬁes the infobox
type and the subsequent lines specify various attributes
of the described entity.
An excerpt of the extracted data is as follows:
dbr:Ford_GT40
dbp:name "Ford GT40"@en;
dbp:manufacturer dbr:Ford_Advanced_Vehicles;
dbp:engine 4181;
dbp:production 1964;
This extraction output has weaknesses: The resource
is not associated to a class in the ontology and parsed
values are cleaned up and assigned a datatyper based
on heuristics. In particular, the raw infobox extractor
searches for values in the following order: dates, coordinates, numbers, links and strings as default. Thus, the
datatype assignment for the same property in different
resources is non deterministic. The engine for example is extracted as a number but if another instance of
the template used “cc4181” it would be extracted as
string. This behaviour makes querying for properties in
the dbp namespace inconsistent. Those problems can
be overcome by the mapping-based infobox extraction
presented in the next subsection.
2.4. Mapping-Based Infobox Extraction
In order to homogenize the description of information in the knowledge base, in 2010 a community effort was initiated to develop an ontology schema and
mappings from Wikipedia infobox properties to this
ontology. The alignment between Wikipedia infoboxes
and the ontology is performed via community-provided
mappings that help to normalize name variations in
properties and classes. Heterogeneity in the Wikipedia
infobox system, like using different infoboxes for the
same type of entity or using different property names for
the same property (cf. Section 2.3), can be alleviated in
this way. This signiﬁcantly increases the quality of the
raw Wikipedia infobox data by typing resources, merging name variations and assigning speciﬁc datatypes to
the values.
This effort is realized using the DBpedia Mappings
Wiki3, a MediaWiki installation set up to enable users
to collaboratively create and edit mappings. These mappings are speciﬁed using the DBpedia Mapping Language. The mapping language makes use of MediaWiki
templates that deﬁne DBpedia ontology classes and
properties as well as template/table to ontology mappings. A mapping assigns a type from the DBpedia ontology to the entities that are described by the corresponding infobox. In addition, attributes in the infobox
are mapped to properties in the DBpedia ontology. In
the following, we show a mapping that maps infoboxes
that use the Infobox automobile template to the
DBpedia ontology:
{{TemplateMapping
|mapToClass = Automobile
|mappings =
{{PropertyMapping
| templateProperty = name
3 
Lehmann et al. / DBpedia
Overview of the DBpedia extractors (cf. Table 16 for a complete list
of preﬁxes.).
Description
Extracts the ﬁrst lines of the
Wikipedia article.
dbr:Berlin dbo:abstract "Berlin is the capital city of
article categories
Extracts the categorization of the
dbr:Oliver Twist dc:subject dbr:Category:English novels .
category label
Extracts labels for categories.
dbr:Category:English novels rdfs:label "English novels" .
category hierarchy
Extracts information about which
concept is a category and how categories are related using the SKOS
Vocabulary.
dbr:Category:World War II skos:broader
dbr:Category:Modern history .
disambiguation
Extracts disambiguation links.
dbr:Alien dbo:wikiPageDisambiguates dbr:Alien (film) .
external links
Extracts links to external web
pages related to the concept.
dbr:Animal Farm dbo:wikiPageExternalLink
< > .
geo coordinates
Extracts geo-coordinates.
dbr:Berlin georss:point "52.5006 13.3989" .
grammatical gender
Extracts grammatical genders for
dbr:Abraham Lincoln foaf:gender "male" .
Extracts links to the ofﬁcial homepage of an instance.
dbr:Alabama foaf:homepage < > .
Extracts the ﬁrst image of a
Wikipedia page.
dbr:Berlin foaf:depiction < Berlin.jpg> .
Extracts all properties from all infoboxes.
dbr:Animal Farm dbo:date "March 2010" .
interlanguage
Extracts interwiki links.
dbr:Albedo dbo:wikiPageInterLanguageLink dbr-de:Albedo .
Extracts the article title as label.
dbr:Berlin rdfs:label "Berlin" .
lexicalizations
Extracts information about surface
forms and their association with
concepts (only N-Quad format).
dbr:Pine sptl:lexicalization lx:pine tree ls:Pine pine tree .
lx:pine tree rdfs:label "pine tree" .
ls:Pine pine tree sptl:pUriGivenSf "0.941" .
Extraction based on mappings of
Wikipedia infoboxes to the DBpedia ontology.
dbr:Berlin dbo:country dbr:Germany .
Extracts page ids of articles.
dbr:Autism dbo:wikiPageID "25" .
page links
Wikipedia articles.
dbr:Autism dbo:wikiPageWikiLink dbr:Human brain .
persondata
Extracts information about persons represented using the Person-
Data template.
dbr:Andre Agassi foaf:birthDate "1970-04-29" .
Extracts PND (Personennamendatei) data about a person.
dbr:William Shakespeare dbo:individualisedPnd "118613723" .
Extracts redirect links between articles in Wikipedia.
dbr:ArtificialLanguages dbo:wikiPageRedirects
dbr:Constructed language .
revision ID
Extracts the revision ID of the
Wikipedia article.
dbr:Autism < >
< > .
thematic concept
Extracts ‘thematic’ concepts, the
centres of discussion for categories.
dbr:Category:Music skos:subject dbr:Music .
topic signatures
Extracts topic signatures.
dbr:Alkane sptl:topicSignature "carbon alkanes atoms" .
Extracts links to corresponding articles in Wikipedia.
dbr:AnAmericanInParis foaf:isPrimaryTopicOf
< > .
Lehmann et al. / DBpedia
| ontologyProperty = foaf:name }}
{{PropertyMapping
| templateProperty = manufacturer
| ontologyProperty = manufacturer }}
{{DateIntervalMapping
| templateProperty = production
| startDateOntologyProperty =
productionStartDate
| endDateOntologyProperty =
productionEndDate }}
{{IntermediateNodeMapping
| nodeClass = AutomobileEngine
| correspondingProperty = engine
| mappings =
{{PropertyMapping
| templateProperty = engine
| ontologyProperty = displacement
| unit = Volume }}
{{PropertyMapping
| templateProperty = engine
| ontologyProperty = powerOutput
| unit = Power }}
The RDF statements that are extracted from the previous infobox example are shown below. As we can see,
the production period is correctly split into a start year
and an end year and the engine is represented by a distinct RDF node. It is worth mentioning that all values
are canonicalized to basic units. For example, in the
engine mapping we state that engine is a Volume
and thus, the extractor converts “4181cc” (cubic centimeters) to cubic meters (“0.004181”). Additionally,
there can exist multiple mappings on the same property
that search for different datatypes or different units. For
example, a number with “PS” as a sufﬁx for engine.
dbr:Ford_GT40
dbo:Automobile;
rdfs:label "Ford GT40"@en;
dbo:manufacturer
dbr:Ford_Advanced_Vehicles;
dbo:productionStartYear
"1964"ˆˆxsd:gYear;
dbo:productionEndYear "1969"ˆˆxsd:gYear;
dbo:engine [
rdf:type AutomobileEngine;
dbo:displacement "0.004181";
The DBpedia Mapping Wiki is not only used to map
different templates within a single language edition
of Wikipedia to the DBpedia ontology, but is used to
map templates from all Wikipedia language editions
to the shared DBpedia ontology. Figure 2 shows how
the infobox properties author and συγγϱαϕϵας – author in Greek – are both being mapped to the global
identiﬁer dbo:author. That means, in turn, that information from all language versions of DBpedia can
be merged and DBpedias for smaller languages can be
augmented with knowledge from larger DBpedias such
as the English edition. Conversely, the larger DBpedia editions can beneﬁt from more specialized knowledge from localized editions, such as data about smaller
towns which is often only present in the corresponding
language edition .
Besides hosting of the mappings and DBpedia ontology deﬁnition, the DBpedia Mappings Wiki offers
various tools which support users in their work:
– Mapping Syntax Validator The mapping syntax
validator checks for syntactic correctness and highlights inconsistencies such as missing property
deﬁnitions.
– Extraction Tester The extraction tester linked on
each mapping page tests a mapping against a set
of example Wikipedia pages. This gives direct
feedback about whether a mapping works and how
the resulting data is structured.
– Mapping Tool The DBpedia Mapping Tool is a
graphical user interface that supports users to create and edit mappings.
2.5. URI Schemes
For every Wikipedia article, the framework introduces a number of URIs to represent the concepts described on a particular page. Up to 2011, DBpedia published URIs only under the 
domain. The main namespaces were:
– (preﬁx
dbr) for representing article data. There is a oneto-one mapping between a Wikipedia page and a
DBpedia resource based on the article title. For
example, for the Wikipedia article on Berlin4, DBpedia will produce the URI dbr:Berlin. Exceptions
in this rule appear when intermediate nodes are
extracted from the mapping-based infobox extractor as unique URIs (e.g., the engine mapping
example in Section 2.4).
– (preﬁx
dbp) for representing properties extracted from
the raw infobox extraction (cf. Section 2.3), e.g.
dbp:population.
4 
Lehmann et al. / DBpedia
Fig. 2. Depiction of the mapping from the Greek and English Wikipedia templates about books to the same DBpedia Ontology class .
– (preﬁx
dbo) for representing the DBpedia ontology (cf.
Section 2.4), e.g. dbo:populationTotal.
Although data from other Wikipedia language editions were extracted, they used the same namespaces.
This was achieved by exploiting the Wikipedia interlanguage links5. For every page in a language other
than English, the page was extracted only if the page
contained an inter-language link to an English page. In
that case, using the English link, the data was extracted
under the English resource name (i.e. dbr:Berlin).
Recent DBpedia internationalisation developments
showed that this approach omitted valuable data .
Thus, starting from the DBpedia 3.7 release6, two types
of data sets were generated. The localized data sets contain all things that are described in a speciﬁc language.
Within the datasets, things are identiﬁed with language
speciﬁc URIs such as http://<lang>.dbpedia.
org/resource/ for article data and http://
<lang>.dbpedia.org/property/ for property data. In addition, we produce a canonicalized data
set for each language. The canonicalized data sets only
contain things for which a corresponding page in the
English edition of Wikipedia exists. Within all canoni-
5 
Interlanguage_links
6A list of all DBpedia releases is provided in Table 17
calized data sets, the same thing is identiﬁed with the
same URI from the generic language-agnostic namespace 
2.6. NLP Extraction
DBpedia provides a number of data sets which have
been created to support Natural Language Processing
(NLP) tasks . Currently, four datasets are extracted:
topic signatures, grammatical gender, lexicalizations
and thematic concept. While the topic signatures and
the grammatical gender extractors primarily extract data
from the article text, the lexicalizations and thematic
concept extractors make use of the wiki markup.
DBpedia entities can be referred to using many different names and abbreviations. The Lexicalization data
set provides access to alternative names for entities
and concepts, associated with several scores estimating
the association strength between name and URI. These
scores distinguish more common names for speciﬁc
entities from rarely used ones and also show how ambiguous a name is with respect to all possible concepts
that it can mean.
The topic signatures data set enables the description
of DBpedia resources based on unstructured information, as compared to the structured factual data provided
by the mapping-based and raw extractors. We build a
Vector Space Model (VSM) where each DBpedia resource is a point in a multidimensional space of words.
Lehmann et al. / DBpedia
Each DBpedia resource is represented by a vector, and
each word occurring in Wikipedia is a dimension of
this vector. Word scores are computed using the tf-idf
weight, with the intention to measure how strong is the
association between a word and a DBpedia resource.
Note that word stems are used in this context in order to
generalize over inﬂected words. We use the computed
weights to select the strongest related word stems for
each entity and build topic signatures .
There are two more Feature Extractors related to Natural Language Processing. The thematic concepts data
set relies on Wikipedia’s category system to capture the
idea of a ‘theme’, a subject that is discussed in its articles. Many of the categories in Wikipedia are linked to
an article that describes the main topic of that category.
We rely on this information to mark DBpedia entities
and concepts that are ‘thematic’, that is, they are the
centre of discussion for a category.
The grammatical gender data set uses a simple heuristic to decide on a grammatical gender for instances of
the class Person in DBpedia. While parsing an article
in the English Wikipedia, if there is a mapping from
an infobox in this article to the class dbo:Person,
we record the frequency of gender-speciﬁc pronouns in
their declined forms (Subject, Object, Possessive Adjective, Possessive Pronoun and Reﬂexive) – i.e. he,
him, his, himself (masculine) and she, her, hers, herself
(feminine). Grammatical genders for DBpedia entities
are assigned based on the dominating gender in these
2.7. Summary of Other Recent Developments
In this section we summarize the improvements of
the DBpedia extraction framework since the publication of the previous DBpedia overview article in
2009. One of the major changes on the implementation
level is that the extraction framework has been rewritten in Scala in 20107 to improve the efﬁciency of the
extractors by an order of magnitude compared to the
previous PHP based framework. The new more modular framework also allows to extract data from tables
in Wikipedia pages and supports extraction from multiple MediaWiki templates per page. Another signiﬁcant
change was the creation and utilization of the DBpedia
Mappings Wiki as described earlier. Further signiﬁcant
changes include the mentioned NLP extractors and the
introduction of URI schemes.
7Table 17 provides an overview of the project’s evolution through
In addition, there were several smaller improvements
and general maintenance: Overall, over the past four
years, the parsing of the MediaWiki markup improved
quite a lot which led to better overall coverage, for
example, concerning references and parser functions.
In addition, the collection of MediaWiki namespace
identiﬁers for many languages is now performed semiautomatically leading to a high accuracy of detection.
This concerns common title preﬁxes such as User, File,
Template, Help, Portal etc. in English that indicate
pages that do not contain encyclopedic content and
would produce noise in the data. They are important for
speciﬁc extractors as well, for instance, the category hierarchy data set is produced from pages of the Category
namespace. Furthermore, the output of the extraction
system now supports more formats and several compliance issues regarding URIs, IRIs, N-Triples and Turtle
were ﬁxed.
The individual data extractors have been improved as
well in both number and quality in many areas. The abstract extraction was enhanced producing more accurate
plain text representations of the beginning of Wikipedia
article texts. More diverse and more speciﬁc datatypes
do exist (e.g. many currencies and XSD datatypes such
as xsd:gYearMonth, xsd:positiveInteger,
etc.) and for a number of classes and properties, speciﬁc
datatypes were added (e.g. inhabitants/km2 for the population density of populated places and m3/s for the
discharge of rivers). Many issues related to data parsers
were resolved and the quality of the owl:sameAs
data set for multiple language versions was increased
by an implementation that takes bijective relations into
There are also further extractors, e.g. for Wikipedia
page IDs and revisions. Moreover, redirect and disambiguation extractors were introduced and improved. For
the redirect data, the transitive closure is computed
while taking care of catching cycles in the links. The
redirects also help regarding infobox coverage in the
mapping-based extraction by resolving alternative template names. Moreover, in the PHP framework, if an
infobox value pointed to a redirect, this redirection was
not properly resolved and thus resulted in RDF links
that led to URIs which did not contain any further information. Resolving redirects affected approximately
15% of all links, and hence increased the overall interconnectivity of resources in the DBpedia ontology.
Finally, a new heuristic to increase the connectiveness of DBpedia instances was introduced. If an infobox
contains a string value that is not linked to another
Wikipedia article, the extraction framework searches
Lehmann et al. / DBpedia
rdf:type owl:Class
rdf:type owl:DatatypeProperty
rdf:type owl:ObjectProperty
dbo:PopulatedPlace
dbo:Species
dbo:Settlement
dbo:Person
dbo:Athlete
dbo:Eukaryote
dbo:Organisation
dbo:producer
dbo:writer
dbo:birthPlace
dbo:family
P dbo:conservationStatus
dbo:releaseDate
dbo:runtime
dbo:birthDate
dbo:deathDate
dbo:areaTotal
dbo:elevation
dbo:utcOffset
dbo:populationTotal
dbo:areaCode
rdfs:subClassOf
rdfs:subClassOf
rdfs:subClassOf
rdfs:subClassOf
rdfs:subClassOf
rdfs:subClassOf
rdfs:subClassOf
rdfs:subClassOf
rdfs:domain
rdfs:domain
rdfs:domain
rdfs:domain
rdfs:domain
rdfs:domain
rdfs:domain
rdfs:domain
rdfs:domain
rdfs:domain
dbo:subsequentWork
dbo:location
dbo:canton
rdfs:subClassOf
rdfs:subClassOf
Fig. 3. Snapshot of a part of the DBpedia ontology.
for hyperlinks in the same Wikipedia article that have
the same anchor text as the infobox value string. If such
a link exists, the target of that link is used to replace
the string value in the infobox. This method further
increases the number of object property assertions in
the DBpedia ontology.
Orthogonal to the previously mentioned improvements, there have been various efforts to assess the quality of the DBpedia datasets. developed a framework for estimating the quality of DBpedia and a sample of 75 resources were analysed. A more comprehensive effort was performed in by providing a
distributed web-based interface for quality assessment. In this study, 17 data quality problem types were
analysed by 58 users covering 521 resources in DBpedia.
3. DBpedia Ontology
The DBpedia ontology consists of 320 classes which
form a subsumption hierarchy and are described by
1,650 different properties. With a maximal depth of
5, the subsumption hierarchy is intentionally kept
rather shallow which ﬁts use cases in which the ontology is visualized or navigated. Figure 3 depicts
a part of the DBpedia ontology, indicating the relations among the top ten classes of the DBpedia ontology, i.e. the classes with the highest number of
instances. The complete DBpedia ontology can be
browsed online at 
org/server/ontology/classes/.
DBpedia Version
Number of ontology elements
Properties
Fig. 4. Growth of the DBpedia ontology.
The DBpedia ontology is maintained and extended
by the community in the DBpedia Mappings Wiki. Figure 4 depicts the growth of the DBpedia ontology over
time. While the number of classes is not growing too
much due to the already good coverage of the initial
version of the ontology, the number of properties increases over time due to the collaboration on the DBpedia Mappings Wiki and the addition of more detailed
information to infoboxes by Wikipedia editors.
3.1. Mapping Statistics
As of April 2013, there exist mapping communities
for 27 languages, 23 of which are active. Figure 5 shows
statistics for the coverage of these mappings in DBpedia. Figures (a) and (c) refer to the absolute number
of template and property mappings that are deﬁned for
every DBpedia language edition. Figures (b) and (d) de-
Lehmann et al. / DBpedia
Fig. 5. Mapping coverage statistics for all mapping-enabled languages.
pict the percentage of the deﬁned template and property
mappings compared to the total number of available
templates and properties for every Wikipedia language
edition. Figures (e) and (g) show the occurrences (instances) that the deﬁned template and property mappings have in Wikipedia. Finally, ﬁgures (f) and (h)
give the percentage of the mapped templates and properties occurences, compared to the total templates and
property occurences in a Wikipedia language edition.
It can be observed in the ﬁgure that the Portuguese
DBpedia language edition is the most complete regarding mapping coverage. Other language editions such
as Bulgarian, Dutch, English, Greek, Polish and Spanish have mapped templates covering more than 50%
of total template occurrences. In addition, almost all
languages have covered more than 20% of property occurrences, with Bulgarian and Portuguese reaching up
The mapping activity of the ontology enrichment
process along with the editing of the ten most active
mapping language communities is depicted in Figure 6.
It is interesting to notice that the high mapping activity peaks coincide with the DBpedia release dates.
For instance, the DBpedia 3.7 version was released on
September 2011 and the 2nd and 3rd quarter of that
year have a very high activity compared to the 4th quarter. In the last two years , most of the
DBpedia mapping language communities have deﬁned
Lehmann et al. / DBpedia
Fig. 6. Mapping community activity for (a) ontology and (b) 10 most active language editions
Fig. 7. English property mappings occurrence frequency (both axes
are in log scale)
their own chapters and have their own release dates.
Thus, recent mapping activity shows less ﬂuctuation.
Finally, Figure 7 shows the English property mappings occurrence frequency. Both axes are in log scale
and represent the number of property mappings (x axis)
that have exactly y occurrences (y axis). The occurrence
frequency follows a long tail distribution. Thus, a low
number of property mappings have a high number of
occurrences and a high number of property mappings
have a low number of occurences.
3.2. Instance Data
The DBpedia 3.8 release contains localized versions
of DBpedia for 111 languages which have been extracted from the Wikipedia edition in the corresponding language. For 20 of these languages, we report in
this section the overall number of entities being described by the localized versions as well as the number of facts (i.e. statements) that have been extracted
from infoboxes describing these things. Afterwards, we
report on the number of instances of popular classes
within the 20 DBpedia versions as well as the conceptual overlap between the languages.
Table 2 shows the overall number of things, ontology and raw-infobox properties, infobox statements and
type statements for the 20 languages. The column headings have the following meaning: LD = Localized data
sets (see Section 2.5); CD = Canonicalized data sets
(see Section 2.5); all = Overall number of instances in
the data set, including instances without infobox data;
with MD = Number of instances for which mappingbased infobox data exists; Raw Properties = Number
of different properties that are generated by the raw
infobox extractor; Mapping Properties = Number of
different properties that are generated by the mappingbased infobox extractor; Raw Statements = Number of
statements (facts) that are generated by the raw infobox
extractor; Mapping Statements = Number of statements
(facts) that are generated by the mapping-based infobox
extractor.
It is interesting to see that the English version of DBpedia describes about three times more instances than
the second and third largest language editions (French,
German). Comparing the ﬁrst column of the table with
the second and third reveals which portion of the instances of a speciﬁc language correspond to instances
in the English version of DBpedia and which portion
of the instances is described by clean, mapping-based
infobox data. The difference between the number of
properties in the raw infobox data set and the cleaner
mapping-based infobox data set (columns 4 and 5) results on the one hand from multiple Wikipedia infobox
properties being mapped to a single ontology property.
On the other hand, it reﬂects the number of mappings
that have been so far created in the Mapping Wiki for a
speciﬁc language.
Table 3 reports the number of instances for a set of
popular classes from the third and forth hierarchy level
Lehmann et al. / DBpedia
Basic statistics about Localized DBpedia Editions.
Inst. LD all
Inst. CD all
Inst. with MD CD
Raw Prop. CD
Map. Prop. CD
Raw Statem. CD
Map. Statem. CD
65,143,840
33,742,015
12,227,870
of the ontology within the canonicalized DBpedia data
sets for each language. The indented classes are subclasses of the superclasses set in bold. The zero values in the table indicate that no infoboxes have been
mapped to a speciﬁc ontology class within the corresponding language so far. Again, the English version of
DBpedia covers by far the most instances.
Table 4 shows, for the canonicalized, mapping-based
data set, how many instances are described in multiple languages. The Instances column contains the total
number of instances per class across all 20 languages,
the second column contains the number of instances
that are described only in a single language version, the
next column contains the number of instances that are
contained in two languages but not in three or more languages, etc. For example, 12,936 persons are described
in ﬁve languages but not in six or more languages. The
number 871,630 for the class Person means that all
20 language versions together describe 871,630 different persons. The number is higher than the number of
persons described in the canonicalized English infobox
data set (763,643) listed in Table 3, since there are
infoboxes in non-English articles describing a person
without a corresponding infobox in the English article
describing the same person. Summing up columns 2 to
10+ for the Person class, we see that 195,263 persons
are described in two or more languages. The large difference of this number compared to the total number of
871,630 persons is due to the much smaller size of the
localized DBpedia versions compared to the English
one (cf. Table 2).
3.3. Internationalisation Community
The introduction of the mapping-based infobox
extractor alongside live synchronisation approaches
in allowed the international DBpedia community
to easily deﬁne infobox-to-ontology mappings. As a
result of this development, there are presently mappings
for 27 languages8. The DBpedia 3.7 release9 in September 2011 was the ﬁrst DBpedia release to use the localized I18n (Internationalisation) DBpedia extraction
framework .
At the time of writing, DBpedia chapters for 14 languages have been founded: Basque, Czech, Dutch, English, French, German, Greek, Italian, Japanese, Korean, Polish, Portuguese, Russian and Spanish.10 Be-
8Arabic (ar), Bulgarian (bg), Bengali (bn), Catalan (ca), Czech
(cs), German (de), Greek (el), English (en), Spanish (es), Estonian
(et), Basque (eu), French (fr), Irish (ga), Hindi (hi), Croatian (hr),
Hungarian (hu), Indonesian (id), Italian (it), Japanese (ja), Korean
(ko), Dutch (nl), Polish (pl), Portuguese (pt), Russian (ru), Slovene
(sl), Turkish (tr), Urdu (ur)
9 
10Accessed on 25/09/2013: 
Internationalization/Chapters
Lehmann et al. / DBpedia
Number of instances per class within 10 localized DBpedia versions.
Politician
Popul.Place
Organisation
Educ.Inst.
Music.Work
Cross-language overlap: Number of instances that are described in multiple languages.
Organisation
sides providing mappings from infoboxes in the corresponding Wikipedia editions, DBpedia chapters organise a local community and provide hosting for data sets
and associated services.
While at the moment chapters are deﬁned by ownership of the IP and server of the sub domain A record
(e.g. given by DBpedia maintainers, the DBpedia internationalisation committee11 is manifesting its structure and each language
edition has a representative with a vote in elections. In
some cases (e.g. Greek12 and Dutch13) the existence of
a local DBpedia chapter has had a positive effect on the
creation of localized LOD clouds .
In the weeks leading to a new release, the DBpedia project organises a mapping sprint, where communities from each language work together to improve
mappings, increase coverage and detect bugs in the extraction process. The progress of the mapping effort
11 
Internationalization
12 
13 
is tracked through statistics on the number of mapped
templates and properties, as well as the number of
times these templates and properties occur in Wikipedia.
These statistics provide an estimate of the coverage of
each Wikipedia edition in terms of how many entities
will be typed and how many properties from those entities will be extracted. Therefore, they can be used
by each language edition to prioritize properties and
templates with higher impact on the coverage.
The mapping statistics have also been used as a way
to promote a healthy competition between language
editions. A sprint page was created with bar charts that
show how close each language is from achieving total coverage (as shown in Figure 5), and line charts
showing the progress over time highlighting when one
language is overtaking another in their race for higher
coverage. The mapping sprints have served as a great
motivator for the crowd-sourcing efforts, as it can be
noted from the increase in the number of mapping contributions in the weeks leading to a release.
Lehmann et al. / DBpedia
4. Live Synchronisation
Wikipedia articles are continuously revised at a very
high rate, e.g. the English Wikipedia, in June 2013,
has approximately 3.3 million edits per month which
is equal to 77 edits per minute14. This high change
frequency leads to DBpedia data quickly being outdated, which in turn leads to the need for a methodology
to keep DBpedia in synchronisation with Wikipedia.
As a consequence, the DBpedia Live system was developed, which works on a continuous stream of updates from Wikipedia and processes that stream on the
ﬂy . It allows extracted data to stay up-to-date
with a small delay of at most a few minutes. Since the
English Wikipedia is the largest among all Wikipedia
editions with respect to the number of articles and the
number of edits per month, it was the ﬁrst language
DBpedia Live supported15. Meanwhile, DBpedia Live
for Dutch16 was developed.
4.1. DBpedia Live System Architecture
In order for live synchronisation to be possible, we
need access to the changes made in Wikipedia. The
Wikimedia foundation kindly provided us access to
their update stream using the OAI-PMH protocol .
This protocol allows a programme to pull page updates
in XML via HTTP. A Java component, serving as a
proxy, constantly retrieves new updates and feeds them
to the DBpedia Live framework. This proxy is necessary to decouple the stream from the framework to
simplify maintenance of the software. The live extraction workﬂow uses this update stream to extract new
knowledge upon relevant changes in Wikipedia articles.
The overall architecture of DBpedia Live is indicated
in Figure 8. The major components of the system are
as follows:
– Local Wikipedia Mirror: A local copy of a
Wikipedia language edition is installed which is
kept in real-time synchronisation with its live version using the OAI-PMH protocol. Keeping a local
Wikipedia mirror allows us to exceed any access
limits posed by Wikipedia.
– Mappings Wiki: The DBpedia Mappings Wiki,
described in Section 2.4, serves as secondary input
14 
15 
16 
Fig. 8. Overview of DBpedia Live extraction framework.
source. Changes of the mappings wiki are also
consumed via an OAI-PMH stream. Note that a
single mapping change can affect a high number
of DBpedia resources.
– DBpedia Live Extraction Manager: This is the
core component of the DBpedia Live extraction
architecture. The manager takes feeds of pages for
re-processing as input and applies all the enabled
extractors. After processing a page, the extracted
triples are a) inserted into a backend triple store
(in our case Virtuoso ), updating the old triples
and b) saved as changesets into a compressed N-
Triples ﬁle structure.
– Synchronisation Tool: This tool allows third parties to keep DBpedia Live mirrors up-to-date by
harvesting the produced changesets.
4.2. Features of DBpedia Live
The core components of the DBpedia Live Extraction
framework provide the following features:
– Mapping-Affected Pages: The update of all pages
that are affected by a mapping change.
– Unmodiﬁed Pages: The update of unmodiﬁed
pages at regular intervals.
– Changesets Publication: The publication of triplechangesets.
– Synchronisation Tool: A synchronisation tool for
harvesting updates to DBpedia Live mirrors.
– Data Isolation: Separate data from different
Mapping-Affected Pages:
Whenever an infobox mapping change occurs, all the Wikipedia pages that use
that infobox are reprocessed. Taking Figure 2 as an
example, if a new property mapping is introduced (i.e.
dbo:translator) or an existing (i.e. dbo:illustrator) is
updated or deleted, then all entities belonging to the
class dbo:Book are reprocessed. Thus, upon a mapping
change, we identify all the affected Wikipedia pages
and feed them for reprocessing.
Lehmann et al. / DBpedia
Unmodiﬁed Pages:
Extraction framework improvements or activation / deactivation of DBpedia extractors
might never be applied to rarely modiﬁed pages. To
overcome this problem, we obtain a list of the pages
which have not been processed over a period of time
(30 days in our case) and feed that list to the DBpedia
Live extraction framework for reprocessing. This feed
has a lower priority than the update or the mapping
affected pages feed and ensures that all articles reﬂect a
recent state of the output of the extraction framework.
Publication of Changesets:
Whenever a Wikipedia
article is processed, we get two disjoint sets of triples. A
set for the added triples, and another set for the deleted
triples. We write those two sets into N-Triples ﬁles,
compress them, and publish the compressed ﬁles as
changesets. If another DBpedia Live mirror wants to
synchronise with the DBpedia Live endpoint, it can just
download those ﬁles, decompress and integrate them.
Synchronisation Tool:
The synchronisation tool enables a DBpedia Live mirror to stay in synchronisation
with our live endpoint. It downloads the changeset ﬁles
sequentially, decompresses them and updates the target
SPARQL endpoint via insert and delete operations.
Data Isolation:
In order to keep the data isolated,
DBpedia Live keeps different sources of data in
different SPARQL graphs. Data from the article
update feeds are contained in the graph with the
URI static data (i.e.
links to the LOD cloud) are kept in 
dbpedia.org and the DBpedia ontology is stored
in All data is
also accessible under the 
graph for combined queries. Next versions of DBpedia Live will also separate data from the raw infobox
extraction and mapping-based infobox extraction.
5. Interlinking
DBpedia is interlinked with numerous external data
sets following the Linked Data principles. In this section, we give an overview of the number and types
of outgoing links that point from DBpedia into other
data sets, as well as the external data sets that set links
pointing to DBpedia resources.
5.1. Outgoing Links
Similar to the DBpedia ontology, DBpedia also follows a community approach for adding links to other
third party data sets. The DBpedia project maintains
a link repository17 for which conventions for adding
linksets and linkset metadata are deﬁned. The adherence to those guidelines is supervised by a linking committee. Linksets which are added to the repository are
used for the subsequent ofﬁcial DBpedia release as well
as for DBpedia Live. Table 5 lists the linksets created
by the DBpedia community as of April 2013. The ﬁrst
column names the data set that is the target of the links.
The second and third column contain the predicate that
is used for linking as well as the overall number of links
that is set between DBpedia and the external data set.
The last column names the tool that was used to generate the links. The value S refers to Silk, L to LIMES,
C to custom script and a missing entry means that the
dataset is copied from the previous releases and not
regenerated.
An example for the usage of links is the combination of data about European Union project funding
(FTS) and data about countries in DBpedia. The
query below compares funding per year (from FTS) and
country with the gross domestic product of a country
(from DBpedia)18 .
SELECT * { {
SELECT ?ftsyear ?ftscountry (SUM(?amount) AS
?com rdf:type fts-o:Commitment .
?com fts-o:year ?year .
?year rdfs:label ?ftsyear .
?com fts-o:benefit ?benefit .
?benefit fts-o:detailAmount ?amount .
?benefit fts-o:beneficiary ?beneficiary .
?beneficiary fts-o:country ?country .
?country owl:sameAs ?ftscountry .
SELECT ?dbpcountry ?gdpyear ?gdpnominal {
?dbpcountry rdf:type dbo:Country .
?dbpcountry dbp:gdpNominal ?gdpnominal .
?dbpcountry dbp:gdpNominalYear ?gdpyear .
FILTER ((?ftsyear = str(?gdpyear)) &&
(?ftscountry = ?dbpcountry)) }
In addition to providing outgoing links on an
instance-level, DBpedia also sets links on schemalevel pointing from the DBpedia ontology to equivalent terms in other schemas. Links to other schemata
can be set by the community within the DBpedia Mappings Wiki by using owl:equivalentClass in
class templates and owl:equivalentProperty
in datatype or object property templates, respectively.
In particular, in 2011 Google, Microsoft, and Yahoo!
announced their collaboration on Schema.org, a col-
17 
18Endpoint: 
Results: 
Lehmann et al. / DBpedia
Data sets linked from DBpedia
Amsterdam Museum
owl:sameAs
BBC Wildlife Finder
owl:sameAs
Book Mashup
owl:sameAs
dc:publisher
owl:sameAs
owl:sameAs
DBLP Bibliography
owl:sameAs
owl:sameAs
owl:sameAs
owl:sameAs
owl:sameAs
Eurostat (Linked Stats)
owl:sameAs
Eurostat (WBSG)
owl:sameAs
CIA World Factbook
owl:sameAs
ﬂickr wrappr
dbp:hasPhoto-
Collection
owl:sameAs
owl:sameAs
owl:sameAs
GeoSpecies
owl:sameAs
owl:sameAs
Project Gutenberg
owl:sameAs
Italian Public Schools
owl:sameAs
LinkedGeoData
owl:sameAs
owl:sameAs
MusicBrainz
owl:sameAs
New York Times
owl:sameAs
owl:sameAs
OpenEI (Open Energy)
owl:sameAs
owl:sameAs
owl:sameAs
TCMGeneDIT
owl:sameAs
owl:sameAs
WikiCompany
owl:sameAs
dbp:wordnet type
18 100 000
27 211 732
lection of vocabularies for marking up content on web
pages. The DBpedia 3.8 ontology contains 45 equivalent class and 31 equivalent property links pointing to
 terms.
Top 10 data sets in Sindice ordered by the number of links to DBpedia.
Link Predicate Count
Link Count
okaboo.com
tfri.gov.tw
naplesplus.us
fu-berlin.de
freebase.com
geonames.org
opencyc.org
geospecies.org
faviki.com
5.2. Incoming Links
DBpedia is being linked to from a variety of data
sets. The overall number of links pointing to DBpedia
from other data sets is 39,007,478 according to the Data
Hub.19 However, those counts are entered by users and
may not always be valid and up-to-date.
In order to identify actually published and online
data sets that link to DBpedia, we used Sindice .
The Sindice project crawls RDF resources on the web
and indexes those resources. In Sindice, a data set is
deﬁned by the second-level domain name of the entity’s URI, e.g. all resources available at the domain
fu-berlin.de are considered to belong to the same
data set. A triple is considered to be a link if the data
set of subject and object are different. Furthermore, the
Sindice data we used for analysis only considers authoritative entities: The data set of a subject of a triple
must match the domain it was retrieved from, otherwise
it is not considered. Sindice computes a graph summary over all resources they store. With the help
of the Sindice team, we examined this graph summary
to obtain all links pointing to DBpedia. As shown in
Table 7, Sindice knows about 248 data sets linking to
DBpedia. 70 of those data sets link to DBpedia via
owl:sameAs, but other link predicates are also very
common as evident in this table. In total, Sindice has
indexed 4 million links pointing to DBpedia. Table 6
lists the 10 data sets which set most links to DBpedia
along with the used link predicate and the number of
It should be noted that the data in Sindice is not complete, for instance it does not contain all data sets that
19See for
Lehmann et al. / DBpedia
Sindice summary statistics for incoming links to DBpedia.
Total links:
Total distinct data sets:
Total distinct predicates:
Top 10 datasets by incoming links in Sindice.
dbpedia.org
creativecommons.org
rkbexplorer.com
nytimes.com
geospecies.org
livejournal.com
are catalogued by the DataHub20. However, it crawls for
RDFa snippets, converts microformats etc., which are
not captured by the DataHub. Despite the inaccuracy,
the relative comparison of different datasets can still
give us insights. Therefore, we analysed the link structure of all Sindice datasets using the Sindice cluster.
Table 8 shows the datasets with most incoming links.
Those are authorities in the network structure of the
web of data and DBpedia is currently ranked second in
terms of incoming links.
6. DBpedia Usage Statistics
DBpedia is served on the web in three forms: First,
it is provided in the form of downloadable data sets
where each data set contains the results of one of the
extractors listed in Table 1. Second, DBpedia is served
via a public SPARQL endpoint and, third, it provides
dereferencable URIs according to the Linked Data principles. In this section, we explore some of the statistics
gathered during the hosting of DBpedia over the last
two of years.
6.1. Download Statistics for the DBpedia Data Sets
DBpedia covers more than 100 languages, but those
languages vary with respect to the download popular-
20 
Download Count (in Thousands)
Download Volume (in TB)
Fig. 9. The download count and download volume (in GB) of the
English language of DBpedia.
ity as well. The top ﬁve languages with respect to the
download volume are English, Chinese, German, Catalan, and French respectively. The download count and
download volume of the English language is indicated
in Figure 9. To host the DBpedia dataset downloads,
a bandwidth of approximately 6 TB per month is currently needed.
Furthermore, DBpedia consists of several data sets
which vary with respect to their download popularity.
The download count and the download volume of each
data set during the year 2012 is depicted in Figure 10.
In those statistics we ﬁltered out all IP addresses, which
requested a ﬁle more than 1000 times per month.21
Pagelinks are the most downloaded dataset, although
they are not semantically rich as they do not reveal
which type of links exists between two resources. Supposedly, they are used for network analysis or providing
relevant links in user interfaces and downloaded more
often as they are not provided via the ofﬁcial SPARQL
6.2. Public Static DBpedia SPARQL Endpoint
The main public DBpedia SPARQL endpoint22 is
hosted using the Virtuoso Universal Server (Enterprise
Edition) version 6.4 software in a 4-nodes cluster con-
ﬁguration. This cluster setup provides parallelization
of query execution, even when the cluster nodes are
on the same machine, as splitting a query over several
nodes allows better use of parallel threads on modern
multi-core CPUs on standard commodity hardware.
Virtuoso supports horizontal scale-out, either by redistributing the existing cluster nodes onto multiple machines, or by adding several separate clusters with a
round robin HTTP front-end. This allows the cluster
setup to grow in line with desired response times for
an RDF data set collection. As the size of the DBpedia
21The IP address was only ﬁltered for that speciﬁc ﬁle and month
in those cases.
22 
Lehmann et al. / DBpedia
Download Count (in Thousands)
Download Volume (in TB)
Fig. 10. The download count and download volume (in GB) of the DBpedia data sets.
Hardware of the machines serving the public SPARQL endpoint.
Conﬁguration
AMD Opteron 8220 2.80Ghz, 4 Cores, 32GB
Intel Xeon E5520 2.27Ghz, 8 Cores, 48GB
Intel Xeon E5-2630 2.30GHz, 8 Cores, 64GB
data set increased and its use by the Linked Data community grew, the project migrated to increasingly powerful, but still moderately priced, hardware as shown in
Table 9. The Virtuoso instance is conﬁgured to process
queries within a 1,200 second timeout window and a
maximum result set size of 50,000 rows. It provides
OFFSET and LIMIT support for paging alongside the
ability to produce partial results.
The log ﬁles used in the following analysis excluded
trafﬁc generated by:
1. clients that have been temporarily rate limited
after a burst period,
2. clients that have been banned after misuse,
3. applications, spiders and other crawlers that are
blocked after frequently hitting the rate limit or
generally use too many resources.
Virtuoso supports HTTP Access Control Lists
(ACLs) which allow the administrator to rate limit certain IP addresses or whole IP ranges. A maximum number of requests per second (currently 15) as well as
a bandwidth limit per request (currently 10MB) are
enforced. If the client software can handle compression, replies are compressed to further save bandwidth.
Exception rules can be conﬁgured for multiple clients
hidden behind a NAT ﬁrewall (appearing as a single
IP address) or for temporary requests for higher rate
Number of unique sites accessing DBpedia endpoints.
limits. When a client hits an ACL limit, the system
reports an appropriate HTTP status code23 like 509 and
quickly drops the connection. The system further uses
an iptables based ﬁrewall for permanent blocking of
clients identiﬁed by their IP addresses.
6.3. Public Static Endpoint Statistics
The statistics presented in this section were extracted
from reports generated by Webalizer v2.2124. Table 10
and Table 11 show various DBpedia SPARQL endpoint
usage statistics for the DBpedia 3.3 to 3.8 releases. Note
that the usage of all endpoints mentioned in Table 12 is
counted. The Avg/Day column represents the average
number of hits (resp. visits/sites) per day, followed by
the Median and Standard Deviation. The last column
shows the maximum number of hits (resp. visits/sites)
that was recorded on a single day for each data set
version. Visits (i.e. sessions of subsequent queries from
the same client) are determined by a ﬂoating 30 minute
23 
status_codes
24 
Lehmann et al. / DBpedia
Number of endpoint hits (left) and visits (right).
Hits per service to in thousands.
time window. All requests from behind a NAT ﬁrewall
are logged under the same external IP address and are
therefore counted towards the same visit if they occur
within the 30 minute interval.
Table 10 shows the increasing popularity of DBpedia.
There is a distinct dip in hits to the SPARQL endpoint in
DBpedia 3.5, which is partially due to more strict initial
limits for bot-related trafﬁc which were later relaxed.
The sudden drop of visits between the 3.7 and the 3.8
data sets can be attributed to:
1. applications starting to use their own private
DBpedia endpoint
2. blocking of apps that were abusing the DBpedia
3. uptake of the language speciﬁc DBpedia endpoints and DBpedia Live
6.4. Query Types and Trends
The DBpedia server is not only a SPARQL endpoint,
but also serves as a Linked Data Hub returning resources in a number of different formats. For each data
set we randomly selected 14 days worth of log ﬁles and
processed those in order to show the various services
called. Table 12 shows the number of hits to the various
endpoints.
The /resource endpoint uses the Accept: line in the
HTTP header sent by the client to return a HTTP sta-
Fig. 11. Trafﬁc Linked Data versus SPARQL endpoint
tus code 30x to redirect the client to either the /page
(HTML based) or /data (formats like RDF/XML or Turtle) equivalent of the article. Clients also frequently
mint their own URLs to either /page or /data version of
an articles directly, or download the raw data directly.
This explains why the count of /page and /data hits
in the table is larger than the number of hits on the
/resource endpoint. The /ontology and /property endpoints return meta information about the DBpedia ontology. While all of these endpoints themselves may
use SPARQL queries to generate various page content,
these requests are handled by the internal Virtuoso engine directly and do not show up as extra calls to the
/sparql endpoint in our analysis.
Figure 11 shows the percentages of trafﬁc hits that
were generated by the main endpoints. As we can see,
the usage of the SPARQL endpoint has doubled from
about 22 percent in 2009 to about 44 percent in 2013.
However, this still means that 56 percent of trafﬁc hits
are directed to the Linked Data service.
In Table 13, we focussed on the calls to the /sparql
endpoint and counted the number of statements per type.
As the log ﬁles only record the full SPARQL query on
a GET request, all the POST requests are counted as
Finally, we analyzed each SPARQL query and
counted the use of keywords and constructs like:
Lehmann et al. / DBpedia
Hits per statement type in thousands.
Trends in SPARQL select (rounded values in %).
– DISTINCT
– FUNCTIONS like CONCAT, CONTAINS, ISIRI
– Use of GEO objects
– GROUP BY
– LIMIT / OFFSET
– OPTIONAL
– ORDER BY
For the GEO objects we counted the use of SPARQL
PREFIX geo: and wgs84*: declarations and usage in
property tags. Table 14 shows the use of various keywords as a percentage of the total select queries made
to the /sparql endpoint for the sample sets. In general,
we observed that queries became more complex over
time indicating an increasing maturity and higher expectations of the user base.
6.5. Statistics for DBpedia Live
Since its ofﬁcial release at the end of June 2011,
DBpedia Live attracted a steadily increasing number
of users. Furthermore, more users tend to use the synchronisation tool to synchronise their own DBpedia
Live mirrors. This leads to an increasing number of live
update requests, i.e. changeset downloads. Figure 12
indicates the number of daily SPARQL and synchronisation requests sent to DBpedia Live endpoint in the
period between August 2012 and January 2013.
Fig. 12. Number of daily requests sent to the DBpedia Live for a)
SPARQL queries and b) synchronisation requests from August 2012
until January 2013
7. Use Cases and Applications
Due to DBpedia’s coverage of various domains as
well as its steady growth as a hub on the Web of Data,
the data sets provided by DBpedia can serve many purposes. Such applications include improving search and
exploration of Wikipedia, data proliferation for applications, mashups as well as text analysis and annotation
7.1. Natural Language Processing
DBpedia can support many tasks in Natural Language Processing (NLP) . For that purpose, DBpedia
includes a number of specialized data sets25. For instance, the lexicalizations data set can be used to estimate the ambiguity of phrases, to help select unambiguous identiﬁers for ambiguous phrases, or to provide
alternative names for entities, just to mention a few examples. Topic signatures can be useful in tasks such as
query expansion or document summarization, and has
been successfully employed to classify ambiguously
described images as good depictions of DBpedia entities . The thematic concepts data set of resources
can be used for creating a corpus from Wikipedia to be
used as training data for topic classiﬁers, among other
25 
Lehmann et al. / DBpedia
things (see below). The grammatical gender data set
can, for example, be used to add a gender feature in
co-reference resolution.
7.1.1. Annotation: Entity Disambiguation
An important use case for NLP is annotating texts
or other content with semantic information. Named
entity recognition and disambiguation – also known as
key phrase extraction and entity linking tasks – refers
to the task of ﬁnding real world entities in text and
linking them to unique identiﬁers. One of the main
challenges in this regard is ambiguity: an entity name,
or surface form, may be used in different contexts to
refer to different concepts. Many different methods
have been developed to resolve this ambiguity with
fairly high accuracy .
As DBpedia reﬂects a vast amount of structured real
world knowledge obtained from Wikipedia, DBpedia
URIs can be used as identiﬁers for the majority of domains in text annotation. Consequently, interlinking text
documents with Linked Data enables the Web of Data
to be used as background knowledge within documentoriented applications such as semantic search or faceted
browsing (cf. Section 7.3).
Many applications performing this task of annotating
text with entities in fact use DBpedia entities as targets.
For example, DBpedia Spotlight is an open source
tool26 including a free web service that detects mentions of DBpedia resources in text. It uses the lexicalizations in conjunction with the topic signatures data
set as context model in order to be able to disambiguate
found mentions. The main advantage of this system is
its comprehensiveness and ﬂexibility, allowing one to
conﬁgure it based on quality measures such as prominence, contextual ambiguity, topical pertinence and disambiguation conﬁdence, as well as the DBpedia ontology. The resources that should be annotated can be
speciﬁed by a list of resource types or by more complex
relationships within the knowledge base described as
SPARQL queries.
There are numerous other NLP APIs that link entities in text to DBpedia: AlchemyAPI27, Semantic API
from Ontos28, Open Calais29 and Zemanta30 among others. Furthermore, the DBpedia ontology has been used
for training named entity recognition systems (without
26 
27 
28 
29 
30 
disambiguation) in the context of the Apache Stanbol
project31.
A related project is ImageSnippets32, which is a system for annotating images. It uses DBpedia as one of
its main datasets for unambiguously identifying entities
depicted within an image.
Tag disambiguation
Similar to linking entities in text
to DBpedia, user-generated tags attached to multimedia
content such as music, photos or videos can also be connected to the Linked Data hub. This has previously been
implemented by letting the user resolve ambiguities.
For example, Faviki33 suggests a set of DBpedia entities
coming from Zemanta’s API and lets the user choose
the desired one. Alternatively, similar disambiguation
techniques as mentioned above can be utilized to choose
entities from tags automatically . The BBC34 
employs DBpedia URIs for tagging their programmes.
Short clips and full episodes are tagged using two different tools while utilizing DBpedia to beneﬁt from
global identiﬁers that can be easily integrated with other
knowledge bases.
7.1.2. Question Answering
DBpedia provides a wealth of human knowledge
across different domains and languages, which makes
it an excellent target for question answering and keyword search approaches. One of the most prominent
efforts in this area is the DeepQA project, which resulted in the IBM Watson system . The Watson
system won a $1 million prize in Jeopardy and relies
on several data sets including DBpedia35. DBpedia is
also the primary target for several QA systems in the
Question Answering over Linked Data (QALD) workshop series36. Several QA systems, such as TBSL ,
PowerAqua , FREyA and QAKiS have been
applied to DBpedia using the QALD benchmark questions. DBpedia is interesting as a test case for such
systems. Due to its large schema and data size as well
as its topic diversity, it provides signiﬁcant scientiﬁc
challenges. In particular, it would be difﬁcult to provide
capable QA systems for DBpedia based only on simple
patterns or via domain speciﬁc dictionaries, because of
its size and broad coverage. Therefore, a question an-
31 
32 
33 
34 
35 
watson.php
36 
Lehmann et al. / DBpedia
swering system, which is able to reliable answer questions over DBpedia correctly, could be seen as a truly
intelligent system. In the latest QALD series, question
answering benchmarks also exploit national DBpedia
chapters for multilingual question answering.
Similarly, the slot ﬁlling task in natural language
processing poses the challenge of ﬁnding values for a
given entity and property from mining text. This can
be viewed as question answering with static questions
but changing targets. DBpedia can be exploited for fact
validation or training data in this task, as was done by
the Watson team and others .
7.2. Digital Libraries and Archives
In the case of libraries and archives, DBpedia could
offer a broad range of information on a broad range of
domains. In particular, DBpedia could provide:
– Context information for bibliographic and archive
records: Background information such as an author’s demographics, a ﬁlm’s homepage or an image could be used to enhance user interaction.
– Stable and curated identiﬁers for linking: DBpedia
is a hub of Linked Open Data. Thus, (re-)using
commonly used identiﬁers could ease integration
with other libraries or knowledge bases.
– A basis for a thesaurus for subject indexing: The
broad range of Wikipedia topics in addition to
the stable URIs could form the basis for a global
classiﬁcation system.
Libraries have already invested both in Linked Data
and Wikipedia (and transitively to DBpedia) though
the realization of the Virtual International Authority
Files (VIAF) project.37 Recently, it was announced that
VIAF added a total of 250,000 reciprocal authority
links to Wikipedia.38 These links are already harvested
by DBpedia Live and will also be included in the next
static DBpedia release. This creates a huge opportunity
for libraries that use VIAF to get connected to DBpedia
and the LOD cloud in general.
7.3. Knowledge Exploration
Since DBpedia spans many domains and has a diverse schema, many knowledge exploration tools either
used DBpedia as a testbed or were speciﬁcally built
37 
38Accessed
12/02/2013:
 
research/news/2012/12-07a.html
for DBpedia. We give a brief overview of tools and
structure them in categories:
Facet Based Browsers
An award-winning39 facetbased browser used the Neofonie search engine to combine facts in DBpedia with full-text from Wikipedia in
order to compute hierarchical facets . Another facet
based browser, which allows to create complex graph
structures of facets in a visually appealing interface and
ﬁlter them is gFacet . A generic SPARQL based
facet explorer, which also uses a graph based visualisation of facets, is LODLive . The OpenLink built-in
facet based browser40 is an interface, which enables
developers to explore DBpedia, compute aggregations
over facets and view the underlying SPARQL queries.
Search and Querying
The DBpedia Query Builder41
allows developers to easily create simple SPARQL
queries, more speciﬁcally sets of triple patterns via intelligent autocompletion. The autocompletion functionality ensures that only URIs, which lead to solutions are
suggested to the user. The RelFinder tool provides
an intuitive interface, which allows to explore the neighborhood and connections between resources speciﬁed
by the user. For instance, the user can view the shortest paths connecting certain persons in DBpedia. Sem-
Lens allows to create statistical analysis queries
and correlations in RDF data and DBpedia in particular.
Spatial Applications
DBpedia Mobile is a location
aware client, which renders a map of nearby locations
from DBpedia, provides icons for schema classes and
supports more than 30 languages from various DBpedia
language editions. It can follow RDF links to other
data sets linked from DBpedia and supports powerful
SPARQL ﬁlters to restrict the viewed data.
7.4. Applications of the Extraction Framework:
Wiktionary Extraction
Wiktionary is one of the biggest collaboratively created lexical-semantic and linguistic resources, available in 171 languages (of which approximately 147 can
be considered active42). It contains information about
hundreds of spoken and even ancient languages. In the
39 
40 
41 
42 
Lehmann et al. / DBpedia
case of the English Wiktionary there are nearly 3 million detailed descriptions of words covering several domains43. Such descriptions provide, for a lexical word,
a hierarchical disambiguation to its language, part of
speech, sometimes etymologies, synonyms, hyponyms,
hyperonyms, example sentences, and most prominently
Due to its fast changing nature, together with the
fragmentation of the project into Wiktionary language
editions (WLE) with independent layout rules a, conﬁgurable mediator/wrapper approach is taken for its automated transformation into a structured knowledge base.
The workﬂow of this dedicated Wiktionary extractor
being part of the Wiktionary2RDF project is as
follows: For every WLE to be transformed an XML
conﬁguration ﬁle is provided as input. This conﬁguration is used by the Wiktionary extractor, invoked by
the DBpedia extraction framework, to ﬁrst generate a
schema reﬂecting the conﬁgured page structure (wrapper part). After this, these language speciﬁc schemas
are converted to a global schema (mediator part) and
later serialized to RDF.
To enable non-programmers (the community of
adopters and domain experts) to tailor and maintain
the WLE wrappers themselves, a simple XML dialect
was created to encode the page structure to be parsed
and declare triple patterns, that deﬁne how the resulting
RDF should be built. The described setup is run against
Wiktionary dumps. The resulting data set is open in
every aspect and hosted as Linked Data.44 Statistics are
shown in Table 15.
8. Related Work
8.1. Cross Domain Community Knowledge Bases
8.1.1. Wikidata
In March 2012, the Wikimedia Germany e.V. started
the development of Wikidata45. Wikidata is a free
knowledge base about the world that can be read and
edited by humans and machines alike. It provides data
in all languages of the Wikimedia projects, and allows
for central access to the data in a similar vein as Wikimedia Commons does for multimedia ﬁles. Things described in the Wikidata knowledge base are called items
 
for a simple example page
44 
45 
and can have labels, descriptions and aliases in all languages. Wikidata does not aim at offering a single truth
about things, but providing statements given in a particular context. Rather than stating that Berlin has a
population of 3.5 million, Wikidata contains the statement about Berlin’s population being 3.5 million as of
2011 according to the German statistical ofﬁce. Thus,
Wikidata can offer a variety of statements from different sources and dates. As there are potentially many
different statements for a given item and property, ranks
can be added to statements to deﬁne their status (preferred, normal or deprecated). The initial development
was divided in three phases:
– The ﬁrst phase (interwiki links) created an entity
base for the Wikimedia projects. This provides
a better alternative to the previous interlanguage
link system.
– The second phase (infoboxes) gathered infoboxrelated data for a subset of the entities, with the
explicit goal of augmenting the infoboxes that are
currently widely used with data from Wikidata.
– The third phase (lists) will expand the set of properties beyond those related to infoboxes, and will
provide ways of exploiting this data within and
outside the Wikimedia projects.
At the time of writing of this article, the development
of the third phase is ongoing.
Wikidata already contains 11.95 million items and
348 properties that can be used to describe them. Since
March 2013 the Wikidata extension is live on all
Wikipedia language editions and thus their pages can
be linked to items in Wikidata and include data from
Wikidata also offers a Linked Data interface46 as
well as regular RDF dumps of all its data. The planned
collaboration with Wikidata is outlined in Section 9.
8.1.2. Freebase
Freebase47 is a graph database, which also extracts
structured data from Wikipedia and makes it available
in RDF. Both DBpedia and Freebase link to each other
and provide identiﬁers based on those for Wikipedia
articles. They both provide dumps of the extracted data,
as well as APIs or endpoints to access the data and
allow their communities to inﬂuence the schema of the
data. There are, however, also major differences be-
46 
Development/LinkedDataInterface
47 
Lehmann et al. / DBpedia
Statistical comparison of extractions for different languages.
#resources
#predicates
28,593,364
11,804,039
35,032,121
20,462,349
12,813,437
tween both projects. DBpedia focuses on being an RDF
representation of Wikipedia and serving as a hub on the
Web of Data, whereas Freebase uses several sources to
provide broad coverage. The store behind Freebase is
the GraphD graph database, which allows to efﬁciently store metadata for each fact. This graph store is
append-only. Deleted triples are marked and the system
can easily revert to a previous version. This is necessary, since Freebase data can be directly edited by users,
whereas information in DBpedia can only indirectly be
edited by modifying the content of Wikipedia or the
Mappings Wiki. From an organisational point of view,
Freebase is mainly run by Google, whereas DBpedia is
an open community project. In particular in focus areas
of Google and areas in which Freebase includes other
data sources, the Freebase database provides a higher
coverage than DBpedia.
8.1.3. YAGO
One of the projects that pursues similar goals
to DBpedia is YAGO48 . YAGO is identical to
DBpedia in that each article in Wikipedia becomes an
entity in YAGO. Based on this, it uses the leaf categories in the Wikipedia category graph to infer type
information about an entity. One of its key features is to
link this type information to WordNet. WordNet synsets
are represented as classes and the extracted types of
entities may become subclasses of such a synset. In the
YAGO2 system , declarative extraction rules were
introduced, which can extract facts from different parts
of Wikipedia articles, e.g. infoboxes and categories, as
well as other sources. YAGO2 also supports spatial and
temporal dimensions for facts at the core of its system.
One of the main differences between DBpedia and
YAGO in general is that DBpedia tries to stay very
close to Wikipedia and provide an RDF version of its
content. YAGO focuses on extracting a smaller number
of relations compared to DBpedia to achieve very high
precision and consistent knowledge. The two knowledge bases offer different type systems: whereas the
DBpedia ontology is manually maintained, YAGO is
48 
backed by WordNet and Wikipedia leaf categories.
Due to this, YAGO contains many more classes than
DBpedia. Another difference is that the integration of
attributes and objects in infoboxes is done via mappings
in DBpedia and, therefore, by the DBpedia community
itself, whereas this task is facilitated by expert-designed
declarative rules in YAGO2.
The two knowledge bases are connected, e.g. DBpedia
offers the YAGO type hierarchy as an alternative to
the DBpedia ontology and sameAs links are provided
in both directions. While the underlying systems are
very different, both projects share similar aims and
positively complement and inﬂuence each other.
8.2. Knowledge Extraction from Wikipedia
Since its ofﬁcial start in 2001, Wikipedia has always
been the target of automatic extraction of information
due to its easy availability, open license and encyclopedic knowledge. A large number of parsers, scraper
projects and publications exist. In this section, we restrict ourselves to approaches that are either notable, recent or pertinent to DBpedia. MediaWiki.org maintains
an up-to-date list of software projects49, who are able to
process wiki syntax, as well as a list of data extraction
extensions50 for MediaWiki.
JWPL (Java Wikipedia Library, ) is an opensource, Java-based API that allows to access information provided by the Wikipedia API (redirects, categories, articles and link structure). JWPL contains a
MediaWiki Markup parser that can be used to further
analyze the contents of a Wikipedia page. Data is also
provided as XML dump and is incorporated in the lexical resource UBY51 for language tools.
Several different approaches to extract knowledge
from Wikipedia are presented in . Given features
49 
50 
Matrix/data_extraction
51 
lexical-resources/uby/
Lehmann et al. / DBpedia
like anchor texts, interlanguage links, category links
and redirect pages are utilized e.g. for word-sense disambiguations or synonyms, translations, taxonomic relations and abbreviation or hypernym resolution, respectively. Apart from this, link structures are used to
build the Wikipedia Thesaurus Web service52. Additional projects that exploit the mentioned features are
listed on the Special Interest Group on Wikipedia Mining (SIGWP) Web site53.
An earlier approach to improve the quality of the
infobox schemata and contents is described in .
The presented methodology encompasses a three step
process of preprocessing, classiﬁcation and extraction.
During preprocessing reﬁned target infobox schemata
are created applying statistical methods and training
sets are extracted based on real Wikipedia data. After
assigning a class and the corresponding target schema
(classiﬁcation) the training sets are used to extract target infobox values from the document’s text applying
machine learning algorithms.
The idea of using structured data from certain
markup structures was also applied to other user-driven
Web encyclopedias. In the authors describe their effort building an integrated Chinese Linking Open Data
(CLOD) source based on the Chinese Wikipedia and
the two widely used and large encyclopedias Baidu
Baike54 and Hudong Baike55. Apart from utilizing MediaWiki and HTML Markup for the actual extraction,
the Wikipedia interlanguage links were used to link the
CLOD source to the English DBpedia.
A more generic approach to achieve a better crosslingual knowledge-linkage beyond the use of Wikipedia
interlanguage links is presented in . Focusing on
wiki knowledge bases the authors introduce their solution based on structural properties like similar linkage
structures, the assignment to similar categories and similar interests of the authors of wiki documents in the
considered languages. Since this approach is languagefeature-agnostic it is not restricted to certain languages.
KnowItAll56 is a web scale knowledge extraction
effort, which is domain-independent, and uses generic
extraction rules, co-occurrence statistics and Naive
Bayes classiﬁcation . Cyc is a large com-
52 
53 
54 
55 
56 
knowitall/
mon sense knowledge base, which is now partially
released as OpenCyc and also available as an OWL
ontology. OpenCyc is linked to DBpedia, which provides an ontological embedding in its comprehensive
structures. WikiTaxonomy is a large taxonomy derived from categories in Wikipedia by classifying categories as instances or classes and deriving a subsumption hierarchy. The KOG system reﬁnes existing
Wikipedia infoboxes based on machine learning techniques using both SVMs and a more powerful jointinference approach expressed in Markov Logic Networks. KYLIN is a system which autonomously
extracts structured data from Wikipedia and uses selfsupervised linking. Auer et al. introduced an infobox extraction approach for Wikipedia, which later
became the DBpedia project.
9. Conclusions and Future Work
In this system report, we presented an overview on
recent advances of the DBpedia community project.
The technical innovations described in this article included in particular: (1) the extraction based on the
community-curated DBpedia ontology, (2) the live synchronisation of DBpedia with Wikipedia and DBpedia
mirrors through update propagation, and (3) the facilitation of the internationalisation of DBpedia. As a result,
we demonstrated that in the past four years DBpedia
matured and improved signiﬁcantly in terms of coverage, usability, and data quality.
With DBpedia, we also aim to provide a proofof-concept and blueprint for the feasibility of largescale knowledge extraction from crowd-sourced content repositories. There are a large number of further
crowd-sourced content repositories and DBpedia already had an impact on their structured data publishing
and interlinking. Two examples are Wiktionary with
the Wiktionary extraction meanwhile becoming
part of DBpedia and LinkedGeoData , which aims
to implement similar data extraction, publishing and
linking strategies for OpenStreetMaps.
In the future, we see in particular the following directions for advancing the DBpedia project:
Multilingual data integration and fusion. An area,
which is still largely unexplored is the integration and
fusion between different DBpedia language editions.
Non-English DBpedia editions comprise a better and
different coverage of local culture. When we are able to
precisely identify equivalent, overlapping and complementary parts in different DBpedia language editions,
Lehmann et al. / DBpedia
we can reach signiﬁcantly increased coverage. On the
other hand, comparing the values of a speciﬁc property between different language editions will help us
to spot extraction errors as well as wrong or outdated
information in Wikipedia.
Community-driven data quality improvement. In the
future, we also aim to engage a larger community of
DBpedia users in feedback loops, which help us to
identify data quality problems and corresponding deﬁciencies of the DBpedia extraction framework. By constantly monitoring the data quality and integrating improvements into the mappings to the DBpedia ontology
as well as ﬁxes into the extraction framework, we aim to
demonstrate that the Wikipedia community is not only
capable of creating the largest encyclopedia, but also
the most comprehensive and structured knowledge base.
With the DBpedia quality evaluation campaign we
were making a ﬁrst step in this direction.
Inline extraction. Currently DBpedia extracts information primarily from templates. In the future, we
envision to also extract semantic information from
typed links. Typed links is a feature of Semantic MediaWiki, which was backported and implemented as
a very lightweight extension for MediaWiki57. If this
extension is deployed at Wikipedia installations, this
opens up completely new possibilities for more ﬁnegrained and non-invasive knowledge representations
and extraction from Wikipedia.
Collaboration between Wikidata and DBpedia.
While DBpedia provides a comprehensive and current
view on entity descriptions extracted from Wikipedia,
Wikidata offers a variety of factual statements from
different sources and dates. One of the richest sources
of DBpedia are Wikipedia infoboxes, which are structured but at the same time heterogeneous and nonstandardized (thus making the extraction error prone in
certain cases). The aim of Wikidata is to populate infoboxes automatically from a centrally managed, highquality fact database. In this regard, both projects complement each other and there are several ongoing collaboration activities. In future versions, DBpedia will
include more raw data provided by Wikidata and add
services such as Linked Data/SPARQL endpoints, RDF
dumps, linking and ontology mapping for Wikidata.
Feedback for Wikipedia. A promising prospect is that
DBpedia can help to identify misrepresentations, errors
and inconsistencies in Wikipedia. In the future, we plan
57 
LightweightRDFa
to provide more feedback to the Wikipedia community
about the quality of Wikipedia. This can, for instance,
be achieved in the form of sanity checks, which are
implemented as SPARQL queries on the DBpedia Live
endpoint, which identify data quality issues and are executed in certain intervals. For example, a query could
check that the birthday of a person must always be before the death day or spot outliers that differ signiﬁcantly from the range of the majority of the other values. In case a Wikipedia editor makes a mistake or typo
when adding such information to a page, this could be
automatically identiﬁed and provided as feedback to
Wikipedians.
Integrate DBpedia and NLP. Despite recent advances
(cf. Section 7), there is still a huge potential for employing Linked Data background knowledge in various Natural Language Processing (NLP) tasks. One
very promising research avenue in this regard is to
employ DBpedia as structured background knowledge
for named entity recognition and disambiguation. Currently, most approaches use statistical information such
as co-occurrence for named entity disambiguation.
However, co-occurrence is not always easy to determine (depends on training data) and update (requires recomputation). With DBpedia and in particular DBpedia
Live, we have comprehensive and evolving background
knowledge comprising information on the relationship
between a large number of real-world entities. Consequently, we can employ this information for deciding
to what entity a certain surface form should be mapped.
Acknowledgment
We would like to thank and acknowledge the support
of the following people and organisations to DBpedia:
– all Wikipedia contributors
– all DBpedia Mappings Wiki contributors
– OpenLink Software for providing and maintaining
the server infrastructure for the main DBpedia
– Kingsley Idehen for SPARQL and Linked Data
hosting and community support
– Christopher Sahnwaldt for DBpedia development
and release management
– Claus Stadler for DBpedia development
– Paul Kreis for DBpedia development
– people who helped contributing data for certain
parts of the article:
∗Instance Data Analysis: Volha Bryl (working at
University of Mannheim)
Lehmann et al. / DBpedia
∗Sindice analysis: Stphane Campinas, Szymon
Danielczyk, and Gabriela Vulcu (working at
∗Freebase: Shawn Simister, and Tom Morris
(working at Freebase)
This work was supported by grants from the European Union’s 7th Framework Programme provided for
the projects LOD2 (GA no. 257943), GeoKnow (GA
no. 318159) and Dicode (GA no. 257184).
List of namespace preﬁxes.
 
 
 
 
 
 
 pos#