From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic
Review on Evaluating Explainable AI
MEIKE NAUTA, University of Twente, the Netherlands and University of Duisburg-Essen, Germany
JAN TRIENES, University of Duisburg-Essen, Germany
SHREYASI PATHAK, University of Twente, the Netherlands and University of Duisburg-Essen, Germany
ELISA NGUYEN and MICHELLE PETERS, University of Twente, the Netherlands
YASMIN SCHMITT and JÖRG SCHLÖTTERER, University of Duisburg-Essen, Germany
MAURICE VAN KEULEN, University of Twente, the Netherlands
CHRISTIN SEIFERT, University of Duisburg-Essen, Germany
The rising popularity of explainable artificial intelligence (XAI) to understand high-performing black boxes raised the question of how
to evaluate explanations of machine learning (ML) models. While interpretability and explainability are often presented as a subjectively
validated binary property, we consider it a multi-faceted concept. We identify 12 conceptual properties, such as Compactness and
Correctness, that should be evaluated for comprehensively assessing the quality of an explanation. Our so-called Co-12 properties
serve as categorization scheme for systematically reviewing the evaluation practices of more than 300 papers published in the last 7
years at major AI and ML conferences that introduce an XAI method. We find that 1 in 3 papers evaluate exclusively with anecdotal
evidence, and 1 in 5 papers evaluate with users. This survey also contributes to the call for objective, quantifiable evaluation methods
by presenting an extensive overview of quantitative XAI evaluation methods. Our systematic collection of evaluation methods provides
researchers and practitioners with concrete tools to thoroughly validate, benchmark and compare new and existing XAI methods.
The Co-12 categorization scheme and our identified evaluation methods open up opportunities to include quantitative metrics as
optimization criteria during model training in order to optimize for accuracy and interpretability simultaneously.
CCS Concepts: • Computing methodologies →Machine learning; Artificial intelligence; • General and reference →Evaluation.
Additional Key Words and Phrases: explainable artificial intelligence, interpretable machine learning, evaluation, explainability,
interpretability, quantitative evaluation methods, explainable AI, XAI
Authors’ addresses: Meike Nauta, , University of Twente, Enschede, the Netherlands and University of Duisburg-Essen, Essen,
Germany; Jan Trienes, , University of Duisburg-Essen, Essen, Germany; Shreyasi Pathak, , University of
Twente, Enschede, the Netherlands and University of Duisburg-Essen, Essen, Germany; Elisa Nguyen, ; Michelle Peters,
 , University of Twente, Enschede, the Netherlands; Yasmin Schmitt; Jörg Schlötterer, , University of
Duisburg-Essen, Essen, Germany; Maurice van Keulen, , University of Twente, Enschede, the Netherlands; Christin Seifert,
 , University of Duisburg-Essen, Essen, Germany.
 
Nauta et al.
INTRODUCTION
The last decades have seen rapid development and extensive usage of Artificial Intelligence (AI) and Machine Learning (ML). The size and complexity of these models grew in pursuit of predictive performance. However, the focus on
accuracy alone is increasingly coming under criticism, since it leaves us with big black-box models with non-transparent
decision making which prevents users from assessing, understanding and potentially correcting the system. The
necessity for interpretable and explainable AI (XAI) therefore arises, aiming to make AI systems and their results more
understandable to humans . Especially the emergence of deep learning in the last decade has led to a high interest in
developing methods for explaining and interpreting black-box systems.
With an increasing number of XAI methods, the demand grows for suitable XAI evaluation metrics .
This need is not only recognized by the AI community; also the Human Computing Interaction (HCI) community is
concerned with developing transferable evaluation methods for XAI . In addition, a research agenda for Hybrid
Intelligence has explicitly formulated a research question asking how the quality and strength of explanations can
be evaluated. Whereas traditional performance indicators exist to evaluate prediction accuracy and computational
complexity, auxiliary criteria such as interpretability may not be easily quantified . This difficulty is part of the
reason for the huge variation in explanation techniques, and the optimal evaluation methods and measures could
depend on the application domain, the type of explanation, the type of data, the background knowledge of the user and
the question to be answered. The XAI community has yet to agree upon standardized evaluation metrics to go beyond
often reported anecdotal evidence showing individual, convincing examples that pass the first test of having “facevalidity” . Evaluation is then only based on “the researchers’ intuition of what constitutes a good explanation” .
The lack of quantitative evaluation impedes interpretability research, since anecdotal inspection is not sufficient for
robust verification . Many authors (e.g. ) argue that relying on such anecdotal evidence alone is
insufficient and that other aspects of the explanations should be evaluated as well.
Whereas interpretability can be presented as a binary property, we consider interpretability a multi-faceted characteristic and argue that a quantitative way of measuring interpretability should result in a multi-dimensional view
indicating the extent to which certain properties are satisfied. Having a set of quantitative, and preferably automated,
metrics for various properties would allow researchers and practitioners to a) assess and validate the interpretability of
a single explanation method and its explanations, b) objectively compare and benchmark multiple explanation methods,
c) add interpretability as optimization criteria during model training to tune the accuracy-interpretability trade-off.
Contributions. This survey contributes to the demand for XAI evaluation methods with a systematic review on
the evaluation of explainability and interpretability1 methods. Specifically, we collected 606 papers 
published at twelve flagship computer science conferences in a structured manner, of which 312 introduced an
XAI method (Section 3.1). Our categorization of explainable AI methods is available on our interactive website at
 Analysis of this set of papers provide quantitative insights into the extent
and nature of research activity in XAI and the evaluation of the resulting explanations (Section 5). For instance, we
have found that feature importance is the most common explanation type and that the majority of XAI methods
explain single predictions rather than providing global insights about the model reasoning. Moreover, 1 in 3 papers
evaluate exclusively with anecdotal evidence, and 1 in 5 papers evaluate with a user study. Additionally, we argue that
explainability is a multi-faceted concept and make this explicit with our Co-12 properties of explanation quality. We use
1Regarding terminology: ‘interpretability’ and ‘explainability’ are closely related and often used interchangeably in the XAI context . We equate
them in this survey as well. The same holds for ‘explainable artificial intelligence’ and ‘interpretable machine learning’. See also Section 1.1.
A Systematic Review on Evaluating Explainable AI
Co-12 as categorization scheme for the analysis of quantitative evaluation methods in papers that introduce, apply or
evaluate an XAI method (361 papers in total). As a result, Section 6 presents an overview of quantitative evaluation and
benchmarking methods for explainable AI. Hence, we address the frequently reported lack of quantitative evaluation
methods and respond to the call for automated and quantifiable evaluation metrics for robust and
falsifiable explainability research . We hope that our collection of evaluation methods will facilitate a more complete
and inclusive evaluation for objectively validating and comparing new and existing XAI methods. Our overview can
serve as a handbook for researchers and practitioners that are looking for suitable evaluation methods to evaluate
multiple aspects of their XAI method. Lastly, Section 7 discusses the implications of our results and identifies research
opportunities for the XAI domain. The most promising potential we see for XAI research is to shift from evaluating
explanations to incorporating explanation quality metrics into the training process to optimize for explainability.
Reader’s Guide. Since Explainable AI is of interest to a broad range of people from XAI researchers to ML practitioners,
we provide guidance for various types of readers as follows:
• All readers: Table 2 presents our Co-12 properties, a high-level decomposition of explanation quality, such as
Completeness, Correctness and Compactness. Our definition of explanation is introduced in Section 1.1.
• Readers who want to evaluate XAI methods: Table 3 summarizes our collection of automated, quantitative
evaluation methods. This overview provides tools for researchers and practitioners to thoroughly validate and
compare XAI methods. Section 6 describes each automated evaluation method and its variations in more detail.
• Readers interested in trends and evaluation practices in XAI: Sections 4 and 5 quantitatively summarize
our findings on research activity in XAI and evaluation practices: from anecdotal evidence to user
studies. E.g., we find that 1 in 5 papers evaluate with users. Additionally, Section 7 provides a unifying, conclusive
view on XAI evaluation practices and presents research opportunities.
• Readers looking for available XAI methods: Our website with
312 papers that introduce an explainable AI method, is a useful starting point for anyone searching for specific
explainable AI methods. Based on our categorization as presented in Figure 2, readers can filter XAI methods on
e.g. type of data, type of explanation and type of task. Different explanation types are summarized in Table 1.
• Readers interested in theory of XAI evaluation: Section 2 is recommended as background reading, as it
summarizes related work, presents pros and cons of evaluating with users and discusses the discrepancy between
objective and subjective evaluation.
Comparison with other surveys. In contrast to most XAI surveys that review explainability methods, we focus on
the evaluation of explainability. Some surveys discuss evaluation as part of a broader review of XAI methods , or mainly discuss evaluation with user studies . Others
discuss XAI evaluation within a limited scope, by focusing on an application domain or subarea of XAI: e.g. on the
healthcare domain , the evaluation of local explanations , contrastive and counterfactual explanations 
or explanations and their evaluation for recommender systems . Closest related to our work is the survey
of Zhou et al. that presents a concise overview and discussion regarding evaluating XAI without a structured
literature review, and the work of Vilone and Longo that present a list of concepts related to explainability and
discuss evaluation methods in combination with explainable AI methods. In contrast to previous surveys, we conducted
a large-scale, systematic review on the evaluation of explainability in a broad context, resulting in qualitative and
quantitative insights and therefore aiming to offer guidance on future XAI evaluations.
Nauta et al.
Definitions and Terminology
Explanations have been discussed for decades in many research areas. However, various contexts may require different
types of explanations. No definition therefore precisely captures the scope of all different settings. In this survey, we
focus on the context of explainable artificial intelligence and interpretable machine learning. Those terms, together with
explainability and interpretability (and to some extent also intelligibility) are often used interchangeably . Some argue that the terms are closely related but distinguish between them , although there is no
consensus on what the distinction exactly is . We equate them (and use them interchangeably) to keep a general,
inclusive discussion and to ensure that we do not exclude work because of different terminology. We consider related
concepts such as fairness, safety, causality, ethical decision making and privacy out of scope.
We frame explanations in the context of explainable artificial intelligence and define an explanation as follows:
Definition
An explanation is a presentation of (aspects of) the reasoning, functioning and/or behavior of a machine
learning model in human-understandable terms.
The definition of explanation is inspired by work of van Lent et al. who coined the term XAI. Quoting van Lent
et al. : “Ideally, this Explainable AI can present the user with an easily understood chain of reasoning from the
user’s order, through the AI’s knowledge and inference, to the resulting behavior”. The fact that the explanation should
be “easily understood” is also emphasized by others: “systems are interpretable if their operations can be understood by
a human” and “an interpretation is the mapping of an abstract concept into a domain that the human can make
sense of” . We adopt the phrasing of Doshi-Velez and Kim who define interpretability as the ability “to explain
or to present in understandable terms to a human”.
We specifically included “reasoning, functioning and/or behavior” in our definition to capture different types of
explanations, which can be roughly related to the three approaches identified by Gilpin et al. . Reasoning refers
to the process on how a model came to a particular decision. In Gilpin’s terms, it explains the “processing” of data to
answer the question “Why does this particular input lead to that particular output?”. Functioning refers to the (internal)
workings and internal data structures of the machine learning models, and therefore relates to the “representation of
data” . Behavior refers to how the model globally operates without specifically analyzing the internal workings (e.g.
by observing input and output) which can “simplify interpretation” . The inclusive or indicates that an explanation
can satisfy multiple goals.
Terminology and Notation. In this paragraph, we introduce our terminology with corresponding notation to allow an
unambiguous review and discussion in the rest of this paper. Let 𝑓be a predictive machine learning model, such as a
neural network or decision tree, trained to take some data as input and predict the corresponding output. Given input
𝑥, the predictive model 𝑓outputs a prediction 𝑓(𝑥). Let 𝑒denote an explanation method which generates explanations.
The explanation method can produce a local explanation which explains a single prediction, denoted as 𝑒(𝑓(𝑥)), or a
global explanation explaining the predictive model as a whole: 𝑒(𝑓). The explanations generated by explanation method
𝑒are of a certain explanation type, e.g. a decision tree, heatmap or rule list. In case of an intrinsically interpretable
model (also called self-explaining ), the predictive model is already explainable by design, since interpretability is
built into the architecture. For such self-explaining methods, the model and explanation method are the same, i.e., 𝑒= 𝑓,
such that 𝑓(𝑥) = 𝑒(𝑥).
A Systematic Review on Evaluating Explainable AI
RELATED WORK: ANECDOTAL EVIDENCE, FUNCTIONAL EVALUATION AND USER STUDIES
We identified two main themes in the literature on XAI evaluations: 1) the difference between evaluating plausibility
and correctness of an explanation, 2) XAI evaluation with or without users. The following two subsections summarize
opinions from existing literature on each of these themes.
Evaluating Plausibility or Correctness of an Explanation
Whereas standard evaluation metrics exist to evaluate the performance of a predictive model, there is no agreed-upon
evaluation strategy for explainable AI. As a result, a common evaluation strategy is to show individual, potentially cherrypicked, examples that look reasonable and pass the first test of having “face-validity” . Many authors argue
that relying on such anecdotal evidence alone is insufficient and can even be “misleading” . Leavitt and Morcos 
note that researchers too frequently assume that an explanation method and the resulting explanation are faithful.
“Intuition is essential for building understanding” but “unverified intuition [...] can facilitate misapprehension” .
They argue that the lack of quantitative evaluation impedes interpretability research, since anecdotal inspection is not
sufficient for robust verification.
Related, several papers warn that evaluating the plausibility and convincingness of an explanation to humans is
different from evaluating its correctness, and these evaluation criteria should not be conflated. Jacovi and Goldberg 
argue that it is not guaranteed that a plausible explanation is also truthfully reflecting the reasoning of the model.
Petsiuk et al. believe that “keeping humans out of the loop for evaluation makes it more fair and true to the
classifier’s own view on the problem rather than representing a human’s view”. Gilpin et al. explain that an
unreasonable-looking explanation could indicate either an error in the reasoning of the predictive model, or an error in
the explanation producing method. Visual inspection on the plausibility of the explanation, such as anecdotal evidence,
cannot make this distinction. We can relate this to the well-known phrase “garbage in, garbage out": when the machine
learning model is trained on flawed data, it learns nonsensical relations which are in turn shown by the explanation.
The explanation might then be perceived as being wrong, although it is truthfully reflecting the model’s reasoning.
Zhang et al. identify this as main shortcoming when evaluating explainable AI and state that checking whether an
explanation “looks reasonable” only evaluates the accuracy of the black box model and is not evaluating the faithfulness
of the explanation. Adebayo et al. motivate this issue with a clear example: they show that saliency maps to explain
computer vision tasks can be highly similar to edge detectors. Visual inspection would be insufficient to differentiate
edge detection from model-sensitive explanations. “Here the human observer is at risk of confirmation bias when
interpreting the highlighted edges as an explanation of the class prediction.” .
These commentaries relate to the inherent coupling of evaluating the black box’ predictive accuracy with explanation
quality. As pointed out by Robnik-Šikonja and Bohanec , the correctness of an explanation and the accuracy of the
predictive model may be orthogonal. Although the correctness of the explanation is independent of the correctness of
the prediction, visual inspection cannot distinguish between the two. Samek et al. analyze this issue for heatmaps
that explain computer vision algorithms: “the heatmap quality does not only depend on the algorithms used to compute
a heatmap, but also on the performance of the classifier, whose efficiency largely depends on the model being used,
and the amount and quality of available training data.”. Gilpin et al. find it unethical to optimize an explanation
towards hiding undesirable attributes. They argue that explanation methods should be evaluated on “how they behave
on the curve from maximum interpretability to maximum completeness” .
Nauta et al.
Evaluating With or Without User Studies
Related to the discussion on evaluating plausibility is the discussion on evaluating with or without users. Doshi-Velez
and Kim propose to categorize the evaluation of interpretability with a 3-level taxonomy. The top level contains
application-grounded evaluation and involves human subject experiments with domain experts within a real application,
such that the method can be evaluated by the intended users with respect to a particular task. The second level contains
human-grounded evaluation and involves user studies with lay persons on simplified tasks that “maintain the essence of
the target application” . This evaluation level is suitable when researchers want to evaluate more general notions of
explanation quality instead of one particular end-goal, or when reaching the target user is difficult due to e.g. high costs
or a low number of available domain experts. The controlled human experiments can result in subjective results by
asking users for perceived quality, or objective results by measuring performance of participants on specific tasks.
The third level of the taxonomy contains the functionally-grounded evaluation approach, which includes
evaluation where human experiments are not needed but instead uses computational proxy measures for interpretability.
For example, measuring the size of the explanation or validating feature importance by perturbing model input. Doshi-
Velez and Kim discuss the potential advantages of this evaluation type: besides saving time and costs and therefore
being more scalable, it can be particularly appropriate when user studies are unethical or when the method is not yet
mature enough for evaluation with users. However, they also emphasize that these proxy metrics are best suited once
user studies have already confirmed the interpretability of the model class. This is in accordance with Miller et al. 
who state that proxy metrics are valid evaluations, but the authors support more intensive human evaluations to have
“real-world impact”.
Others put extra arguments forward in favor of automated metrics where no user involvement is needed. User
studies for machine learning research often depend on online crowd platforms such as Amazon Mechanical Turk
which can lead to ethical issues. Critics argue that these platforms are largely unregulated and that workers are poorly
compensated . Results from user studies are also “rarely replicable or even comparable” . Additionally,
besides saving time and resources , Herman and Ancona et al. argue that user studies imply a strong bias
towards simpler explanations that are closer to the user’s expectations, “at the cost of penalizing those methods that
might more closely reflect the network behavior” . "A good explanation method should not reflect what humans
attend to, but what task methods attend to" . This relates to the discussion in Section 2.1 on anecdotal evidence
as evaluation strategy. Validating explanations with users can unintentionally combine the evaluation of explanation
correctness with evaluating the correctness of the predictive model. Leavitt and Morcos therefore plead for “clear,
specific, testable and falsifiable hypotheses” that dissociate the evaluation of the explanation method from the predictive
model . Quantitative evaluation also allows a formal comparison between various explanation methods ,
and contrasting them under different applications and purposes . In the context of usability evaluation in the HCI
community, Greenberg and Buxton argue that there is a risk of executing user studies in an early design phase,
since this can quash creative ideas or promote poor ideas. Miller therefore argues that proxy studies are especially valid
in early development . Qi et al. indicate that “evaluating explanations objectively without a human study is
also important because simple parameter variations can easily generate thousands of different explanations, vastly
outpacing the speed of human studies”.
A Systematic Review on Evaluating Explainable AI
Screening for
companion papers
Excluded from
Applying inclusion
Excluded from
Initial search
Analysis of quantitative
evaluation methods
Original work introducing, applying and/or
evaluating one or more methods for explaining a
machine learning model.
Categorization of XAI methods
and analysis of evaluation practice
Original work introducing a method
for explaining a machine learning model. (filter)
Papers covered in survey
Fig. 1. Flow diagram showing the number of papers through the different phases of the reviewing process. A more detailed description
of our paper collection process, inclusion and exclusion criteria, and reviewing process is available in Supplementary Material.
Discussion: Relation to Our Work
The discussions in the field have illustrated that evaluation of explanations is not self-evident and shows various pitfalls.
We also found that explainability is indeed not a binary property, and that various aspects of an explanation should be
evaluated independently of each other. Since knowledge about which aspects constitute a good explanation is scattered,
this survey presents an aggregated view of what to evaluate by introducing twelve properties on explanation quality
(Section 3.3). Our property ‘Correctness’ addresses the faithfulness of an explanation with respect to the predictive
model, whereas ‘Coherence’ addresses the plausibility of an explanation. Hence, both properties can be evaluated
separately. Additionally, Section 6 presents an overview of quantitative evaluation methods and categorizes them along
our so-called Co-12 properties. This contribution concretely addresses how to evaluate different aspects of an explanation,
and therefore provides conceptual guidance to the XAI community. We specifically focus on functionally-grounded
evaluation. A concise overview of evaluation methods with user studies is provided in Supplementary Material.
METHODOLOGY
Paper Selection
We collected papers in a structured manner to provide both quantitative and qualitative insights about the XAI domain
based on a large corpus of scientific work on XAI evaluation methods. Since the literature on XAI is highly diverse and
distributed across different (sub)disciplines, we selected literature published from 2014 to 2020 at one of the following
twelve prominent conferences: AAAI, IJCAI, NeurIPS (formerly NIPS), ICML, ICLR, CVPR, ICCV, ACL, WWW, ICDM,
SIGKDD (also called KDD), SIGIR. We used DBLP2 to conduct a keyword search in publication titles with the following
search query: explain* OR explanat* OR interpret* to capture terms including explainable, explaining, explanation,
interpretable and interpretability. This search, conducted on 4th of May 2021, resulted in 606 papers. We manually
excluded companion papers (such as workshop papers and tutorials), resulting in 494 papers. To only include relevant
papers in our analysis, we apply the following inclusion criterion:
Original work introducing, applying and/or evaluating one or more methods for explaining a machine learning model.
Applying the inclusion criteria led to 361 papers being included, as shown in Figure 1. Subsequently, we can apply a
filter that only selects the papers that introduce an XAI method, resulting in 312 papers. We apply this filter to analyze
how introduced XAI methods are evaluated when they are first presented (Section 5). For collecting all evaluation
methods, we review all 361 included papers since 49 papers do not introduce a new XAI method, but could contain
relevant evaluation metrics to compare and evaluate existing XAI methods. We do not want such papers to skew our
quantitative results in Section 5, but include them in our evaluation overview in Section 6 for completeness. A more
detailed description of our paper collection process, inclusion and exclusion criteria, and reviewing process is available
in Supplementary Material.
2 
Nauta et al.
Review Protocol: Categorization of Explainable AI Methods
Time Series
Structured
(data-agnostic)
(Deep) Neural
Tree Ensemble
Bayesian or
Hierarchical
Explanation
Interpretability
Support Vector
(model-agnostic)
Supervised
Explanation
Type of Data
Type of Task
Decision Rules
Disentanglement
Decision Tree
Importance
Feature Plot
Localization
Prototypes
Representation
Visualization
Representation
(excl. decision rules)
Explanation
Explanation
Inspection
Transparent
Box Design
Classification
Regression
Policy Learning
Type of Explanation
Type of Problem
Type of Predictive Model
Type of Method used to Explain
Fig. 2. Categorization of explainable AI methods along 6 dimensions: type of data used as input 𝑥to predictive model 𝑓, type of
explanation, type of explanation problem that is addressed, type of predictive model to be explained (𝑓), type of task for which model
𝑓is used, and the type of method used to explain. Papers can address multiple categories per dimension.
For further analysis of the included papers, we categorize each paper and analyze the properties of the explanation
method and the evaluation of the explanations in more detail. For each included paper, we review the main content and
do not consider appendices and supplementary material. Each included paper is first categorized along six dimensions,
in order to create a structured overview of XAI methods. Figure 2 summarizes the 6 dimensions and their corresponding
categories. The following paragraphs discuss three dimensions in more detail.
We adopt the taxonomy of Guidotti et al. , presenting four types of problems that an XAI method can solve: (i)
Model Explanation – globally explaining model 𝑓through an interpretable, predictive model; (ii) Model Inspection
– globally explaining some specific property of model 𝑓or its prediction; (iii) Outcome Explanation – explaining an
outcome/prediction of 𝑓on a particular input instance; (iv) Transparent Box Design – the explanation method is an
interpretable model (i.e., 𝑒= 𝑓) also making the predictions. Note that a set of outcome explanations can collectively
comprise a global explanation for model inspection (cf. e.g. ). For details regarding this taxonomy, we refer
to Section 4 of the survey by Guidotti et al. .
We identified three different types of general types of methods used to explain a machine learning model: i)
Post-hoc explanation methods (also called reverse engineering ): explain an already trained predictive model; ii)
Interpretability built into the predictive model, such as white-box models, attention mechanisms or interpretability
constraints (e.g. sparsity) included in the training process of the predictive model; and iii) Supervised explanation
training, where a ground-truth explanation is provided in order to train the model to output an explanation.
Evaluation methods are often specific for a specific type of explanation, thus we categorize each paper by type
of explanation. In contrast to most XAI surveys, we disregard the explanation construction approach but focus on
the explanation’s output format. Since we could not find a complete and recent overview of explanation types, we
reviewed recent XAI surveys and adapted and extended the categories identified by Guidotti et al. 
with these insights. We grouped some explanation types and separated others based on the expected difference in
A Systematic Review on Evaluating Explainable AI
Table 1. Overview of types of explanations.
Description and Examples
Decision Rules
Logical rules, incl. decision sets , anchors , decision tables and programs .
Decision Tree
Rooted graph with conditional statement at each node, e.g. ProtoTree .
Disentanglement
Disentangled representation, where each disjoint feature might have a semantic meaning,
e.g. InfoGAN .
Importance
Set of 1-dimensional non-binary values/scores to indicate feature relevance, feature contribution
or attribution. A feature is not necessarily an input feature to predictive model 𝑓, but it should be
a feature in the explanation. Examples include SHAP and importance scores by LIME .
Feature Plot
Plot or figure showing relations or interactions between features or between feature(s) and
outcome. Examples include Partial Dependence Plot , Individual Conditional Expectation
plot and Feature Auditing .
Graphical network structure with nodes and edges, e.g. Abstract Policy Graph , Knowledge
graph , Flow graph and Finite State Automata .
Map with at least 2 dimensions visually highlighting non-binary feature attribution, activation,
sensitivity, attention or saliency. Includes attention maps , perturbation masks and
Layer-Wise Relevance Propagation .
Localization
Binary feature importance. Features can be any type of covariate used in the explanation,
such as words, tabular features, or bounding boxes. Examples include binary maps with image
patches , segmentation and bounding boxes .
Prototypes
(Parts of) Representative examples, including concepts , influential training instances ,
prototypical parts , nearest neighbors and criticisms .
Representation
Artificially produced visualization to explain representations of the predictive model. Examples
include generated data samples , Activation Maximization and feature visualization .
Representation
Visualization
Charts or plots to visualize representations of the predictive model, including visualizations of
dimensionality reduction with scatter plots , visual cluster analysis and Principal
Component Analysis.
Textual explanation via natural language, e.g. .
White-box Model
Intrinsically interpretable models. Predictive model 𝑓is interpretable and therefore acts as
explanation. Examples include a scoring sheet and linear regression. Decision Rules and
Decision Trees do not fall into this category, since they are categories on their own.
Explanation that does not fit any other category.
evaluation metrics resulting in the 14 categories outlined in Table 1. Note that some explanation methods combine
multiple explanation types, thus the categories are not mutually exclusive w.r.t. explanation methods. We do not
consider counterfactual explanations a separate category. Although counterfactual explanations answer a different type
of question, the type of explanation utilized is still one of the above categories.
Review Protocol: Evaluation of XAI methods with Co-12 Properties
When reviewing the evaluation of an XAI method, we distinguish between evaluation with users and without users. We
focus on the evaluation of explanation method 𝑒and/or its produced explanations. Hence, this is not about the evaluation
of predictive model 𝑓, and we do not take the evaluation of predictions into account. Therefore, evaluation metrics
that evaluate 𝑓or the predictions of 𝑓, such as task accuracy and computation time, are not included. Additionally,
Nauta et al.
Exclusively
anecdotal evidence?
Without User Study
- Application-grounded
Human-grounded
Description
Quantitative or Qualitative
Co-12 Property
evaluation method
With User Study
Fig. 3. Our review protocol when reviewing the evaluation of an XAI method.
evaluation metrics for 𝑒that do not directly influence explanation quality (such as run time or construction overhead of
𝑒 ), are also excluded. Lastly, we excluded quantitative methods that use explanations to get more insights into the
predictive model. Such approaches are not about evaluating explanations, but rather utilize explanations to investigate
model 𝑓. For example, explainable AI is used to analyze whether model 𝑓is fair or biased , whether 𝑓is
right for the right reasons , or whether 𝑓is overfitting .
As summarized in Figure 3, we analyzed for each paper whether exclusively anecdotal evidence is presented for
evaluating the quality of the XAI method. For all other evaluation methods, we collected a short description on how
it works, whether the measure is qualitative or quantitative and which property is evaluated. Additionally, for user
studies, we assess whether the study is application-grounded, i.e. in a real application with domain experts (following
the taxonomy of ) or human-grounded, i.e. using simplified, but similar tasks with lay persons.
Co-12 Explanation Quality Properties. Different aspects regarding explanation quality can be evaluated, as also
discussed in Section 2.1. We therefore argue that explainability is a non-binary characteristic, that can be measured by
evaluating to what degree certain properties are satisfied. Based on conceptual literature that discusses explanation
quality and properties of a good explanation, we identified twelve desired explanation properties that together present
an aggregated view of what to evaluate. We paid specific attention to covering as much of the reviewed properties as
possible, minimizing semantic overlap between properties and grouping different terminology that describe a similar
property. Our so-called Co-12 properties (pronounce as co-twelve) regarding explanation quality are presented below
and summarized in Table 2.
Correctness addresses the truthfulness/faithfulness of the explanation with respect to predictive model 𝑓, the model
to be explained. Hence, it indicates how truthful the explanations are compared to the “true” black box behavior (either
locally or globally). Note that this property is not about the predictive accuracy of the black box model, but about
the descriptive accuracy of the explanation . Ideally, an explanation is “nothing but the truth” , and high
correctness is desired .
Completeness addresses the extent to which the explanation explains predictive model 𝑓. Ideally, the explanation
provides “the whole truth” . High completeness is desired in order to provide
enough detail, but it should be balanced with compactness and correctness: “don’t overwhelm” .
• Reasoning-completeness indicates the extent to which the explanation describes the entire internal dynamic
of the model . One extreme is “revealing all the mathematical operations and parameters in the system” 
such as white-box models which are by definition fully reasoning-complete. The other extreme are global
surrogate models that are trained to give the same predictions as black box 𝑓, without considering any internal
reasoning of 𝑓. A design choice should be made regarding the reasoning-completeness by selecting an explanation
type suited for a specific context. Therefore, reasoning-completeness is often only evaluated qualitatively to
compare different explanation types.
A Systematic Review on Evaluating Explainable AI
Table 2. Our Co-12 explanation quality properties, grouped by their most prominent dimension: Content, Presentation or User.
Co-12 Property
Description
Correctness
Describes how faithful the explanation is w.r.t. the black box.
Key idea: Nothing but the truth
Completeness
Describes how much of the black box behavior is described in the explanation.
Key idea: The whole truth
Consistency
Describes how deterministic and implementation-invariant the explanation method is.
Key idea: Identical inputs should have identical explanations
Continuity
Describes how continuous and generalizable the explanation function is.
Key idea: Similar inputs should have similar explanations
Contrastivity
Describes how discriminative the explanation is w.r.t. other events or targets.
Key idea: Answers “why not?” or “what if?” questions
Covariate complexity
Describes how complex the (interactions of) features in the explanation are.
Key idea: Human-understandable concepts in the explanation
Presentation
Compactness
Describes the size of the explanation.
Key idea: Less is more
Composition
Describes the presentation format and organization of the explanation.
Key idea: How something is explained
Confidence
Describes the presence and accuracy of probability information in the explanation.
Key idea: Confidence measure of the explanation or model output
Describes how relevant the explanation is to the user and their needs.
Key idea: How much does the explanation matter in practice?
Describes how accordant the explanation is with prior knowledge and beliefs.
Key idea: Plausibility or reasonableness to users
Controllability
Describes how interactive or controllable an explanation is for a user.
Key idea: Can the user influence the explanation?
• Output-completeness addresses the extent to which the explanation covers the output of model 𝑓. Thus, it is
a “quantification of unexplainable feature components” and measures how well the explanation method
agrees with the predictions of the original predictive model .
Consistency checks that identical inputs have identical explanations . In practice, this property addresses
to what extent the explanation method is deterministic. Additionally, for explanation methods that do not consider the
internals of the black box but only observe input and output, consistency regards implementation invariance which
states that two models that give the same outputs for all inputs should have the same explanations . Atanasova
et al. add that models with the same architecture but trained from different random seeds should give the same
explanations when they follow the same reasoning path. For explanation methods that do consider the internals of
the black box, Montavon argues that implementation invariance is still a desired property, but should then be
evaluated without changing the actual function.
Continuity considers how continuous (i.e. smooth) the explanation function is that is learned by the explanation
method. A continuous function ensures that small variations in the input, for which the model response is nearly
Nauta et al.
identical, do not lead to large changes in the explanation . Continuity also adds to
generalizability beyond a particular input or generalizability to new contexts .
Contrastivity addresses the discriminativeness of an explanation and aims to facilitate comparisons in relation to
other targets or events . Miller argues that an explanation should not only explain an event, but explain it “relative
to some other event that did not occur” . Honegger adds the separability property that non-identical instances
from different populations must have dissimilar explanations.
Covariate complexity considers the complexity of the covariates (i.e. features) used in the explanation in terms of
semantic meaning and interactions between the covariates and the target. The covariates in the explanation should
be comprehensible , and “concepts should have an immediate human-understandable interpretation” . This
could mean that the variables used in the explanation are different from the features given as input to model 𝑓, since
“interpretable data representations” are desired . Also non-complex interactions between features are desired, such
as monotonicity . Wilson et al. found that humans favor smooth and simpler functions, and have inductive
biases towards recognizable patterns, such as step functions or a sawtooth pattern.
Compactness considers the size of the explanation and is motivated by human cognitive capacity limitations.
Explanations should be sparse, short and not redundant to avoid presenting an explanation that is too big to understand .
Composition considers the presentation format, organization and structure of the explanation , such that the
way in which the explanation is presented to the user increases its “clarity” . As mentioned by Huysmans et
al. , “some representation formats are generally considered to be more easily interpretable than others”. Hence, this
property is about how something is explained instead of what is explained. Examples include the usage of higher-level
information , abstractions or suitable terminology , and not using explanations that are circular .
Others compare the interpretability of different representation formats, such as Booth et al. evaluating logical
sentences of different forms to investigate how to best present propositional theories to humans, and Huysmans et
al. comparing the comprehensibility of decision tables, trees and rule-based models.
Confidence concerns whether the explanation has a measure of certainty or other probability information. It can
reflect two facets of certainty: i) a confidence measure of the black box prediction , or ii) the truthfulness
or likelihood of the explanation . Opinions are divided about the last facet of this property, since it is
argued that referring to probabilities might not be so effective, since people have difficulties to correctly estimate
probabilities .
Context addresses the extent to which the user and their needs are taken into account for comprehensible explanations . Explanations should be relevant to the user’s needs and level of expertise . Srinivasan and
Chander argue, from a cognitive science perspective, that explanations should not only serve AI scientists, but a
whole variety of stakeholders, e.g. policy makers and customers.
Coherence assesses to what extent the explanation is consistent with relevant background knowledge, beliefs and
general consensus and hence addresses reasonableness , plausibility and “agreement with human
rationales” . It is often argued that evaluating coherence alone (with e.g. anecdotal evidence) is not sufficient ,
and that coherence and correctness should not be conflated but evaluated separately . Note that this property
addresses external coherence, and is different from internal coherence to indicate that parts in an explanation fit
together .
Controllability indicates to what extent a user can control, correct or interact with an explanation ,
since it is argued that “explanations are social” .
A Systematic Review on Evaluating Explainable AI
OVERALL STATISTICS OF INCLUDED PAPERS
Publication Year
# papers introducing, applying and/or evaluating an XAI method
Total: 361
1. NeurIPS: 66
2. AAAI: 48
3. ACL: 36
4. ICML: 35
5. KDD: 35
6. CVPR: 31
7. IJCAI: 31
8. SIGIR: 19
9. ICDM: 18
10. ICLR: 18
11. ICCV: 13, biyearly
12. WWW: 11
(a) Number of included papers per venue that introduce, apply or
evaluate an XAI method (gray line shows average). The box on the
left lists the total number of included papers.
Publication Year
# papers not introducing, but
applying or evaluating an XAI method
(b) Number of included papers that do not introduce, but
rather apply and/or evaluate an XAI method, plotted by
publication year.
Fig. 4. Number of included papers per publication year.
Figure 4a shows that the number of papers on explainable AI and interpretable machine learning is growing over the
years. There were few papers in our set published in 2014 or 2015 (5 and 3 in total, respectively), which could be related
to the fact that the topic itself was less popular or that terms as ‘interpretability’ and ‘explainable AI’ were not yet
used in these years . We see a steady increase of included papers since 2016. Especially 2018 shows a significant
increase, which corresponds with findings by Barredo Arrieta et al. . However, our paper selection does not show
an exponential growth as found by Adadi and Berrada , although some conferences such as ACL and ICML do show
an exponential increase. NeurIPS (formerly NIPS) is in our dataset the conference with the most papers on explainable
AI and interpretable ML. Especially the large jumps in 2018 and 2020 are striking. Also at AAAI the number of papers
on explainability increased substantially over the last years.
Additionally, we analyzed the number of papers that do not introduce an XAI method, but apply or evaluate them
(i.e., the 49 papers that would be excluded by the filter as shown in Figure 1). Figure 4b shows that this number
increased substantially in 2019 and again in 2020. This trend could indicate that the awareness regarding evaluation
and comparison of XAI methods has grown in the last years, which again could point towards an increasing maturity
of the field.
STATISTICS ON XAI METHODS AND THEIR EVALUATION
In this section we analyze the 312 papers that introduce a method for explaining a machine learning model. An
interactive website for this labeled dataset is available at Figure 5 presents
summary statistics regarding the categorization of XAI methods. Supplementary material presents a more detailed
analysis of the categorization of XAI methods. In the remainder of this section, we focus on the evaluation of XAI
Nauta et al.
Feature Importance
Localization
Prototypes
Disentanglement
Decision Rules
Representation Synthesis
Representation Visualization
White-box model
Feature plot
Decision Tree
Types of Explanations
Tabular / structured
Time series
User-item matrix
Types of Data
(Deep) Neural Network
Tree Ensemble
Bayesian or Hierarchical
Logistic Regression*
Support Vector Machine
Types of Models to be Explained
Classification
Regression
Recommendation*
Question Answering*
Representation learning*
Policy learning
Generation*
Clustering*
Retrieval*
Anomaly detection*
Types of Tasks
Post-hoc explanation
Interpretability
built into the
predictive model
Supervised
explanation training
Types of Methods
Outcome Explanation
Model Inspection
Transparent Box Design
Model Explanation
Types of Problems
Fig. 5. Categorization of papers introducing an explainable AI method, following the six dimensions as presented in Section 3.2. Note
that categories are non-exclusive, so a paper can fall into multiple categories per dimension. *: category is manually added after the
reviewing process and might therefore not be complete (i.e. high precision, potentially low recall).
Includes User Study
Exclusively Anecdotal Evidence
Includes Quantitative Evaluation
(a) Evaluation practices of the 312 papers that introduce a
method for explaining a machine learning model.
Correctness
Output-completeness
Reasoning-completeness
Consistency
Continuity
Contrastivity
Covariate complexity
Compactness
Composition
Confidence
Controllability
(b) Total number of unique Co-12 properties quantitatively evaluated in a paper that introduces an XAI method.
Fig. 6. XAI evaluation practice.
Our summary statistics on XAI evaluation:
• 33% only evaluated with anecdotal evidence
• 58% applied quantitative evaluation
• 22% evaluated with human subjects in a user study, of which 23% evaluated with domain experts, i.e. applicationgrounded .
Earlier research has found that few papers quantitatively evaluated their explanations. Only 5% of the papers analyzed
by Adadi et al. evaluated their interpretable machine learning method and quantified its relevance. Nunes
and Jannach reported that only 21% of their 190 analyzed studies that presented an XAI technique or
tool contained “any form of evaluation, except from toy examples” . We have reviewed more papers, including
more recent papers, to shed light on the evaluation practices from 2014 to 2020. Our statistics regarding the usage of
quantitative evaluation are higher, which, although possibly influenced by the venues we collected from, indicates that
A Systematic Review on Evaluating Explainable AI
the evaluation of XAI has become more extensive over the years. Some of the papers that do not quantitatively evaluate
their XAI method argue that their model architecture is inherently interpretable, and therefore do not explicitly evaluate
its interpretability. Lipton however notes that even white-box models might not be interpretable anymore when
their size exceed the limited capacity of human cognition, and Jacovi and Goldberg suggest that intrinsically
interpretable methods should be held to the same standards as post-hoc interpretation methods with similar evaluation
methods. Others do not explicitly evaluate their method with quantitative evaluation metrics, but present mathematical
theory to support their claims (e.g. ).
Figure 6a gives more insight into the XAI evaluation practices over time. Years 2014 and 2015 are excluded from this
graph, since only 5 and 3 papers respectively were included for those years, leading to unreliable statistics. It confirms
that the fraction of papers that quantitatively evaluated their XAI method has slightly increased over the years, whereas
the fraction of papers only evaluating with anecdotal evidence shows a decreasing trend. Hence, the evaluation practice
in the XAI domain is effectively maturing. The number of papers including a user study for evaluation remains however
relatively constant over the years at around 20%. Figure 6b analyzes the number of evaluated Co-12 properties (as
introduced in Section 3.3) per paper that introduces an XAI method and quantitatively evaluates it. The leftmost bar
shows that 73 papers in our set quantitatively evaluated exactly one Co-12 property, which was often Coherence. The
two leftmost bars show that the majority of the papers that introduce an XAI method and quantitatively evaluate it,
evaluate one or two Co-12 properties. Coherence and Output-completeness are the Co-12 properties that are evaluated
most often, followed by Correctness, Compactness and Covariate complexity.
QUANTITATIVE EVALUATION METHODS FOR XAI
This section presents quantitative evaluation methods that we identified in the 361 included papers. This implies that
we do not only consider papers that introduce a method for explaining a machine learning model, but also include the
papers that apply or evaluate an XAI method. Our goal is that this section can serve as inspiration and guidance for
researchers and practitioners looking for suitable evaluation methods for new or existing XAI methods. We focus on
functionally-grounded evaluation methods (i.e., without user studies) and summarize quantitative evaluation methods
with user studies in Supplementary Material.
We clustered all identified quantitative evaluation metrics based on our Co-12 properties and named each of these
resulting evaluation methods. Table 3 describes each evaluation method we identified, while listing the types of
explanations that were mainly related with this method and the papers that applied this evaluation method. Variations
and additional information regarding each evaluation method are discussed in Sections 6.1 to 6.12, grouped per Co-12
property. For details regarding the implementation of the specific evaluation metric we refer to the original papers.
Table 4 relates each evaluation method to the corresponding Co-12 properties. The columns indicate what Co-12
properties can be measured with the corresponding evaluation method. We note that this does not mean that all
cited authors also evaluated all possible properties, but that each related Co-12 property was evaluated in at least one
paper. For a thorough and structured evaluation, we argue that it would be good practice to select multiple evaluation
methods that together cover as much of the Co-12 properties as possible. Such an extensive evaluation would result in a
multi-dimensional view on the degree of explainability.
Functionally Evaluating Correctness
The correctness property addresses to what extent the explanation is faithful to the predictive model it explains.
Important to emphasize here is that an explanation that looks reasonable to a user is not guaranteed to also be truthfully
Nauta et al.
Table 3. Descriptions of automated, quantitative evaluation methods (i.e. without user study), with references to papers that apply
this method. The italic text lists the types of explanations that we found mainly related with an evaluation method.
Name, Description and Main Explanation Types
References
CORRECTNESS (Section 6.1)
Model Parameter Randomization Check – Feature importance, Heatmap, Localization
Randomly perturb the internals of the predictive model and check that the explanation changes.
 
Explanation Randomization Check – Feature importance, Heatmap
Randomly perturb the explanation (which is built into the predictive model) and check that the
output of the predictive model changes.
 
White Box Check – Feature importance, Decision Rules, White-box model, Localization
Apply the explanation method to an interpretable white box model and check the correspondence
of the explanation with the white box reasoning.
[58, 121, 124, 144, 216, 219,
Controlled Synthetic Data Check
Feature importance, Heatmap, Prototypes, Localization, White-box model, Graph
Controlled experiment: Create a synthetic dataset such that the predictive model should follow
a particular reasoning, known a priori (important: checking this assumption by e.g. reporting
almost-perfect accuracy). Evaluate whether the explanation shows the same reasoning as the
data generation process.
 
Single Deletion – Feature importance, Heatmap
Delete, mask or perturb a single feature in the input and evaluate the change in output of the
predictive model. Measure correlation with explanation’s importance score.
 
Incremental Deletion (or Incremental Addition) – Feature importance, Heatmap
One by one delete (or perturb) or add features to the input, based on explanation’s order, and
measure for each new input the change in output of the predictive model. Report average change
in log-odds score, AUC, steepness of curve or number of features needed for a different decision.
Compare with random ranking or other baselines.
 
OUTPUT-COMPLETENESS (Section 6.2)
Preservation Check – Feature importance, Heatmap, Localization, Text, Prototypes
Giving the explanation (or data based on the explanation) as input to the predictive model should
result in the same decision as for the original, full input sample.
 
Deletion Check – Feature importance, Heatmap, Localization
Giving input without explanation’s relevant features should result in a different decision by the
predictive model than the decision for the original, full input sample.
 
Feature importance, Heatmap, Decision Rules, Decision Tree, Prototypes, Text, Localization, Whitebox model
Measure the agreement between the output of the predictive model and the explanation when
applied to the same input sample(s).
 
Predictive Performance
Feature importance, Heatmap, Decision Rules, Decision Tree, Prototypes, White-box model
Predictive performance of the interpretable model or predictive explanation with respect to the
ground-truth data.
[12, 44, 58, 82, 97, 134, 145,
157, 202, 207, 208, 218, 220,
220, 243, 292, 300, 306, 316,
CONSISTENCY (Section 6.3)
Implementation Invariance – Feature Importance
Evaluate whether the explanation method is invariant to specific implementations of the predictive model by validating whether two implementations that give the same output for an input,
also get the same explanation.
A Systematic Review on Evaluating Explainable AI
Table 3. Continued
Name, Description and Main Explanation Types
References
CONTINUITY (Section 6.4)
Stability for Slight Variations
Feature importance, Heatmap, Graph, Text, Localization, Decision Rules, White-box model
Measure the similarity between explanations for two slightly different samples. Small variations
in the input, for which the model response is nearly identical, should not lead to large changes in
the explanation.
 
Fidelity for Slight Variations – Decision Rules, White-box model
Measure the agreement between interpretable predictions for original and slightly different
samples: an explanation for original input 𝑥should accurately predict the model’s output for a
slightly different sample 𝑥′.
 
Connectedness – Prototypes, Representation Synthesis
Measure how connected a counterfactual explanation is to samples in the training data: ideally, the
counterfactual is not an outlier, and there is a continuous path between a generated counterfactual
and a training sample.
 
CONTRASTIVITY (Section 6.5)
Target Sensitivity – Heatmap
The explanation for a particular target or model output (e.g. class) should be different from an
explanation for another target.
[188, 209, 247, 253, 277,
Target Discriminativeness – Disentanglement, Representation Synthesis, Text
The explanation should be target-discriminative such that another model can predict the right
target (e.g. class label) from the explanation, in either a supervised or unsupervised fashion.
 
Data Randomization Check – Feature importance, Heatmap, Localization
Randomly change labels in a copy of the training dataset, train a model on this randomized dataset
and check that the explanations for this model on a test set are different from the explanations
for the model trained on the original training data.
 
COVARIATE COMPLEXITY (Section 6.6)
Covariate Homogeneity
Prototypes, Disentanglement, Localization, Heatmap, Representation Synthesis
Evaluate how consistently a covariate (i.e. feature) in an explanation represents a predefined
human-interpretable concept.
 
Covariate Regularity – Decision Rules, Feature Importance
Evaluate the regularity of an explanation by measuring its Shannon entropy, in order to quantify
how noisy the explanation is and how easy it is to memorize the explanation.
 
COMPACTNESS (Section 6.7)
Feature importance, Heatmap, Decision Rules, Decision Tree, Prototypes, Text, Graph, Localization,
White-box model, Representation Synthesis
Total size (absolute) or sparsity (relative) of the explanation.
[8, 35, 38, 58, 61, 82, 115,
130, 131, 136, 143, 145, 153,
177, 205–210, 218–220, 228,
238, 255, 259, 263, 273, 282,
283, 285, 288, 292–294, 305,
Redundancy – Feature importance, Decision Rules, Text, White-box model
Calculate the redundancy or overlap between parts of the explanation.
 
Counterfactual Compactness – Prototypes, Representation Synthesis, Text
Given a counterfactual explanation showing what needs to be changed in the input in order to
change the prediction of the predictive model, measure how much needs to be changed.
[8, 88, 127, 130, 151, 201,
Nauta et al.
Table 3. Continued
Name, Description and Main Explanation Types
References
COMPOSITION (Section 6.8)
Perceptual Realism – Representation Synthesis, Text
Measure how realistic a generated explanation is compared to real, original samples.
 
CONFIDENCE (Section 6.9)
Confidence Accuracy – Feature Importance, Prototypes
Measure the accuracy of confidence/uncertainty estimates if these are present in the explanation.
CONTEXT (Section 6.10)
Pragmatism – Decision Rules, Representation Synthesis
The cost or degree of difficulty for a user to act upon suggestions by a counterfactual explanation
that explains what the user should change to attain a particular outcome by the predictive model.
 
Simulated User Study – Feature Importance, Localization
Create a synthetic dataset such that the utility of explanations for user-relevant tasks can be
automatically evaluated.
 
COHERENCE (Section 6.11)
Alignment with Domain Knowledge
Feature importance, Heatmap, Localization, Text, Disentanglement, Prototypes
Compare the generated explanation with a ‘ground-truth’ expected explanation based on domain
knowledge.
 
XAI Methods Agreement – Feature importance, Heatmap, Localization
Quantitatively compare explanations from different XAI methods and evaluate their agreement.
 
CONTROLLABILITY (Section 6.12)
Human Feedback Impact – Interactive Explanation, Text
Measure the improvement of explanation quality after human feedback, where the user is seen
as a system component.
reflecting the reasoning of the model . Checking the correctness of an explanation with respect to the predictive
model 𝑓is therefore different from the plausibility to the user.
The Model Parameter Randomization Check was introduced by Adebayo et al. as “sanity check” for the
faithfulness and sensitivity of the explanation to predictive model 𝑓. Perturbation of model 𝑓can be done by randomizing
parameters or re-initializing weights, after which the explanation is expected to change. If the explanation after
randomization is the same as the original explanation, then the explanation is not sensitive to 𝑓and hence not
correct w.r.t. reasoning of model 𝑓. (Our recommendation is to do multiple randomization runs to ensure that the two
explanations are not accidentally similar.) However, if the explanations are different, it is not a guarantee that the
original explanation is fully correct. It is therefore presented as a sanity check: the sensitivity of 𝑒to parametrization
changes in predictive model 𝑓is a necessary but not sufficient condition for correctness. A related method is the
Explanation Randomization Check which is applicable to explanations that are built into predictive model 𝑓, such
A Systematic Review on Evaluating Explainable AI
Table 4. Identified quantitative methods to evaluate explanations without user studies, related to their Co-12 properties. Bold check
mark indicates prominent Co-12 property. Superscript R and O indicate Reasoning-completeness resp. Output-completeness.
Correctness
Completeness
Consistency
Continuity
Contrastivity
Covariate Compl.
Compactness
Composition
Confidence
Controllability
Model Parameter Randomization Check
Explanation Randomization Check
White Box Check
Controlled Synthetic Data Check
Single Deletion
Incremental Deletion (or Incremental Addition)
Preservation Check
Deletion Check
Predictive Performance
Implementation Invariance
Stability for Slight Variations
Fidelity for Slight Variations
Connectedness
Target Sensitivity
Target Discriminativeness
Data Randomization Check
Covariate Homogeneity
Covariate Regularity
Redundancy
Counterfactual Compactness
Perceptual Realism
Confidence Accuracy
Pragmatism
Simulated User Study
Alignment with Domain Knowledge
XAI Methods’ Agreement
Human Feedback Impact
as attention and backpropagated relevance vectors. Permuting and randomizing the explanation within the model
should change the model’s output, and therefore evaluates the sensitivity of the explanation to the predictive model.
The White Box Check is designed to evaluate correctness by training a white-box model as predictive model and
applying the explanation method to the white-box model as if it was a black box. Since the reasoning of a white-box
model is known, the explanation can subsequently be compared with the true reasoning in order to evaluate how
closely the explanation resembles the model’s reasoning. Therefore, also Reasoning-completeness is evaluated since the
‘golden’ reasoning is known and can be compared to the degree of information in the explanation. Instead of a fully
transparent model, Ramamurthy et al. use a random forest and compare the explanation with feature importance
scores output by the forest based on established methodology for these type of models.
Nauta et al.
The Controlled Synthetic Data Check is useful for evaluating explanations for black box models. By designing a
dataset in such a way that with relatively high confidence one could say that predictive model 𝑓reasons in a particular
way, ‘gold’ explanations can be created that follow the data generation process. Subsequently, the agreement of the
generated explanations with these true explanations can be measured. For example, Oramas et al. generate an
artificial image dataset of flowers where the color is the discriminative feature between classes. They subsequently
compare their explanations with the (location of the) discriminative, colored area. Recently, a set of synthetic benchmarks
for XAI was published . The controlled synthetic data check also implies that Reasoning-completeness is evaluated,
but then only on discriminatory feature level and not necessarily how the features are aggregated by the model. An
important prerequisite for the Controlled Synthetic Data Check is that it should be reasonable to assume that the black
box has learned the intended reasoning. Since the predictive model is a black box, reporting the task accuracy of model
𝑓or using other checks on the output is good practice to validate that it is safe to assume that the model has picked up
the intended reasoning.
The correctness of real-valued feature importance scores and heatmaps is often evaluated by removing, perturbing
or masking features from the input and measuring how that affects the (confidence of the) output of predictive model 𝑓.
The Single Deletion method evaluates the change in output when removing or perturbing one feature and compares
that with the explanation’s importance score. In a correct explanation, the explanation’s feature importance score
should be proportional to the shift in output distribution. In other words, the feature with the highest importance score
should also lead to the biggest change in output from 𝑓, and features with a low importance should not result in a
significant change. The Single Deletion method also allows to check for specific properties, such as the “null attribute”
indicating that omitting a feature that has no effect on the output of the model, should have an importance score
of zero . Features can also be removed one by one in an iterative, incremental fashion: Incremental Deletion.
Given the exponential number of possible subsets, features are often removed incrementally in either descending
order (i.e. most important feature first, then top-2 most important features, etc.) or ascending order (least important
or most unimportant first). Due to the high computational costs of iterative removal, some authors (e.g. )
do not evaluate output of 𝑓at each iteration, but only evaluate it for specific subsets, such as removing the top-k
most influential and least influential features. Authors using Incremental Deletion often refer to work of Shrikumar et
al. who calculate the difference in log-odds scores by 𝑓, and Samek et al. who measure the Area over the
Perturbation Curve when perturbing a region in an image. Analyzing the area or steepness of a curve however assumes
that the majority of the importance is placed on only a few features. Although this is typically the case for softmax
scores , there might be cases where this assumption is invalid. It is therefore good practice to compare the curve
with other baselines, such as a random ranking. Instead of starting with the full input and incrementally removing
features, some authors start with an ‘empty’ input and incrementally add features: Incremental Addition (e.g. ).
Incremental Deletion/Addition can include the evaluation of output-completeness. Where correctness evaluates
whether the importance score value is correct, output-completeness evaluates whether the set of important features is
sufficient to explain the output of model 𝑓. For example: an explanation showing one relevant pixel might be correct
but is probably not output-complete, and an explanation showing the full input image is output-complete but the
importance score of pixels is probably incorrect. Output-completeness is evaluated for Incremental Deletion at the
point where all important features are removed. An output-complete explanation should result in a wrong decision
by model 𝑓when all features in the explanation are removed from the input. For Incremental Addition, the output
of the model when only the important features are present should be similar to the output for the full input (see also
Preservation Check and Deletion Check). Besides using Incremental Deletion/Addition to evaluate correctness and
A Systematic Review on Evaluating Explainable AI
output-completeness, compactness can be evaluated by counting how many features of the explanation need to be
removed, perturbed or added in order to change the decision of the predictive model . The
motivation is that an explanation is easier to contemplate at once when this minimal set is small. Figure 7 visualizes the
evaluation curve for Incremental Deletion and the points on this curve where output-completeness and compactness
can be evaluated.
The criticism on Single Deletion and Incremental Deletion is that deletion of features, such as setting them to
zero, can lead to out-of-distribution samples . A solution would be to train a new model on the modified
data. For example, Frye et al. train separate classifiers for different feature subsets, and Hooker et al. remove
the 𝑘% most important features and then retrain the predictive model. This however implies that the correctness
with respect to the original model is not evaluated. Chang et al. solve the out-of-distribution issue by applying
a Generative Adversarial Network (GAN) to fill in deleted features, and Ismail et al. replace masked features
with values from the original data distribution. Recently, it was found that also the shape of a mask could leak class
information to the model and various approaches are presented to circumvent these OoD and shape-leakage
issues, e.g. .
Functionally Evaluating Output-Completeness
The Preservation Check and Deletion Check, following terminology of , evaluate the output-completeness of
the explanation: i.e., does the explanation hold enough information to explain the output of model 𝑓. Explanations
evaluated in this way are usually outcome explanations or model inspections. The methodology is similar to the
Incremental Deletion and Incremental Addition method, but instead of incrementally deleting features, the whole
explanation is removed from the input at once (analogously for preservation). This visualizes a single point on the
incremental deletion curve, as shown in Figure 7. These checks are measured by calculating the change in accuracy
or confidence of the predictive model (hence, output-completeness). Ideally, the accuracy of 𝑓for the Deletion Check
Number of masked features
(ranked from high to low importance)
Prediction
confidence/accuracy
based on masked
Random ranking
Explanation’s
Correctness
Completeness
Classification
decision changes
Compactness
All important features
(according to explanation)
Fig. 7. The evaluation curve for the Incremental Deletion method to evaluate correctness, output-completeness and compactness.
An explanation is output-complete if the model’s decision changes when all important features are removed. Compactness can be
measured at point of output-completeness, or at model’s decision boundary. Explanation is correct when deleting the most important
features leads to the biggest drop in prediction output compared to randomly deleting features.
Nauta et al.
should lead to a significant drop in accuracy whereas the accuracy after the Preservation Check should stay similar.
Nam et al. note that a small change in accuracy can be inevitable due to distortion of the input (such as color and
shape in images) which can result in unpredictable noise that affect the model’s output. It is therefore good practice to
only consider significant changes by comparing with random deletions/preservations.
Fidelity measures the agreement between the output of predictive model 𝑓and the explanation when applied to
the input, and therefore evaluates how well the explanations mimic the output of model 𝑓 . It can be applied
to model explanations and outcome explanations. For example, when a decision tree is trained to generate the same
predictions as a neural network and therefore acts as an explainable surrogate model, the predictions of the decision tree
can be compared with the predictions of the neural network . Fidelity is often defined as the fraction of data samples
for which predictive model 𝑓and an explanation make the same decision, but can also be reported as approximation
error by calculating the average absolute difference or mean squared error (e.g. ). Others use slight variations
on fidelity such as the Kullback–Leibler divergence between the outputs (e.g. ), conditional entropy ,
correlation , likelihood comparison or evaluating how adding explanations to the model would improve
prediction performance . Le et al. also introduced a metric called “influence” which combines fidelity with
information gain and compactness. We emphasize that fidelity is not evaluating correctness, although sometimes
presented as such. Since only the outputs of the model and the explanation are compared, there is no guarantee that the
explanation follows the same reasoning as 𝑓. For example, Anders et al. theoretically showed that for any classifier,
one can always construct another classifier that gives the same output as the original classifier for all data instances,
but has arbitrarily manipulated explanation maps.
When predictive model 𝑓equals explanation method 𝑒, the explanation is a transparent box design (such as decision
rules, decision tree or another white-box model that is explainable and makes the predictions). Hence, fidelity is not
applicable for transparent boxes. Instead, the Predictive Performance of the transparent box model with respect to the
ground-truth task data can be evaluated, such as classification accuracy. In case of 𝑓≠𝑒, the predictive performance of
𝑓is not directly related to the explanation quality and therefore not included as explanation evaluation metric (although
it is generally good practice to report task accuracy and related metrics for evaluating 𝑓, since this can influence
the perceived coherence of the explanation). However, some authors (e.g. ) evaluate output-completeness
by comparing the accuracy of their explanation method with the accuracy of predictive model 𝑓. The decrease in
accuracy then quantifies output-completeness. This approach however does not capture whether 𝑒and 𝑓misclassify
the same test samples. Rather than comparing the predictions of 𝑒and 𝑓with the ground-truth labels, it is therefore
more informative to compare them with each other (fidelity). Another implementation of Predictive Performance is
coverage, which quantifies the fraction of samples to which the explanation applies. The higher the coverage, the more
output-complete the explanation is and the higher the predictive performance can be. In case of outcome explanations,
a set of explanations can be generated for the training set, after which the coverage for this set is evaluated on the test
set. With decision rules for example, coverage would measure the fraction of instances that is classified by at least one
rule in the rule set .
Functionally Evaluating Consistency
Consistency evaluates whether identical inputs have identical explanations. In practice, this can address to what extent
the explanation method is deterministic. Determinism of an explanation method is usually a design choice and therefore
only (implicitly) qualitatively discussed. A quantitative method related to consistency is Implementation Invariance
which states that two models that give the same outputs for all inputs (regardless of their internal implementation)
A Systematic Review on Evaluating Explainable AI
should have the same explanations. Although definitions differ slightly between authors, Tseng et al. evaluated
Implementation Invariance by computing the Jaccard similarity between feature importance scores across random
initializations of the predictive model. Fernando et al. use a special version of Implementation Invariance by
specifically focusing on ‘reference inputs’ for the DeepSHAP explanation method. They explain that a plain black image
is a standard reference input to compute relative importance for image classification, and analyze the sensitivity of
explanations to different reference inputs for retrieval tasks.
Functionally Evaluating Continuity
Continuity addresses the generalizability of explanations and can be measured with the Stability for Slight Variations
method, which measures the similarity between explanations for an original input sample and a slightly different version
of this sample. Alvarez-Melis and Jaakkola , among others, introduced the term stability, but also sensitivity 
and robustness are used. Most authors add a small amount of noise to an original input sample or otherwise
slightly perturb a sample. A few others evaluate stability between two original samples that are similar, e.g. by using
a local neighborhood criterion . Similarity between explanations can be quantified with various metrics, often
dependent on the type of explanation. Examples include rank order correlation , top-k intersection ,
cosine similarity , rule match , normalized distance and the structural similarity index (SSIM) . Nie et
al. show however that one should always check whether the output of the predictive model stays the same for a
slightly perturbed input, before evaluating the similarity of explanations.
Instead of comparing the explanations, others evaluate the Fidelity for Slight Variations by comparing the
predictions of 𝑓for original and slightly perturbed inputs. The reasoning is that an explanation, which should be a
predictive model such that 𝑒= 𝑓, for original input 𝑥should accurately predict the model’s output for a slightly different
sample 𝑥′ . Lakkaraju et al. argue that explanations should have high fidelity on both the original input data
and on slightly shifted input to ensure robustness of the explanations.
Continuity can also be evaluated for counterfactual explanations, which address hypothesized events that did not
occur in reality , to answer questions as “How would the prediction have been if the input had been different?” .
The explanation can be a (generated) data instance that is predicted to belong to a different class , such as ‘your loan
would have been accepted if your income would be 10k higher’. Laugel et al. and Pawelczyk et al. evaluate their
counterfactual explanations by calculating Connectedness. They argue that generated counterfactual samples should
be “justified” , meaning that there is a continuous path from the counterfactual to a sample in the ground-truth
training data. Closely related is requiring that the counterfactual explanation is in proximity of actual instances to
prevent that the explanation is an outlier . Connectedness addresses the lack of robustness and unknown
generalization ability of some predictive models. Requiring connectedness would prevent generating counterfactual
explanations that are based on artifacts which are correct with respect to a model that is not robust, but not near an
instance from the training data. The latter can be undesirable for the Co-12 properties Context and Coherence.
Functionally Evaluating Contrastivity
Contrastivity addresses the discriminativeness of explanations with respect to a ground-truth label or other target.
Therefore the Target Sensitivity evaluation method captures “the intuition that class-specific features highlighted by
an explanation should differ between classes” . Interestingly, we found that Target Sensitivity was only evaluated
for heatmaps. Sixt et al. confirmed the relevance of this evaluation method by proving that some attribution
methods can converge to class-insensitive explanations. Moreover, Adebayo et al. showed that visual inspection can
Nauta et al.
favor plausible heatmaps which are not target-sensitive to the underlying reasoning of the model. They illustrated
this by revealing the high similarity between edge detectors for image data and explanatory saliency maps. Ideally,
explanations are target-sensitive and should differ between targets and, in case of image data, should not be static edge
detectors . This can be checked by comparing explanations for different targets or output logits 
or explanations before and after an adversarial attack . An adversarial attack fools the predictive model
𝑓such that it makes a different prediction for a slightly perturbed input. A different prediction should then also
lead to a different explanation. Target Sensitivity can be measured with L1, L2 or Hamming distance between the
explanations , histogram intersection or the structural similarity index measure (SSIM) between
two heatmaps . In all these cases, a large difference between the explanations is desired. Wagner et al. go a
step further and argue that explanations should be empty for targets for which there is no evidence in the input sample
(e.g. a class that is visually not present in an image). They generate an explanation for the least likely class and compute
the fraction of explanations that was not empty .
Another desirable property regarding contrastivity is high Target Discriminativeness, since that implies a good
“informativeness for a downstream prediction task” . To evaluate the target-discriminativeness, either an external
classifier is trained on predicting the right target given the explanation , or a cluster method is applied on
the explanations . Explanations in these papers are often interpretable representations or text. The performance
of the classifier or clustering method is evaluated against ground-truth targets. Kindermans et al. evaluate Target
Discriminativeness slightly different by evaluating how much of the target can be reconstructed when the explanation
is removed from the input.
The Data Randomization Check, introduced by Adebayo et al. , is a sanity check for the “sensitivity of an
explanation method to the relationship between instances and labels”, and has the advantage that it is model-agnostic.
Ideally, explanations explain the mapping between input and output that the predictive model has learned. If a model
is successfully trained on a dataset with random labels, which has been shown to be possible with deep neural
networks , it has learned the data generation process by memorizing the random labels. The accuracy on an
unseen test set will never be better than random guessing . Therefore, for a given test sample, the explanation of a
model trained on randomized data should be different from an explanation of a model trained on the original dataset.
Randomizing the underlying data generation process changes the target function the model is trying to learn, which
should imply a change in the explanation.
Functionally Evaluating Covariate Complexity
Covariate complexity is concerned with the use of human-understandable concepts to explain features (i.e. covariates)
and their interactions. It can be qualitatively addressed by motivating a design choice, such as using a bag of words
instead of uninterpretable word embeddings as covariates , applying element-wise feature mappings which are
claimed to be more interpretable or selecting a predictive model that satisfies a monotonicity requirement .
A quantitative method to evaluate covariate complexity is measuring Covariate Homogeneity. Generally, this
implies evaluating how consistently a covariate represents a predefined human-interpretable concept. The exact
implementation mainly depends on the type of explanation and type of data. For example, in case of image data,
the human-interpretable concepts can be labeled object parts (e.g. legs, beak and tail of an animal) such that the
interpretability of a cluster can be evaluated by checking whether it consistently represents the same part semantics
for different images. Given an annotated dataset with predefined concepts (such as object parts), the Intersection
over Union or distance between learned covariates (e.g. prototypes) and interpretable
A Systematic Review on Evaluating Explainable AI
concepts can be calculated. Zheng et al. evaluate the predictive power of their features for predicting human
generated features, and Fyshe et al. measure the distance from their learned representations to a semantic groundtruth provided by humans. In case of clusters, the purity of a cluster w.r.t. ground-truth labels can be measured .
Homogeneity is also relevant for the explanation type ‘Disentanglement’. Such explanation methods aim to learn
a latent representation that is disentangled such that each dimension corresponds to an interpretable concept, after
which these representations can be used in downstream tasks. Covariate Homogeneity evaluates in this case the
extent to which each dimension corresponds to exactly one (interpretable) concept, factor or attribute. Examples of
such interpretable factors include color of an object in an image, or the presence of a smile on a face . This can be
quantified by measuring the mutual information between a dimension and a concept . Ideally, a dimension has high
mutual information with a single concept and zero mutual information with all other concepts. Others vary exactly
one dimension and keep all others fixed. Then, the output variance per concept can be measured , or the accuracy
of a classifier that should predict the index of the specific factor of variation (ceteris paribus) . Another approach,
originally suggested by , is to generate data while keeping exactly one covariate fixed and varying the others, and
evaluate whether the variance in one dimension is exactly zero .
Another method is a specific metric to evaluate the Covariate Regularity of explanations. Yu and Varshney 
argue that a decision rule is easier to memorize if it is less entropic and therefore measure the Shannon entropy of a
rule’s feature distribution. Tseng et al. measure the Shannon entropy of feature importance scores in order to
indicate how noisy the feature attributions are.
Functionally Evaluating Compactness
Many authors have evaluated the compactness of their explanations by measuring their Size (absolute) or sparsity
(relative), since explanation should not overwhelm a user. The implemented metric usually depends on the type of data
and type of explanation. Examples include: the number of features in an explanation (e.g. ),
average path length in a decision tree (e.g. ), reduction w.r.t. complete data sample (e.g. ), or
the number of decision rules in a set (e.g. ). Additionally, some evaluate the Redundancy
of their explanations. A lower overlap of information within the explanation would signify higher interpretability.
Redundancy can be measured with information gain , or the overlap ratio .
In case of counterfactual explanations, compactness can be evaluated by measuring Counterfactual Compactness.
A counterfactual explanation, usually an outcome explanation, shows what should change in the input in order to change
the corresponding prediction of model 𝑓, and therefore also addresses Contrastivity. Counterfactual Compactness
quantifies how much needs to be changed for a different outcome. Generally, as few changes as possible are desired
to generate a compact counterfactual explanation. Counterfactual Compactness can be quantified by measuring the
distance between the input and counterfactual explanation or the number of transformations that are
required such as the number of features that need to be changed. Note that a counterfactual explanation with
a distance of 0 implies that the contrastivity (and specifically the target sensitivity) is also 0: apparently nothing needs
to be changed in the explanation for a different prediction.
Functionally Evaluating Composition
The Co-12 property Composition describes the format and organization of an explanation and focuses on how something
is explained and presented. Composition is often qualitatively discussed by claiming that the predictive model has an
Nauta et al.
interpretable architecture, or by showing anecdotal evidence, for example by concluding that introduced heatmaps are
“the most crisp” , or by making specific design choices such as considering the colors used in explanations to make
them easier to analyze . Composition can also be evaluated with users, as shown in Supplementary Material. We
identified one functionally-grounded quantitative evaluation method which we term Perceptual Realism. Perceptual
Realism for images can be evaluated by using Fréchet Inception Distance (FID) scores to evaluate the quality
of generated images. FID measures the similarity between generated images and real, original images and has been
shown to be consistent with human judgment . FID scores are generally used to evaluate Representation Synthesis
explanations generated by GANs . For textual explanations, composition is in most cases indirectly evaluated
by measuring standard metrics such as BLEU and ROUGE with respect to a ground-truth explanation (as
incorporated in the ‘Alignment with Domain Knowledge’ evaluation method). However, the perplexity metric as used
in e.g. does not need a reference input and therefore evaluates the quality of a text in a similar fashion as the FID
scores for images.
Functionally Evaluating Confidence
To check whether the explanation contains probability or uncertainty information, authors usually make a design
choice whether their XAI method will contain a confidence measure regarding the output of model 𝑓or the likelihood
of the explanation. Only two papers in our set of included papers explicitly evaluated their confidence information.
Schwab and Karlen introduce a feature importance method that produces uncertainty estimates. They assess
the Confidence Accuracy of the uncertainty estimates by measuring their correlation with ground-truth changes in
outputs when masking features in held-out test samples. As additional baseline, they compare with random uncertainty
estimates. Ghalwash et al. evaluate their uncertainty estimates for time series classification by analyzing the
correlation between uncertainty thresholds and the model’s accuracy, and additionally evaluate how the uncertainty
evolves over time.
Functionally Evaluating Context
The Co-12 property Context takes the user and their needs into account. Naturally, this is evaluated with user studies
(as also shown in Supplementary Material). However, we identified two quantitative evaluation methods where user
studies are not necessary. The first method is Pragmatism, which quantifies, based on domain knowledge, the degree
of difficulty for an individual to act upon the suggestions in a counterfactual explanation . The underlying
intuition is that there might exist various counterfactual explanations that show what should be changed in order to get
a different prediction from model 𝑓. Whereas the Counterfactual Compactness method evaluates the compactness of
this counterfactual with respect to the original input, the Pragmatism method also takes the user’s context into account.
A cost per feature quantifies the degree of difficulty for a user to change that feature. This cost can also be infinite: e.g.,
a person cannot get younger so a counterfactual explanation showing that the user should decrease their age is not
actionable and hence not pragmatic. In addition to the cost per feature, also the degree or cost of a feature change could
be taken into account, related to Compactness. Rawal and Lakkaraju explain that it is probably easier for a user to
increase income by 5K than 50K.
The second method involves Simulated User Studies. Ribeiro et al. perform two experiments to evaluate
whether users would 1) trust predictions and 2) be able to use explanations as model selection method. In their first
experiment, they define some input features as being “untrustworthy” and assume that end users do not want such
features to be used by the predictive model. They evaluate whether the prediction changes when all untrustworthy
A Systematic Review on Evaluating Explainable AI
features are removed from the explanation. Their second experiment trains two predictive models that have a similar
validation accuracy but one performs worse on the test set. They evaluate whether their explanation method can be
used to identify the better model by revealing artificially introduced spurious correlations. Singla et al. evaluate in
a similar manner the performance of their explanation method in identifying biases in data.
Functionally Evaluating Coherence
To quantitatively evaluate whether explanations generated by an XAI method align with domain knowledge, general
beliefs and consensus, they are often compared with some expected explanations framed as ‘ground-truth’, which
we call Alignment with Domain Knowledge. This ground-truth is contained in an annotated dataset. For imaging
data, often ‘location coherence’ is evaluated by comparing a heatmap or localization explanation with ground-truth
object bounding boxes, segmentation masks, landmarks or human attention maps. Correspondence between the
ground-truth and the explanation can then be quantified with the Intersection over Union (also known as Jaccard index,
e.g. ), outside-inside relevance ratio (e.g. ), point localization error (e.g. ),
pointing game accuracy (whether a point falls into a ground-truth region, e.g. ) or rank correlation with
human attention maps . For textual explanations, standard natural language generation metrics such as
ROUGE and BLEU are often used to evaluate the overlap of the generated explanations with a ground-truth
text, e.g. . Indirectly, these metrics also evaluate composition since
it has been shown that ROUGE and BLEU correlate with human judgments on fluency of a text . For real-valued
explanations such as feature importance, one could measure (rank) correlation between the generated explanation and
ground-truth annotation in the dataset (e.g. ). We would like
to highlight that some common correlation metrics were criticized, such as Kendall’s tau which could be misleading at
the tail end of distributions and Spearman correlation has limitations for global rankings .
As alternative to evaluating the Alignment with Domain Knowledge, for example when a ground-truth is not
available, one could evaluate coherence by calculating the XAI Methods Agreement. Existing XAI methods, which
are already established within the community and/or proven to adhere to certain desirable properties, are then usually
considered as ground-truth, and those explanations are compared with the explanations from other methods.
For both Alignment with Domain Knowledge and XAI Methods Agreement, we would like to emphasize that
these methods only evaluate Coherence with respect to expectations, and not Correctness with respect to the predictive
model 𝑓. Hence, a coherent explanation could be incorrect and vice versa. For example, an explanation highlighting
snow in the background to distinguish between a husky and a wolf (example from Ribeiro et al. ), would score low
on location coherence w.r.t. object segmentation masks, but is correctly showing the reasoning of this bad classifier. It
is therefore good practice to evaluate multiple Co-12 properties and specifically evaluate correctness and coherence
independently, as also discussed in Section 2.1.
Functionally Evaluating Controllability
Controllability addresses the interactivity of explanations, which is applicable to e.g. conversational explanation
methods , interactive interfaces , human-in-the-loop explanation learning methods (e.g. as ) or
methods that enable the user to correct explanations . We note that XAI methods can also have (hyper)parameters
to tune the explanations, such as a regularizer for explanation size, but we do not consider such parameters as being
evaluation methods for Controllability. The evaluation of Controllability is usually qualitative by discussing why the
controllable format improves the quality of the explanations, or by only showing an example of the Controllability.
Nauta et al.
We identified two papers that quantified Controllability by measuring the improvement of explanation quality after
human feedback: Human Feedback Impact. Chen et al. and Dong et al. measure the accuracy of their
textual explanations after iterative user feedback. Although users are involved in this evaluation method, it is not a
standard user study since the user is seen as a system component: the XAI methods use optimization criteria that require
humans-in-the-loop for optimal output. Additionally, Chen et al. define the “Concept-level feedback Satisfaction
Ratio” which measures whether concepts for which a user has indicated to be interested in are present in the explanation,
and whether concepts where the user is not interested in are removed from the explanation. This Satisfaction Ratio
does not require direct user feedback, and could also be applied to an existing dataset with user interests.
IMPLICATIONS AND RESEARCH OPPORTUNITIES
We strongly believe that explainable AI has great potential: XAI can justify algorithmic decisions, XAI can allow users
to control and improve systems by identifying and correcting errors, and XAI can contribute to knowledge discovery by
revealing learned patterns . However, to reach this full potential, XAI methods should be extensively validated
in order to ensure that they are reliable and useful. Our analysis has shown that the field has been maturing the last
few years, but at the same time we also see that explainability is still often presented as a binary property. We argue
that explainability is a multi-faceted concept and make this explicit with our Co-12 properties describing different
conceptual aspects of explanation quality. Instead of evaluating only one property, it is essential to get insight into,
and preferably quantify, all properties such that an informed trade-off can be made. “Best is not directly a judgment of
truth but instead a summary judgment of accessible explanatory virtues” . In practice, such a multi-dimensional
overview could be implemented as a radar chart or as a set of consumer labels as proposed by Seifert et al. that
comprehensively and concisely conveys the strengths and weaknesses of the explanation or explanation method. Our
collection of identified evaluation methods also shows that quantitative evaluation methods exist for each of the
Co-12 properties. On the other hand, our analysis reveals that the majority of XAI evaluation focused on evaluating
Coherence, Completeness, Compactness or Correctness. We hope that our collection of evaluation methods will stimulate
and facilitate a more complete and inclusive evaluation in order to objectively validate and compare new and
existing XAI methods. Eventually, we are convinced that XAI methods should be kept to minimal standards, similarly as
such standards exist for predictive models. Our overview of evaluation methods provides researchers and practitioners
with concrete tools to evaluate every Co-12 property while using unified terminology, and therefore contributes to
standardization. We also see a research opportunity to develop new evaluation methods for Co-12 properties that are
currently insufficiently addressed, and to develop variants on existing evaluation methods to make them suited for
different types of data and explanations.
Besides, we acknowledge that it might be unreasonable to expect an XAI method to score well on all Co-12 properties.
In practice, trade-offs between desired explanation properties will have to be made when developing an XAI
method. Coherence might contradict with Correctness, as discussed in Section 2.1, and Completeness and Compactness
might be considered diametrical opposites. The application domain or practical feasibility can determine which Co-12
properties should be emphasized. Herman proposes to optimize explanations for content-related properties first
(correctness and completeness in particular), without making effort to simplify the explanation. “This separation of
concern encourages more rapid innovation and reduces the cost of evaluation” . Subsequently, a second step
can consist of altering the explanation to “incorporate human cognitive function, user preferences, and expertise into
the explanation” . Eventually, any trade-off can be made as long as it is sufficiently motivated. Insights from
other research areas, such as social sciences, psychology and HCI, can also provide the XAI community with more
A Systematic Review on Evaluating Explainable AI
guidance regarding what aspects of an explanation are important to evaluate. Combining strengths in multi-disciplinary
collaborations can subsequently result in innovative XAI evaluation methods.
Additionally, we think that our collected set of evaluation methods can not only be used for thorough evaluation, but
also for multi-dimensional optimization of interpretability. Some papers already optimize for interpretability by
using a regularization term or objective function during training of the predictive model (e.g. ) or by relating rewards to explainability and presentation quality in reinforcement learning (e.g. ).
However, in practice these interpretability optimizers usually involve only one to two Co-12 properties. Interestingly,
some interpretability regularizers are not (yet) used as evaluation metric although their quantitative nature would make
them suited to be used as XAI evaluation method as well. For example, Park et al. optimize for interpretability
with spatial auto-correlation, which we have not seen as evaluation metric but might be suited for evaluating heatmaps.
This shows that the optimization and quantitative evaluation of explanation methods are closely related. We recognize
a research opportunity to study how evaluation methods can be incorporated in the training process of predictive
models, in order to tune the so-called “accuracy-interpretability trade-off” during training instead of only analyzing it
afterwards. Also the quantitative evaluation methods where user involvement is required can be used for optimizing an
interpretable model by adopting a human-in-the-loop approach (as done in e.g. ).
Lastly, we believe that our annotated dataset containing the categorization of 312 XAI papers (such as type of data
and explanation, as shown in Fig. 2) is a rich source of information and can be a useful starting point for more in-depth
research. Our dataset is therefore publicly available at such that others
can efficiently collect XAI papers that adhere to specific criteria such that subtopics can be analyzed in more detail.
Acknowledgments. We would like to thank Ziekenhuis Groep Twente (ZGT) for supporting this project, and Marion
Koelle for the useful suggestions to improve the structure of this work.
Nauta et al.
SUPPLEMENTARY MATERIAL
Supplementary material presenting quantitative evaluation methods with user studies (Table 5), providing a more
detailed discussion on the extent and nature of XAI research, and describing our paper collection and reviewing process
in more detail.
Quantitative Evaluation with User Studies
Whereas the main paper focuses on automated quantitative evaluation methods, Table 5 summarizes the main quantitative evaluation methods we identified that were applied in user studies. Generally, we can distinguish between
subjective evaluation and objective evaluation. The subjective methods usually evaluate Coherence by measuring how
a user perceives an explanation. In contrast, the Forward Simulatability, Teaching Ability, Intruder Detection and
Synthetic Artifact Rediscovery are objective evaluation methods. The Forward Simulatability method is comparable
to the functionally-grounded Preservation Check: instead of evaluating whether the predictive model gives the right
output given the explanation, the user acts as a surrogate model to evaluate whether the explanation is output-complete
for a user. The Teaching Ability is a related approach but also evaluates whether the explanations are generalizable
such that a user can, after being trained with explanations, make a correct prediction without having an explanation.
We refer to other work (e.g. ) for a more detailed discussion on evaluation with human subjects.
Categorization and Analysis of XAI Methods
Figure 5 visualizes the categorization of papers per dimension. Generally, it can be seen that the top-3 in each dimension
covers the majority of the papers. Especially the imbalance regarding the types of models to be explained is striking,
since a large majority of the literature (75%) focuses on explaining neural networks, which could be due to their black
box nature and state-of-the-art performance. The ‘other’ category follows with 17%, which usually involves models
which are introduced in the paper and which are specifically designed to explain a certain prediction task. 15% of the
papers are XAI methods which can, according to the authors, be applied to any predictive model. This model-agnostic
category includes for example methods that explain a latent representation which can come from any model.
Our analysis also shows that there is a high diversity in explanation types. Interesting is that the top-3 explanation
types are dominated by feature importance methods: 27% of the papers use standard feature importance scores, followed
by heatmaps (2-dimensional feature importance) and localization (binary feature importance). The types of data that are
given as input to predictive model 𝑓are also diverse, although images, text and tabular data are most used. Additionally,
we found that image data make up the majority for heatmap explanations whereas textual input is mostly used for
textual explanations. Feature importance seems to be the most generally applicable explanation type since it is used for
all data types, and is also often used for ‘Other’ data types that do not fall into the predefined categories.
We also categorized the main task of the predictive models. These statistics might be highly influenced by our
selection of publication venues and also the difficulty of a task can play a role. A great majority (63%) of the papers
presents a model for classification. Besides the fact that classification is a broad concept, it can also be related with
the popularity of outcome explanations: explaining a particular classification decision can be a good use case for
outcome explanations. As shown in Figure 5, the type of problem that is addressed in 64% of the papers is the outcome
explanation, meaning that an explanation aims to explain a single prediction. In contrast, the second most addressed
type, model inspection (30% of the papers), gives global explanations about some property of predictive model 𝑓, such
as global feature importance. This can imply that users or model developers are more interested in understanding
A Systematic Review on Evaluating Explainable AI
Table 5. Descriptions of quantitative evaluation methods with user studies, with references to papers that apply this method. Bold
check mark indicates prominent Co-12 property.
Name and Description of Quantitative Metric, with References
Correctness
Output-completeness
Consistency
Continuity
Contrastivity
Covariate Complexity
Compactness
Composition
Confidence
Controllability
Forward Simulatability
Given an explanation (and possibly the corresponding input sample), ask users to guess
or identify the model’s prediction (human-output-completeness). Additionally, the user’s
prediction speed can be measured, or the difference in simulation accuracy between whether
or not explanations are shown. 
Teaching Ability
Train users with explanations to understand the model’s reasoning, after which humans
should predict the ground-truth for a new data instance without having an explanation.
Additionally, the user’s prediction speed can be measured. 
Subjective Satisfaction
Ask users to rate explanations on properties such as satisfaction, reasonableness, usefulness,
fluency, relevance, sufficiency and trust. 
Subjective Comparison
Show users explanations from different XAI methods (or explanations from humans) and
evaluate which method is perceived as being better (in terms of e.g. perceived accuracy,
usefulness or understandability). 
Perceived Homogeneity
Ask users to evaluate the purity or disentanglement of explanations, by e.g. verifying that a
dimension corresponds to a single interpretable factor. 
Intruder Detection
Given an explanatory prototype or disentangled concept, show users a set of instances of
which one is an intruder, and ask which instance does not correspond with the explanation.
 
Synthetic Artifact Rediscovery
A controlled experiment where a property of the predictive model is changed, after which
it is evaluated whether humans can reveal this property with the help of explanations.
 
specific decisions than getting global insights, or that global XAI methods are more difficult to develop. The transparent
box designs are by themselves already interpretable, such as a decision tree. Few papers present a model explanation,
meaning that a second, interpretable model is learned to mimic the output of the black box. We hypothesize that the
model explanation is not often addressed since there is no guarantee that the original black box and the interpretable
surrogate model agree on their internal reasoning . Hence, the resulting explanation could be incorrect with
respect to the workings of the original predictive model. Interesting to add is that the outcome explanation is also the
dominant problem for papers that do not introduce, but apply or evaluate an existing XAI method. Specifically, 82% of
the papers excluded by the filter focus on outcome explanations. This might indicate that outcome explanations are
easier to apply and compare among different XAI methods than global explanations.
Nauta et al.
% of included papers (with filter)
Includes User Study
Exclusively Anecdotal Evidence
Fig. 8. Fraction of papers per venue that introduce an XAI method and evaluate with user studies (blue) or exclusively with anecdotal
evidence (orange). The percentages are based on the total number of included papers per venue that introduce an XAI method.
Lastly, we see that the type of method used to explain is dominated by two types: post-hoc explanation methods that
aim to explain an already trained model, and interpretability built into the predictive model. Built-in interpretability is a
broad category ranging from models that are intrinsically interpretable to additions or restrictions to the predictive model
architecture. The latter includes attention mechanisms, regularizers in the training process to improve interpretability,
or a combined architecture that merges the prediction and explanation task. A minority produces explanations based
on supervised explanation training, such as optimizing the XAI method to generate explanations that are similar to
ground-truth explanations from a dataset. Interesting to note is that all supervised explanation training methods in our
dataset produce outcome explanations.
XAI Evaluation Practice per Venue. Figure 8 provides more insight into the XAI evaluation practices per venue. It
shows that generally the more application-oriented conferences evaluate only with anecdotal evidence in roughly half
of the cases. In contrast, the more theoretical venues have percentages around 20%-30%. We could not see a clear trend
regarding the usage of user studies, although the relative differences between conferences is striking.
Methodology
This section describes in more detail our paper collection process, inclusion and exclusion criteria, and reviewing
Identification of Paper Candidates. We collected papers in a structured manner to provide both quantitative and
qualitative insights about the XAI domain based on a large corpus of scientific work on XAI evaluation methods. Since
the literature on XAI is highly diverse and distributed across different (sub)disciplines, we selected literature by filtering
on publication venue, date and title.
Publication Venue. To obtain a representative and sufficiently large, yet feasible selection of papers, we considered
literature from all areas of AI, ranging from Computer Vision and Information Retrieval to Natural Language Processing
A Systematic Review on Evaluating Explainable AI
and Data Mining. Specifically, we considered literature from the following twelve prominent3 conferences: AAAI,
IJCAI, NeurIPS (formerly NIPS), ICML, ICLR, CVPR, ICCV, ACL, WWW, ICDM, SIGKDD (also called KDD), SIGIR. This
selection criterion also implies that all work included in the set is original, peer-reviewed and written in English. We
are aware of the fact that we exclude relevant papers published at other venues, but do believe that our selection of
venues is sufficiently representative to enable extrapolation of our results and conclusions to XAI literature as a whole.
Publication Year. We scoped our selection to work published from 2014 to 2020. This criterion is motivated by the
fact that XAI has gained renewed interest since the emergence of deep learning, and the fact that annual international
conference series dedicated exclusively to explainability or interpretability were organized from 2014 onwards .
Keywords in Title. We conducted a keyword search in publication titles with the following search query: explain*
OR explanat* OR interpret* to capture terms including explainable, explaining, explanation, interpretable and
interpretability. We are aware of the fact that this query excludes papers with related terms (such as intelligibility and
transparency), and papers that specify a specific explanation method (such as feature attribution). However, to reflect
time and resource constraints, we aimed for high precision instead of high recall. For a similar reason, we did not
consider keywords in abstracts or full-texts, since we found that searching for general terms as explain leads to a surge
in irrelevant results.
Final Search Query. We used the search engine of computer science bibliography DBLP4 to collect the initial
selection of papers. Combining the aforementioned criteria results in the following query to DBLP:
explain | explanat | interpret year:2020: | year:2019: | year:2018: | year:2017: | year:2016: | year:2015: |
year:2014: venue:ICDM: | venue:KDD: | venue:NIPS: | venue:NeurIPS: | venue:CVPR: | venue:ICCV: | venue:AAAI:
| venue:IJCAI: | venue:SIGIR: | venue:ACL: | venue:WWW: | venue:ICLR: | venue:ICML:
This search, conducted on 4th of May 2021, resulted in 606 papers.
Inclusion and Exclusion. Before screening the main content of each paper according to inclusion criteria, we
applied an exclusion criterion since we found that the search result from the query contained more than the main
conference papers.
Exclusion. We manually excluded companion papers, which include extended abstracts and papers from workshops,
doctoral consortium and early career tracks, invited talks, senior member presentations, demonstrations, companion
proceedings, challenges and tutorials. Applying this exclusion criterion to the initial query result resulted in 494 papers.
Inclusion. The resulting 494 papers are screened according to an inclusion criterion, in order to only include relevant
papers in our analysis. With our inclusion criterion, we focus on papers in the explainable AI domain and therefore
exclude papers that use the terms “explain” or “interpret” in other contexts.
Inclusion Criterion
Original work introducing, applying and/or evaluating one or more methods for explaining a machine
learning model.
With “introducing”, we mean that the work presents a new method for explaining a machine learning model. The term
“machine learning” implies learning from data. Since we require that this machine learning model should be explained
(see the main paper for our definition of ‘explanation’), we do not include papers that only explain the data rather than
explaining how a predictive model does something.
3As indicated by their A* ranking according to the CORE 2021 rankings portal ( 
4 
Nauta et al.
Applying the inclusion criteria to the set of 494 papers, led to 361 papers being included. Subsequently, we can
apply a filter that only selects the papers that introduce an XAI method, resulting in 312 papers. We apply this filter
to analyze how introduced XAI methods are evaluated when they are first presented. For collecting all evaluation
methods, we review all 361 included papers since 49 papers do not introduce a new XAI method, but could contain
relevant evaluation metrics to compare and evaluate existing XAI methods. We do not want such papers to skew our
quantitative results on XAI methods (Figure 5), but include them in our evaluation overview for completeness.
Inclusion and Reviewing Process. Screening papers for inclusion and reviewing them according to a review
protocol was a collaborative task. All authors have a background in machine learning and have explainable AI as
research interest. Each author reviewed papers individually but there was frequent communication within the team
to align and verify inclusion and categorization decisions. For 81 papers, the inclusion criteria were checked by two
reviewers in order to measure the inter-rater agreement for quality assurance. Besides a random sample of papers
that were reviewed twice, the majority in this set were papers where the initial reviewer indicated that they were
not confident about the decision, after which another reviewer checked the paper. We therefore emphasize that the
following agreement metrics can be biased towards lower scores due to the perceived difficulty of the papers in the
specific subset. The two reviewers were in agreement on the inclusion decision for 68 out of 81 papers, resulting in a
Cohen’s kappa 𝜅= 0.625 and a Matthews correlation coefficient 𝑀𝐶𝐶= 0.647 (the latter is said to be better suited for
binary and imbalanced data ). These results indicate substantial agreement . In case of disagreement or low
confidence by both reviewers, discussion took place to come to a final inclusion decision. In addition to disagreement,
also more informal discussion took place whenever a reviewer was in doubt on aspects of the review protocol. After
reviewing, the first author did an extra check regarding the categorization of evaluation methods of all included papers.
A Systematic Review on Evaluating Explainable AI