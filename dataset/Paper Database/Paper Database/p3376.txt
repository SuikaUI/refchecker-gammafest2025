LadaBERT: Lightweight Adaptation of BERT
through Hybrid Model Compression
Yihuan Mao1,∗, Yujing Wang2,3,†, Chufan Wu1, Chen Zhang2, Yang Wang2
Quanlu Zhang2, Yaming Yang2, Yunhai Tong3, Jing Bai2
1Tsinghua University
2Microsoft Research Asia
3Key Laboratory of Machine Perception, MOE, School of EECS, Peking University
 , {yujwang,yhtong}@pku.edu.cn, 
{yujwang,zhac,t-yangwa,yayaming,quzha,jbai}@microsoft.com
BERT is a cutting-edge language representation model pre-trained by a large corpus, which
achieves superior performances on various natural language understanding tasks. However, a
major blocking issue of applying BERT to online services is that it is memory-intensive and
leads to unsatisfactory latency of user requests. Existing solutions leverage knowledge distillation frameworks to learn smaller models that imitate the behaviors of BERT. However, the training procedure of knowledge distillation is expensive itself as it requires sufﬁcient training data
to imitate the teacher model. In this paper, we address this issue by proposing a hybrid solution
named LadaBERT (Lightweight adaptation of BERT through hybrid model compression), which
combines the advantages of different model compression methods, including weight pruning,
matrix factorization and knowledge distillation. LadaBERT achieves state-of-the-art accuracy on
various public datasets while the training overheads can be reduced by an order of magnitude.
Introduction
The pre-trained language model, BERT has led to a big breakthrough in various
kinds of natural language understanding tasks. Ideally, people can start from a pre-trained BERT checkpoint and ﬁne-tune it on a speciﬁc downstream task. However, the original BERT models are memoryexhaustive and latency-prohibitive to be served in embedded devices or CPU-based online environments.
As the memory and latency constraints vary in different scenarios, the pre-trained BERT model should be
adaptive to different requirements with accuracy retained to the largest extent. Existing BERT-oriented
model compression solutions largely depend on knowledge distillation , which is
inefﬁcient and resource-consuming because a large training corpus is required to learn the behaviors of
a teacher. For example, DistilBERT is re-trained on the same corpus as pre-training
a vanilla BERT from scratch; and TinyBERT utilizes expensive data augmentation
to ﬁt the distillation target. The costs of these model compression methods are as large as pre-training,
which are unaffordable for low-resource settings. Therefore, it is straight-forward to ask, can we design a
lightweight method to generate adaptive models with comparable accuracy using signiﬁcantly less time
and resource consumption?
In this paper, we propose LadaBERT (Lightweight adaptation of BERT through hybrid model compression) to tackle this problem. Speciﬁcally, LadaBERT is based on an iterative hybrid model compression framework consisting of weighting pruning, matrix factorization and knowledge distillation.
Initially, the architecture and weights of student model are inherited from the BERT teacher. In each
iteration, the student model is ﬁrst compressed by a small ratio based on weight pruning and matrix
factorization, and is then ﬁne-tuned under the guidance of teacher model through knowledge distillation.
Because weight pruning and matrix factorization help to generate better initial and intermediate status
for knowledge distillation, both accuracy and efﬁciency can be greatly improved.
We conduct extensive experiments on ﬁve public datasets of natural language understanding. As an
example, the performance comparison of LadaBERT and state-of-the-art models on MNLI-m dataset is
illustrated in Figure 1.
∗The work was done when the author visited Microsoft Research Asia.
†Corresponding Author
 
Figure 1: Accuracy comparison on MNLI-m dataset
We can see that LadaBERT outperforms other
BERT-oriented model compression baselines
at various model compression ratios.
Especially, LadaBERT outperforms BERT-PKD signiﬁcantly under 2.5× compression ratio and outperforms TinyBERT under 7.5× compression
ratio while the training speed is accelerated by
an order of magnitude.
The rest of this paper is organized as follows.
First, we summarize the related works of model
compression and their applications to BERT in
Section 2. Then, the methodology of LadaBERT
is introduced in Section 3, and experimental results are presented in Section 4. At last, we conclude this work and discuss future works in Section 5.
Related Work
Deep Neural Networks (DNNs) have achieved great success in many areas in recent years, but the memory consumption and computational cost expand greatly with the growing complexity of models. Thus,
model compression has become an indispensable technique in practice, especially for low-resource scenarios. Here we review the current progresses of model compression techniques brieﬂy, and present their
application to pre-trained BERT models.
Model compression algorithms
Existing model compression algorithms can be divided into four categories, namely weight pruning,
matrix factorization, weight quantization and knowledge distillation.
Numerous researches have shown that removing a large portion of connections or neurons does not
cause signiﬁcant performance drop in deep neural networks. For example, Han et al. proposed
a method to reduce the storage and computation of neural networks by removing unimportant connections, resulting in sparse networks without affecting the model accuracy. Li et al. presented an
acceleration method for convolution neural network by pruning whole ﬁlters together with their connecting ﬁlter maps. This approach does not generate sparse connectivity patterns and brings a much larger
acceleration ratio with existing BLAS libraries for dense matrix multiplications.
Matrix factorization was also widely studied in the deep learning domain, the goal of which is to
decompose a matrix into the product of two matrices in lower dimensions. Sainath et al explored
a low-rank matrix factorization method of DNN layers for acoustic modeling. Xu et al. 
applied singular value decomposition to deep neural network acoustic models and achieved comparable
performances with state-of-the-art models through much fewer parameters. GroupReduce focused on the compression of neural language models and applied low-rank matrix approximation
to vocabulary-partition. Winata et al. carried out experiments for low-rank matrix factorization
on different NLP tasks and demonstrated that it was more effective in general than weight pruning.
Weight quantization is another common technique for compressing deep neural networks, which aims
to reduce the number of bits to represent every weight in the model. With weight quantization, the
weights can be reduced to at most 1-bit binary value from 32-bits ﬂoating point numbers. Zhou et
al. showed that quantizing weights to 8-bits does not hurt the performance; Binarized Neural
Networks contained binary weights and activations of only one bit; and Incremental
Network Quantization converted a pre-trained full-precision neural network into lowprecision counterpart through three interdependent operations: weight partition, groupwise quantization
and re-training.
Knowledge distillation trains a compact and smaller model to approximate the
Figure 2: Overview of LadaBERT framework
function learned by a large and complex model. A preliminary step of knowledge distillation is to
train a deep network (the teacher model) that automatically generates soft labels for training instances.
This “synthetic” label is then used to train a smaller network (the student model), which assimilates the
function learned by the teacher model. Chen et al. successfully applied knowledge distillation to
object detection tasks by introducing several modiﬁcations, including a weighted cross-entropy loss, a
teacher bounded loss, and adaptation layers to model intermediate teacher distributions. Li et al. 
developed a framework to learn from noisy labels, where the knowledge learned from a clean dataset and
semantic knowledge graph were leveraged to correct the wrong labels.
To improve the performance of model compression, there are also numerous attempts to develop hybrid model compression methods that combine more than one category of algorithms. Han et al. 
combined quantization, hamming coding and weight pruning to conduct model compression on image
classiﬁcation tasks. Yu et al. proposed a uniﬁed framework for low-rank and sparse decomposition of weight matrices with feature map reconstructions. Polino et al. advocated a combination of
distillation and quantization techniques and proposed two hybrid models, i.e., quantiﬁed distillation and
differentiable quantization. Li et al., compressed DNN-based acoustic models through knowledge
distillation and pruning.
BERT model compression
In the natural language processing community, there is a growing interest recently to study BERToriented model compression for shipping its performance gain into latency-critical or low-resource scenarios. Most existing works focus on knowledge distillation. For instance, BERT-PKD 
is a patient knowledge distillation approach that compresses the original BERT model into a lightweight
shallow network. Different from traditional knowledge distillation methods, BERT-PKD enables an
exploitation of rich information in the teacher’s hidden layers by utilizing a layer-wise distillation constraint. DistillBERT pre-trains a smaller general-purpose language model on the
same corpus as vanilla BERT. Distilled BiLSTM adopts a single-layer BiLSTM as
the student model and achieves comparable results with ELMo through much fewer
parameters and less inference time. TinyBERT exploits a novel attention-based distillation schema that encourages the linguistic knowledge in teacher to be well transferred into the student
model. It adopts a two-stage learning framework, including general distillation (pre-training from scratch
via distillation loss) and task-speciﬁc distillation with data augmentation. Both procedures require huge
resources and long training times (from several days to weeks), which is cumbersome for industrial
applications. Therefore, we are aiming to explore more lightweight solutions in this paper.
Lightweight Adaptation of BERT
The overall pipeline of LadaBERT (Lightweight Adaptation of BERT) is illustrated in Figure 2. As
shown in the ﬁgure, the pre-trained BERT model (e.g., BERT-Base) is served as the teacher as well as the
initial status of the student model. Then, the student model is compressed towards smaller parameter size
iteratively through a hybrid model compression approach until the target size is reached. Concretely, in
each iteration, the parameter size of student model is ﬁrst reduced by 1−∆based on weight pruning and
matrix factorization, and then the parameters are ﬁne-tuned by the loss function of knowledge distillation.
The motivation behind is that matrix factorization and weight pruning are complementary to each other.
Matrix factorization calculates the optimal approximation under a certain rank, while weight pruning
introduces additional sparsity to the decomposed matrices. Moreover, both weight pruning and matrix
factorization generate better initial and intermediate status of the student model, which improve the
efﬁciency and effectiveness of knowledge distillation. In the following subsections, we will introduce
the algorithms in detail.
Matrix factorization
We use Singular Value Decomposition (SVD) for matrix factorization. All parameter matrices, including the embedding ones, are compressed by SVD. Without loss of generality, we assume a matrix of
parameters W ∈Rm×n, the singular value decomposition of which can be written as:
where U ∈Rm×p and V ∈Rp×n. Σ = diag(σ1, σ2, . . . , σp) is a diagonal matrix composed of singular
values and p is the full rank of W satisfying p ≤min(m, n).
To compress this weight matrix, we select a lower rank r < p. The diagonal matrix Σ is truncated by
selecting the top r singular values. i.e., Σr = diag(σ1, σ2, . . . , σr), while U and V are also truncated
by selecting the top r columns and rows respectively, resulting in Ur ∈Rm×r and Vr ∈Rr×n.
Then, low-rank matrix approximation of W can be formulated as:
W = UrΣrVT
Σr)T = ABT
In this way, the original weight matrix W is decomposed to two smaller matrices, where A =
√Σr ∈Rn×r and B = Vr
√Σr ∈Rm×r. These two matrices are initialized by SVD and will
be further ﬁne-tuned during training.
Given a rank r ≤min(m, n), the compression ratio of matrix factorization is deﬁned as:
Psvd = (m + n)r
Therefore, for a target model compression ratio Psvd, the desired rank r can be calculated by:
Weight pruning
Weight pruning is an unstructured compression method that induces desirable sparsity
for a neural network model. For a neural network f(x; θ) with parameters θ, weight pruning ﬁnds a
binary mask M ∈{0, 1}|θ| subject to a given sparsity ratio, Pweight. The neural network after pruning
will be f(x; M · θ), where the non-zero parameter size is ||M||1 = Pweight ·|θ|, where |θ| is the number
of parameters in θ. For example, when Pm = 0.3, there are 70% zeros and 30% ones in the mask m. In
our implementation, we adopt a simple pruning strategy : the binary mask is
generated by setting the smallest weights to zeros.
To combine the beneﬁts of weight pruning and matrix factorization, we leverage a hybrid approach
that applies weight pruning on the basis of decomposed matrices generated by SVD. Following Equation
(2), SVD-based matrix factorization for any weight matrix W can be written as: Wsvd = Am×rBT
Then, weight pruning is applied on the decomposed matrices A ∈Rm×r and B ∈Rn×r separately. The
weight matrix after hybrid compression is formulated as:
Whybrid = (MA · A)(MB · B)T
where MA and MB are binary masks derived by the weight pruning algorithm with compression ratio
Pweight. The compression ratio after hybrid compression can be calculated by:
Phybrid = Psvd · Pweight = (m + n)r
In LadaBERT, the hybrid compression produce is applied to each layer of the pre-trained BERT model.
Given an overall model compression target P, the following constraint should be satisﬁed:
P · |θ| = Pembd · |θembd| + Phybrid|θencd| + |θcls|
where |θ| is the total number of model parameters and P is the target compression ratio; |θembd| denotes
the parameter number of embedding layer, which has a relative compression ratio of Pembd, and |θencd|
denotes the number of parameters of all layers in BERT encoder, which have a compression ratio of
Phybrid. The classiﬁcation layers (MLP layers with Softmax activation) have a relative small number of
parameters (|θcls|), so they are not modiﬁed in model compression. In general, we have three ﬂexible
hyper-parameters for ﬁne-grained compression: Pembed, Psvd and Pweight, which can be optimized by
random search on the validation data.
Knowledge distillation
Knowledge distillation (KD) has been widely used to transfer knowledge from a large teacher model to
a smaller student model. In other words, the student model mimics the behavior of the teacher model
by minimizing the knowledge distillation loss functions. Various types of knowledge distillation can
be employed at different sub-layers. Generally, all types of knowledge distillation can be modeled as
minimizing the following loss function:
f(s)(x), f(t)(x)
Where X denotes the training set and x is a sample input in the set. f(s)(x) and f(t)(x) represent
intermediate outputs or weight matrices for the student model and teacher model respectively. L(·) represents for a loss function which can be carefully designed for different types of knowledge distillation.
We partly follow the recent technique proposed by TinyBERT , which applies knowledge distillation constraints upon embedding, self-attention, hidden representation and prediction levels.
Concretely, there are four types of knowledge distillation constraints as follows:
• Embedding-layer distillation is performed upon the embedding layer. f(x) ∈Rn×d represents
for the word embedding output for input x, where n is the input word length and d is the dimension
of word embedding. Mean Squared Error (MSE) is adopted as the loss function L(·).
• Attention-layer distillation is performed upon the self-attention sub-layer. f(x) = {aij} ∈Rn×n
represents the attention output for each self-attention sub-layer, and L(·) denotes MSE loss function.
• Hidden-layer distillation is performed at each fully-connected sub-layer in the Transformer architectures. f(x) denotes the output representation of the corresponding sub-layer, and L(·) also
adopts MSE loss function.
• Prediction-layer distillation makes the student model to learns the predictions from a teacher
model directly. It is identical to a vanilla form of knowledge distillation .
It takes soft cross-entropy loss function, which can be formulated as:
Lpred = −σ(ft(x)) · log (σ(fs(x)/t))
where σ(·) denotes Softmax function, ft(x) and fs(x) are the predictive logits of teacher and
student models respectively. t is a temperature value, which generally works well at t = 1 .
Experiments
Datasets & Baselines
We compare LadaBERT with state-of-the-art model compression approaches on ﬁve public datasets of
different tasks of natural language understanding, including sentiment classiﬁcation (SST-2), natural language inference (MNLI-m, MNLI-mm, QNLI) and pairwise semantic equivalence (QQP). The statistics
of these datasets are described in Table 1.
Table 1: Dataset Statistics
The baseline approaches are summarized below.
• Weight pruning and Matrix factorization are two simple baselines described in Section 3.3. We
evaluate both pruning methods in an iterative manner until the target compression ratio is reached.
• Hybrid pruning is a combination of matrix factorization and weight pruning, which conducts iterative weight pruning on the basis of SVD-based matrix factorization. It is performed iteratively
until the desired compression ratio is achieved.
• BERT-FT, BERT-KD and BERT-PKD are reported in , where BERT-FT directly
ﬁne-tunes the model via supervision labels, BERT-KD is the vanilla knowledge distillation algorithm , and BERT-PKD stands for Patient Knowledge Distillation proposed in
 . The student model is composed of 3 Transformer layers, resulting in a 2.5×
compression ratio. Each layer has the same hidden size as the pre-trained teacher, so the initial
parameters of student model can be inherited from the corresponding teacher.
• TinyBERT instantiates a tiny student model, which has totally 14.5M parameters
(7.5× compression ratio) composed of 4 layers, 312 hidden units, 1200 intermediate size and 12
heads. For a fair comparison, we reproduce the TinyBERT pipeline1 without general distillation
and data augmentation, which is time-exhaustive and resource-consuming.
• BERT-Small has the same model architecture as TinyBERT, but is directly pre-trained by the ofﬁcial BERT pipeline. The performance values are copied from for reference.
• Distilled-BiLSTM leverages a single-layer bidirectional-LSTM as the student
model, where the hidden units and intermediate size are set to be 300 and 400 respectively, resulting
in a 10.8× compression ratio. This model requires an expensive training process similar to vanilla
We leverage the pre-trained checkpoint of base-bert-uncased2 as the initial model for compression,
which contains 12 layers, 12 heads, 110M parameters, and 768 hidden units per layer. Hyper-parameter
1 
2 models/2018 10 18/uncased L12 H768 A12.zip
selection is conducted on the validation data for each dataset. After training, the prediction results are
submitted to the GLUE-benchmark evaluation platform3 to get the evaluation performance on test data.
For a comprehensive evaluation, we experiment with four settings of LadaBERT, namely LadaBERT-
1, -2, -3 and -4, which reduce the model parameters of BERT-Base by 2.5, 5.0, 7.5 and 10.0 times
respectively. In our experiment, we set the batch size as 32 and learning rate as 2e-5. The optimizer is
BertAdam with the default setting . Fine-grained compression ratios are optimized
by random search on SST dataset and transferred to other datasets (shown in Table 2). Following , the temperature value in distillation loss function is set as 1 in all experiments without tuning.
Table 2: Fine-grained compression ratios
Embedding layer
Matrix factorization
Weight pruning
LadaBERT-1
LadaBERT-2
LadaBERT-3
LadaBERT-4
Performance Comparison
Table 3: Performance comparison on various model sizes
LadaBERT-1
Weight pruning
matrix factorization
Hybrid pruning
LadaBERT-2
Weight pruning
matrix factorization
Hybrid pruning
LadaBERT-3
BERT-Small
Weight pruning
matrix factorization
Hybrid pruning
LadaBERT-4
Distilled-BiLSTM
Weight pruning
matrix factorization
Hybrid pruning
The evaluation results of LadaBERT and state-of-the-art approaches are listed in Table 3, where the
models are ranked by parameter sizes for feasible comparison. As shown in the table, LadaBERT consistently outperforms the strongest baselines under similar model sizes. In addition, the performance
of LadaBERT demonstrates the superiority of a combination of SVD-based matrix factorization, weight
pruning and knowledge distillation.
3 
Figure 3: Learning curve on MNLI-m dataset.
Figure 4: Learning curve on QQP dataset.
With model size of 2.5× reduction, LadaBERT-1 performs signiﬁcantly better than BERT-PKD, boosting the performance by relative 8.9, 8.1, 6.1, 3.8 and 5.8 percentages on MNLI-m, MNLI-mm, SST-2,
QQP and QNLI datasets respectively. Recall that BERT-PKD initializes the student model by selecting
3 of 12 layers in the pre-trained BERT-Base model. It turns out that the discarded layers have a huge
impact on the model performance, which is hard to be recovered by knowledge distillation. On the other
hand, LadaBERT generates the student model by iterative pruning on the pre-trained teacher. In this
way, the original knowledge in the teacher model can be preserved to the largest extent, and the beneﬁt
is complementary to knowledge distillation.
LadaBERT-3 has a comparable size as TinyBERT with a 7.5× compression ratio. As shown in the
results, TinyBERT does not work well without expensive data augmentation and general distillation,
hindering its application to low-resource settings. The reason is that the student model of TinyBERT
is distilled from scratch, so it requires much more data to mimic the teacher’s behaviors. Instead, LadaBERT has better initial and intermediate status calculated by hybrid model compression, which is much
more light-weighted and achieves competitive performances with much faster learning speed (learning
curve comparison is shown in Section 4.4). Moreover, LadaBERT-3 outperforms BERT-Small on most
of the datasets, which is pre-trained from scratch by the ofﬁcial BERT pipeline. This means that LadaBERT can quickly adapt to smaller model sizes and achieve competitive performance without expansive
re-training on a large corpus.
Moreover, Distilled-BiLSTM performs well on SST-2 dataset with more than 10× compression ratio,
owing to good generalization ability of LSTM model on small datasets. Nevertheless, the performance of
LadaBERT-4 is competitive on larger datasets such as MNLI and QQP. This is impressive as LadaBERT
is much more efﬁcient without exhaustive re-training on a large corpus. In addition, the inference speed
of BiLSTM is slower than transformer-based models with similar parameter sizes.
Learning curve comparison
To further demonstrate the efﬁciency of LadaBERT, we visualize the learning curves on MNLI-m and
QQP datasets in Figure 3 and 4, where LadaBERT-3 is compared to the strongest baseline, TinyBERT,
under 7.5× compression ratio. As shown in the ﬁgures, LadaBERT-3 achieves good performances much
faster and results in a better convergence point. After training 2×104 steps (batches) on MNLI-m dataset,
the performance of LadaBERT-3 is already comparable to TinyBERT after convergence (approximately
2 × 105 steps), achieving nearly 10 times acceleration. And on QQP dataset, both performance improvement and training speed acceleration are very signiﬁcant. This clearly shows the superiority of combining
matrix factorization, weight pruning and knowledge distillation in a collaborative manner. On the other
hand, TinyBERT is based on pure knowledge distillation, so the learning speed is much slower.
Effect of low-rank + sparsity
In this paper, we demonstrate that a combination of matrix factorization and weight pruning is better
than single solutions for BERT-oriented model compression. Similar phenomena has been reported in
computer vision, showing that low-rank and sparsity are complementary to each other .
Here we provide another explanation to support our observation.
Figure 5: Distribution of pruning errors
In Figure 5, we visualize the distribution of
element biases for a weight matrix in the neural network after pruning to 20% of its original parameter size.
For illustration, we consider the matrix initialized by real pretrained
BERT weights, and the pruning process is done
at once. We deﬁne the biases to be calculated
by Biasij = ˆMij −Mij, where ˆM denotes the
weight matrix after pruning.
The yellow line in Figure 5 shows the distribution of biases generated by pure weight pruning,
which has a sudden drop at the pruning threshold. The orange line represents for pure SVD
pruning, which turns out to be smoother and is
aligned with Gaussian distribution.
line shows the result of hybrid pruning, which conducts weight pruning on the decomposed matrices.
First, we apply SVD-based matrix factorization to reduce 60% of total parameters. Then, weight pruning
is applied on the decomposed matrices by 50%, resulting in 20% parameters while the bias distribution
changes slightly. As visualized in Figure 5, it has smaller mean and deviation of bias distribution than
that of pure matrix factorization. In addition, it seems that a smoother weight distribution is more feasible for the ﬁne-tuning procedure. Therefore, it is reasonable that a hybrid model compression approach
is advantageous than pure weight pruning.
Conclusion
Model compression is a common way to deal with latency-critical or memory-intensive scenarios. Existing model compression methods for BERT are expansive as they require re-training on a large corpus to
reserve the original performance. In this paper, we propose LadaBERT, a lightweight model compression
pipeline that generates an adaptive BERT model efﬁciently based on a given task and speciﬁc constraint.
It is based on a hybrid solution, which conducts matrix factorization, weight pruning and knowledge
distillation in a collaborative fashion. The experimental results demonstrate that LadaBERT is able to
achieve comparable performance with other state-of-the-art solutions using much less training data and
computation budget. Therefore, LadaBERT can be easily plugged into various applications to achieve
competitive performances with little training overheads. In the future, we would like to apply LadaBERT
to large-scale industrial applications, such as search relevance and query recommendation.