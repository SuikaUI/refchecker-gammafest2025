Anomaly Detection in Video Sequence with Appearance-Motion Correspondence
Trong-Nguyen Nguyen, Jean Meunier
DIRO, University of Montreal
{nguyetn, meunier}@iro.umontreal.ca
Anomaly detection in surveillance videos is currently a
challenge because of the diversity of possible events. We
propose a deep convolutional neural network (CNN) that
addresses this problem by learning a correspondence between common object appearances (e.g. pedestrian, background, tree, etc.) and their associated motions. Our model
is designed as a combination of a reconstruction network
and an image translation model that share the same encoder. The former sub-network determines the most significant structures that appear in video frames and the latter
one attempts to associate motion templates to such structures. The training stage is performed using only videos
of normal events and the model is then capable to estimate
frame-level scores for an unknown input. The experiments
on 6 benchmark datasets demonstrate the competitive performance of the proposed approach with respect to state-ofthe-art methods.
1. Introduction
Anomaly detection in video sequences is a necessary
functionality for surveillance systems. Because abnormal
events rarely occur in real-world videos, this task is signiﬁcantly time-consuming and may require a large amount
of resource (e.g. people) to perform manual checking. A
method than can automatically determine potential frames
of anomalous events is thus crucial.
Our model is a combination of a convolutional autoencoder (Conv-AE) and a U-Net with skip connections 
that share the same encoder sub-network.
Other related
works employed either an AE or a U-Net to perform the
anomaly detection in different ways. Hasan et al. estimate regularity score for frames in video sequences according to reconstruction models. Their two AEs (with and
without convolutional layers) work on two different inputs:
hand-crafted features (HOG and HOF with trajectory-based
properties ) and concatenation of 10 consecutive frames
along the temporal axis. The reconstruction error is used to
indicate their regularity score. Unlike that work, the input
of our Conv-AE is a single frame and the temporal factor is
considered in the other stream via U-Net. The purpose of
our Conv-AE is to learn only regular appearance structures.
On the contrary, Ravanbakhsh et al. employ the U-
Net structure proposed in to translate an input from
video frame to a corresponding optical ﬂow and vice versa.
We argue that the use of two CNNs with the same structure
may be redundant and an appropriate modiﬁcation and/or
combination would improve the model ability. Compared
with , our network keeps the stream translating a video
frame to an optical ﬂow (but using our proposed structure
instead of ) while replaces the other U-Net by a Conv-
AE that shares the encoding ﬂow.
Inspired by the good performance of the video prediction model in , Liu et al. present a model that uses
a U-Net structure to predict a frame from a number of recent ones and then estimates the corresponding optical ﬂow.
The model is optimized according to the difference between
the outputted and original versions of video frame as well
as the optical ﬂow together with an adversarial loss. Our
work also predicts an optical ﬂow but directly from a single
frame in order to determine the association between a scene
appearance and its typical motion. Since a ﬁxed procedure
of optical ﬂow estimation (FlowNet ) is embedded inside
the network in , the selection of such method is thus
limited because the estimator has to be fully differentiable
to perform an end-to-end training. Our model, however,
has a stream that directly estimates a mapping from input
frame to optical ﬂow. We only use a pretrained estimator
for ground truth calculation and the model signal does not
propagate through it during the training as well as inference
Our main contributions are summarized as follows:
• We design a CNN that combines a Conv-AE and a U-
Net, in which each stream has its own contribution for
the task of detecting anomalous frames. The model
can be trained end-to-end.
• We integrate an Inception module modiﬁed from 
right after the input layer to reduce the effect of network’s depth since this depth is considered as a hyperarXiv:1908.06351v1 [cs.CV] 17 Aug 2019
parameter that requires a careful selection.
• We propose a patch-based scheme estimating framelevel normality score that reduces the effect of noise
which appears in the model outputs.
• Experiments on 6 benchmark datasets demonstrate the
potential of our model with competitive performance
compared with state-of-the-art methods. We also provide discussions for these datasets that should be useful for future works.
The remainder of this paper is organized as follows: a
summary of related studies is given in Section 2; Section 3
describes the details of our method; experiments and discussions for the 6 benchmark datasets are presented in Section 4; and Section 5 concludes this work.
2. Related work
We brieﬂy describe the principal categories that lead to
very different approaches for anomaly detection in video.
2.1. Trajectory
The diversity of possible anomalous events is the main
challenge of the anomaly detection problem.
researchers simplify this issue by explicitly specifying
anomalies (e.g. ) or particular relevant attributes that
can be used effectively for anomaly detection, in which
the most common one is motion trajectory.
These studies aim to learn patterns of object trajectories determined
from normal events .
There are four
main stages in the methodology including object detection,
tracking, trajectory-based feature extraction and classiﬁcation/detection. The advantages of methods in this category
are the simple implementation and fast execution. However,
their effectiveness may signiﬁcantly degrade when working
on videos with cluttered background since the trajectory determination depends on the result of object detection and
tracking. Moreover, trajectory anomalies do not cover the
whole spectrum of anomalies in video surveillance.
2.2. Sparse coding
Instead of explicitly deﬁning and estimating speciﬁc
anomaly attributes, other researchers consider an input sequence of frames as a collection of small 3D patches. Concretely, a number of consecutive frames are concatenated
along the temporal axis and then split into same-size 3D
patches according to a window sliding on the image plane.
In the inference stage, each 3D patch extracted from unknown inputs is represented as a sparse combination of
training samples of normal events. The reconstruction error is considered as the score supporting the ﬁnal decision.
Such sparsity-based methods have achieved state-of-the-art
performances . The main drawback is the high computational cost in ﬁnding combination coefﬁcients due to
sparse representation. Some studies thus attempt to reduce
the complexity by modifying the learning algorithms and/or
data structures . Beside window-based split, 3D
patches are also determined using keypoint detectors 
while other researchers attempt to learn the relation between training patches according to their distribution 
or graph-based representation .
2.3. Deep learning
Since deep learning models currently achieve top performance in a wide range of vision applications such as image
classiﬁcation , object detection and image captioning , many CNNs have been proposed
to deal with the problem of anomaly detection in videos.
Typical structures of image reconstruction and translation
are usually employed and the difference between their output and ground truth is used to indicate the frame-level
score . Some researchers apply pretrained classiﬁcation models (such as VGG ) to extract useful features from input videos . Results of object detection
and/or foreground estimation are also used for the determination of anomalous events in .
3. Proposed method
An overview of our model is visualized in Figure 1. The
model includes two processing streams. The ﬁrst one is performed via a Conv-AE to learn common appearance spatial
structures in normal events. The second stream is to determine an association between each input pattern and its corresponding motion represented by an optical ﬂow of 3 channels (xy displacements and magnitude). The skip connections in U-Net are useful for image translation since it directly transforms low-level features (e.g. edge, image patch)
from original domains to the decoded ones. Such connections are not employed in the appearance stream because the
network may let the input information go through these connections instead of emphasizing underlying attributes via
the bottleneck.
Our model does not use any fully-connected layer, so it
can theoretically work on images of any resolution. In order
to simplify the model as well as make it be appropriate for
possible further extensions, we ﬁxed the size of input layer
as 128 × 192 × 3. The image size is set to a ratio of 1:1.5
instead of 1:1 as in related works (e.g. ) in order
to preserve the aspect of objects in surveillance videos.
3.1. Inception module
The Inception module was originally proposed to let
a CNN decide its ﬁlter size (in a few layers) automatically . A number of convolutional operations with various ﬁlter resolutions are performed in parallel and the ob-
Convolution
Batch-normalization
Activation
Deconvolution
Concatenation
(skip connection)
Figure 1. Overview of our model structure together with the spatial resolution of feature maps in each block (i.e. a sequence of
layers with the same output shape). The number of channels corresponding to each layer in each block is also presented (in parentheses). The input and two output layers have the same size of
128×192×3. There are three clusters of layers: common encoder
(left), appearance decoder (top right) and motion decoder (bottom
right). Each concatenation is performed along the channel axis
right before operating the next deconvolution. The model input
is a single video frame It and the outputs from the two decoders
are a reconstructed frame ˆIt and an optical ﬂow ˆFt predicting the
motion between It and It+1. Best viewed in color.
tained feature maps are then concatenated along the channel
axis. The use of this module in our work can be explained
under an alternative perspective as follows. The proposed
network has an encoder-decoder structure with bottleneck.
A very deep architecture may eliminate the features that are
helpful for decoding. On the contrary, a shallow network
takes the risk of missing high-level abstractions. Therefore,
we apply an Inception module to let the model select its appropriate convolutional operations.
This work focuses on surveillance videos acquired from
a ﬁxed position. Given a convolutional layer with a predeﬁned receptive ﬁeld (i.e. ﬁlter size) right after the input
layer, the information abstraction would be different for the
same object captured at various distances. This property
is propagated for next layers, we thus expect the model to
early determine low-level features by putting the Inception
module right after the input layer. We remove the maxpooling in this module since the input is a regular video
frame instead of a collection of feature maps. Our Inception
module is modiﬁed from including 4 streams of convolutions of ﬁlter sizes 1 × 1, 3 × 3, 5 × 5 and 7 × 7. Each
convolutional layer of ﬁlter larger than 1 × 1 is factorized
into a sequence of layers with smaller receptive ﬁelds in order to reduce the computational cost as suggested in .
3.2. Appearance convolutional autoencoder
Our Conv-AE supports the detection of strange (abnormal) objects within input frames by learning common appearance templates in normal events.
This sub-network
consists of the encoder and the top decoder without any
skip connection as shown in Figure 1. The encoder is constructed by a sequence of blocks including triple layers:
convolution, batch-normalization (BatchNorm) and leaky-
ReLU activation . The ﬁrst block (right after the Inception module) does not contain BatchNorm layer as suggested in for our U-Net task in Section 3.3. Instead of
using pooling layer to reduce the resolution of feature maps,
we apply strided convolution. Such parametric operation is
expected to support the network ﬁnding an informative way
to downsample the spatial resolution of feature maps as well
as learning the further upsampling in decoding stage .
The decoder is also a sequence of layer blocks that increases the spatial resolution while reduces the number of
feature maps after each deconvolution layer.
layer (with pdrop = 0.3) is attached before the ReLU activation in each block as a regularization that reduces the
risk of overﬁtting during the training stage .
Since the Conv-AE is to learn common appearance patterns of normal events, we consider the l2 distance between
the input image I and its reconstruction ˆI. The model thus
forces to produce an image with similar intensity for each
pixel. The intensity loss is estimated as
Lint(I, ˆI) = ∥I −ˆI∥2
A drawback of using only l2 loss is the blur in the output, we
thus add a constraint that attempts to preserve the original
gradient (i.e. the sharpness) in the reconstructed image. The
gradient loss is deﬁned as the difference between absolute
gradients along the two spatial dimensions as
Lgrad(I, ˆI) =
where gd denotes the image gradient along the d-axis. The
ﬁnal loss function of the appearance Conv-AE is formed as
a summation of the intensity and gradient losses.
Lappe(I, ˆI) = Lint(I, ˆI) + Lgrad(I, ˆI)
This loss combination has been reported to give good performance for the task of video prediction .
3.3. Motion prediction U-Net
Beside the appearance of strange object structures, unusual motions of typical objects would also be appropriate to provide an assessment of a video frame. Recall that
each block in the encoder is to emphasize spatial abstractions of common objects within training frames. Our U-
Net sub-network thus focuses on learning the association
between such patterns and corresponding motions.
ground truth optical ﬂow employed in this work is estimated by a pretrained FlowNet2 . Compared with related models, the optical ﬂow outputted from FlowNet2 is
not only much smoother but also preserves motion discontinuities with sharper boundaries. The motion stream is expected to associate typical motions to common appearance
objects while ignoring the static background patterns.
The decoder of our U-Net has the same structure as the
Conv-AE except for the skip connections. These concatenations are to combine the feature maps upsampled from
a higher level of abstraction with the ones containing lowlevel details. The use of leaky-ReLU activation in the encoder also keeps weak responses that may be informative
for the translation in the decoder.
Unlike the Conv-AE in Section 3.2, the loss between an
outputted optical ﬂow and its ground truth is measured by
l1 distance. There are two main reasons for this. First,
the FlowNet2 model is formed as a fusion of multiple networks providing optical ﬂows from coarse (noisy) to ﬁne
(smooth), the result might thus contain noise or even amplify noisy regions during the smoothing procedure. Second, because the selection of optical ﬂow estimation is not
limited to FlowNet2, the training ground truth obtained
from other algorithms might therefore possibly have small
patches of wrong and/or noisy motion measure. In order to
reduce the effect of such outliers when learning the motion
association, we apply l1 distance loss
= ∥Ft −ˆFt∥1
where Ft is the ground truth optical ﬂow estimated from
two consecutive frames It and It+1, and ˆFt is the output
of our U-Net given It. In summary, this stream attempts to
predict instant motions of objects appearing in the video.
3.4. Additional motion-related objective function
Beside the distance-based loss Lflow, we also add another loss that penalizes the underlying distribution of predicted optical ﬂow to be similar to ground truth. The generative adversarial network (GAN) was originally introduced to allow a CNN learning an implicit distribution
of patterns. The model consists of a generator that creates
fake samples from noise and a discriminator that attempts to
distinguish such outputs from the real patterns. Many modiﬁed GAN versions have been proposed for the task of data
generation. The discriminator also plays the role of a regularization in many models. Inspired by where using
a GAN loss is reported to provide better results compared
with employing only distance-based ones, we apply such
strategy as an additional objective function.
Our generator is the entire network in Figure 1 while the
discriminator conditionally performs the classiﬁcation on
predicted optical ﬂow. A visualization of our discriminator
Convolution
Batch-normalization
Leaky ReLU
Binary classiﬁcation
Figure 2. The architecture of our discriminator. The input layer of
shape 128 × 192 × 6 is fed by the concatenation of a video frame
and its optical ﬂow (that is either ground truth or outputted from
the U-Net). The output layer is sigmoid activation of 512 feature
maps of spatial resolution 16 × 24. Best viewed in color.
architecture is shown in Figure 2. Notice that the discriminator is not employed in the inference stage. Although the
recent study employed a Least Square GAN and
achieved state-of-the-art performance in detecting anomalous video frames, our model follows the strategy of typical conditional GAN (cGAN) where both the ground truth
video frame and its corresponding optical ﬂow are fed into
the discriminator. There are two reasons leading to this decision. First, the cGAN theoretically avoids the problem of
mode collapse in vanilla GAN since ground truth information (i.e. labels, real samples) is fed into the discriminator.
The model is thus expected to efﬁciently learn the distribution of training samples. Second, cGAN is appropriate for
a CNN of image translation as demonstrated in .
Finally, the adversarial loss is directly computed on the
last layer containing activated feature maps in the discriminator. This calculation is different from where a
convolutional layer is employed to collapse previous feature
channels into a 2D map. The common sense of our model
and the two others is the structural penalization where the
classiﬁcation is performed according to image patches instead of the whole image. However, we strictly constrain
patches at feature-level so that each feature map must attempt to provide a classiﬁcation result. This design is inspired from the study demonstrating that each convolutional channel attends to particular semantic patterns.
Given an input video frame I and its associated optical
ﬂow F obtained from FlowNet2, the proposed network in
Figure 1 (the generator denoted as G) produces a reconstructed frame ˆI and a predicted optical ﬂow ˆF, while the
discriminator D estimates a probability that the optical ﬂow
associated to I is the ground truth F. The GAN objective
function consists of two loss functions:
LD(I, F, ˆF) =
−logD(I, F)x,y,c
−log[1 −D(I, ˆF)x,y,c]
LG(I, ˆI, F, ˆF) =
−logD(I, ˆF)x,y,c
+ λaLappe(I, ˆI) + λfLflow(F, ˆF)
where x, y and c respectively indicate the spatial position
and the corresponding channel of a unit in the feature maps
outputted from D, and λ values are the weights associated
to partial losses within our proposed model. Our GAN is
optimized by alternately minimizing the two GAN losses.
In our experiments (see Section 4), we assigned 0.25 for
λG, 1 for λa and 2 for λf. This GAN aims to emphasize the
efﬁciency of motion prediction.
3.5. Anomaly detection
Our model aims to provide a score of normality for each
frame. In related studies, such scores are usually quantities measuring the similarity between a ground truth and
the reconstructed/predicted output. There are two common
scores employed in CNN approaches: Lp distance and Peak
Signal to Noise Ratio (PSNR). The normality of each video
frame is decided by comparing its score with a threshold. It
is obvious that an anomalous event occurring within a small
image region may be missed due to the summation and/or
average operations over all pixel positions. We hence propose another score estimation scheme considering only a
small patch instead of the entire frame.
First, we deﬁne partial scores individually estimated on
the two model streams sharing the same patch position as
i,j∈P (Ii,j −ˆIi,j)2
i,j∈P (Fi,j −ˆFi,j)2
where P indicates an image patch and |P| is its number
of pixels.
Our frame-level score is then computed as a
weighted combination of the two partial scores as follows:
S = log[wF SF ( ˜P)] + λS log[wISI( ˜P)]
where wF and wI are the weights calculated according to
the training data, λS is to control the contribution of partial
scores to the summation, and ˜P is the patch providing the
highest value of SF in the considering frame, i.e.
P slides on frame
The weights wF and wI are estimated as the inverse of average scores obtained on the training data of n images:
i=1 SFi( ˜Pi)
i=1 SIi( ˜Pi)
This helps to normalize the two scores on the same scale.
The size of P was set to 16 × 16 in our experiments. Typically, such patches are determined by a sliding window. In
Figure 3. Examples of normal (top) and abnormal (bottom) frames
in the CUHK Avenue, UCSD Ped2, Exit Gate, and Entrance Gate
(from left to right) datasets. Anomalous events are highlighted
including a man picking a bag, bicycle appearance, and loitering.
realistic implementation, it can be performed using a convolutional operation with a ﬁlter of size 16 × 16. λS was
empirically set to 0.2 since the model focuses on motion
prediction efﬁciency.
Finally, we perform a normalization on frame-level
scores in each evaluated video as suggested in related studies such as . Our ﬁnal frame-level score is
max(S1..m)
where t is the frame index in a video containing m frames.
The score estimated from a frame of abnormal event is expected to be higher compared with the ones of normal event.
4. Experiments
We performed experiments on various benchmark
datasets of anomaly detection including CUHK Avenue , UCSD Ped2 , Subway Entrance Gate and
Exit Gate , Trafﬁc-Belleview and Trafﬁc-Train .
Their training data contain only normal events. Some examples of normal and abnormal frames in the ﬁrst 4 datasets
are shown in Figure 3. The ﬁrst two datasets are provided
with frame-level ground truth, we thus employ area under
curve (AUC) of the receiver operating characteristic (ROC)
curve measured according to frame-level scores outputted
from the proposed model to indicate the performance. The
next two Subway datasets are evaluated on event-level that
requires some additional operations described below. The
last two datasets are evaluated according to the average
precision (AP) since the precision-recall (PR) curve was
usually used for their assessment . We used the
FlowNet2 pretrained on FlyingThing3D and ChairsS-
DHom datasets as the ground truth optical ﬂow estimator. The GAN was trained using Adam algorithm 
where the initial learning rates were set to 2 × 10−4 for the
generator G and 2 × 10−5 for the discriminator D. The description, experimental results and a discussion corresponding to each evaluation are presented in the remaining of this
Conv-AE 
Discriminative learning 
Hashing ﬁlters 
Unmask late fusion 
AMDN (double fusion) 
ConvLSTM-AE 
DeepAppearance 
FRCN action 
Stacked RNN 
AbnormalGAN 
GrowingGas 
Future frame prediction 
Our proposed method
Table 1. Frame-level performance (AUC) of anomaly detection on
the CUHK Avenue and UCSD Ped2 datasets. The methods are
ordered according to the year of publication.
4.1. CUHK Avenue and UCSD Ped2
The Avenue dataset consists of 30652 frames that are
split into 16 clips for training and 21 clips for testing. This
dataset was captured in a campus avenue and contains various types of anomaly such as unusual action (e.g. running),
wrong moving direction and abnormal object (e.g. bicycle).
This also provides some challenges for evaluation such as
slight camera shake and the occurrence of a few outliers.
The UCSD anomaly dataset includes two subsets Ped1
and Ped2 acquired from static cameras overlooking pedestrian walkways. The anomalies are the appearance of nonpedestrian object (e.g. vehicle) and strange pedestrian motion. The difference between the two subsets is the walking direction (toward and away from the camera in Ped1,
parallel to the camera plane in Ped2). We select only the
Ped2 dataset for two reasons. First, our optical ﬂow estimator (FlowNet2) does not work well on very small and thin
pedestrians appearing too far from the camera. Nevertheless, examples of people walking towards and away from
the camera are available in the CUHK Avenue dataset allowing to evaluate performance in this situation. Second,
we observed that some events were labeled as normality in
the training data but were considered as anomalous in the
test data (e.g. people walking on grass). Therefore, the Ped2
dataset (16 training and 12 testing clips) was used in our experiments.
The frame-level assessment results in Table 1 show that
our model outperforms all other recent methods in the task
of anomaly detection. Examples of reconstructed frames
and predicted optical ﬂows obtained from the appearance
and motion streams are given in Figure 4. Considering the
ﬁrst example, the truck was reconstructed as a collection
(a) The appearance of a truck and a bicycle. (Ped2)
(b) A bicycle is running in a low contrast region. (Ped2)
(c) A man is running. (Avenue)
(d) A man is tossing papers. (Avenue)
Figure 4. (Best viewed in color) Results on the Ped2 and Avenue
datasets. Each example consists of 3 image columns that are input
frame and its optical ﬂow (left), reconstructed frame and predicted
motion (middle), and the frame superimposed by the motion error
map below (right). The ﬂow ﬁeld color coding is the same as .
of pedestrian patterns since it is a new object observed by
the model. The corresponding predicted motion was thus
completely different from the ground truth. The processing of the bicycle on the right image edge was also similar.
The second scene shows that the model still worked well on
a crowded scene with many pedestrians and an anomalous
Entrance (66)
Subspace 
MPPCA 
Sparse dict. 
Conv-AE 
IT-AE 
Hashing ﬁlters 
Early fusion 
Late fusion 
Our method
Table 2. Our results of anomaly detection on the Subway datasets.
In the ground truth, the numbers of abnormal events in the Entrance and Exit are respectively 66 and 19. The term TP indicates
the number of true positive detections while FA is the counting of
false alarms. The methods are listed in temporal order.
object having similar intensities with the background. In the
next two Avenue frames, the model expected slower moving
speed and another motion direction as observed in the training data. In addition, notice that the reconstructed man’s
trouser color was slightly different from the input frame
while the back ground was well restored. This demonstrates
that the model reasonably determined the low-signiﬁcance
relation between the color of a pattern and its movement.
4.2. Subway Entrance and Exit gates
This dataset contains videos capturing the entrance gate
and exit gate of a subway station. Their lengths are respectively 96 and 43 minutes. The anomalous events in these
two videos are wrong direction (e.g. passenger exits through
the entrance gate), no payment, loitering, irregular interaction (e.g. a person walks awkwardly to avoid another) and
miscellaneous (e.g. sudden changing of walking speed).
We performed the evaluation according to the ground
truth of events with the training and test sets provided
in , in which the normal events in the ﬁrst 15 minutes
of the Entrance Gate video and 5 minutes of the Exit Gate
were used in training stage. Notice that the experiments
were performed individually for the two videos.
Since the dataset does not provide the frame-level
ground truth, we employ the assessment scheme in to
determine anomalous events in the experiments. In detail,
the persistence algorithm is applied on the sequence
of scores to locate local maxima, in which each maximum
point indicates an anomalous event. In order to reduce the
effect of possible noisy detected extrema, nearby events are
combined to provide only an anomalous one.
Our event-based assessment results are presented in Table 2. It shows that our model detected most anomalous
events but also generated more false alarm than other recent
studies. By taking a closer look at these false alarms, we determined that some events denoted as normal in the test set
can be considered as anomaly under other circumstances.
A visualization of some false alarms and missed anomaly
detections in the Entrance dataset is given in Figure 5.
Figure 5 shows that the normality decision of movement
stopping and loitering was unstable since the cases (a)-(e)
were missed while (f)-(h) were wrongly detected. There are
two possible reasons: (1) the use of maximum localization
as in is not ideal when the anomaly score smoothly
and/or slowly changes, and (2) the training set (according
to ) contains loitering event [caused by the man in (b)
and (e)]. The ambiguity in ground truth annotation is also
shown in the event (h) where a loitering man appeared on
the right side but was not labeled as anomaly. In the event
(i), the model predicted that the man would go through the
left gate but he suddenly changed to the right one (the color
indicates the motion direction). Since this action does not
occur in the training data, the model determined it as an
anomalous event. Regarding the last example (j), the motion stream expected the passenger to go to the train because
most people at this location move to the left side in the training data. In other words, the model may forget training patterns moving to the right side. In this case, using sparse
coding approaches can be appropriate since the
effect of the frequency of training patterns is reduced.
4.3. Trafﬁc-Belleview and Trafﬁc-Train
The Trafﬁc-Belleview dataset was acquired by a surveillance camera looking at the trafﬁc on a road intersection
from a high viewpoint. In the training data (300 frames),
vehicles only run on the main street. The appearance and
movement of vehicles from/to left or right roads is deﬁned
as anomaly in the test set containing a total of 2618 frames.
The video is gray-scale and has a low quality.
Unlike the previous benchmark datasets, the Trafﬁc-
Train can be considered as the most challenging dataset
since the lighting conditions vary drastically together with
camera jitter. The camera was mounted in a train and people movement is deﬁned as anomaly. The training and test
sets consist of 800 and 4160 frames, respectively.
Our average precision of frame-level assessment is presented in Table 3. Figure 6 shows examples of problems
that the model encountered when dealing with the trafﬁc
datasets as well as illustrates the change of lighting conditions in the Train dataset. In Figure 6(b), the predicted motion was very noisy and the passenger at the frame center
was missed in the error map. The effect of optical ﬂow estimator is illustrated in Figure 6(c) where two cars were combined to be a big blob. This bad estimation signiﬁcantly affected the error map though the three cars running on other
way were correctly determined. The results may thus be im-
Figure 5. Examples of missed detections (a)-(e) and false alarms (f)-(j) in our experiments on the Entrance dataset. Each example consists
of 4 images that are (from top to bottom) the input frame, ground truth optical ﬂow, predicted motion and the corresponding motion error
map. The missed detections are: (a)-(c) movement stopping, (d) loitering, and (e) loitering (man) and movement stopping (woman). The
false alarms are: (f)-(g) movement stopping, (h) loitering, (i) changing gate, and (j) passenger going near the railway. Best viewed in color.
GANomaly 
AEs + local feature 
AEs + global feature 
ALOCC D(X) 
ALOCC D(R(X)) 
Our proposed method
SSIM on appearance stream
Table 3. The average precision of frame-level anomaly detection
on the Trafﬁc-Belleview and Trafﬁc-Train datasets.
proved by choosing another optical ﬂow estimator or tuning
the pretrained FlowNet2 by a more appropriate dataset.
As an attempt to reduce the effect of such factors, we
estimated another frame-level score without the support of
motion as in section 3.5. Concretely, we used the Structural Similarity Index (SSIM) to compute the similarity
between an input frame and its reconstruction provided by
the appearance stream. Compared with other common measures such as MSE or PSNR, SSIM can work well on jitter
images where pixel by pixel comparison is not appropriate.
Table 3 shows that this modiﬁcation improved the anomaly
detection results, especially with the Train dataset.
Further details including ROC and PR curves, visualization of some feature maps and evaluation results of each
single stream are provided in the supplementary materials.
5. Conclusion
This paper presents an anomaly detection approach that
exploits the correspondence between pattern appearances
and their motions. The model is designed as a combination
of two streams. The ﬁrst one attempts to reconstruct the
appearance according to its auto-encoder architecture while
the second stream uses a U-Net structure to predict the in-
(a) The change of lighting in the Trafﬁc-Train dataset.
(b) Passengers moving in the stopping train.
(c) Cars turning to the left way.
Figure 6. (Best viewed in color) Some testing results on the two
trafﬁc datasets. Each example consists of 6 images as in Figure 4.
stant motion given an input video frame. By sharing the
same encoder, the model is forced to learn the correspondence.
A patch-based scheme of anomaly score estimation is proposed to reduce the effect of noise in model outputs. Experiments on 6 benchmark datasets demonstrated
the potential of our method. Detailed discussions are also
presented to provide improvement suggestions for further
References
 Amit Adam, Ehud Rivlin, Ilan Shimshoni, and David
Reinitz. Robust real-time unusual event detection using multiple ﬁxed-location monitors.
IEEE Trans. Pattern Anal.
Mach. Intell., 30(3):555–560, 2008.
 Samet Akcay, Amir Atapour-Abarghouei, and Toby P.
Ganomaly: Semisupervised anomaly detection
via adversarial training. In Computer Vision – ACCV 2018,
Cham, 2018. Springer International Publishing.
 Arslan Basharat, Alexei Gritai, and Mubarak Shah. Learning
object motion patterns for anomaly detection and improved
object detection.
In 2008 IEEE Conference on Computer
Vision and Pattern Recognition, pages 1–8, June 2008.
 Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian
Shao, Wei Liu, and Tat-Seng Chua. Sca-cnn: Spatial and
channel-wise attention in convolutional networks for image
captioning. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), July 2017.
 Kai-Wen Cheng, Yie-Tarng Chen, and Wen-Hsien Fang.
Video anomaly detection and localization using hierarchical feature representation and gaussian process regression.
In 2015 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 2909–2917, June 2015.
 Yang Cong, Junsong Yuan, and Ji Liu. Sparse reconstruction
cost for abnormal event detection.
In CVPR 2011, pages
3449–3456, June 2011.
 Allison Del Giorno, J. Andrew Bagnell, and Martial Hebert.
A discriminative framework for anomaly detection in large
videos. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max
Welling, editors, Computer Vision – ECCV 2016, pages 334–
349, Cham, 2016. Springer International Publishing.
 Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip
H¨ausser, Caner Hazirbas, Vladimir Golkov, Patrick van der
Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical ﬂow with convolutional networks. In 2015 IEEE
International Conference on Computer Vision (ICCV), pages
2758–2766, Dec 2015.
 Ehsan Elhamifar and Rene Vidal. Sparse subspace clustering. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 2790–2797, June 2009.
 Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q.
Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2672–2680. Curran Associates,
Inc., 2014.
 Mahmudul Hasan, Jonghyun Choi, Jan Neumann, Amit K.
Roy-Chowdhury, and Larry S. Davis.
Learning temporal
regularity in video sequences.
In 2016 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
733–742, June 2016.
 Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Girshick. Mask r-cnn. In 2017 IEEE International Conference
on Computer Vision (ICCV), pages 2980–2988, Oct 2017.
 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In 2016 IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 770–778, June 2016.
 Ryota Hinami, Tao Mei, and Shin’ichi Satoh. Joint detection
and recounting of abnormal events by learning deep generic
knowledge. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 3639–3647, Oct 2017.
 Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper,
Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolution of optical ﬂow estimation with deep networks. In The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
 Radu Tudor Ionescu, Sorina Smeureanu, Bogdan Alexe, and
Marius Popescu. Unmasking the abnormal events in video.
In 2017 IEEE International Conference on Computer Vision
(ICCV), pages 2914–2922, Oct 2017.
 Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A.
Efros. Image-to-image translation with conditional adversarial networks.
In 2017 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 5967–5976,
July 2017.
 Justin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap:
Fully convolutional localization networks for dense captioning. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4565–4574, June 2016.
 Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 39(4):664–
676, April 2017.
 Jaechul Kim and Kristen Grauman. Observe locally, infer
globally: A space-time mrf for detecting abnormal activities with incremental updates.
In 2009 IEEE Conference
on Computer Vision and Pattern Recognition, pages 2921–
2928, June 2009.
 Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. CoRR, abs/1412.6980, 2014.
 Yeara
Extracting
functions.
 
persistence1d.html. [Accessed 15-Feb-2019].
 Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q.
Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1097–1105. Curran Associates,
Inc., 2012.
 Weixin Li, Vijay Mahadevan, and Nuno Vasconcelos.
Anomaly detection and localization in crowded scenes. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
36(1):18–32, Jan 2014.
 Wen Liu, Weixin Luo, Dongze Lian, and Shenghua Gao. Future frame prediction for anomaly detection a new baseline.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), June 2018.
 Cewu Lu, Jianping Shi, and Jiaya Jia. Abnormal event detection at 150 fps in matlab. In 2013 IEEE International Conference on Computer Vision, pages 2720–2727, Dec 2013.
 Weixin Luo, Wen Liu, and Shenghua Gao.
Remembering history with convolutional lstm for anomaly detection.
In 2017 IEEE International Conference on Multimedia and
Expo (ICME), pages 439–444, July 2017.
 Weixin Luo, Wen Liu, and Shenghua Gao. A revisit of sparse
coding based anomaly detection in stacked rnn framework.
In 2017 IEEE International Conference on Computer Vision
(ICCV), pages 341–349, Oct 2017.
 Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. Rectiﬁer nonlinearities improve neural network acoustic models.
In in ICML Workshop on Deep Learning for Audio, Speech
and Language Processing, 2013.
 Vijay Mahadevan, Weixin Li, Viral Bhalodia, and Nuno Vasconcelos. Anomaly detection in crowded scenes. In 2010
IEEE Computer Society Conference on Computer Vision and
Pattern Recognition, pages 1975–1981, June 2010.
 Xudong Mao, Qing Li, Haoran Xie, Raymond Y.K. Lau,
Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 2813–2821, Oct
 Micha¨el Mathieu, Camille Couprie, and Yann LeCun. Deep
multi-scale video prediction beyond mean square error.
CoRR, abs/1511.05440, 2015.
 Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer,
Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A
large dataset to train convolutional networks for disparity,
optical ﬂow, and scene ﬂow estimation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
June 2016.
 G´erard Medioni, Isaac Cohen, Franc¸ois Bremond, Somboon
Hongeng, and Ramakant Nevatia. Event detection and analysis from video streams. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(8):873–889, Aug 2001.
 Medhini G. Narasimhan and Sowmya Kamath S. Dynamic
video anomaly detection and localization using sparse denoising autoencoders. Multimedia Tools and Applications,
77(11):13173–13195, Jun 2018.
 Claudio Piciarelli, Christian Micheloni, and Gian Luca
Foresti. Trajectory-based anomalous event detection. IEEE
Transactions on Circuits and Systems for Video Technology,
18(11):1544–1554, Nov 2008.
 Mahdyar Ravanbakhsh, Moin Nabi, Enver Sangineto, Lucio Marcenaro, Carlo Regazzoni, and Nicu Sebe. Abnormal
event detection in videos using generative adversarial nets.
In 2017 IEEE International Conference on Image Processing (ICIP), pages 1577–1581, Sep. 2017.
 Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks.
In C. Cortes, N. D. Lawrence, D. D.
Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 91–99. Curran
Associates, Inc., 2015.
 Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
In Nassir Navab, Joachim Hornegger, William M. Wells, and
Alejandro F. Frangi, editors, Medical Image Computing and
Computer-Assisted Intervention – MICCAI 2015, pages 234–
241, Cham, 2015. Springer International Publishing.
 Mohammad Sabokrou, Mohammad Khalooei, Mahmood
Fathy, and Ehsan Adeli.
Adversarially learned one-class
classiﬁer for novelty detection.
In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), June
 Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014.
 Sorina Smeureanu, Radu Tudor Ionescu, Marius Popescu,
and Bogdan Alexe. Deep appearance features for abnormal
behavior detection in video. In Sebastiano Battiato, Giovanni
Gallo, Raimondo Schettini, and Filippo Stanco, editors, Image Analysis and Processing - ICIAP 2017, pages 779–789,
Cham, 2017. Springer International Publishing.
 Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas
Brox, and Martin A. Riedmiller. Striving for simplicity: The
all convolutional net. CoRR, abs/1412.6806, 2014.
 Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. Dropout: A simple
way to prevent neural networks from overﬁtting. Journal of
Machine Learning Research, 15:1929–1958, 2014.
 Waqas Sultani, Chen Chen, and Mubarak Shah. Real-world
anomaly detection in surveillance videos. In 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 6479–6488, June 2018.
 Qianru Sun, Hong Liu, and Tatsuya Harada. Online growing
neural gas for anomaly detection in changing surveillance
scenes. Pattern Recognition, 64:187 – 201, 2017.
 Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich.
Going deeper with
convolutions. In 2015 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 1–9, June 2015.
 Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In 2016 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
2818–2826, June 2016.
 Heng Wang and Cordelia Schmid. Action recognition with
improved trajectories. In 2013 IEEE International Conference on Computer Vision, pages 3551–3558, Dec 2013.
 Zhou Wang, Alan Conrad Bovik, Hamid Rahim Sheikh, and
Eero P. Simoncelli. Image quality assessment: from error
visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600–612, April 2004.
 Dan Xu, Yan Yan, Elisa Ricci, and Nicu Sebe. Detecting
anomalous events in videos by learning deep representations
of appearance and motion. Computer Vision and Image Understanding, 156:117 – 127, 2017. Image and Video Understanding in Big Data.
 Andrei Zaharescu and Richard Wildes.
Anomalous behaviour detection using spatiotemporal oriented energies,
subset inclusion histogram comparison and event-driven processing. In Kostas Daniilidis, Petros Maragos, and Nikos
Paragios, editors, Computer Vision – ECCV 2010, pages
563–576, Berlin, Heidelberg, 2010. Springer Berlin Heidelberg.
 Tianzhu Zhang, Hanqing Lu, and Stan Z. Li. Learning semantic scene models by object classiﬁcation and trajectory
clustering. In 2009 IEEE Conference on Computer Vision
and Pattern Recognition, pages 1940–1947, June 2009.
 Ying Zhang, Huchuan Lu, Lihe Zhang, Xiang Ruan, and
Shun Sakai. Video anomaly detection based on locality sensitive hashing ﬁlters.
Pattern Recognition, 59:302 – 311,
2016. Compositional Models and Structured Learning for
Visual Recognition.
 Bin Zhao, Li Fei-Fei, and Eric P. Xing. Online detection
of unusual events in videos via dynamic sparse coding. In
CVPR 2011, pages 3313–3320, June 2011.
Anomaly Detection in Video Sequence with Appearance-Motion Correspondence
—– Supplementary Material —–
Trong-Nguyen Nguyen, Jean Meunier
DIRO, University of Montreal
{nguyetn, meunier}@iro.umontreal.ca
This supplementary material provides these contents:
• ROC curves of our frame-level scores on the CUHK
Avenue and UCSD Ped2 datasets, and Precision-
Recall (PR) curves on the trafﬁc datasets.
• Experimental results of using either appearance reconstruction stream or motion prediction stream for
score estimation.
• Impact of integrating motion stream and patch-based
score estimation.
• Visualization of some feature maps in different blocks
obtained in our experiments.
• Reconstructed frames and predicted motions after
some training epochs.
1. Flow ﬁeld color coding
Figure 1 shows the color coding used in visualization of
our optical ﬂow in the main paper. This color coding is
similar to where the color indicates motion direction and
the saturation corresponds to the pixel displacement.
displacement
Figure 1: The color coding used for visualizing our optical
ﬂow in the main paper.
False positive rate
True positive rate
Our method, AUC = 0.962
FRCN action, AUC = 0.922
Hashing ﬁlters, AUC = 0.910
Double fusion, AUC = 0.908
False positive rate
True positive rate
CUHK Avenue
Our method, AUC = 0.869
Sparse dict., AUC = 0.809
Disc. learning, AUC = 0.783
Traﬃc-Belleview
Our method, AP = 0.751
Appearance SSIM, AP = 0.830
GANomaly, AP = 0.735
Global feature, AP = 0.776
ALOCC D(R(X)), AP = 0.805
Traﬃc-Train
Our method, AP = 0.490
Appearance SSIM, AP = 0.798
GANomaly, AP = 0.194
Global feature, AP = 0.216
ALOCC D(R(X)), AP = 0.237
ROC curves on the Ped2 and Avenue
datasets. Bottom: PR curves on the Belleview and Train
datasets. The corresponding Area Under Curve (AUC) and
Average Precision (AP) are also provided. Best viewed in
2. Evaluation curves on 4 datasets
Figure 2 displays ROC and PR curves of our frame-level
scores obtained in the experiments. Some state-of-the-art
methods are also added into the ﬁgure to provide a visual
comparison. These methods consist of FRCN action ,
hashing ﬁlters , AMDN double fusion , sparse dictionary , discriminative learning , GANomaly , autoencoder with global features and ALOCC . The ROC
curves of the ﬁrst 5 mentioned studies are provided in their
original papers.
 
3. Experimental results on single streams
As indicated in the main paper, our frame-level score is
estimated as a weighted combination of two partial scores
S = log[wF SF ( ˜P)] + λS log[wISI( ˜P)]
where SF ( ˜P) and SI( ˜P) are respectively partial scores calculated from the motion and appearance streams, wF and
wI are corresponding weights computed from the training
data, λS is a hyperparameter controlling the contribution of
partial scores to the summation, and ˜P is the patch providing the highest value of SF in the considering frame.
In this section, we present the evaluation results in the
cases of using only one of the two partial scores as the
frame-level score indicator (see Figure 3). Both AUC and
average precision (AP) measures are also provided for a
convenient comparison with other studies. Note that the
AUC and AP values are not comparable though there is a
connection between ROC and PR spaces, and they are both
affected by the balance of the two classes in each dataset .
Figure 3 shows that the combination of the two partial
scores improved the detection ability since its AUC and AP
increased compared with individual measures. For the Subway datasets, this combination reduced the risk of false detection, but the number of detected anomalous events was
also slightly decreased (Subway Entrance).
4. Impact of motion stream and patch-based
score estimation for anomaly detection
Table 1 shows the experimental results obtained on the
6 benchmark datasets using patch-based normality assessment and SSIM on appearance stream. We also remove the
motion stream and the motion-oriented discriminator (Sections 3.3 & 3.4 in main paper) for the assessment of motion impact. SSIM was suggested due to the errors in op-
Proposed architecture with motion stream
Architecture without motion stream
Note: True Positive / False Alarm for Entrance, Exit; †AUROC; ‡AP.
Table 1: Experimental results using patch-based normality
assessment and SSIM on appearance stream.
tical ﬂow measurement (camera jitter in Trafﬁc-Train and
low-quality frames in Belleview). Without motion stream,
the model becomes a reconstruction auto-encoder of single
frame, and the results on the ﬁrst 5 datasets still demonstrate
the efﬁciency of the proposed patch-based normality score.
Appearance
Combination
Frame-level score estimation
Evaluation value
Area Under Curve
Average Precision
Appearance
Combination
Frame-level score estimation
Evaluation value
CUHK Avenue
Area Under Curve
Average Precision
Appearance
Combination
Frame-level score estimation
Evaluation value
Area Under Curve
Average Precision
Appearance
Combination
Frame-level score estimation
Evaluation value
Traffic-Train
Area Under Curve
Average Precision
Appearance
Combination
Frame-level score estimation
Number of events
Subway Entrance
True Detection
False Alarm
Appearance
Combination
Frame-level score estimation
Number of events
Subway Exit
True Detection
False Alarm
Figure 3: Evaluation results of our model using only the appearance reconstruction (Conv-AE), the motion prediction
(U-Net) and their combination. The frame-level AUROC
and Average Precision scores are provided for the Ped2, Avenue, Belleview and Train datasets. The numbers of true
positive detections (i.e. true positive) and false alarms are
presented for the Entrance and Exit datasets.
Using motion signiﬁcantly improved results of the ﬁrst 4
datasets while SSIM on appearance stream was just slightly
reduced for the others (i.e. 0.830 vs. 0.832 for Belleview,
and 0.798 vs. 0.808 for Trafﬁc-Train).
5. Feature maps
A visualization of some feature maps given an input
frame for each dataset is shown in Figure 4. Each example
is represented by 4 rows of images. We illustrate two feature
maps (grouped in a red bounding box) for each layer block,
except for the Inception module where 4 feature maps are
Training epoch
Batch size
CUHK Avenue
Subway Entrance
Subway Exit
Trafﬁc-Train
Trafﬁc-Belleview
Table 2: Number of training epochs and batch size in our
experiments. These values were selected according to the
number of training images in each dataset and the memory
capacity of our hardware (Intel i7-7700K, 16 GB memory,
GTX 1080).
shown for the 1 × 1, 3 × 3, 5 × 5 and 7 × 7 convolutional
ﬁlters. The ﬁrst two rows include the input frame, activation
maps resulting from the Inception module and subsequent
blocks of the shared encoder. The third and fourth rows respectively consist of feature maps in the decoder of motion
and appearance streams. The value of units in each map was
normalized to provide a good visualization.
Figure 4 shows that our motion stream attempts to emphasize the image edges to provide a smooth optical ﬂow
(because FlowNet2 was used as the ground truth motion
estimator) while the other one tends to reconstruct appearance textures. By observing all feature maps provided by
the Inception module, we found that 7 × 7 convolutional
ﬁlters extracted informative details only on the CUHK Avenue, Subway Entrance and Trafﬁc-Belleview datasets (best
viewed when the feature map is enlarged). It demonstrated
the reasonable use of Inception module right after the input
layer to let the network automatically decides its appropriate low-level ﬁlter sizes.
6. Model optimization during training phase
In this section, we show the outputs of the proposed
model after some training epochs given the same input for
each dataset. The number of training epochs and batch size
are presented in Table 2.
In Figure 5, the correspondence between a reconstructed
frame and its predicted motion can be clearly observed. A
sharper frame would be obtained together with a motion
with more details (e.g. epochs 2 vs. 4 in the UCSD Ped2 experiment) as the number of epochs increases. It also demonstrates that the model encountered difﬁculty in optimizing
the two streams on the Trafﬁc-Train dataset due to the sudden change of lighting and camera jitter. However, the overall structure of the acquired scene was still preserved (e.g.
poles and seats). The use of SSIM on the input frame and
its reconstruction hence improved the anomaly detection results (presented in the main paper).