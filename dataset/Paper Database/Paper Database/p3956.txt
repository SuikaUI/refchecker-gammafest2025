Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4884–4895
Florence, Italy, July 28 - August 2, 2019. c⃝2019 Association for Computational Linguistics
Handling Divergent Reference Texts when Evaluating
Table-to-Text Generation
Bhuwan Dhingra† ∗
Manaal Faruqui‡
Ankur Parikh‡
Ming-Wei Chang‡
Dipanjan Das‡
William W. Cohen†‡
† Carnegie Mellon University
‡ Google Research
 
{mfaruqui,aparikh,mingweichang,dipanjand,wcohen}@google.com
Automatically constructed datasets for generating text from semi-structured data (tables),
such as WikiBio , often contain reference texts that diverge from
the information in the corresponding semistructured data. We show that metrics which
rely solely on the reference texts, such as
BLEU and ROUGE, show poor correlation
with human judgments when those references
We propose a new metric, PAR-
ENT, which aligns n-grams from the reference and generated texts to the semi-structured
data before computing their precision and recall.
Through a large scale human evaluation study of table-to-text models for WikiBio,
we show that PARENT correlates with human
judgments better than existing text generation
metrics. We also adapt and evaluate the information extraction based evaluation proposed
in Wiseman et al. , and show that PAR-
ENT has comparable correlation to it, while
being easier to use. We show that PARENT
is also applicable when the reference texts are
elicited from humans using the data from the
WebNLG challenge.1
Introduction
The task of generating natural language descriptions of structured data (such as tables) 
has seen a growth in interest with the rise of sequence to sequence models that provide an easy
way of encoding tables and generating text from
them .
For text generation tasks, the only gold standard
metric is to show the output to humans for judging its quality, but this is too expensive to apply
∗Work done during an internship at Google.
 
~bdhingra/pages/parent.html
repeatedly anytime small modiﬁcations are made
to a system. Hence, automatic metrics that compare the generated text to one or more reference
texts are routinely used to compare models . For table-to-text generation,
automatic evaluation has largely relied on BLEU
 and ROUGE .
The underlying assumption behind these metrics
is that the reference text is gold-standard, i.e., it
is the ideal target text that a system should generate. In practice, however, when datasets are collected automatically and heuristically, the reference texts are often not ideal. Figure 1 shows an
example from the WikiBio dataset . Here the reference contains extra information which no system can be expected to produce
given only the associated table. We call such reference texts divergent from the table.
We show that existing automatic metrics, including BLEU, correlate poorly with human judgments when the evaluation sets contain divergent
references (§5.4). For many table-to-text generation tasks, the tables themselves are in a pseudonatural language format , and E2E-NLG ). In such cases we propose to compare the
generated text to the underlying table as well to
improve evaluation.
We develop a new metric,
PARENT (Precision And Recall of Entailed Ngrams from the Table) (§3). When computing precision, PARENT effectively uses a union of the
reference and the table, to reward correct information missing from the reference. When computing
recall, it uses an intersection of the reference and
the table, to ignore extra incorrect information in
the reference. The union and intersection are computed with the help of an entailment model to decide if a text n-gram is entailed by the table.2 We
2Here “entailed” means can be reasonably inferred from
Figure 1: A table from the WikiBio dataset (right), its reference description and three hypothetical generated texts
with scores assigned to them by automatic evaluation metrics. Text which cannot be inferred from the table is in
red, and text which can be inferred but isn’t present in the reference is in green. PARENT is our proposed metric.
show that this method is more effective than using the table as an additional reference. Our main
contributions are:
• We conduct a large-scale human evaluation of
the outputs from 16 table-to-text models on
1100 examples from the WikiBio dataset, many
of which have divergent references (§5.2).
• We propose a new metric, PARENT (§3), and
show that it improves correlation with human
judgments over existing metrics, both when
comparing similar systems (such as different
hyperparameters of a neural network) and when
comparing vastly different systems (such as
template-based and neural models).
• We also develop information extraction based
metrics, inspired from Wiseman et al. ,
by training a model to extract tables from the
reference texts (§4). We ﬁnd that these metrics
have comparable correlation to PARENT, with
the latter being easier to use out of the box.
• We analyze the sensitivity of the metrics to divergence by collecting labels for which references contain only information also present in
the tables. We show that PARENT maintains
high correlation as the number of such examples is varied. (§5.5).
• We also demonstrate the applicability of PAR-
ENT on the data released as part of the
WebNLG challenge ,
where the references are elicited from humans,
and hence are of high quality (§5.4).
Table-to-Text Generation
We brieﬂy review the task of generating natural language descriptions of semi-structured data,
which we refer to as tables henceforth . Tables can
be expressed as set of records T
where each record is a tuple (entity, attribute,
value). When all the records are about the same
entity, we can truncate the records to (attribute,
value) pairs. For example, for the table in Figure 1,
the records are {(Birth Name, Michael Dahlquist),
 , ...}. The task is to generate a text G which summarizes the records in a
ﬂuent and grammatical manner.3 For training and
evaluation we further assume that we have a reference description R available for each table. We
let DM = {(T i, Ri, Gi)}N
i=1 denote an evaluation
set of tables, references and texts generated from
a model M, and Ri
n denote the collection of
n-grams of order n in Ri and Gi, respectively. We
use #Rin(g) to denote the count of n-gram g in
n, and #Gin,Rin(g) to denote the minimum of its
counts in Ri
n. Our goal is to assign a score
to the model, which correlates highly with human
judgments of the quality of that model.
Divergent References.
In this paper we are interested in the case where reference texts diverge
from the tables. In Figure 1, the reference, though
technically correct and ﬂuent, mentions information which cannot be gleaned from the associated table. It also fails to mention useful information which a generation system might correctly
include (e.g. candidate 3 in the ﬁgure). We call
such references divergent from the associated table. This phenomenon is quite common – in WikiBio we found that 62% of the references mention extra information (§5.5). Divergence is common in human-curated translation datasets as well
 .
How does divergence affect automatic evalua-
3In some cases the system is expected to summarize all
the records (e.g. WebNLG); in others the system is expected
to only summarize the salient records (e.g. WikiBio).
tion? As a motivating example, consider the three
candidate generations shown in Figure 1. Clearly,
candidate 1 is the worst since it “hallucinates”
false information, and candidate 3 is the best since
it is correct and mentions more information than
candidate 2. However, BLEU and ROUGE, which
only compare the candidates to the reference, penalize candidate 3 for both excluding the divergent
information in the reference (in red) and including correct information from the table (in green).4
PARENT, which compares to both the table and
reference, correctly ranks the three candidates.
PARENT evaluates each instance (T i, Ri, Gi)
separately, by computing the precision and recall
of Gi against both T i and Ri.
Entailment Probability.
The table is in a semistructured form, and hence not directly comparable to the unstructured generated or reference
texts. To bridge this gap, we introduce the notion
of entailment probability, which we deﬁne as the
probability that the presence of an n-gram g in a
text is “correct” given the associated table. We denote this probability as w(g) = Pr(g ⇐T i). Estimating this probability is in itself a challenging
language understanding task, since the information in the table may be expressed in varied forms
in text. Here, we describe two simple models of
lexical entailment, inspired by work on the Recognizing Textual Entailment Challenge . We found these simple models to be effective; while more sophisticated models may be used
if there are complex inferences between the table
and text, they are beyond the scope of this paper.
1. Word Overlap Model: Let ¯T i denote all the
lexical items present in the table T i, including
both attribute names and their values. Then,
j=1 1(gj ∈¯T i)/n, where n is the
length of g, and gj is the jth token in g.
2. Co-occurrence Model: Originally proposed for the RTE task,
this model computes the probability of a term
gj in the n-gram being entailed by the table as
the maximum of its probabilities of being en-
4BLEU is usually computed at the corpus-level, however
here we show its value for a single sentence purely for illustration purposes. The remaining BLEU scores in this paper
are all at the corpus-level.
tailed by each lexical item v in the table:
Pr(gj ⇐T i) = max
v∈¯T i Pr(gj ⇐v).
Pr(gj ⇐v) is estimated using co-occurrence
counts from a training set of table-reference
Then the overall probability of the ngram being entailed is taken as the geometric
average w(g) =
j=1 Pr(gj ⇐T i)
We note that these models are not sensitive to
paraphrases between the table and text. For tasks
where this is important, embedding-based similarities may be used, but those are beyond the scope
of this paper. Next we discuss how to compute the
precision and recall of the generation.
Entailed Precision.
When computing precision,
we want to check what fraction of the n-grams in
n are correct. We consider an n-gram g to be
correct either if it occurs in the reference Ri
if it has a high probability of being entailed by the
table (i.e. w(g) is high). Let Pr(g ∈Ri
#Gin,Rin(g)
denote the probability that an n-gram
n also appears in Ri
n. Then, the entailed precision En
p for n-grams of order n is given by:
n) + Pr(g /∈Ri
g∈Gin #Gin(g)
g∈Gin #Gin(g)w(g) + #Gin,Rin(g)[1 −w(g)]
g∈Gin #Gin(g)
In words, an n-gram receives a reward of 1 if it
appears in the reference, with probability Pr(g ∈
n), and otherwise it receives a reward of w(g).
Both numerator and denominator are weighted by
the count of the n-gram in Gi
n. Pr(g ∈Ri
n) rewards an n-gram for appearing as many times as
it appears in the reference, not more. We combine
precisions for n-gram orders 1-4 using a geometric
5Glickman and Dagan used a product instead of
geometric mean. Here we use a geometric mean to ensure
that n-grams of different lengths have comparable probabilities of being entailed.
6It is unlikely that an automated system produces the same
extra n-gram as present in the reference, thus a match with
the reference n-gram is considered positive. For example, in
Figure 1, it is highly unlikely that a system would produce
“Silkworm” when it is not present in the table.
average, similar to BLEU:
Entailed Recall.
We compute recall against
both the reference (Er(Ri)), to ensure proper sentence structure in the generated text, and the table
(Er(T i)), to ensure that texts which mention more
information from the table get higher scores (e.g.
candidate 3 in Figure 1). These are combined using a geometric average:
Er = Er(Ri)(1−λ)Er(T i)λ
The parameter λ trades-off how much the generated text should match the reference, versus how
much it should cover information from the table.
The geometric average, which acts as an AND operation, ensures that the overall recall is high only
when both the components are high. We found
this necessary to assign low scores to bad systems
which, for example, copy values from the table
without phrasing them in natural language.
When computing Er(Ri), divergent references
will have n-grams with low w(g). We want to exclude these from the computation of recall, and
hence their contributions are weighted by w(g):
g∈Rin #Gin,Rin(g)w(g)
g∈Rin #Rin(g)w(g) .
Similar to precision, we combine recalls for n =
1-4 using a geometric average to get Er(Ri).
For computing Er(T i), note that a table is a
set of records T i = {rk}K
k=1. For a record rk,
let ¯rk denote its string value . Then:
Er(T i) = 1
|¯rk|LCS(¯rk, Gi),
where ¯rk denotes the number of tokens in the value
string, and LCS(x, y) is the length of the longest
common subsequence between x and y. The LCS
function, borrowed from ROUGE, ensures that entity names in ¯rk appear in the same order in the text
as the table. Higher values of Er(T i) denote that
more records are likely to be mentioned in Gi.
The entailed precision and recall are combined
into an F-score to give the PARENT metric for one
instance. The system-level PARENT score for a
model M is the average of instance level PARENT
scores across the evaluation set:
PARENT(Gi, Ri, T i)
Smoothing & Multiple References.
The danger with geometric averages is that if any of the
components being averaged become 0, the average
will also be 0. Hence, we adopt a smoothing technique from Chen and Cherry that assigns
a small positive value ϵ to any of En
r (Ri) and
Er(T i) which are 0. When multiple references are
available for a table, we compute PARENT against
each reference and take the maximum as its overall
score, similar to METEOR proposed to use an auxiliary model, trained to extract structured records
from text, for evaluation.
However, the extraction model presented in that work is limited to the
closed-domain setting of basketball game tables
and summaries.
In particular, they assume that
each table has exactly the same set of attributes for
each entity, and that the entities can be identiﬁed
in the text via string matching. These assumptions
are not valid for the open-domain WikiBio dataset,
and hence we train our own extraction model to
replicate their evaluation scheme.
Our extraction system is a pointer-generator
network , which learns to produce
a linearized version of the table from the text.8 The
network learns which attributes need to be populated in the output table, along with their values.
It is trained on the training set of WikiBio. At test
7For WikiBio, on average λ = 0.6 using this heuristic.
8 All (attribute, value) pairs are merged into 1 long string
using special separator tokens between them.
time we parsed the output strings into a set of (attribute, value) tuples and compare it to the ground
truth table. The F-score of this text-to-table system
was 35.1%, which is comparable to other challenging open-domain settings .
More details are included in the Appendix A.1.
Given this information extraction system, we
consider the following metrics for evaluation,
along the lines of Wiseman et al. . Content Selection (CS): F-score for the (attribute,
value) pairs extracted from the generated text
compared to those extracted from the reference.
Relation Generation (RG): Precision for the (attribute, value) pairs extracted from the generated
text compared to those in the ground truth table.
RG-F: Since our task emphasizes the recall of information from the table as well, we consider another variant which computes the F-score of the
extracted pairs to those in the table. We omit the
content ordering metric, since our extraction system does not align records to the input text.
Experiments & Results
In this section we compare several automatic evaluation metrics by checking their correlation with
the scores assigned by humans to table-to-text
models. Speciﬁcally, given l models M1, . . . , Ml,
and their outputs on an evaluation set, we show
these generated texts to humans to judge their
quality, and obtain aggregated human evaluation
scores for all the models, ¯h = (h1, . . . , hl) (§5.2).
Next, to evaluate an automatic metric, we compute the scores it assigns to each model, ¯a =
(a1, . . . , al), and check the Pearson correlation between ¯h and ¯a .9
Data & Models
Our main experiments are on the WikiBio dataset
 , which is automatically constructed and contains many divergent references.
In §5.6 we also present results on the data released
as part of the WebNLG challenge.
We developed several models of varying quality for generating text from the tables in WikiBio.
This gives us a diverse set of outputs to evaluate
the automatic metrics on. Table 1 lists the models along with their hyperparameter settings and
their scores from the human evaluation (§5.2). Our
focus is primarily on neural sequence-to-sequence
methods since these are most widely used, but we
9We observed similar trends for Spearman correlation.
References
0.20 ± 0.03
-0.19 ± 0.04
-0.28 ± 0.03
Seq2Seq + Att
-0.12 ± 0.03
1,4,8 0,1,2,3
0.40 ± 0.03
Table 1: Models used for WikiBio, with the human
evaluation scores for these model outputs and the reference texts.
Pointer-Generator network.
Human scores computed using Thurstone’s method
 .
also include a template-based baseline. All neural
models were trained on the WikiBio training set.
Training details and sample outputs are included
in Appendices A.2 & A.3.
We divide these models into two categories and
measure correlation separately for both the categories.
The ﬁrst category, WikiBio-Systems,
includes one model each from the four families
listed in Table 1. This category tests whether a
metric can be used to compare different model
families with a large variation in the quality of
their outputs.
The second category, WikiBio-
Hyperparams, includes 13 different hyperparameter settings of PG-Net , which
was the best performing system overall. 9 of these
were obtained by varying the beam size and length
normalization penalty of the decoder network , and the remaining 4 were obtained
by re-scoring beams of size 8 with the information
extraction model described in §4. All the models
in this category produce high quality ﬂuent texts,
and differ primarily on the quantity and accuracy
of the information they express. Here we are testing whether a metric can be used to compare similar systems with a small variation in performance.
This is an important use-case as metrics are often
used to tune hyperparameters of a model.
Human Evaluation
We collected human judgments on the quality of
the 16 models trained for WikiBio, plus the reference texts. Workers on a crowd-sourcing platform,
proﬁcient in English, were shown a table with
pairs of generated texts, or a generated text and the
reference, and asked to select the one they prefer.
Figure 2 shows the instructions they were given.
Paired comparisons have been shown to be superior to rating scales for comparing generated texts
Figure 2: Instructions to crowd-workers for comparing two generated texts.
 . However, for measuring correlation the comparisons need to be aggregated into real-valued scores, ¯h = (h1, . . . , hl),
for each of the l = 16 models. For this, we use
Thurstone’s method ,
which assigns a score to each model based on how
many times it was preferred over an alternative.
The data collection was performed separately
for models in the WikiBio-Systems and WikiBio-
Hyperparams categories. 1100 tables were sampled from the development set, and for each table
we got 8 different sentence pairs annotated across
the two categories, resulting in a total of 8800 pairwise comparisons. Each pair was judged by one
worker only which means there may be noise at
the instance-level, but the aggregated system-level
scores had low variance (cf. Table 1). In total
around 500 different workers were involved in the
annotation. References were also included in the
evaluation, and they received a lower score than
PG-Net, highlighting the divergence in WikiBio.
Compared Metrics
et al., 2002), ROUGE , METEOR
 , CIDEr and CIDEr-
D using their publicly
available implementations.
Information Extraction based: We compare the
CS, RG and RG-F metrics discussed in §4.
Text & Table: We compare a variant of BLEU,
denoted as BLEU-T, where the values from the
table are used as additional references.
T draws inspiration from iBLEU but instead rewards n-grams which match
the table rather than penalizing them.
PARENT, we compare both the word-overlap
model (PARENT-W) and the co-occurrence model
(PARENT-C) for determining entailment. We also
compare versions where a single λ is tuned on the
entire dataset to maximize correlation with human
judgments, denoted as PARENT*-W/C.
Hyperparams
0.518±0.07C,W
-0.585±0.15C,W
0.674±0.06C,W
-0.516±0.15C,W
0.646±0.06C,W
-0.372±0.16C,W
0.697±0.06C,W
-0.079±0.24C,W
0.548±0.07C,W
0.407±0.15C,W
0.735±0.06W
-0.604±0.16C,W
0.688±0.11W
0.587±0.14C,W
0.645±0.07C,W
0.749±0.12
0.753±0.06W
0.763±0.12
0.776±0.05W
0.755±0.12
0.912±0.03
0.763±0.12
0.976±0.01
0.793±0.11
0.982±0.01
0.844±0.10
Table 2: Correlation of metrics with human judgments
on WikiBio. A superscript of C/W indicates that the
correlation is signiﬁcantly lower than that of PARENT-
C/W using a bootstrap conﬁdence test for α = 0.1.
Correlation Comparison
We use bootstrap sampling (500 iterations) over
the 1100 tables for which we collected human annotations to get an idea of how the correlation of
each metric varies with the underlying data. In
each iteration, we sample with replacement, tables
along with their references and all the generated
texts for that table. Then we compute aggregated
human evaluation and metric scores for each of the
models and compute the correlation between the
two. We report the average correlation across all
bootstrap samples for each metric in Table 2. The
distribution of correlations for the best performing
metrics are shown in Figure 3.
Table 2 also indicates whether PARENT is signiﬁcantly better than a baseline metric. Graham
and Baldwin suggest using the William’s
test for this purpose, but since we are computing
correlations between only 4/13 systems at a time,
this test has very weak power in our case. Hence,
we use the bootstrap samples to obtain a 1 −α
conﬁdence interval of the difference in correlation
WikiBio-Systems
WikiBio-Hyperparams
Figure 3: Distribution of metric correlations across 500
bootstrap samples. PRT = PARENT.
between PARENT and any other metric and check
whether this is above 0 .
Correlations are higher for the systems category than the hyperparams category. The latter
is a more difﬁcult setting since very similar models are compared, and hence the variance of the
correlations is also high. Commonly used metrics
which only rely on the reference (BLEU, ROUGE,
METEOR, CIDEr) have only weak correlations
with human judgments. In the hyperparams category, these are often negative, implying that tuning models based on these may lead to selecting
worse models. BLEU performs the best among
these, and adding n-grams from the table as references improves this further (BLEU-T).
Among the extractive evaluation metrics, CS,
which also only relies on the reference, has poor
correlation in the hyperparams category.
and both variants of the PARENT metric achieve
the highest correlation for both settings. There is
no signiﬁcant difference among these for the hyperparams category, but for systems, PARENT-W
is signiﬁcantly better than the other two. While
RG-F needs a full information extraction pipeline
in its implementation, PARENT-C only relies on
co-occurrence counts, and PARENT-W can be
used out-of-the-box for any dataset. To our knowledge, this is the ﬁrst rigorous evaluation of using
information extraction for generation evaluation.
On this dataset, the word-overlap model showed
higher correlation than the co-occurrence model
for entailment.
In §5.6 we will show that for
the WebNLG dataset, where more paraphrasing
is involved between the table and text, the opposite is true. Lastly, we note that the heuristic for
selecting λ is sufﬁcient to produce high correlations for PARENT, however, if human annotations
are available, this can be tuned to produce signiﬁcantly higher correlations (PARENT*-W/C).
% Entailed
WikiBio-Systems
% Entailed
WikiBio-Hyperparams
Figure 4: Correlation of the metrics to human judgment
as the percentage of entailed examples in WikiBio is
In this section we further analyze the performance
of PARENT-W10 under different conditions, and
compare to the other best metrics from Table 2.
Effect of Divergence.
To study the correlation
as we vary the number of divergent references,
we also collected binary labels from workers for
whether a reference is entailed by the corresponding table. We deﬁne a reference as entailed when it
mentions only information which can be inferred
from the table. Each table and reference pair was
judged by 3 independent workers, and we used the
majority vote as the label for that pair. Overall,
only 38% of the references were labeled as entailed by the table. Fleiss’ κ was 0.30, which indicates a fair agreement. We found the workers
sometimes disagreed on what information can be
reasonably entailed by the table.
Figure 4 shows the correlations as we vary the
percent of entailed examples in the evaluation set
of WikiBio. Each point is obtained by ﬁxing the
desired proportion of entailed examples, and sampling subsets from the full set which satisfy this
proportion. PARENT and RG-F remain stable and
show a high correlation across the entire range,
whereas BLEU and BLEU-T vary a lot. In the hyperparams category, the latter two have the worst
correlation when the evaluation set contains only
entailed examples, which may seem surprising.
However, on closer examination we found that this
subset tends to omit a lot of information from the
tables. Systems which produce more information
than these references are penalized by BLEU, but
not in the human evaluation. PARENT overcomes
this issue by measuring recall against the table in
addition to the reference.
10The trends were similar for PARENT-C.
Table 3: Accuracy on making the same judgments
as humans between pairs of generated texts.
0.01∗/0.05†/0.10‡:
accuracy is signiﬁcantly higher
than the next best accuracy to the left using a paired
McNemar’s test.
Ablation Study.
We check how different components in the computation of PARENT contribute
to its correlation to human judgments.
Speciﬁcally, we remove the probability w(g) of an ngram g being entailed by the table from Eqs. 2
and 5.11 The average correlation for PARENT-W
drops to 0.168 in this case. We also try a variant
of PARENT with λ = 0, which removes the contribution of Table Recall (Eq. 4). The average correlation is 0.328 in this case. With these components, the correlation is 0.838, showing that they
are crucial to the performance of PARENT.
Sentence Level Discrimination.
Chaganty et al.
 point out that hill-climbing on an automatic
metric is meaningless if that metric has a low
instance-level correlation to human judgments. In
Table 3 we show the average accuracy of the metrics in making the same judgments as humans between pairs of generated texts. Both variants of
PARENT are signiﬁcantly better than the other
metrics, however the best accuracy is only 60% for
the binary task. This is a challenging task, since
there are typically only subtle differences between
the texts. Achieving higher instance-level accuracies will require more sophisticated language understanding models for evaluation.
WebNLG Dataset
To check how PARENT correlates with human
judgments when the references are elicited from
humans (and less likely to be divergent), we check
its correlation with the human ratings provided for
the systems competing in the WebNLG challenge
 . The task is to generate text
describing 1-5 RDF triples (e.g. John E Blaha,
birthPlace, San Antonio), and human ratings were
collected for the outputs of 9 participating systems
on 223 instances. These systems include a mix of
pipelined, statistical and neural methods. Each instance has upto 3 reference texts associated with
11When computing precision we set w(g) = 0, and when
computing recall we set w(g) = 1 for all g.
0.788±0.04 0.792±0.04 0.576±0.06 0.719
0.788±0.04 0.792±0.04 0.576±0.06 0.719
0.804±0.03 0.753±0.04 0.860±0.02 0.806
0.858±0.02 0.811±0.03 0.775±0.03 0.815
0.849±0.02 0.801±0.03 0.816±0.02 0.822
0.838±0.04 0.796±0.04 0.853±0.02 0.829
PARENT-W 0.821±0.03 0.768±0.04 0.887±0.02 0.825
0.851±0.03 0.809±0.04 0.877±0.02 0.846
Table 4: Average pearson correlation across 500 bootstrap samples of each metric to human ratings for each
aspect of the generations from the WebNLG challenge.
the RDF triples, which we use for evaluation.
The human ratings were collected on 3 distinct aspects – grammaticality, ﬂuency and semantics, where semantics corresponds to the degree to
which a generated text agrees with the meaning of
the underlying RDF triples. We report the correlation of several metrics with these ratings in Table 4.12 Both variants of PARENT are either competitive or better than the other metrics in terms of
the average correlation to all three aspects. This
shows that PARENT is applicable for high quality
references as well.
While BLEU has the highest correlation for the
grammar and ﬂuency aspects, PARENT does best
for semantics. This suggests that the inclusion of
source tables into the evaluation orients the metric
more towards measuring the ﬁdelity of the content
of the generation. A similar trend is seen comparing BLEU and BLEU-T. As modern neural text
generation systems are typically very ﬂuent, measuring their ﬁdelity is of increasing importance.
Between the two entailment models, PARENT-
C is better due to its higher correlation with the
grammaticality and ﬂuency aspects.
Distribution of λ.
The λ parameter in the calculation of PARENT decides whether to compute recall against the table or the reference (Eq. 4). Figure 5 shows the distribution of the values taken by
1 −λ using the heuristic described in §3 for instances in both WikiBio and WebNLG. For WikiBio, the recall of the references against the table
is generally low, and hence the recall of the generated text relies more on the table. For WebNLG,
where the references are elicited from humans,
this recall is much higher (often 1.0), and hence
12 We omit extractive evaluation metrics since no extraction systems are publicly available for this dataset, and developing one is beyond the scope of this work.
Figure 5: Histogram of the recall of the references
against the table (Eq. 6), which is used to set 1 −λ.
Lower values indicate that the metric relies more on
the table and less on the reference.
the recall of the generated text relies more on the
reference.
Related Work
Over the years several studies have evaluated automatic metrics for measuring text generation performance . The only consensus from these studies
seems to be that no single metric is suitable across
all tasks. A recurring theme is that metrics like
BLEU and NIST are not suitable for judging content quality in NLG. Recently,
Novikova et al. did a comprehensive study
of several metrics on the outputs of state-of-the-art
NLG systems, and found that while they showed
acceptable correlation with human judgments at
the system level, they failed to show any correlation at the sentence level. Ours is the ﬁrst study
which checks the quality of metrics when tableto-text references are divergent. We show that in
this case even system level correlations can be unreliable.
Hallucination refers to when an NLG system generates
text which mentions extra information than what
is present in the source from which it is generated.
Divergence can be viewed as hallucination in the
reference text itself. PARENT deals with hallucination by discounting n-grams which do not overlap with either the reference or the table.
PARENT draws inspiration from iBLEU , a metric for evaluating paraphrase generation, which compares the generated
text to both the source text and the reference.
While iBLEU penalizes texts which match the
source, here we reward such texts since our task
values accuracy of generated text more than the
need for paraphrasing the tabular content . Similar to SARI for text simpliﬁcation and Q-BLEU for question
generation , PARENT
falls under the category of task-speciﬁc metrics.
Conclusions
We study the automatic evaluation of table-to-text
systems when the references diverge from the table. We propose a new metric, PARENT, which
shows the highest correlation with humans across
a range of settings with divergent references in
We also perform the ﬁrst empirical
evaluation of information extraction based metrics
 , and ﬁnd RG-F to be effective. Lastly, we show that PARENT is comparable to the best existing metrics when references
are elicited by humans on the WebNLG data.
Acknowledgements
Bhuwan Dhingra is supported by a fellowship
from Siemens, and by grants from Google. We
thank Maruan Al-Shedivat, Ian Tenney, Tom
Kwiatkowski, Michael Collins, Slav Petrov, Jason
Baldridge, David Reitter and other members of the
Google AI Language team for helpful discussions
and suggestions. We thank Sam Wiseman for sharing data for an earlier version of this paper. We
also thank the anonymous reviewers for their feedback.