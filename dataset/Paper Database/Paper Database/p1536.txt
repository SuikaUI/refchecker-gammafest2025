More is Less: A More Complicated Network with Less Inference Complexity∗
Xuanyi Dong1†, Junshi Huang2, Yi Yang1, Shuicheng Yan2,3
1CAI, University of Technology Sydney, 2360 AI Institute, 3National University of Singapore
 ; 
 ; 
In this paper, we present a novel and general network
structure towards accelerating the inference process of convolutional neural networks, which is more complicated in
network structure yet with less inference complexity. The
core idea is to equip each original convolutional layer
with another low-cost collaborative layer (LCCL), and the
element-wise multiplication of the ReLU outputs of these
two parallel layers produces the layer-wise output.
combined layer is potentially more discriminative than the
original convolutional layer, and its inference is faster for
two reasons: 1) the zero cells of the LCCL feature maps will
remain zero after element-wise multiplication, and thus it is
safe to skip the calculation of the corresponding high-cost
convolution in the original convolutional layer; 2) LCCL
is very fast if it is implemented as a 1 × 1 convolution or
only a single ﬁlter shared by all channels. Extensive experiments on the CIFAR-10, CIFAR-100 and ILSCRC-2012
benchmarks show that our proposed network structure can
accelerate the inference process by 32% on average with
negligible performance drop.
1. Introduction
Despite the continuously improved performance of convolutional neural networks (CNNs) ,
their computation costs are still tremendous. Without the
support of high-efﬁciency servers, it is hard to establish
CNN models on real-world applications. For example, to
process a 224 × 224 image, AlexNet requires 725M
FLOPs with 61M parameters, VGG-S involves 2640M
FLOPs with 103M parameters, and GoogleNet needs
1566M FLOPs with 6.9M parameters. Therefore, to leverage the success of deep neural networks on mobile devices
with limited computational capacity, accelerating network
inference has become imperative.
∗This paper was accepted by the IEEE CVPR 2017
†This work was done when Xuanxi Dong was an Intern at 360 AI Institute.
bighorn, bighorn sheep
Collaborative
Intermediate
Result (after ReLU)
Conv Layers
fully connected
Figure 1. Basic acceleration block. The orange panel in the ﬁgure
shows two different kinds of low-cost collaborative kernels. One
uses 1 × 1 convolution, and the other uses shared kernels (W
j for i, j ∈[1, T]). The black response map represents the output of the original convolutional layer with the kernel W, and the
orange response map is generated by the low-cost collaborative
layer. The purple cells represent the zero elements, of which the
calculation of corresponding positions can be skipped in the original convolutional layer. We apply element-wise multiplication on
the activated response maps from the original convolutional layer
and low-cost layer to generate the ﬁnal results of this basic acceleration block.
In this paper, we investigate the acceleration of CNN
models based on the observation that the response maps
of many convolutional layers are usually sparse after
ReLU activation. Therefore, instead of fully calculating the layer response, we can skip calculating the zero cells
in the ReLU output and only compute the values of non-zero
cells in each response map. Theoretically, the locations of
zero cells can be predicted by a lower cost layer. The values of non-zero cells from this lower-cost layer can be collaboratively updated by the responses of the original ﬁlters.
Eventually, the low-cost collaborative layer (LCCL) accompanied by the original layer constitute the basic element of
our proposed low-cost collaborative network (LCCN).
 
To equip each original convolutional layer with a LCCL,
we apply an element-wise multiplication on the response
maps from the LCCL and the original convolutional layer,
as illustrated in Fig. 1. In the training phase, this architecture can be naturally trained by the existing stochastic gradient descent (SGD) algorithm with backpropagation. First
we calculate the response map V
′ of the LCCL after the
activation layer, and use V
′ to guide the calculation of the
ﬁnal response maps.
Despite the considerable amount of research where a
sparse-based framework is used to accelerate the network
inference, e.g. , we claim that LCCN
is unique. Generally, most of these sparsity-based methods integrate the sparsity property as a regularizer into the learning of parameters, which usually harms
the performance of network. Moreover, to further accelerate performance, some methods even arbitrarily zeroize
the values of the response maps according to a pre-deﬁned
threshold. Compared with these methods, our LCCN automatically sets the negatives as zero, and precisely calculates the positive values in the response map with the help
of the LCCL. This two-stream strategy reaches a remarkable acceleration rate while maintaining a comparable performance level to the original network.
The main contributions are summarized as follows:
• We propose a general architecture to accelerate CNNs,
which leverages low-cost collaborative layers to accelerate each convolutional layer.
• To the best of our knowledge, this is the ﬁrst work
to leverage a low-cost layer to accelerate the network.
Equipping each convolutional layer with a collaborative layer is quite different from the existing acceleration algorithms.
• Experimental studies show signiﬁcant improvements
by the LCCN on many deep neural networks when
compared with existing methods (e.g., a 34% speedup
on ResNet-110).
2. Related Work
Tensor decomposition with low-rank
approximation-based methods are commonly used to accelerate deep convolutional networks. For example, in ,
the authors exploited the redundancy between convolutional
ﬁlters and used low-rank approximation to compress convolutional weight tensors and fully connected weight matrices. Yang et al. use an adaptive fastfood transform
was used to replace a fully connected layer with a series of
simple matrix multiplications, rather than the original dense
and large ones. Liu et al. propose a sparse decomposition to reduce the redundancy in convolutional parameters.
In , the authors used generalized singular vector decomposition (GSVD) to decompose an original layer to two
approximated layers with reduced computation complexity.
Fixed Point.
Some popular approaches to accelerate
test phase computation are based on “ﬁxed point”. In ,
the authors trained deep neural networks with a dynamic
ﬁxed point format, which achieves success on a set of stateof-the-art neural networks. Gupta et al. use stochastic rounding to train deep networks with 16-bit wide ﬁxedpoint number representation. In , a standard network
with binary weights represented by 1-bit was trained to
speed up networks. Then, Rastegari et al. further explored binary networks and expanded it to binarize the data
tensor of each layer, increasing the speed by 57 times.
Product Quantization. Some other researchers focus
on product quantization to compress and accelerate CNN
models. The authors of proposed a framework to accelerate the test phase computation process with the network
parameters quantized and learn better quantization with error correction.
Han et al. proposed to use a pruning stage to reduce the connections between neurons, and
then ﬁne tuned networks with weight sharing to quantify
the number of bits of the convolutional parameters from
32 to 5. In another work , the authors trained neural
networks with extremely low precision, and extended success to quantized recurrent neural networks. Zhou et al.
 generalized the method of binary neural networks to
allow networks with arbitrary bit-width in weights, activations, and gradients.
Sparsity. Some algorithms exploit the sparsity property
of convolutional kernels or response maps in CNN architecture. In , many neurons were decimated by incorporating sparse constraints into the objective function. In ,
a CNN model was proposed to process spatially-sparse inputs, which can be exploited to increase the speed of the
evaluation process. In , the authors used the groupsparsity regularizer to prune the convolutional kernel tensor
in a group-wise fashion. In , they increased the speed
of convolutional layers by skipping their evaluation at some
ﬁxed spatial positions. In , the authors presented a compression technique to prune the ﬁlters with minor effects on
the output accuracy.
Architecture. Some researchers improve the efﬁciency
of networks by carefully designing the structure of neural networks. In , a simple model was trained by distilling the knowledge from multiple cumbersome models,
which helps to reduce the computation cost while preserving the accuracy. Romero et al. extended the knowledge distillation approach to train a student network, which
is deeper but thinner than the teacher network, by extracting the knowledge of teacher network. In this way, the student network uses less parameters and running time to gain
considerable speedup compared with the teacher network.
Iandola et al. proposed a small DNN architecture to
achieve similar performance as AlexNet by only using 50x
fewer parameters and much less computation time via the
same strategy.
3. Low-Cost Collaborative Network
In this section, we present our proposed architecture
for the acceleration of deep convolutional neural networks.
First, we introduce the basic notations used in the following
sections. Then, we demonstrate the detailed formulation of
the acceleration block and extend our framework to general convolutional neural networks. Finally, we discuss the
computation complexity of our acceleration architecture.
3.1. Preliminary
Let’s recall the convolutional operator. For simplicity,
we discuss the problem without the bias term. Given one
convolution layer, we assume the shapes of input tensor U
and output tensor V are X×Y ×C and X×Y ×T, where X
and Y are the width and height of the response map, respectively. C and T represent the channel number of response
map U and V . A tensor W with size k × k × C × T is used
as the weight ﬁlter of this convolutional layer. Vt(x, y) represents the element of V (x, y, t). Then, the convolutional
operator can be written as:
Vt(x, y) =
Wt(i, j, c)U(x + i −1, y + i −1, c) (1)
where Wt(x, y) is the element of W(x, y, t).
In the LCCN, the output map of each LCCL should
have the same size as the corresponding convolutional layer,
which means that the shape of tensor V
′ is X×Y ×T. Similarly, we assume the weight kernel of V
′. Therefore,
the formula of the LCCN can be written as:
t (x, y) =
t (i, j, c)U(x + i −1, y + i −1, c) (2)
3.2. Overall Structure
Our acceleration block is illustrated in Fig. 1.
green block V ∗represents the ﬁnal response map collaboratively calculated by the original convolutional layer and
the LCCL. Generally, it can be formulated as:
t (x, y) =
t (x, y) = 0
t (x, y) × Vt(x, y)
t (x, y) ̸= 0
where V is the output response map from the original convolutional layer and V
′ is from LCCL.
In this formula, the element-wise product is applied to V
′ to calculate the ﬁnal response map. Due to the small
size of LCCL, the computation cost of V
′ can be ignored.
Meanwhile, since the zero cells in V
′ will stay zero after
the element-wise multiplication, the computation cost of V
is further reduced by skipping the calculation of zero cells
according to the positions of zero cells in V
′. Obviously,
this strategy leads to increasing speed in a single convolutional layer. To further accelerate the whole network, we
can equip most convolutional layers with LCCLs.
3.3. Kernel Selection
As illustrated in the orange box in Fig. 1, the ﬁrst form
exploits a 1×1×C×T kernel (k
′ = 1) for each original kernel to collaboratively estimate the ﬁnal response map. The
second structure uses a k
′ × C × 1 ﬁlter (we carefully
tune the parameter k’ and set k’ = k) shared across all the
original ﬁlters to calculate the ﬁnal result. Both these collaborative layers use less time during inference when compared with the original convolutional layer, thus they are
theoretically able to obtain acceleration.
In many efﬁcient deep learning frameworks such as
Caffe , the convolution operation is reformulated as matrix multiplication by ﬂattening certain dimensions of tensors, such as:
V = U ∗× W ∗
s.t. U ∗∈RXY ×k2C , W ∗∈Rk2C×T
Each row of the matrix U ∗is related to the spatial position of the output tensor transformed from the tensor U, and
W ∗is a reshaped tensor from weight ﬁlters W. These efﬁcient implementations take advantage of the high-efﬁciency
of BLAS libraries, e.g., GEMM1 and GEMV2.
Since each position of the skipped cell in V ∗corresponds
to one row of the matrix U ∗, we can achieve a realistic
speedup in BLAS libraries by reducing the matrix size in the
multiplication function. Different structures of the LCCL
need different implementations. For a k ×k ×C ×1 kernel,
the positions of the skipped cells in the original convolutional layer are the same in different channels. In this situation, we can reduce the size of U ∗to S
′ × k2C, where S
the number of non-zero elements in V
′. For a 1×1×C ×T
kernel, the positions of zero cells are different in different
channels, so it is infeasible to directly use the matrix-matrix
multiplication function to calculate the result of LCCL, i.e.
In this case, we have to separate the matrix-matrix
multiplication into multiple matrix-vector multiplications.
However, this approach is difﬁcult to achieve the desired
acceleration effect. The unsatisfying acceleration performance of 1 × 1 × C × T ﬁlters is caused by the inferior ef-
ﬁciency of multiple GEMV, and some extra operations also
cost more time (e.g., data reconstruction). Therefore, we
choose the k × k × C × 1 structure for our LCCL in our
experiments, and leave the acceleration of 1 × 1 × C × T
ﬁlters as our future work.
1matrix-matrix multiplication function
2matrix-vector multiplication function
Residual Block
Residual Block with LCCL
Collaborative
Layer (+Activ)
Collaborative
Layer (+Activ)
Conv Layer
Figure 2. Connection strategy of collaborating LCCL with the original convolutional layer. The top ﬁgure shows the pre-activation residual
block ; the bottom ﬁgure shows a “Bef-Aft” connection strategy to speed up the residual block. “Activ” represents that the collaborative
layer is followed by BN and ReLU activation. The ﬁrst LCCL receives the input tensor before being activated by BN and ReLU, and the
second one receives the input tensor after BN and ReLU. (Best viewed in the original pdf ﬁle.)
3.4. Sparsity Improvement
According to the previous discussion, the simplest way
for model acceleration is directly multiplying the tensor
′ and tensor V . However, this approach cannot achieve
favourable acceleration performance due to the low sparsity
To improve the sparsity of V
′, ReLU activation is a
simple and effective way by setting the negative values as
zeros. Moreover, due to the redundancy of positive activations, we can also append L1 loss in the LCCL to further
improve the sparsity rate. In this way, we achieve a smooth
L1L2(X) = µ∥X∥+ ρ|X| regularizer penalty for each V
However, there are thousands of free parameters in the regularizer term and the additional loss always degrades the
classiﬁcation performance, as it’s difﬁcult to achieve the
balance between the classiﬁcation performance and the acceleration rate.
Recently, the Batch Normalization (BN) is proposed
to improve the network performance and increase the convergence speed during training by stabilizing the distribution and reducing the internal covariate shift of input data.
During this process, we observe that the sparsity rate of each
LCCL is also increased. As shown in Table 1, we can ﬁnd
Without BN
res-block-1.2
res-block-2.2
res-block-2.2
Table 1. Sparsity of the LCCL for different activations with the
same training setting. “With BN” means we activate the response
map of the LCCL by BN and ReLU; “Without BN” means we only
use ReLU activation. “x.y” means the y-th block at x-th stage of
ResNet. We equip six convolutional layers with LCCL on ResNet-
that the BN layer advances the sparsity of LCCL followed
by ReLU activation, and thus can further improve the acceleration rate of our LCCN. We conjecture that the BN layer
balances the distribution of V
′ and reduces the redundancy
of positive values in V
′ by discarding some redundant activations. Therefore, to increase the acceleration rate, we
carefully integrate the BN layer into our LCCL.
Inspired by the pre-activation residual networks , we
exploit different strategies for activation and integration of
the LCCL. Generally, the input of this collaborative layer
can be either before activation or after activation. Taking
pre-activation residual networks as an example, we illustrate the “Bef-Aft” connection strategy at the bottom of
Fig. 2. “Bef” represents the case that the input tensor is from
the ﬂow before BN and ReLU activation. “Aft” represents
the case that the input tensor is the same to the original convolutional layer after BN and ReLU activation. According
to the “Bef-Aft” strategy in Fig. 2. the “Bef-Bef”, “Aft-Bef”
and “Aft-Aft” strategies can be easily derived. During our
experiments, we ﬁnd that input tensors with the “Bef” strategy are quite diverse when compared with the corresponding convolutional layer due to different activations. In this
strategy, the LCCL cannot accurately predict the zero cells
for the original convolutional layer. So it is better to use the
same input tensor as the original convolutional layer, i.e. the
“Aft” strategy.
3.5. Computation Complexity
Now we analyze the test-phase numerical calculation
with our acceleration architecture. For each convolutional
layer, the forward procedure mainly consists of two components, i.e. the low cost collaborative layer and the skipcalculation convolutional layer. Suppose the sparsity (ratio
of zero elements) of the response map V
′ is r. We formulate the detailed computation cost of the convolutional layer
and compare it with the one equipped with our LCCL.
Architecture
Speed-Up Ratio
XY TC(k′2 + k2r)
1 −(k′2/k2 + r)
(1 × 1 kernel)
XY TC(1 + k2r)
1 −(1/k2 + r)
(weight sharing)
XY Tk2(1 + Cr)
1 −(1/C + r)
Table 2. Theoretical numerical calculation acceleration for convolutional layers.
As shown in Table 2, the speedup ratio is highly dependent on r. The term 1/C costs little time since the channel
of the input tensor is always wide in most CNN models and
it barely affects the acceleration performance. According to
the experiments, the sparsity r reaches a high ratio in certain layers. These two facts indicate that we can obtain a
considerable speedup ratio. Detailed statistical results are
described in the experiments section.
In residual-based networks, if the output of one layer in
the residual block is all zero, we can skip the calculation
of descendant convolutional layers and directly predict the
results of this block. This property helps further accelerate
the residual networks.
4. Experiments
In this section, we conduct experiments on three benchmark datasets to validate the effectiveness of our acceleration method.
4.1. Benchmark Datasets and Experimental Setting
We mainly evaluate our LCCN on three benchmarks:
CIFAR-10, CIFAR-100 and ILSVRC-12 .
CIFAR-10 dataset contains 60,000 32 × 32 images, which
are categorized into 10 classes and each class contains 6,000
images. The dataset is split into 50,000 training images and
10,000 testing images. The CIFAR-100 dataset is similar to CIFAR-10, except that it has 100 classes and 600
images per class. Each class contains 500 training images
and 100 testing images. For CIFAR-10 and CIFAR-100,
we split the 50k training dataset into 45k/5k for validation.
ImageNet 2012 dataset is a famous benchmark which
contains 1.28 million training images of 1,000 classes. We
evaluate on the 50k validation images using both the top-1
and top-5 error rates.
Deep residual networks have shown impressive performance with good convergence behaviors.
Their signiﬁcance has increased, as shown by the amount of research being undertaken.
We mainly apply our
LCCN to increase the speed of these improved deep residual
networks. In the CIFAR experiments, we use the default parameter setting as . However, it is obvious that our
LCCN is more complicated than the original CNN model,
which leads to a requirement for more training epochs to
converge into a stable situation. So we increase the training
epochs and perform a different learning rate strategies to
train our LCCN. We start the learning rate at 0.01 to warm
up the network and then increase it to 0.1 after 3% of the total iterations. Then it is divided by 10 at 45%, 70% and 90%
iterations where the errors plateau. We tune the training
epoch numbers from {200, 400, 600, 800, 1000} according
to the validation data
On ILSVRC-12, we follow the same parameter settings
as but use different data argumentation strategies.
(1) Scale augmentation: we use the scale and aspect ratio
augmentation instead of the scale augmentation 
used in . (2) Color augmentation: we use the photometric distortions from to improve the standard color
augmentation used in . (3) Weight decay: we
apply weight decay to all weights and biases. These three
differences should slightly improve performance (refer to
Facebook implementation3). According to our experiences
with CIFAR, we extend the training epoch to 200, and use a
learning rate starting at 0.1 and then is divided by 10 every
66 epochs.
For the CIFAR experiments, we report the acceleration
performance and the top-1 error to compare with the results provided in the original paper . On ILSVRC-
12, since we use different data argumentation strategies, we
report the top-1 error of the original CNN models trained
in the same way as ours, and we mainly compare the accuracy drop with other state-of-the-art acceleration algorithms including: (1) Binary-Weight-Networks (BWN) 
that binarizes the convolutional weights;
Networks (XNOR) that binarizes both the convolutional weights and the data tensor; (3) Pruning Filters for Ef-
3 
Figure 3. Sparsity for the response maps from each collaborative convolutional layer in ResNet-20. We use LCCL to modify 18 convolutional layers to speed up ResNet-20. “x.y” represents the y-th residual block in the x-th generalized convolutional block. “conv1” and
“conv2” represent the ﬁrst and the second collaboration convolutional in the corresponding residual block.
ﬁcient ConvNets (PFEC) which prunes the ﬁlters with
small effect on the output accuracy from CNNs.
4.2. Experiments on CIFAR-10 and CIFAR-100
First, we study the inﬂuence on performance of using
different connection strategies proposed in the Kernel Selection and Sparsity Improvement sections.
We use the
pre-activation ResNet-20 as our base model, and apply the
LCCL to all convolutional layers within the residual blocks.
Using the same training strategy, the results of four different
connection strategies are shown in Table 3.
Both collaborative layers with the after-activation
method show the best performance with a considerable
speedup ratio. Because the Aft strategy receives the same
distribution of input to that of the corresponding convolution layer. We also try to use the L1L2 loss to restrict the
output maps of each LCCL. But this will add thousands of
extra values that need to be optimized in the L1L2 loss function. In this case, the networks are difﬁcult to converge and
the performance is too bad to be compared.
Top-1 Err.
Table 3. Before-activation and after-activation for connection strategy on ResNet-20. Each LCCL uses 3 × 3 × k kernel.
Furthermore, we analyze the performance inﬂuenced by
using different kernels in the LCCL. There are two forms
of LCCL that collaborate with the corresponding convolutional layer. One is a tensor of size 1 × 1 × C × T (denoted
as 1 × 1), and the other is a tensor of size k × k × C × 1
1 × 1 × C × T
k × k × C × 1
Table 4. Comparison of top-1 error rate on two different collaborative layers. (The ‘Ratio’ represents the speedup ratio)
(denoted as k × k). As shown in Table 4, the k × k kernel
shows signiﬁcant performance improvement with a similar
speedup ratio compared with a 1×1 kernel. It can be caused
by that the k×k kernel has a larger reception ﬁeld than 1×1.
Statistics on the sparsity of each response map generated from the LCCL are illustrated in Fig. 3. This LCCN
is based on ResNet-20 with each residual block equipped
with a LCCL conﬁgured by a 1 × 1 × C × T kernel. To get
stable and robust results, we increase the training epochs
as many as possible, and the sparsity variations for all 400
epochs are provided. The ﬁrst few collaborative layers show
a great speedup ratio, saving more than 50% of the computation cost. Even if the last few collaboration layers behave
less than the ﬁrst few, the k × k × C × 1 based method is
capable of achieving more than 30% increase in speed.
Hitherto, we have demonstrated the feasibility of training CNN models equipped with our LCCL using different
low-cost collaborative kernels and strategies. Considering
the performance and realistic implementation, we select the
weight sharing kernel for our LCCL. This will be used in
all following experiments as default.
Furthermore, we experiment with more CNN models accelerated by our LCCN on CIFAR-10 and CIFAR-
100. Except for ResNet-164 which uses a bottleneck
ResNet 
Table 5. Top-1 Error and Speed-Up of eight different CNN models
on CIFAR-10 (symbol “*” means the bottleneck structure). Ori.
Err represents the top-1 error of the original convolution network.
residual block
, all other models use a basic residual block
. We use LCCL to accelerate all convolutional layers except for the ﬁrst layer, which takes the original image as the input tensor. The ﬁrst convolutional layer
operates on the original image, and it costs a little time due
to the small input channels (RGB 3 channels). In a bottleneck structure, it is hard to reach a good convergence with
all the convolutional layers accelerated. The convolutional
layer with 1×1 kernel is mainly used to reduce dimension to
remove computational bottlenecks, which overlaps with the
acceleration effect of our LCCL. This property makes layers with 1 × 1 kernel more sensitive to collaboration with
our LCCL. Thus, we apply our LCCL to modify the ﬁrst
and second convolutional layer in the bottleneck residual
block on CIFAR-10. And for CIFAR-100, we only modify the second convolutional layer with 3 × 3 kernel in the
bottleneck residual block. The details of theoretical numerical calculation acceleration and accuracy performance are
presented in Table 5 and Table 6.
ResNet 
Table 6. Top-1 error and speed-up of seven different CNN models
on CIFAR-100 (symbol “*” means the bottleneck structure). Ori.
Err represents the top-1 error of the original convolution network.
Experiments show our LCCL works well on much
deeper convolutional networks, such as pre-activation
ResNet-164 or WRN-40-4 . Convolutional operators dominate the computation cost of the whole network,
which hold more than 90% of the FLOPs in residual based
networks. Therefore, it is beneﬁcial for our LCCN to accelerate such convolutionally-dominated networks, rather than
the networks with high-cost fully connected layers. In practice, we are always able to achieve more than a 30% calculation reduction for deep residual based networks. With a
similar calculation quantity, our LCCL is capable of outperforming original deep residual networks.
For example, on the CIFAR-100 dataset, LCCN on WRN-52-1 obtains higher accuracy than the original WRN-40-1 with only
about 2% more cost in FLOPs. Note that our acceleration is
data-driven, and can achieve a much higher speedup ratio on
“easy” data. In cases where high accuracy is not achievable,
it predicts many zeros which harms the network structure.
Theoretically, the LCCN will achieve the same accuracy
as the original one if we set LCCL as an identity (dense)
network. To improve efﬁciency, the outputs of LCCL need
to be sparse, which may marginally sacriﬁce accuracy for
some cases. We also observe accuracy gain for some other
cases (WRN-52-1 in Table 6), because the sparse structure
can reduce the risk of overﬁtting.
4.3. Experiments on ILSVRC-12
We test our LCCN on ResNet-18, 34 with some structural adjustments. On ResNet-18, we accelerate all convolutional layers in the residual block. However, ResNet-34
is hard to optimize with all the convolutional layers accelerated. So, we skip the ﬁrst residual block at each stage (layer
2, 3, 8, 9, 16, 17, 28, 29) to make it more sensitive to collaboration. The performance of the original model and our
LCCN with the same setting are shown in Table 7.
Top-1 Error
Top-5 Error
Table 7. Top-1 and Top-5 Error of LCCN on ImageNet classiﬁcation task.
We demonstrate the success of LCCN on ResNet-
18, 34 , and all of them obtain a meaningful speedup
with a slight performance drop.
Top-1 Acc. Drop
Top-5 Acc. Drop
Table 8. Comparison with other acceleration methods on ResNet.
Acc. Drop represents the accuracy drop.
We compare our method with other state-of-the-art
methods, shown in Table 8. As we can see, similar to other
acceleration methods, there is some performance drop.
However, our method achieves better accuracy than other
acceleration methods.
4.4. Theoretical vs. Realistic Speedup
There is often a wide gap between theoretical and realistic speedup ratio. It is caused by the limitation of efﬁciency
of BLAS libraries, IO delay, buffer switch or some others.
So we compare the theoretical and realistic speedup with
our LCCN. We test the realistic speed based on Caffe ,
an open source deep learning framework. OpenBLAS is
used as the BLAS library in Caffe for our experiments. We
set CPU only mode and use a single thread to make a fair
comparison. The results are shown in Table 9.
Table 9. Comparison on the theoretical and realistic speedup.
Discussion. As shown in Table 9, our realistic speedup
ratio is less than the theoretical one, which is caused mainly
by two reasons.
First, we use data reconstruction and
matrix-matrix multiplication to achieve the convolution operator as Caffe . The data reconstruction operation costs
too much time, making the cost of our LCCL much higher
than its theoretical speed. Second, the frontal convolution
layers usually take more time but contain less sparsity than
the rear ones, which reduces the overall acceleration effect
of the whole convolution neural network. These two defects
can be solved in theory, and we will focus on the realistic
speedup in future.
Platform. The idea of reducing matrix size in convolutional networks can be applied to GPUs as well in principle,
even though some modiﬁcations on our LCCN should be
made to better leverage the existing GPU libraries. Further,
our method is independent from platform, and should work
on the FPGA platform with customization.
4.5. Visualization of LCCL
Here is an interesting observation about our LCCL. We
visualize the results of LCCN on PASCAL VOC2007 
training dataset. We choose ResNet-50 as the competitor,
and add an additional 20 channels’ convolutional layer with
an average pooling layer as the classiﬁer. For our LCCN,
we equip the last 6 layers of this competitor model with our
LCCL. After ﬁne tuning, the feature maps generated from
the last LCCL and the corresponding convolutional layer
of the competitor model are visualized in Fig. 4. As we
can observe, our LCCL might have the ability to highlight
the ﬁelds of foreground objects, and eliminates the impact
of the background via the collaboration property. For example, in the second triplet, car and person are activated
simultaneously in the same response map by the LCCL.
At the ﬁrst glance, these highlighted areas look similar
with the locations obtained by attention model. But they
are intrinsically different in many ways, e.g., motivations,
computation operations, response meaning and structures.
The feature maps (after ReLU) generated from the
last LCCL of our LCCN and the corresponding convolutional
layer of ResNet-50 are visualized for testing samples of PASCAL
VOC2007 dataset. Each triplet represents one picture and its corresponding feature maps. The activated area of LCCL seems highlight more foreground objects than that of ResNet-50. In the meantime, LCCL is possible to depress the background area.
5. Conclusion
In this paper, we propose a more complicated network
structure yet with less inference complexity to accelerate
the deep convolutional neural networks. We equip a lowcost collaborative layer to the original convolution layer.
This collaboration structure speeds up the test-phase computation by skipping the calculation of zero cells predicted
by the LCCL. In order to solve the the difﬁculty of achieving acceleration on basic LCCN structures, we introduce
ReLU and BN to enhance sparsity and maintain performance. The acceleration of our LCCN is data-dependent,
which is more reasonable than hard acceleration structures.
In the experiments, we accelerate various models on CI-
FAR and ILSVRC-12, and our approach achieves signiﬁcant speed-up, with only slight loss in the classiﬁcation accuracy. Furthermore, our LCCN can be applied on most
tasks based on convolutional networks (e.g., detection, segmentation and identiﬁcation).
Meanwhile, our LCCN is
capable of plugging in some other acceleration algorithms
(e.g., ﬁx-point or pruning-based methods), which will further enhance the acceleration performance.