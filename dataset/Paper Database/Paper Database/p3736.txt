TEXTBUGGER: Generating Adversarial Text Against
Real-world Applications
Jinfeng Li∗, Shouling Ji∗†
, Tianyu Du∗, Bo Li‡ and Ting Wang§
∗Institute of Cyberspace Research and College of Computer Science and Technology, Zhejiang University
Email: {lijinfeng0713, sji, zjradty}@zju.edu.cn
† Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies
‡ University of Illinois Urbana-Champaign, Email: 
§ Lehigh University, Email: 
Abstract—Deep Learning-based Text Understanding (DLTU)
is the backbone technique behind various applications, including
question answering, machine translation, and text classiﬁcation.
Despite its tremendous popularity, the security vulnerabilities of
DLTU are still largely unknown, which is highly concerning given
its increasing use in security-sensitive applications such as sentiment analysis and toxic content detection. In this paper, we show
that DLTU is inherently vulnerable to adversarial text attacks,
in which maliciously crafted texts trigger target DLTU systems
and services to misbehave. Speciﬁcally, we present TEXTBUGGER,
a general attack framework for generating adversarial texts. In
contrast to prior works, TEXTBUGGER differs in signiﬁcant ways:
(i) effective – it outperforms state-of-the-art attacks in terms of
attack success rate; (ii) evasive – it preserves the utility of benign
text, with 94.9% of the adversarial text correctly recognized
by human readers; and (iii) efﬁcient – it generates adversarial
text with computational complexity sub-linear to the text length.
We empirically evaluate TEXTBUGGER on a set of real-world
DLTU systems and services used for sentiment analysis and toxic
content detection, demonstrating its effectiveness, evasiveness, and
efﬁciency. For instance, TEXTBUGGER achieves 100% success rate
on the IMDB dataset based on Amazon AWS Comprehend within
4.61 seconds and preserves 97% semantic similarity. We further
discuss possible defense mechanisms to mitigate such attack
and the adversary’s potential countermeasures, which leads to
promising directions for further research.
INTRODUCTION
Deep neural networks (DNNs) have been shown to achieve
great success in various tasks such as classiﬁcation, regression, and decision making. Such advances in DNNs have led
to broad deployment of systems on important problems in
physical world. However, though DNNs models have exhibited
state-of-the-art performance in a lot of applications, recently
they have been found to be vulnerable against adversarial
examples which are carefully generated by adding small
perturbations to the legitimate inputs to fool the targeted
models . Such discovery has also raised
serious concerns, especially when deploying such machine
learning models to security-sensitive tasks.
Shouling Ji is the corresponding author.
In the meantime, DNNs-based text classiﬁcation plays a
more and more important role in information understanding and analysis nowadays. For instance, many online recommendation systems rely on the sentiment analysis of
user reviews/comments . Generally, such systems would
classify the reviews/comments into two or three categories
and then take the results into consideration when ranking
movies/products. Text classiﬁcation is also important for enhancing the safety of online discussion environments, e.g.,
automatically detect online toxic content , including irony,
sarcasm, insults, harassment and abusive content.
Many studies have investigated the security of current machine learning models and proposed different attack methods,
including causative attacks and exploratory attacks .
Causative attacks aim to manipulate the training data thus
misleading the classiﬁer itself, and exploratory attacks craft
malicious testing instances (adversarial examples) so as to
evade a given classiﬁer. To defend against these attacks, several
mechanisms have been proposed to obtain robust classiﬁers
 . Recently, adversarial attacks have been shown to be
able to achieve a high attack success rate in image classiﬁcation tasks , which has posed severe physical threats to many
intelligent devices (e.g., self-driving cars) .
While existing works on adversarial examples mainly focus
on the image domain, it is more challenging to deal with text
data due to its discrete property, which is hard to optimize.
Furthermore, in the image domain, the perturbation can often
be made virtually imperceptible to human perception, causing
humans and state-of-the-art models to disagree. However, in
the text domain, small perturbations are usually clearly perceptible, and the replacement of a single word may drastically
alter the semantics of the sentence. In general, existing attack
algorithms designed for images cannot be directly applied
to text, and we need to study new attack techniques and
corresponding defenses.
Recently, some mechanisms are proposed towards generating adversarial texts . These work proposed to generate
adversarial texts by replacing a word with an out-of-vocabulary
one . Although seminal, they are limited in practice
due to the following reasons: (i) they are not computationally
efﬁcient, (ii) they are designed under the white-box setting, (iii)
they require manual intervention, and/or (iv) they are designed
against a particular NLP model and are not comprehensively
evaluated. Thus, the efﬁciency and effectiveness of current
adversarial text generation techniques and the robustness of
Network and Distributed Systems Security (NDSS) Symposium 2019
24-27 February 2019, San Diego, CA, USA
ISBN 1-891562-55-X
 
www.ndss-symposium.org
 
Task: Sentiment Analysis. Classiﬁer: CNN. Original label: 99.8% Negative. Adversarial label: 81!"% Positive.
Text: I love these awful awf ul 80's summer camp movies. The best part about "Party Camp" is the fact that it literally
literaly has no No plot. The cliches clichs here are limitless: the nerds vs. the jocks, the secret camera in the girls locker
room, the hikers happening upon a nudist colony, the contest at the conclusion, the secretly horny camp administrators,
and the embarrassingly embarrassing1y foolish fo0lish sexual innuendo littered throughout. This movie will make you
laugh, but never intentionally. I repeat, never.
Task: Sentiment Analysis. Classiﬁer: Amazon AWS. Original label: 100% Negative. Adversarial label: 89% Positive.
Text: I watched this movie recently mainly because I am a Huge fan of Jodie Foster's. I saw this movie was made right
between her 2 Oscar award winning performances, so my expectations were fairly high. Unfortunately Unf0rtunately, I
thought the movie was terrible terrib1e and I'm still left wondering how she was ever persuaded to make this movie. The
script is really weak wea k.
Task: Toxic Content Detection. Classiﬁer: Perspective. Original label: 92% Toxic. Adversarial label: 78% Non-toxic.
Text: reason why requesting i want to report something so can ips report stuﬀ, or can only registered users can? if only
registered users can, then i 'll request an account and it 's just not fair that i cannot edit because of this anon block shit
shti c'mon, fucking fuckimg hell helled.
Task: Toxic Content Detection. Classiﬁer: LSTM. Original label: 96.7% Toxic. Adversarial label: 83.5% Non-toxic.
Text: hello how are you? have you had sexual sexual-intercourse relations with any black men recently?
Adversarial examples against two natural language classiﬁcation tasks. Replacing a fraction of the words in a document with adversarially-chosen bugs
fools classiﬁers into predicting an incorrect label. The new document is classiﬁed correctly by humans and preserves most of the original meaning although it
contains small perturbations.
popular text classiﬁcation models need to be studied.
In this paper, we propose TEXTBUGGER, a framework that
can effectively and efﬁciently generate utility-preserving (i.e.,
keep its original meaning for human readers) adversarial texts
against state-of-the-art text classiﬁcation systems under both
white-box and black-box settings. In the white-box scenario,
we ﬁrst ﬁnd important words by computing the Jacobian matrix
of the classiﬁer and then choose an optimal perturbation from
the generated ﬁve kinds of perturbations. In the black-box
scenario, we ﬁrst ﬁnd the important sentences, and then use
a scoring function to ﬁnd important words to manipulate.
Through extensive experiments under both settings, we show
that an adversary can deceive multiple real-world online DLTU
systems with the generated adversarial texts1, including Google
Cloud NLP, Microsoft Azure Text Analytics, IBM Watson Natural Language Understanding and Amazon AWS Comprehend,
etc. Several adversarial examples are shown in Fig. 1. The
existence of such adversarial examples causes a lot of concerns
for text classiﬁcation systems and seriously undermines their
usability.
Our Contribution. Our main contributions can be summarized as follows.
We propose TEXTBUGGER, a framework that can
effectively and efﬁciently generate utility-preserving
adversarial texts under both white-box and black-box
We evaluate TEXTBUGGER on a group of state-ofthe-art machine learning models and popular realworld online DLTU applications, including sentiment
analysis and toxic content detection. Experimental
results show that TEXTBUGGER is very effective and
1We have reported our ﬁndings to their companies, and they replied that
they would ﬁx these bugs in the next version.
efﬁcient. For instance, TEXTBUGGER achieves 100%
attack success rate on the IMDB dataset when targeting the Amazon AWS and Microsoft Azure platforms
under black-box settings. We shows that transferability
also exists in the text domain and the adversarial texts
generated against ofﬂine models can be successfully
transferred to multiple popular online DLTU systems.
We conduct a user study on our generated adversarial
texts and show that TEXTBUGGER has little impact
on human understanding.
We further discuss two potential defense strategies to
defend against the above attacks along with preliminary evaluations. Our results can encourage building
more robust DLTU systems in the future.
ATTACK DESIGN
A. Problem Formulation
Given a pre-trained text classiﬁcation model F : X →Y,
which maps from feature space X to a set of classes Y, an
adversary aims to generate an adversarial document xadv from
a legitimate document x ∈X whose ground truth label is y ∈
Y, so that F(xadv) = t (t ̸= y). The adversary also requires
S(x, xadv) ≥ϵ for a domain-speciﬁc similarity function S :
X × X →R+, where the bound ϵ ∈R captures the notion
of utility-preserving alteration. For instance, in the context of
text classiﬁcation tasks, we may use S to capture the semantic
similarity between x and xadv.
B. Threat Model
We consider both white-box and black-box settings to
evaluate different adversarial abilities.
White-box Setting. We assume that attackers have complete knowledge about the targeted model including the model
ParallelDots API: An example of deep learning text classiﬁcation
platform, which is a black-box scenario.
architecture parameters. White-box attacks ﬁnd or approximate
the worst-case attack for a particular model and input based
on the kerckhoff’s principle . Therefore, white-box attacks
can expose a model’s worst case vulnerabilities.
Black-box Setting. With the development of machine
learning, many companies have launched their own Machine-
Learning-as-a-Service (MLaaS) for DLTU tasks such as text
classiﬁcation. Generally, MLaaS platforms have similar system
design: the model is deployed on the cloud servers, and users
can only access the model via an API. In such cases, we
assume that the attacker is not aware of the model architecture,
parameters or training data, and is only capable of querying
the target model with output as the prediction or conﬁdence
scores. Note that the free usage of the API is limited among
these platforms. Therefore, if the attackers want to conduct
practical attacks against these platforms, they must take such
limitation and cost into consideration. Speciﬁcally, we take the
ParallelDots2 as an example and show its sentiment analysis
API and the abusive content classiﬁer API in Fig. 2. From
Fig. 2, we can see that the sentiment analysis API would return
the conﬁdence value of three classes, i.e., “positive”, “neutral”
and “negative”. Similarly, the abusive content classiﬁer would
return the conﬁdence value of two classes, i.e., “abusive” and
“non abusive”. For both APIs, the sum of conﬁdence values of
an instance equal to 1, and the class with the highest conﬁdence
value is considered as the input’s class.
C. TEXTBUGGER
We propose efﬁcient strategies to change a word slightly,
which is sufﬁcient for creating adversarial texts in both whitebox settings and black-box settings. Speciﬁcally, we call the
slightly changed words “bugs”.
1) White-box Attack: We ﬁrst ﬁnd important words by
computing the Jacobian matrix of the classiﬁer F, and generate
ﬁve kinds of bugs. Then we choose an optimal bug in terms of
the change of the conﬁdence value. The algorithm of whitebox attack is shown in Algorithm 1.
Step 1: Find Important Words (line 2-5). The ﬁrst step
is to compute the Jacobian matrix for the given input text
x = (x1, x2, · · · , xN) (line 2-4), where xi is the ith word,
and N represents the total number of words within the input
text. For a text classiﬁcation task, the output of F is more than
one dimension. Therefore the matrix is as follows:
JF(x) = ∂F(x)
i∈1..N,j∈1..K
2 
Algorithm 1 TEXTBUGGER under white-box settings
Input: legitimate document x and its ground truth label y,
classiﬁer F(·), threshould ϵ
Output: adversarial document xadv
1: Inititialize: x′ ←x
2: for word xi in x do
Compute Cxi according to Eq.2;
4: end for
5: Wordered ←Sort(x1, x2, · · · , xm) according to Cxi;
6: for xi in Wordered do
bug = SelectBug(xi, x′, y, F(·));
x′ ←replace xi with bug in x′
if S(x, x′) ≤ϵ then
Return None.
else if Fl(x′) ̸= y then
Solution found. Return x′.
14: end for
15: return None
where K represents the total number of classes in Y, and
Fj(·) represents the conﬁdence value of the jth class. The
importance of word xi is deﬁned as:
Cxi = JF(i,y) = ∂Fy(x)
i.e., the partial derivative of the conﬁdence value based on the
predicted class y regarding to the input word xi. This allows us
to ﬁnd the important words that have signiﬁcant impact on the
classiﬁer’s outputs. Once we have calculated the importance
score of each word within the input sequences, we sort these
words in inverse order according to the importance value (line
Step 2: Bugs Generation (line 6-14). To generate bugs,
many operations can be used. However, we prefer small
changes to the original words as we require the generated
adversarial sentence is visually and semantically similar to the
original one for human understanding. Therefore, we consider
two kinds of perturbations, i.e., character-level perturbation
and word-level perturbation.
For character-level perturbation, one key observation is
that words are symbolic, and learning-based DLTU systems
usually use a dictionary to represent a ﬁnite set of possible
words. The size of the typical word dictionary is much smaller
than the possible combinations of characters at a similar
length (e.g., about 26n for the English case, where n is the
length of the word). This means if we deliberately misspell
important words, we can easily convert those important words
to “unknown” (i.e., words not in the dictionary). The unknown
words will be mapped to the “unknown” embedding vector in
deep learning modeling. Our results strongly indicate that such
simple strategy can effectively force text classiﬁcation models
to behave incorrectly.
For word-level perturbation, we expect that the classiﬁer
can be fooled after replacing a few words, which are obtained
by nearest neighbor searching in the embedding space, without
changing the original meaning. However, we found that in
some word embedding models (e.g., word2vec), semantically
opposite words such as “worst” and “better” are highly syntactically similar in texts, thus “better” would be considered as
the nearest neighbor of “worst”. However, changing “worst”
to “better” would completely change the sentiment of the
input text. Therefore, we make use of a semantic-preserving
technique, i.e., replace the word with its topk nearest neighbors
in a context-aware word vector space. Speciﬁcally, we use
the pre-trained GloVe model provided by Stanford for
word embedding and set topk = 5 in the experiment. Thus,
the neighbors are guaranteed to be semantically similar to the
original one.
According to previous studies, the meaning of the text is
very likely to be preserved or inferred by the reader after
a few character changes . Meanwhile, replacing words
with semantically and syntactically similar words can ensure
that the examples are perceptibly similar . Based on these
observations, we propose ﬁve bug generation methods for
TEXTBUGGER: (1) Insert: Insert a space into the word3. Generally, words are segmented by spaces in English. Therefore,
we can deceive classiﬁers by inserting spaces into words. (2)
Delete: Delete a random character of the word except for
the ﬁrst and the last character. (3) Swap: Swap random two
adjacent letters in the word but do not alter the ﬁrst or last
letter4. This is a common occurrence when typing quickly
and is easy to implement. (4) Substitute-C (Sub-C): Replace
characters with visually similar characters (e.g., replacing “o”
with “0”, “l” with “1”, “a” with “@”) or adjacent characters in
the keyboard (e.g., replacing “m” with “n”). (5) Substitute-W
(Sub-W): Replace a word with its topk nearest neighbors in a
context-aware word vector space. Several substitute examples
are shown in Table I.
As shown in Algorithm 2, after generating ﬁve bugs,
we choose the optimal bug according to the change of the
conﬁdence value, i.e., choosing the bug that decreases the
conﬁdence value of the ground truth class the most. Then we
will replace the word with the optimal bug to obtain a new text
x′ (line 8). If the classiﬁer gives the new text a different label
(i.e., Fl(x′) ̸= y) while preserving the semantic similarity
(which is detailed in Section III-D) above the threshold (i.e.,
S(x, x′) ≥ϵ), the adversarial text is found (line 9-13). If not,
we repeat above steps to replace the next word in Wordered
until we ﬁnd the solution or fail to ﬁnd a semantic-preserving
adversarial example.
Algorithm 2 Bug Selection algorithm
1: function SELECTBUG(w, x, y, F(·))
bugs = BugGenerator(w);
for bk in bugs do
candidate(k) = replace w with bk in x;
score(k) = Fy(x) −Fy(candidate(k));
bugbest = arg maxbk score(k);
return bugbest;
9: end function
2) Black-box Attack: Under the black-box setting, gradients
of the model are not directly available, and we need to
change the input sequences directly without the guidance of
3Considering the usability of text, we apply this method only when the
length of the word is shorter than 6 characters since long words might be
split into two legitimate words.
4For this reason, this method is only applied to words longer than 4 letters.
EXAMPLES FOR FIVE BUG GENERATION METHODS.
Algorithm 3 TEXTBUGGER under black-box settings
Input: legitimate document x and its ground truth label y,
classiﬁer F(·), threshould ϵ
Output: adversarial document xadv
1: Inititialize: x′ ←x
2: for si in document x do
Csentence(i) = Fy(si);
4: end for
5: Sordered ←Sort(sentences) according to Csentence(i);
6: Delete sentences in Sordered if Fl(si) ̸= y;
7: for si in Sordered do
for wj in si do
Compute Cwj according to Eq.3;
Wordered ←Sort(words) according to Cwj;
for wj in Wordered do
bug = SelectBug(wj, x′, y, F(·));
x′ ←replace wj with bug in x′
if S(x, x′) ≤ϵ then
Return None.
else if Fl(x′) ̸= y then
Solution found. Return x′.
21: end for
22: return None
gradients. Therefore different from white-box attacks, where
we can directly select important words based on gradient
information, in black-box attacks, we will ﬁrst ﬁnd important
sentences and then the important words within them. Brieﬂy,
the process of generating word-based adversarial examples on
text under black-box setting contains three steps: (1) Find the
important sentences. (2) Use a scoring function to determine
the importance of each word regarding to the classiﬁcation
result, and rank the words based on their scores. (3) Use the
bug selection algorithm to change the selected words. The
black-box adversarial text generation algorithm is shown in
Algorithm 3.
Step 1: Find Important Sentences (line 2-6). Generally,
when people express their opinions, most of the sentences
are describing facts and the main opinions usually depend on
only a few of sentences which have a greater impact on the
classiﬁcation results. Therefore, to improve the efﬁciency of
TEXTBUGGER, we ﬁrst ﬁnd the important sentences that contribute to the ﬁnal prediction results most and then prioritize
to manipulate them.
Suppose the input document x = (s1, s2, · · · , sn), where
si represents the sentence at the ith position. First, we use
the spaCy library5 to segment each document into sentences.
Then we ﬁlter out the sentences that have different predicted
5 
Word Contribution
Sentiment Score
Illustration of how to select important words to apply perturbations
for the input sentence “It is so laddish and juvenile, only teenage boys could
possibly ﬁnd it funny”. The sentiment score of each word is the classiﬁcation
result’s conﬁdence value of the new text that deleting the word from the
original text. The contribution of each word is the difference between the new
conﬁdence score and the original conﬁdence score.
labels with the original document label (i.e., ﬁlter out Fl(si) ̸=
y). Then, we sort the important sentences in an inverse order
according to their importance score. The importance score of
a sentence si is represented with the conﬁdence value of the
predicted class Fy, i.e., Csi = Fy(si).
Step 2: Find Important Words (line 8-11). Considering
the vast search space of possible changes, we should ﬁrst
ﬁnd the most important words that contribute the most to the
original prediction results, and then modify them slightly by
controlling the semantic similarity.
One reasonable choice is to directly measure the effect of
removing the ith word, since comparing the prediction before
and after removing a word reﬂects how the word inﬂuences the
classiﬁcation result as shown in Fig. 3. Therefore, we introduce
a scoring fuction that determine the importance of the jth word
Cwj =Fy(w1, w2, · · ·, wm) −Fy(w1, · · ·, wj−1, wj+1, · · ·, wm) (3)
The proposed scoring function has the following properties:
(1) It is able to correctly reﬂect the importance of words for the
prediction, (2) it calculates word scores without the knowledge
of the parameters and structure of the classiﬁcation model, and
(3) it is efﬁcient to calculate.
Step 3: Bugs Generation (line 12-20). This step is similar
as that in white-box setting.
ATTACK EVALUATION: SENTIMENT ANALYSIS
Sentiment analysis refers to the use of NLP, statistics, or
machine learning methods to extract, identify or characterize
the sentiment content of a text unit. It is widely applied to
helping a business understand the social sentiment of their
products or services by monitoring online conversations.
In this section, we investigate the practical performance
of the proposed method for generating adversarial texts for
sentiment analysis. We start with introducing the datasets,
targeted models, baseline algorithms, evaluation metrics and
implementation details. Then we will analyze the results and
discuss potential reasons for the observed performance.
A. Datasets
We study adversarial examples of text on two popular
public benchmark datasets for sentiment analysis. The ﬁnal
adversarial examples are generated and evaluated on the test
IMDB . This dataset contains 50,000 positive and
negative movie reviews that crawled from online sources, with
215.63 words as average length for each sample. It has been
divided into two parts, i.e., 25,000 reviews for training and
25,000 reviews for testing. Speciﬁcally, we held out 20% of
the training set as a validation set and all parameters are tuned
based on it.
Rotten Tomatoes Movie Reviews (MR) . This dataset
is a collection of movie reviews collected by Pang and Lee in
 . It contains 5,331 positive and 5,331 negative processed
sentences/snippets and has an average length of 32 words. In
our experiment, we divide this dataset into three parts, i.e.,
80%, 10%, 10% as training, validation and testing, respectively.
B. Targeted Models
For white-box attacks, we evaluated TEXTBUGGER on LR,
Kim’s CNN and the LSTM used in . In our implementation, the model’s parameters are ﬁne-tuned according to
the sensitivity analysis on model performance conducted by
Zhang et al. . Meanwhile, all models were trained in a
hold-out test strategy, and hyper-parameters were tuned only
on the validation set.
For black-box attacks, we evaluated the TEXTBUGGER on
ten sentiment analysis platforms/models, i.e., Google Cloud
NLP, IBM Waston Natural Language Understanding (IBM
Watson), Microsoft Azure Text Analytics (Microsoft Azure),
Amazon AWS Comprehend (Amazon AWS), Facebook fast-
Text (fastText), ParallelDots, TheySay Sentiment, Aylien Sentiment, TextProcessing, and Mashape Sentiment. For fastText,
we used a pre-trained model6 provided by Facebook. This
model is trained on the Amazon Review Polarity dataset and
we do not have any information about the models’ parameters
or architecture.
C. Baseline Algorithms
We implemented and compared the other three methods
with our white-box attack method. In total, the three methods
are: (1) Random: Randomly selects words to modify. For each
sentence, we select 10% words to modify. (2) FGSM+Nearest
Neighbor Search (NNS): The FGSM method was ﬁrst proposed in to generate adversarial images, which adds to
the whole image the noise that is proportional to sign(∇(Lx)),
where L represent the loss function and x is the input data.
It was combined with NNS to generate adversarial texts as
in : ﬁrst, generating adversarial embeddings by applying
FGSM on the embedding vector of the texts, then reconstructing the adversarial texts via NNS. (3) DeepFool+NNS:
The DeepFool method is ﬁrst proposed in to generate
adversarial images, which iteratively ﬁnds the optimal direction
to search for the minimum distance to cross the decision
6 models/
amazon review polarity.bin
boundary. It was combined with NNS to generate adversarial
texts as in .
D. Evaluation Metrics
We use four metrics, i.e., edit distance, Jaccard similarity
coefﬁcient, Euclidean distance and semantic similarity, to
evaluate the utility of the generated adversarial texts. Specifically, the edit distance and Jaccard similarity coefﬁcient are
calculated on the raw texts, while the Euclidean distance and
semantic similarity are calculated on word vectors.
Edit Distance. Edit distance is a way of quantifying how
dissimilar two strings (e.g., sentences) are by counting the minimum number of operations required to transform one string to
the other. Speciﬁcally, different deﬁnitions of the edit distance
use different sets of string operations. In our experiment, we
use the most common metrics, i.e., the Levenshtein distance,
whose operations include removal, insertion, and substitution
of characters in the string.
Jaccard Similarity Coefﬁcient. The Jaccard similarity
coefﬁcient is a statistic used for measuring the similarity and
diversity of ﬁnite sample sets. It is deﬁned as the size of the
intersection divided by the size of the union of the sample sets:
J(A, B) = |A ∩B|
|A| + |B| −|A ∩B|
Larger Jaccard similarity coefﬁcient means higher sample
similarity. In our experiment, one sample set consists of all
the words in the sample.
Euclidean Distance. Euclidean distance is a measure of the
true straight line distance between two points in the Euclidean
space. If p = (p1, p2, · · · , pn) and q = (q1, q2, · · · , qn) are
two samples in the word vector space, then the Euclidean
distance between p and q is given by:
(p1 −q1)2 + (p2 −q2)2 + · · · + (pn −qn)2
In our experiment, the Euclidean space is exactly the word
vector space.
Semantic Similarity. The above three metrics can only
reﬂect the magnitude of the perturbation to some extent.
They cannot guarantee that the generated adversarial texts will
preserve semantic similarity from original texts. Therefore, we
need a ﬁne-grained metric that measures the degree to which
two pieces of text carry the similar meaning so as to control
the quality of the generated adversarial texts.
In our experiment, we ﬁrst use the Universal Sentence
Encoder , a model trained on a number of natural language
prediction tasks that require modeling the meaning of word
sequences, to encode sentences into high dimensional vectors.
Then, we use the cosine similarity to measure the semantic
similarity between original texts and adversarial texts. The
cosine similarity of two n-dimensional vectors p and q is
deﬁned as:
||p|| · ||q|| =
i=1 pi × qi
i=1(pi)2 ×
Generally, it works better than other distance measures because
the norm of the vector is related to the overall frequency of
which words occur in the training corpus. The direction of
a vector and the cosine distance is unaffected by this, so a
common word like “frog” will still be similar to a less frequent
word like “Anura” which is its scientiﬁc name.
Since our main goal is to successfully generate adversarial
texts, we only need to control the semantic similarity to be
above a speciﬁc threshold.
E. Implementation
We conducted the experiments on a server with two Intel
Xeon E5-2640 v4 CPUs running at 2.40GHz, 64 GB memory,
4TB HDD and a GeForce GTX 1080 Ti GPU card. We
repeated each experiment 5 times and report the mean value.
This replication is important because training is stochastic and
thus introduces variance in performance .
In our experiment, we did not ﬁlter out stop-words before
feature extraction as most NLP tasks do. This is because we
observe that the stop-words also have impact on the prediction
results. In particular, our experiments utilize the 300-dimension
GloVe embeddings7 trained on 840 billion tokens of Common
Crawl. Words not present in the set of pre-trained words are
initialized by randomly sampling from the uniform distribution
in [-0.1, 0.1]. Furthermore, the semantic similarity threshold ϵ
is set as 0.8 to guarantee a good trade-off between quality and
strength of the generated adversarial text.
F. Attack Performance
Effectiveness and Efﬁciency. The main results of whitebox attacks on the IMDB and MR datasets and comparison
of the performance of baseline methods are summarized in
Table II, where the third column of Table II shows the original
model accuracy in non-adversarial setting. We do not give
the average time of generating one adversarial example under
white-box settings since the models are ofﬂine and the attack is
very efﬁcient (e.g., generating hundreds of adversarial texts in
one second). From Table II, we can see that randomly choosing
words to change (i.e., Random in Table II) has hardly any
inﬂuence on the ﬁnal result. This implies randomly changing
words would not fool classiﬁers and choosing important words
to modify is necessary for successful attack. From Table II,
we can also see that the targeted models all perform quite
well in non-adversarial setting. However, the adversarial texts
generated by TEXTBUGGER still has high attack success rate
on these models. In addition, the linear model is more susceptible to adversarial texts than deep learning models. Speciﬁcally,
TEXTBUGGER only perturbs a few words to achieve a high
attack success rate and performs much better than baseline
algorithms against all models as shown in Table II. For
instance, it only perturbs 4.9% words of one sample when
achieving 95.2% success rate on the IMDB dataset against
the LR model, while all baselines achieve no more than
42% success rate in this case. As the IMDB dataset has an
average length of 215.63 words, TEXTBUGGER only perturbed
about 10 words for one sample to conduct successful attacks.
This means that TEXTBUGGER can successfully mislead the
classiﬁers into assigning signiﬁcantly higher positive scores to
the negative reviews via subtle manipulation.
7 
RESULTS OF THE WHITE-BOX ATTACKS ON IMDB AND MR DATASETS.
FGSM+NNS 
DeepFool+NNS 
TEXTBUGGER
TABLE III.
RESULTS OF THE BLACK-BOX ATTACK ON IMDB.
Targeted Model
Original Accuracy
DeepWordBug 
TEXTBUGGER
Success Rate
Perturbed Word
Success Rate
Perturbed Word
Google Cloud NLP
IBM Waston
Microsoft Azure
Amazon AWS
Facebook fastText
ParallelDots
Aylien Sentiment
TextProcessing
Mashape Sentiment
RESULTS OF THE BLACK-BOX ATTACK ON MR.
Targeted Model
Original Accuracy
DeepWordBug 
TEXTBUGGER
Success Rate
Perturbed Word
Success Rate
Perturbed Word
Google Cloud NLP
IBM Waston
Microsoft Azure
Amazon AWS
Facebook fastText
ParallelDots
Aylien Sentiment
TextProcessing
Mashape Sentiment
The main results of black-box attacks on the IMDB and
MR datasets and comparison of the performance of different
methods are summarized in Tables III and IV respectively,
and the second column of which shows the original model
accuracy in non-adversarial setting. From Tables III and IV,
we can see that TEXTBUGGER achieves high attack success
rate and performs much better than DeepWordBug against
all real-world online DLTU platforms. For instance, it achieves
100% success rate on the IMDB dataset when targeting Azure
and AWS platforms, while DeepWordBug only achieves 56.3%
and 68.1% success rate respectively. Besides, TEXTBUGGER
only perturbs a few words to achieve a high success rate as
shown in Tables III and IV. For instance, it only perturbs 7%
words of one sample when achieving 96.8% success rate on the
MR dataset targeting the Microsoft Azure platform. As the MR
dataset has an average length of 32 words, TEXTBUGGER only
perturbed about 2 words for one sample to conduct successful
attacks. Again, that means an adversary can subtly modify
highly negative reviews in a way that the classiﬁer assigns
signiﬁcantly higher positive scores to them.
The Impact of Document Length. We also study the
impact of document length on the effectiveness and efﬁciency
of the attacks and the corresponding results are shown in
Fig. 4. From Fig. 4(a), we can see that the document length
has little impact on the attack success rate. This implies
attackers can achieve high success rate no matter how long
the sample is. However, the conﬁdence value of prediction
results decrease for IBM Watson and Google Cloud NLP as
shown in Fig. 4(b). This means the attack on long documents
would be a bit weaker than that on short documents. From
Fig. 4(c), we can see that the time required for generating
one adversarial text and the average length of documents are
positively correlated overall for Microsoft Azure and Google
Cloud NLP. There is a very intuitive reason: the longer the
length of the document is, the more information it contains
that may need to be modiﬁed. Therefore, as the length of
the document grows, the time required for generating one
adversarial text increases slightly, since it takes more time to
ﬁnd important sentences. For IBM Watson, the run time ﬁrst
increases before 60 words, then vibrates after that. We carefully
analyzed the generated adversarial texts and found that when
the document length is less than 60 words, the total length of
the perturbed sentences increases sharply with the growth of
Success Rate
Google Cloud NLP
Microsoft Azure
IBM Watson
(a) Success Rate
100 125 150 175 200
Change in Score
Google Cloud NLP
Microsoft Azure
IBM Watson
Google Cloud NLP
Microsoft Azure
IBM Watson
The impact of document length (i.e. number of words in a document) on attack’s performance against three online platforms: Google Cloud NLP, IBM
Watson and Microsoft Azure. The sub-ﬁgures are: (a) the success rate and document length, (b) the change of negative class’s conﬁdence value. For instance,
the original text is classiﬁed as negative with 90% conﬁdence, while the adversarial text is classiﬁed as positive with 80% conﬁdence (20% negative), the score
changes 0.9-0.2=0.7. (c) the document length and the average time of generating an adversarial text.
Sentiment Score
Original Text
Perturbed Text
Sentiment Score
Original Text
Perturbed Text
Sentiment Score
Original Text
Perturbed Text
Sentiment Score
Original Text
Perturbed Text
The change of sentiment score evaluated on IMDB and MR datasets for 5 black-box platforms/models. For Google Cloud NLP (Google), IBM Watson
(Watson), the range of “negative” score is [-1, 0] and the range of “positive” score is . For Microsoft Azure (Azure), the range of “negative” score is [0,
0.5] and the range of “positive” score is [0.5, 1]. For Amazon AWS (AWS) and fastText, the range of “negative” score is [0.5, 1] and the range of “positive”
score is [0, 0.5].
document length. However, when the document length exceeds
60 words, the total length of the perturbed sentences changes
negligibly. In general, generating one adversarial text only
needs no more than 100 seconds for all the three platforms
while the maximum length of a document is limited to 200
words. This means TEXTBUGGER method is very efﬁcient in
Adversarial Text Examples. Two successful examples
for sentiment analysis are shown in Fig. 1. The ﬁrst adversarial text for sentiment analysis in Fig. 1 contains six
modiﬁcations, i.e., one insert operation (“awful” to “aw ful”),
one Sub-W operation (“no” to “No”), two delete operations
(“literally” to “literaly”, “cliches” to “clichs”), and two Sub-C
operations (“embarrassingly” to “embarrassing1y”, “foolish”
to “fo0lish”). These modiﬁcations successfully convert the
prediction result of the CNN model, i.e., from 99.8% negative
to 81.0% positive. Note that the modiﬁcation from “no” to
“No” only capitalizes the ﬁrst letter but really affects the
prediction result. After further analysis, we ﬁnd capitalization
operation is common for both ofﬂine models and online
platforms. We guess the embedding model may be trained
without changing uppercase letters to lowercase, thus causing
the same word in different forms get two different word
vectors. Furthermore, capitalization sometimes may cause outof-vocabulary phenomenon. The second adversarial text for
sentiment analysis in Fig. 1 contains three modiﬁcations,
i.e., one insert operation (“weak” to “wea k”) and two Sub-
C operations (“Unfortunately” to “Unf0rtunately”, “terrible”
to “terrib1e”). These modiﬁcations successfully convert the
prediction result of the Amazon AWS sentiment analysis API.
Score Distribution. Even though TEXTBUGGER fails to
convert the negative reviews to positive reviews in some cases,
it can still reduce the conﬁdence value of the classiﬁcation
results. Therefore, we computed the change of the conﬁdence
value over all the samples including the failed samples before
and after modiﬁcation and show the results in Fig. 5. From
Fig. 5, we can see that the overall score of the texts has been
moved to the positive direction.
G. Utility Analysis
For white-box attacks, the similarity between original texts
and adversarial texts against LR, CNN and LSTM models are
shown in Figs. 6 and 7. We do not compare TEXTBUGGER
with baselines in terms of utility since baselines only achieve
low success rate as shown in Table V. From Figs. 6(a), 6(b),
7(a) and 7(b), we can see that adversarial texts preserve
good utility in terms of word-level. Speciﬁcally, Fig. 6(a)
shows that almost 80% adversarial texts have no more than
25 edit distance comparing with original texts for LR and
CNN models. Meanwhile, Figs. 6(c), 6(d), 7(c) and 7(d) show
that adversarial texts preserve good utility in terms of vectorlevel. Speciﬁcally, from Fig. 6(d), we can see that almost 90%
adversarial texts preserve at least 0.9 semantic similarity of the
original texts. This indicates that TEXTBUGGER can generate
utility-preserving adversarial texts which fool the classiﬁers
with high success rate.
For black-box attacks, the average similarity between original texts and adversarial texts against 10 platforms/models
are shown in Figs. 8 and 9. From Figs. 8(a), 8(b), 9(a)
and 9(b), we can see that the adversarial texts generated by
Edit Distance
(a) Edit Distance
Jaccard Coefficient
(b) Jaccard Coefﬁcient
Euclidean Distance
(c) Euclidean Distance
Semantic Similarity
(d) Semantic Similarity
The utility of adversarial texts generated on IMDB dataset under
white-box settings for LR, CNN and LSTM models.
Edit Distance
(a) Edit Distance
Jaccard Coefficient
(b) Jaccard Coefﬁcient
10.0 12.5 15.0 17.5
Euclidean Distance
(c) Euclidean Distance
Semantic Similarity
(d) Semantic Similarity
The utility of adversarial texts generated on MR dataset under whitebox settings for LR, CNN and LSTM models.
TEXTBUGGER are more similar to original texts than that
generated by DeepWordBug in word-level. From Figs. 8(c),
8(d), 9(c) and 9(d) we can see that the adversarial texts
generated by TEXTBUGGER are more similar to original texts
than that generated by DeepWordBug in the word vector space.
These results implies that the adversarial texts generated by
TEXTBUGGER preserve more utility than that generated by
DeepWordBug. One reason is that DeepWordBug randomly
chooses a bug from generated bugs, while TEXTBUGGER
chooses the optimal bug that can change the prediction score
most. Therefore, DeepWordBug needs to manipulate more
words than TEXTBUGGER to achieve successful attack.
The Impact of Document Length. We also study the
impact of word length on the utility of generated adversarial
texts and show the results in Fig. 10. From Fig. 10(a), for IBM
Watson and Microsoft Azure, we can see that the number of
Edit Distance
TextBugger
DeepWordBug
(a) Edit Distance
Jaccard Coefficient
TextBugger
DeepWordBug
(b) Jaccard Coefﬁcient
Euclidean Distance
TextBugger
DeepWordBug
(c) Euclidean Distance
Semantic Similarity
TextBugger
DeepWordBug
(d) Semantic Similarity
The average utility of adversarial texts generated on IMDB dataset
under black-box settings for 10 platforms.
Edit Distance
TextBugger
DeepWordBug
(a) Edit Distance
Jaccard Coefficient
TextBugger
DeepWordBug
(b) Jaccard Coefﬁcient
Euclidean Distance
TextBugger
DeepWordBug
(c) Euclidean Distance
Semantic Similarity
TextBugger
DeepWordBug
(d) Semantic Similarity
Fig. 9. The average utility of adversarial texts generated on MR dataset under
black-box settings for 10 platforms.
perturbed words roughly has a positive correlation with the
average length of texts; for Google Cloud NLP, the number
of perturbed words changes little with the increasing length
of texts. However, as shown in Fig. 10(b), the increasing
perturbed words do not decrease the semantic similarity of the
adversarial texts. This is because longer text would have richer
semantic information, while the proportion of the perturbed
words is always controlled within a small range by TEXTBUG-
GER. Therefore, with the length of input text increasing, the
perturbed words have smaller impact on the semantic similarity
between original and adversarial texts.
H. Discussion
Toxic Words Distribution. To demonstrate the effectiveness of our method, we visualize the found important words
according to their frequency in Fig. 11(a), in which the words
Perturbed Words
Google Cloud NLP
Microsoft Azure
IBM Watson
(a) Number of Perturbed Words
Semantic Similarity
Google Cloud NLP
Microsoft Azure
IBM Watson
(b) Semantic Similarity
Fig. 10. The impact of document length on the utility of generated adversarial
texts in three online platforms: Google Cloud NLP, IBM Watson and Microsoft
Azure. The subﬁgures are: (a) the number of perturbed words and document
length, (b) the document length and the semantic similarity between generated
adversarial texts and original texts.
(a) Word Cloud
Google Watson Azure
AWS fastText
Proportion
(b) Bug Distribution
(a) The word cloud is generated from IMDB dataset against the
CNN model. (b) The bug distribution of the adversarial texts is generated from
IMDB dataset against the online platforms.
higher frequency will be represented with larger font. From
Fig. 11(a), we can see that the found important words are
indeed negative words, e.g., “bad”, “awful”, “stupid”, “worst”,
“terrible”, etc for negative texts. Slight modiﬁcation on these
negative words would decrease the negative extent of input
texts. This is why TEXTBUGGER can generate adversarial texts
whose only difference to the original texts are few characterlevel modiﬁcations.
Types of Perturbations. The proportion of each operation
chosen by the adversary for the experiments are shown in
Fig. 11(b). We can see that insert is the dominant operation for Microsoft Azure and Amazon AWS, while Sub-
C is the dominant operation for IBM Watson and fastText.
One reason could be that Sub-C is deliberately designed for
creating visually similar adversarial texts, while swap, insert
and delete are common in typo errors. Therefore, the bugs
generated by Sub-C are less likely to be found in the largescale word vector space, thus causing the “out-of-vocabulary”
phenomenon. Meanwhile, delete and Sub-W are used less
than the others. One reason is that Sub-W should satisfy
two conditions: substituting with semantic similar words while
changing the score largely in the ﬁve types of bugs. Therefore,
the proportion of Sub-W is less than other operations.
ATTACK EVALUATION: TOXIC CONTENT DETECTION
Toxic content detection aims to apply NLP, statistics, and
machine learning methods to detect illegal or toxic-related
(e.g., irony, sarcasm, insults, harassment, racism, pornography,
terrorism, and riots, etc.) content for online systems. Such toxic
content detection can help moderators to improve the online
conversation environment.
In this section, we investigate practical performance of
the proposed method for generating adversarial texts against
real-world toxic content detection systems. We start with
introducing the datasets, targeted models and implementation
details. Then we will analyze the results and discuss potential
reasons for the observed performance.
A. Dataset
We apply the dataset provided by the Kaggle Toxic Comment Classiﬁcation competition8. This dataset contains a large
number of Wikipedia comments which have been labeled by
human raters for toxic behavior. There are six types of indicated toxicity, i.e., “toxic”, “severe toxic”, “obscene”, “threat”,
“insult”, and “identity hate” in the original dataset. We consider these categories as toxic and perform binary classiﬁcation
for toxic content detection. For more coherent comparisons, a
balanced subset of this dataset is constructed for evaluation.
This is achieved by random sampling of the non-toxic texts,
obtaining a subset with equal number of samples with the
toxic texts. Further, we removed some abnormal texts (i.e.,
containing multiple repeated characters) and select the samples
that have no more than 200 words for our experiment, due to
the fact that some APIs limit the maximum length of input
sentences. We obtained 12,630 toxic texts and non-toxic texts
respectively.
B. Targeted Model & Implementation
For white-box experiments, we evaluated the TEXTBUG-
GER on self-trained LR, CNN and LSTM models as we
do in Section III. All models are trained in a hold-out test
strategy, i.e., 80%, 10%, 10% of the data was used for training,
validation and test, respectively. Hyper-parameters were tuned
only on the validation set, and the ﬁnal adversarial examples
are generated and evaluated on the test set.
For black-box experiments, we evaluated the TEXTBUG-
GER on ﬁve toxic content detection platforms/models, including Google Perspective, IBM Natural Language Classiﬁer,
Facebook fastText, ParallelDots AI, and Aylien Offensive
Detector. Since the IBM Natural Language Classiﬁer and
the Facebook fastText need to be trained by ourselves9, we
selected 80% of the Kaggle dataset for training and the rest
for testing. Note that we do not selected samples for validation
since these two models only require training and testing set.
The implementation details of our toxic content attack are
similar with that in the sentiment analysis attack, including the
baselines.
C. Attack Performance
Effectiveness and Efﬁciency. Tables V and VI summarize
the main results of the white-box and black-box attacks on the
Kaggle dataset. We can observe that under white-box settings,
the Random strategy has minor inﬂuence on the ﬁnal results in
Table V. On the contrary, TEXTBUGGER only perturbs a few
words to achieve high attack success rate and performs much
better than baseline algorithms against all models/platforms.
8 
9We do not know the models’ parameters or architechtures because they
only provide training and predicting interfaces.
RESULTS OF THE WHITE-BOX ATTACK ON KAGGLE DATASET.
Targeted Model
Original Accuracy
FGSM+NNS 
DeepFool+NNS 
TEXTBUGGER
RESULTS OF THE BLACK-BOX ATTACK ON KAGGLE DATASET.
Targeted Platform/Model
Original Accuracy
DeepWordBug 
TEXTBUGGER
Success Rate
Perturbed Word
Success Rate
Perturbed Word
Google Perspective
IBM Classiﬁer
Facebook fastText
ParallelDots
Aylien Offensive Detector
Perspective
Aylien ParallelDots
Spam Score
Original Text
Perturbed Text
Score distribution of the after-modiﬁcation texts. These texts are
generated from Kaggle dataset against LR model.
For instance, as shown in Table V, it only perturbs 10.3%
words of one sample to achieve 92.3% success rate on the LR
model, while all baselines achieve no more than 40% attack
success rate. As the Kaggle dataset has an average length of 55
words, TEXTBUGGER only perturbed about 6 words for one
sample to conduct successful attacks. Furthermore, as shown
in Table VI, it only perturbs 4.0% words (i.e., about 3 words)
of one sample when achieves 82.1% attack success rate on the
ParallelDots platform. These results imply that an adversary
can successfully mislead the system into assigning signiﬁcantly
different toxicity scores to the original sentences via modifying
them slightly.
Successful Attack Examples. Two successful examples
are shown in Fig. 1 as demonstration. The ﬁrst adversarial text
for toxic content detection in Fig. 1 contains one Sub-W operation (“sexual” to “sexual-intercourse”), which successfully
converts the prediction result of the LSTM model from 96.7%
toxic to 83.5% non-toxic. The second adversarial text for toxic
content detection in Fig. 1 contains three modiﬁcations, i.e.,
one swap operation (“shit” to “shti”), one Sub-C operation
(“fucking” to “fuckimg”) and one Sub-W operation (“hell”
to “helled”). These modiﬁcations successfully convert the
prediction result of the Perspective API from 92% toxic to
78% non-toxic10.
Score Distribution. We also measured the change of the
10Since the Perspective API only returns the toxic score, we consider that
22% toxic score is equal to 78% non-toxic score.
conﬁdence value over all the samples including the failed
samples before and after modiﬁcations. The results are shown
in Fig. 12, where the overall score of the after-modiﬁcation
texts has drifted to non-toxic for all platforms/models.
D. Utility Analysis
Figs. 13 and 14 show the similarity between original texts
and adversarial texts under white-box and black-box settings
respectively. First, Fig. 14 clearly shows that the adversarial
texts generated by TEXTBUGGER preserve more utility than
that generated by DeepWordBug. Second, from Figs. 13(a),
13(b), 14(a) and 14(b), we can observe that the adversarial
texts preserve good utility in terms of word-level. Speciﬁcally,
Fig. 13(a) shows that almost 80% adversarial texts have no
more than 20 edit distance comparing with the original texts for
three models. Meanwhile, Figs. 13(c), 13(d), 14(c) and 14(d)
show that the generated adversarial texts preserve good utility
in terms of vector-level. Speciﬁcally, from Fig. 13(d), we can
see that almost 90% adverasrial texts preserve 0.9 semantic similarity of the original texts. These results imply that
TEXTBUGGER can fool classiﬁers with high success rate while
preserving good utility of the generated adversarial texts.
E. Discussion
Toxic Words Distribution. Fig. 15(a) shows the visualization of the found important words according to their
frequency, where the higher frequency words have larger font
sizes. Observe that the found important words are indeed
toxic words, e.g., “fuck”, “dick”, etc. It is clear that slightly
perturbing these toxic words would decrease the toxic score
of toxic content.
Bug Distribution. Fig. 15(b) shows the proportion of
each operation chosen by the adversary for the black-box
attack. Observe that Sub-C is the dominant operation for all
platforms, and Sub-W is still the least used operation. We do
not give detailed analysis since the results are similar to that
in Section III.
FURTHER ANALYSIS
A. Transferability
In the image domain, an important property of adversarial examples is the transferability, i.e., adversarial images
Edit Distance
(a) Edit Distance
Jaccard Coefficient
(b) Jaccard Coefﬁcient
Euclidean Distance
(c) Euclidean Distance
Semantic Similarity
(d) Semantic Similarity
Fig. 13. The utility of adversarial texts generated on the Kaggle dataset under
white-box settings for LR, CNN and LSTM models.
Edit Distance
TextBugger
DeepWordBug
(a) Edit Distance
Jaccard Coefficient
TextBugger
DeepWordBug
(b) Jaccard Similarity Coefﬁcient
Euclidean Distance
TextBugger
DeepWordBug
(c) Euclidean Distance
Semantic Similarity
TextBugger
DeepWordBug
(d) Semantic Similarity
The average utility of adversarial texts generated on Kaggle dataset
under black-box settings for 5 platforms.
generated for one classiﬁer are likely to be misclassiﬁed by
other classiﬁers. This property can be used to transform blackbox attacks to white-box attacks as demonstrated in .
Therefore, we wonder whether adversarial texts also have this
In this evaluation, we generated adversarial texts on all
three datasets for LR, CNN, and LSTM models. Then, we
evaluated the attack success rate of the generated adversarial
texts against other models/platforms. The experimental results
are shown in Tables VII and VIII. From Table VII, we can
see that there is a moderate degree of transferability among
models. For instance, the adversarial texts generated on the
MR dataset targeting the LR model have 39.5% success
rate when attacking the Azure platform. This demonstrates
that the adversarial texts generated by TEXTBUGGER can
successfully transfer across multiple models. From Table VIII,
(a) Word Cloud
Perspective
AylienParallelDots
Proportion
(b) Bug Distribution
(a) The word cloud is generated from Kaggle dataset against the
CNN model. (b) The bug distribution of the adversarial texts is generated from
Kaggle dataset against the online platforms.
TABLE VII.
TRANSFERABILITY ON IMDB AND MR DATASETS.
Dataset Model
White-box Models
Black-box APIs
Google fastText AWS
TABLE VIII.
TRANSFERABILITY ON KAGGLE DATASET.
White-box Models
Black-box APIs
Perspective
fastText Aylien
ParallelDots
92.3% 28.6% 32.3%
23.7% 82.5% 35.6%
21.5% 26.9% 94.8%
we can see that the adversarial texts generated on the Kaggle
dataset also has good transferability on Aylien and ParallelDots
toxic content detection platforms. For instance, the adversarial
texts against the LR model has 54.3% attack success rate
on the ParallelDots platform. This means attackers can use
transferability to attack online platforms even they have call
B. User study
We perform a user study with human participants on
Amazon Mechanical Turk (MTurk) to see whether the applied
perturbation will change the human perception of the text’s
sentiment. Before the study, we consulted with the IRB ofﬁce
and this study was approved and we did not collect any other
information of participants except for necessary result data.
First, we randomly sampled 500 legitimate samples and
500 adversarial samples from IMDB and Kaggle datasets,
respectively. Among them, half were generated under whitebox settings and half were generated under black-box setting.
All the selected adversarial samples successfully fooled the
targeted classiﬁers. Then, we presented these samples to the
participants and asked them to label the sentiment/toxicity
of these samples, i.e., the text is positive/non-toxic or negative/toxic. Meanwhile, we also asked them to mark the
suspicious words or inappropriate expression in the samples.
To avoid labeling bias, we allow each user to annotate at most
20 reviews and collect 3 annotations from different users for
each sample. Finally, 3,177 valid annotations from 297 AMT
workers were obtained in total.
After examining the results, we ﬁnd that 95.5% legitimate
Existed, found
Existed, not found
Perturbed, not found
Perturbed, found
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Proportion
The detailed results of user study. (a) The distribution of all mistakes
in the samples, including originally existed errors and manully perturbed bugs.
(b) The proportion of found bugs accounting for each kind of bug added in
the samples. For instance, if there are totally 10 Sub-C perturbations in the
samples and we only ﬁnd 3 of them, the ratio is 3/10=0.3.
RESULTS OF SC ON IMDB AND MR DATASETS.
Attack Success Rate
TEXTBUGGER
DeepWordBug
TEXTBUGGER
DeepWordBug
samples can be correctly classiﬁed and 94.9% adversarial
samples can be classiﬁed as their original labels. Furthermore,
we observe that for both legitimate and adversarial samples,
almost all the incorrect classiﬁcations are made on several
speciﬁc samples that have some ambiguous expressions. This
indicates that TEXTBUGGER did not affect the human judgment on the polarity of the text, i.e., the utility is preserved in
the adversarial samples from human perspective, which shows
that the generated adversarial texts are of high quality.
Some detailed results are shown in Fig. 16. From
Fig. 16(a), we can see that in our randomly selected samples, the originally existed errors (including spelling mistakes,
grammatical errors, etc.) account for 34.5% of all errors,
and the bugs we added account for 65.5% of all errors.
Among them, 38.0% (13.1%/34.5%) of existed errors and
30.1% (19.7%/65.5%) of the added bugs are successfully
found by participants, which implies that our perturbation is
inconspicuous. From Fig. 16(b), we can see that insert is the
easiest bug to ﬁnd, followed by Sub-C. Speciﬁcally, the found
Sub-C perturbations are almost the substitution of “o” to “0”,
and the substitution of “l” to “1” is seldom found. In addition,
the Sub-W perturbation is the hardest to ﬁnd.
POTENTIAL DEFENSES
To the best of our knowledge, there are few defense
methods for the adversarial text attack. Therefore, we conduct
a preliminary exploration of two potential defense schemes,
i.e., spelling check and adversarial training. Speciﬁcally, we
evaluate the spelling check under the black-box setting and
evaluate the adversarial training under the white-box setting.
By default, we use the same implementation settings as that
in Section IV.
Spelling Check (SC). In this experiment, we use a contextaware spelling check service provided by Microsoft Azure11.
11 
RESULTS OF SC ON KAGGLE DATASET.
Attack Success Rate
Perspective
ParallelDots
TEXTBUGGER
DeepWordBug
Proportion
Perspective IBM
fastText Aylien Parallel
Proportion
(b) Kaggle
The ratio of the bugs corrected by spelling check to the total bugs
generated on IMDB and Kaggle datasets.
Experimental results are shown in Tables IX and X, from
which we can see that though many generated adversarial
texts can be detected by spell checking, TEXTBUGGER still
have higher success rate than DeepWordBug on multiple online
platforms after correcting the misspelled words. For instance,
when targeting on Perspective API, TEXTBUGGER has 35.6%
success rate while DeepWordBug only has 16.5% after spelling
check. This means TEXTBUGGER is still effective and stronger
than DeepWordBug.
Further, we analyze the difﬁculty of correcting each kind
of bug. Speciﬁcally, we wonder which kind of bugs is the
easiest to correct and which kind of bugs is the hardest to
correct. We count the number of corrected bugs of each kind
and show the results in Fig. 17. From Fig. 17, we can see
that the easiest bug to correct is insert and delete for IMDB
and Kaggle respectively. The hardest bug to correct is Sub-
W, which has less than 10% successfully correction ratio.
This phenomenon partly accounts for why TEXTBUGGER is
stronger than DeepWordBug.
Adversarial Training (AT). Adversarial training means
training the model with generated adversarial examples. For
instance, in the context of toxic content detection systems,
we need to include different modiﬁed versions of the toxic
documents into the training data. This method can improve
the robustness of machine learning models against adversarial
examples .
In our experiment, we trained the targeted model with the
combined dataset for 10 epochs, and the learning rate is set to
be 0.0005. We show the performance of this scheme along with
detailed settings in Table XI, where accuracy means the prediction accuracy of the new models on the legitimate samples,
and success rate with adversarial training (SR with AT) denotes
the percentage of the adversarial samples that are misclassiﬁed
as wrong labels by the new models. From Table XI, we can see
that the success rate of adversarial texts decreases while the
models’ performance on legitimate samples does not change
too much with AT. Therefore, adversarial training might be
effective in defending TEXTBUGGER.
However, a limitation of adversarial training is that it needs
to know the details of the attack strategy and to have sufﬁcient
RESULTS OF AT ON THREE DATASETS.
SR with AT
adversarial texts for training. In practice, however, attackers
usually do not make their approaches or adversarial texts
public. Therefore, adversarial training is limited in defending
unknown adversarial attacks.
Improvement
TEXTBUGGER.
TEXTBUGGER can be partly defended by the above methods,
attackers can take some strategies to improve the robustness of
their attacks. For instance, attackers can increase the proportion
of Sub-W as it is almost cannot be corrected by spelling check.
In addition, attackers can adjust the proportion of different
strategies among different platforms. For instance, attackers
can increase the proportion of swap on the Kaggle dataset
when targeting the Perspective and Aylien API, since less
than 40% swap modiﬁcations have been corrected as shown
in Fig. 17(b). Attackers can also keep their adversarial attack
strategies private and change the parameters of the attack
frequently to evade the AT defense.
DISCUSSION
Extension to Targeted Attack. In this paper, we only
perform untargeted attacks, i.e., changing the model’s output.
However, TEXTBUGGER can be easily adapted for targeted
attacks (i.e., forcing the model to give a particular output)
by modifying Eq.2 from computing the Jacobian matrix with
respect to the ground truth label to computing the Jacobian
matrix with respect to the targeted label.
Limitations
Work. Though our results
demonstrate the existence of natural-language adversarial perturbations, our perturbations could be improved via a more
sophisticated algorithm that takes advantage of language processing technologies, such as syntactic parsing, named entity
recognition, and paraphrasing. Furthermore, the existing attack
procedure of ﬁnding and modifying salient words can be
extended to beam search and phrase-level modiﬁcation, which
is an interesting future work. Developing effective and robust
defense schemes is also a promising future work.
RELATED WORK
A. Adversarial Attacks for Text
Gradient-based Methods. In one of the ﬁrst attempts
at tricking deep neural text classiﬁers , Papernot et al.
proposed a white-box adversarial attack and applied it repetitively to modify an input text until the generated sequence is
misclassiﬁed. While their attack was able to fool the classi-
ﬁer, their word-level changes signiﬁcantly affect the original
meaning. In , Ebrahimi et al. proposed a gradient-based
optimization method that changes one token to another by
using the gradients of the model with respect to the onehot vector input. In , Samanta et al. used the embedding
gradient to determine important words. Then, heuristic driven
rules together with hand-crafted synonyms and typos were
Out-of-Vocabulary Word. Some existing works generate
adversarial examples for text by replacing a word with one
legible but out-of-vocabulary word . In , Belinkov
et al. showed that character-level machine translation systems
are overly sensitive to random character manipulations, such
as keyboard typos. Similarly, Gao et al. proposed DeepWord-
Bug , which applies character perturbations to generate
adversarial texts against deep learning classiﬁers. However, this
method is not computationally efﬁcient and cannot be applied
in practice. In , Hosseini et al. showed that simple modiﬁcations, such as adding spaces or dots between characters, can
drastically change the toxicity score from Perspective API.
Replace with Semantically/Syntactically Similar Words.
In , Alzantot et al. generated adversarial text against sentiment analysis models by leveraging a genetic algorithm and
only replacing words with semantically similar ones. In ,
Ribeiro et al. replaced tokens by random words of the same
POS tag with a probability proportional to the embedding
similarity.
Other Methods. In , Jia et al. generated adversarial
examples for evaluating reading comprehension systems by
adding distracting sentences to the input document. However,
their method requires manual intervention to polish the added
sentences. In , Zhao et al. used Generative Adversarial
Networks (GANs) to generate adversarial sequences for textual
entailment and machine translation applications. However, this
method requires neural text generation, which is limited to
short texts.
B. Defense
To the best of our knowledge, existing defense methods for
adversarial examples mainly focus on the image domain and
have not been systematically studied in the text domain. For
instance, the adversarial training, one of the famous defense
methods for adversarial images, has been only used as a
regularization technique in the DLTU task . These
works only focused on improving the accuracy on clean
examples, rather than defending textual adversarial examples.
C. Remarks
In summary, the following aspects distinguish TEXTBUG-
GER from existing adversarial attacks on DLTU systems. First,
we use both character-level and word-level perturbations to
generate adversarial texts, in contrast to previous works that
use the projected gradient or linguistic-driven steps .
Second, we demonstrate that our method has great efﬁciency
while previous works seldom evaluate the efﬁciency of their
methods . Finally, most if not all previous works only
evaluate their method on self-implemented models ,
or just evaluate them on one or two public ofﬂine models
 . By contrast, we evaluate the generated adversarial
examples on 15 popular real-world online DLTU systems,
including Google Cloud NLP, IBM Watson, Amazon AWS,
Microsoft Azure, Facebook fastText, etc. The results demonstrate that TEXTBUGGER is more general and robust.
CONCLUSION
Overall, we study adversarial attacks against state-ofthe-art sentiment analysis and toxic content detection models/platforms under both white-box and black-box settings. Extensive experimental results demonstrate that TEXTBUGGER
is effective and efﬁcient for generating targeted adversarial
NLP. The transferability of such examples hint at potential
vulnerabilities in many real applications, including text ﬁltering systems (e.g., racism, pornography, terrorism, and riots),
online recommendation systems, etc. Our ﬁndings also show
the possibility of spelling check and adversarial training in
defending against such attacks. Ensemble of linguisticallyaware or structurally-aware based defense system can be
further explored to improve robustness.
ACKNOWLEDGMENT
This work was partly supported by NSFC under No.
61772466, the Zhejiang Provincial Natural Science Foundation
for Distinguished Young Scholars under No. LR19F020003,
the Provincial Key Research and Development Program of
Zhejiang, China under No. 2017C01055, and the Alibaba-ZJU
Joint Research Institute of Frontier Technologies. Ting Wang is
partially supported by the National Science Foundation under
Grant No. 1566526 and 1718787. Bo Li is partially supported
by the Defense Advanced Research Projects Agency (DARPA).