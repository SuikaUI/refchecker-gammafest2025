THE POWER OF SPARSITY IN
CONVOLUTIONAL NEURAL NETWORKS
Soravit Changpinyo ∗
Department of Computer Science
University of Southern California
Los Angeles, CA 90020, USA
 
Mark Sandler and Andrey Zhmoginov
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94043, USA
{sandler,azhmogin}@google.com
Deep convolutional networks are well-known for their high computational and
memory demands. Given limited resources, how does one design a network that
balances its size, training time, and prediction accuracy? A surprisingly effective
approach to trade accuracy for size and speed is to simply reduce the number of
channels in each convolutional layer by a ﬁxed fraction and retrain the network. In
many cases this leads to signiﬁcantly smaller networks with only minimal changes
to accuracy. In this paper, we take a step further by empirically examining a
strategy for deactivating connections between ﬁlters in convolutional layers in a
way that allows us to harvest savings both in run-time and memory for many
network architectures. More speciﬁcally, we generalize 2D convolution to use a
channel-wise sparse connection structure and show that this leads to signiﬁcantly
better results than the baseline approach for large networks including VGG and
Inception V3.
INTRODUCTION
Deep neural networks combined with large-scale labeled data have become a standard recipe for
achieving state-of-the-art performance on supervised learning tasks in recent years. Despite of their
success, the capability of deep neural networks to model highly nonlinear functions comes with high
computational and memory demands both during the model training and inference. In particular, the
number of parameters of neural network models is often designed to be huge to account for the scale,
diversity, and complexity of data that they learn from. While advances in hardware have somewhat
alleviated the issue, network size, speed, and power consumption are all limiting factors when it
comes to production deployment on mobile and embedded devices. On the other hand, it is wellknown that there is signiﬁcant redundancy among the weights of neural networks. For example,
Denil et al. show that it is possible to learn less than 5% of the network parameters and
predict the rest without losing predictive accuracy. This evidence suggests that neural networks are
often over-parameterized.
These motivate the research on neural network compression. However, several immediate questions arise: Are these parameters easy to identify? Could we just make the network 5% of its size
and retrain? Or are more advanced methods required? There is an extensive literature in the last
few years that explores the question of network compression using advanced techniques, including
network prunning, loss-based compression, quantization, and matrix decomposition. We overview
many of these directions in the next section. However, there is surprisingly little research on whether
this over-parameterization can simply be re-captured by more efﬁcient architectures that could be
obtained from original architectures via simple transformations.
Our approach is inspired by a very simple yet successful method called depth multiplier . In this method the depth (the number of channels) of each convolutional layer in a given
network is simply reduced by a ﬁxed fraction and the network is retrained. We generalize this approach by removing the constraint that every input ﬁlter (or channel) must be fully connected to
every output ﬁlter. Instead, we use a sparse connection matrix, where each output convolution chan-
∗The work was done while the author was doing an internship at Google Research.
 
nel is connected only to a small random fraction of the input channels. Note that, for convolutional
networks, this still allows for efﬁcient computation since the one channel spatial convolution across
the entire plane remains unchanged.
We empirically demonstrate the effectiveness of our approach on four networks (MNIST, CIFAR
Net, Inception-V3 and VGG-16) of different sizes. Our results suggest that our approach outperforms dense convolutions with depth multiplier at high compression rates.
For Inception V3 , we show that we can train a network with only about 300K
of convolutional parameters1 and about 100M multiply-adds that achieves above 52% accuracy after it is fully trained. The corresponding depth-multiplier network has only about 41% accuracy.
Another network that we consider is VGG-16n, a slightly modiﬁed version of VGG-16 , with 7x fewer parameters and similar accuracy. We found VGG-16n to start
training much faster than the original VGG-16 which was trained incrementally in the original literature. We explore the impact of sparsiﬁcation and the number of parameters on the quality of the
network by building the networks up to 30x smaller than VGG-16n introduce hyperbolic and exponential biases to the objective. Optimal Brain Damage and Optimal Brain Surgeon
 prune the networks based on second-order derivatives of the objectives.
Recent work by Han et al. alternates between pruning near-zero weights, which are
encouraged by ℓ1 or ℓ2 regularization, and retraining the pruned networks.
More complex regularizers have also been considered. Wen et al. and Li et al. put
structured sparsity regularizers on the weights, while Murray & Chiang put them on the hidden units. Feng & Darrell explore a nonparametric prior based on the Indian buffet processes
 on layers. Hu et al. prune neurons based on the analysis of
their outputs on a large dataset. Anwar et al. consider special sparsity patterns: channel-wise
(removing a feature map/channel from a layer), kernel-wise (removing all connections between two
feature maps in consecutive layers), and intra-kernel-strided (removing connections between two
features with particular stride and offset). They also propose to use particle ﬁlter to decide the
importance of connections and paths during training.
Another line of work explores ﬁxed network architectures with some subsets of connections removed. For example, LeCun et al. remove connections between the ﬁrst two convolutional
feature maps in a completely uniform manner. This is similar to our approach but they only consider a pre-deﬁned pattern in which the same number of input feature map are assigned to each
output feature map (Random Connection Table in Torch’s SpatialConvolutionMap function). Fur-
1Here and elsewhere, we ignore the parameters for the softmax classiﬁer since they simply describe a linear
transformation and depend on number of classes.
ther, they do not explore how sparse connections affect performance compared to dense networks.
Along a similar vein, Cires¸an et al. remove random connections in their MNIST experiments. However, they do not try to preserve the spatial convolutional density and it might be a
challenge to harvest the savings on existing hardware. Ioannou et al. explore three types of
hierarchical arrangements of ﬁlter groups for CNNs, which depend on different assumptions about
co-dependency of ﬁlters within each layer. These arrangements include columnar topologies inspired by AlexNet , tree-like topologies previously used by Ioannou et al.
 , and root-like topologies. Finally, Howard proposes the depth multiplier method to
scale down the number of ﬁlters in each convolutional layer by a factor. In this case, depth multiplier can be thought of channel-wise pruning mentioned in . However, depth
multiplier modiﬁes the network architectures before training and removes each layer’s feature maps
in a uniform manner.
With the exception of and depth multiplier , the above previous work performs connection pruning that leads to irregular
network architectures. Thus, those techniques require additional efforts to represent network connections and might or might not allow for direct computational savings.
Quantization
Reducing the degree of redundancy of model parameters can be done in the form
of quantization of network parameters. Hwang & Sung ; Arora et al. and Courbariaux
et al. ; Rastegari et al. propose to train CNNs with ternary weights and binary
weights, respectively. Gong et al. use vector quantization for parameters in fully connected
layers. Anwar et al. quantize a network with the squared error minimization. Chen et al.
 randomly group network parameters using a hash function. We note that this technique
could be complementary to network pruning. For example, Han et al. combine connection
pruning in with quantization and Huffman coding.
Decomposition
Another approach is based on low-rank decomposition of the parameters. Decomposition methods include truncated SVD , decomposition to rank-1 bases
 , CP decomposition (PARAFAC or CANDECOMP) ,
Tensor-Train decomposition of Oseledets , sparse dictionary learning
of Mairal et al. and PCA , asymmetric (3D) decomposition using reconstruction loss of non-linear responses combined with a rank selection method based on PCA accumulated
energy , and Tucker decomposition using the kernel tensor reconstruction loss
combined with a rank selection method based on global analytic variational Bayesian matrix factorization .
REGULARIZATION OF NEURAL NETWORKS
Hinton et al. ; Srivastava et al. propose Dropout for regularizing fully connected layers
within neural networks layers by randomly setting a subset of activations to zero during training.
Wan et al. later propose DropConnect, a generalization of Dropout that instead randomly sets
a subset of weights or connections to zero. Our approach could be thought as related to DropConnect, but (1) we remove connections before training; (2) we focus on connections between convolutional layers; and (3) we kill connections in a more regular manner by restricting connection patterns
to be the same along spatial dimensions.
Recently, Han et al. and Jin et al. propose a form of regularization where dropped
connections are unfrozen and the network is retrained. This idea is similar to our incremental training
approach. However, (1) we do not start with a full network; (2) we do not unfreeze connections all
at once; and (3) we preserve regularity of the convolution operation.
NEURAL NETWORK ARCHITECTURES
Network compression and architectures are closely related. The goal of compression is to remove
redundancy in network parameters; therefore, the knowledge about traits that determine architecture’s success would be desirable. Other than the discovery that depth is an important factor , little is known about such traits.
Some previous work performs architecture search but without the main goal of doing compression
 . Recent work proposes shortcut/skip con-
nections to convolutional networks. See, among others, highway networks ,
residual networks , networks with stochastic depth , and
densely connected convolutional networks .
A CNN architecture consist of (1) convolutional layers, (2) pooling layers, (3) fully connected layers,
and (4) a topology that governs how these layers are organized. Given an architecture, our general
goal is to transform it into another architecture with a smaller number of parameters. In this paper,
we limit ourselves to transformation functions that keep the general topology of the input architecture intact. Moreover, the main focus will be on the convolutional layers and convolution operations,
as they impose highest computational and memory burden for most if not all large networks.
DEPTH MULTIPLIER
We ﬁrst give a description of the depth multiplier method used in Howard . Given a hyperparameter α ∈(0, 1], the depth multiplier approach scales down the number of ﬁlters in each
convolutional layers by α. Note that depth here refers to the third dimension of the activation volume of a single layer, not the number of layers in the whole network.
Let nl−1 and nl be the number of input and output ﬁlters at layer l, respectively. After the operation nl−1 and nl become ⌈αnl−1⌉and ⌈αnl⌉and the number of parameters (and the number of
multiplications) becomes ≈α2 of the original number.
The result of this operation is a network that is both 1/α2 smaller and faster. Many large networks
can be signiﬁcantly reduced in size using this method with only a small loss of precision . It is our belief that this method establishes a strong baseline to which any other advanced
techniques should compare themselves. To the best of our knowledge, we are not aware of such
comparisons in the literature.
SPARSE RANDOM
Instead of looking at depth multiplier as deactivating channels in the convolutional layers, we can
look at it from the perspective of deactivating connections. From this point of view, depth multiplier
kills the connections between two convolutional layers such that (a) the connection patterns are
still the same across spatial dimensions and (b) all “alive” input channels are fully connected to all
“alive” output channels.
We generalize this approach by relaxing (b) while maintaining (a). That is, for every output channel, we connect it to a small subset of input channels. In other words, dense connections between
a small number of channels become sparse connections between larger number of channels. This
can be summarized in Fig. 1. The advantage of this is that the actual convolution can still be computed efﬁciently because sparsity is introduced only at the outer loop of the convolution operation
and we can still take the advantage of the continuous memory layout. For more details regarding
implementations of the two approaches, please refer to the Appendix.
More concretely, let nl−1 and nl be the number of channels of layer l −1 and layer l, respectively.
For a sparsity coefﬁcient α, each output ﬁlter j only connects to an α fraction of ﬁlters of the
previous layer. Thus, instead of having a connectivity matrix Wsij of dimension k2 × nl−1 × nl,
we have a sparse matrix with non-zero entries at Wsaijj, where aij is an index matrix of dimension
k2 × αnl−1 × nl and k is the kernel size.
INCREMENTAL TRAINING
In contrast to depth multiplier, a sparse convolutional network deﬁnes a connection pattern on a
much bigger network. Therefore, an interesting extension is to consider incremental training: we
start with a network that only contains a small fraction of connections (in our experiments we use
1% and 0.1%) and add connections over time. This is motivated by an intuition that the network can
use learned channels in new contexts by introducing additional connections. The potential practical
advantage of this approach is that since we start training with very small networks and grow them
over time, this approach has a potential to speed up the whole training process signiﬁcantly. We note
Figure 1: Connection tensors of depth multiplier (left) and sparse random (right) approaches for
nl−1 = 5 and nl = 10. Yellow denotes active connections. For both approaches, the connection
pattern is the same across spatial dimension and ﬁxed before training. However, in the sparse random
approach, each output channel is connected to a (possibly) different subset of input channels, and
vice versa.
that depth multiplier will not beneﬁt from this approach as any newly activated connections would
require learning new ﬁlters from scratch.
In this section, we approach a question of why sparse convolutions are frequently more efﬁcient than
the dense convolutions with the same number of parameters. Our main intuition is that the sparse
convolutional networks promote diversity. It is much harder to learn equivalent set of channels as, at
high sparsity, channels have distinct connection structure or even overlapping connections. This can
be formalized with a simple observation that any dense network is in fact a part of an exponentially
large equivalence class, which is guaranteed to produce the same output for every input.
Lemma 1 Any dense convolutional neural network with no cross-channel nonlinearities, distinct
weights and biases, and with l hidden layers of sizes n1, n2, ..., nl, has at least Ql
i=1 ni! distinct
equivalent networks which produce the same output.
Proof Let I denote the input to the network, Ci be the convolutional operator, σi denote the nonlinearity operator applied to the i-th convolution layer and S be a ﬁnal transformation (e.g. softmax
classiﬁer). We assume that σi is a function that operates on each of the channels independently. We
note that this is the case for almost any modern network. The output of the network can then be
written as:
N(I) ≡S ◦σl ◦Cl ◦σl−1 ◦· · · ◦σ1 ◦C1(I)
where we use ◦to denote function composition to avoid numerous parentheses. The convolution
operator Ci operates on input with ni−1 channels and produces an output with ni channels. Now,
ﬁx arbitrary set of permutation functions πi, where πi can permute depth of size ni. Since πi is
a linear function, it follows that C′
Ciπi−1 is a valid convolutional operator, which can be
obtained from Ci by permuting its bias according to πi and its weight matrix along input and output
dimensions according to πi−1 and πi respectively. For a new network deﬁned as:
N ′(I) = S′ ◦σl ◦C′
l ◦σl−1 ◦· · · ◦σ1 ◦C′
where π0 is an identity operator and S′ ≡S ◦πl, we claim that N ′(I) ≡N(I). Indeed, since
nonlinearities do not apply cross-depth we have πnσnπ−1
≡σn and thus:
N ′(I) = S′ ◦σl ◦C′
l ◦σl−1 ◦· · · ◦σ1 ◦C′
= S ◦πl ◦σl ◦π−1
◦Cl ◦πl−1 ◦· · · ◦π1 ◦σ1 ◦π−1
◦C1(I) = N(I).
Thus, any set of permutations on hidden units deﬁnes an equivalent network.
It is obvious that sparse networks are much more immune to parameter permutation – indeed every
channel at layer l is likely to have a unique tree describing its connection matrix all the way down.
Exploring this direction is an interesting open question.
EXPERIMENTS
In this section, we demonstrate the effectiveness of the sparse random approach by comparing it to
the depth multiplier approach at different compression rates. Moreover, we examine several settings
in the incremental training where connections gradually become active during the training process.
Networks and Datasets
Our experiments are conducted on 4 networks for 3 different datasets.
All our experiments use open-source TensorFlow networks Abadi et al. .
MNIST AND CIFAR-10
We use standard networks provided by TensorFlow. For MNIST, it has
3-layer convolutional layers and achieves 99.5% accuracy when fully trained. For CIFAR-10, it has
2 convolutional layers and achieves 87% accuracy.
We use open source Inception-V3 network and a slightly modiﬁed version of VGG-16 called VGG-16n on ImageNet ILSVRC
2012 .
Random connections
Connections are activated according to their likelihood from the uniform
distribution. In addition, they are activated in such a way that there are no connections going in or
coming out of dead ﬁlters (i.e., any connection must have a path to input image and a path to the
ﬁnal prediction.). All connections in fully connected layers are retained.
Implementation details
All code is implemented in TensorFlow . Deactivating connections is done by applying masks to parameter tensors. The Inception-v3 and VGG-16n
networks are trained on 8 Tesla K80 GPUs, each with batch size 256 (32 per gpu) and batch normalization was used for all networks.
COMPARISON BETWEEN SPARSE RANDOM AND DEPTH MULTIPLIER
MNIST AND CIFAR-10
We ﬁrst compare depth multiplier and sparse random for the two small networks on MNIST and
CIFAR-10. We compare the accuracy of the two approaches when the numbers of connections are
roughly the same, based on a hyperparameter α. For dense convolutions, we pick a multiplier α and
each ﬁlter depth is scaled down by √α and then rounded up. In sparse convolutions, a fraction α of
connections are randomly deactivated if those parameters connect at least two ﬁlters on each layer;
otherwise, a fraction of √α is used instead if the parameters connect layers with only one ﬁlter left.
The accuracy numbers are averaged over 5 rounds for MNIST and 2 rounds on CIFAR-10.
We show in Fig. 2 and Fig. 3 that the sparse networks have comparable or higher accuracy for the
same number of parameters, with comparable accuracy at higher density. We note however that
these networks are so small that at high compression rates most of operations are concentrated at the
ﬁrst layer, which is negligible for large networks. Moreover, in MNIST example, the size of network
changes most dramatically from 2000 to 2 million parameters, while affecting accuracy only by 1%.
This observation suggests that there might be beneﬁts of maintaining the number of ﬁlters to be high
and/or breaking the symmetry of connections. We explore this in the next section.
INCEPTION-V3 ON IMAGENET
We consider different values of sparsity ranging from 0.003 to 1, and depth multiplier from 0.05
to 1. Our experiments show (see Table 1 and Fig. 4) signiﬁcant advantage of sparse networks over
equivalently sized dense networks. We note that due to time constraints the reported quantitative
numbers are preliminary, as the networks have not ﬁnished converging. We expect the ﬁnal numbers
to match the reported number for Inception V3 Szegedy et al. , and the smaller networks to
have comparable improvement.
VGG-16 ON IMAGENET
In our experiments with the VGG-16 network , we modify the model
architecture (calling it VGG-16n) by removing the two fully-connected layers with depth 4096 and
replacing them with a 2 × 2 maxpool layer followed by a 3 × 3 convolutional layer with the depth
of 1024. This alone sped up our training signiﬁcantly. The comparison between depth multiplier
and sparse connection approaches is shown in Fig. 5. The modiﬁed VGG-16n network has about 7
times fewer parameters, but appears to have comparable precision.
Figure 2: Comparison of accuracy (averaged over 5 rounds) vs. Number of parameters/Number of
multiply-adds between dense and sparse convolutions on MNIST dataset. Note that though sparse
convolution result in better parameter trade-off curve, the multiply-add curve shows the opposite
Table 1: Inception V3: Preliminary quantitative results after 100 Epochs. Note the smallest sparse
network is actually a hybrid network - we used both depth multiplier (0.5) and sparsity (0.01). The
number of parameters is the number of parameters excluding the softmax layer.
Accuracy for sparse convolutions
Accuracy for Depth Multiplier
Multiplier
Original network:
INCREMENTAL TRAINING
Finally, we show that incremental training is a promising direction. We start with a very sparse
model and increase its density over time, using the approach described in Sect. 3.2.1. We note that
a naive approach where we simply add ﬁlters results in training process basically equivalent to as if
Figure 3: Comparison of accuracy (averaged over 2 rounds) vs. Number of parameters/Number of
multiply-adds between dense and sparse convolutions on CIFAR-10 dataset.
Figure 4: Inception V3: Comparison of Precision@1 vs.
Number of parameters/Number of
multiply-adds between dense and sparse convolutions on ImageNet/Inception-V3. The full network
corresponds to the right-most point of the curve.
Figure 5: VGG 16: Preliminary Quantitative Results. Comparison of Precision@1 vs. Number of
parameters/Number of multiply-adds between dense and sparse convolutions on ImageNet/VGG-
16n. The full network corresponds to the right-most point of the curve. Original VGG-16 as described in Simonyan & Zisserman (blue star) and the same model trained by us from scratch
(red cross) are also shown.
it started from scratch in every step. On the other hand, when the network densiﬁes over time, all
channels already possess some discriminative power and that information is utilized.
In our experiments, we initially start training Inception-V3 with only 1% or 0.1% of connections
enabled. Then, we double the number of connections every T steps. We use T = 10, 000, T =
25, 000 and T = 50, 000. The results are presented in Fig. 6. We show that the networks trained with
the incremental approach regardless of the doubling period can catch up with the full Inception-V3
network (in some cases with small gains). Moreover, they recover very quickly from adding more
(untrained) connections. In fact, the recovery is so fast that it is shorter than our saving interval
for all the networks except for the network with 10K doubling period (resulting in the sharp drop).
We believe that incremental training is a promising direction to speeding up the training of large
convolutional neural networks since early stages of the training require much less computation.
CONCLUSION AND FUTURE WORK
We have proposed a new compression technique that uses a sparse random connection structure
between input-output ﬁlters in convolutional layers of CNNs. We ﬁx this structure before training
and use the same structure across spatial dimensions to harvest savings from modern hardware. We
show that this approach is especially useful at very high compression rates for large networks. For
example, this simple method when applied to Inception V3 (Fig. 4) achieves AlexNet-level accuracy
Figure 6: Incremental Training Of Inception V3: We show Precision@1 during the training process,
where the networks densify over time. The saturation points show where the networks actually reach
their full density.
 with fewer than 400K parameters and VGG-level one (Fig. 5) with roughly
3.5M parameters.
The simplicity of our approach is instructive in that it establishes a strong baseline to compare against
when developing more advanced techniques. On the other hand, the uncanny match in performance
of dense and equivalently-sized sparse networks with sparsity > 0.1 suggests that there might be
some fundamental property of network architectures that is controlled by the number of parameters,
regardless of how they are organized. Exploring this further might yield additional insights on
understanding neural networks.
In addition, we show that our method leads to an interesting novel incremental training technique,
where we take advantage of sparse (and smaller) models to build a dense network. One interesting
open direction is to enable incremental training not to simply densify the network over time, but also
increase the number of channels. This would allow us to grow the network without having to ﬁx its
original shape in place.
Examining actual gains on modern GPUs is left for future work. Nevertheless, we believe that
our results help guide the future hardware and software optimization for neural networks. We also
note that the story is different in embedded applications where CUDA does not exist yet, but this
is beyond the scope of the paper. Additionally, another interesting future research direction is to
investigate the effect of different masking schedules.