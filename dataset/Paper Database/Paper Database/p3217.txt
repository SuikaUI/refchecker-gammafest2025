Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 72–78
Vancouver, Canada, July 30 - August 4, 2017. c⃝2017 Association for Computational Linguistics
 
Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 72–78
Vancouver, Canada, July 30 - August 4, 2017. c⃝2017 Association for Computational Linguistics
 
Learning to Parse and Translate Improves Neural Machine Translation
Akiko Eriguchi†, Yoshimasa Tsuruoka†, and Kyunghyun Cho‡
†The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, Japan
{eriguchi, tsuruoka}@logos.t.u-tokyo.ac.jp
‡New York University, New York, NY 10012, USA
 
There has been relatively little attention
to incorporating linguistic prior to neural machine translation.
Much of the
previous work was further constrained to
considering linguistic prior on the source
side. In this paper, we propose a hybrid
model, called NMT+RNNG, that learns
to parse and translate by combining the
recurrent neural network grammar into
the attention-based neural machine translation. Our approach encourages the neural machine translation model to incorporate linguistic prior during training, and
lets it translate on its own afterward. Extensive experiments with four language
pairs show the effectiveness of the proposed NMT+RNNG.
Introduction
Neural Machine Translation (NMT) has enjoyed
impressive success without relying on much, if
any, prior linguistic knowledge. Some of the most
recent studies have for instance demonstrated that
NMT systems work comparably to other systems
even when the source and target sentences are
given simply as ﬂat sequences of characters or statistically, not
linguistically, motivated subword units . Shi et al. 
recently made an observation that the encoder of
NMT captures syntactic properties of a source sentence automatically, indirectly suggesting that explicit linguistic prior may not be necessary.
On the other hand, there have only been a
couple of recent studies showing the potential
beneﬁt of explicitly encoding the linguistic prior
into NMT. Sennrich and Haddow for instance proposed to augment each source word with
its corresponding part-of-speech tag, lemmatized
form and dependency label. Eriguchi et al. 
instead replaced the sequential encoder with a
tree-based encoder which computes the representation of the source sentence following its parse
tree. Stahlberg et al. let the lattice from a
hierarchical phrase-based system guide the decoding process of neural machine translation, which
results in two separate models rather than a single
end-to-end one. Despite the promising improvements, these explicit approaches are limited in that
the trained translation model strictly requires the
availability of external tools during inference time.
More recently, researchers have proposed methods to incorporate target-side syntax into NMT
models. Alvarez-Melis and Jaakkola have
proposed a doubly-recurrent neural network that
can generate a tree-structured sentence, but its effectiveness in a full scale NMT task is yet to be
shown. Aharoni and Goldberg introduced
a method to serialize a parsed tree and to train the
serialized parsed sentences.
We propose to implicitly incorporate linguistic prior based on the idea of multi-task learning . More
speciﬁcally, we design a hybrid decoder for NMT,
called NMT+RNNG1, that combines a usual conditional language model and a recently proposed recurrent neural network grammars . This is done by plugging
in the conventional language model decoder in the
place of the buffer in RNNG, while sharing a subset of parameters, such as word vectors, between
the language model and RNNG. We train this hybrid model to maximize both the log-probability of
a target sentence and the log-probability of a parse
action sequence. We use an external parser to generate target parse actions,
but unlike the previous explicit approaches, we do
not need it during test time.
1Our code is available at 
tempra28/nmtrnng.
We evaluate the proposed NMT+RNNG on four
language pairs ({JP, Cs, De, Ru}-En). We observe
signiﬁcant improvements in terms of BLEU scores
on three out of four language pairs and RIBES
scores on all the language pairs.
Neural Machine Translation
Neural machine translation is a recently proposed
framework for building a machine translation system based purely on neural networks.
It is often built as an attention-based encoder-decoder
network with two recurrent
networks—encoder and decoder—and an attention model. The encoder, which is often implemented as a bidirectional recurrent network with
long short-term memory units or gated recurrent
units , ﬁrst reads a source
sentence represented as a sequence of words x =
(x1, x2, . . . , xN). The encoder returns a sequence
of hidden states h = (h1, h2, . . . , hN). Each hidden state hi is a concatenation of those from the
forward and backward recurrent network: hi =
h−→h i; ←−h i
−→h i =−→f enc(−→h i−1, Vx(xi)),
←−h i =←−f enc(←−h i+1, Vx(xi)).
Vx(xi) refers to the word vector of the i-th source
The decoder is implemented as a conditional recurrent language model which models the target
sentence, or translation, as
log p(y|x) =
log p(yj|y<j, x),
where y = (y1, . . . , yM). Each of the conditional
probabilities in the r.h.s is computed by
p(yj = y|y<j, x) = softmax(W ⊤
˜sj = tanh(Wc[sj; cj]),
sj = fdec(sj−1, [Vy(yj−1); ˜sj−1]),
where fdec is a recurrent activation function, such
as LSTM or GRU, and Wy is the output word vector of the word y.
cj is a time-dependent context vector that is
computed by the attention model using the sequence h of hidden states from the encoder. The
attention model ﬁrst compares the current hidden
state sj against each of the hidden states and assigns a scalar score: βi,j = exp(h⊤
i Wdsj) . These scores are then normalized across the hidden states to sum to 1, that is
i βi,j . The time-dependent context vector
is then a weighted-sum of the hidden states with
these attention weights: cj = P
Recurrent Neural Network Grammars
A recurrent neural network grammar is a probabilistic syntax-based
language model.
Unlike a usual recurrent language model , an
RNNG simultaneously models both tokens and
their tree-based composition.
This is done by
having a (output) buffer, stack and action history, each of which is implemented as a stack
LSTM . At each time
step, the action sLSTM predicts the next action
based on the (current) hidden states of the buffer,
stack and action sLSTM. That is,
p(at = a|a<t) ∝eW ⊤
a faction(hbuffer
where Wa is the vector of the action a. If the selected action is shift, the word at the beginning of
the buffer is moved to the stack. When the reduce action is selected, the top-two words in the
stack are reduced to build a partial tree. Additionally, the action may be one of many possible
non-terminal symbols, in which case the predicted
non-terminal symbol is pushed to the stack.
The hidden states of the buffer, stack and action
sLSTM are correspondingly updated by
= StackLSTM(hbuffer
, Vy(yt−1)),
= StackLSTM(hstack
top , rt),
= StackLSTM(haction
, Va(at−1)),
where Vy and Va are functions returning the target
word and action vectors. The input vector rt of the
stack sLSTM is computed recursively by
rt = tanh(Wr[rd; rp; Va(at)]),
where rd and rp are the corresponding vectors
of the parent and dependent phrases, respectively .
This process is iterated until a complete parse tree is built. Note that
the original paper of RNNG 
uses constituency trees, but we employ dependency trees in this paper. Both types of trees are
represented as a sequence of the three types of actions in a transition-based parsing model.
When the complete sentence is provided, the
buffer simply summarizes the shifted words.
When the RNNG is used as a generator, the buffer
further generates the next word when the selected
action is shift. The latter can be done by replacing
the buffer with a recurrent language model, which
is the idea on which our proposal is based.
Learning to Parse and Translate
Our main proposal in this paper is to hybridize the
decoder of the neural machine translation and the
RNNG. We continue from the earlier observation
that we can replace the buffer of RNNG to a recurrent language model that simultaneously summarizes the shifted words as well as generates future
words. We replace the RNNG’s buffer with the
neural translation model’s decoder in two steps.
Construction
First, we replace the hidden state
of the buffer hbuffer (in Eq. (5)) with the hidden
state of the decoder of the attention-based neural
machine translation from Eq. (3). As is clear from
those two equations, both the buffer sLSTM and
the translation decoder take as input the previous
hidden state (hbuffer
and sj−1, respectively) and
the previously decoded word (or the previously
shifted word in the case of the RNNG’s buffer),
and returns its summary state. The only difference
is that the translation decoder additionally considers the state ˜sj−1. Once the buffer of the RNNG
is replaced with the NMT decoder in our proposed
model, the NMT decoder is also under control of
the actions provided by the RNNG.2 Second, we
let the next word prediction of the translation decoder as a generator of RNNG. In other words,
the generator of RNNG will output a word, when
asked by the shift action, according to the conditional distribution deﬁned by the translation decoder in Eq. (1). Once the buffer sLSTM is replaced with the neural translation decoder, the action sLSTM naturally takes as input the translation
decoder’s hidden state when computing the action
conditional distribution in Eq. (4). We call this hybrid model NMT+RNNG.
2The j-th hidden state in Eq. (3) is calculated only when
the action (shift) is predicted by the RNNG. This is why our
proposed model can handle the sequences of words and actions which have different lengths.
Learning and Inference
After this integration,
our hybrid NMT+RNNG models the conditional
distribution over all possible pairs of translation and its parse given a source sentence, i.e.,
p(y, a|x).
Assuming the availability of parse
annotation in the target-side of a parallel corpus, we train the whole model jointly to maximize E(x,y,a)∼data [log p(y, a|x)]. In doing so, we
notice that there are two separate paths through
which the neural translation decoder receives error signal.
First, the decoder is updated in order to maximize the conditional probability of the
correct next word, which has already existed in
the original neural machine translation. Second,
the decoder is updated also to maximize the conditional probability of the correct parsing action,
which is a novel learning signal introduced by the
proposed hybridization. Furthermore, the second
learning signal affects the encoder as well, encouraging the whole neural translation model to be
aware of the syntactic structure of the target language. Later in the experiments, we show that this
additional learning signal is useful for translation,
even though we discard the RNNG (the stack and
action sLSTMs) in the inference time.
Knowledge Distillation for Parsing
A major challenge in training the proposed hybrid
model is that there is not a parallel corpus augmented with gold-standard target-side parse, and
vice versa. In other words, we must either parse
the target-side sentences of an existing parallel
corpus or translate sentences with existing goldstandard parses. As the target task of the proposed
model is translation, we start with a parallel corpus and annotate the target-side sentences. It is
however costly to manually annotate any corpus
of reasonable size .
We instead resort to noisy, but automated annotation using an existing parser. This approach
of automated annotation can be considered along
the line of recently proposed techniques of knowledge distillation and distant
supervision . In knowledge distillation, a teacher network is trained purely on a
training set with ground-truth annotations, and the
annotations predicted by this teacher are used to
train a student network, which is similar to our approach where the external parser could be thought
of as a teacher and the proposed hybrid network’s
RNNG as a student. On the other hand, what we
Voc. (src, tgt, act)
(33,867, 27,347, 82)
(33,820, 30,684, 80)
(32,442, 27,979, 82)
(23,509, 28,591, 80)
Table 1: Statistics of parallel corpora.
propose here is a special case of distant supervision in that the external parser provides noisy annotations to otherwise an unlabeled training set.
Speciﬁcally, we use SyntaxNet, released by Andor et al. , on a target sentence.3 We convert
a parse tree into a sequence of one of three transition actions (SHIFT, REDUCE-L, REDUCE-R).
We label each REDUCE action with a corresponding dependency label and treat it as a more ﬁnegrained action.
Experiments
Language Pairs and Corpora
We compare the proposed NMT+RNNG against
the baseline model on four different language
pairs–Jp-En, Cs-En, De-En and Ru-En. The basic statistics of the training data are presented in
Table 1. We mapped all the low-frequency words
to the unique symbol “UNK” and inserted a special symbol “EOS” at the end of both source and
target sentences.
We use the ASPEC corpus (“train1.txt”) from
the WAT’16 Jp-En translation task. We tokenize
each Japanese sentence with KyTea and preprocess according to the recommendations from WAT’16 . We use the
ﬁrst 100K sentence pairs of length shorter than 50
for training. The vocabulary is constructed with
all the unique tokens that appear at least twice in
the training corpus. We use “dev.txt” and “test.txt”
provided by WAT’16 respectively as development
and test sets.
Cs, De and Ru
We use News Commentary v8.
We removed noisy metacharacters and used the tokenizer from Moses to build a
vocabulary of each language using unique tokens
that appear at least 6, 6 and 5 times respectively for
Cs, Ru and De. The target-side (English) vocabulary was constructed with all the unique tokens
3When the target sentence is parsed as data preprocessing,
we use all the vocabularies in a corpus and do not cut off
any words. We use the plain SyntaxNet and do not train it
furthermore.
appearing more than three times in each corpus.
We also excluded the sentence pairs which include
empty lines in either a source sentence or a target
sentence. We only use sentence pairs of length 50
or less for training. We use “newstest2015” and
“newstest2016” as development and test sets respectively.
Models, Learning and Inference
In all our experiments, each recurrent network has
a single layer of LSTM units of 256 dimensions,
and the word vectors and the action vectors are
of 256 and 128 dimensions, respectively. To reduce computational overhead, we use BlackOut with 2000 negative samples and α =
0.4. When employing BlackOut, we shared the
negative samples of each target word in a sentence in training time , which is similar to the previous work . For the proposed NMT+RNNG, we
share the target word vectors between the decoder
(buffer) and the stack sLSTM.
Each weight is initialized from the uniform distribution [−0.1, 0.1].
The bias vectors and the
weights of the softmax and BlackOut are initialized to be zero. The forget gate biases of LSTMs
and Stack-LSTMs are initialized to 1 as recommended in J´ozefowicz et al. .
stochastic gradient descent with minibatches of
128 examples. The learning rate starts from 1.0,
and is halved each time the perplexity on the development set increases. We clip the norm of the
gradient with the threshold set to 3.0 (2.0 for the baseline models on Ru-
En and Cs-En to avoid NaN and Inf). When the
perplexity of development data increased in training time, we halved the learning rate of stochastic
gradient descent and reloaded the previous model.
The RNNG’s stack computes the vector of a dependency parse tree which consists of the generated target words by the buffer. Since the complete
parse tree has a “ROOT” node, the special token of
the end of a sentence (“EOS”) is considered as the
ROOT. We use beam search in the inference time,
with the beam width selected based on the development set performance.
It took about 15 minutes per epoch and about 20
minutes respectively for the baseline and the proposed model to train a full JP-EN parallel corpus
in our implementation.4
4We run all the experiments on multi-core CPUs 
to compute the statistical signiﬁcance. We use † to
mark those signiﬁcant cases with p < 0.005.
Jp-En (Dev)
w/o Buffer
w/o Action
Table 3: Effect of each component in RNNG.
Results and Analysis
In Table 2, we report the translation qualities of
the tested models on all the four language pairs.
We report both BLEU and
RIBES .
Except for De-
En, measured in BLEU, we observe the statistically signiﬁcant improvement by the proposed
NMT+RNNG over the baseline model. It is worthwhile to note that these signiﬁcant improvements
have been achieved without any additional parameters nor computational overhead in the inference
Since each component in RNNG may
be omitted, we ablate each component in the proposed NMT+RNNG to verify their necessity.5 As
shown in Table 3, we see that the best performance
could only be achieved when all the three components were present. Removing the stack had the
most adverse effect, which was found to be the
case for parsing as well by Kuncoro et al. .
The decoder part of our proposed model consists
of two components: the NMT decoder to generthreads on Intel(R) Xeon(R) CPU E5-2680 v2 @2.80GHz)
5 Since the buffer is the decoder, it is not possible to completely remove it. Instead we simply remove the dependency
of the action distribution on it.
Figure 1: An example of translation and its dependency relations obtained by our proposed model.
ate a translated sentence and the RNNG decoder
to predict its parsing actions. The proposed model
can therefore output a dependency structure along
with a translated sentence.
Figure 1 shows an
example of JP-EN translation in the development
dataset and its dependency parse tree obtained by
the proposed model. The special symbol (“EOS”)
is treated as the root node (“ROOT”) of the parsed
The translated sentence was generated by
using beam search, which is the same setting of
NMT+RNNG shown in Table 3. The parsing actions were obtained by greedy search.
The resulting dependency structure is mostly correct but
contains a few errors; for example, dependency relation between “The” and “ transition” should not
be “pobj”.
Conclusion
We propose a hybrid model, to which we refer
as NMT+RNNG, that combines the decoder of an
attention-based neural translation model with the
RNNG. This model learns to parse and translate simultaneously, and training it encourages both the
encoder and decoder to better incorporate linguistic priors. Our experiments conﬁrmed its effectiveness on four language pairs ({JP, Cs, De, Ru}-
En). The RNNG can in principle be trained without ground-truth parses, and this would eliminate
the need of external parsers completely. We leave
the investigation into this possibility for future research.
Acknowledgments
We thank Yuchen Qiao and Kenjiro Taura for their
help to speed up the implementations of training
and also Kazuma Hashimoto for his valuable comments and discussions. This work was supported
by JST CREST Grant Number JPMJCR1513 and
JSPS KAKENHI Grant Number 15J12597 and
KC thanks support by eBay, Facebook, Google and NVIDIA.