Imagic: Text-Based Real Image Editing with Diffusion Models
Bahjat Kawar˚ 1,2
Shiran Zada˚ 1
Oran Lang1
Huiwen Chang1
Tali Dekel1,3
Inbar Mosseri1
Michal Irani1,3
1Google Research
3Weizmann Institute of Science
Input Image
Target Text:
Edited Image
“A bird spreading
Target Text:
“A sitting dog”
Input Image
Edited Image
“A person giving
the thumbs up”
“Two kissing
Input Image
Edited Image
“A goat jumping
over a cat”
“A children’s drawing
of a waterfall”
Figure 1. Imagic – Editing a single real image. Our method can perform various text-based semantic edits on a single real input image,
including highly complex non-rigid changes such as posture changes and editing multiple objects. Here, we show pairs of 1024ˆ1024
input (real) images, and edited outputs with their respective target texts.
Text-conditioned image editing has recently attracted
considerable interest.
However, most methods are currently limited to one of the following: speciﬁc editing types
(e.g., object overlay, style transfer), synthetically generated
images, or requiring multiple input images of a common
In this paper we demonstrate, for the very ﬁrst
time, the ability to apply complex (e.g., non-rigid) textbased semantic edits to a single real image.
For example, we can change the posture and composition of one
or multiple objects inside an image, while preserving its
original characteristics. Our method can make a standing dog sit down, cause a bird to spread its wings, etc.
– each within its single high-resolution user-provided natural image.
Contrary to previous work, our proposed
method requires only a single input image and a target
text (the desired edit).
It operates on real images, and
˚ Equal contribution.
The ﬁrst author performed this work as an intern at Google Research.
Project page: 
does not require any additional inputs (such as image
masks or additional views of the object).
Our method,
called Imagic, leverages a pre-trained text-to-image diffusion model for this task.
It produces a text embedding
that aligns with both the input image and the target text,
while ﬁne-tuning the diffusion model to capture the imagespeciﬁc appearance. We demonstrate the quality and versatility of Imagic on numerous inputs from various domains,
showcasing a plethora of high quality complex semantic
image edits, all within a single uniﬁed framework. To better
assess performance, we introduce TEdBench, a highly challenging image editing benchmark. We conduct a user study,
whose ﬁndings show that human raters prefer Imagic to previous leading editing methods on TEdBench.
1. Introduction
Applying non-trivial semantic edits to real photos has
long been an interesting task in image processing .
It has attracted considerable interest in recent years, enabled by the considerable advancements of deep learningbased systems. Image editing becomes especially impresarXiv:2210.09276v3 [cs.CV] 20 Mar 2023
Input Image
“A sitting dog”
Edited Images
“A jumping dog”
“A dog playing
with a toy”
“A dog lying down”
“A person in a greeting
pose to Namaste hands”
“A person with
crossed arms”
“A person giving
the thumbs up”
holding a cup”
“A cat wearing a
“A cat wearing an
“A cat wearing a
jean jacket”
“A cat wearing a
“A horse with its
head down”
“A horse with
“A brown horse in
a grass ﬁeld”
“A pistachio cake”
“A chocolate cake”
“A strawberry cake”
“A wedding cake”
“A jumping dog
holding a frisbee”
“A person making
a heart sign”
“A drawing of a cat”
“A cartoon of a
“A slice of cake”
Figure 2. Different target texts applied to the same image. Imagic edits the same image differently depending on the input text.
sive when the desired edit is described by a simple natural language text prompt, since this aligns well with human
communication. Many methods were developed for textbased image editing, showing promising results and continually improving . However, the current leading methods suffer from, to varying degrees, several drawbacks: (i) they are limited to a speciﬁc set of edits such as
painting over the image, adding an object, or transferring
style ; (ii) they can operate only on images from a
speciﬁc domain or synthetically generated images ;
or (iii) they require auxiliary inputs in addition to the input image, such as image masks indicating the desired edit
location, multiple images of the same subject, or a text describing the original image .
In this paper, we propose a semantic image editing
method that mitigates all the above problems. Given only an
input image to be edited and a single text prompt describing
the target edit, our method can perform sophisticated nonrigid edits on real high-resolution images. The resulting image outputs align well with the target text, while preserving
the overall background, structure, and composition of the
original image. For example, we can make two parrots kiss
(A) Text Embedding Optimization
(B) Model Fine-Tuning
(C) Interpolation & Generation
"A bird spreading wings."
Target Emb etgt
Optimized Emb eopt
Pre-Trained
Diffusion Model
Reconstruction Loss
Pre-Trained
Diffusion Model
Reconstruction Loss
Fine-Tuned
Diffusion Process
interpolate
Figure 3. Schematic description of Imagic. Given a real image and a target text prompt: (A) We encode the target text and get the initial
text embedding etgt, then optimize it to reconstruct the input image, obtaining eopt; (B) We then ﬁne-tune the generative model to improve
ﬁdelity to the input image while ﬁxing eopt; (C) Finally, we interpolate eopt with etgt to generate the ﬁnal editing result.
or make a person give the thumbs up, as demonstrated in
Figure 1. Our method, which we call Imagic, provides the
ﬁrst demonstration of text-based semantic editing that applies such sophisticated manipulations to a single real highresolution image, including editing multiple objects. In addition, Imagic can also perform a wide variety of edits, including style changes, color changes, and object additions.
To achieve this feat, we take advantage of the recent success of text-to-image diffusion models . Diffusion models are powerful state-of-the-art generative models,
capable of high quality image synthesis . When conditioned on natural language text prompts, they are able to
generate images that align well with the requested text. We
adapt them in our work to edit real images instead of synthesizing new ones. We do so in a simple 3-step process, as
depicted in Figure 3: We ﬁrst optimize a text embedding so
that it results in images similar to the input image. Then, we
ﬁne-tune the pre-trained generative diffusion model (conditioned on the optimized embedding) to better reconstruct
the input image. Finally, we linearly interpolate between
the target text embedding and the optimized one, resulting
in a representation that combines both the input image and
the target text. This representation is then passed to the generative diffusion process with the ﬁne-tuned model, which
outputs our ﬁnal edited image.
We conduct several experiments and apply our method
on numerous images from various domains. Our method
outputs high quality images that both resemble the input
image to a high degree, and align well with the target
text. These results showcase the generality, versatility, and
quality of Imagic.
We additionally conduct an ablation
study, highlighting the effect of each element of our method.
When compared to recent approaches suggested in the literature, Imagic exhibits signiﬁcantly better editing quality and faithfulness to the original image, especially when
tasked with sophisticated non-rigid edits. This is further
supported by a human perceptual evaluation study, where
raters strongly prefer Imagic over other methods on a novel
benchmark called TEdBench – Textual Editing Benchmark.
We summarize our main contributions as follows:
1. We present Imagic, the ﬁrst text-based semantic image
editing technique that allows for complex non-rigid edits
on a single real input image, while preserving its overall
structure and composition.
2. We demonstrate a semantically meaningful linear interpolation between two text embedding sequences, uncovering strong compositional capabilities of text-to-image
diffusion models.
3. We introduce TEdBench – a novel and challenging complex image editing benchmark, which enables comparisons of different text-based image editing methods.
2. Related Work
Following recent advancements in image synthesis quality , many works utilized the latent space of pretrained generative adversarial networks (GANs) to perform
a variety of image manipulations . Multiple techniques for applying such manipulations on real images were suggested, including optimization-based methods , encoder-based methods , and methods adjusting the model per input . In addition
to GAN-based methods, some techniques utilize other deep
learning-based systems for image editing .
More recently, diffusion models were utilized for similar
image manipulation tasks, showcasing remarkable results.
SDEdit adds intermediate noise to an image (possibly
augmented by user-provided brush strokes), then denoises
it using a diffusion process conditioned on the desired edit,
which is limited to global edits. DDIB encodes an input
image using DDIM inversion with a source class (or text),
and decodes it back conditioned on the target class (or text)
to obtain an edited version.
DiffusionCLIP utilizes
language-vision model gradients, DDIM inversion , and
model ﬁne-tuning to edit images using a domain-speciﬁc
diffusion model. It was also suggested to edit images by
Input Image
Edited images using different random seeds
“A photo of a bird spreading wings”
“A children’s drawing of a forest”
Figure 4. Multiple edit options. Imagic utilizes a probabilistic model, enabling it to generate multiple options with different random seeds.
synthesizing data in user-provided masks, while keeping the
rest of the image intact . Liu et al. guide a
diffusion process with a text and an image, synthesising images similar to the given one, and aligned with the given
text. Hertz et al. alter a text-to-image diffusion process by manipulating cross-attention layers, providing more
ﬁne-grained control over generated images, and can edit
real images in cases where DDIM inversion provides meaningful attention maps. Textual Inversion and Dream-
Booth synthesize novel views of a given subject given
3–5 images of the subject and a target text (rather than edit a
single image), with DreamBooth requiring additional generated images for ﬁne-tuning the models. In this work, we
provide the ﬁrst text-based semantic image editing tool that
operates on a single real image, maintains high ﬁdelity to it,
and applies non-rigid edits given a single free-form natural
language text prompt.
3. Imagic: Diffusion-Based Real Image Editing
3.1. Preliminaries
Diffusion models are a family of generative models that has recently gained traction, as they advanced the state-of-the-art in image generation , and have been deployed in various downstream applications such as image restoration , adversarial puriﬁcation , image compression , image classiﬁcation , and others .
The core premise of these models is to initialize with a
randomly sampled noise image xT „ Np0, Iq, then iteratively reﬁne it in a controlled fashion, until it is synthesized
into a photorealistic image x0. Each intermediate sample
xt (for t P t0, . . . , Tu) satisﬁes
xt “ ?αtx0 `
with 0 “ αT ă αT ´1 ă ¨ ¨ ¨ ă α1 ă α0 “ 1 being hyperparameters of the diffusion schedule, and ϵt „ Np0, Iq.
Each reﬁnement step consists of an application of a neural
network fθpxt, tq on the current sample xt, followed by a
random Gaussian noise perturbation, obtaining xt´1. The
network is trained for a simple denoising objective, aiming
for fθpxt, tq « ϵt . This leads to a learned image
distribution with high ﬁdelity to the target distribution, enabling stellar generative performance.
This method can be generalized for learning conditional
distributions – by conditioning the denoising network on an
auxiliary input y, the network fθpxt, t, yq and its resulting
diffusion process can faithfully sample from a data distribution conditioned on y. The conditioning input y can be a
low-resolution version of the desired image or a class
label . Furthermore, y can also be on a text sequence
describing the desired image . By incorporating knowledge from large language models (LLMs) 
or hybrid vision-language models , these text-to-image
diffusion models have unlocked a new capability – users can
generate realistic high-resolution images using only a text
prompt describing the desired scene. In all these methods, a
low-resolution image is ﬁrst synthesized using a generative
diffusion process, and then it is transformed into a highresolution one using additional auxiliary models.
3.2. Our Method
Given an input image x and a target text which describes
the desired edit, our goal is to edit the image in a way that
satisﬁes the given text, while preserving a maximal amount
of detail from x (e.g., small details in the background and
the identity of the object within the image). To achieve this
feat, we utilize the text embedding layer of the diffusion
model to perform semantic manipulations. Similar to GANbased approaches , we begin by ﬁnding meaningful representation which, when fed through the generative
process, yields images similar to the input image. We then
ﬁne-tune the generative model to better reconstruct the input image and ﬁnally manipulate the latent representation
to obtain the edit result.
Increasing η
Input Image
Edited Image
Target Text: “A blue car”
Target Text: “A bar stool”
Figure 5. Smooth interpolation.
We can smoothly interpolate between the optimized text embedding and the target text embedding,
resulting in a gradual editing of the input image toward the required text as η increases (See animated GIFs in supplementary material).
More formally, as depicted in Figure 3, our method consists of 3 stages: (i) we optimize the text embedding to ﬁnd
one that best matches the given image in the vicinity of the
target text embedding; (ii) we ﬁne-tune the diffusion models
to better match the given image; and (iii) we linearly interpolate between the optimized embedding and the target text
embedding, in order to ﬁnd a point that achieves both ﬁdelity to the input image and target text alignment. We now
turn to describe each step in more detail.
Text embedding optimization
The target text is ﬁrst
passed through a text encoder , which outputs its corresponding text embedding etgt P RT ˆd, where T is the
number of tokens in the given target text, and d is the token embedding dimension. We then freeze the parameters
of the generative diffusion model fθ, and optimize the target text embedding etgt using the denoising diffusion objective :
Lpx, e, θq “ Et,ϵ
}ϵ ´ fθpxt, t, eq}2
where t„Uniformr1, Ts, xt is a noisy version of x (the input image) obtained using ϵ„Np0, Iq and Equation 1, and
θ are the pre-trained diffusion model weights. This results
in a text embedding that matches our input image as closely
as possible. We run this process for relatively few steps, in
order to remain close to the initial target text embedding,
obtaining eopt. This proximity enables meaningful linear
interpolation in the embedding space, which does not exhibit linear behavior for distant embeddings.
Model ﬁne-tuning
Note that the obtained optimized embedding eopt does not necessarily lead to the input image x
exactly when passed through the generative diffusion process, as our optimization runs for a small number of steps
(see top left image in Figure 7). Therefore, in the second
stage of our method, we close this gap by optimizing the
model parameters θ using the same loss function presented
in Equation 2, while freezing the optimized embedding.
This process shifts the model to ﬁt the input image x at the
point eopt. In parallel, we ﬁne-tune any auxiliary diffusion
models present in the underlying generative method, such
as super-resolution models.
We ﬁne-tune them with the
same reconstruction loss, but conditioned on etgt, as they
will operate on an edited image. The optimization of these
auxiliary models ensures the preservation of high-frequency
details from x that are not present in the base resolution.
Empirically, we found that at inference time, inputting etgt
to the auxiliary models performs better than using eopt.
Text embedding interpolation
Since the generative diffusion model was trained to fully recreate the input image
x at the optimized embedding eopt, we use it to apply the
desired edit by advancing in the direction of the target text
embedding etgt. More formally, our third stage is a simple linear interpolation between etgt and eopt. For a given
hyperparameter η P r0, 1s, we obtain
¯e “ η ¨ etgt ` p1 ´ ηq ¨ eopt,
which is the embedding that represents the desired edited
image. We then apply the base generative diffusion process
using the ﬁne-tuned model, conditioned on ¯e. This results in
a low-resolution edited image, which is then super-resolved
using the ﬁne-tuned auxiliary models, conditioned on the
target text. This generative process outputs our ﬁnal highresolution edited image ¯x.
3.3. Implementation Details
Our framework is general and can be combined with
different generative models. We demonstrate it using two
different state-of-the-art text-to-image generative diffusion
models: Imagen and Stable Diffusion .
Imagen consists of 3 separate text-conditioned diffusion models: (i) a generative diffusion model for 64ˆ64pixel images; (ii) a super-resolution (SR) diffusion model
turning 64ˆ64-pixel images into 256ˆ256 ones;
(iii) another SR model transforming 256ˆ256-pixel images
Imagic (Ours)
“A photo of a tree
with autumn leaves”
“A photo of a
sitting dog”
“A goat jumping
over a cat”
“A horse raising
Method comparison.
We compare SDEdit ,
DDIB , and Text2LIVE to our method. Imagic successfully applies the desired edit, while preserving the original image
details well.
into the 1024ˆ1024 resolution. By cascading these 3 models and using classiﬁer-free guidance , Imagen constitutes a powerful text-guided image generation scheme.
We optimize the text embedding using the 64ˆ64 diffusion model and the Adam optimizer for 100 steps
and a ﬁxed learning rate of 1e´3. We then ﬁne-tune the
64ˆ64 diffusion model by continuing Imagen’s training
for 1500 steps for our input image, conditioned on the
optimized embedding.
In parallel, we also ﬁne-tune the
64ˆ64 Ñ 256ˆ256 SR diffusion model using the target
text embedding and the original image for 1500 steps, in order to capture high-frequency details from the original image. We ﬁnd that ﬁne-tuning the 256ˆ256 Ñ 1024ˆ1024
model adds little to no effect to the results, therefore we opt
to use its pre-trained version conditioned on the target text.
This entire optimization process takes around 8 minutes per
image on two TPUv4 chips.
Afterwards, we interpolate the text embeddings according to Equation 3. Because of the ﬁne-tuning process, using
η“0 will generate the original image, and as η increases,
the image will start to align with the target text. To maintain both image ﬁdelity and target text alignment, we choose
an intermediate η, usually residing between 0.6 and 0.8 (see
Figure 9). We then generate with Imagen with its provided hyperparameters. We ﬁnd that using the DDIM 
sampling scheme generally provides slightly improved results over the more stochastic DDPM scheme.
In addition to Imagen, we also implement our method
with the publicly available Stable Diffusion model (based
on Latent Diffusion Models ). This model applies the
diffusion process in the latent space (of size 4ˆ64ˆ64) of a
pre-trained autoencoder, working with 512ˆ512-pixel images. We apply our method in the latent space as well. We
optimize the text embedding for 1000 steps with a learning
rate of 2e´3 using Adam . Then, we ﬁne-tune the diffusion model for 1500 steps with a learning rate of 5e´7.
This process takes 7 minutes on a single Tesla A100 GPU.
4. Experiments
4.1. Qualitative Evaluation
We applied our method on a multitude of real images
from various domains, with simple text prompts describing different editing categories such as: style, appearance,
color, posture, and composition. We collect high-resolution
free-to-use images from Unsplash and Pixabay. After optimization, we generate each edit with 8 random seeds and
choose the best result. Imagic is able to apply various editing categories on general input images and texts, as we
show in Figure 1 and the supplementary material. We experiment with different text prompts for the same image in Figure 2, showing the versatility of Imagic. Since the underlying generative diffusion model that we utilize is probabilistic, our method can generate different results for a single
image-text pair. We show multiple options for the same edit
using different random seeds in Figure 4, slightly tweaking η for each seed. This stochasticity allows the user to
choose among these different options, as natural language
text prompts can generally be ambiguous and imprecise.
While we use Imagen in most of our experiments,
Imagic is agnostic to the generative model choice. Thus,
we also implement Imagic with Stable Diffusion . In
Figure 5 (and in the supplementary material) we show that
Imagic successfully performs complex non-rigid edits also
using Stable Diffusion while preserving the image-speciﬁc
appearance. Furthermore, Imagic (using Stable Diffusion)
exhibits smooth semantic interpolation properties as η is
changed. We hypothesize that this smoothness property is a
byproduct of the diffusion process taking place in a semantic latent space, rather than in the image pixel space.
4.2. Comparisons
We compare Imagic to the current leading generalpurpose techniques that operate on a single input real-world
image, and edit it based on a text prompt.
Namely, we
compare our method to Text2LIVE , DDIB , and
SDEdit . We use Text2LIVE’s default provided hyperparameters. We feed it with a text description of the tar-
Input Image:
, Target Text: “A photo of a pistachio cake”
Figure 7. Embedding interpolation. Varying η with the same seed, using the pre-trained (top) and ﬁne-tuned (bottom) models.
User study results.
Preference rates (with 95%
conﬁdence intervals) for image editing quality of Imagic over
SDEdit , DDIB , and Text2LIVE .
get object (e.g., “dog”) and one of the desired edit (e.g.,
“sitting dog”). For SDEdit and DDIB, we apply their proposed technique with the same Imagen model and target text prompt that we use. We keep the diffusion hyperparameters from Imagen, and choose the intermediate diffusion timestep for SDEdit independently for each image
to achieve the best target text alignment without drastically
changing the image contents. For DDIB, we provide an additional source text.
Figure 6 shows editing results of different methods. For
SDEdit and Imagic, we sample 8 images using different random seeds and display the result with the best alignment
to both the target text and the input image. As can be observed, our method maintains high ﬁdelity to the input image while aptly performing the desired edits. When tasked
with a complex non-rigid edit such as making a dog sit,
our method signiﬁcantly outperforms previous techniques.
Imagic constitutes the ﬁrst demonstration of such sophisticated text-based edits applied on a single real-world image.
We verify this claim through a user study in subsection 4.3.
4.3. TEdBench and User Study
Text-based image editing methods are a relatively recent
development, and Imagic is the ﬁrst to apply complex nonrigid edits. As such, no standard benchmark exists for evaluating non-rigid text-based image editing. We introduce
TEdBench (Textual Editing Benchmark), a novel collection
of 100 pairs of input images and target texts describing a desired complex non-rigid edit. We hope that future research
will beneﬁt from TEdBench as a standardized evaluation set
for this task.
We quantitatively evaluate Imagic’s performance via an
Image Fidelity
Text Alignment
Editability–ﬁdelity
CLIP score (target
text alignment) and 1´LPIPS
(input image ﬁdelity) as functions of η, averaged over 150
inputs. Edited images tend to
match both the input image and
text in the highlighted area.
extensive human perceptual evaluation study on TEdBench,
performed using Amazon Mechanical Turk.
Participants
were shown an input image and a target text, and were
asked to choose the better editing result from one of two options, using the standard practice of Two-Alternative Forced
Choice (2AFC) . The options to choose from were
our result and a baseline result from one of: SDEdit ,
DDIB , or Text2LIVE . In total, we collected 9213
answers, whose results are summarized in Figure 8. As can
be seen, evaluators exhibit a strong preference towards our
method, with a preference rate of more than 70% across all
considered baselines. See supplementary material for more
details about the user study and method implementations.
4.4. Ablation Study
Fine-tuning and optimization
We generate edited images for different η values using the pre-trained 64 ˆ 64
diffusion model and our ﬁne-tuned one, in order to gauge
the effect of ﬁne-tuning on the output quality. We use the
same optimized embedding and random seed, and qualitatively evaluate the results in Figure 7. Without ﬁne-tuning,
the scheme does not fully reconstruct the original image
at η “ 0, and fails to retain the image’s details as η increases. In contrast, ﬁne-tuning imposes details from the
input image beyond just the optimized embedding, allowing
our scheme to retain these details for intermediate values of
η, thereby enabling semantically meaningful linear interpolation. Thus, we conclude that model ﬁne-tuning is essential
for our method’s success. Furthermore, we experiment with
the number of text embedding optimization steps in the supplementary material. Our ﬁndings suggest that optimizing
the text embedding with a smaller number of steps limits
our editing capabilities, while optimizing for more than 100
steps yields little to no added value.
Interpolation intensity
As can be observed in Figure 7,
ﬁne-tuning increases the η value at which the model strays
from reconstructing the input image. While the optimal η
value may vary per input (as different edits require different intensities), we attempt to identify the region in which
the edit is best applied. To that end, we apply our editing
scheme with different η values, and calculate the outputs’
CLIP score w.r.t. the target text, and their LPIPS
score w.r.t. the input image subtracted from 1. A higher
CLIP score indicates better output alignment with the target
text, and a higher 1´LPIPS indicates higher ﬁdelity to the
input image. We repeat this process for 150 image-text inputs, and show the average results in Figure 9. We observe
that for η values smaller than 0.4, outputs are almost identical to the input images. For η P r0.6, 0.8s, the images begin
to change (according to LPIPS), and align better with the
text (as the CLIP score rises). Therefore, we identify this
area as the most probable for obtaining satisfactory results.
Note that while they provide a good sense of text or image
alignment on average, CLIP score and LPIPS are imprecise
measures that rely on neural network backbones, and their
values noticeably differ for each different input image-text
pair. As such, they are not suited for reliably choosing η
for each input in an automatic way, nor can they faithfully
assess an editing method’s performance.
4.5. Limitations
We identify two main failure cases of our method: In
some cases, the desired edit is applied very subtly (if at all),
therefore not aligning well with the target text. In other
cases, the edit is applied well, but it affects extrinsic image
details such as zoom or camera angle. We show examples
of these two failure cases in the ﬁrst and second row of Figure 10, respectively. When the edit is not applied strongly
enough, increasing η usually achieves the desired result, but
it sometimes leads to a signiﬁcant loss of original image details (for all tested random seeds) in a handful of cases. As
for zoom and camera angle changes, these usually occur before the desired edit takes place, as we progress from a low
η value to a large one, which makes circumventing them dif-
ﬁcult. We demonstrate this in the supplementary material,
and include additional failure cases in TEdBench as well.
These limitations can possibly be mitigated by optimizing the text embedding or the diffusion model differently,
or by incorporating cross-attention control akin to Hertz et
al. . We leave those options for future work. Also, since
our method relies on a pre-trained text-to-image diffusion
model, it inherits the model’s generative limitations and biases. Therefore, unwanted artifacts are produced when the
desired edit involves generating failure cases of the underlying model. For instance, Imagen is known to show substandard generative performance on human faces . Additionally, the optimization required by Imagic (and other
Input Image
Target Text:
Edited Image
“A photo of a
trafﬁc jam”
Target Text:
“A photo of
a race car”
Input Image
Edited Image
“A dog lying down”
“Pizza with
pepperoni”
Figure 10. Failure cases. Insufﬁcient consistency with the target
text (top), or changes in camera viewing angle (bottom).
editing methods ) is slow, and may hinder their direct
deployment in user-facing applications.
5. Conclusions and Future Work
We propose a novel image editing method called Imagic.
Our method accepts a single image and a simple text prompt
describing the desired edit, and aims to apply this edit while
preserving a maximal amount of details from the image.
To that end, we utilize a pre-trained text-to-image diffusion
model and use it to ﬁnd a text embedding that represents
the input image. Then, we ﬁne-tune the diffusion model to
ﬁt the image better, and ﬁnally we linearly interpolate between the embedding representing the image and the target
text embedding, obtaining a semantically meaningful mixture of them. This enables our scheme to provide edited images using the interpolated embedding. Contrary to other
editing methods, our approach can produce sophisticated
non-rigid edits that may alter the pose, geometry, and/or
composition of objects within the image as requested, in
addition to simpler edits such as style or color. It requires
the user to provide only a single image and a simple target
text prompt, without the need for additional auxiliary inputs
such as image masks.
Our future work may focus on further improving the
method’s ﬁdelity to the input image and identity preservation, as well as its sensitivity to random seeds and to the
interpolation parameter η. Another intriguing research direction would be the development of an automated method
for choosing η for each requested edit.
Societal Impact
Our method aims to enable complex
editing of real world images using textual descriptions of
the target edit. As such, it is prone to societal biases of the
underlying text-based generative models, albeit to a lesser
extent than purely generative methods since we rely mostly
on the input image for editing. However, as with other approaches that use generative models for image editing, such
techniques might be used by malicious parties for synthesizing fake imagery to mislead viewers. To mitigate this,
further research on the identiﬁcation of synthetically edited
or generated content is needed.