DeepInf: Social Influence Prediction with Deep Learning
Jiezhong Qiu†, Jian Tang♯♮, Hao Ma‡, Yuxiao Dong‡, Kuansan Wang‡, and Jie Tang†∗
†Department of Computer Science and Technology, Tsinghua University
‡Microsoft Research, Redmond
♯HEC Montreal, Canada
♮Montreal Institute for Learning Algorithms, Canada
 , 
{haoma,yuxdong,kuansanw}@microsoft.com, 
Social and information networking activities such as on Facebook,
Twitter, WeChat, and Weibo have become an indispensable part of
our everyday life, where we can easily access friends’ behaviors
and are in turn influenced by them. Consequently, an effective
social influence prediction for each user is critical for a variety of
applications such as online recommendation and advertising.
Conventional social influence prediction approaches typically
design various hand-crafted rules to extract user- and networkspecific features. However, their effectiveness heavily relies on the
knowledge of domain experts. As a result, it is usually difficult
to generalize them into different domains. Inspired by the recent
success of deep neural networks in a wide range of computing applications, we design an end-to-end framework, DeepInf1, to learn
users’ latent feature representation for predicting social influence.
In general, DeepInf takes a user’s local network as the input to a
graph neural network for learning her latent social representation.
We design strategies to incorporate both network structures and
user-specific features into convolutional neural and attention networks. Extensive experiments on Open Academic Graph, Twitter,
Weibo, and Digg, representing different types of social and information networks, demonstrate that the proposed end-to-end model,
DeepInf, significantly outperforms traditional feature engineeringbased approaches, suggesting the effectiveness of representation
learning for social applications.
CCS CONCEPTS
• Information systems →Data mining; Social networks; •
Applied computing →Sociology;
∗Jie Tang is the corresponding author.
1Code is available at 
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from .
KDD ’18, August 19–23, 2018, London, United Kingdom
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5552-0/18/08...$15.00
 
Representation Learning; Network Embedding; Graph Convolution;
Graph Attention; Social Influence; Social Networks
ACM Reference Format:
Jiezhong Qiu, Jian Tang, Hao Ma, Yuxiao Dong, Kuansan Wang, and Jie Tang.
2018. DeepInf: Social Influence Prediction with Deep Learning . In KDD ’18:
The 24th ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining, August 19–23, 2018, London, United Kingdom. ACM, New York,
NY, USA, 10 pages. 
INTRODUCTION
Social influence is everywhere around us, not only in our daily
physical life but also on the virtual Web space. The term social influence typically refers to the phenomenon that a person’s emotions,
opinions, or behaviors are affected by others. With the global penetration of online and mobile social platforms, people have witnessed
the impact of social influence in every field, such as presidential elections , advertising , and innovation adoption . To date,
there is little doubt that social influence has become a prevalent,
yet complex force that drives our social decisions, making a clear
need for methodologies to characterize, understand, and quantify
the underlying mechanisms and dynamics of social influence.
Indeed, extensive work has been done on social influence prediction in the literature . For example, Matsubara et al.
 studied the dynamics of social influence by carefully designing differential equations extended from the classic ‘Susceptible-
Infected’ (SI) model; Most recently, Li et al. proposed an end-toend predictor for inferring cascade size by incorporating recurrent
neural network (RNN) and representation learning. All these approaches mainly aim to predict the global or aggregated patterns
of social influence such as the cascade size within a time-frame.
However, in many online applications such as advertising and recommendation, it is critical to effectively predict the social influence
for each individual, i.e., user-level social influence prediction.
In this paper, we focus on the prediction of user-level social
influence. We aim to predict the action status of a user given the
action statuses of her near neighbors and her local structural information. For example, in Figure 1, for the central user v, if some of
her friends (black circles) bought a product, will she buy the same
product in the future? The problem mentioned above is prevalent
in real-world applications whereas its complexity and non-linearity
have frequently been observed, such as the “S-shaped” curve in 
 
Figure 1: A motivating example of social influence locality
prediction. The goal is to predict v’s action status, given 1)
the observed action statuses (black and gray circles are used
to indicate “active” and “inactive”, respectively) of her near
neighbors and 2) the local network she is embedded in.
and the celebrated “structural diversity” in . The above observations inspire a lot of user-level influence prediction models, most
of which consider complicated hand-crafted features,
which require extensive knowledge of specific domains and are
usually difficult to generalize to different domains.
Inspired by the recent success of neural networks in representation learning, we design an end-to-end approach to discover hidden
and predictive signals in social influence automatically. By architecting network embedding , graph convolution , and graph
attention mechanism into a unified framework, we expect that
the end-to-end model can achieve better performance than conventional methods with feature engineering. In specific, we propose
a deep learning based framework, DeepInf, to represent both influence dynamics and network structures into a latent space. To
predict the action status of a user v, we first sample her local neighbors through random walks with restart. After obtaining a local
network as shown in Figure 1, we leverage both graph convolution
and attention techniques to learn latent predictive signals.
We demonstrate the effectiveness and efficiency of our proposed
framework on four social and information networks from different
domains—Open Academic Graph (OAG), Digg, Twitter, and Weibo.
We compare DeepInf with several conventional methods such as
linear models with hand-crafted features as well as the state-ofthe-art graph classification model . Experimental results suggest
that the DeepInf model can significantly improve the prediction
performance, demonstrating the promise of representation learning
for social and information network mining tasks.
Organization The rest of this paper is organized as follows: Section 2 formulates social influence prediction problem. Section 3
introduces the proposed framework in detail. In Section 4 and 5, we
conduct extensive experiments and case studies. Finally, Section 6
summarizes related work and Section 7 concludes this work.
PROBLEM FORMULATION
In this section, we introduce necessary definitions and then formulate the problem of predicting social influence.
Definition 2.1. r-neighbors and r-ego network Let G = (V, E)
be a static social network, where V denotes the set of users and
E ⊆V × V denotes the set of relationships2. For a user v, its rneighbors are defined as Γrv = {u : d(u,v) ≤r} where d(u,v) is the
shortest path distance (in terms of the number of hops) between
u and v in the network G. The r-ego network of user v is the subnetwork induced by Γrv , denoted by Grv.
Definition 2.2. Social Action Users in social networks perform
social actions, such as retweet. At each timestamp t, we observe a
binary action status of user u, stu ∈{0, 1}, where stu = 1 indicates
user u has performed this action before or on the timestamp t, and
stu = 0 indicates that the user has not performed this action yet.
Such an action log can be available from many social networks, e.g.,
the “retweet” action in Twitter and the citation action in academic
social networks.
Given the above definitions, we introduce social influence locality, which amounts to a kind of closed world assumption: users’
social decisions and actions are influenced only by their near neighbors within the network, while external sources are assumed to be
not present.
Problem 1. Social Influence Locality Social influence locality models the probability ofv’s action status conditioned on her r-ego
network Grv and the action states of her r-neighbors. More formally,
given Grv and Stv = {stu : u ∈Γrv \ {v}}, social influence locality aims
to quantify the activation probability of v after a given time interval
Practically, suppose we have N instances, each instance is a
3-tuple (v,a,t), where v is a user, a is a social action and t is a
timestamp. For such a 3-tuple (v,a,t), we also know v’s r-ego
network—Grv, the action statuses of v’s r-neighbors—Stv, and v’s
future action status at t + ∆t, i.e., st+∆t
. We then formulate social
influence prediction as a binary graph classification problem which
can be solved by minimizing the following negative log likelihood
objective w.r.t model parameters Θ:
Especially, in this work, we assume ∆t is sufficiently large, that is,
we want to predict the action status of the ego user v at the end of
our observation window.
MODEL FRAMEWORK
In this section, we formally propose DeepInf, a deep learning based
model, to parameterize the probability in Eq. 1 and automatically
detect the mechanisms and dynamics of social influence. The framework firstly samples a fixed-size sub-network as the proxy for each
r-ego network (see Section 3.1). The sampled sub-networks are
then fed into a deep neural network with mini-batch learning (see
Section 3.2). Finally, the model output is compared with ground
truth to minimize the negative log-likelihood loss.
Sampling Near Neighbors
Given a user v, a straightforward way to extract her r-ego network
Grv is to perform Breadth-First-Search (BFS) starting from user v.
2In this work, we consider undirected relationships.
action status
mini-batch
Normalization
Figure 2: Model Framework of DeepInf. (a) Raw input which consists of a mini-batch of B instances; Each instance is a subnetwork comprised of n users who are sampled using random walk with restart as described in Section 3.1. In this example,
we keep our eyes on ego user v (marked as blue) and one of her active neighbor u (marked as orange). (b) An embedding
layer which maps each user to her D-dimensional representation; (c) An Instance Normalization layer . For each instance,
this layer normalizes users’ embedding xu’s according to Eq. 3. The output embedding yu’s have zero mean and unit variance
within each instance. (d) The formal input layer which concatenates together network embedding, two dummy features (one
indicates whether the user is active, the other indicates whether the user is the ego), and other customized vertex features (see
Table 2 for example). (e) A GCN or GAT layer. avv and avu indicate the attention coefficients along self-loop (v,v) and edge
(v,u), respectively; The value of these attention coefficients can be chosen between Eq. 5 and Eq. 7 according to the choice
between GCN and GAT. (f) and (g) Compare model output and ground truth, we get the negative log likelihood loss. In this
example, ego user v was finally activated (marked as black).
However, for different users, Grv’s may have different sizes. Meanwhile, the size (regarding the number of vertices) of Grv’s can be
very large due to the small-world property in social networks .
Such variously sized data is unsuited to most deep learning models. To remedy these issues, we sample a fixed-size sub-network
from v’s r-ego network, instead of directly dealing with the r-ego
A natural choice of the sampling method is to perform random
walk with restart (RWR) . Inspired by which suggest
that people are more likely to be influenced by active neighbors
than inactive ones, we start random walks from either the ego user
v or one of her active neighbors randomly. Next, the random walk
iteratively travels to its neighborhood with the probability that
is proportional to the weight of each edge. Besides, at each step,
the walk is assigned a probability to return to the starting node,
that is, either the ego user v or one of v’s active neighbors. The
RWR runs until it successfully collects a fixed number of vertices,
denoted by ¯Γrv with
= n. We then regard the sub-network ¯Grv
induced by ¯Γrv as a proxy of the r-ego network Grv, and denote
¯Stv = {stu : u ∈¯Γrv \ {v}} to be the action statuses of v’s sampled
neighbors. Therefore, we re-define the optimization objective in
Eq. 1 to be:
Neural Network Model
With the retrieved ¯Grv and ¯Stv for each user, we design an effective
neural network model to incorporate both the structural properties
in ¯Grv and action statuses in ¯Stv. The output of the neural network
model is a hidden representation for the ego user v, which is then
used to predict her action status—st+∆t
. As shown in Figure 2, the
proposed neural network model consists of a network embedding
layer, an instance normalization layer, an input layer, several graph
convolutional or graph attention layers, and an output layer. In this
section, we introduce these layers one by one and build the model
step by step.
Embedding Layer With the recent emergence of representation
learning , the network embedding technique has been extensively studied to discover and encode network structural properties
into a low-dimensional latent space. More formally, network embedding learns an embedding matrix X ∈RD×|V |, with each column
corresponding to the representation of a vertex (user) in the network G. In the proposed model, we use a pre-trained embedding
layer which maps a user u to her D-dimensional representation
xu ∈RD, as shown in Figure 2(b).
Instance Normalization Instance normalization is a recently proposed technique in image style transfer . We adopt
this technique in our social influence prediction task. As shown in
Figure 2(c), for each user u ∈¯Γrv , after retrieving her representation xu from the embedding layer, the instance normalization yu is
yud = xud −µd
for each embedding dimension d = 1, · · · , D, where
(xud −µd)2
Here µd and σd are the mean and variance, and ϵ is a small number
for numerical stability. Intuitively, such normalization can remove
instance-specific mean and variance, which encourages the downstream model to focus on users’ relative positions in latent embedding space rather than their absolute positions. As we will see
later in Section 5, instance normalization can help avoid overfitting
during training.
Input Layer As illustrated in Figure 2(d), the input layer constructs a feature vector for each user. Besides the normalized lowdimensional embedding comes from up-stream instance normalization layer, it also considers two binary variables. The first variable
indicates users’ action statuses, and the other indicates whether the
user is the ego user. Also, the input layer covers all other customized
vertex features such as structural features, content features, and
demographic features.
GCN Based Network Encoding Graph Convolutional Network (GCN) is a semi-supervised learning algorithm for graphstructured data. The GCN model is built by stacking multiple GCN
layers. The input to each GCN layer is a vertex feature matrix,
H ∈Rn×F , where n is the number of vertices, and F is the number
of features. Each row of H, denoted by h⊤
i , is associated with a vertex. Generally speaking, the essence of the GCN layer is a nonlinear
transformation that outputs H ′ ∈Rn×F ′ as follows:
H ′ = GCN(H ) = д  A(G)H W ⊤+ b ,
whereW ∈RF ′×F ,b ∈RF ′ are model parameters, д is a non-linear
activation function, A(G) is a n × n matrix that captures structural
information of graph G. GCN instantiates A(G) to be a static matrix
closely related to the normalized graph Laplaican :
AGCN(G) = D−1/2AD−1/2,
where A is the adjacency matrix3 of G, and D = diag(A1) is the
degree matrix.
Multi-head Graph Attention Graph Attention (GAT) is a
recent proposed technique that introduces the attention mechanism
into GCN. GAT defines matrix AGAT(G) = [aij]n×n through a selfattention mechanism. More formally, an attention coefficient eij is
firstly computed by an attention function attn : RF ′ × RF ′ →R,
which measures the importance of vertex j to vertex i:
eij = attn  W hi, W hj
Different from traditional self-attention mechanisms where the attention coefficients between all pairs of instances will be computed,
GAT only evaluates eij for (i, j) ∈E( ¯Grv) or i = j, i.e., (i, j) is either
an edge or a self-loop. In doing so, it is able to better leverage and
capture the graph structural information. After that, to make coefficients comparable among vertices, a softmax function is adopted
to normalize attention coefficients:
aij = softmax(eij) =
i exp (eik) .
Following Velickovic et al. , the attention function is instantiated with a dot product and a LeakyReLU nonlinearity. For an edge or a self-loop (i, j), the dot product is performed between parameter c and the concatenation of the feature vectors of the two end points—Whi and Whj, i.e., eij =
LeakyReLU  c⊤
, where the LeakyReLU has negative
3GCN applies self-loop trick on graph G by adding self-loop on each vertex, i.e.,
Table 1: Summary of datasets. |V | and |E| indicates the number of vertices and edges in graph G = (V, E), while N is the
number of social influence locality instances (observations)
as described in Section 2.
12,508,413
308,489,739
slop 0.2. To sum up, the normalized attention coefficients can be
expressed as:
exp  LeakyReLU  c ⊤
W hi ||W hj
i exp (LeakyReLU (c ⊤[W hi ||W hk])),
where || denotes the vector concatenation operation.
Once obtained the normalized attention coefficients, i.e., aij’s,
we can plugin AGAT(G) = [aij]n×n into Eq. 5. This completes the
definition of a single-head graph attention. In addition, we apply
multi-head graph attention as suggested by Velickovic et al. and
Vaswani et al. . The multi-head attention mechanism performs
K independent single attention in parallel, i.e., we have K independent parametersW1, · · · ,WK and attention matrix A1, · · · ,AK .
Multi-head attention aggregate the output of K single attention
together through an aggregation function:
H ′ = д  Aggregate  A1(G)H W ⊤
1 , · · · , AK (G)H W ⊤
We concatenate the outputs of each single-head attention to aggregate them except an average operator for the last layer.
Output Layer and Loss Function This layer (see Figure 2(f))
outputs a two-dimension representation for each user, we compare
the representation of the ego user with ground truth, and then
optimize the log-likelihood loss as described in Eq. 2.
Mini-batch Learning When sampling from r-ego network, we
force the sampled sub-networks to have a fixed size n. Benefiting
from such homogeneity, we can apply mini-batch learning here for
efficient training. As shown in Figure 2(a), in each iteration, we first
randomly sample B instances to be a mini-batch. Then we optimize
our model w.r.t. the sampled mini-batch. Such method runs much
faster than full-batch learning and still introduces enough noise
during optimization.
EXPERIMENT SETUP
We set up our experiments with large-scale real-world datasets to
quantitatively evaluate the proposed DeepInf framework.
Our experiments are conducted on four social networks from different domains —- OAG, Digg, Twitter, and Weibo. Table 1 lists
statistics of the four datasets.
OAG4 OAG (Open Academic Graph) dataset is generated by linking two large academic graphs: Microsoft Academic Graph 
and AMiner . Similar to the treatment in , we choose 20
4www.openacademic.ai/oag/
popular conferences from data mining, information retrieval, machine learning, natural language processing, computer vision, and
database research communities 5. The social network is defined
to be the co-author network, and the social action is defined to
be citation behaviors — a researcher cites a paper from the above
conferences. We are interested in how one’s citation behaviors are
influenced by her collaborators.
Digg Digg is a news aggregator which allows people to vote
web content, a.k.a, story, up or down. The dataset contains data
about stories promoted to Digg’s front page over a period of a
month in 2009. For each story, it contains the list of all Digg users
who have voted for the story up to the time of data collection and
the time stamp of each vote. The voters’ friendship links are also
retrieved.
Twitter The Twitter dataset was built after monitoring the
spreading processes on Twitter before, during and after the announcement of the discovery of a new particle with the features
of the elusive Higgs boson on Jul. 4th, 2012. The social network is
defined to be the Twitter friendship network, and the social action
is defined to be whether a user retweets “Higgs” related tweets.
Weibo Weibo6 is the most popular Chinese microblogging
service. The dataset is from and can be downloaded here.7 The
complete dataset contains the directed following networks and
tweets (posting logs) of 1,776,950 users between Sep. 28th, 2012 and
Oct. 29th, 2012. The social action is defined as retweeting behaviors
in Weibo — a user forwards (retweets) a post (tweet).
Data Preparation We process the above four datasets following
the practice in existing work . More concretely, for a user
v who was influenced to perform a social action a at some timestamp t, we generate a positive instance. Next, for each neighbor
of the influenced user v, if she was never observed to be active in
our observation window, we create a negative instance. Our target
is to distinguish positive instances from negative ones. However,
the achieved datasets are facing data imbalance problems in two
respects. The first comes from the number of active neighbors. As
observed by Zhang et al. , structural features become significantly correlated with social influence locality when the ego user
has a relatively large number of active neighbors. However, the
number of active neighbors is imbalanced in most social influence
data sets. For example, in Weibo, around 80% instances only have
one active neighbor and the instances with the number of active
neighbors ≥3 only occupies 8.57%. Therefore, when we train our
model on such imbalanced datasets, the model will be dominated
by observations with few active neighbors. To deal with the imbalance issue and show the superiority of our model in capturing local
structural information, we filter out observations with few active
neighbors. Especially, in each data set, we only consider instances
where ego users have ≥3 active neighbors. The second problem
comes from label imbalance. For example, in the Weibo dataset, the
ratio between negative instances and positive instances is about
5KDD, WWW, ICDM, SDM, WSDM, CIKM, SIGIR, NIPS, ICML, AAAI, IJCAI, ACL,
EMNLP, CVPR, ICCV, ECCV, MM, SIGMOD, ICDE, and VLDB.
6www.weibo.com
7 
Table 2: List of features used in this work.
Description
Coreness .
Pagerank .
Hub score and authority score .
Eigenvector Centrality .
Clustering Coefficient .
Rarity (reciprocal of ego user’s degree) .
Pre-trained network embedding (DeepWalk , 64-dim).
The number/ratio of active neighbors .
Density of subgnetwork induced by active neighbors .
#Connected components formed by active neighbors .
300:1. To address this issue, we sample a more balanced dataset
with the ratio between negative and positive to be 3:1.
Evaluation Metrics
To evaluate our framework quantitatively, we use the following
performance metrics:
Prediction Performance We evaluate the predictive performance
of DeepInf in terms of Area Under Curve (AUC) , Precision (Prec.),
Recall (Rec.), and F1-Measure (F1).
Parameter Sensitivity We analyze several hyper-parameters in
our model and test how different hyper-parameter choices can
influence prediction performance.
Case Study We use case studies to further demonstrate and explain
the effectiveness of our proposed framework.
Comparison Methods
We compare DeepInf with several baselines.
Logistic Regression (LR) We use logistic regression (LR) to train
a classification model. The model considers three categories of
features: (1) vertex features for the ego-user; (2) pre-trained network
embedding (DeepWalk ) for ego-user; (3) hand-crafted egonetwork features. The features we used are listed in Table 2.
Support Vector Machine (SVM) We also use support vector
machine (SVM) with linear kernel as the classification model. The
model use the same features as logistic regression (LR).
PSCN As we model social influence locality prediction as a
graph classification problem, we compare our framework with the
state-of-the-art graph classification models, PSCN . For each
graph, PSCN selects w vertices according to a user-defined ranking
function, e.g., degree and betweenness centrality. Then for each
selected vertex, it assembles its top k near neighbors according
to breadth-first search order. For each graph, The above process
constructs a vertex sequence of length w ×k with F channels, where
F is the number of features for each vertex. Finally, PSCN applies
1-dimensional convolutional layers on it.
DeepInf and its Variants We implement two variants of DeepInf,
denoted by DeepInf-GCN and DeepInf-GAT, respectively. DeepInf-
GCN uses graph convolutional layer as building blocks of our framework, i.e., setting A(G) = D−1/2AD−1/2 in Eq. 5. DeepInf-GAT uses
graph attention as shown in Eq. 7. However, both DeepInf and
PSCN accept vertex-level features only. Due to this limitation, we
do not use the ego-network features in these two models. Instead,
we expect that DeepInf can discover the ego-network features and
other predictive signals automatically.
Hyper-parameter Setting & Implementation Details As for
our framework, DeepInf, we first perform random walk with a
restart probability 0.8, and the size of sampled sub-network is set
to be 50. For the embedding layer, a 64-dimension network embedding is pre-trained using DeepWalk . Then we choose to
use a three-layer GCN or GAT structure for DeepInf, both the first
and second GCN/GAT layers contain 128 hidden units, while the
third layer (output layer) contains 2 hidden units for binary prediction. Especially, for DeepInf with multi-head graph attention,
both the first and second layer consists of K = 8 attention heads
with each computing 16 hidden units (for a total of 8 × 16 = 128
hidden units). For detailed model configuration, we adopt exponential linear units (ELU) as nonlinearity (function д in Eq. 5). All
the parameters are initialized with Glorot initialization and
trained using the Adagrad optimizer with learning rate 0.1 (0.05
for Digg dataset), weight decay 5e−4 (1e−3 for Digg dataset), and
dropout rate 0.2. We use 75%, 12.5%, 12.5% instances for training,
validation and test, respectively; the mini-batch size is set to be
1024 across all data sets.
As for PSCN, in our experiments, we find that the recommended
betweenness centrality ranking function does not work well in
predicting social influence. We turn to use breadth-first search
order starting from the ego user as the ranking function. When
BFS order is not unique, we break ties by ranking active users first.
We select w = 16 and k = 5 by validation and then apply two
1-dimensional convolutional layers. The first conv layer has 16
output channels, a stride of 5, and a kernel size of 5. The second
conv layer has 8 output channels, a stride of 1, and a kernel size of 1.
The outputs of the second layer are then fed into a fully-connected
layer to predict labels.
Finally, we allow PSCN and DeepInf to run at most 500 epochs
over the training data, and the best model was selected by early
stopping using loss on the validation sets. We release the code for
PSCN and DeepInf used in this work at 
DeepInf, both implemented with PyTorch.
EXPERIMENTAL RESULTS
We compare the prediction performance of all methods across the
four datasets in Table 3 and list the relative performance gain in
Table 4, where the gain is over the closest baseline. In addition, we
compare the variants of DeepInf and list the results in Table 5. We
have several interesting observations and insights.
(1) As shown in Figure 3, DeepInf-GAT achieves significantly
better performance over baselines in terms of both AUC and F1,
demonstrating the effectiveness of our proposed framework. In
OAG and Digg, DeepInf-GAT discovers the hidden mechanism
Table 3: Prediction performance of different methods on the
four datasets (%).
DeepInf-GAT
DeepInf-GAT
DeepInf-GAT
DeepInf-GAT
Table 4: Relative gain of DeepInf-GAT in terms of AUC
against the best baseline.
DeepInf-GAT
Relative Gain
Table 5: Prediction performance of variants of DeepInf (%).
DeepInf-GCN
DeepInf-GAT
DeepInf-GCN
DeepInf-GAT
DeepInf-GCN
DeepInf-GAT
DeepInf-GCN
DeepInf-GAT
and dynamics of social influence locality, giving us 3.8% relative
performance gain w.r.t. AUC.
(2) For PSCN, it selects a subset of vertices according to a userdefined ranking function. As mentioned in Section 4, instead of
using betweenness centrality, we propose to use BFS order-based
ranking function. Such ranking function can be regarded as a predefined graph attention mechanism where the ego user pays much
more attention to her active neighbors. PSCN outperform linear
predictors such as LR and SVM but does not perform as well as
DeepInf-GAT.
(3) An interesting observation is the inferiority of DeepInf-GCN,
as shown in Table 5. Previously, we have seen the success of GCN
in may label classification tasks . However, in this application, DeepInf-GCN achieves the worst performance over all the
Table 6: Prediction performance of DeepInf-GAT (%)
with/without vertex features as introduced in Table 2.
methods. We attribute its inferiority to the homophily assumption
of GCN—similar vertices are more likely to link with each other
than dissimilar ones. Under such assumption, for a specific vertex,
GCN computes its hidden representation by taking an unweighted
average over its neighbors’ representations. However, in our application, the homophily assumption may not be true. By averaging
over neighbors, GCN may mix predictive signals with noise. On
the other hand, as pointed out by , active neighbors are more
important than inactive neighbors, which also encourages us to use
graph attention which treats neighbors differently.
(4) In experiments shown in Table 3, 4, and 5, we still rely on
several vertex features, such as page rank score and clustering
coefficient. However, we want to avoid using any hand-crafted features and make DeepInf a “pure” end-to-end learning framework.
Quite surprisingly, we can still achieve comparable performance (as
shown in Table 6), even we do not consider any hand-crafted features except the pre-trained network embedding.
Parameter Analysis
In this section, we investigate how the prediction performance
varies with the hyper-parameters in sampling near neighbors and
the neural network model. We conduct the parameter analyses on
the Weibo dataset unless otherwise stated.
Return Probability of Random Walk with Restart When sampling near neighbors, the return probability of random walk with
restart (RWR) controls the “shape” of the sampled r-ego network.
Figure 3(a) shows the prediction performance (in terms of AUC
and F1) by varying the return probability from 10% to 90%. As the
increasing of return probability, the prediction performance also
increases slightly, illustrating the locality pattern of social influence.
Size of Sampled Networks Another parameter that controls the
sampled r-ego network is the size of sampled networks. Figure 3(b)
shows the prediction performance (in terms of AUC and F1) by
varying the size from 10 to 100. We can observe a slow increase
of prediction performance when we sample more near neighbors.
This is not surprising because we have more information as the
size of sampled networks increases.
Negative Positive Ratio As we mentioned in Section. 5, the positive and negative observations are imbalanced in our datasets. To
investigate how such imbalance influence the prediction performance, we vary the ratio between negative and positive instances
from 1 to 10 , and show the performance in Figure 3(c). We can
observe a decreasing trend w.r.t. the F1 measure, while the AUC
score stays stable.
#Head for Multi-head Attention Another hyper-parameter we
analyze is the number of heads used for multi-head attention. For a
fair comparison, we fixed the number of total hidden units to be 128.
We vary the number of heads to be 1, 2, 4, 8, 16, 32, 64, 128, i.e., each
head has 128, 64, 32, 16, 8, 4, 2, 1 hidden units, respectively. As shown
in Figure 3(d), we can see that DeepInf benefits from the multihead mechanism. However, as the decreasing of the number of
hidden units associated with each head, the prediction performance
decreases.
Effect of Instance Normalization As claimed in Section 3, we
use an Instance Normalization (IN) layer to avoid overfitting, especially when training set is small, e.g., Digg. Figure 4(a) and Figure 4(b) illustrate the training loss and test AUC of DeepInf-GAT on
the Digg dataset trained with and without IN layer. We can see that
IN significantly avoids overfitting and makes the training process
more robust.
Discussion on GAT and Case Study
Besides the concatenation-based attention used in GAT (Eq. 7), we
also try other popular attention mechanisms, e.g., the dot product
attention or the bilinear attention as summarized in . However, those attention mechanisms do not perform as well as the
concatenation-based one. In this section, we introduce the orderpreserving property of GAT . Based on the property, we attempt
to explain the effectiveness of DeepInf-GAT through case studies.
Observation 1. Order-preserving of Graph Attention Suppose
(i, j), (i,k), (i′, j) and (i′,k) are either edges or self-loops, and aij,
aik, ai′j, ai′k are the attention coefficients associated with them. If
aij > aik then ai′j > ai′k.
Proof. As introduced in Eq. 7, the graph attention coefficient
for edge (or self-loop) (i, j) is defined as aij = softmax(eij), where
eij = LeakyReLU  c ⊤
W hi ||W hj
If we rewrite c⊤=
eij = LeakyReLU  p⊤W hi + q⊤W hj
Due to the strict monotonicity of softmax and LeakyReLU, aij >
aik implies q⊤Whj > q⊤Whk. Apply the strict monotonicity of
LeakyReLU and softmax again, we get ai′j > ai′k.
The above observation shows the following fact—although each
vertex only pay attention to its neighbors in GAT (local attention),
the attention coefficients have a global ranking, which is determined
by q⊤Whj only. Thus we can define a score function score(j) =
q⊤Whj. Then each vertex pays attention to its neighbors according
to this score function—a higher score function value indicates a
higher attention coefficient. Thus, plotting the value of the scoring
function can illustrate where are the “popular areas” or “important
areas” of the network. Furthermore, multi-head attention provides
a multi-view mechanism—for K heads, we have K score functions,
scorek(j) = q⊤
kWkhj, k = 1, · · · ,K, highlighting different areas
of the network. To better illustrate this mechanism, we perform a
few case studies. As shown in Figure 5, we choose four instances
(a) Return Probability of RWR
(b) Size of Sampled Networks
(c) Negative Positive Ratio
(d) Number of Heads
Figure 3: Parameter analysis. (a) Return probability of random walk with restart. (b) Size of sampled networks. (c) Negative
positive ratio. (d) The number of heads used for multi-head attention.
Training Loss
Without IN
(a) Training Loss
Test AUC (%)
Without IN
(b) Test AUC (%)
Figure 4: The (a) training loss/(b) test AUC of DeepInf-GAT
on Digg data set trained with and without Instance Normalization, vs. the number of epochs. Instance Normalization
helps avoid overfitting.
from the Digg dataset (each row corresponding to one instance) and
select three representative attention heads from the first GAT layer.
Quite interestingly, we can observe explainable and heterogeneous
patterns discovered by different attention heads. For example, as
shown in Figure 5, the first attention head tend to focus on the
ego-user, while the second and the third highlight active users and
inactive users, respectively. However, this property does not hold
for other attention mechanisms. Due to the page limit, we do not
discuss them here.
RELATED WORK
Our study is closely related to a large body of literature on social
influence analysis and graph representation learning .
Social Influence Analysis Most existing work has focused on
social influence modeled as a macro-social process (a.k.a., cascade),
with a few that have explored the alternative user-level mechanism that considers the locality of social influence in practice. At
the macro level, researchers are interested in global patterns of
social influence. Such global patterns includes various respects of
a cascade and their correlation with the final cascade size, e.g.,
the rise-and-fall patterns , external influence sources , and
conformity phenomenon . Recently, there have been efforts to
detect those global patterns automatically using deep learning, e.g.,
the DeepCas model which formulate cascade prediction as a
sequence problem and solve it with Recurrent Neural Network.
Figure 5: Case study. How different graph attention heads
highlight different areas of the network. (a) Four selected
cases from the Digg dataset. Active and inactive users are
marked as black and gray, respectively. User v is the egouser that we are interested in. (b)(c)(d) Three representative
attention heads.
Another line of studies focuses on the user-level mechanism in social influence where each user is only influenced by her near neighbors. Examples of such work include pairwise influence ,
topic-level influence , group formation and structural
diversity . Such user-level models act as fundamental
building blocks of many real-world problems and applications. For
example, in the influence maximization problem , both independent cascade and linear threshold models assume a pairwise
influence model; In social recommendation , a key assumption
is social influence—the ratings and reviews of existing users will
influence future customers’ decisions through social interaction.
Another example is a large-scale field experiment by Facebook Bond
et al. during the 2010 US congressional elections, the results
showed how online social influence changes offline voting behavior.
Graph Representation Learning Representation learning 
has been a hot topic in research communities. In the context of
graph mining, there have been many efforts to graph representation learning. One line of studies focus on vertex (node) embedding, i.e., to learn a low-dimensional latent factors for each vertex.
Examples include DeepWalk , LINE , node2vec , metapath2vec , NetMF , etc. Another line of studies pay attention
to representation of graphs, i.e., to learn latent representations
of sub-structures for graphs, including, graph kernel , deep
graph kernel , and state-of-the-art method PSCN . Recently,
there have been several attempts to incorporate semi-supervised
information into graph representation learning. Typical examples
include GCN , GraphSAGE , and the state-of-the-art model
CONCLUSION
In this work, we study the social influence locality problem. We formulate this problem from a deep learning perspective and propose
a graph-based learning framework DeepInf by incorporating the
recently developed network embedding, graph convolution, and
self-attention techniques. We test the proposed framework on four
social and information networks—OAG, Digg, Twitter, and Weibo.
Our extensive experimental analysis shows DeepInf significantly
outperforms baselines with rich hand-craft features in predicting
social influence locality. This work explores the potential of network representation learning in social influence analysis and gives
the very first attempt to explain the dynamics of social influence.
The general idea behind the proposed DeepInf can be extended
to many network mining tasks. Our DeepInf can effectively and
efficiently summarize a local area in a network. Such summarized
representations can then be fed into various down-stream applications, such as link prediction, similarity search, network alignment,
etc. Therefore, we would like to explore this promising direction
for future work. Another exciting direction is the sampling of near
neighbors. In this work, we perform random walk with restart without considering any side information. Meanwhile, the sampling
procedure is loosely coupled with the neural network model. It is
also exciting to combine both sampling and learning together by
leveraging reinforcement learning.
Acknowledgements. We thank Linjun Zhou, Yutao Zhang, and
Jing Zhang for their comments. Jiezhong Qiu and Jie Tang are supported by NSFC 61561130160 and National Basic Research Program
of China 2015CB358700.