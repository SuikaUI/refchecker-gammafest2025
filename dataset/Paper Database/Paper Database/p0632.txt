A Survey of Natural Language Generation
CHENHE DONG∗, Sun Yat-Sen University, China
YINGHUI LI∗, Tsinghua University, China
HAIFAN GONG, Sun Yat-Sen University, China
MIAOXIN CHEN, Tsinghua University, China
JUNXIN LI, Tsinghua University, China
YING SHEN†, Sun Yat-Sen University, China
MIN YANG†, Chinese Academy of Science, China
This paper oﬀers a comprehensive review of the research on Natural Language Generation (NLG) over the
past two decades, especially in relation to data-to-text generation and text-to-text generation deep learning
methods, as well as new applications of NLG technology. This survey aims to (a) give the latest synthesis
of deep learning research on the NLG core tasks, as well as the architectures adopted in the ﬁeld; (b) detail
meticulously and comprehensively various NLG tasks and datasets, and draw attention to the challenges in
NLG evaluation, focusing on diﬀerent evaluation methods and their relationships; (c) highlight some future
emphasis and relatively recent research issues that arise due to the increasing synergy between NLG and
other artiﬁcial intelligence areas, such as computer vision, text and computational creativity.
CCS Concepts: • Computing methodologies →Natural language generation.
Additional Key Words and Phrases: natural language generation, data-to-text generation, text-to-text generation, deep learning, evaluation
ACM Reference Format:
Chenhe Dong, Yinghui Li, Haifan Gong, Miaoxin Chen, Junxin Li, Ying Shen, and Min Yang. 2022. A Survey of
Natural Language Generation. ACM Comput. Surv. 1, 1, Article 1 , 38 pages. 
INTRODUCTION
This paper surveys the current state of the art in Natural Language Generation (NLG), deﬁned
as the task of generating text from underlying non-linguistic representation of information .
NLG has been receiving more and more attention from researchers because of its extremely challenging and promising application prospects.
∗Equal contribution
†Corresponding author
Authors’ addresses: Chenhe Dong, , Sun Yat-Sen University, Guangzhou, China, 510275;
Yinghui Li, , Tsinghua University, Shenzhen, China, 518055; Haifan Gong, gonghf@
mail2.sysu.edu.cn, Sun Yat-Sen University, Guangzhou, China, 510275; Miaoxin Chen, , Tsinghua University, Shenzhen, China, 518055; Junxin Li, , Tsinghua University, Shenzhen,
China, 518055; Ying Shen, , Sun Yat-Sen University, Guangzhou, China, 510275; Min Yang,
 , Chinese Academy of Science, Shenzhen, China, 510100.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and
the full citation on the ﬁrst page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior speciﬁc permission and/or a fee. Request permissions from .
© 2022 Association for Computing Machinery.
0360-0300/2022/1-ART1 $15.00
 
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
Dong and Li, et al.
What is Natural Language Generation?
Natural Language Generation (NLG) is the process of producing a natural language text in order to
meet speciﬁed communicative goals. The texts that are generated may range from a single phrase
given in answer to a question, through multi-sentence remarks and questions within a dialog, to
full-page explanations.
In contrast with the organization of the Natural Language Understanding (NLU) process – which
can follow the traditional stages of a linguistic analysis: morphology, syntax, semantics, pragmatics/discourse – the generation process has a fundamentally diﬀerent character. Generation proceeds involve the content planning, determination and realization from content to form, from
intentions and perspectives to linearly arrayed words and syntactic markers. Coupled with its application, the situation, and the discourse, they provide the basis for making choices among the
alternative wordings and constructions that the language provides, which is the primary eﬀort
in constructing a text deliberately . With its opposite ﬂow of information, one might assume
that a generation process could be organized like an understanding process but with the stages in
opposite order.
Both data-to-text generation and text-to-text generation are instances of NLG. Generating text
from image is an application of data-to-text generation. A further text-to-text generation complication is dividing NLG tasks into three categories, i.e., text abbreviation, text expansion, text
rewriting and reasoning. The text abbreviation task is formulated to condense information from
long texts to short ones, typically including research on text summarization ,
question generation , and distractor generation . The text expansion tasks, such as short text expansion and
topic-to-essay generation , generate complete sentences or even texts from
some meaningful words by considering and adding elements like conjunctions and prepositions to
transform the input words into linguistically correct outputs. The goal of text rewriting and reasoning task is to rewrite the text into another style or applying reasoning methods to create responses.
There are two sub-tasks: text style transfer , and dialogue
generation . The task of visual based text generation targets at generate the explanation or summarization of the given image or video, involving the study of image
caption , video caption ,
and visual storytelling .
Why a Survey on Natural Language Generation?
Here, we will explain the reasons and motivations why the natural language generation is worth
reviewing and investigating.
Reiter et al. provided the most classical survey of NLG. However, the ﬁeld of NLG has
changed drastically in the last 20 years, with the emergence of successful deep learning methods. For example, since 2014, various neural encoder-decoder models pioneered by sequence-tosequence (Seq2Seq) have been proposed to achieve the goal by learning to map input text to output
text. In addition, the evaluation of NLG output should start to receive systematic attention.
Since Reiter et al. published their book, various other NLG overview texts have also appeared. Gatter et al. introduce the core tasks, applications and evaluation metrics of natural
language generation. While useful, this survey is not highly timely and does not include the stateof-the-art research on the novel deep learning models such as graph neural networks. Perera et al.
 cover some tasks or architectures of NLG. Santhanam et al. review the NLG research
progress of dialogue systems. Mogandala et al. study the integration of vision and language
in multimodal NLG, such as image dialogue and video storytelling. Otter et al. concludes the
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
A Survey of Natural Language Generation
progress of deep learning for NLP, display some classic text generation methods but barely discuss the research progress of NLG. Yu et al. oﬀers a survey of the knowledge-enhanced text
generation methods.
The goal of the our survey is to present a highly timely overview of NLG developments from the
aspect of data-to-text generation and text-to-text generation. Though NLG has been a part of AI
and Natural language processing for long time, it has only recently begun to take full advantage
of recent advances in data-driven, machine learning and deep learning approaches. Therefore this
survey will focus on introducing the latest development and future directions of deep learning
methods in ﬁeld of NLG. This survey can have broad audiences, researchers and practitioners, in
academia and industry.
Contribution of this Survey
In this paper, we provide a thorough review of diﬀerent natural language generation tasks as well
as its corresponding datasets and methods. To summarize, this paper presents an extensive survey
of natural language generations with the following contributions:
(1) To give an up-to-date synthesis of deep learning research on the core tasks in NLG, as well
as the architectures adopted in the ﬁeld;
(2) To detail meticulously and comprehensively various NLG tasks and datasets, and draw attention to the challenges in NLG evaluation, focusing on diﬀerent evaluation methods and
their relationships.
(3) To highlight some future emphasis and relatively recent research issues that arise due to the
increasing synergy between NLG and other artiﬁcial intelligence areas, such as computer
vision, text and computational creativity.
The rest of this survey is organized as follows. In Sec.2, we introduce the general methods of
NLG to give a comprehensive understanding. From Sec.3 to Sec.6, we will give a comprehensive introduction to the four main areas of NLG from the perspectives of task, data, and methods. In Sec.7,
we present the important evaluation metrics used in various aforementioned NLG tasks. Besides,
we propose some problems and challenges of NLG as well as several future research directions in
Sec.8. And ﬁnally we conclude our survey in Sec.9.
GENERAL METHODS OF NLG
In general, the task of natural language generation (NLG) targets at ﬁnding an optimal sequence
푦<푇+1 = (푦1,푦2, ...,푦푇) that satisﬁes:
푦<푇+1 = arg max
log 푃휃(푦<푇+1|푥) = arg max
log 푃휃(푦푡|푦<푡, 푥),
where푇represents the number of tokens of the generated sequence, Y represents a set containing
all possible sequences, and 푃휃(푦푡|푦<푡,푥) is the conditional probability of the next token 푦푡based
on its previous tokens 푦<푡= (푦1,푦2, ...,푦푡−1) and the source sequence 푥with model parameters 휃.
The general methods to deal with the tasks of NLG mainly contain: Recurrent Neural Network,
Transformer, Attention Mechanism, Copy and Pointing Mechanisms, Generative Adversarial Network, Memory Network, Graph Neural Network, and Pre-trained Model.
Recurrent Neural Network
As proposed by , the encoder of the sequence-to-sequence (Seq2Seq) framework is an Recurrent Neural Network (RNN), it will traverse every token (word) of the input, the input of each
time is the hidden state and input of the previous time, and then there will be an output and a
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
Dong and Li, et al.
new hidden state. The new hidden state will be used as the input hidden state of the next time.
We usually only keep the hidden state of the last time, which encodes the semantics of the whole
sentence. After the encoder processing, the last hidden state will be regarded as the initial hidden
state of the decoder. The Decoder is also an RNN, which outputs one word at a time. The input of
each time is the hidden state of the previous time and the output of the previous time. The initial
hidden state is the last hidden state of the encoder, and the input is special. Then, RNN is used
to calculate the new hidden state and output the ﬁrst word, and then the new hidden state and
the ﬁrst word are used to calculate the second word. Until EOS is encountered and the output is
ﬁnished. A standard RNN computes a sequence of outputs (푦1, ...,푦푇) given a sequence of inputs
(푥1, ..., 푥푇) by iterating the following equation:
푦푡= 푊yh · ℎ푡= 푊yh · 휎(푊hx푥푡+푊hhℎ푡−1),
where 휎is activation function, 푊hx,푊hh,푊yh are learnable parameters, and ℎ푡is the hidden
state at 푡-th timestep.
Transformer
Transformer is based on the encoder-decoder framework, and both encoder and decoder are
composed of stacked identiﬁed layers. The encoder is used to map an input sequence of symbol
representations to another sequence of continuous representations, and then the decoder autoregressively generates an output sequence based on its previously generated symbols and the continuous representations from encoder. In the encoder, each layer contains two sub-layers, which
are multi-head self-attention mechanism (MultiHeadAttn) and position-wise fully connected feedforward network (FFN) respectively. The multi-head self-attention can be formulated by:
MultiHeadAttn(푄, 퐾,푉) = Concat(head1, ..., headℎ)푊푂,
head푖= Attention(푄푊푄
Attention(푄, 퐾,푉) = softmax(푄퐾⊤
where 푄,퐾,푉are the query, key, and value matrices, 푑푘is the dimension of queries and keys. And
the feed-forward network can be formulated by:
FFN(푥) = max(0,푥푊1 + 푏1)푊2 + 푏2.
Each of the stacked layer is surrounded by a residual connection followed by layer normalization.
While in the decoder, each layer contains three sub-layers with an additional multi-head attention
over the encoder’s output. The self-attention in decoder is masked to prevent attending to subsequent tokens. In addition, to inject the position information, the position encodings are added to
the input embeddings as formulated by:
푃퐸푝표푠,2푖= 푠푖푛(푝표푠/100002푖/푑푚표푑푒푙), 푃퐸푝표푠,2푖+1 = 푐표푠(푝표푠/100002푖/푑푚표푑푒푙),
where 푝표푠is the position and 푖is the dimension.
The most commonly used loss function is the conditional language modeling loss, which can be
formulated as:
log 푃휃(푦푡|푦<푡, 푥),
where log 푃휃(푦푡|푦<푡, 푥) is the log-likelihood of the 푡-th generated token conditioned on the previously generated sequence 푦<푡and the source sequence 푥.
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
A Survey of Natural Language Generation
Atention Mechanism
Attention mechanism is used to perform a mapping from a query to a series of key value pairs.
There are three steps in the calculation of attention. The ﬁrst step is to calculate the similarity
between query and each key to get the weight. The common similarity functions are dot product,
splicing, perceptron, etc.; the second step is to normalize these weights by using a softmax function;
the last step is to sum the weight and the corresponding key value to get the ﬁnal attention. The
tokens in an article are ﬁrst fed into the encoder to generate a sequence of encoder hidden states
ℎ푖, then the decoder receives the word embedding of previous step and derive the decoder state 푠푡
at each step 푡. After that, the attentive context vector ℎ∗
푡can be obtained based on the attention
distribution 푎푡as:
푖ℎ푖, where 푎푡
푖= softmax(푒푡
푖= 푣⊤tanh(푊ℎℎ푖+푊푠푠푡+ 푏푎푡푡푛),
in which 푣,푊ℎ,푊푠,푏푎푡푡푛are learnable parameters. Finally, the predicted vocabulary distribution
푃푣표푐푎푏is obtained by:
푃푣표푐푎푏= softmax(푉′(푉[푠푡,ℎ∗
푡] + 푏) + 푏′),
where 푉,푉′,푏,푏′ are learnable parameters.
Copy and Pointing Mechanisms
The copy and pointing mechanisms proposed by PGN is widely used in abstractive summarization, which is designed for alleviating the problem of inaccurate reproduced factual details
via a pointer, dealing with out-of-vocabulary words and repetition via a generator and a coverage
mechanism, respectively. In the pointer and generator, a generation probability 푝푔푒푛based on the
context vector ℎ∗
푡, decoder state 푠푡and decoder input 푥푡at step 푡is calculated by:
푝푔푒푛= 휎(푤⊤
푥푥푡+ 푏푝푡푟),
which serves as a soft switch to choose between generating a word based on the vocabulary probability, or copying a word from the source document based on the attention distribution. The
probability distribution over the extended vocabulary 푃(푤) can be formulated as:
푃(푤) = 푝푔푒푛푃푣표푐푎푏(푤) + (1 −푝푔푒푛)
And in the coverage mechanism, a coverage vector 푐푡at step 푡is calculated by the sum of attention
distributions over previous decoder timesteps as푐푡= Í푡−1
푡′=0 푎푡′, which is then added to the attention
mechanism and the primary loss function as:
푖= 푣⊤tanh(푊ℎℎ푖+푊푠푠푡+ 푤푐푐푡
푖+ 푏푎푡푡푛),
푙표푠푠푡= −log푃(푤∗
푡is the target word at step 푡.
Generative Adversarial Network
The generative adversarial network (GAN) is a framework that uses an adversarial training
process to estimate generative models. This framework can be regarded as a minimax two-player
game containing a generative model and a discriminative model, and these two models are simultaneously trained. The generator (G) captures the data distribution and tries to produce fake samples,
and the discriminator (D) attempts to determine whether the samples come from the model distribution or data distribution. In detail, G is trained to maximize the probability identiﬁed by D for
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
Dong and Li, et al.
the sample coming from the data rather than G, while D is trained to maximize the probability
of assigning the correct label to training samples and samples generated by G. The training process continues until the counterfeits are indistiguishable from the genuine articles. The training
objective with value function 푉(퐷,퐺) can be formulated as:
퐷푉(퐷,퐺) = Ex∼푝푑푎푡푎(x) [log 퐷(x)] + Ez∼푝z(z) [log(1 −퐷(퐺(z)))].
Memory Network
The end-to-end memory network is based on the recurrent neural network. Before outputting
a symbol, the recurrence reads from a possibly large external memory multiple times. In this framework, a discrete set of input sentences is written to the memory up to a ﬁxed size, and a continuous
representation for each memory sentence and the query (i.e., the question) is calculated and is then
processed through multiple hops to output the answer. Speciﬁcally, for the single hop operation,
the matching probability between each query and memory is ﬁrst computed by the inner product
followed by a softmax in the embedding space. The input set 푥and the query 푞are ﬁrst embedded
to obtain the memory vectors 푚and 푢, respectively, and the match probability 푝푖of the 푖-th input
sentence is calculated as:
푝푖= softmax(푢⊤푚푖).
Then the output memory representation 표is obtained by a weighted sum over another embedded
memory sentences 푐based on the matching probability as:
Finally, the ﬁnal prediction ˆ푎can be obtained through a weight matrix and a softmax over the sum
of output memory vector 표and input embedding 푢as:
ˆ푎= softmax(푊(표+ 푢)).
To handle multiple hop operations, the memory layers are repeatedly stacked, and the input of
each layer is the sum of the output memory vector and the input from its previous layer.
Graph Neural Network
The graph neural network (GNN) is used to process the data in graph form, which can capture
the dependency information between nodes of a graph via message passing. Compared with CNN
and RNN, GNN can propagate on each node respectively and is able to ignore the input orders
of nodes, which is more computational eﬃcient. The representation of each node in a graph is
iteratively updated by aggregating information from its neighboring nodes and edges. So far, many
propagation strategies have been proposed, such as convolution , RNN based gate mechanism
 , and attention mechanism . Meanwhile, many works attempt to improve the training
method, such as sampling-based training and unsupervised training . Take the graph
convolution network as an example, it follows the layer-wise propogation rule as:
퐻(푙+1) = 휎
2 퐻(푙)푊(푙)
where 휎(·) refers to an activation fuction, ˜퐴= 퐴+퐼푁is the adjacency matrix of the graph (including
the identity matrix 퐼푁), ˜퐷푖푖= Í
푗˜퐴푖푗is the degree of node 푖, 푊(푙) is a trainable weight matrix in
the 푙-th layer, and 퐻(푙) is the matrix of node feature vectors in the 푙-th layer.
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
A Survey of Natural Language Generation
Pre-trained Model
The pre-trained models can be divided into two categories: non-contextual and contextual. The
non-contextual pre-trained models (e.g., Word2vec , GloVe ) can learn high quality of word
and phrase representations, and are widely used to initialize the embeddings and improve the
model performance for generation tasks. However, since the non-contextual embeddings are static,
it is unable to handle context-dependent words and out-of-vocabulary words.
To overcome these problems, contextual pre-trained models are proposed, which can dynamically change the word embeddings when the word appears in diﬀerent sentences. Traditional
contextual models mainly focus on the tasks of natural language understanding (NLU), which are
based on the architectures of LSTM (e.g., ELMo ) or Transformer encoder (e.g., BERT ).
To tackle the more challenging tasks of natural language generation (NLG), the architecture of
Transformer decoder is widely adopted, and the pre-training objective is further modiﬁed to adapt
to the NLG task. For example, GPT uses the standard language modeling as the pre-training
objective. T5 designs a uniﬁed text-to-text framework for both NLU and NLG. During pretraining, each corrupted token span in the input sequence is replaced with a sentinel token, and
the output sequence is composed of the dropped-out spans. MASS predicts the sentence
fragment with input of masked sequence during pre-training. UniLM propose a uniﬁed pretraining framework for both NLU and NLG tasks, which is based on a shared Transformer model
with three diﬀerent types of self-attention masks to switch among diﬀerent tasks, including unidirectional, bidirectional, and sequence-to-sequence. BART designs a reconstruction objective
to restore corrupted documents, where ﬁve types of transformation strategies are proposed, including token masking, token deletion, text inﬁlling, sentence permutation, and document rotation.
PLATO propose a pre-training framework targeting the dialogue generation tasks with two
reciprocal pre-training tasks, i.e., response generation and latent act recognition. The latent discrete variables are also introduced to solve the one-to-many mapping problem. ERNIE-GEN 
is a multi-ﬂow Seq2Seq pre-training model, which pays attention to the exposure bias problem of
pre-training models on downstream NLG tasks such as question generation, and aims to make the
NLG models generate more human-like and ﬂuent texts. OFA present a uniﬁed multimodal
pre-training framework to unify various vision and language tasks, such as NLU, NLG, and image
classiﬁcation. Transformer is applied as the backbone architecture, and the sparse coding and a
uniﬁed vocabulary are utilized to represent the images and linguistic words.
TEXT ABBREVIATION
The goal of text abbreviation is to distill key information from long texts to short ones, which
consists of three subtopics: text summarization, question generation, and distractor generation.
There are two kinds of methods for text abbreviation: extractive and abstractive methods. Since
the abstractive approach is more ﬂexible and can create more human-like sentences than the extractive approach, it has been paid more and more attentions in recent years and is our main focus in
this paper. Text summarization is the process of generating entirely new phrases and sentences to
capture the meaning of the source document. Question generation concentrates on automatically
generating questions from a given sentence or paragraph. Distractor generation is the automatic
generation of adequate distractors for a given question answer pair generated from a given article
to form an adequate multiple-choice question. We summarize the most representative methods for
each subtask in Table 1.
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
Dong and Li, et al.
Table 1. Natrual language generation models for text abbreviation.
Description
Text Summarization
MASS 
Transformer
Transformer + Multi-task Learning
PEGASUS 
Transformer + Multi-task Learning
Transformer + RNN + CNN + Long-term Dependency
ProphetNet 
Transformer + Long-term Dependency
En-Semantic-Model 
RNN + Long-term Dependency
Post-Editing Factual Error Corrector 
Transformer + Factual Consistency
SpanFact 
Transformer + Factual Consistency
Question Generation
Key-Phrase-based Question Generator 
Keyphrase + Template
Dynamic Mathematical Question Generator 
Constraint Handling Rules
KB-based Factoid Question Generator 
Teacher Forcing and RL Based Question Generator 
Paragraph-level Question Generator 
Answer-Position-aware Question Generator 
RNN + Answer-focused
ASs2s 
RNN + Answer-focused
NQG-MP 
RNN + Multi-task Learning
Paraphrase Enhanced Question Generator 
RNN + Multi-task Learning
CGC-QG 
RNN + Multi-task Learning + GNN
PathQG 
RNN + Multi-task Learning + KG
UniLM 
Transformer + Multi-task Learning
ERNIE-GEN 
Transformer + Multi-task Learning
Distractor Generation
Educational Ontology Distractor Generator 
Ontology + Embedding
Learning to Rank Based Distractor Generator 
Embedding + Ranking + GAN + RL
BERT-based Distractor Generation 
BERT + Multi-task Learning
Hierarchical Dual-attention Distractor Generator 
RNN + Answer Interaction
HMD-Net 
Transformer + RNN + Answer Interaction
Code Compression Distractor Generator 
Abstract Syntax Tree
CSG-DS 
LDA + KB + Ranking
Named Entity Distractor Generator 
Tree + Clustering
Text Summarization. There are mainly four datasets in the ﬁeld of text summarization as
shown below.
CNN/DailyMail. The CNN/DailyMail dataset is a large scale reading comprehension dataset.
This dataset contains 93k and 220k articles collected from the CNN and Daily Mail websites, respectively, where each article has its matching abstractive summary.
NYT. The New York Times (NYT) dataset contains large amount of articles written
and published by the New York Times between 1987 and 2007. In this dataset, most of the articles
are manually summarized and tagged by a staﬀof library scientists, and there are over 650,000
article-summary pairs.
XSum. The extreme summarization (XSum) dataset is an extreme summarization dataset
containing BBC articles and corresponding single sentence summaries. In this dataset, 226,711
Wayback archived BBC articles are collected, which range from 2010 to 2017 and cover a wide
variety of domains.
Gigaword. The English Gigaword dataset is a comprehensive collection of English
newswire text data acquired by the Linguistic Data Consortium. This corpus contains four distinct international sources of English newswire, and has totally 4,111,240 documents.
Qestion Generation. The two popular datasets for the task of question generation are shown
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
A Survey of Natural Language Generation
SQuAD. The Stanford Question Answering Dataset (SQuAD) is a large reading comprehension dataset created by crowdworkers. The questions in this dataset are posed by crowdworkers
based on a set of Wikipedia articles, and the answers are text segments from the corresponding
passages. In total, SQuAD contains 107,785 question-answer pairs on 536 articles.
MS MARCO. The Microsoft Machine Reading Comprehension (MS MARCO) dataset is a
collection of anonymized search queries issued through Bing or Cortana for reading comprehension. The dataset contains both answerable and unanswerable questions. Each answerable question
has a set of extracted passages from the retrieved response documents of Bing. There are totally
1,010,916 questions and 8,841,823 answering passages extracted from 3,563,535 web documents in
this dataset.
Distractor Generation. There are three datasets widely used for distractor generation as
shown below.
SciQ. SciQ is a crowdsourced multiple choice question answering dataset, which consists
of 13.7k science exam questions. The domain of this dataset covers biology, chemistry, earth science, and physics.
MCQL. The MCQL dataset is a collection of multiple choice questions at the Cambridge O
level and college level, which is crawled from the Web. This dataset totally contains 7.1k questions
covering biology, phisics, and chemistry.
RACE. RACE is a reading comprehension dataset collected from the English exams in Chinese middle and high schools. This dataset contains 27,933 passages and 97,687 questions, covering
all types of human articles.
Text Summarization. Recently, the most common methods in this ﬁeld are encoder-decoder
based pre-trained language models. Song et al. design a novel pre-training objective to jointly
pre-train the encoder and decoder, where the decoder learns to predict the masked sentence fragments in the encoder side. Given an unpaired source sentence 푥from the source domain X, the
model with parameter 휃predicts the sentence fragment 푥푢:푣from position 푢to 푣with the masked
sequence 푥\푢:푣as input, and the objective function is formulated as:
log 푃(푥푢:푣|푥\푢:푣;휃) =
<푡,푥\푢:푣;휃).
Lewis et al. present a denoising autoencoder for pre-training, which consists of a series of
noising strategies to corrupt text and a training objective to reconstruct the original sentence. In
addition, Zhang et al. propose a self-supervised pre-training objective speciﬁc for the text
summarization task, namely the gap-sentences generation objective, and their PEGASUS model
achieves state-of-the-art performances on the mainstream datasets.
However, the pre-trained language models’ ability of capturing long-term dependencies and
maintaining global coherence is poor. To solve this problem, Cai et al. introduce an additional
encoder with a bidirectional RNN and a convolution module to simultaneously model sequential
context and capture local importance to the base Transformer model. The encoder ﬁrst applies a
bidirectional LSTM on the source text embedding 퐸and derive the hidden states sequence 퐻, then
uses three convolution operations with kernel sizes of 1,3,5 to learn n-gram features 퐷from 퐻,
and ﬁnally a gated linear unit is applied to select features, which can be formulated by:
푅= 휎(푊푑퐷+ 푏푑) ⊙(푊ℎ퐻+ 푏ℎ),
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
Dong and Li, et al.
where 퐻= BiLSTM(퐸). Yan et al. make several improvements to the traditional language
models. Speciﬁcally, a new self-supervised objective called future n-gram prediction has been applied, and the n-stream self-attention mechanism is used in the decoder to be trained to predict the
future n-gram of each time step. Given a source sequence 푥and its target sequence 푦, the future
n-gram prediction objective can be formulated as:
log푝휃(푦푡+푗|푦<푡,푥)
Ding et al. propose an enhanced semantic model based on double encoders and a decoder with
a Gain-Beneﬁt gate structure, which is able to provide richer semantic information and reduce the
inﬂuence of the length of generated texts on the decoding accuracy. The dual-encoder is used to
capture both global and local context semantic information based on the bidirectional RNN, and
the output vector of the Gain-Beneﬁt gate module is calculated by:
P푡= (1 −푎) ⊙M + 푎⊙C푡−1, where 푎= sigmoid(W1C푡+ W2S푡−1 + 푦푡−1),
in which C푡is the contextual semantic representation with both global and local semantic information at current time step, S푡−1 is the decoder hidden state of last time step, 푦푡−1 is the word
generated at last time step, and M is the global semantic vector of the original text.
Except for the poor capability of capturing long-term dependency in traditional pre-trained
language models, the problem of factual inconsistency between the generated content and source
text is also severe and has not been tackledby previous works. To reduce this phenomenon, Meng et
al. propose an end-to-end neural corrector model with a post-editing correction strategy, which
is pre-trained on artiﬁcial corrupted reference summaries. Dong et al. introduce a factual
correction framework containing a QA-span factual correction model and an auto-regressive one.
The QA-span correction model masks and replaces one entity at a time during the iteration process.
Speciﬁcally, given the source text 푥and a masked query푞= (푦′
1, ..., [푀퐴푆퐾], ...,푦′
푚), the correction
model needs to predict the answer span via 푝(푖= 푠푡푎푟푡) and 푝(푖= 푒푛푑), which are calculated based
on the hidden states of the top layer ℎ푖as:
푝(푖= 푠푡푎푟푡) = 푎푠푡푎푟푡
푗=0 exp(푞푠
, where 푞푠
푖= ReLU(푤⊤
in which 퐻is the number of hidden states in the encoder, and 푝(푖= 푒푛푑) is calculated in the similar
way. The auto-regressive correction model masks all entities at the same time without iteration.
Speciﬁcally, given the source text 푥and a masked query 푞= (푦′
1, ..., [푀퐴푆퐾]1, ..., [푀퐴푆퐾]푇, ..., 푦′
from a summary with 푇entities, the correction model runs 푇steps to predict the answer span
of each mask based on the corresponding masked token representation and its previously predicted entity representations. The entity representation s푒푛푡
at time step 푡is predicted based on
the argmax and mean pooling operations, as calculated by:
= Mean-Pool(h푝푠푡푎푟푡, h푝푒푛푑),
where 푝푠푡푎푟푡= arg max(푎푠푡푎푟푡
, ..., 푎푠푡푎푟푡
),푝푒푛푑= arg max =
푐표푢푛푡(푤푖푤푗) −훿
푐표푢푛푡(푤푖) × 푐표푢푛푡(푤푗),
where 푐표푢푛푡(푤푖,푤푗) is the number of a sequence of word 푤푖followed by 푤푗, 푐표푢푛푡(푤푖) is the
number of word 푤푖in the input document, and 훿is a constant to limit the number of phrases
formed by less frequent words. The second method is based on a Naive Bayes model, and the
probability of a unique phrase is measured by:
푃푟[푘푒푦|푇, 퐷] = 푃푟[푇|푘푒푦] × 푃푟[퐷|푘푒푦] × 푃푟[푘푒푦]
where 푃푟[푇|푘푒푦] is the probability that a key-phrase has a푇퐹×퐼퐷퐹score푇, 푃푟[퐷|푘푒푦] is the probability that it has a distance 퐷, and 푃푟[푇, 퐷] is the normalization factor. Bhatia et al. propose
a dynamic question answering generator to generate questions and answers for the mathematical
topic-quadratic equations. The randomization technique, ﬁrst order logic and automated deduction are used for the study.
In the past few years, driven by advances in deep learning, end-to-end neural models based on
Seq2Seq framework have attained more and more attentions and have shown better performances.
Serban et al. study the factoid questions generation with RNN to transduce facts into neural
language questions, and provide an enormous question-answer pair corpus. Yuan et al. use
a Seq2Seq model with teacher forcing to improve the training and adopts policy gradient in reinforcement learning to optimize the generated results. During supervised learning, in addition to
minimize the negative log-likelihood with teacher forcing, two additional signals are introduced
to prevent the model from generating answer words (L푠) and encourage the output variety (L푒)
as formulated below:
푝휃(푦푡= ¯푎|푦<푡, 퐷,퐴), L푒= 휆푒
A refers to the set of words appearing in the answer but not in the ground-truth question,
p푡refers to the full pointer-softmax probability of the 푡-th word, which enables the model to interpolate between copying from the source document and generating from shortlist. And during
reinforcement learning, the total reward 푅푃푃퐿+푄퐴is a combination of question answering reward
푅푄퐴(measured by F1 score) and question ﬂuency reward 푅푃푃퐿(measured by perplexity), as formulated below:
푅푃푃퐿+푄퐴= 휆푄퐴푅푄퐴( ˆ푌) + 휆푃푃퐿푅푃푃퐿( ˆ푌),
푅푃푃퐿( ˆ푌) = −2−1
푡=1 log2 푃퐿푀( ˆ푦푡| ˆ푦<푡),
푅푄퐴( ˆ푌) = F1( ˆ퐴,퐴),
where ˆ퐴= 푀푃퐶푀( ˆ푌) refers to the answer of the generated question by the Multi-Perspective
Context Matching (MPCM) model, 푃퐿푀is a language model. Zhao et al. study the paragraphlevel neural question generation by proposing a maxout pointer and gated self-attention networks,
which mainly deals with the problem that long text (mostly paragraphs) does not perform well in
the Seq2Seq model. In detail, the gated self-attention network contains two steps: 1) the encoded
passage-answer representation u푡is conducted matching against itself to derive the self matching
representation s푡at time step 푡:
s푡= U · a푠
푡= U · softmax(U⊤W푠u푡).
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
Dong and Li, et al.
And 2) the self matching representation s푡is combined with the original passage-answer representation u푡, which is then fed into a feature fusion gate to obtain the ﬁnal encoded passage-answer
representation ˆu푡at time step 푡:
ˆu = g푡⊙f푡+ (1 −g푡) ⊙u푡, where f푡= tanh(W푓[u푡, s푡]), g푡= sigmoid(W푔[u푡, s푡]).
However, the rich information lying in the answer has not been fully explored, which leads to
generating low-quality questions (e.g., having mismatched interrogative words with the answer
type, copying context words far and irrelevant to the answer, including words from the target
answer, and so on). To solve the problem, Sun et al. propose an answer-focused and positionaware model. It incorporates the answer embedding to explicitly generate a question word matching the answer type, and designs a position-aware attention mechanism by modeling the relative
distance with the answer, which guides the model to copy the context words that are more close
and relevant to the answer. Kim et al. propose to separate the target answer from the original
passage, in order to avoid the generated question copying words from the answer. Speciﬁcally, it
replaces the answer with a mask token, and introduces a keyword-net to extract key information
from the answer. Given the encoded answer representation ℎ푎and the context vector 푐푡of current
decoding step, the keyword feature of each layer of the keyword-net 표푙
푡can be formulated as:
푗, where 푝푙
푡푗= softmax((표푙−1
in which 표0
푡is initialized by 푐푡. And the decoding hidden state 푠푡of current timestep is calculated
푠푡= LSTM(푦푡−1,푠푡−1,푐푡,표퐿
where 푦푡−1 is the output token of previous timestep, 퐿is the layer number of the keyword-net.
Moreover, many works recently attempt to conduct multi-task learning with external related
tasks to further enhance the performance of question generation. Wang et al. introduce a
message passing mechanism to simultaneously learn the tasks of phrase extraction and question
generation, which helps the model be aware of question-worthy phrases that are worthwhile to be
asked about. Jia et al. conduct multi-task learning with paraphrase generation and question
generation, which can diversify the question patterns of the question generation module. During
training, the weights of the encoder are shared by all tasks while those of the ﬁrst layer of decoder
are shared with a soft sharing strategy, which is formulated by:
where D is a set of shared decoder parameters, 휃,휙refer to the parameters of the question generation task and paraphrase generation task, respectively. And a min-loss function is employed among
the golden reference question and several expanded question paraphrases, which is represented
Despite the outstanding performance achieved by previous methods, the rich structure information hidden in the passage is ignored, which can be used as an auxiliary knowledge of the
unstructured input text to improve the performance. Liu et al. adopt a graph convolutional
network (GCN) to identify the clue words in the input passage that should be copied into the target
question. Speciﬁcally, the GCN is constructed on the syntactic dependency tree representation of
each passage, and a Gumbel-Softmax layer is applied to the ﬁnal representation of GCN to sample
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
A Survey of Natural Language Generation
the binary clue indicator for each word. A sample y = (푦1, ...,푦푘) drawn from the Gumbel-Softmax
distribution is formulated as:
exp((log(휋푖) + 푔푖)/휏)
푗=1 exp((log(휋푗) + 푔푗)/휏)
where 휏is the temperature parameter, 휋푖is the unnormalized log probability of class 푖, and 푔푖is
the Gumbel noise formulated by:
푔푖= −log(−log(푢푖)), where 푢푖∼Uniform(0, 1).
Wang et al. construct a knowledge graph for each input sentence as the auxiliary structured
knowledge and aims to generate a question based on a query path from the knowledge graph. The
query representation learning is formulated as a sequence labeling problem for identifying the
involved facts to form a query, which is used to generate more relevant and informative questions.
Recently, pre-trained language models have achieved remarkable performances in question generation, far exceeding those of the previous RNN-based methods. Dong et al. propose a uni-
ﬁed model that is pre-trained with three types of natural language understanding or generation
tasks, namely unidirectional, bidirectional, and sequence-to-sequence prediction. Through the pretraining of these three tasks, the model’s question generation performance achieve signiﬁcant improvements on SQuAD. Xiao et al. propose an enhanced multi-ﬂow seq2seq pre-training
and ﬁne-tuning framework to alleviate the exposure bias, which consists of an inﬁlling generation
mechanism and a noise-aware generation method, which achieves state-of-the-art performances
on a wide range of datasets.
Distractor Generation. The researches mainly focus on generating multi-choice question
distractors for ontologies or articles. Traditional methods primarily use hand-crafted rules or ranking method for distractor generation. Stasaski et al. introduce a novel method with several
ontology- and embedding-based approaches. The graph structure of the ontology is used to create
complex problems linking diﬀerent concepts. Liang et al. introduce a ranking method with a
feature-based model and a neural net (NN) based model. The NN-based model consists of a generator 퐺and a discriminator 퐷, where 퐺generates distractors 푑based on a conditional probability
푃(푑|푞, 푎) given question stems 푞and answers 푎, and 퐷predicts whether a distractor sample comes
from the real training data or 퐺. The objective for 퐷is to maximize the log-likelihood as:
max휙E푑∼푃푡푟푢푒(푑|푞,푎) [log(휎(푓휙(푑|푞,푎)))] + E푑∼푃휃(푑|푞,푎) [log(1 −휎(푓휙(푑|푞,푎)))],
where 푓휙(푑,푞,푎) is an arbitrary scoring function parameterized by 휙. And each distractor 푑푖sampled by 퐺is based on another scoring function 푓휃(푑,푞,푎) as formulated as:
푝휃(푑푖|푞,푎) =
exp(휏· 푓휃(푑푖,푞,푎))
푗exp(휏· 푓휃(푑푗,푞,푎)),
where 휏is a temperature hyper-parameter. Then a cascaded learning framework is proposed to
make the ranking more eﬀective, which divides the ranking process into two stages to reduce the
candidates.
Recently, deep learning-based models are widely adopted due to its overwhelming performance.
For example, Chung et al. utilize the BERT modelto generate distractor with the auto-regressive
mechanism in a multi-tasking architecture. Additionally, Gao et al. express the task as a
sequence-to-sequence learning problem based on a hierarchical encoder-decoder network. In this
model, static and dynamic attention mechanisms are adopted on the top of the hierarchical encoding structure, and a question-based initializer is used as the start point to generate distractors in
the decoder. The question 푞, the answer 푎and the word vectors in the 푖-th sentence (w푖,1, ..., w푖,푚)
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
Dong and Li, et al.
are ﬁrst encoded via three separate bidirectional LSTM networks into (q1, ..., q푙), (a1, ..., a푘), and
푖,1, ..., h푖,푚). Then another bidirectional LSTM is applied on the encoded word representations to
derive the contextualized sentence representation (u1, ..., u푛), and an average pooling layer is applied to derive their entire representations q, a, and s푖. After that, the static attention distribution
훾푖can be derived through a matching layer and a normalization layer as:
훾푖= softmax(표푖/휏), where 휏= sigmoid(wq⊤q + 푏푞), 표푖= 휆푞s⊤
푖W푚q −휆푎s⊤
푖W푚a + b푚.
In the decoder size, the decoder generates the hidden state h푑
푡at the푡-th time step through an LSTM
network. Then the sentence-level and word-level dynamic attention 훽푖and 훼푖,푗are formulated as:
푡, 훼푖,푗= h푒
Finally, the static and dynamic attentions are combined into ˜훼푖,푗to reweight the article token
representations and predict the probability distribution 푃푉over vocabulary 푉:
푃푉= softmax(W푉˜h푑
푡+ b푉), where ˜h푑
푡= tanh(W˜h[h푑
푡; c푡]), c푡=
푖,푗, ˜훼푖,푗=
푖,푗훼푖,푗훽푖훾푖
However, the answer interaction is not considered by previous works and the incorrectness of
the generated distractors cannot be guaranteed. To address this problem, Qiu et al. propose
a framework consisting of reforming modules and an attention-based distractor generator, which
is the state-of-the-art method on most widely adopted datasets (e.g., RACE). The reforming modules use the semantic distances to constrain the eﬀect of words that are strongly related to the
correct answer, and the distractor generator leverages the information of the reformed question
and passage to generate the initial state and context vector respectively. In detail, three contextual encoders are ﬁrst applied to encode the passage, question and its answer into P, Q and A,
then an attention mechanism and a fusion kernel are leveraged to enrich the question and answer
representations into ˜Q and ˜A, where ˜Q is formulated by:
˜Q = Fuse(Q, ¯Q) = tanh([Q; ¯Q; Q −¯Q; Q ◦¯Q]W푓+ b푓),
¯Q = Attn(Q, P)P = softmax( QP푇
In the reforming question module, the reformed question ¤Q푖is calculated through a self-attend
layer and a gate layer as:
¤Q = Gate( ˜Q푖, ˜v푎) ˜Q푖= ( ˜Q푖W푞
˜v푎= SelfAlign( ˜A) = softmax( ˜AW푎)⊤˜A.
And in the reforming passage module, the reformed passage ˜P is calculated by:
˜P = Fuse(¤P, ¯P), ¯P = Attn(¤P, ¤Q) ¤Q, ¤P푖= Gate(P푖, ˆv푎)P푖,
ˆv푎= SelfAlign( ˆA), ˆA = Fuse( ˜A, ¯A), ¯A = Attn( ˜A, ˜Q) ˜Q.
Maurya et al. use a single encoder to encode the input triplet and three decoders to generate
three distractors. The encoder employs SoftSel operation and a gated mechanism to capture the
semantic relations among the elements of the input triplet.
In addition, there are also many other scenarios for distractor generation. Srinivas et al. 
develop a semi-automatic tool to help teachers quickly create multiple choice questions on code
understanding. The tool ﬁrst captures each code structure in the form of an abstract syntac tree,
and then train a code model that maps functions to vectors. Ren et al. create a model based
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
A Survey of Natural Language Generation
Table 2. Natrual language generation models for text expansion.
Description
Short Text Expansion
Associated-Query-based Query Expander 
ExpaNet 
Retrieval + Memory Network
FC-LSTM 
LDA + RNN + Ranking
Fiction Sentence Expander 
Topic-to-Essay Generation
MTA-LSTM 
RNN + Global Coherence
SRENN 
RNN + Retrieval + Global Coherence
UD-GAN 
RNN + GAN + RL
KB-based Topic-to-essay Generator 
CNN + RNN + GAN + RL + KB
SCTKG 
CNN + RNN + GAN + RL + KG
on a context-sensitive candidate set generator and a distractor selector for cloze-style multiple
choice questions. The model ﬁrst uses the correct answer as the key, combines the LDA model
to mine the topic of the context, ﬁnds similar words in the semantic database, and then selects a
speciﬁed number of misleading items according to a ranking model. Speciﬁcally, the probability
distribution over all entites subsumed by the concepts in 퐶is calculated based on the posterior
probability 푝(푐|푎,푞) as:
푝푖= 푝(푑푖|푎,푞) ∝
푝(푑푖|푐)푝(푐|푎,푞), where 푝(푐|푎,푞) ∝푝(푐|푎)
in which 푐is the concept, 휋푎,푞is the topic distribution of complete sentence formed by the stem
and key, 훾푐is the topic distribution of concept 푐, 푝(푐|푎) is the prior probability of 푎belonging to
푐, 퐾is the total number of topics, and 푝(푑|푐) is the typicality. Patra et al. develop a system
to generate named entity distractors. The system performs two types of similarity computation
namely statistical and semantic. To speed up the distractor selection procedure, a hierarchical
clustering method is proposed to represent the entities, where the entity similarities are embedded
in a tree structure. The distractors are selected from the nearby entities of the correct answer of
the question in the tree. Speciﬁcally, the statistical distance between the numeric attributes is
calculated by:
푆푖푚(푃,푄) = 1 −1
max(푃푖,푄푖),
where 푃and 푄represent two vectors corresponding to the target entities, 퐿is the total number of
numeric attributes. The hierarchical distance between two entities (푥,푥′) is normalized by:
푆푖푚(푥,푥′) =
푑1(푥,푥) ⊙푑1(푥′, 푥′)
where 푑1(푥,푥′) is the highest tree level connecting 푥and 푥′. And the semantic similarity score
between the key 푥and a candidate distractor 푥′ is formulated as:
푆푖푚(푥,푥′) = |(푡푟푖푝푙푒푡푖∈푥)|&(푡푟푖푝푙푒푡푖∈푥′)|푖
|푡푟푖푝푙푒푡푗∈푥|푗
where the normalization factor is the size of the triplet set corresponding to the key.
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
Dong and Li, et al.
TEXT EXPANSION
The main purpose of text expansion is to inﬂate the short texts to longer ones that contains more
abundant information, which can be divided into two aspects: short text expansion and topic-toessay generation. Short text expansion aims to expand a short text into a richer representation
based on a set of long documents. Topic-to-essay generation aims at generating human-like diverse, and topic-consistent paragraph-level text with a set of given topics. The most representative
methods for each subtask are shown in Table 2.
Short Text Expansion. There are mainly four datasets for short text expansion as listed below.
Wikipedia. constructs this dataset through a snapshot of English Wikipedia. The titles
of Wikipedia articles are regarded as short texts, and the abstract of all Wikipedia articles are
leveraged to construct the related long documents. This dataset contains 30,000 short texts and
4,747,988 long documents in total.
DBLP. uses the DBLP bibliography database to construct this dataset. The titles of computer science literature represent short texts, and the abstracts of all papers are collected to construct the corresponding long documents. Statistically, this dataset consists of 81,479 short texts
and 480,558 long documents.
Programmableweb. establishes a real-world dataset for service recommendation and description expansion, which is crawled from programmableweb.com in 2013 and 2016. The entire
dataset contains 16012 APIs, 7816 Mashups and 16449 links between them.
Fiction Corpus. creates an English ﬁction corpus, which is obtained by applying sentence
compression techniques on a modern ﬁction corpus scraped from online resources. Each story sentence has a corresponding compression. In total, the dataset contains around 17,000,000 sentences.
Topic-to-Essay Generation. There are primarily ﬁve datasets for topic-to-essay generation
as listed below.
ESSAY. The ESSAY dataset is a large collection of topic compressions crawled from the Internet. The topic words are extracted by TextRank. This datast totally contains 305,000 paragraphlevel essays.
ZhiHu. constructs this dataset by crawling from a Chinese question-and-anwering website
called ZhiHu, which consists of 55,000 articles. The topic words of each article are speciﬁed by
users in the community.
Moview Reviews. The Stanford Sentiment Treebank (SST) dataset contains 11,855 single
sentences of movie reviews. This dataset has two sentiment classes for each review, and has a set
of fully labeled parse trees.
Beer Reviews. creates this dataset by crawling from the beer review website BeerAdvocate.
In total, this dataset has 1,586,259 ratings on 66,051 items that are scored by 33,387 users.
Customer Reviews. constructs a customer review dataset consisting of ﬁve electronics products, which is collected from Amazon.com and C|net.com. There are totally 1,886 items in this
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
A Survey of Natural Language Generation
Short Text Expansion. Early works primarily use statistical similarity for short text expansion. Billerbeck et al. propose a method about the query expansion using associated queries for
web search engines. The main method is to associate a query that closely matches the document
in a given log containing a large number of queries. Then, the query associated with the query
document is a reasonable description of the document and can be used to expand the query text.
Beneﬁt from the development of deep learning, many end-to-end frameworks based on neural
networks have been developed. Tang et al. propose an end-to-end solution based on deep
memory network for short text extension. In this work, the original short text 푞is ﬁrstly used
as a query to search for a set of potentially relevant long documents 퐶푞, which will be used as
the material for text expansion, from an external large collection 퐶. In the following process, the
short text 푞is represented as the average vector of words in it, i.e. −→푞, and each document is also
represented by the average vector of words belong to it, i.e. −→푑. To further identify the relevant
documents in 퐶푞, both soft attention and hard attention mechanism are utilized, the information
read from the document set can be written as:
where 푔푖follows the Gumbel(0,1) distribution, and 휏is the temperature hyperparameter. Then the
two sources of information, −→푞and −→표are integrated by GRU as follows:
−→푧= 휎(W(푧)−→푞+ U(푧)−→표),
−→푟= 휎(W(푟)−→푞+ U(푟)−→표),
′ = tanh(W−→푞+ −→푟◦U−→표),
′ = (1 −−→푧) ◦−→푞+ −→푧◦−→표
where ◦means element-wise multiplication, both the Sigmoid function 휎(푥) and 푡푎푛ℎ(푥) are operated on element-wise. The output −→푞
′ is the expanded representation of the input short text 푞. This
process can be repeated several times for the same 푞to continuously extend its representation to
mimic the human behavior when querying a piece of short text.
Shi et al. present a text expansion method for service recommendation system. The description of services is ﬁrst expanded at sentence level by a probabilistic topic model, in this process, the
similarity between target sentence 푢and another sentence 푣from existed corpus can be caculated
푆푖푚푖푙푎푟푖푡푦(푢,푣) = 휇· 퐷퐽푆(푢, 푣) + (1 −휇) · 퐷퐽푆(푆푢,푆푣),
where 푆푢and 푆푣are descriptions contain 푢and 푣, respectively. 휇is a parameter used to balance
weights of sentence and description on the ﬁnal similarity measurement. 퐷퐽푆is the function of
JS divergence which used to measure the similarity between two items; for more details, readers
can refer to . So for each target sentence, a collection of similar sentences can be found and
ranked in descent order to select top N most similar ones for description extension.
Safovish et al. design a neural sentence expander trained on a corpus of ﬁction sentence
compressions for the task of sentence expansion and enhancement, which is the state-of-the-art
method on existing datasets (e.g., Fiction Corpus). In this method, a Seq2Seq model is used to
predict sentences from the original input. To tackle the problem of copying input words during
generation process, the modiﬁed negative log-likelihood loss function is used to increase the signiﬁcance of learning new words. Speciﬁcally, the modiﬁed cross-entropy of generating is caculated
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
Dong and Li, et al.
(1 + 휆I푇−푆(푤푡))log푝(푤푡|푤1, ..., 푤푡−1),
in which I denotes the indicator function for a set, 푇the ground truth, 푆the source tokens, 휆is
a parameter which controls the learning importance of the words in target sentence while not in
original input. With this modiﬁcation, the model no longer degenerates to copying its input and
can be trained as desired. In addition, to increase the novelty of generated sentence, Safovish et al.
 also develop a controlled sampling method for the decoding process so that the model can
generate diverse output.
Topic-to-essay Generation. With the popularity of deep learning, the RNN-based Seq2Seq
framework has been widely used in this ﬁeld. Many works have been proposed to improve the
traditional Seq2Seq framework.
To improve the global coherence of the generated essays, Feng et al. propose an LSTM
model with attention mechanism for essay generation. The main idea of this work is to maintain
a topic coverage vector, each dimension of which represents the degree to which a topic word
needs to be expressed in future generation, to adjust the attention policy, so that the model can
consider more about unexpressed topic words. Speciﬁcally, the topic coverage vector is updated by
parameter 휙푗, and it can be regarded as the discourse-level weight of 푡표푝푖푐푗, the word embedding
of topic word 푖. Topic coverage vector 퐶푡is initialized as a k dimensional vector, i.e. 퐶0, k is the
number of input topic words, and each value of퐶0 is 1.0. At time step 푡in generation process, each
element 푐푡,푗is updated as follows:
푐푡,푗= 푐푡−1,푗−훼푡,푗
in which 훼푡,푗= exp(푔푡푗)/Í퐾
푖=1 exp(푔푡푖) is the attention weight of topic word 푖at time step 푡, and
푔푡푗is the attention score on 푡표푝푖푐푗at time step 푡, which is caculated as:
푔푡푗= 푐푡−1,푗푣푇
푎tanh(푊푎ℎ푡−1 + 푈푎푡표푝푖푐푗),
where ℎ푡−1 is the hidden representation of the LSTM at time step 푡−1, and 푣푎, 푊푎, 푈푎are all
parameters to be optimized. Therefore, the probability of the next word 푦푡can be deﬁned as:
P(푦푡|푦푡−1,푇푡,퐶푡) = softmax(linear(ℎ푡)),
where the topic representation 푇푡is formulated as 푇푡= Í퐾
푖=1 훼푡푗푡표푝푖푐푗.
Wang et al. propose an enhanced neural network based on self-attention and retrieval
mechanisms, the encoder and decoder are constructed with self-attention to model longer dependence. And to alleviate the duplication problem, a retrieval process is adopted to collect topicrelated sentences as an aid for essay generation. Speciﬁcally, input topic words are divided into 푚
groups, then the material 푀= {푆1, ..., 푆푚} can be collected based on cosine distance between the
topic group and sentences in the corpus which is created by dividing the training set, where 푆푖is
a sentence that corresponds to the 푖-th topic group. For the diﬀerent sequence relations between
topic words and material sentences, two encoders with the same structure but diﬀerent parameters are used to get the hidden representation 퐻푇표푝푖푐= ℎ푇
1 , ..., ℎ푇
and 퐻푀푎푡푒푟푖푎푙= ℎ푀
1 , ..., ℎ푀
based on which the hidden states of generated of essay words 퐻= {ℎ1, ..., ℎ푛} are obtained by
essay decoder. Finally the output probabilities of each essay word can be computed as:
푝(푦푡|푦푡−1,푇, 푀) = softmax(linear(ℎ푡)).
Meanwhile, Generative Adversarial Net (GAN) has shown promising results for topic-to-essay
generation, which is eﬀectively applied in including a GAN model and two-level discriminators. The ﬁrst discriminator guides the generator to learn the paragraph-level information and
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
A Survey of Natural Language Generation
sentence syntactic structure with multiple LSTMs, and the second one processes higher level information such as topic and sentiment for essay generation. Then the reward is calculated based
on results of two discriminators, and the generator 퐺휃tries to maximize expected reward from the
initial state till the end state via the formulation:
퐸(푅푡|푆푡−1,휃) =
퐺휃(푦푡|푌)[휆(퐷휙(푌)) + (1 −휆)퐷훾(푌)],
where 휆is a manually set weight, 푌is a complete sequence, 푅푡is the reward for a whole sequence,
퐷휙and 퐷훾are the ﬁrst and second discriminator, respectively.
However, the generated essays of previous works are still lack of novelty and topic-consistency,
based on which, some works leverage the external knowledge to solve this problem. Yang et al.
 introduce a model with the aid of commonsense knowledge in ConceptNet, in detail, each
topic is used as a query to retrive 푘neighboring concepts with pre-trained embeddings stored as
commonsense knowledge in a memory matrix M0, in the decoding phase, the generator 퐺휃refers
to the memory matrix for text generation, the hidden state of the decoder at time-step 푡is:
푠푡= LSTM(푠푡−1, [푒(푦푡−1);푐푡;푚푡]),
where [;] means the concatenation of vectors, 푦푡−1 is the word generated at time-step 푡−1. 푐푡is
the context vector that is computed by integrating the hidden representations of the input topic
sequence, and 푚푡is the memory vector extracted from M푡based on the attention mechanism. As
the generation progresses, the topic information that needs to be expressed keeps changing, which
requires the memory matrix to be dynamically updated, so for each memory entry M푖
candidate update memory eM
푡is computed as:
푡= tanh(U1M푖
푡+ V1푒(푦푡)),
where U1 and V1 are trainable parameters. To determine how much the 푖-th memory entry should
be updated, the adaptive gate mechanism is adopted:
푡= sigmoid(U2M푖
푡+ V2푒(푦푡)),
where U1 and V1 are trainable parameters. M푖
푡is updated by:
푡+1 = (1 −푔푖
in which 1 refers to the vector with all elements 1 and ⊙denotes point-wise multiplication. Following their work, Qiao et al. propose a topic-to-essay generator based on the conditional
variational auto-encoder framework to control the sentiment, and introduces a topic graph attention mechanism to suﬃciently use the structured semantic information which is ignored in ,
so that the quality of generated essays is further improved and reaches the state-of-the-art level
at present on the mainstream datasets (e.g., ZhiHu).
TEXT REWRITING AND REASONING
The target of text rewriting and reasoning is to make a reversion of the text or apply reasoning
methods to generate responses, which mainly contains two subtopics: text style transfer and dialogue generation.
Text style transfer is the method to transform the attribute style of the sentence while preserving its attribute-independent content. Dialogue generation aims to automatically generate approximate answers to a series of given questions in a dialogue system. We show several classical
methods for each subtask in Table 3.
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
Dong and Li, et al.
Table 3. Natrual language generation models for text rewriting and reasoning.
Description
Text Style Transfer
Spearean Modern Language Translator 
ARAE 
RNN + GAN + WAE
FM-GAN 
Unpaired Sentiment-to-Sentiment Translator 
RNN + RL + Unsupervised
Non-Oﬀencive Language Translator 
CNN + RNN + Unsupervised
CNN + RNN + Unsupervised
DualRL 
RNN + RL + Unsupervised
Exploration-Evaluation-based Text Style Translator 
RNN + Metrics Design
Evaluating Style Transfer for Text 
Embedding + CNN + RNN + Metrics Design
Error Margins Matter in Style Transfer Evaluation 
VAE + RNN + Metrics Design
PPVAE 
Dialogue Generation
MARM 
GAN-AEL 
CNN + RNN + GAN
RNN + Context-response Interaction
RNN + KG + Retrieval + Context-response Interaction
Post-KS 
Transformer + KB
푃2 BOT 
Transformer + Multi-task Learning + Persona
KnowledGPT 
Transformer + Knowledge Selection
Transformer + Multi-task Learning
TransferTransfo 
Transformer + Multi-task Learning
Text Style Transfer. The following four datasets are widely applied for the task of text style
Yelp Review. The Yelp Review dataset is provided by the Yelp Dataset Challenge1, which contains
a large amount of business review texts. This dataset contains 1.43M, 10K, and 5K pairs for training,
validation, and testing, respectively.
Amazon Food Review. creates this dataset by crawling reviews from the Fine Foods category
of Amazon. This dataset contains 367K, 10K, and 5K pairs for training, validation, and testing,
respectively.
EMNLP2017 WMT News. picks the News section data from the EMNLP2017 WMT2 Dataset,
which is a large long-text corpus consists of 646,459 words and 397,726 sentences. After being preprocessed, this dataset contains 278,686 and 10,000 sentences for training and testing, respectively.
GYAFC. The Grammarly’s Yahoo Answers Formality Corpus (GYAFC) is a large corpus
for formality stylistic transfer. The informal sentences are collected from Yahoo Answers3 with
domains of Entertainment & Music and Family & Relationships, and the corresponding formal
sentences are created using Amazon Mechanical Turk. This dataset totally contains 106,000 sentence pairs.
Dialogue Generation. There are four popular datasets for dialogue generation as shown below.
DailyDialog. The DailyDialog dataset is a high-quality multi-turn dialog dataset containing
daily conversations. The dialogues in the dataset is formally written by human with reasonable
speaker turns, and often concentrate on a certain topic. Statistically, this dataset contains 13,118
1 
2 
3 
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
A Survey of Natural Language Generation
multi-turn dialogues, nearly 8 average speaker turns per dialogue, and about 15 average tokens
per utterance.
UDC. The Ubuntu Dialogue Corpus (UDC) is created based on the two-person conversations about Ubuntu-related problems in the Ubuntu chat logs4 from 2004 to 2015. This dataset
contains about 1 million multi-turn dialogues, over 7 million utterances, and 100 million words.
Persona-chat. The Persona-chat dataset is an engaging and personal chit-chat dialogue
dataset collected by Amazon Mechanical Turk. Each of the paired crowdworkers condition their
dialogue on a given provided proﬁle. This dataset contains 164,356 utterances in total.
Wizard-of-Wikipedia. The Wizard-of-Wikipedia dataset is a large crowd-sourced collection
for open-domain dialogue. Each of the paired speakers conduct open-ended chit-chat, and one of
the speakers need to link the knowledge to sentences from existing Wikipedia articles connected
to the topic. This dataset contains 22,311 dialogues with 201,999 turns.
Text Style Transfer. In this ﬁeld, the RNN-based Seq2Seq framework and deep latent variable model are broadly adopted. Jhamtani et al. design a copy-enriched sequence-to-sequence
model to transform text from modern English to Shakespearean English. The model ﬁrst uses a
ﬁxed pre-trained embedding vector to represent each token, and uses a bidirectional LSTM to encode sentences. In this work, −−−−−−−→
퐿푆푇푀푒푛푐and ←−−−−−−−
퐿푆푇푀푒푛푐represent the for- ward and reverse encoder.
represent hidden state of encoder model at step t. The following equations describe the model:
= −−−−−−−→
퐿푆푇푀푒푛푐(ℎ푒푛푐
푡−1, 퐸푒푛푐(푥푡)),
= ←−−−−−−−
퐿푆푇푀푒푛푐(ℎ푒푛푐
푡+1, 퐸푒푛푐(푥푡)),
In this work, only the forward and backward encoder states are added, and the standard connection is not used because it does not add additional parameters.Then a mixture model of RNN
and pointer network are employed to transfer the text style. The pointer module provides location based attention, and output probability distribution due to pointer network module can be
expressed as follows:
Zhao et al. propose an adversarially regularized autoencoder framework to generalize the
adversarial autoencoder, which combines a discrete autoencoder with a regularized latent representation of GAN, and can be further formalized by the Wasserstein autoencoder.The model is
trained with coordinate descent across: (1)the encoder and decoder to minimize reconstruction,
(2) the critic function to approximate the 푊term, (3) the encoder adversarially to the critic to
minimize 푊:
휙,휓퐿푟푒푐(휙,휓) = 퐸푋∼푃∗[−log푝휓(푥|푒푛푐휓(푥))],
푤∈푊퐿푐푟푖(푤) = 퐸푋∼푃∗[푓푤(푒푛푐휓(푥))] −퐸e푧∼푃푧[푓푤(e푧)],
휙퐿푒푛푐(휙) = 퐸푋∼푃∗[푓푤(푒푛푐휓(푥))] −퐸e푧∼푃푧[푓푤(e푧)].
Here 푒푛푐휓is a deterministic encoder function. 푃푧is the the prior distribution, and 푓푤(e푧) is the
critic/discriminator.
4 
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
Dong and Li, et al.
Chen et al. utilize optimal transport to improve the ability of traditional GAN in processing
descrete texts with the objective of feature-mover’s distance. In this work, the feature-mover’s
distance(FMD) between two sets of sentence features is then deﬁned as:
퐷퐹푀퐷(푃푓, 푃푓′) = min
푇푖푗· 푐(푓푖, 푓푗′) = min
푇≥0 < 푇,퐶>,
푛are the constraints, and <, > represents the Frobenius dotproduct. In this work, the transport cost is deﬁned as the cosine distance: 푐(푓푖, 푓푗′) = 1 −
and 퐶is the cost matrix.
However, the problem of lacking supervised parallel data has not been well studied by the above
works. To tackle this problem, many unsupervised methods have been proposed. Xu et al. 
propose a cycled reinforcement learning approach through the cooperation between the neutralization and emotionalization modules. The neutralization module extracts the non-emotional part
of the sentence with a single LSTM and a self-attention based sentiment classiﬁer, and the emotionalization module generates emotional words and add them to the semantic content with a
bi-decoder based encoder-decoder framework.
Santos et al. propose an unsupervised style transfer model to convert oﬀensive language
into non-oﬀensive one. A single collaborative classiﬁer is used to train the encoder-decoder network, and an attention mechanism with a cycle consistency loss is adopted to preserve the content.
Prabhumoye et al. realize the style transfer of gender, political slant and sentiment through
an unsupervised back-translation method. The transferring process is divided into two stages. The
ﬁrst stage learns the latent representation with back-translation of the input sentence through a
language translation model, and the second stage adopts an adversarial generation technology to
enable the output to match the desired style.The latent representation with back-translation of the
input sentence can be described as:
푧= 퐸푛푐표푑푒푟(푋푓;휃퐸),
where, 푥푓is the sentence 푥in language 푓. 휃퐸represent the parameters of the encoder of language
푓→language 푒translation system
Luo et al. reformulate the traditional unsupervised style transferring task as a one-step
mapping problem, and propose a dual reinforcement learning framework to train the source-totarget and target-to-source mapping models. In this work, two reward methods that can evaluate
style accuracy and content preservation separately are proposed. 푅푠= 푃(푠푦|푦
′;휓) formulate the
style classiﬁer reward where 휓is the parameter of the classiﬁer and is ﬁxed during the training
process, and 푅푐= 푃(푥|푦
′;휙) represents the reward for preserving content. To encourage the model
to improve both the content preservation and the style accuracy, the ﬁnal reward is the harmonic
mean of the above two rewards:
푅= (1 + 훽2)
(훽2 · 푅푐) + 푅푠
where 훽is a harmonic weight aiming to control the trade-oﬀbetween the two rewards.
Moreover, another challenge in this ﬁeld is that there does not exist reliable evaluation metrics,
which is neglected by previous works. To alleviate this issue, Fu et al. propose two aspects
of evaluation metrics to measure the transfer strength and content preservation of style transfer.
The transfer strength aims to evaluate whether the style is transferred through an LSTM-sigmoid
classiﬁer. The style is deﬁned in (30). This classiﬁer is based on keras examples2. Transfer strength
accuracy is deﬁned as 푁푟푖푔ℎ푡
푁푡표푡푎푙, 푁푡표푡푎푙is the number of test data, and 푁푟푖푔ℎ푡is the number of correct
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
A Survey of Natural Language Generation
case which is transferred to target style.
푝푎푝푒푟(푝표푠푖푡푖푣푒)
표푢푡푝푢푡≤0.5
푛푒푤푠(푛푒푔푎푡푖푣푒)
표푢푡푝푢푡≥0.5 .
The content preservation is used to evaluate the similarity between source and target texts and
is calculated by the embedding cosine distance. Content preservation rate is deﬁned as cosine
distance (31) between source sentence embedding 푣푠and target sentence embedding 푣푡.
∥푣푠∥· ∥푣푡∥.
Mir et al. specify three aspects of evaluation metrics including style transfer intensity, content preservation, and naturalness. The style transfer intensity is measured by Earth Mover’s Distance, the content preservation is calculated by METEOR and embedding-based metrics, and the
naturalness is obtained by an adversarial evaluation method.
It is also instructive that Tikhonov et al. also point out the three signiﬁcant problems encountered in the evaluation metrics of style transfer. These problems mainly illustrate that the
measures of style accuracy and content preservation are often diﬀerent in various style transfer
tasks. Therefore, they propose to take BLEU between input and human-rewritten texts into consideration, so as to better measure the performance of style transfer models. Additonally, Duan
et al. propose the Pre-train and Plug-in Variational Autoencoder (PPVAE), which is a modelagnostic framework towards ﬂexible conditional text generation and consists of PretrainVAE and
PluginVAE, where PretrainVAE aims to learn the original style of the sentence, and PluginVAE
aims to learn the latent space of new style. The PPVAE achieves the state-of-the-art performance
on the Yelp Reviews dataset.
Dialogue Generation. The RNN-based or GAN-based Seq2Seq model is widely leveraged to
handle this task. Zhou et al. propose a mechanism-aware neural machine based on a probabilistic RNN-based Seq2Seq framework. The model ﬁrst uses latent embeddings to represent the
corresponding mechanisms, then an encoder-diverter-decoder framework is leveraged to generate mechanism-aware context. In this study, there are 푀latent mechanisms 푀푖푀
푖=1 for response
generation. Then, 푝(푦|푥) can be expanded as follows:
푝(푦,푚푖|푥) =
푝(푚푖|푥)푝(푦|푚푖,푥),
where 푝(푚푖|푥) represents the probability of the mechanism 푚푖conditioned on 푥. This probability
actually measures the degree that 푚푖can generate the response for x. The bigger of this value is,
the more degree that the mechanism 푚푖can be used to generate the responses for 푥. Additionally,
푝(푦|푚푖,푥) measures the probability that the response y is generated by the mechanism 푚푖for 푥.
With the modeling of 푝(푚푖|푥) and 푝(푦|푚푖, 푥) the objective of likelihood maximization, namely
log푝(푦|푥) =
푝(푚푖|푥)푝(푦|푚푖,푥),
is used to learn the mechanism embeddings 푀푖푀
푖=1 and other model parameters.
Xu et al. introduce a GAN framework comprising a generator, a discriminator, and an
approximate embedding layer to generate informative responses. The generator uses a Seq2Seq
model with GRU to generate responses, and the discriminator uses a convolutional neural network
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
Dong and Li, et al.
to judge the diﬀerence between human responses and machine responses. In the approximate
embedding layer, the overall word embedding approximation is computed as:
푒푗· softmax(푊푝(ℎ푖+ 푍푖) + 푏푝)푗,
where 푤푝and 푏푝are the weight and bias parameters of the word projection layer, respectively,
and ℎ푖is the hidden representation of word 푤푖, from the decoding procedure of the generator 퐺.
However, previous works ignore the semantic and utterance relationships between the context
and response, and thus the obtained responses are not satisfying. To solve this problem, Luo et al.
 propose an auto-encoder matching model with a mapping module. In this model, two autoencoders are leveraged to learn the semantic representations, and the mapping module is used to
learn the utterance-level dependency between the context and response. In this mapping module,
for simplicity, there is only a simple feedforward network for implementation. The mapping module 푀훾transforms the source semantic representation ℎto a new representation 푡. To be speciﬁc,
we implement a multi-layer perceptron (MLP) 푔(·) for 푀훾and train it by minimizing the L2-norm
loss 퐽3(훾) of the transformed representation 푡= 푔(ℎ) and the semantic representation of target
response 푠:
Li et al. design a dual encoder model with an attention mechanism and a graph attention
network. The attention mechanism is responsible of capturing the relationship between context
and response, and the graph attention network is used to integrate the knowledge connections
of domain words. The concept representation in the domain knowledge is constructed by a series
of triples, 퐺(푥) = {푇1,푇2, ...,푇푛} where 푇푖has the same concept node 푢but diﬀerent neighbor
concept 푣and the graph representation of the concept 푔(푥) can be calculated by graph attention
mechanism as:
푖], where 훼푇푖=
푗=1 푒푥푝(훽푇푖), 훽푇푖= ReLU([(푢푒
in which (푢푖,푟푖, 푣푖) = 푅푖∈퐺(푥)is the i-th triple in the dataset.
Some researchers try to improve the response quality by incorporating external knowledge
bases, and propose many strategies to select appropriate knowledge. Lian et al. present a
knowledge selection mechanism by separating the posterior distribution from the prior distribution. The distance between the posterior and prior distributions are minimized by the KL divergence during training, and during inference, the knowledges are selected and incorporated into
the response based on the prior distribution.The Kullback-Leibler divergence loss (KLDivLoss),
to measure the proximity between the prior distribution and the posterior distribution, which is
deﬁned as follows:
푝(푘= 푘푖|푥,푦) log 푝(푘= 푘푖|푥,푦)
푝(푘= 푘푖|푥) ,
where 휃denotes the model parameters.
Jiang et al. propose a knowledge augmented response generation model to improve the
knowledge selection and incorporation. The model consists of a divergent knowledge selector and
a knowledge aware decoder, where the selector conducts a one-hop subject reasoning over facts
to reduce the subject gap in the knowledge selection, and the decoder is used to eﬃciently incorporate the selected fact. In addition, Liu et al. concern about the importance of conversational
understanding for the high-quality chit-chat systems and propose the Persona Perception Bot . Diﬀerent from other existing models, 푃2 BOT focus on a important and previously overlooked concept, mutual persona perception, which is more appropriate to describe the process of
information exchange that enables interlocutors to understand each other. The 푃2 BOT is also the
current state-of-the-art model on the Persona-chat dataset.
Recently, pre-trained language models (e.g., BERT) have shown signiﬁcant improvements over
traditional RNN-based methods in many NLP tasks and are also applied to this task. Wang et
al. propose an encoder-decoder framework containing a BERT encoder and a transformer
decoder. The encoder is used to learn semantic representations for both unstructured text and
conversational history, and the decoder is leveraged to generate the dialogue response.In the encoder of this work,the input embedding is the sum of its token embedding, knowledge indicating
embedding and position embedding:
퐼(푥푖) = 퐸(푥푖) +푇(푥푖) + 푃(푥푖),
where 퐸(푥푖), 푇(푥푖), 푃(푥푖) are word embedding, knowledge indication embedding and position
embedding, respectively. The input embeddings are then fed into BERT model to get the knowledge
and dialogue history encoding representations.
Bao et al. design a pre-training framework with discrete latent variables. The pre-training
tasks include repsonse generation and latent act recognition, which are jointly pre-trained through
a uniﬁed network with shared parameters. Furthermore, inspired by the core idea of transfer learning, Wolf et al. propose the TransferTransfo, which uses the paradigm of transfer learning
to ﬁne-tune the powerful transformer models. The speciﬁc ﬁne-tuning tasks they select include:
language modeling task, next utterance retrieval task, and generation task. Diﬀerent ﬁne-tuning
tasks endow TransferTransfo with generalization performance for dialogue generation tasks in
diﬀerent scenarios.
FROM IMAGE TO TEXT GENERATION
The image-based text generation aims at explaining or summarizing the visual concept of the given
image, which mainly consists of three parts: image caption, video caption, and visual storytelling.
The purpose of image captioning is to generate summaries from an image. Based on image captions, video caption aims to generate the summary of a series of images. Visual storytelling not
only identiﬁes the correlation between objects in a single picture but also gives the logical relationship between consecutive sequential images. It should be noted that the language generation
component of VQA model is relatively similar to that of image caption. There exists the
main distinction that current VQA systems are focused on reasoning process and mainly designed to choose answers from a given candidate answer set, which is not quite
related to the natural language generation. Several popular methods for each subtask are shown
in Table 4.
Image Caption. The literature review of the image caption datasets is shown below.
Flickr30k. contains 31,783 images collected from Flickr. Most of these images depict humans
performing various activities. Each image is paired with 5 crowd-sourced captions.
COCO. is the largest image captioning dataset, containing 82,783, 40,504 and 40,775 images
for training, validation and test respectively. This dataset is more challenging, since most images
contain multiple objects in the context of complex scenes. Each image has 5 human annotated
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
Dong and Li, et al.
Table 4. From Image to Text Generation
Description
Image Caption
Show and Tell 
CNN + LSTM
Faster-RCNN + LSTM
Knowing When to Look 
CNN + LSTM
Exploring Visual Relationship for Image Captioning 
Faster-RCNN + LSTM + GCN
Self-Critical Sequence Training 
CNN + LSTM + RL
Auto-Encoding Scene Graphs 
CNN + LSTM + GCN + RL
Transformer + Multimodal
Video Caption
CNN + LSTM
S2VT 
CNN + LSTM + Knowledge
Dense Caption Events 
CNN + LSTM + Daps
Masked Transformer 
CNN + TCN + Transformer
Hierarchical Reinforcement Learning 
CNN + LSTM + RL
Adversarial Inference 
CNN + LSTM + Discriminator
VideoBERT Pretrain 
CNN + BERT
ActBERT Pretrain 
CNN + BERT + Multi-task Learning
ClipBERT 
CNN + BERT + Clip Sampling
UniViLM 
Transformer + Multimodal
Visual Storytelling
Informative Visual Storytelling 
Knowledgeable Storyteller 
CNN + GRU + Graph
Composite Reward 
CNN + RNN + MLE
CNN + RNN + KG
Visual Genome. is composed of dense annotations of objects, attributes, and relationships
within each image to learn these models. Speciﬁcally, this dataset contains over 108K images where
each image has an average of 35 objects, 26 attributes, and 21 pairwise relationships between
Video Caption. There are mainly three popular datasets for video caption as shown below.
MSR-VTT. is the most widely used video caption dataset, which contains 7,180 videos of
20 categories. This is created by collecting 41.2 hours of 10K web video clips from a commercial
video search engine.
Charades. is collected with a focus on common household activities using the Hollywood
in Homes approach. This dataset contains 9,848 videos with 66,500 annotations describing 157
ActivityNet. is a large dataset that connects videos to a series of temporally annotated sentences. Each sentence describes what occurs in an unique segment of a video. Speciﬁcally, AcitvityNet contains 20k videos with 100k sentence-level description.
Visual Storytelling. The following two datasets are widely used for visual storytelling.
VIST. is the most widely used dataset for visual storytelling. It contains 10,032 visual albums
with 50,136 stories. Each story contains ﬁve narrative sentences, corresponding to ﬁve grounded
images respectively.
VideoStory. contains 20k videos posted publicly on a social media platform amounting to
396 hours of video with 123k sentences.
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
A Survey of Natural Language Generation
Image Captioning. Image captioning aims to generate a description of the given image.
The Show-Tell model proposed an encoder-decoder based framework that encodes images
into feature vectors with Convolution Neural Networks (CNN), and decodes the feature vectors
into words with Recurrent Neural Networks (RNN). To obtain the ﬁne-grained visual concepts,
attention-based image captioning model was proposed to ground words with the corresponding part of imaging. Considering the fact that region aware feature better ﬁts the human visual
system, Anderson et al. Propose a recognized baseline called Bottom-Up-Top-Down (BUTD)
for image caption. The BUTD is composed of two LSTM layers. The ﬁrst LSTM layer is designed
to capture the top-down visual attention model, while the second LSTM layer is regarded as a language model. Given the mean-pooled image feature 풗, a word embedding matrix푊푒, and a one-hot
encoding Π푡of the input word at time 푡we could obtain the output 풉1
푡= LSTM  
푡−1,풗,푊푒Π푡
and the normalized attention weight 푎푖,푡can be represented by the following formulations:
푎tanh  푊푣푎풗푖+푊ℎ푎풉1
휶푡= sofmax (풂푡) ,
The attended image feature used as input to the language LSTM is calculated as a convex combination of all input features as ˆ푣푡= Í퐾
푖=1 훼푖,푡푣푖. The input to the language model LSTM consists of the
attended image feature, concatenated with the output of the attention LSTM, given by 풉2
Using the notation 푦1:푇to refer to a sequence of words, at each time step t the conditional distribution over possible output words is given by:
푝(푦푡| 푦1:푡−1) = sofmax  푊푝풉2
The distribution over complete output sequences is calculated as the product of conditional distributions:
푝(푦푡| 푦1:푡−1) .
To reduce exposure bias and metric mismatching in sequential training, notable eﬀorts are made
to optimise non-diﬀerentiable metrics using reinforcement learning . To further boost
accuracy, detected semantic concepts are adopted in captioning framework. A more
structured representation over concepts calling scene graph is further explored in image
captioning which can take advantage of detected objects and their relationships. Instead of using
a fully detected scene graph to improve captioning accuracy, Chen et al. propose to employ
Abstract Scene Graph as control signal to generate intention-aware and diverse image captions.
With the progress of multimodal representation learning, Wang et al. propose the OFA,
a uniﬁed multimodal pretrained model that can be applied in all modalities and various tasks.
The OFA is simple yet eﬀective, and it achieves new state-of-the-art performance on the kinds of
multimodal tasks, such as image captioning, text-to-image generation, and VQA.
Video Caption. The aim of video caption is to describe or summarize a video in natural
language. It is a non-trivial task for computers since it is diﬃcult to select the useful visual features from a video clip and describe what’s happening in a way that obeys the common sense of
The currently prevailing architecture for video caption is composed of a CNN-like visual encoder
and a RNN-like linguistic decoder. Donahue et al. design the Long-term Recurrent Convectional Networks (LRCNs) that is both temporally and spatially deep. After that, Venugopalan et al.
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
Dong and Li, et al.
 introduce the S2VT, a Seq2Seq approach for video to text with the knowledge from text corpora. Krishna et al. propose a captioning module that uses contextual information from past
and future events to capture the dependencies between the events in a video. However, Krishna et
al. fails to take advantage of language to beneﬁt event proposal with the co-training diagram.
Thus, Zhou et al. propose a video caption framework that produces proposal and description
simultaneously. To describe a video with multiple ﬁne-grained actions, Wang et al. propose a
hierarchical reinforcement learning framework that a high-level agent learns to design sub-goals
and a low-level worker recognizes the primitive actions to fulﬁll the sub-goal. By introducing the
discriminator that considers visual relevance to the video, language diversity & ﬂuency, and coherence across sentences, Park et al. generate more accuracy video descriptions. To resolve the
dilemma that encoders of vision-language tasks are not trained end to end, Lei et al. propose
ClipBERT, a framework that applies the sparse sampling to use a few sampled clips to achieves better performance. Additionally, with the rise of multimodal learning, Luo et al. propose UniVL,
a uniﬁed multimodal pre-training model for vedio captioning. The UniVL consists of four components (i.e., two encoders for single-modal, a cross-modal encoder and a decoder) and is pre-trained
with ﬁve tasks, including language understanding and generation tasks. The highlight of UniVL
is that it uses both understanding and generative tasks for cross-modal pre-training, leading to its
state-of-the-art performance on video captioning.
Recent years, self-supervised learning has become increasingly important with its power to
leverage the abundance of unlabeled data. Sun et al. propose VideoBERT to learn bidirectional
joint distributions over sequences of visual and linguistic tokens without any explicit supervision.
Zhu et al. propose ActNet, which models global and local visual cues for ﬁne-grained visual
and linguistic relation learning.
Visual Storytelling. Visual Storytelling not only needs to identify the correlation between
objects in a single picture, but also Need to identify and learn the logical relationship between
consecutive sequential images. In practice, Visual Storytelling is prone to problems such as single
narrative words, rigid sentences, incoherent context logic, and lack of emotion in story descriptions. aims at generating a coherent and reasonable story with a series of images . To deal
with the issue that Visual Storytelling usually focus on generating general description rather than
the details of meaningful visual contents, Li et al. propose to mine the cross-modal rules to
assistant the concept inference. Yang et al. present a commonsense-driven generative model
to introduce crucial commonsense from the external knowledge base for visual storytelling. Due
to the limitation of maximum likelihood estimation on training, the majority of existing models
encourage high resemblance to texts in the training database, which makes the description overly
rigid and lack in diverse expressions. Therefore, Mo et al. cast the task as a reinforcement
learning task and propose an Adversarial All-in-one Learning (AAL) framework to learn a reward
model, which simultaneously incorporates the information of all images in the photo stream and
all texts in the paragraph, and optimize a generative model with the estimated reward. To make the
Visual Storytelling model topic adaptively, Li et al. introduce a gradient-based meta-learning
algorithm. Conventional storytelling approaches usually focused on optimizing metrics such as
BLEU, ROUGE and CIDEr. In this paper, Hu et al. revisit the issue from a diﬀerent perspective,
by delving into what deﬁnes a natural and thematically coherent story. In addition, considering
the inability of previous methods to explore latent information beyond the image and thus fail to
capture consistent dependencies from the global representation, Li et al. propose the KAGS,
a knowledge-enriched attention network with group-wise semantic model which achieves new
state-of-the-art performance with respect to both objective and subjective evaluation metrics.
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
A Survey of Natural Language Generation
NLG EVALUATION METRICS
In the research ﬁeld of Artiﬁcial Intelligence, the evaluation metrics for models of kinds of tasks
have always been the focus of attention for a long time, and the same is true in the ﬁelds of NLP
 . In this section, we mainly introduce several automatic evaluation metrics for NLG, which
can be divided into two categories: untrained evaluation metrics, and machine-learned evaluation
metrics .
Untrained Evaluation Metrics
This category of metric is most widely used in the NLG community since it is easy to be implemented and does not involve additional training cost, which compares machine-generated texts
to human-generated ones simply based on content overlap, string distance or lexical diversity. We
mainly introduce ﬁve metrics of such category, including BLEU, ROUGE, METEOR, Distinct, and
Self-BLEU.
The Bilingual Evaluation Understudy (BLEU) metric is used to calculate the co-occurrence
frequency of two sentences based on the weighted average of matched n-gram phrases. BLEU was
originally used to evaluate machine translation, and has been used for more and more NLG tasks,
such as question generation , topic-to-essay generation , text style transfer , and
dialogue generation .
The Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metric is used to measure
the similarity between the generated and reference texts based on the recall score. This metric is
commonly used in the ﬁeld of text summarization, including four types: ROUGE-n measures the
n-gram co-occurrence statistics; ROUGE-l measures the longest common subsequence; ROUGE-w
measures the weighted longest common subsequence; ROUGE-s measures the skip-bigram cooccurrence statistics. ROUGE has also been widely applied to other NLG tasks such as question
generation , distractor generation , and dialogue generation .
The Metric for Evaluation of Translation with Explicit Ordering (METEOR) metric is an
improvement over BLEU to address several weaknesses including four aspects: lack of recall, use
of higher order n-grams, lack of explicit word-matching between translation and reference, and
use of geometric averaging of n-grams, which is calculated by the harmonic mean of the unigram
precision and recall. In addition to machine translation, METEOR has also been widely used in
text summarization , question generation , and dialogue generation .
The Distinct metric is used to measure the diversity of response sequences for dialogue
generation. It calculates the number of distinct unigrams and bigrams in generated responses to
reﬂect the diversity degree. To avoid preference for long sequences, the value is scaled by the total
number of generated tokens.
The Self-BLEU metric is also a metric to measure the diversity. Diﬀerent from BLEU that
only evaluates the similarity between two sentences, Self-BLEU is used to measure the resemblance degree between one sentence (hypothesis) and the rest sentences (reference) in a generated
collection. It ﬁrst calculates the BLEU score of every generated sentence against other sentences,
then the average BLEU score is deﬁned as the Self-BLEU score of the document, where a lower
Self-BLEU score implies higher diversity.
Machine-learned Evaluation Metrics
This category of metric is based on machine-learned models to simulate human judges, which
evaluates the similarity between machine-generatedtexts or between machine-generated texts and
human-generated ones. We mainly introduce three metrics of such category, containing ADEM,
BLEURT, and BERTScore.
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
Dong and Li, et al.
The Automatic Dialogue Evaluation Model (ADEM) metric is used to automatically evaluate
the quality of dialogue responses, where the evaluation model is trained in a semi-supervised manner with a hierarchical recurrent neural network (RNN) to predict the response scores. Speciﬁcally,
given the dialogue context c, model response ˆr, and reference response r encoded by a hierarchical
RNN, the predicted score can be calculated by:
푠푐표푟푒= (c⊤푀ˆr + r⊤푁ˆr −훼)/훽,
where 푀, 푁are learnable matrices initialized by the identity, and 훼, 훽are scalar constants to initialize the predicted scores in range .
The Bilingual Evaluation Understudy with Representations from Transformers (BLEURT) metric is based on BERT with a novel pre-training scheme. Before ﬁne-tuning BERT on
rating data to predict human rating scores, a pre-training method is applied, where BERT is pretrained on a large number of synthetic sentence pairs on several lexical- and semantic-level supervision signals in a multi-task manner. This pre-training process is important and can improve the
robustness to quality drifts of generation systems.
The BERTScore metric uses pre-trained contextual embeddings from BERT to measure
the similarity between two sentences. Given the contextual embeddings of a reference sentence 푥
and a candidate sentence ˆ푥, namely x, ˆx, the recall, precision, and F1 scores are calculated by:
퐹BERT = 2 푃BERT · 푅BERT
푃BERT + 푅BERT
where the recall is calculated by matching each token in 푥to a token in ˆ푥, the precision is obtained
by matching each token in ˆ푥to a token 푥, and greedy matching is adopted to match the most
similar tokens.
Human Evaluation Metrics
For generation, human evaluation focuses on the explanation of two key matters: diversity and
creativity, i.e., the capacity of varying their texts in form and emphasis to ﬁt an enormous range of
speaking situations, and the potential to express any object or relation as a natural language text. In
further detail, human evaluation is implemented to evaluate on three aspects: Grammar (whether
a generated sentence is ﬂuent without grammatical error), Faithful (whether the output is faithful
to input), and Coherent (whether a sentence is logically coherent and the order of expression is in
line with human writing habits). This needs to organize the capabilities of the people who work
on generation in ﬁeld of computational linguistics and artiﬁcial intelligence.
PROBLEMS AND CHALLENGES
In this section, we primarily point out four problems and challenges that deserve to be tackled and
investigated further, including the evaluation method, external knowledge engagement, controllable generation, and multimodal scenarios.
Evaluation Method. Evaluation method is still an important and open research area for the ﬁeld
of NLG. As pointed by , traditional untrained evaluation metrics do not always correlate well
with human judgements, while recent machine-learned metrics need a large amount of human
ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.
A Survey of Natural Language Generation
annotations and not always have good transferability. Hence, there still exists a signiﬁcant amount
of challenges and improvement room in this area.
External Knowledge Engagement. Considering the limited information lying in the original texts
and the diﬃculty of generating satisfying sentences , it is crucial to incorporate external
knowledge to enhance the performance. Therefore, how to obtain useful and correlative knowledge and how to eﬀectively incorporate the knowledge still deserve to be investigated.
Controllable Generation. Another challenging problem is how to generate controllable natural
language as we would like it to be. Although a great body of work has been done in this area to
study how to perform various kinds of controlled text generation, there is still a lack of uniform
paradigms and standards about it. More importantly, how to measure the controllability of the
generated text remains an open question, for diﬀerent controlled contents.
Multimodal Scenarios. Recently, research on various applications in multimodal scenarios have
gradually attracted more and more attention from NLP researchers. How to apply natural language
generation methods in multimodal scenarios has been a worthy problem and promising direction.
It is reasonable to believe that the utilization of rich multimodal information into natural language
generation tasks will surely further advance the progress and development in this direction.
CONCLUSIONS
Over the past few years, natural language generation tasks and methods have become important
and indispensable in natural language processing. This progress owes to advances in various deep
learning-based methods. This article describes deep learning research on natural language generation with a historical perspective, emphasizing the special character of the problems to be solved. It
begins by contrasting generation with language understanding, establishing basic concepts about
the tasks, datasets, and the deep learning methods through it. A section of evaluation metrics
from the output of generation systems follows, showing what kinds of performance are possible
and where the diﬃculties are. Finally, some open problems are suggested to indicate the major
challenges and future research directions of natural language generation.
ACKNOWLEDGEMENT
This work was supported in part by the 173 program No. 2021-JCJQ-JJ-0029, the Shenzhen General
Research Project under Grant JCYJ20190808182805919and in part by the National Natural Science
Foundation of China under Grant 61602013.