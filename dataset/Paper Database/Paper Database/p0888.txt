Masked-attention Mask Transformer for Universal Image Segmentation
Bowen Cheng1,2* Ishan Misra1
Alexander G. Schwing2
Alexander Kirillov1
Rohit Girdhar1
1Facebook AI Research (FAIR)
2University of Illinois at Urbana-Champaign (UIUC)
 
Image segmentation groups pixels with different semantics, e.g., category or instance membership. Each choice
of semantics deﬁnes a task. While only the semantics of
each task differ, current research focuses on designing specialized architectures for each task. We present Maskedattention Mask Transformer (Mask2Former), a new architecture capable of addressing any image segmentation task
(panoptic, instance or semantic). Its key components include masked attention, which extracts localized features by
constraining cross-attention within predicted mask regions.
In addition to reducing the research effort by at least three
times, it outperforms the best specialized architectures by
a signiﬁcant margin on four popular datasets. Most notably, Mask2Former sets a new state-of-the-art for panoptic
segmentation (57.8 PQ on COCO), instance segmentation
(50.1 AP on COCO) and semantic segmentation (57.7 mIoU
on ADE20K).
1. Introduction
Image segmentation studies the problem of grouping
pixels. Different semantics for grouping pixels, e.g., category or instance membership, have led to different types
of segmentation tasks, such as panoptic, instance or semantic segmentation. While these tasks differ only in semantics,
current methods develop specialized architectures for each
task. Per-pixel classiﬁcation architectures based on Fully
Convolutional Networks (FCNs) are used for semantic
segmentation, while mask classiﬁcation architectures 
that predict a set of binary masks each associated with a
single category, dominate instance-level segmentation. Although such specialized architectures have
advanced each individual task, they lack the ﬂexibility to
generalize to the other tasks. For example, FCN-based architectures struggle at instance segmentation, leading to the
evolution of different architectures for instance segmentation compared to semantic segmentation. Thus, duplicate
research and (hardware) optimization effort is spent on each
*Work done during an internship at Facebook AI Research.
Universal architectures:
Mask2Former (ours)
MaskFormer
SOTA specialized architectures:
Max-DeepLab
Swin-HTC++
Figure 1. State-of-the-art segmentation architectures are typically
specialized for each image segmentation task. Although recent
work has proposed universal architectures that attempt all tasks
and are competitive on semantic and panoptic segmentation, they
struggle with segmenting instances. We propose Mask2Former,
which, for the ﬁrst time, outperforms the best specialized architectures on three studied segmentation tasks on multiple datasets.
specialized architecture for every task.
To address this fragmentation, recent work has
attempted to design universal architectures, that are capable
of addressing all segmentation tasks with the same architecture (i.e., universal image segmentation). These architectures are typically based on an end-to-end set prediction
objective (e.g., DETR ), and successfully tackle multiple
tasks without modifying the architecture, loss, or the training procedure. Note, universal architectures are still trained
separately for different tasks and datasets, albeit having the
same architecture. In addition to being ﬂexible, universal
architectures have recently shown state-of-the-art results on
semantic and panoptic segmentation .
However, recent work still focuses on advancing specialized architectures , which raises the question: why haven’t
universal architectures replaced specialized ones?
Although existing universal architectures are ﬂexible
enough to tackle any segmentation task, as shown in Figure 1, in practice their performance lags behind the best
specialized architectures. For instance, the best reported
 
performance of universal architectures , is currently
lower (> 9 AP) than the SOTA specialized architecture
for instance segmentation .
Beyond the inferior performance, universal architectures are also harder to train.
They typically require more advanced hardware and a much
longer training schedule.
For example, training Mask-
Former takes 300 epochs to reach 40.1 AP and it can
only ﬁt a single image in a GPU with 32G memory. In contrast, the specialized Swin-HTC++ obtains better performance in only 72 epochs. Both the performance and training efﬁciency issues hamper the deployment of universal
architectures.
In this work, we propose a universal image segmentation architecture named Masked-attention Mask Transformer (Mask2Former) that outperforms specialized architectures across different segmentation tasks, while still
being easy to train on every task. We build upon a simple meta architecture consisting of a backbone feature extractor , a pixel decoder and a Transformer decoder . We propose key improvements that
enable better results and efﬁcient training. First, we use
masked attention in the Transformer decoder which restricts
the attention to localized features centered around predicted
segments, which can be either objects or regions depending on the speciﬁc semantic for grouping. Compared to
the cross-attention used in a standard Transformer decoder
which attends to all locations in an image, our masked attention leads to faster convergence and improved performance.
Second, we use multi-scale high-resolution features which
help the model to segment small objects/regions. Third,
we propose optimization improvements such as switching
the order of self and cross-attention, making query features
learnable, and removing dropout; all of which improve performance without additional compute. Finally, we save 3×
training memory without affecting the performance by calculating mask loss on few randomly sampled points. These
improvements not only boost the model performance, but
also make training signiﬁcantly easier, making universal architectures more accessible to users with limited compute.
We evaluate Mask2Former on three image segmentation tasks (panoptic, instance and semantic segmentation)
using four popular datasets (COCO , Cityscapes ,
ADE20K and Mapillary Vistas ).
For the ﬁrst
time, on all these benchmarks, our single architecture
performs on par or better than specialized architectures.
Mask2Former sets the new state-of-the-art of 57.8 PQ on
COCO panoptic segmentation , 50.1 AP on COCO instance segmentation and 57.7 mIoU on ADE20K semantic segmentation using the exact same architecture.
2. Related Work
Specialized semantic segmentation architectures typically treat the task as a per-pixel classiﬁcation problem.
FCN-based architectures independently predict a category label for every pixel. Follow-up methods ﬁnd context to play an important role for precise per-pixel classi-
ﬁcation and focus on designing customized context modules or self-attention variants .
Specialized instance segmentation architectures are typically based upon “mask classiﬁcation.” They predict a set
of binary masks each associated with a single class label.
The pioneering work, Mask R-CNN , generates masks
from detected bounding boxes. Follow-up methods either
focus on detecting more precise bounding boxes , or
ﬁnding new ways to generate a dynamic number of masks,
e.g., using dynamic kernels or clustering algorithms . Although the performance has been advanced in each task, these specialized innovations lack the
ﬂexibility to generalize from one to the other, leading to
duplicated research effort. For instance, although multiple
approaches have been proposed for building feature pyramid representations , as we show in our experiments,
BiFPN performs better for instance segmentation while
FaPN performs better for semantic segmentation.
Panoptic segmentation has been proposed to unify both semantic and instance segmentation tasks . Architectures
for panoptic segmentation either combine the best of specialized semantic and instance segmentation architectures
into a single framework or design novel objectives that equally treat semantic regions and instance objects . Despite those new architectures, researchers
continue to develop specialized architectures for different
image segmentation tasks . We ﬁnd panoptic architectures usually only report performance on a single panoptic segmentation task , which does not guarantee good
performance on other tasks (Figure 1). For example, panoptic segmentation does not measure architectures’ abilities to
rank predictions as instance segmentations. Thus, we refrain from referring to architectures that are only evaluated
for panoptic segmentation as universal architectures. Instead, here, we evaluate our Mask2Former on all studied
tasks to guarantee generalizability.
Universal architectures have emerged with DETR and
show that mask classiﬁcation architectures with an end-toend set prediction objective are general enough for any image segmentation task. MaskFormer shows that mask
classiﬁcation based on DETR not only performs well on
panoptic segmentation but also achieves state-of-the-art on
semantic segmentation. K-Net further extends set prediction to instance segmentation. Unfortunately, these architectures fail to replace specialized models as their performance on particular tasks or datasets is still worse than the
best specialized architecture (e.g., MaskFormer cannot
segment instances well). To our knowledge, Mask2Former
is the ﬁrst architecture that outperforms state-of-the-art specialized architectures on all considered tasks and datasets.
3. Masked-attention Mask Transformer
We now present Mask2Former. We ﬁrst review a meta
architecture for mask classiﬁcation that Mask2Former is
built upon. Then, we introduce our new Transformer decoder with masked attention which is the key to better convergence and results. Lastly, we propose training improvements that make Mask2Former efﬁcient and accessible.
3.1. Mask classiﬁcation preliminaries
Mask classiﬁcation architectures group pixels into N
segments by predicting N binary masks, along with N corresponding category labels.
Mask classiﬁcation is sufﬁciently general to address any segmentation task by assigning different semantics, e.g., categories or instances, to different segments. However, the challenge is to ﬁnd good
representations for each segment. For example, Mask R-
CNN uses bounding boxes as the representation which
limits its application to semantic segmentation. Inspired by
DETR , each segment in an image can be represented as
a C-dimensional feature vector (“object query”) and can be
processed by a Transformer decoder, trained with a set prediction objective. A simple meta architecture would consist of three components. A backbone that extracts lowresolution features from an image. A pixel decoder that
gradually upsamples low-resolution features from the output of the backbone to generate high-resolution per-pixel
embeddings. And ﬁnally a Transformer decoder that operates on image features to process object queries. The ﬁnal
binary mask predictions are decoded from per-pixel embeddings with object queries. One successful instantiation of
such a meta architecture is MaskFormer , and we refer
readers to for more details.
3.2. Transformer decoder with masked attention
Mask2Former adopts the aforementioned meta architecture, with our proposed Transformer decoder (Figure 2
right) replacing the standard one. The key components of
our Transformer decoder include a masked attention operator, which extracts localized features by constraining crossattention to within the foreground region of the predicted
mask for each query, instead of attending to the full feature map. To handle small objects, we propose an efﬁcient
multi-scale strategy to utilize high-resolution features. It
feeds successive feature maps from the pixel decoder’s feature pyramid into successive Transformer decoder layers in
a round robin fashion. Finally, we incorporate optimization improvements that boost model performance without
introducing additional computation. We now discuss these
improvements in detail.
Pixel Decoder
Transformer
masked attention
add & norm
self-attention
add & norm
add & norm
Mask2Former overview.
Mask2Former adopts the
same meta architecture as MaskFormer with a backbone, a
pixel decoder and a Transformer decoder.
We propose a new
Transformer decoder with masked attention instead of the standard
cross-attention (Section 3.2.1). To deal with small objects, we propose an efﬁcient way of utilizing high-resolution features from a
pixel decoder by feeding one scale of the multi-scale feature to one
Transformer decoder layer at a time (Section 3.2.2). In addition,
we switch the order of self and cross-attention (i.e., our masked
attention), make query features learnable, and remove dropout to
make computation more effective (Section 3.2.3). Note that positional embeddings and predictions from intermediate Transformer
decoder layers are omitted in this ﬁgure for readability.
Masked attention
Context features have been shown to be important for image segmentation . However, recent studies 
suggest that the slow convergence of Transformer-based
models is due to global context in the cross-attention layer,
as it takes many training epochs for cross-attention to learn
to attend to localized object regions . We hypothesize
that local features are enough to update query features and
context information can be gathered through self-attention.
For this we propose masked attention, a variant of crossattention that only attends within the foreground region of
the predicted mask for each query.
Standard cross-attention (with residual path) computes
Xl = softmax(QlKT
l )Vl + Xl−1.
Here, l is the layer index, Xl
∈RN×C refers to N
C-dimensional query features at the lth layer and Ql =
fQ(Xl−1) ∈RN×C. X0 denotes input query features to
the Transformer decoder. Kl, Vl ∈RHlWl×C are the image features under transformation fK(·) and fV (·) respectively, and Hl and Wl are the spatial resolution of image
features that we will introduce next in Section 3.2.2. fQ,
fK and fV are linear transformations.
Our masked attention modulates the attention matrix via
Xl = softmax(Ml−1 + QlKT
l )Vl + Xl−1.
Moreover, the attention mask Ml−1 at feature location
Ml−1(x, y) =
if Ml−1(x, y) = 1
Here, Ml−1
{0, 1}N×HlWl is the binarized output
(thresholded at 0.5) of the resized mask prediction of the
previous (l −1)-th Transformer decoder layer. It is resized
to the same resolution of Kl. M0 is the binary mask prediction obtained from X0, i.e., before feeding query features
into the Transformer decoder.
High-resolution features
High-resolution features improve model performance, especially for small objects . However, this is computationally demanding. Thus, we propose an efﬁcient multi-scale
strategy to introduce high-resolution features while controlling the increase in computation. Instead of always using
the high-resolution feature map, we utilize a feature pyramid which consists of both low- and high-resolution features and feed one resolution of the multi-scale feature to
one Transformer decoder layer at a time.
Speciﬁcally, we use the feature pyramid produced by
the pixel decoder with resolution 1/32, 1/16 and 1/8 of
the original image. For each resolution, we add both a sinusoidal positional embedding epos ∈RHlWl×C, following , and a learnable scale-level embedding elvl ∈R1×C,
following . We use those, from lowest-resolution to
highest-resolution for the corresponding Transformer decoder layer as shown in Figure 2 left. We repeat this 3-layer
Transformer decoder L times. Our ﬁnal Transformer decoder hence has 3L layers. More speciﬁcally, the ﬁrst three
layers receive a feature map of resolution H1 = H/32,
H2 = H/16, H3 = H/8 and W1 = W/32, W2 = W/16,
W3 = W/8, where H and W are the original image resolution. This pattern is repeated in a round robin fashion for
all following layers.
Optimization improvements
A standard Transformer decoder layer consists of three
modules to process query features in the following order: a
self-attention module, a cross-attention and a feed-forward
network (FFN). Moreover, query features (X0) are zero initialized before being fed into the Transformer decoder and
are associated with learnable positional embeddings. Furthermore, dropout is applied to both residual connections
and attention maps.
To optimize the Transformer decoder design, we make
the following three improvements.
First, we switch the
order of self- and cross-attention (our new “masked attention”) to make computation more effective: query features
to the ﬁrst self-attention layer are image-independent and
do not have signals from the image, thus applying selfattention is unlikely to enrich information.
Second, we
make query features (X0) learnable as well (we still keep
the learnable query positional embeddings), and learnable
query features are directly supervised before being used in
the Transformer decoder to predict masks (M0). We ﬁnd
these learnable query features function like a region proposal network and have the ability to generate mask
proposals. Finally, we ﬁnd dropout is not necessary and
usually decreases performance. We thus completely remove
dropout in our decoder.
3.3. Improving training efﬁciency
One limitation of training universal architectures is the
large memory consumption due to high-resolution mask
prediction, making them less accessible than the more
memory-friendly specialized architectures . For example, MaskFormer can only ﬁt a single image in a
GPU with 32G memory. Motivated by PointRend and
Implicit PointRend , which show a segmentation model
can be trained with its mask loss calculated on K randomly
sampled points instead of the whole mask, we calculate the
mask loss with sampled points in both the matching and
the ﬁnal loss calculation. More speciﬁcally, in the matching loss that constructs the cost matrix for bipartite matching, we uniformly sample the same set of K points for all
prediction and ground truth masks. In the ﬁnal loss between predictions and their matched ground truths, we sample different sets of K points for different pairs of prediction and ground truth using importance sampling . We
set K = 12544, i.e., 112 × 112 points. This new training
strategy effectively reduces training memory by 3×, from
18GB to 6GB per image, making Mask2Former more accessible to users with limited computational resources.
4. Experiments
We demonstrate Mask2Former is an effective architecture for universal image segmentation through comparisons with specialized state-of-the-art architectures on standard benchmarks.
We evaluate our proposed design decisions through ablations on all three tasks.
Finally we
show Mask2Former generalizes beyond the standard benchmarks, obtaining state-of-the-art results on four datasets.
Datasets. We study Mask2Former using four widely used
image segmentation datasets that support semantic, instance
and panoptic segmentation: COCO (80 “things” and
53 “stuff” categories), ADE20K (100 “things” and
50 “stuff” categories), Cityscapes (8 “things” and 11
“stuff” categories) and Mapillary Vistas (37 “things”
and 28 “stuff” categories).
Panoptic and semantic seg-
query type
100 queries
MaskFormer 
100 queries
Mask2Former (ours)
100 queries
100 queries
MaskFormer 
100 queries
Mask2Former (ours)
100 queries
Max-DeepLab 
128 queries
MaskFormer 
100 queries
K-Net 
100 queries
Mask2Former (ours)
200 queries
Table 1. Panoptic segmentation on COCO panoptic val2017 with 133 categories. Mask2Former consistently outperforms Mask-
Former by a large margin with different backbones on all metrics. Our best model outperforms prior state-of-the-art MaskFormer by
5.1 PQ and K-Net by 3.2 PQ. Backbones pre-trained on ImageNet-22K are marked with †.
mentation tasks are evaluated on the union of “things” and
“stuff” categories while instance segmentation is only evaluated on the “things” categories.
Evaluation metrics. For panoptic segmentation, we use
the standard PQ (panoptic quality) metric . We further report APTh
pan, which is the AP evaluated on the “thing”
categories using instance segmentation annotations, and
mIoUpan, which is the mIoU for semantic segmentation
by merging instance masks from the same category, of the
same model trained only with panoptic segmentation annotations. For instance segmentation, we use the standard AP
(average precision) metric . For semantic segmentation,
we use mIoU (mean Intersection-over-Union) .
4.1. Implementation details
We adopt settings from with the following differences:
Pixel decoder. Mask2Former is compatible with any existing pixel decoder module. In MaskFormer , FPN 
is chosen as the default for its simplicity. Since our goal
is to demonstrate strong performance across different segmentation tasks, we use the more advanced multi-scale deformable attention Transformer (MSDeformAttn) as
our default pixel decoder. Speciﬁcally, we use 6 MSDeformAttn layers applied to feature maps with resolution 1/8,
1/16 and 1/32, and use a simple upsampling layer with lateral connection on the ﬁnal 1/8 feature map to generate the
feature map of resolution 1/4 as the per-pixel embedding.
In our ablation study, we show that this pixel decoder provides best results across different segmentation tasks.
Transformer decoder. We use our Transformer decoder
proposed in Section 3.2 with L = 3 (i.e., 9 layers total) and
100 queries by default. An auxiliary loss is added to every
intermediate Transformer decoder layer and to the learnable
query features before the Transformer decoder.
Loss weights. We use the binary cross-entropy loss (instead
of focal loss in ) and the dice loss for our mask
loss: Lmask = λceLce + λdiceLdice. We set λce = 5.0 and
λdice = 5.0. The ﬁnal loss is a combination of mask loss and
classiﬁcation loss: Lmask +λclsLcls and we set λcls = 2.0 for
predictions matched with a ground truth and 0.1 for the “no
object,” i.e., predictions that have not been matched with
any ground truth.
Post-processing. We use the exact same post-processing
as to acquire the expected output format for panoptic
and semantic segmentation from pairs of binary masks and
class predictions. Instance segmentation requires additional
conﬁdence scores for each prediction. We multiply class
conﬁdence and mask conﬁdence (i.e., averaged foreground
per-pixel binary mask probability) for a ﬁnal conﬁdence.
4.2. Training settings
Panoptic and instance segmentation.
We use Detectron2 and follow the updated Mask R-CNN baseline settings1 for the COCO dataset. More speciﬁcally, we
use AdamW optimizer and the step learning rate schedule. We use an initial learning rate of 0.0001 and a weight
decay of 0.05 for all backbones. A learning rate multiplier
of 0.1 is applied to the backbone and we decay the learning
rate at 0.9 and 0.95 fractions of the total number of training
steps by a factor of 10. If not stated otherwise, we train our
models for 50 epochs with a batch size of 16. For data augmentation, we use the large-scale jittering (LSJ) augmentation with a random scale sampled from range 0.1 to
2.0 followed by a ﬁxed size crop to 1024×1024. We use the
standard Mask R-CNN inference setting where we resize an
image with shorter side to 800 and longer side up-to 1333.
We also report FLOPs and fps. FLOPs are averaged over
100 validation images (COCO images have varying sizes).
Frames-per-second (fps) is measured on a V100 GPU with
a batch size of 1 by taking the average runtime on the entire
validation set including post-processing time.
Semantic segmentation.
We follow the same settings
as to train our models, except: 1) a learning rate multiplier of 0.1 is applied to both CNN and Transformer backbones instead of only applying it to CNN backbones in ,
2) both ResNet and Swin backbones use an initial learning
rate of 0.0001 and a weight decay of 0.05, instead of using
1 
main / MODEL _ ZOO . md # new - baselines - using - large - scale jitter-and-longer-training-schedule
query type
APboundary
MaskFormer 
100 queries
Mask R-CNN 
dense anchors
Mask R-CNN 
dense anchors
Mask2Former (ours)
100 queries
Mask R-CNN 
dense anchors
Mask R-CNN 
dense anchors
Mask2Former (ours)
100 queries
QueryInst 
300 queries
Swin-HTC++ 
dense anchors
Mask2Former (ours)
200 queries
Table 2. Instance segmentation on COCO val2017 with 80 categories. Mask2Former outperforms strong Mask R-CNN baselines
for both AP and APboundary metrics when training with 8× fewer epochs. Our best model is also competitive to the state-of-the-art
specialized instance segmentation model on COCO and has higher boundary quality. For a fair comparison, we only consider single-scale
inference and models trained using only COCO train2017 set data. Backbones pre-trained on ImageNet-22K are marked with †.
different learning rates in .
4.3. Main results
Panoptic segmentation. We compare Mask2Former with
state-of-the-art models for panoptic segmentation on the
COCO panoptic dataset in Table 1.
Mask2Former
consistently outperforms MaskFormer by more than 5 PQ
across different backbones while converging 6× faster.
With Swin-L backbone, our Mask2Former sets a new stateof-the-art of 57.8 PQ, outperforming existing state-of-theart by 5.1 PQ and concurrent work, K-Net , by
3.2 PQ. Mask2Former even outperforms the best ensemble
models with extra training data in the COCO challenge (see
Appendix A.1 for test set results).
Beyond the PQ metric, our Mask2Former also achieves
higher performance on two other metrics compared to
DETR and MaskFormer: APTh
pan, which is the AP evaluated on the 80 “thing” categories using instance segmentation annotation, and mIoUpan, which is the mIoU evaluated on the 133 categories for semantic segmentation converted from panoptic segmentation annotation. This shows
Mask2Former’s universality: trained only with panoptic
segmentation annotations, it can be used for instance and
semantic segmentation.
Instance segmentation. We compare Mask2Former with
state-of-the-art models on the COCO dataset in Table 2. With ResNet backbone, Mask2Former outperforms a strong Mask R-CNN baseline using largescale jittering (LSJ) augmentation while requiring 8× fewer training iterations. With Swin-L backbone,
Mask2Former outperforms the state-of-the-art HTC++ .
Although we only observe +0.6 AP improvement over
HTC++, the Boundary AP improves by 2.1, suggesting
that our predictions have a better boundary quality thanks to
the high-resolution mask predictions. Note that for a fair
comparison, we only consider single-scale inference and
models trained with only COCO train2017 set data.
With a ResNet-50 backbone Mask2Former improves
over MaskFormer on small objects by 7.0 APS, while overmethod
mIoU (s.s.)
mIoU (m.s.)
MaskFormer 
Mask2Former (ours)
Swin-UperNet 
MaskFormer 
Mask2Former (ours)
MaskFormer 
FaPN-MaskFormer Swin-L-FaPN†
BEiT-UperNet 
Mask2Former (ours)
Swin-L-FaPN†
Semantic segmentation on ADE20K val with
150 categories. Mask2Former consistently outperforms Mask-
Former by a large margin with different backbones (all
Mask2Former models use MSDeformAttn as pixel decoder,
except Swin-L-FaPN uses FaPN ). Our best model outperforms the best specialized model, BEiT . We report both singlescale (s.s.) and multi-scale (m.s.) inference results. Backbones
pre-trained on ImageNet-22K are marked with †.
all the highest gains come from large objects (+10.6 APL).
The performance on APS still lags behind other state-of-theart models. Hence there still remains room for improvement
on small objects, e.g., by using dilated backbones like in
DETR , which we leave for future work.
Semantic segmentation. We compare Mask2Former with
state-of-the-art models for semantic segmentation on the
ADE20K dataset in Table 3. Mask2Former outperforms MaskFormer across different backbones, suggesting that the proposed improvements even boost semantic segmentation results where was already state-ofthe-art. With Swin-L as backbone and FaPN as pixel
decoder, Mask2Former sets a new state-of-the-art of 57.7
mIoU. We also report the test set results in Appendix A.3.
4.4. Ablation studies
We now analyze Mask2Former through a series of ablation studies using a ResNet-50 backbone . To test the
generality of the proposed components for universal image
segmentation, all ablations are performed on three tasks.
Mask2Former (ours)
−masked attention
37.8 (-5.9)
47.1 (-4.8)
45.5 (-1.7)
−high-resolution features
41.5 (-2.2)
50.2 (-1.7)
46.1 (-1.1)
(a) Masked attention and high-resolution features (from efﬁcient multi-scale
strategy) lead to the most gains. More detailed ablations are in Table 4c and
Table 4d. We remove one component at a time.
Mask2Former (ours)
−learnable query features
42.9 (-0.8)
51.2 (-0.7)
45.4 (-1.8)
−cross-attention ﬁrst
43.2 (-0.5)
51.6 (-0.3)
46.3 (-0.9)
−remove dropout
43.0 (-0.7)
51.3 (-0.6)
47.2 (-0.0)
−all 3 components above
42.3 (-1.4)
50.8 (-1.1)
46.3 (-0.9)
(b) Optimization improvements increase the performance without introducing extra compute. Following DETR , query features are zero-initialized
when not learnable. We remove one component at a time.
cross-attention
mask pooling 
masked attention
(c) Masked attention. Our masked attention
performs better than other variants of crossattention across all tasks.
single scale (1/32)
single scale (1/16)
single scale (1/8)
na¨ıve m.s. (3 scales)
efﬁcient m.s. (3 scales)
(d) Feature resolution. High-resolution features (single scale 1/8) are important. Our efﬁcient multi-scale
(efﬁcient m.s.) strategy effectively reduces the FLOPs.
Semantic FPN 
BiFPN 
MSDeformAttn 
(e) Pixel decoder. MSDeformAttn consistently performs the best across all tasks.
Table 4. Mask2Former ablations. We perform ablations on three tasks: instance (AP on COCO val2017), panoptic (PQ on COCO
panoptic val2017) and semantic (mIoU on ADE20K val) segmentation. FLOPs are measured on COCO instance segmentation.
Transformer decoder. We validate the importance of each
component by removing them one at a time. As shown in
Table 4a, masked attention leads to the biggest improvement across all tasks. The improvement is larger for instance and panoptic segmentation than for semantic segmentation. Moreover, using high-resolution features from
the efﬁcient multi-scale strategy is also important. Table 4b
shows additional optimization improvements further improve the performance without extra computation.
Masked attention. Concurrent work has proposed other
variants of cross-attention that aim to improve the
convergence and performance of DETR for object detection. Most recently, K-Net replaced cross-attention
with a mask pooling operation that averages features within
mask regions. We validate the importance of our masked
attention in Table 4c. While existing cross-attention variants may improve on a speciﬁc task, our masked attention
performs the best on all three tasks.
Feature resolution.
Table 4d shows that Mask2Former
beneﬁts from using high-resolution features (e.g., a single
scale of 1/8) in the Transformer decoder. However, this introduces additional computation. Our efﬁcient multi-scale
(efﬁcient m.s.) strategy effectively reduces the FLOPs without affecting the performance. Note that, naively concatenating multi-scale features as input to every Transformer
decoder layer (na¨ıve m.s.) does not yield additional gains.
Pixel decoder. As shown in Table 4e, Mask2Former is compatible with any existing pixel decoder. However, we observe different pixel decoders specialize in different tasks:
while BiFPN performs better on instance-level segmentation, FaPN works better for semantic segmentation. Among all studied pixel decoders, the MSDeformaAttn consistently performs the best across all tasks
and thus is selected as our default. This set of ablations also
matching loss
training loss
point (ours)
point (ours)
Table 5. Calculating loss with points vs. masks. Training with
point loss reduces training memory without inﬂuencing the performance. Matching with point loss further improves performance.
suggests that designing a module like a pixel decoder for a
speciﬁc task does not guarantee generalization across segmentation tasks. Mask2Former, as a universal model, could
serve as a testbed for a generalizable module design.
Calculating loss with points vs. masks. In Table 5 we
study the performance and memory implications when calculating the loss based on either mask or sampled points.
Calculating the ﬁnal training loss with sampled points reduces training memory by 3× without affecting the performance. Additionally, calculating the matching loss with
sampled points improves performance across all three tasks.
Learnable queries as region proposals. Region proposals , either in the form of boxes or masks, are regions that are likely to be “objects.” With learnable queries
being supervised by the mask loss, predictions from learnable queries can serve as mask proposals. In Figure 3 top,
we visualize mask predictions of selected learnable queries
before feeding them into the Transformer decoder (the proposal generation process is shown in Figure 3 bottom right).
In Figure 3 bottom left, we further perform a quantitative analysis on the quality of these proposals by calculating the class-agnostic average recall with 100 predictions
(AR@100) on COCO val2017. We ﬁnd these learnable
queries already achieve good AR@100 compared to the ﬁ-
AR@100 on COCO val2017
Learnable queries
Pixel Decoder
Figure 3. Learnable queries as “region proposals”. Top: We
visualize mask predictions of four selected learnable queries before feeding them into the Transformer decoder (using R50 backbone). Bottom left: We calculate the class-agnostic average recall
with 100 proposals (AR@100) and observe that these learnable
queries provide good proposals compared to the ﬁnal predictions
of Mask2Former after the Transformer decoder layers (layer 9).
Bottom right: Illustration of proposal generation process.
panoptic model
semantic model
pan mIoUpan mIoU (s.s.) (m.s.)
Panoptic FCN 
Panoptic-DeepLab SWideRNet 66.4
Panoptic-DeepLab SWideRNet 67.5∗43.9∗
ViT-L† 
SegFormer 
MiT-B5 
Mask2Former (ours)
Table 6. Cityscapes val. Mask2Former is competitive to specialized models on Cityscapes. Panoptic segmentation models use
single-scale inference by default, multi-scale numbers are marked
with ∗. For semantic segmentation, we report both single-scale
(s.s.) and multi-scale (m.s.) inference results. Backbones pretrained on ImageNet-22K are marked with †.
nal predictions of Mask2Former after the Transformer decoder layers, i.e., layer 9, and AR@100 consistently improves with more decoder layers.
4.5. Generalization to other datasets
To show our Mask2Former can generalize beyond the
COCO dataset, we further perform experiments on other
popular image segmentation datasets. In Table 6, we show
results on Cityscapes . Please see Appendix B for detailed training settings on each dataset as well as more results on ADE20K and Mapillary Vistas .
(b) ADE20K
(c) Cityscapes
Limitations of Mask2Former.
Although a single
Mask2Former can address any segmentation task, we still need
to train it on different tasks.
Across three datasets we ﬁnd
Mask2Former trained with panoptic annotations performs slightly
worse than the exact same model trained speciﬁcally for instance
and semantic segmentation tasks with the corresponding data.
We observe that our Mask2Former is competitive to
state-of-the-art methods on these datasets as well. It suggests Mask2Former can serve as a universal image segmentation model and results generalize across datasets.
4.6. Limitations
Our ultimate goal is to train a single model for all image segmentation tasks. In Table 7, we ﬁnd Mask2Former
trained on panoptic segmentation only performs slightly
worse than the exact same model trained with the corresponding annotations for instance and semantic segmentation tasks across three datasets.
This suggests that even
though Mask2Former can generalize to different tasks, it
still needs to be trained for those speciﬁc tasks. In the future, we hope to develop a model that can be trained only
once for multiple tasks and even for multiple datasets.
Furthermore, as seen in Tables 2 and 4d, even though it
improves over baselines, Mask2Former struggles with segmenting small objects and is unable to fully leverage multiscale features. We believe better utilization of the feature
pyramid and designing losses for small objects are critical.
5. Conclusion
We present Mask2Former for universal image segmentation. Built upon a simple meta framework with a
new Transformer decoder using the proposed masked attention, Mask2Former obtains top results in all three major image segmentation tasks (panoptic, instance and semantic) on
four popular datasets, outperforming even the best specialized models designed for each benchmark while remaining
easy to train. Mask2Former saves 3× research effort compared to designing specialized models for each task, and it
is accessible to users with limited computational resources.
We hope to attract interest in universal model design.
Ethical considerations: While our technical innovations do not appear to
have any inherent biases, the models trained with our approach on realworld datasets should undergo ethical review to ensure the predictions do
not propagate problematic stereotypes, and the approach is not used for
applications including but not limited to illegal surveillance.
Acknowledgments: Thanks to Nicolas Carion and Xingyi Zhou for helpful feedback.
BC and AS are supported in part by NSF #1718221,
2008387, 2045586, 2106825, MRI #1725729, NIFA 2020-67021-32799
and Cisco Systems Inc. (CG 1377144 - thanks for access to Arcetri).