Proceedings of NAACL-HLT 2016, pages 30–34,
San Diego, California, June 12-17, 2016. c⃝2016 Association for Computational Linguistics
Multi-Source Neural Translation
Barret Zoph and Kevin Knight
Information Sciences Institute
Department of Computer Science
University of Southern California
{zoph,knight}@isi.edu
We build a multi-source machine translation
model and train it to maximize the probability of a target English string given French and
German sources. Using the neural encoderdecoder framework, we explore several combination methods and report up to +4.8 Bleu
increases on top of a very strong attentionbased neural translation model.
Introduction
Kay points out that if a document is translated once, it is likely to be translated again and
again into other languages. This gives rise to an interesting idea: a human does the ﬁrst translation by
hand, then turns the rest over to machine translation
(MT). The translation system now has two strings
as input, which can reduce ambiguity via “triangulation” (Kay’s term).
For example, the normally
ambiguous English word “bank” may be more easily translated into French in the presence of a second, German input string containing the word “Flussufer” (river bank).
Och and Ney describe such a multi-source
MT system. They ﬁrst train separate bilingual MT
systems F→E, G→E, etc. At runtime, they separately translate input strings f and g into candidate target strings e1 and e2, then select the best one
of the two. A typical selection factor is the product of the system scores. Schwartz revisits
such factors in the context of log-linear models and
Bleu score, while Max et al. re-rank F→E
n-best lists using n-gram precision with respect to
G→E translations. Callison-Burch exploits
hypothesis selection in multi-source MT to expand
available corpora, via co-training.
Others use system combination techniques to
merge hypotheses at the word level, creating the
ability to synthesize new translations outside those
proposed by the single-source translators.
methods include confusion networks , source-side string
combination , and median
strings .
The above work all relies on base MT systems
trained on bilingual data, using traditional methods.
This follows early work in sentence alignment and word alignment
 , which exploited trilingual text, but
did not build trilingual models.
Previous authors
possibly considered a three-dimensional translation
table t(e|f, g) to be prohibitive.
In this paper, by contrast, we train a P(e|f, g)
model directly on trilingual data, and we use that
model to decode an (f, g) pair simultaneously. We
view this as a kind of multi-tape transduction with two input tapes and one output
tape. Our contributions are as follows:
• We train a P(e|f, g) model directly on trilingual data, and we use it to decode a new source
string pair (f, g) into target string e.
• We show positive Bleu improvements over
strong single-source baselines.
• We show that improvements are best when the
two source languages are more distant from
each other.
We are able to achieve these results using
Figure 1: The encoder-decoder framework for neural machine
translation (NMT) . Here, a source sentence C B A (presented in reverse order as A B C) is translated
into a target sentence W X Y Z. At each step, an evolving realvalued vector summarizes the state of the encoder (white) and
decoder (gray).
the framework of neural encoder-decoder models,
where multi-target MT and
multi-source, cross-modal mappings have been explored .
Multi-Source Neural MT
In the neural encoder-decoder framework for MT
 , we use a recurrent neural network (encoder) to convert a source sentence into a
dense, ﬁxed-length vector. We then use another recurrent network (decoder) to convert that vector in a
target sentence.1
In this paper, we use a four-layer encoder-decoder
system (Figure 1) with long short-term memory
(LSTM) units 
trained for maximum likelihood (via a softmax
layer) with back-propagation through time . For our baseline single-source MT system we
use two different models, one of which implements
the local attention plus feed-input model from Luong et al. .
Figure 2 shows our approach to multi-source MT.
Each source language has its own encoder.
question is how to combine the hidden states and cell
states from each encoder, to pass on to the decoder.
Black combiner blocks implement a function whose
input is two hidden states (h1 and h2) and two cell
states (c1 and c2), and whose output is a single hid-
1We follow previous authors in presenting the source sentence to the encoder in reverse order.
den state h and cell state c. We propose two combination methods.
Basic Combination Method
The Basic method works by concatenating the two
hidden states from the source encoders, applying a
linear transformation Wc , then
sending its output through a tanh non-linearity. This
operation is represented by the equation:
Wc[h1; h2]
Wc and all other weights in the network are learned
from example string triples drawn from a trilingual
training corpus.
The new cell state is simply the sum of the two
cell states from the encoders.
c = c1 + c2
We also attempted to concatenate cell states and apply a linear transformation, but training diverges due
to large cell values.
Child-Sum Method
Our second combination method is inspired by the
Child-Sum Tree-LSTMs of Tai et al. . Here,
we use an LSTM variant to combine the two hidden
states and cells. The standard LSTM input, output,
and new cell value are all calculated. Then cell states
from each encoder get their own forget gates. The
ﬁnal cell state and hidden state are calculated as in a
normal LSTM. More precisely:
i = sigmoid
f = sigmoid
o = sigmoid
1 h1 + W o
1 h1 + W u
c = if ⊙uf + f1 ⊙c1 + f2 ⊙c2
h = of ⊙tanh(cf)
This method employs eight new matrices (the
equations),
1000 x 1000. The ⊙symbol represents an elementwise multiplication. In equation 3, i represents the
input gate of a typical LSTM cell. In equation 4,
Figure 2: Multi-source encoder-decoder model for MT. We have two source sentences (C B A and K J I) in different languages.
Each language has its own encoder; it passes its ﬁnal hidden and cell state to a set of combiners (in black). The output of a combiner
is a hidden state and cell state of the same dimension.
there are two forget gates indexed by the subscript i
that serve as the forget gates for each of the incoming cells for each of the encoders. In equation 5, o
represents the output gate of a normal LSTM. i, f,
o, and u are all size-1000 vectors.
Multi-Source Attention
Our single-source attention model is modeled off the
local-p attention model with feed input from Luong
et al. , where hidden states from the top decoder layer can look back at the top hidden states
from the encoder. The top decoder hidden state is
combined with a weighted sum of the encoder hidden states, to make a better hidden state vector ( ˜ht),
which is passed to the softmax output layer. With
input-feeding, the hidden state from the attention
model is sent down to the bottom decoder layer at
the next time step.
The local-p attention model from Luong et al.
 works as follows. First, a position to look at
in the source encoder is predicted by equation 9:
pt = S · sigmoid(vT
p tanh(Wpht))
S is the source sentence length, and vp and Wp are
learned parameters, with vp being a vector of dimension 1000, and Wp being a matrix of dimension
1000 x 1000.
After pt is computed, a window of size 2D + 1 is
looked at in the top layer of the source encoder centered around pt (D = 10). For each hidden state in
this window, we compute an alignment score at(s),
between 0 and 1. This alignment score is computed
by equations 10, 11 and 12:
at(s) = align(ht, hs)exp
−(s −pt)2
align(ht, hs) =
exp(score(ht, hs))
s′ exp(score(ht, hs′))
score(ht, hs) = hT
In equation 10, σ is set to be D/2 and s is the
source index for that hidden state. Wa is a learnable
parameter of dimension 1000 x 1000.
Once all of the alignments are calculated, ct is created by taking a weighted sum of all source hidden
states multiplied by their alignment weight.
The ﬁnal hidden state sent to the softmax layer is
˜ht = tanh
Wc[ht; ct]
We modify this attention model to look at both
source encoders simultaneously. We create a context
vector from each source encoder named c1
instead of the just ct in the single-source attention
˜ht = tanh
In our multi-source attention model we now have
two pt variables, one for each source encoder. We
Word tokens
Word types
Segment pairs
Ave. segment
length (tokens)
Figure 3: Trilingual corpus statistics.
also have two separate sets of alignments and therefore now have two ct values denoted by c1
mentioned above. We also have distinct Wa, vp, and
Wp parameters for each encoder.
Experiments
We use English, French, and German data from a
subset of the WMT 2014 dataset .
Figure 3 shows statistics for our training set. For development, we use the 3000 sentences supplied by
WMT. For testing, we use a 1503-line trilingual subset of the WMT test set.
For the single-source models, we follow the training procedure used in Luong et al. , but with
15 epochs and halving the learning rate every full
epoch after the 10th epoch. We also re-scale the
normalized gradient when norm > 5. For training,
we use a minibatch size of 128, a hidden state size
of 1000, and dropout as in Zaremba et al. .
The dropout rate is 0.2, the initial parameter range
is [-0.1, +0.1], and the learning rate is 1.0. For the
normal and multi-source attention models, we adjust these parameters to 0.3, [-0.08, +0.08], and 0.7,
respectively, to adjust for overﬁtting.
Figure 4 shows our results for target English,
with source languages French and German. We see
that the Basic combination method yields a +4.8
Bleu improvement over the strongest single-source,
attention-based system. It also improves Bleu by
+2.2 over the non-attention baseline.
The Child-
Sum method gives improvements of +4.4 and +1.4.
We conﬁrm that two copies of the same French input
yields no BLEU improvement. Figure 5 shows the
action of the multi-attention model during decoding.
When our source languages are English and
French (Figure 6), we observe smaller BLEU gains
(up to +1.1). This is evidence that the more distinct
the source languages, the better they disambiguate
each other.
Target = English
French+German
French+German
French+French
French+German
French+German
CS-Attent.
Figure 4: Multi-source MT for target English, with source languages French and German. Ppl reports test-set perplexity as
the system predicts English tokens. BLEU is scored using the
multi-bleu.perl script from Moses. For our evaluation we use a
single reference and they are case sensitive.
Source 1: UNK Aspekte sind ebenfalls wichtig .
Target: UNK aspects are important , too .
Source 2: Les aspects UNK sont également importants .
Figure 5: Action of the multi-attention model as the neural
decoder generates target English from French/German sources
(test set). Lines show strengths of at(s).
Conclusion
We describe a multi-source neural MT system that
gets up to +4.8 Bleu gains over a very strong
attention-based, single-source baseline.
We obtain this result through a novel encoder-vector combination method and a novel multi-attention system. We release the code for these experiments at
www.github.com/isi-nlp/Zoph RNN.
Target = German
French+English
French+English
French+English
French+English
CS-Attent.
Figure 6: Multi-source MT results for target German, with
source languages French and English.
Acknowledgments
This work was carried out with funding from
(HR0011-15-C-0115)
(W911NF-10-1-0533).