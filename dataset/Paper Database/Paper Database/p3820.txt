CSWin Transformer: A General Vision Transformer Backbone with
Cross-Shaped Windows
Xiaoyi Dong1*, Jianmin Bao2, Dongdong Chen3, Weiming Zhang1,
Nenghai Yu1, Lu Yuan3, Dong Chen2, Baining Guo2
1University of Science and Technology of China
2Microsoft Research Asia 3Microsoft Cloud + AI
{dlight@mail., zhangwm@, ynh@}.ustc.edu.cn
 
{jianbao, luyuan, doch, bainguo }@microsoft.com
We present CSWin Transformer, an efﬁcient and effective Transformer-based backbone for general-purpose vision
tasks. A challenging issue in Transformer design is that
global self-attention is very expensive to compute whereas
local self-attention often limits the ﬁeld of interactions of
each token. To address this issue, we develop the Cross-
Shaped Window self-attention mechanism for computing
self-attention in the horizontal and vertical stripes in parallel
that form a cross-shaped window, with each stripe obtained
by splitting the input feature into stripes of equal width. We
provide a mathematical analysis of the effect of the stripe
width and vary the stripe width for different layers of the
Transformer network which achieves strong modeling capability while limiting the computation cost. We also introduce
Locally-enhanced Positional Encoding (LePE), which handles the local positional information better than existing
encoding schemes. LePE naturally supports arbitrary input
resolutions, and is thus especially effective and friendly for
downstream tasks. Incorporated with these designs and a hierarchical structure, CSWin Transformer demonstrates competitive performance on common vision tasks. Speciﬁcally,
it achieves 85.4% Top-1 accuracy on ImageNet-1K without
any extra training data or label, 53.9 box AP and 46.4 mask
AP on the COCO detection task, and 52.2 mIOU on the
ADE20K semantic segmentation task, surpassing previous
state-of-the-art Swin Transformer backbone by +1.2, +2.0,
+1.4, and +2.0 respectively under the similar FLOPs setting.
By further pretraining on the larger dataset ImageNet-21K,
we achieve 87.5% Top-1 accuracy on ImageNet-1K and high
segmentation performance on ADE20K with 55.7 mIoU.
*Work done during an internship at Microsoft Research Asia.
1. Introduction
Transformer-based architectures have recently achieved competitive performances compared to their
CNN counterparts in various vision tasks. By leveraging
the multi-head self-attention mechanism, these vision Transformers demonstrate a high capability in modeling the longrange dependencies, which is especially helpful for handling
high-resolution inputs in downstream tasks, e.g., object detection and segmentation. Despite the success, the Transformer architecture with full-attention mechanism is
computationally inefﬁcient.
To improve the efﬁciency, one typical way is to limit
the attention region of each token from full-attention to local/windowed attention . To bridge the connection
between windows, researchers further proposed halo and
shift operations to exchange information through nearby windows. However, the receptive ﬁeld is enlarged quite slowly
and it requires stacking a great number of blocks to achieve
global self-attention. A sufﬁciently large receptive ﬁeld is
crucial to the performance especially for the downstream
tasks(e.g., object detection and segmentation). Therefore it
is important to achieve large receptive ﬁled efﬁciently while
keeping the computation cost low.
In this paper, we present the Cross-Shaped Window
(CSWin) self-attention, which is illustrated in Figure 1 and
compared with existing self-attention mechanisms. With
CSWin self-attention, we perform the self-attention calculation in the horizontal and vertical stripes in parallel, with
each stripe obtained by splitting the input feature into stripes
of equal width. This stripe width is an important parameter
of the cross-shaped window because it allows us to achieve
strong modelling capability while limiting the computation
cost. Speciﬁcally, we adjust the stripe width according to the
depth of the network: small widths for shallow layers and
larger widths for deep layers. A larger stripe width encourages a stronger connection between long-range elements and
 
Shifted Local
Sequential Axial
Split Head
Full Attention
Criss-Cross
Dynaic Stripe Window + Parallel Grouing Heads = CSWin
Slide Local
Local + Global
Figure 1. Illustration of different self-attention mechanisms, our CSWin is fundamentally different from two aspects. First, we split
multi-heads ({h1, . . . , hK}) into two groups and perform self-attention in horizontal and vertical stripes simultaneously. Second, we adjust
the stripe width according to the depth network, which can achieve better trade-off between computation cost and capability
achieves better network capacity with a small increase in
computation cost. We will provide a mathematical analysis
of how the stripe width affects the modeling capability and
computation cost.
It is worthwhile to note that with CSWin self-attention
mechanism, the self-attention in horizontal and vertical
stripes are calculated in parallel. We split the multi-heads
into parallel groups and apply different self-attention operations onto different groups. This parallel strategy introduces no extra computation cost while enlarging the area
for computing self-attention within each Transformer block.
This strategy is fundamentally different from existing selfattention mechanisms that apply the same
attention operation across multi-heads((Figure 1 b,c,d,e), and
perform different attention operations sequentially(Figure 1
c,e). We will show through ablation analysis that this difference makes CSWin self-attention much more effective for
general vision tasks.
Based on the CSWin self-attention mechanism, we follow the hierarchical design and propose a new vision
Transformer architecture named “CSWin Transformer” for
general-purpose vision tasks. This architecture provides
signiﬁcantly stronger modeling power while limiting computation cost. To further enhance this vision Transformer, we
introduce an effective positional encoding, Locally-enhanced
Positional Encoding (LePE), which is especially effective
and friendly for input varying downstream tasks such as object detection and segmentation. Compared with previous
positional encoding methods , our LePE imposes
the positional information within each Transformer block
and directly operates on the attention results instead of the
attention calculation. The LePE makes CSWin Transformer
more effective and friendly for the downstream tasks.
As a general vision Transformer backbone, the CSWin
Transformer demonstrates strong performance on image classiﬁcation, object detection and semantic segmentation tasks.
Under the similar FLOPs and model size, CSWin Transformer variants signiﬁcantly outperforms previous stateof-the-art (SOTA) vision Transformers. For example, our
base variant CSWin-B achieves 85.4% Top-1 accuracy on
ImageNet-1K without any extra training data or label, 53.9
box AP and 46.4 mask AP on the COCO detection task, 51.7
mIOU on the ADE20K semantic segmentation task, surpassing previous state-of-the-art Swin Transformer counterpart
by +1.2, +2.0, 1.4 and +2.0 respectively. Under a smaller
FLOPs setting, our tiny variant CSWin-T even shows larger
performance gains, i.e.,, +1.4 point on ImageNet classiﬁcation, +3.0 box AP, +2.0 mask AP on COCO detection and
+4.6 on ADE20K segmentation. Furthermore, when pretraining CSWin Transformer on the larger dataset ImageNet-21K,
we achieve 87.5% Top-1 accuracy on ImageNet-1K and high
segmentation performance on ADE20K with 55.7 mIoU.
2. Related Work
Vision Transformers.
Convolutional neural networks
(CNN) have dominated the computer vision ﬁeld for many
years and achieved tremendous successes . Recently, the pioneering work ViT 
demonstrates that pure Transformer-based architectures can
also achieve very competitive results, indicating the potential
of handling the vision tasks and natural language processing
(NLP) tasks under a uniﬁed framework. Built upon the success of ViT, many efforts have been devoted to designing better Transformer based architectures for various vision tasks,
including low-level image processing , image classiﬁcation , object
detection and semantic segmentation .
Rather than concentrating on one special task, some recent
works try to design a general vision Transformer
backbone for general-purpose vision tasks. They all follow
the hierarchical Transformer architecture but adopt different self-attention mechanisms. The main beneﬁt of the hierarchical design is to utilize the multi-scale features and
reduce the computation complexity by progressively decreas-
Transformer Block
Cross-Shaped
Window Self-Attention
CSwin Transformer Block
Convolutional
Token Embedding
Transformer Block
Transformer Block
Transformer Block
Figure 2. Left: the overall architecture of our proposed CSWin Transformer, Right: the illustration of CSWin Transformer block.
ing the number of tokens. In this paper,we propose a new
hierarchical vision Transformer backbone by introducing
cross-shaped window self-attention and locally-enhanced
positional encoding.
Efﬁcient Self-attentions. In the NLP ﬁeld, many efﬁcient
attention mechanisms have been
designed to improve the Transformer efﬁciency for handling long sequences. Since the image resolution is often
very high in vision tasks, designing efﬁcient self-attention
mechanisms is also very crucial. However, many existing vision Transformers still adopt the original full
self-attention, whose computation complexity is quadratic
to the image size. To reduce the complexity, the recent
vision Transformers adopt the local self-attention
mechanism and its shifted/haloed version to add the
interaction across different local windows. Besides, axial
self-attention and criss-cross attention propose calculating attention within stripe windows along horizontal
or/and vertical axis. While the performance of axial attention is limited by its sequential mechanism and restricted
window size, criss-cross attention is inefﬁcient in practice
due to its overlapped window design and ineffective due to
its restricted window size. They are the most related works
with our CSWin, which could be viewed as a much general
and efﬁcient format of these previous works.
Positional Encoding. Since self-attention is permutationinvariant and ignores the token positional information, positional encoding is widely used in Transformers to add
such positional information back. Typical positional encoding mechanisms include absolute positional encoding
(APE) , relative positional encoding (RPE) and
conditional positional encoding (CPE) . APE and RPE
are often deﬁned as the sinusoidal functions of a series of
frequencies or the learnable parameters, which are designed
for a speciﬁc input size and are not friendly to varying input
resolutions. CPE takes the feature as input and can generate
the positional encoding for arbitrary input resolutions. Then
the generated positional encoding will be added onto the
input feature. Our LePE shares a similar spirit as CPE, but
proposes to add the positional encoding as a parallel module to the self-attention operation and operates on projected
values in each Transformer block. This design decouples
positional encoding from the self-attention calculation, and
can enforce stronger local inductive bias.
3.1. Overall Architecture
The overall architecture of CSWin Transformer is illustrated in Figure 2. For an input image with size of H×W ×3,
we follow and leverage the overlapped convolutional
token embedding (7 × 7 convolution layer with stride 4) )
to obtain H
4 patch tokens, and the dimension of each
token is C. To produce a hierarchical representation, the
whole network consists of four stages. A convolution layer
(3 × 3, stride 2) is used between two adjacent stages to reduce the number of tokens and double the channel dimension.
Therefore, the constructed feature maps have
tokens for the ith stage, which is similar to traditional CNN
backbones like VGG/ResNet. Each stage consists of Ni
sequential CSWin Transformer Blocks and maintains the
number of tokens. CSWin Transformer Block has the overall similar topology as the vanilla multi-head self-attention
Transformer block with two differences: 1) It replaces the
self-attention mechanism with our proposed Cross-Shaped
Window Self-Attention; 2) In order to introduce the local
inductive bias, LePE is added as a parallel module to the
self-attention branch.
3.2. Cross-Shaped Window Self-Attention
Despite the strong long-range context modeling capability, the computation complexity of the original full selfattention mechanism is quadratic to feature map size. Therefore, it will suffer from huge computation cost for vision
tasks that take high resolution feature maps as input, such
as object detection and segmentation. To alleviate this issue,
existing works suggest to perform self-attention in a
local attention window and apply halo or shifted window to
enlarge the receptive ﬁled. However, the token within each
Transformer block still has limited attention area and requires stacking more blocks to achieve global receptive ﬁled.
To enlarge the attention area and achieve global self-attention
more efﬁciently, we present the cross-shaped window selfattention mechanism, which is achieved by performing selfattention in horizontal and vertical stripes in parallel that
APE/CPE(X)
Transformer block
Transformer block
Transformer block
Figure 3. Comparison among different positional encoding mechanisms: APE and CPE introduce the positional information before feeding
into the Transformer blocks, while RPE and our LePE operate in each Transformer block. Different from RPE that adds the positional
information into the attention calculation, our LePE operates directly upon V and acts as a parallel module. ∗Here we only draw the
self-attention part to represent the Transformer block for simplicity.
form a cross-shaped window.
Horizontal and Vertical Stripes. According to the multihead self-attention mechanism, the input feature X
R(H×W )×C will be ﬁrst linearly projected to K heads, and
then each head will perform local self-attention within either
the horizontal or vertical stripes.
For horizontal stripes self-attention, X is evenly partitioned into non-overlapping horizontal stripes [X1, .., XM]
of equal width sw, and each of them contains sw × W tokens. Here, sw is the stripe width and can be adjusted to
balance the learning capacity and computation complexity.
Formally, suppose the projected queries, keys and values of
the kth head all have dimension dk, then the output of the
horizontal stripes self-attention for kth head is deﬁned as:
X = [X1, X2, . . . , XM],
k = Attention(XiW Q
H-Attentionk(X) = [Y 1
k , . . . , Y M
Where where Xi ∈R(sw×W )×C and M = H/sw, i =
1, . . . , M. W Q
k ∈RC×dk, W K
k ∈RC×dk, W V
represent the projection matrices of queries, keys and values
for the kth head respectively, and dk is set as C/K. The
vertical stripes self-attention can be similarly derived, and
its output for kth head is denoted as V-Attentionk(X).
Assuming natural images do not have directional bias,
we equally split the K heads into two parallel groups (each
has K/2 heads, K is often an even value). The ﬁrst group
of heads perform horizontal stripes self-attention while the
second group of heads perform vertical stripes self-attention.
Finally the output of these two parallel groups will be concatenated back together.
CSWin-Attention(X) = Concat(head1, ..., headK)W O
H-Attentionk(X)
k = 1, . . . , K/2
V-Attentionk(X)
k = K/2 + 1, . . . , K
Where W O ∈RC×C is the commonly used projection
matrix that projects the self-attention results into the target output dimension (set as C by default). As described
above, one key insight in our self-attention mechanism design is splitting the multi-heads into different groups and
applying different self-attention operations accordingly. In
other words, the attention area of each token within one
Transformer block is enlarged via multi-head grouping. By
contrast, existing self-attention mechanisms apply the same
self-attention operations across different multi-heads. In the
experiment parts, we will show that this design will bring
better performance.
Computation Complexity Analysis.
The computation
complexity of CSWin self-attention is:
Ω(CSWin) = HWC ∗(4C + sw ∗H + sw ∗W)
For high-resolution inputs, considering H, W will be
larger than C in the early stages and smaller than C in the
later stages, we choose small sw for early stages and larger
sw for later stages. In other words, adjusting sw provides the
ﬂexibility to enlarge the attention area of each token in later
stages in an efﬁcient way. Besides, to make the intermediate
feature map size divisible by sw for 224 × 224 input, we
empirically set sw to 1, 2, 7, 7 for four stages by default.
Locally-Enhanced Positional Encoding. Since the selfattention operation is permutation-invariant, it will ignore
the important positional information within the 2D image.
To add such information back, different positional encoding
mechanisms have been utilized in existing vision Transformers. In Figure 3, we show some typical positional encoding
mechanisms and compare them with our proposed locallyenhanced positional encoding. In details, APE and
CPE add the positional information into the input token
before feeding into the Transformer blocks, while RPE 
and our LePE incorporate the positional information within
each Transformer block. But different from RPE that adds
the positional information within the attention calculation
(i.e., Softmax(QKT )), we consider a more straightforward
manner and impose the positional information upon the linearly projected values. Meanwhile, we notice that RPE
introduces bias in a per head manner, while our LePE is a
per-channel bias, which may show more potential to serve
as positional embeddings.
Mathematically, we denote the input sequence as x =
(x1, . . . , xn) of n elements, and the output of the attention
z = (z1, . . . , zn) of the same length, where xi, zi ∈RC.
Self-attention computation could be formulated as:
αijvj, αij = exp(qT
where qi, ki, vi are the queue, key and value get by a
linear transformation of the input xi and d is the feature
#Dim #Blocks
#Param. FLOPs
1,2,21,1 1,2,7,7 2,4,8,16
2,4,32,2 1,2,7,7 2,4,8,16
2,4,32,2 1,2,7,7 4,8,16,32
2,4,32,2 1,2,7,7 6,12,24,48 173M
Table 1. Detailed conﬁgurations of different variants of CSWin
Transformer. The FLOPs are calculated with 224 × 224 input.
dimension. Then our Locally-Enhanced position encoding
performs as a learnable per-element bias and Eq.4 could be
formulated as:
i represents the kth element of vector zi. To make
the LePE suitable to varying input size, we set a distance
threshold to the LePE and set it to 0 if the Chebyshev distance of token i and j is greater than a threshold τ (τ = 3 in
the default setting).
3.3. CSWin Transformer Block
Equipped with the above self-attention mechanism and
positional embedding mechanism, CSWin Transformer
block is formally deﬁned as:
ˆXl = CSWin-Attention
where Xl denotes the output of l-th Transformer block or
the precedent convolutional layer of each stage.
3.4. Architecture Variants
For a fair comparison with other vision Transformers
under similar settings, we build four different variants of
CSWin Transformer as shown in Table 1: CSWin-T (Tiny),
CSWin-S (Small), CSWin-B (Base), CSWin-L (Large). They
are designed by changing the base channel dimension C and
the block number of each stage. In all these variants, the
expansion ratio of each MLP is set as 4. The head number of
the four stages is set as 2, 4, 8, 16 in the ﬁrst three variants
and 6, 12, 24, 48 in the last variant respectively.
4. Experiments
To show the effectiveness of CSWin Transformer as a general vision backbone, we conduct experiments on ImageNet-
1K classiﬁcation, COCO object detection, and
ADE20K semantic segmentation. We also perform
comprehensive ablation studies to analyze each component
of CSWin Transformer. As most of the methods we compared did not report downstream inference speed, we use an
extra section to report it for simplicity.
Image Size #Param. FLOPs Throughput Top-1
Eff-B4 
Eff-B5 
Eff-B6 
DeiT-S 
DeiT-B 
DeiT-B 
PVT-S 
PVT-M 
PVT-L 
T2Tt-14 
T2Tt-19 
T2Tt-24 
CvT-13 
CvT-21 
CvT-21 
Swin-T 
Swin-S 
Swin-B 
Swin-B 
Table 2. Comparison of different models on ImageNet-1K.
Method Param Size FLOPs Top-1 Method Param Size FLOPs Top-1
R-101x3 388M 3842 204.6G 84.4 R-152x4 937M 4802 840.5G 85.4
ViT-B/16 86M 3842 55.4G 84.0 ViT-L/16 307M 3842 190.7G 85.2
88M 2242 15.4G 85.2
197M 2242 34.5G 86.3
3842 47.1G 86.4
3842 103.9G 87.3
2242 15.0G 85.9
2242 31.5G 86.5
CSWin-B 78M 3842 47.0G 87.0 CSWin-L 173M 3842 96.8G 87.5
ImageNet-1K ﬁne-tuning results by pre-training on
ImageNet-21K datasets.
4.1. ImageNet-1K Classiﬁcation
For fair comparison, we follow the training strategy
in DeiT as other baseline Transformer architectures
 . Speciﬁcally, all our models are trained for 300
epochs with the input size of 224×224. We use the AdamW
optimizer with weight decay of 0.05 for CSWin-T/S and
0.1 for CSWin-B. The default batch size and initial learning
rate are set to 1024 and 0.001, and the cosine learning rate
scheduler with 20 epochs linear warm-up is used. We apply
increasing stochastic depth augmentation for CSWin-T,
CSWin-S, and CSWin-B with the maximum rate as 0.1, 0.3,
0.5 respectively. When reporting the results of 384 × 384
input, we ﬁne-tune the models for 30 epochs with the weight
Mask R-CNN 1x schedule
Mask R-CNN 3x + MS schedule
Res50 
PVT-S 
ViL-S 
TwinsP-S 
Twins-S 
Swin-T 
Res101 
X101-32 
PVT-M 
ViL-M 
TwinsP-B 
Twins-B 
Swin-S 
X101-64 
PVT-L 
ViL-B 
TwinsP-L 
Twins-L 
Swin-B 
Table 4. Object detection and instance segmentation performance on the COCO val2017 with the Mask R-CNN framework. The FLOPs (G)
are measured at resolution 800 × 1280, and the models are pre-trained on the ImageNet-1K. ResNet/ResNeXt results are copied from .
decay of 1e-8, learning rate of 1e-5, batch size of 512.
In Table 11, we compare our CSWin Transformer with
state-of-the-art CNN and Transformer architectures. With
the limitation of pages, we only compare with a few classical
methods here and make a comprehensive comparison in the
supplemental materials.
It shows that our CSWin Transformers outperform previous state-of-the-art vision Transformers by large margins.
For example, CSWin-T achieves 82.7% Top-1 accuracy with
only 4.3G FLOPs, surpassing CvT-13, Swin-T and DeiT-S
by 1.1%, 1.4% and 2.9% respectively. And for the small and
base model setting, our CSWin-S and CSWin-B also achieve
the best performance. When ﬁnetuned on the 384 × 384
input, a similar trend is observed, which well demonstrates
the powerful learning capacity of our CSWin Transformers.
Compared with state-of-the-art CNNs, we ﬁnd our CSWin
Transformer is the only Transformer based architecture that
achieves comparable or even better results than Efﬁcient-
Net under the small and base settings, while using less
computation complexity . It is also worth noting that neural architecture search is used in EfﬁcientNet but not in our
CSWin Transformer design.
We further pre-train CSWin Transformer on ImageNet-
21K dataset, which contains 14.2M images and 21K classes.
Models are trained for 90 epochs with the input size of
224×224. We use the AdamW optimizer with weight decay
of 0.1 for CSWin-B and 0.2 for CSWin-L, and the default
batch size and initial learning rate are set to 2048 and 0.001.
When ﬁne-tuning on ImageNet-1K, we train the models for
30 epochs with the weight decay of 1e-8, learning rate of
1e-5, batch size of 512. The increasing stochastic depth 
#Params FLOPs
Cascade Mask R-CNN 3x +MS
75 AP m AP m
Res50 
Swin-T 
X101-32 
Swin-S 
X101-64 
Swin-B 
Table 5. Object detection and instance segmentation performance
on the COCO val2017 with Cascade Mask R-CNN.
augmentation for both CSWin-B and CSWin-L is set to 0.1.
Table.3 reports the results of pre-training on ImageNet-
21K. Compared to the results of CSWin-B pre-trained on
ImageNet-1K, the large-scale data of ImageNet-21K brings
a 1.6%∼1.7% gain. CSWin-B and CSWin-L achieve 87.0%
and 87.5% top-1 accuracy, surpassing previous methods.
4.2. COCO Object Detection
Next, we evaluate CSWin Transformer on the COCO
objection detection task with the Mask R-CNN and
Cascade Mask R-CNN framework respectively. Speciﬁcally, we pretrain the backbones on the ImageNet-1K dataset
and follow the ﬁnetuning strategy used in Swin Transformer
 on the COCO training set.
We compare CSWin Transformer with various backbones:
previous CNN backbones ResNet , ResNeXt(X) ,
and Transformer backbones PVT , Twins , and
Swin . Table 4 reports the results of the Mask R-CNN
Semantic FPN 80k
Upernet 160k
#Param.FLOPsmIoU #Param.FLOPsSS/MS mIoU
Res50 
PVT-S 
TwinsP-S 
Twins-S 
Swin-T 
Res101 
PVT-M 
TwinsP-B 
Twins-B 
Swin-S 
X101-64 
PVT-L 
TwinsP-L 
Twins-L 
Swin-B 
Swin-B† 
Swin-L† 
Table 6. Performance comparison of different backbones on the
ADE20K segmentation task. Two different frameworks semantic
FPN and Upernet are used. FLOPs are calculated with resolution
512 × 2048. ResNet/ResNeXt results and Swin FPN results are
copied from and respectively. † means the model is pretrained on ImageNet-21K and ﬁnetuned with 640×640 resolution.
framework with “1×” (12 training epoch) and “3 × +MS”
(36 training epoch with multi-scale training) schedule. It
shows that our CSWin Transformer variants clearly outperforms all the CNN and Transformer counterparts. In details,
our CSWin-T outperforms Swin-T by +4.5 box AP, +3.1
mask AP with the 1× schedule and +3.0 box AP, +2.0 mask
AP with the 3× schedule respectively. We also achieve
similar performance gain on small and base conﬁguration.
Table 5 reports the results with the Cascade Mask R-
CNN framework. Though Cascade Mask R-CNN is overall
stronger than Mask R-CNN, we observe CSWin Transformers still surpass the counterparts by promising margins under
different model conﬁgurations.
4.3. ADE20K Semantic Segmentation
We further investigate the capability of CSWin Transformer for Semantic Segmentation on the ADE20K 
dataset. Here we employ the semantic FPN and Upernet as the basic framework. For fair comparison, we
follow previous works and train Semantic FPN 80k
iterations with batch size as 16, and Upernet 160k iterations
with batch size as 16, more details are provided in the supplementary material. In Table 6, we report the results of
different methods in terms of mIoU and Multi-scale tested
Cascade Mask R-CNN on COCO
UperNet on ADE20K
#Param. FLOPs FPS
#Param. FLOPs FPS mIoU
745G 15.3 50.5/43.7
945G 18.5 44.5
757G 14.2 52.5/45.3
959G 17.3 49.3
838G 12.0 51.8/44.7
1038G 15.2 47.6
820G 11.7 53.7/46.4
1027G 15.6 50.4
982G 11.2 51.9/45.0
1188G 9.92 48.1
1222G 9.08 51.1
Table 7. FPS comparison with Swin on downstream tasks.
mIoU (MS mIoU). It can be seen that, our CSWin Transformers signiﬁcantly outperform previous state-of-the-arts
under different conﬁgurations. In details, CSWin-T, CSWin-
S, CSWin-B achieve +6.7, +4.0, +3.9 higher mIOU than the
Swin counterparts with the Semantic FPN framework, and
+4.8, +2.8, +3.0 higher mIOU with the Upernet framework.
Compared to the CNN counterparts, the performance gain
is very promising and demonstrates the potential of vision
Transformers again. When using the ImageNet-21K pretrained model, our CSWin-L further achieves 55.7 mIoU
and surpasses the previous best model by +2.2 mIoU, while
using less computation complexity.
4.4. Inference Speed.
Here we report the inference speed of our CSWin and
Swin works. For downstream tasks, we report the FPS of
Cascade Mask R-CNN for object detection on COCO and
UperNet for semantic segmentation on ADE20K. In most
cases, the speed of our model is only slightly slower than
Swin (less than 10%), but our model outperforms Swin by
large margins. For example, on COCO, CSWin-S are +1.9%
box AP and +1.7% mask AP higher than Swin-S with similar inference speed(11.7 FPS vs. 12 FPS). Note that our
CSWin-T performs better than Swin-B on box AP(+0.6%),
mask AP(+0.3%) with much faster inference speed(14.2
FPS vs. 11.2 FPS), indicating our CSWin achieves better
accuracy/FPS trade-offs.
4.5. Ablation Study
To better understand CSWin Transformers, we compare
each key component with the previous works under a completely fair setting that we use the same architecture and
hyper-parameter for the following experiments, and only
vary one component for each ablation. For time consideration, we use Mask R-CNN with 1x schedule as the default
setting for detection and instance segmentation evaluation,
and Semantic FPN with 80k iterations and single-scale test
for segmentation evaluation.
Parallel Multi-Head Grouping. We ﬁrst study the effectiveness of our novel “Parallel Multi-Head Grouping” strategy. Here we compare Axial-Attention and Criss-Cross-
Attention under the CSWin-T backbone. “Attention
region” is used as the computation cost metric for detailed
#Param. FLOPs FPS Top1(%) #Param. FLOPs FPS APb APm #Param. FLOPs FPS mIoU(%)
27.9 43.4 39.4
CSWin (ﬁx sw=1)
26.8 45.2 40.8
Criss-Cross
CSWin (ﬁx sw=2)
25.1 45.6 41.4
CSWin (sw=1,2,7,7; Seq)
22.3 45.1 41.1
CSWin (sw=1,2,7,7)
21.1 46.7 42.2
Table 8. Stripes-Based attention mechanism comparison. ‘Seq’ means sequential multi-head attention like Axial-attention. ‘Attention
Region’ means the average number of tokens that each head calculates attention with.
sw= 
sw= 
sw= 
Figure 4. Ablation on dynamic window size.
comparison. To simplify, we assume the attention is calculated on a square input that H = W.
In Table.8, we ﬁnd that the “parallel multi-head grouping”
is efﬁcient and effective, especially for downstream tasks.
When we replace the Parallel manner with Sequential, the
performance of CSWin degrades on all tasks. When comparing with previous methods under the similar attention region
constrain, our sw = 1 CSWin performs slightly better than
Axial on ImageNet, while outperforming it by a large margin
on downstream tasks. Our sw = 2 CSWin performs slightly
better than Criss-Cross Attention, while the speed of CSWin
is 2× ∼5× faster than it on different tasks, this further
proves that our “parallel” design is much more efﬁcient.
Dynamic Stripe Width . In Fig.4 we study the trade off
between stripe width and accuracy. We ﬁnd that with the increase of stripe width, the compution cost(FLOPS) increase,
and the Top-1 classiﬁcation accuracy improves greatly at the
beginning and slows down when the width is large enough.
Our default setting achieves a good trade-off between accuracy and FLOPs.
Attention Mechanism Comparison. Following the above
analysis on each component of CSWin self-attention, we
further compare with existing self-attention mechanisms. As
some of the methods need even layers in each stage, for a
fair comparison, we use the Swin-T as backbone and
only change the self-attention mechanism. In detail, we
use 2, 2, 6, 2 blocks for the four stages with the 96 base channel, non-overlapped token embedding , and RPE .
The results are reported in Table 9. Obviously, our CSWin
self-attention mechanism performs better than existing selfattention mechanisms across all the tasks.
Positional Encoding Comparison. The proposed LePE
is specially designed to enhance the local positional infor-
Sliding window 
Shifted window 
Spatially Sep 
Sequential Axial 
Criss-Cross 
Cross-shaped window
Table 9. Comparison of different self-attention mechanisms.
Table 10. Comparison of different positional encoding mechanisms.
mation on downstream tasks for various input resolutions.
Here we use CSWin-T as the backbone and only very
the position encoding. In Table 10, we compare our LePE
with other recent positional encoding mechanisms(APE ,
CPE , and RPE ) for image classiﬁcation, object
detection and image segmentation. Besides, we also test
the variants without positional encoding (No PE) and CPE*,
which is obtained by applying CPE before every Transformer
block. According to the comparison results, we see that: 1)
Positional encoding can bring performance gain by introducing the local inductive bias; 2) Though RPE achieves similar
performance on the classiﬁcation task with ﬁxed input resolution, our LePE performs better (+1.2 box AP and +0.9 mask
AP on COCO, +0.9 mIoU on ADE20K) on downstream
tasks where the input resolution varies; 3) Compared to APE
and CPE, our LePE also achieves better performance.
5. Conclusion
In this paper, we have presented a new Vision Transformer architecture named CSWin Transformer. The core
design of CSWin Transformer is the CSWin Self-Attention,
which performs self-attention in the horizontal and vertical
stripes by splitting the multi-heads into parallel groups. This
multi-head grouping design can enlarge the attention area
of each token within one Transformer block efﬁciently. On
the other hand, the mathematical analysis also allows us to
increase the stripe width along the network depth to further
enlarge the attention area with subtle extra computation cost.
We further introduce locally-enhanced positional encoding
into CSWin Transformer for downstream tasks. We achieved
the state-of-the-art performance on various vision tasks under constrained computation complexity. We are looking
forward to applying it for more vision tasks.