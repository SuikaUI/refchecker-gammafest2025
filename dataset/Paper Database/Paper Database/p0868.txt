RoBERTa: A Robustly Optimized BERT Pretraining Approach
Yinhan Liu∗§
Myle Ott∗§
Naman Goyal∗§
Jingfei Du∗§
Mandar Joshi†
Danqi Chen§
Omer Levy§
Mike Lewis§
Luke Zettlemoyer†§
Veselin Stoyanov§
† Paul G. Allen School of Computer Science & Engineering,
University of Washington, Seattle, WA
{mandar90,lsz}@cs.washington.edu
§ Facebook AI
{yinhanliu,myleott,naman,jingfeidu,
danqi,omerlevy,mikelewis,lsz,ves}@fb.com
Language model pretraining has led to signiﬁcant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different
sizes, and, as we will show, hyperparameter
choices have signiﬁcant impact on the ﬁnal results. We present a replication study of BERT
pretraining that carefully
measures the impact of many key hyperparameters and training data size. We ﬁnd that BERT
was signiﬁcantly undertrained, and can match
or exceed the performance of every model
published after it.
Our best model achieves
state-of-the-art results on GLUE, RACE and
SQuAD. These results highlight the importance of previously overlooked design choices,
and raise questions about the source of recently reported improvements. We release our
models and code.1
Introduction
Self-training methods such as ELMo , XLM , which includes a
careful evaluation of the effects of hyperparmeter
tuning and training set size. We ﬁnd that BERT
was signiﬁcantly undertrained and propose an improved recipe for training BERT models, which
we call RoBERTa, that can match or exceed the
performance of all of the post-BERT methods.
Our modiﬁcations are simple, they include: (1)
training the model longer, with bigger batches,
over more data; (2) removing the next sentence
prediction objective; (3) training on longer sequences; and (4) dynamically changing the masking pattern applied to the training data. We also
collect a large new dataset (CC-NEWS) of comparable size to other privately used datasets, to better
control for training set size effects.
When controlling for training data, our improved training procedure improves upon the published BERT results on both GLUE and SQuAD.
When trained for longer over additional data, our
model achieves a score of 88.5 on the public
GLUE leaderboard, matching the 88.4 reported
by Yang et al. .
Our model establishes a
new state-of-the-art on 4/9 of the GLUE tasks:
MNLI, QNLI, RTE and STS-B. We also match
state-of-the-art results on SQuAD and RACE.
Overall, we re-establish that BERT’s masked language model training objective is competitive
with other recently proposed training objectives
such as perturbed autoregressive language modeling .2
In summary, the contributions of this paper
are: (1) We present a set of important BERT design choices and training strategies and introduce
2It is possible that these other methods could also improve
with more tuning. We leave this exploration to future work.
alternatives that lead to better downstream task
performance; (2) We use a novel dataset, CC-
NEWS, and conﬁrm that using more data for pretraining further improves performance on downstream tasks; (3) Our training improvements show
that masked language model pretraining, under
the right design choices, is competitive with all
other recently published methods. We release our
model, pretraining and ﬁne-tuning code implemented in PyTorch .
Background
In this section, we give a brief overview of the
BERT pretraining approach
and some of the training choices that we will examine experimentally in the following section.
BERT takes as input a concatenation of two
 , which we will not
review in detail. We use a transformer architecture
with L layers. Each block uses A self-attention
heads and hidden dimension H.
Training Objectives
During pretraining, BERT uses two objectives:
masked language modeling and next sentence prediction.
Masked Language Model (MLM)
sample of the tokens in the input sequence is
selected and replaced with the special token
[MASK]. The MLM objective is a cross-entropy
loss on predicting the masked tokens. BERT uniformly selects 15% of the input tokens for possible replacement. Of the selected tokens, 80% are
replaced with [MASK], 10% are left unchanged,
and 10% are replaced by a randomly selected vocabulary token.
In the original implementation, random masking and replacement is performed once in the beginning and saved for the duration of training, although in practice, data is duplicated so the mask
is not always the same for every training sentence
(see Section 4.1).
Next Sentence Prediction (NSP)
NSP is a binary classiﬁcation loss for predicting whether two
segments follow each other in the original text.
Positive examples are created by taking consecutive sentences from the text corpus. Negative examples are created by pairing segments from different documents. Positive and negative examples
are sampled with equal probability.
The NSP objective was designed to improve
performance on downstream tasks, such as Natural
Language Inference , which
require reasoning about the relationships between
pairs of sentences.
Optimization
BERT is optimized with Adam using the following parameters: β1 = 0.9,
1e-6 and L2 weight decay of 0.01.
The learning rate is warmed up
over the ﬁrst 10,000 steps to a peak value of
1e-4, and then linearly decayed.
BERT trains
with a dropout of 0.1 on all layers and attention weights, and a GELU activation function . Models are
pretrained for S = 1,000,000 updates, with minibatches containing B = 256 sequences of maximum length T = 512 tokens.
BERT is trained on a combination of BOOKCOR-
PUS plus English WIKIPEDIA,
which totals 16GB of uncompressed text.3
Experimental Setup
In this section, we describe the experimental setup
for our replication study of BERT.
Implementation
We reimplement BERT in FAIRSEQ use the same dataset but report having
only 13GB of text after data cleaning. This is most likely due
to subtle differences in cleaning of the Wikipedia data.
optimization hyperparameters, given in Section 2,
except for the peak learning rate and number of
warmup steps, which are tuned separately for each
setting. We additionally found training to be very
sensitive to the Adam epsilon term, and in some
cases we obtained better performance or improved
stability after tuning it. Similarly, we found setting
β2 = 0.98 to improve stability when training with
large batch sizes.
We pretrain with sequences of at most T = 512
tokens. Unlike Devlin et al. , we do not randomly inject short sequences, and we do not train
with a reduced sequence length for the ﬁrst 90% of
updates. We train only with full-length sequences.
We train with mixed precision ﬂoating point
arithmetic on DGX-1 machines, each with 8 ×
32GB Nvidia V100 GPUs interconnected by In-
ﬁniband .
BERT-style pretraining crucially relies on large
quantities of text.
Baevski et al. demonstrate that increasing data size can result in improved end-task performance.
Several efforts
have trained on datasets larger and more diverse
than the original BERT . Unfortunately, not all of the additional datasets can be
publicly released. For our study, we focus on gathering as much data as possible for experimentation, allowing us to match the overall quality and
quantity of data as appropriate for each comparison.
We consider ﬁve English-language corpora of
varying sizes and domains, totaling over 160GB
of uncompressed text. We use the following text
• BOOKCORPUS plus English
WIKIPEDIA. This is the original data used to
train BERT. (16GB).
• CC-NEWS, which we collected from the English portion of the CommonCrawl News
dataset .
The data contains 63
million English news articles crawled between
September 2016 and February 2019. (76GB after ﬁltering).4
• OPENWEBTEXT ,
an open-source recreation of the WebText cor-
4We use news-please to collect and extract CC-NEWS. CC-NEWS is similar to the RE-
ALNEWS dataset described in Zellers et al. .
pus described in Radford et al. . The text
is web content extracted from URLs shared on
Reddit with at least three upvotes. (38GB).5
• STORIES, a dataset introduced in Trinh and Le
 containing a subset of CommonCrawl
data ﬁltered to match the story-like style of
Winograd schemas. (31GB).
Evaluation
Following previous work, we evaluate our pretrained models on downstream tasks using the following three benchmarks.
Understanding Evaluation (GLUE) benchmark is a collection of 9 datasets for evaluating
natural language understanding systems.6 Tasks
are framed as either single-sentence classiﬁcation
or sentence-pair classiﬁcation tasks. The GLUE
organizers provide training and development data
splits as well as a submission server and leaderboard that allows participants to evaluate and compare their systems on private held-out test data.
For the replication study in Section 4, we report
results on the development sets after ﬁnetuning
the pretrained models on the corresponding singletask training data (i.e., without multi-task training
or ensembling). Our ﬁnetuning procedure follows
the original BERT paper .
In Section 5 we additionally report test set results obtained from the public leaderboard. These
results depend on a several task-speciﬁc modiﬁcations, which we describe in Section 5.1.
Dataset (SQuAD) provides a paragraph of context
and a question. The task is to answer the question
by extracting the relevant span from the context.
We evaluate on two versions of SQuAD: V1.1
and V2.0 . In V1.1
the context always contains an answer, whereas in
5The authors and their afﬁliated institutions are not in any
way afﬁliated with the creation of the OpenWebText dataset.
(Warstadt et al.,
(Socher et al.,
(Dolan and Brockett,
Textual Similarity Benchmark (STS) ,
Quora Question Pairs (QQP) , Multi-
Genre NLI (MNLI) , Question NLI
 and
Winograd NLI (WNLI) .
V2.0 some questions are not answered in the provided context, making the task more challenging.
For SQuAD V1.1 we adopt the same span prediction method as BERT . For
SQuAD V2.0, we add an additional binary classi-
ﬁer to predict whether the question is answerable,
which we train jointly by summing the classiﬁcation and span loss terms. During evaluation, we
only predict span indices on pairs that are classi-
ﬁed as answerable.
The ReAding Comprehension from Examinations (RACE) task is a
large-scale reading comprehension dataset with
more than 28,000 passages and nearly 100,000
questions. The dataset is collected from English
examinations in China, which are designed for
middle and high school students. In RACE, each
passage is associated with multiple questions. For
every question, the task is to select one correct answer from four options. RACE has signiﬁcantly
longer context than other popular reading comprehension datasets and the proportion of questions
that requires reasoning is very large.
Training Procedure Analysis
This section explores and quantiﬁes which choices
are important for successfully pretraining BERT
models. We keep the model architecture ﬁxed.7
Speciﬁcally, we begin by training BERT models
with the same conﬁguration as BERTBASE (L =
12, H = 768, A = 12, 110M params).
Static vs. Dynamic Masking
As discussed in Section 2, BERT relies on randomly masking and predicting tokens. The original BERT implementation performed masking
once during data preprocessing, resulting in a single static mask. To avoid using the same mask for
each training instance in every epoch, training data
was duplicated 10 times so that each sequence is
masked in 10 different ways over the 40 epochs of
training. Thus, each training sequence was seen
with the same mask four times during training.
We compare this strategy with dynamic masking where we generate the masking pattern every
time we feed a sequence to the model. This becomes crucial when pretraining for more steps or
with larger datasets.
7Studying architectural changes, including larger architectures, is an important area for future work.
Our reimplementation:
Comparison between static and dynamic
masking for BERTBASE. We report F1 for SQuAD and
accuracy for MNLI-m and SST-2. Reported results are
medians over 5 random initializations (seeds). Reference results are from Yang et al. .
BERTBASE results from Devlin et al. to our
reimplementation with either static or dynamic
We ﬁnd that our reimplementation
with static masking performs similar to the
original BERT model, and dynamic masking is
comparable or slightly better than static masking.
Given these results and the additional efﬁciency
beneﬁts of dynamic masking, we use dynamic
masking in the remainder of the experiments.
Model Input Format and Next Sentence
Prediction
In the original BERT pretraining procedure, the
model observes two concatenated document segments, which are either sampled contiguously
from the same document (with p = 0.5) or from
distinct documents. In addition to the masked language modeling objective, the model is trained to
predict whether the observed document segments
come from the same or distinct documents via an
auxiliary Next Sentence Prediction (NSP) loss.
The NSP loss was hypothesized to be an important factor in training the original BERT model.
Devlin et al. observe that removing NSP
hurts performance, with signiﬁcant performance
degradation on QNLI, MNLI, and SQuAD 1.1.
However, some recent work has questioned the
necessity of the NSP loss .
To better understand this discrepancy, we compare several alternative training formats:
• SEGMENT-PAIR+NSP: This follows the original
input format used in BERT ,
with the NSP loss. Each input has a pair of segments, which can each contain multiple natural
sentences, but the total combined length must
be less than 512 tokens.
SQuAD 1.1/2.0
Our reimplementation (with NSP loss):
SEGMENT-PAIR
SENTENCE-PAIR
Our reimplementation (without NSP loss):
FULL-SENTENCES
DOC-SENTENCES
XLNetBASE (K = 7)
XLNetBASE (K = 6)
Table 2: Development set results for base models pretrained over BOOKCORPUS and WIKIPEDIA. All models are
trained for 1M steps with a batch size of 256 sequences. We report F1 for SQuAD and accuracy for MNLI-m,
SST-2 and RACE. Reported results are medians over ﬁve random initializations (seeds). Results for BERTBASE and
XLNetBASE are from Yang et al. .
• SENTENCE-PAIR+NSP: Each input contains a
pair of natural sentences, either sampled from
a contiguous portion of one document or from
separate documents. Since these inputs are signiﬁcantly shorter than 512 tokens, we increase
the batch size so that the total number of tokens
remains similar to SEGMENT-PAIR+NSP. We retain the NSP loss.
• FULL-SENTENCES: Each input is packed with
full sentences sampled contiguously from one
or more documents, such that the total length is
at most 512 tokens. Inputs may cross document
boundaries. When we reach the end of one document, we begin sampling sentences from the
next document and add an extra separator token
between documents. We remove the NSP loss.
• DOC-SENTENCES: Inputs are constructed similarly to FULL-SENTENCES, except that they
may not cross document boundaries.
sampled near the end of a document may be
shorter than 512 tokens, so we dynamically increase the batch size in these cases to achieve
a similar number of total tokens as FULL-
SENTENCES. We remove the NSP loss.
Table 2 shows results for the four different settings.
We ﬁrst compare the original
SEGMENT-PAIR input format from Devlin et al.
 to the SENTENCE-PAIR format; both formats retain the NSP loss, but the latter uses single sentences.
We ﬁnd that using individual
sentences hurts performance on downstream
tasks, which we hypothesize is because the model
is not able to learn long-range dependencies.
We next compare training without the NSP
loss and training with blocks of text from a single document (DOC-SENTENCES).
We ﬁnd that
this setting outperforms the originally published
BERTBASE results and that removing the NSP loss
matches or slightly improves downstream task
performance, in contrast to Devlin et al. .
It is possible that the original BERT implementation may only have removed the loss term while
still retaining the SEGMENT-PAIR input format.
Finally we ﬁnd that restricting sequences to
come from a single document (DOC-SENTENCES)
performs slightly better than packing sequences
from multiple documents (FULL-SENTENCES).
However, because the DOC-SENTENCES format
results in variable batch sizes, we use FULL-
SENTENCES in the remainder of our experiments
for easier comparison with related work.
Training with large batches
Past work in Neural Machine Translation has
shown that training with very large mini-batches
can both improve optimization speed and end-task
performance when the learning rate is increased
appropriately . Recent work has
shown that BERT is also amenable to large batch
training .
Devlin et al.
originally
BERTBASE for 1M steps with a batch size of
256 sequences.
This is equivalent in computational cost, via gradient accumulation, to training
for 125K steps with a batch size of 2K sequences,
or for 31K steps with a batch size of 8K.
In Table 3 we compare perplexity and endbsz
Table 3: Perplexity on held-out training data (ppl) and
development set accuracy for base models trained over
BOOKCORPUS and WIKIPEDIA with varying batch
sizes (bsz). We tune the learning rate (lr) for each setting. Models make the same number of passes over the
data (epochs) and have the same computational cost.
task performance of BERTBASE as we increase the
batch size, controlling for the number of passes
through the training data. We observe that training with large batches improves perplexity for the
masked language modeling objective, as well as
end-task accuracy. Large batches are also easier to
parallelize via distributed data parallel training,8
and in later experiments we train with batches of
8K sequences.
Notably You et al. train BERT with even
larger batche sizes, up to 32K sequences. We leave
further exploration of the limits of large batch
training to future work.
Text Encoding
Byte-Pair Encoding (BPE) 
is a hybrid between character- and word-level representations that allows handling the large vocabularies common in natural language corpora. Instead of full words, BPE relies on subwords units,
which are extracted by performing statistical analysis of the training corpus.
BPE vocabulary sizes typically range from
10K-100K subword units. However, unicode characters can account for a sizeable portion of this
vocabulary when modeling large and diverse corpora, such as the ones considered in this work.
Radford et al. introduce a clever implementation of BPE that uses bytes instead of unicode characters as the base subword units. Using
bytes makes it possible to learn a subword vocabulary of a modest size (50K units) that can still encode any input text without introducing any “unknown” tokens.
8Large batch training can improve training efﬁciency even
without large scale parallel hardware through gradient accumulation, whereby gradients from multiple mini-batches
are accumulated locally before each optimization step. This
functionality is supported natively in FAIRSEQ uses a character-level
BPE vocabulary of size 30K, which is learned
after preprocessing the input with heuristic tokenization rules. Following Radford et al. ,
we instead consider training BERT with a larger
byte-level BPE vocabulary containing 50K subword units, without any additional preprocessing
or tokenization of the input. This adds approximately 15M and 20M additional parameters for
BERTBASE and BERTLARGE, respectively.
Early experiments revealed only slight differences between these encodings,
Radford et al. 
BPE achieving
worse end-task performance on some tasks. Nevertheless, we believe the advantages of a universal encoding scheme outweighs the minor degredation in performance and use this encoding in
the remainder of our experiments.
A more detailed comparison of these encodings is left to future work.
In the previous section we propose modiﬁcations
to the BERT pretraining procedure that improve
end-task performance.
We now aggregate these
improvements and evaluate their combined impact.
We call this conﬁguration RoBERTa for
Robustly optimized BERT approach.
Speciﬁcally, RoBERTa is trained with dynamic masking (Section 4.1), FULL-SENTENCES without NSP
loss (Section 4.2), large mini-batches (Section 4.3)
and a larger byte-level BPE (Section 4.4).
Additionally, we investigate two other important factors that have been under-emphasized in
previous work: (1) the data used for pretraining,
and (2) the number of training passes through the
data. For example, the recently proposed XLNet
architecture is pretrained using nearly 10 times more data than the original
BERT . It is also trained with
a batch size eight times larger for half as many optimization steps, thus seeing four times as many
sequences in pretraining compared to BERT.
To help disentangle the importance of these factors from other modeling choices (e.g., the pretraining objective), we begin by training RoBERTa
following the BERTLARGE architecture (L = 24,
H = 1024, A = 16, 355M parameters).
pretrain for 100K steps over a comparable BOOK-
CORPUS plus WIKIPEDIA dataset as was used in
(v1.1/2.0)
with BOOKS + WIKI
+ additional data (§3.2)
+ pretrain longer
+ pretrain even longer
with BOOKS + WIKI
XLNetLARGE
with BOOKS + WIKI
+ additional data
Table 4: Development set results for RoBERTa as we pretrain over more data (16GB →160GB of text) and pretrain
for longer (100K →300K →500K steps). Each row accumulates improvements from the rows above. RoBERTa
matches the architecture and training objective of BERTLARGE. Results for BERTLARGE and XLNetLARGE are from
Devlin et al. and Yang et al. , respectively. Complete results on all GLUE tasks can be found in the
Devlin et al. . We pretrain our model using
1024 V100 GPUs for approximately one day.
We present our results in Table 4. When
controlling for training data, we observe that
RoBERTa provides a large improvement over the
originally reported BERTLARGE results, reafﬁrming
the importance of the design choices we explored
in Section 4.
Next, we combine this data with the three additional datasets described in Section 3.2.
train RoBERTa over the combined data with the
same number of training steps as before (100K).
In total, we pretrain over 160GB of text. We observe further improvements in performance across
all downstream tasks, validating the importance of
data size and diversity in pretraining.9
Finally, we pretrain RoBERTa for signiﬁcantly
longer, increasing the number of pretraining steps
from 100K to 300K, and then further to 500K. We
again observe signiﬁcant gains in downstream task
performance, and the 300K and 500K step models outperform XLNetLARGE across most tasks. We
note that even our longest-trained model does not
appear to overﬁt our data and would likely beneﬁt
from additional training.
In the rest of the paper, we evaluate our best
RoBERTa model on the three different benchmarks: GLUE, SQuaD and RACE. Speciﬁcally
9Our experiments conﬂate increases in data size and diversity. We leave a more careful analysis of these two dimensions to future work.
we consider RoBERTa trained for 500K steps over
all ﬁve of the datasets introduced in Section 3.2.
GLUE Results
For GLUE we consider two ﬁnetuning settings.
In the ﬁrst setting (single-task, dev) we ﬁnetune
RoBERTa separately for each of the GLUE tasks,
using only the training data for the corresponding task. We consider a limited hyperparameter
sweep for each task, with batch sizes ∈{16, 32}
and learning rates ∈{1e−5, 2e−5, 3e−5}, with a
linear warmup for the ﬁrst 6% of steps followed by
a linear decay to 0. We ﬁnetune for 10 epochs and
perform early stopping based on each task’s evaluation metric on the dev set. The rest of the hyperparameters remain the same as during pretraining.
In this setting, we report the median development
set results for each task over ﬁve random initializations, without model ensembling.
In the second setting (ensembles, test), we compare RoBERTa to other approaches on the test set
via the GLUE leaderboard. While many submissions to the GLUE leaderboard depend on multitask ﬁnetuning, our submission depends only on
single-task ﬁnetuning. For RTE, STS and MRPC
we found it helpful to ﬁnetune starting from the
MNLI single-task model, rather than the baseline
pretrained RoBERTa. We explore a slightly wider
hyperparameter space, described in the Appendix,
and ensemble between 5 and 7 models per task.
Single-task single models on dev
XLNetLARGE
Ensembles on test 
Table 5: Results on GLUE. All results are based on a 24-layer architecture. BERTLARGE and XLNetLARGE results
are from Devlin et al. and Yang et al. , respectively. RoBERTa results on the development set are a
median over ﬁve runs. RoBERTa results on the test set are ensembles of single-task models. For RTE, STS and
MRPC we ﬁnetune starting from the MNLI model instead of the baseline pretrained model. Averages are obtained
from the GLUE leaderboard.
Task-speciﬁc modiﬁcations
Two of the GLUE
tasks require task-speciﬁc ﬁnetuning approaches
to achieve competitive leaderboard results.
Recent submissions on the GLUE
leaderboard adopt a pairwise ranking formulation
for the QNLI task, in which candidate answers
are mined from the training set and compared to
one another, and a single (question, candidate)
pair is classiﬁed as positive . This formulation signiﬁcantly
simpliﬁes the task, but is not directly comparable
to BERT . Following recent
work, we adopt the ranking approach for our test
submission, but for direct comparison with BERT
we report development set results based on a pure
classiﬁcation approach.
We found the provided NLI-format
data to be challenging to work with.
we use the reformatted WNLI data from Super-
GLUE , which indicates the
span of the query pronoun and referent. We ﬁnetune RoBERTa using the margin ranking loss from
Kocijan et al. . For a given input sentence,
we use spaCy to
extract additional candidate noun phrases from the
sentence and ﬁnetune our model so that it assigns
higher scores to positive referent phrases than for
any of the generated negative candidate phrases.
One unfortunate consequence of this formulation
is that we can only make use of the positive training examples, which excludes over half of the provided training examples.10
10While we only use the provided WNLI training data, our
We present our results in Table 5. In the
ﬁrst setting (single-task, dev), RoBERTa achieves
state-of-the-art results on all 9 of the GLUE
task development sets. Crucially, RoBERTa uses
the same masked language modeling pretraining objective and architecture as BERTLARGE, yet
consistently outperforms both BERTLARGE and
XLNetLARGE. This raises questions about the relative importance of model architecture and pretraining objective, compared to more mundane details like dataset size and training time that we explore in this work.
In the second setting (ensembles, test), we
submit RoBERTa to the GLUE leaderboard and
achieve state-of-the-art results on 4 out of 9 tasks
and the highest average score to date. This is especially exciting because RoBERTa does not depend
on multi-task ﬁnetuning, unlike most of the other
top submissions. We expect future work may further improve these results by incorporating more
sophisticated multi-task ﬁnetuning procedures.
SQuAD Results
We adopt a much simpler approach for SQuAD
compared to past work.
In particular, while
 augment their training data
with additional QA datasets, we only ﬁnetune
RoBERTa using the provided SQuAD training
data. Yang et al. also employed a custom
layer-wise learning rate schedule to ﬁnetune
results could potentially be improved by augmenting this with
additional pronoun disambiguation datasets.
Single models on dev, w/o data augmentation
XLNetLARGE
Single models on test 
XLNetLARGE
XLNet + SG-Net Veriﬁer
Table 6: Results on SQuAD. † indicates results that depend on additional external training data. RoBERTa
uses only the provided SQuAD data in both dev and
test settings. BERTLARGE and XLNetLARGE results are
from Devlin et al. and Yang et al. , respectively.
XLNet, while we use the same learning rate for
all layers.
For SQuAD v1.1 we follow the same ﬁnetuning procedure as Devlin et al. . For SQuAD
v2.0, we additionally classify whether a given
question is answerable; we train this classiﬁer
jointly with the span predictor by summing the
classiﬁcation and span loss terms.
We present our results in Table 6. On
the SQuAD v1.1 development set, RoBERTa
matches the state-of-the-art set by XLNet. On the
SQuAD v2.0 development set, RoBERTa sets a
new state-of-the-art, improving over XLNet by 0.4
points (EM) and 0.6 points (F1).
We also submit RoBERTa to the public SQuAD
2.0 leaderboard and evaluate its performance relative to other systems. Most of the top systems
build upon either BERT or
XLNet , both of which rely on
additional external training data. In contrast, our
submission does not use any additional data.
Our single RoBERTa model outperforms all but
one of the single model submissions, and is the
top scoring system among those that do not rely
on data augmentation.
RACE Results
In RACE, systems are provided with a passage of
text, an associated question, and four candidate answers. Systems are required to classify which of
the four candidate answers is correct.
We modify RoBERTa for this task by concate-
Single models on test 
XLNetLARGE
Table 7: Results on the RACE test set. BERTLARGE and
XLNetLARGE results are from Yang et al. .
nating each candidate answer with the corresponding question and passage. We then encode each of
these four sequences and pass the resulting [CLS]
representations through a fully-connected layer,
which is used to predict the correct answer. We
truncate question-answer pairs that are longer than
128 tokens and, if needed, the passage so that the
total length is at most 512 tokens.
Results on the RACE test sets are presented in
Table 7. RoBERTa achieves state-of-the-art results
on both middle-school and high-school settings.
Related Work
Pretraining
objectives,
 , and
masked language modeling , and multiple variants
of autoregressive pretraining .
Performance is also typically improved by training
 . Our goal was to replicate,
simplify, and better tune the training of BERT,
as a reference point for better understanding the
relative performance of all of these methods.
Conclusion
We carefully evaluate a number of design decisions when pretraining BERT models.
ﬁnd that performance can be substantially improved by training the model longer, with bigger
batches over more data; removing the next sentence prediction objective; training on longer sequences; and dynamically changing the masking
pattern applied to the training data. Our improved
pretraining procedure, which we call RoBERTa,
achieves state-of-the-art results on GLUE, RACE
and SQuAD, without multi-task ﬁnetuning for
GLUE or additional data for SQuAD. These results illustrate the importance of these previously overlooked design decisions and suggest
that BERT’s pretraining objective remains competitive with recently proposed alternatives.
additionally
pretraining