Learning Multi-Domain Convolutional Neural Networks for Visual Tracking
Hyeonseob Nam
Bohyung Han
Dept. of Computer Science and Engineering, POSTECH, Korea
{namhs09, bhhan}@postech.ac.kr
We propose a novel visual tracking algorithm based on
the representations from a discriminatively trained Convolutional Neural Network (CNN). Our algorithm pretrains
a CNN using a large set of videos with tracking groundtruths to obtain a generic target representation. Our network is composed of shared layers and multiple branches
of domain-speciﬁc layers, where domains correspond to individual training sequences and each branch is responsible
for binary classiﬁcation to identify the target in each domain. We train the network with respect to each domain
iteratively to obtain generic target representations in the
shared layers. When tracking a target in a new sequence,
we construct a new network by combining the shared layers in the pretrained CNN with a new binary classiﬁcation
layer, which is updated online. Online tracking is performed
by evaluating the candidate windows randomly sampled
around the previous target state. The proposed algorithm
illustrates outstanding performance compared with stateof-the-art methods in existing tracking benchmarks.
1. Introduction
Convolutional Neural Networks (CNNs) have recently
been applied to various computer vision tasks such as image classiﬁcation , semantic segmentation ,
object detection , and many others . Such great
success of CNNs is mostly attributed to their outstanding
performance in representing visual data. Visual tracking,
however, has been less affected by these popular trends
since it is difﬁcult to collect a large amount of training data
for video processing applications and training algorithms
specialized for visual tracking are not available yet, while
the approaches based on low-level handcraft features still
work well in practice . Several recent tracking algorithms have addressed the data deﬁciency
issue by transferring pretrained CNNs on a large-scale classiﬁcation dataset such as ImageNet . Although these
methods may be sufﬁcient to obtain generic feature representations, its effectiveness in terms of tracking is limited
due to the fundamental inconsistency between classiﬁcation
and tracking problems, i.e., predicting object class labels
versus locating targets of arbitrary classes.
To fully exploit the representation power of CNNs in visual tracking, it is desirable to train them on large-scale data
specialized for visual tracking, which cover a wide range
of variations in the combination of target and background.
However, it is truly challenging to learn a uniﬁed representation based on the video sequences that have completely
different characteristics. Note that individual sequences involve different types of targets whose class labels, moving
patterns, and appearances are different, and tracking algorithms suffer from sequence-speciﬁc challenges including
occlusion, deformation, lighting condition change, motion
blur, etc. Training CNNs is even more difﬁcult since the
same kind of objects can be considered as a target in a sequence and as a background object in another. Due to such
variations and inconsistencies across sequences, we believe
that the ordinary learning methods based on the standard
classiﬁcation task are not appropriate, and another approach
to capture sequence-independent information should be incorporated for better representations for tracking.
Motivated by this fact, we propose a novel CNN architecture, referred to as Multi-Domain Network (MDNet), to
learn the shared representation of targets from multiple annotated video sequences for tracking, where each video is
regarded as a separate domain. The proposed network has
separate branches of domain-speciﬁc layers for binary classiﬁcation at the end of the network, and shares the common
information captured from all sequences in the preceding
layers for generic representation learning. Each domain in
MDNet is trained separately and iteratively while the shared
layers are updated in every iteration. By employing this
strategy, we separate domain-independent information from
domain-speciﬁc one and learn generic feature representations for visual tracking. Another interesting aspect of our
architecture is that we design the CNN with a small number
of layers compared to the networks for classiﬁcation tasks
such as AlexNet and VGG nets .
We also propose an effective online tracking framework
based on the representations learned by MDNet. When a
 
test sequence is given, all the existing branches of binary
classiﬁcation layers, which were used in the training phase,
are removed and a new single branch is constructed to compute the target scores in the test sequence. The new classiﬁcation layer and the fully connected layers within the shared
layers are then ﬁne-tuned online during tracking to adapt to
the new domain. The online update is conducted to model
long-term and short-term appearance variations of a target
for robustness and adaptiveness, respectively, and an effective and efﬁcient hard negative mining technique is incorporated in the learning procedure.
Our algorithm consists of multi-domain representation
learning and online visual tracking. The main contributions
of our work are summarized below:
• We propose a multi-domain learning framework based
on CNNs, which separates domain-independent information from domain-speciﬁc one, to capture shared
representations effectively.
• Our framework is successfully applied to visual tracking, where the CNN pretrained by multi-domain learning is updated online in the context of a new sequence
to learn domain-speciﬁc information adaptively.
• Our extensive experiment demonstrates the outstanding performance of our tracking algorithm compared to the state-of-the-art techniques in two public benchmarks: Object Tracking Benchmark and
VOT2014 .
The rest of the paper is organized as follows. We ﬁrst
review related work in Section 2, and discuss our multidomain learning approach for visual tracking in Section 3.
Section 4 describes our online learning and tracking algorithm, and Section 5 demonstrates the experimental results
in two tracking benchmark datasets.
2. Related Work
2.1. Visual Tracking Algorithms
Visual tracking is one of the fundamental problems in
computer vision and has been actively studied for decades.
Most tracking algorithms fall into either generative or discriminative approaches. Generative methods describe the
target appearances using generative models and search for
the target regions that ﬁt the models best. Various generative target appearance modeling algorithms have been proposed including sparse representation , density estimation , and incremental subspace learning .
In contrast, discriminate methods aim to build a model that
distinguishes the target object from the background. These
tracking algorithms typically learn classiﬁers based on multiple instance learning , P-N learning , online boosting , structured output SVMs , etc.
In recent years, correlation ﬁlters have gained attention
in the area of visual tracking due to their computational ef-
ﬁciency and competitive performance . Bolme
et al. proposed a fast correlation tracker with a minimum
output sum of squared error (MOSSE) ﬁlter, which runs in
hundreds of frames per second. Henriques et al. formulated kernelized correlation ﬁlters (KCF) using circulant
matrices, and efﬁciently incorporated multi-channel features in a Fourier domain. Several variations of KCF tracker
have been subsequently investigated to improve tracking
performance. For example, DSST learns separate ﬁlters for translation and scaling, and MUSTer employs
short-term and long-term memory stores inspired by a psychological memory model. Although these approaches are
satisfactory in constrained environments, they have an inherent limitation that they resort to low-level hand-crafted
features, which are vulnerable in dynamic situations including illumination changes, occlusion, deformations, etc.
2.2. Convolutional Neural Networks
CNNs have demonstrated their outstanding representation power in a wide range of computer vision applications . Krizhevsky et al. 
brought signiﬁcant performance improvement in image
classiﬁcation by training a deep CNN with a large-scale
dataset and an efﬁcient GPU implementation. R-CNN 
applies a CNN to an object detection task, where the training data are scarce, by pretraining on a large auxiliary
dataset and ﬁne-tuning on the target dataset.
Despite such huge success of CNNs, only a limited number of tracking algorithms using the representations from
CNNs have been proposed so far . An early
tracking algorithm based on a CNN can handle only predeﬁned target object classes, e.g., human, since the CNN
is trained ofﬂine before tracking and ﬁxed afterwards .
Although proposes an online learning method based
on a pool of CNNs, it suffers from lack of training data
to train deep networks and its accuracy is not particularly
good compared to the methods based on hand-craft features.
A few recent approaches transfer CNNs pretrained
on a large-scale dataset constructed for image classiﬁcation,
but the representation may not be very effective due to the
fundamental difference between classiﬁcation and tracking
tasks. Contrary to the existing approaches, our algorithm
takes advantage of large-scale visual tracking data for pretraining a CNN and obtain effective representations.
2.3. Multi-Domain Learning
Our approach to pretrain deep CNNs belongs to multidomain learning, which refers to a learning method in which
the training data are originated from multiple domains and
the domain information is incorporated in learning procedure. Multi-domain learning is popular in natural lan-
Figure 1: The architecture of our Multi-Domain Network, which consists of shared layers and K branches of domain-speciﬁc
layers. Yellow and blue bounding boxes denote the positive and negative samples in each domain, respectively.
guage processing (e.g., sentiment classiﬁcation with multiple products and spam ﬁltering with multiple users), and
various approaches have been proposed . In computer vision community, multi-domain learning is discussed
in only a few domain adaptation approaches. For example, Duan et al. introduced a domain-weighted combination of SVMs for video concept detection, and Hoffman
et al. presented a mixture-transform model for object
classiﬁcation.
3. Multi-Domain Network (MDNet)
This section describes our CNN architecture and multidomain learning approach to obtain domain-independent
representations for visual tracking.
3.1. Network Architecture
The architecture of our network is illustrated in Figure 1.
It receives a 107×107 RGB input1, and has ﬁve hidden layers including three convolutional layers (conv1-3) and two
fully connected layers (fc4-5). Additionally, the network
has K branches for the last fully connected layers (fc61fc6K) corresponding to K domains, in other words, training
sequences. The convolutional layers are identical to the corresponding parts of VGG-M network except that the feature map sizes are adjusted by our input size. The next two
fully connected layers have 512 output units and are combined with ReLUs and dropouts. Each of the K branches
contains a binary classiﬁcation layer with softmax crossentropy loss, which is responsible for distinguishing target
and background in each domain. Note that we refer to fc61fc6K as domain-speciﬁc layers and all the preceding layers
as shared layers.
Our network architecture is substantially smaller than the
ones commonly used in typical recognition tasks such as
AlexNet and VGG-Nets . We believe that such
1This input size is designed to obtain 3×3 feature maps in conv3:
107 = 75 (receptive ﬁeld) + 2 × 16 (stride).
a simple architecture is more appropriate for visual tracking
due to the following reasons. First, visual tracking aims to
distinguish only two classes, target and background, which
requires much less model complexity than general visual
recognition problems such as ImageNet classiﬁcation with
1000 classes. Second, a deep CNN is less effective for precise target localization since the spatial information tends
to be diluted as a network goes deeper . Third, since
targets in visual tracking are typically small, it is desirable
to make input size small, which reduces the depth of the
network naturally. Finally, a smaller network is obviously
more efﬁcient in visual tracking problem, where training
and testing are performed online. When we tested larger
networks, the algorithm is less accurate and becomes slower
signiﬁcantly.
3.2. Learning Algorithm
The goal of our learning algorithm is to train a multidomain CNN disambiguating target and background in an
arbitrary domain, which is not straightforward since the
training data from different domains have different notions
of target and background. However, there still exist some
common properties that are desirable for target representations in all domains, such as robustness to illumination
changes, motion blur, scale variations, etc. To extract useful features satisfying these common properties, we separate domain-independent information from domain-speciﬁc
one by incorporating a multi-domain learning framework.
Our CNN is trained by the Stochastic Gradient Descent
(SGD) method, where each domain is handled exclusively
in each iteration. In the kth iteration, the network is updated
based on a minibatch that consists of the training samples
from the (k mod K)th sequence, where only a single branch
fc6(k mod K) is enabled. It is repeated until the network is
converged or the predeﬁned number of iterations is reached.
Through this learning procedure, domain-independent information is modeled in the shared layers from which useful
generic feature representations are obtained.
(a) 1st minibatch
(b) 5th minibatch
(c) 30th minibatch
Figure 2: Identiﬁed training examples through our hard negative mining in Bolt2 (top) and Doll (bottom) sequences.
Red and blue bounding boxes denote positive and negative
samples in each minibatch, respectively. The negative samples becomes hard to classify as training proceeds.
4. Online Tracking using MDNet
Once we complete the multi-domain learning described
in Section 3.2, the multiple branches of domain-speciﬁc layers (fc61-fc6K) are replaced with a single branch (fc6) for
a new test sequence. Then we ﬁne-tune the new domainspeciﬁc layer and the fully connected layers in the shared
network online at the same time. The detailed tracking procedure is discussed in this section.
4.1. Tracking Control and Network Update
We consider two complementary aspects in visual tracking, robustness and adaptiveness, by long-term and shortterm updates. Long-term updates are performed in regular
intervals using the positive samples collected for a long period of time while short-term updates are conducted whenever potential tracking failures are detected—when the estimated target is classiﬁed as background—using the positive
samples in a short-term period. In both cases we use the
negative samples observed in the short-term since old negative examples are often redundant or irrelevant to the current frame. Note that we maintain a single network during
tracking, where these two kinds of updates are performed
depending on how fast the target appearance changes.
To estimate the target state in each frame, N target candidates x1, . . . , xN sampled around the previous target state
are evaluated using the network, and we obtain their positive scores f +(xi) and negative scores f −(xi) from the
network. The optimal target state x∗is given by ﬁnding the
example with the maximum positive score as
x∗= argmax
xi f +(xi).
4.2. Hard Minibatch Mining
The majority of negative examples are typically trivial or
redundant in tracking-by-detection approaches, while only
a few distracting negative samples are effective in training
a classiﬁer. Hence, the ordinary SGD method, where the
training samples evenly contribute to learning, easily suffers
from a drift problem since the distractors are considered insufﬁciently. A popular solution in object detection for this
issue is hard negative mining , where training and testing procedures are alternated to identify the hard negative
examples, typically false positives, and we adopt this idea
for our online learning procedure.
We integrate hard negative mining step into minibatch
selection.
In each iteration of our learning procedure, a
minibatch consists of M + positives and M −
h hard negatives. The hard negative examples are identiﬁed by testing M −(≫M −
h ) negative samples and selecting the ones
with top M −
h positive scores. As the learning proceeds and
the network becomes more discriminative, the classiﬁcation
in a minibatch becomes more challenging as illustrated in
Figure 2. This approach examines a predeﬁned number of
samples and identiﬁes critical negative examples effectively
without explicitly running a detector to extract false positives as in the standard hard negative mining techniques.
4.3. Bounding Box Regression
Due to the high-level abstraction of CNN-based features
and our data augmentation strategy which samples multiple positive examples around the target (which will be described in more detail in the next subsection), our network
sometimes fails to ﬁnd tight bounding boxes enclosing the
target. We apply the bounding box regression technique,
which is popular in object detection , to improve
target localization accuracy. Given the ﬁrst frame of a test
sequence, we train a simple linear regression model to predict the precise target location using conv3 features of the
samples near the target location. In the subsequent frames,
we adjust the target locations estimated from Eq. (1) using
the regression model if the estimated targets are reliable (i.e.
f +(x∗) > 0.5). The bounding box regressor is trained only
in the ﬁrst frame since it is time consuming for online update and incremental learning of the regression model may
not be very helpful considering its risk. Refer to for
details as we use the same formulation and parameters.
4.4. Implementation Details
The overall procedure of our tracking algorithm is presented in Algorithm 1. The ﬁlter weights in the jth layer
of CNN are denoted by wj, where w1:5 are pretrained by
mutli-domain learning and w6 is initialized randomly for a
new sequence. Only the weights in the fully connected layers w4:6 are updated online whereas the ones in the convolutional layers w1:3 are ﬁxed throughout tracking; this strategy is beneﬁcial to not only computational efﬁciency but
also avoiding overﬁtting by preserving domain-independent
information. Ts and Tl are frame index sets in short-term
(τs = 20) and long-term (τl = 100) periods, respectively.
Algorithm 1 Online tracking algorithm
Input : Pretrained CNN ﬁlters {w1, . . . , w5}
Initial target state x1
Output: Estimated target states x∗
1: Randomly initialize the last layer w6.
2: Train a bounding box regression model.
3: Draw positive samples S+
1 and negative samples S−
4: Update {w4, w5, w6} using S+
5: Ts ←{1} and Tl ←{1}.
Draw target candidate samples xi
Find the optimal target state x∗
t by Eq. (1).
t ) > 0.5 then
Draw training samples S+
Ts ←Ts ∪{t}, Tl ←Tl ∪{t}.
if |Ts| > τs then Ts ←Ts \ {minv∈Ts v}.
if |Tl| > τl then Tl ←Tl \ {minv∈Tl v}.
t using bounding box regression.
t ) < 0.5 then
Update {w4, w5, w6} using S+
v∈Ts and S−
else if t mod 10 = 0 then
Update {w4, w5, w6} using S+
v∈Tl and S−
19: until end of sequence
The further implementation details are described below.
Target candidate generation
To generate target candidates in each frame, we draw N(= 256) samples in translation and scale dimension, xi
t), i = 1, . . . , N,
from a Gaussian distribution whose mean is the previous target state x∗
t−1 and covariance is a diagonal matrix
diag(0.09r2, 0.09r2, 0.25), where r is the mean of the width
and height of the target in the previous frame. The scale of
each candidate bounding box is computed by multiplying
1.05si to the initial target scale.
Training data
For ofﬂine multi-domain learning, we collect 50 positive and 200 negative samples from every frame,
where positive and negative examples have ≥0.7 and ≤0.5
IoU overlap ratios with ground-truth bounding boxes, respectively. Similarly, for online learning, we collect S+
50) positive and S−
t (= 200) negative samples with ≥0.7
and ≤0.3 IoU overlap ratios with the estimated target
bounding boxes, respectively, except that S+
= 5000. For bounding-box regression, we use 1000
training examples with the same parameters as .
Network learning
For multi-domain learning with K
training sequences, we train the network for 100K iterations with learning rates 0.0001 for convolutional layers2
and 0.001 for fully connected layers. At the initial frame
2The convolutional layers are initialized by VGG-M network, which is
pretrained on ImageNet.
Location error threshold
Precision plots of OPE
MDNet [0.948]
MUSTer [0.865]
CNN−SVM [0.852]
MEEM [0.840]
TGPR [0.766]
DSST [0.747]
KCF [0.740]
Struck [0.656]
SCM [0.649]
Overlap threshold
Success rate
Success plots of OPE
MDNet [0.708]
MUSTer [0.641]
CNN−SVM [0.597]
MEEM [0.572]
DSST [0.557]
TGPR [0.529]
KCF [0.514]
SCM [0.499]
Struck [0.474]
(a) OTB50 result
Location error threshold
Precision plots of OPE
MDNet [0.909]
CNN−SVM [0.814]
MEEM [0.786]
MUSTer [0.774]
KCF [0.695]
DSST [0.689]
TGPR [0.688]
Struck [0.636]
SCM [0.557]
Overlap threshold
Success rate
Success plots of OPE
MDNet [0.678]
MUSTer [0.575]
CNN−SVM [0.555]
MEEM [0.533]
DSST [0.517]
KCF [0.477]
TGPR [0.477]
Struck [0.458]
SCM [0.429]
(b) OTB100 result
Figure 3: Precision and success plots on OTB50 and
OTB100 . The numbers in the legend indicate the representative precisions at 20 pixels for precision plots, and
the area-under-curve scores for success plots.
of a test sequence, we train the fully connected layers for
30 iterations with learning rate 0.0001 for fc4-5 and 0.001
for fc6. For online update, we train the fully connected layers for 10 iterations with the learning rate three times larger
than that in the initial frame for fast adaptation. The momentum and weight decay are always set to 0.9 and 0.0005,
respectively. Each mini-batch consists of M +(= 32) positives and M −
h (= 96) hard negatives selected out of M −(=
1024) negative examples.
5. Experiment
We evaluated the proposed tracking algorithm on two
public benchmark datasets, Object Tracking Benchmark
(OTB) and VOT2014 , and compared its performance with state-of-the-art trackers. Our algorithm is implemented in MATLAB using MatConvNet toolbox ,
and runs at around 1 fps with eight cores of 2.20GHz Intel
Xeon E5-2660 and an NVIDIA Tesla K20m GPU.
5.1. Evaluation on OTB
OTB is a popular tracking benchmark that contains
100 fully annotated videos with substantial variations. The
evaluation is based on two metrics: center location error and
bounding box overlap ratio. The one-pass evaluation (OPE)
is employed to compare our algorithm with the six state-ofthe-art trackers including MUSTer , CNN-SVM ,
MEEM , TGPR , DSST and KCF , as well
as the top 2 trackers included in the benchmark—SCM 
Overlap threshold
Success rate
Success plots of OPE − fast motion (39)
MDNet [0.675]
CNN−SVM [0.546]
MEEM [0.538]
MUSTer [0.531]
TGPR [0.468]
Struck [0.461]
KCF [0.459]
DSST [0.458]
SCM [0.290]
Overlap threshold
Success rate
Success plots of OPE − background clutter (31)
MDNet [0.676]
MUSTer [0.581]
CNN−SVM [0.548]
MEEM [0.535]
DSST [0.524]
KCF [0.498]
SCM [0.456]
TGPR [0.449]
Struck [0.420]
Overlap threshold
Success rate
Success plots of OPE − illumination variation (38)
MDNet [0.689]
MUSTer [0.599]
DSST [0.565]
CNN−SVM [0.537]
MEEM [0.530]
KCF [0.479]
SCM [0.457]
TGPR [0.445]
Struck [0.422]
Overlap threshold
Success rate
Success plots of OPE − in−plane rotation (51)
MDNet [0.655]
MUSTer [0.551]
CNN−SVM [0.548]
MEEM [0.535]
DSST [0.509]
KCF [0.469]
TGPR [0.464]
Struck [0.449]
SCM [0.411]
Overlap threshold
Success rate
Success plots of OPE − low resolution (9)
MDNet [0.621]
MUSTer [0.454]
SCM [0.431]
CNN−SVM [0.403]
DSST [0.383]
TGPR [0.382]
Struck [0.371]
MEEM [0.364]
KCF [0.307]
Overlap threshold
Success rate
Success plots of OPE − occlusion (49)
MDNet [0.646]
MUSTer [0.552]
MEEM [0.516]
CNN−SVM [0.515]
DSST [0.462]
KCF [0.443]
TGPR [0.432]
SCM [0.406]
Struck [0.391]
Overlap threshold
Success rate
Success plots of OPE − out of view (14)
MDNet [0.627]
MEEM [0.494]
CNN−SVM [0.488]
MUSTer [0.462]
KCF [0.393]
DSST [0.385]
Struck [0.362]
TGPR [0.354]
SCM [0.320]
Overlap threshold
Success rate
Success plots of OPE − scale variation (64)
MDNet [0.658]
MUSTer [0.510]
CNN−SVM [0.490]
DSST [0.470]
MEEM [0.469]
SCM [0.408]
TGPR [0.405]
Struck [0.401]
KCF [0.394]
Figure 4: The success plots for eight challenge attributes: fast motion, background clutter, illumination variation, in-plain
rotation, low resolution, occlusion, out of view, and scale variation.
and Struck . Note that CNN-SVM is another tracking
algorithm based on the representations from CNN, which
provides a baseline for tracking algorithms that adopt deep
learning. In addition to the results on the entire 100 sequences in (OTB100), we also present the results on
its earlier version containing 50 sequences (OTB50).
For ofﬂine training of MDNet, we use 58 training sequences collected from VOT2013 , VOT2014 and
VOT2015 , excluding the videos included in OTB100.
Figure 3 illustrates the precision and success plots based
on center location error and bounding box overlap ratio, respectively. It clearly illustrates that our algorithm, denoted
by MDNet, outperforms the state-of-the-art trackers significantly in both measures. The exceptional scores at mild
thresholds means our tracker hardly misses targets while
the competitive scores at strict thresholds implies that our
algorithm also ﬁnds tight bounding boxes to targets. For
detailed performance analysis, we also report the results on
various challenge attributes in OTB100, such as occlusion,
rotation, motion blur, etc. Figure 4 demonstrates that our
tracker effectively handles all kinds of challenging situations that often require high-level semantic understanding.
In particular, our tracker successfully track targets in low
resolution while all the trackers based on low-level features
are not successful in the challenge.
To verify the contribution of each component in our algorithm, we implement and evaluate several variations of
our approach. The effectiveness of our multi-domain pretraining technique is tested by comparison with the singledomain learning method (SDNet), where the network is
trained with a single branch using the data from multiple sequences. We also investigate two additional versions of our
tracking algorithm—MDNet without bounding box regres-
Location error threshold
Precision plots of OPE
MDNet [0.909]
MDNet−BB [0.891]
SDNet [0.865]
MDNet−BB−HM [0.816]
Overlap threshold
Success rate
Success plots of OPE
MDNet [0.678]
MDNet−BB [0.650]
SDNet [0.645]
MDNet−BB−HM [0.602]
Figure 5: Precision and success plots on OTB100 for the
internal comparisons.
sion (MDNet–BB) and MDNet without bounding box regression and hard negative mining (MDNet–BB–HM). The
performances of all the variations are not as good as our full
algorithm (MDNet) and each component in our tracking algorithm is helpful to improve performance. The detailed
results are illustrated in Figure 5.
Figure 6 presents the superiority of our algorithm qualitatively compared to the state-of-the-art trackers. Figure 7
shows a few failure cases of our algorithm; slight target
appearance change causes a drift problem in Coupon sequence, and dramatic appearance change makes our tracker
miss the target completely in Jump sequence.
5.2. Evaluation on VOT2014 Dataset
For completeness, we also present the evaluation results
on VOT2014 dataset , which contains 25 sequences
with substantial variations. In the VOT challenge protocol, a tracker is re-initialized whenever tracking fails and
the evaluation module reports both accuracy and robustness, which correspond to the bounding box overlap ratio and the number of failures, respectively.
MDNet (Ours)
Figure 6: Qualitative results of the proposed method on some challenging sequences (Bolt2, ClifBar, Diving, Freeman4,
Human5, Ironman, Matrix and Skating2-1).
Robustness
(a) Baseline result
Robustness
(b) Region noise result
Table 1: The average scores and ranks of accuracy and robustness on the two experiments in VOT2014 . The ﬁrst and
second best scores are highlighted in red and blue colors, respectively.
Figure 7: Failure cases of our method (Coupon and Jump).
Green and red bounding boxes denote the ground-truths and
our tracking results, respectively.
Ranking plot for experiment baseline
Robustness rank
Accuracy rank
(a) Baseline
Ranking plot for experiment region_noise
Robustness rank
Accuracy rank
(b) Region noise
Figure 8: The robustness-accuracy ranking plots of tested
algorithms in VOT2014 dataset. The better trackers are located at the upper-right corner.
two types of experiment settings; trackers are initialized
with either ground-truth bounding boxes (baseline) or randomly perturbed ones (region noise). The VOT evaluation
also provides a ranking analysis based on both statistical
and practical signiﬁcance of the performance gap between
trackers. Please refer to for more details. We compare our algorithm with the top 5 trackers in VOT2014
challenge—DSST , SAMF , KCF , DGT and
PLT 14 —and additional two state-of-the-art trackers
MUSTer and MEEM . Our network is pretrained
using 89 sequences from OTB100, which do not include the
common sequences with the VOT2014 dataset.
As illustrated in Table 1 and Figure 8, MDNet is ranked
top overall—the ﬁrst place in accuracy and the ﬁrst or
second place in robustness; it demonstrates much better accuracy than all other methods, even with fewer reinitializations. Furthermore, MDNet works well with imprecise re-initializations as shown in the region noise experiment results, which implies that it can be effectively
combined with a re-detection module and achieve long-term
tracking. We also report the results with respect to several
visual attributes from the baseline experiment in Figure 9,
Ranking plot for label camera_motion
Robustness rank
Accuracy rank
Ranking plot for label illum_change
Robustness rank
Accuracy rank
Ranking plot for label motion_change
Robustness rank
Accuracy rank
Ranking plot for label occlusion
Robustness rank
Accuracy rank
Ranking plot for label size_change
Robustness rank
Accuracy rank
Ranking plot for label empty
Robustness rank
Accuracy rank
Figure 9: The robustness-accuracy ranking plots for ﬁve visual attributes: camera motion, illumination change, motion
change, occlusion and size change; and an empty attribute.
which shows that our tracker is stable in various challenging
situations.
6. Conclusion
We proposed a novel tracking algorithm based on a CNN
trained in a multi-domain learning framework, which is referred to as MDNet. Our tracking algorithm learns domainindependent representations from pretraining, and captures
domain-speciﬁc information through online learning during
tracking. The proposed network has a simple architecture
compared to the one designed for image classiﬁcation tasks.
The entire network is pretrained ofﬂine, and the fully connected layers including a single domain-speciﬁc layer are
ﬁne-tuned online. We achieved outstanding performance in
two large public tracking benchmarks, OTB and VOT2014,
compared to the state-of-the-art tracking algorithms.