Mach Learn 86:233–272
DOI 10.1007/s10994-011-5263-6
Learning by extrapolation from marginal
to full-multivariate probability distributions:
decreasingly naive Bayesian classiﬁcation
Geoffrey I. Webb · Janice R. Boughton · Fei Zheng ·
Kai Ming Ting · Houssam Salem
Received: 8 December 2009 / Accepted: 15 September 2011 / Published online: 13 October 2011
© The Author(s) 2011
Abstract Averaged n-Dependence Estimators (AnDE) is an approach to probabilistic classiﬁcation learning that learns by extrapolation from marginal to full-multivariate probability
distributions. It utilizes a single parameter that transforms the approach between a lowvariance high-bias learner (Naive Bayes) and a high-variance low-bias learner with Bayes
optimal asymptotic error. It extends the underlying strategy of Averaged One-Dependence
Estimators (AODE), which relaxes the Naive Bayes independence assumption while retaining many of Naive Bayes’ desirable computational and theoretical properties. AnDE
further relaxes the independence assumption by generalizing AODE to higher-levels of dependence. Extensive experimental evaluation shows that the bias-variance trade-off for Averaged 2-Dependence Estimators results in strong predictive accuracy over a wide range of
data sets. It has training time linear with respect to the number of examples, learns in a single
pass through the training data, supports incremental learning, handles directly missing values, and is robust in the face of noise. Beyond the practical utility of its lower-dimensional
variants, AnDE is of interest in that it demonstrates that it is possible to create low-bias
high-variance generative learners and suggests strategies for developing even more powerful classiﬁers.
Keywords Bayesian learning · Classiﬁcation learning · Probabilistic learning · Averaged
one-dependence estimators · Naive Bayes · Semi-naive Bayesian learning · Learning
without model selection · Ensemble learning · Feating
Editor: Peter Flach.
G.I. Webb () · J.R. Boughton · F. Zheng · K.M. Ting · H. Salem
Faculty of Information Technology, Monash University, Clayton, VIC, 3800, Australia
e-mail: 
J.R. Boughton
e-mail: 
e-mail: 
Mach Learn 86:233–272
1 Introduction
This paper presents a family of learning algorithms that utilize a predeﬁned function to
extrapolate from observed marginal distributions to the full multivariate distribution of interest. This stands in contrast to the majority of learning algorithms that instead seek to ﬁt
a model directly to the observed multivariate probability distribution. Whereas learning is
sometimes cast as a problem of searching through a space of hypotheses to ﬁnd one that best
ﬁts the training data , this new approach does not employ search and does
not perform model selection.
The members of this new family of algorithms have a unique combination of features
that is well suited to many applications. We discuss these features in more detail below.
Notable amongst them are training complexity linear with respect to the number of training
examples; single pass learning; direct capacity for incremental learning; and accuracy that
is competitive with the state-of-the-art. They are of further theoretical interest because they
demonstrate that it is possible to create low bias generative learners.
The family contains algorithms that range from low variance coupled with high bias
through to high variance coupled with low bias. Successive members of the family will be
best suited to differing quantities of data, starting with low variance for small data, with
successively lower bias but higher variance suiting ever increasing data quantities . The asymptotic error of the lowest bias variant is Bayes optimal.
One member of this family of algorithms, naive Bayes (NB), is already well known.
A second member, Averaged One-Dependence Estimators (AODE) , has
enjoyed considerable popularity since its introduction in 2005 . The work presented in this paper arises
from the realization that NB and AODE are but two instances of a family of algorithms,
which we call AnDE.
In Sect. 2 we explain the underlying learning strategy, and deﬁne the AnDE family of
algorithms. The AnDE family of algorithms build upon the method pioneered by AODE
 . In Sect. 3 we discuss how the AnDE algorithms relate to Feating , a generic approach to ensembling that also builds upon techniques pioneered
by AODE. In Sect. 4 we present an extensive evaluation of the AnDE family of algorithms,
comparing their performance to relevant Bayesian techniques, to Feating and to the state-ofthe-art Random Forests classiﬁer. Section 5 presents conclusions and directions for future
2 The AnDE family of algorithms
We wish to estimate from a training sample T of t classiﬁed objects the probability P(y |x)
that an example x = ⟨x1,...,xa⟩belongs to class y, where xi is the value of the ith attribute
and y ∈{c1,...,ck}. We use ¯v to denote the average number of values per attribute. These
and other elements of notation are listed in Table 1.
From the deﬁnition of conditional probability we have
P(y |x) = P(y,x)/P(x)
Mach Learn 86:233–272
Table 1 Notation
The unconditioned probability of event e
The conditional probability of event e given event w
An estimate of P(e)
A naive Bayes estimate of P(e)
An AODE estimate of P(e)
An AnDE estimate of P(e)
The number of attributes
The ith class
The number of classes
The number of training examples in T
The average number of values per attribute
A value from the set of all classes {c1,...,ck}
A training sample of t classiﬁed objects
x = ⟨x1,...,xa⟩
The value of the ith attribute of x = ⟨x1,...,xa⟩
x{i,j,...,q}
The subset of attributes values from x with the speciﬁed indices. For example,
x{2,3,5} = ⟨x2,x3,x5⟩
The set of all size-n subsets of {1,...,a}
A function that is 1 if T contains an object with the value xα, otherwise 0
As P(x) = k
i=1 P(ci,x), we can always estimate (1) from estimates of P(y,x) for each class
P(y,x)/P(x) = P(y,x)
In consequence, in the remainder of this paper we consider only the problem of estimating
P(y,x), thereby setting the work in a generative framework.
We deﬁne the dimensionality of a probability or probability estimate as the number of
attributes in the distribution to which the probability or estimate relates. Hence, the dimensionality of P(y,x) is a + 1.
If the training data do not include sufﬁcient examples of x to directly derive accurate
estimates of each P(ci,x), we must extrapolate these estimates from observations of lowerdimensional probabilities in the data. All other things being equal, an estimate of a lowerdimensional probability from a given ﬁnite training set will be more accurate than an estimate of a higher-dimensional probability, and estimates of higher-dimensional probabilities
will vary more from training sample to training sample. Hence, models derived from lowerdimensional probability estimates are likely to have lower variance than models derived
from higher-dimensional probability estimates. On the other hand, models derived from
higher-dimensional probabilities are likely to have lower bias, as less restrictive assumptions are made about the form of the probability distribution.
This is illustrated in Fig. 1, that shows a simple attribute-space with three ternary attributes and a binary class. To classify a new object with attribute-values Age = Old,
Pulse = Slow and Temperature = High, one wishes to infer the class distribution in the
cell highlighted in Fig. 1(a), which is a four-dimensional probability distribution. If there
are insufﬁcient examples to directly estimate that distribution, it might be extrapolated
Mach Learn 86:233–272
Fig. 1 Probabilities of varying dimensionality for an attribute-space with three ternary attributes and a binary
from any of a number of lower dimensional probability distributions. The prior class distribution P(y) is a one-dimensional probability distribution that can be estimated from the
entire attribute-space (Fig. 1(b)). The two-dimensional probabilities P(y ∧Age = Old),
P(y ∧Pulse = Slow), P(y ∧Temperature = High) can be estimated from the regions
depicted in Fig. 1(c–e). The regions associated with the three-dimensional probabili-
Mach Learn 86:233–272
ties P(y ∧Age = Old ∧Pulse = Slow), P(y ∧Age = Old ∧Temperature = High) and
P(y ∧Pulse = Slow ∧Temperature = High) are illustrated in Fig. 1(f–h).
From the deﬁnition of conditional probability we have
P(y,x) = P(y)P(x|y).
If the number of classes, k, is small, it should be possible to obtain a sufﬁciently accurate
estimate of P(y) from the sample frequencies. However, we still have the problem that x
may not occur sufﬁciently frequently in the training data and hence accurate estimates of
P(x|y) cannot be obtained directly from the sample frequencies.
The solution used by NB is to extrapolate to ˆP(x|y) from each two-dimensional probability estimate ˆP(xi |y) by assuming the attributes are independent given the class:
Hence we classify using
ˆPNB(y,x) = ˆP(y)
ˆP(xi |y).
With reference to Fig. 1, NB estimates the distribution in (a) by extrapolation from
the distributions in (b) (that gives ˆP(y)), (c) (that gives ˆP(Age = old | y)), (d) (that gives
ˆP(Pulse = slow | y)) and (e) (that gives ˆP(Temperature = high | y)).
The independence assumption is a very strong assumption about the underlying probability distribution. As a result, NB has very high bias. However, due to the low dimensionality
of the base probabilities from which the model is estimated, it has low variance.
Averaged One-Dependence Estimators (AODE) extends to threedimensional probabilities NB’s search-free strategy of extrapolation from lower-dimensional
probabilities. It does so by averaging the estimates of all of a class of three-dimensional estimators.
A Super-Parent One-Dependence Estimator (SPODE) is a three-dimensional probability
estimator that relaxes the assumption of conditional independence by making all other attributes independent given the class and one privileged attribute, the super-parent, xα. This
is a weaker conditional independence assumption than NB’s, as it is necessarily true if NB’s
is true and may also be true when NB’s is not.
P(y,x) = P(y,xα)P(x|y,xα)
together with a conditional independence assumption
P(x|y,xα) =
P(xi |y,xα).
As this is a weaker assumption than (4), the bias of the model should be lower than that
of NB. However, it is derived from higher-dimensional probability estimates and hence its
variance should be higher.
Mach Learn 86:233–272
AODE exploits the lower bias of SPODEs while addressing their higher variance by
averaging over all estimates of P(y,x) produced by using different super-parents. AODE
seeks to use
ˆP(y,xα)ˆP(x|y,xα)/a.
However, in practice it is desirable to only use estimates of probabilities for which relevant
examples occur in the data. Hence, AODE actually uses
ˆPAODE(y,x) =
δ(xα)ˆP(y,xα)ˆP(x|y,xα)
where δ(xα) is 1 if attribute-value xα is present in the data, otherwise 0. That is, it averages
over all superparents whose value occurs in the data, and defaults to NB if there are no such
superparents.
As AODE uses all of a predeﬁned family of estimators, each of which extrapolates the
desired high-dimensional probability from lower-dimensional probabilities, it does not perform search.
In terms of the example attribute space, AODE extrapolates to Fig. 1(a) from the lowerdimensional probabilities illustrated in Fig. 1(c–h) with (f) conditioned on (c) and (d), (g)
conditioned on (c) and (e), and (g) conditioned on (d) and (e).
AODE has demonstrated strong prediction accuracy (both zero-one loss and meansquared error) with relatively modest computational requirements for low dimensional
data . In consequence, it has enjoyed substantial uptake .
In this paper we generalize to higher-dimensional probabilities the strategy of search-free
extrapolation from lower-dimensional probabilities.
For notational convenience we deﬁne
x{i,j,...,q} = ⟨xi,xj,...,xq⟩.
For example, x{2,3,5} = ⟨x2,x3,x5⟩.
AnDE aims to use
ˆP(y,xs)ˆP(x|y,xs)
indicates the set of all size-n subsets of {1,...,a}.
Mach Learn 86:233–272
However, in practice we also need to avoid using pairs of superparents whose values do
not occur in the data, and hence use
ˆPAnDE(y,x) =
δ(xs)ˆP(y,xs)ˆP(x|y,xs)
ˆPA(n−1)DE(y,x)
otherwise.
Attributes are assumed independent given the superparents and the class. Hence,
P(x|y,xs) is estimated by
ˆP(x|y,xs) =
ˆP(xi |y,xs).
Note that P(xi |y,xs) = 1 when i ∈s. Whereas other probability estimates should be
smoothed or regularized, smoothed estimates should not be used in this case, and in practice
these values are not included in the calculation.
It should be recalled that A0DE is NB and A1DE is AODE.
In terms of the simple attribute-space depicted in Fig. 1, A2DE extrapolates to (a) from
(a) conditioned on each of (f), (g) and (h), and A3DE makes inferences directly from the
class distribution in (a).
When n = a,
= {{1,...a}}, so xs = x. Therefore, the ultimate expression of AnDE,
AaDE seeks to classify using
ˆPAaDE(y,x) = ˆP(y,x)ˆP(x|y,x)
where ˆP(y,x) is estimated directly from T , cascading to ever lower dependence estimators
should the combination of attribute-values not be present in T . As P(x|y,x) and
equal 1, it classiﬁes using only its direct estimate of P(y,x).
Observation 1 The asymptotic classiﬁcation performance of AaDE equals that of the Bayes
optimal classiﬁer.
Proof AaDE classiﬁes using
z∈{c1,...,ck}
where each ˆP(·) is directly estimated from the observed data and hence approaches P(·) as
the quantity of data approaches inﬁnity. Hence, in the limit, AaDE approaches
z∈{c1,...,ck}
which is the Bayes optimal classiﬁer.
However, assuming there is sufﬁcient data to compute the necessary probabilities, and
we wish to store the necessary probabilities rather than computing them as required for
Mach Learn 86:233–272
classiﬁcation, the space complexity of AaDE is O(k ¯va). This is because joint frequencies
must be stored for every combination of attribute value and class value. Except in cases
of low dimensional data, even the computational requirements of A3DE defeat our Weka
implementation, and hence in this paper we present primarily results for A2DE with some
illustrative examples of A3DE.
AnDE forms an (n+2)-dimensional probability table containing the observed frequency
for each combination of n + 1 attribute values and the class labels. The space complexity
of the table is O(k
¯vn+1) and the time complexity of compiling it is O(t
we need to update each entry for every combination of the n + 1 attribute-values for every
instance. The time complexity of classifying a single example is O(ka
) as we need to
consider each attribute for every qualiﬁed combination of n parent attributes within each
We demonstrate that as n increases, averaged n-dependence estimators achieve lower bias
at the cost of higher variance. In consequence, the ideal dimensionality of dependence will
depend on the degree to which the underlying probability distribution ﬁts the assumptions of
the n-dependence estimator, the quantity of data available to estimate the base probabilities,
and the computational demands of averaging over higher-dimensional estimators.
2.3 Weighted averaging
AODE and its generalization AnDE perform an unweighted average of the component ndependence estimators. It has been demonstrated that weighted averaging can improve upon
the accuracy of AODE’s estimates . The empirical evidence suggests that the Bayesian model averaging of
Maximum a Posteriori Linear Mixture of Generative Distributions (MAPLMG) is the most
effective of current approaches . It seems
likely that similar approaches will be equally effective with n-dependence estimators.
It is notable that the introduction of Bayesian model averaging to the AnDE framework
introduces both search and discriminative learning, as a search is performed for the set of
weights that optimize the posterior probabilities relative to the training data. Doing so can
be expected to reduce bias at the cost of introduction of variance.
One of the interesting questions that this paper investigates is the relative payoff for the
investment of additional computation in either performing Bayesian model averaging on
AnDE or increasing n and using A(n+1)DE. Both approaches can be expected to reduce
bias at the cost of an increase in both variance and computation. Which provides the more
effective trade-off?
2.4 Tree Augmented Naive Bayes
An n-dependence Bayesian classiﬁer (n-DBC) is a Bayesian network in
which each attribute depends upon the class and at most n other non-class attributes. An n-
DBC uses (n+2)-dimensional probabilities. Within this framework, NB is a 0-DBC, AODE
is a 1-DBC and the full Bayesian classiﬁer is an (a+1)-DBC.
An alternative to the AnDE approach to relaxing NB’s independence assumption is to
use search to select a single model that adds selected interdependencies between attributes.
Tree Augmented Naive Bayes (TAN) is a popular approach of this
type. It uses conditional mutual information to select a best single parent for each attributes,
in addition to the class. Thus, it is a 1-DBC.
Mach Learn 86:233–272
It is interesting to consider how search for a single Bayesian classiﬁer model compares
with averaging over a class of Bayesian classiﬁer models of the same level of dependence
or of a higher level of dependence. This paper also investigates this issue.
3 Relationship to Feating
Feating is a generic ensemble learning technique that also builds upon
the ensembling strategy of AODE. Like AnDE, Feating operates by building a local model
for each combination of n attribute values. To classify a new instance, Feating applies all
applicable local models and aggregates the results by performing a majority vote of the
resulting classiﬁcations. AnDE is similar to Feating NB. However, Feating aggregates the
predictions of its base learners by taking the mode of the class predictions. For probabilistic classiﬁers, these class predictions correspond to the maximum posterior probability. In
contrast, AnDE uses the ensemble to estimate the joint probability, P(x,y) for each class,
and then calculates its estimate of the posterior probability from this ensembled estimate
of the joint probability. A generic ensembling technique, such as Feating, cannot work by
calculating an ensemble estimate of the joint probabilities because many classiﬁers do not
produce appropriate probability estimates.
Despite the close relationship to Feating, AnDE is worthy of study in its own right for
three reasons.
First, irrespective of which aggregation method is used, coupling the search-less ensembling strategy embodied by Feating with search-less base learner NB creates a learner that
can deliver low bias using a predeﬁned mapping from low dimensional probabilities to the
desired high dimensional probabilities without search or model selection. Hence, AnDE provides an example of an alternative to the traditional search-based learning paradigm which
is able to deliver low bias classiﬁers.
Second, as already noted, AnDE utilizes a different aggregation method to Feating. It is
interesting to examine the consequences of these differences. Cerquides and de Màntaras
 found that weighted ensembles of joint probability estimates achieved lower error
than weighted ensembles of posterior probability estimates, so there is some evidence that
the outcomes may be substantially different.
Third, as there is overlap in the information required by each of its local models,
AnDE can use a single compiled matrix of joint frequencies, saving considerable space
relative to storing all of the local models independently. The space complexity of an
AnDE model is O(k
)¯vn+1 whereas the space complexity of Feating NB to level n
is O(k(a−n)
¯vn+1), which is (n + 1) times the space complexity of AnDE. Most base
models formed by Feating will not have this property, and hence AnDE is a notable special
4 Evaluation
In this section, we evaluate the efﬁcacy of AnDE. Due to relatively high time complexity
of higher-dimensional estimators, the highest level of AnDE with which we perform detailed assessment is A2DE. The primary metrics we use are bias, variance, zero-one loss
and RMSE. To assess computational overheads we use total training and classiﬁcation times
divided by the number of examples.
Mach Learn 86:233–272
We ﬁrst study the performance of NB, AODE and A2DE to reveal how performance
varies as n increases within the AnDE framework. TAN and MAPLMG are studied to show
how the search-free generative AnDE strategy compares with, respectively, discriminative
search for a single Bayesian network classiﬁer of the same dimensionality of dependence,
and discriminative search for a weighted classiﬁer of the next lower dimensionality of dependence. We also compare A2DE, that estimates the mean joint probability of the submodels,
with variants that calculate the mean posterior probability (PA2DE) and perform Feating
of NB by taking the mode of the class predictions of the submodels (FA2DE). Finally, to
explore how the classiﬁcation performance of A2DE compares to state-of-the-art classiﬁers,
we also study Random Forests with ten trees (RF10) and Random Forests
with 100 trees (RF100).
We compare these algorithms implemented in the Weka workbench on the 62 data sets described in Table 2 that have been used previously in related
Table 2 Data sets used for experiments
Auto Imports
Balance Scale
Breast Cancer (Wisconsin)
Car Evaluation
Census-Income (KDD)
Connect-4 Opening
Contact-lenses
Contraceptive Method Choice
Credit Screening
Cylinder Bands
Dermatology
Echocardiogram
Glass Identiﬁcation
Haberman’s Survival
Heart Disease (Cleveland)
Horse Colic
House Votes 84
Hypothyroid(Garavan)
Ionosphere
Iris Classiﬁcation
King-rook-vs-king-pawn
Mach Learn 86:233–272
Table 2 (Continued)
Labor negotiations
Letter Recognition
Liver Disorders (Bupa)
Lung Cancer
Lymphography
MAGIC Gamma Telescope
Nettalk (Phoneme)
New-Thyroid
Optical Digits
Page Blocks
Pen Digits
Pima Indians Diabetes
Postoperative Patient
Primary Tumor
Promoter Gene Sequences
Sick-euthyroid
Sonar Classiﬁcation
SPAM E-mail
Splice-junction Gene Sequences
Teaching Assistant Evaluation
Tic-Tac-Toe Endgame
Waveform-5000
Wine Recognition
research . Each algorithm is tested on each data
set using the repeated cross-validation bias-variance estimation method . In
order to maximize the variation in the training data from trial to trial, we use two-fold cross
validation. To minimize the variance in our measurements we report average values over 50
cross-validation trials.
We also form learning curves for NB, AODE, A2DE and A3DE on the Adult data set to
further investigate how increasing n within the AnDE framework affects performance as the
quantity of data increases.
Mach Learn 86:233–272
The current implementations of AODE and A2DE are limited to categorical data. A number of approaches have been developed for extending AODE to numeric data . These could be generalized to the AnDE framework, but how best to do so is a matter
for future research. Hence, we assess only the relative capacities of these algorithms with
respect to categorical data. To this end, all numeric attributes are discretized. When MDL
discretization , a common discretization method for NB, is used
to discretize quantitative attributes within each cross-validation fold, many attributes have
only one value. In these experiments, we discretize quantitative attributes using three-bin
equal-frequency discretization prior to classiﬁcation.
The base probabilities are estimated using m-estimation (m = 1), as it
often appears to lead to more accurate probabilities than Laplace estimation for NB and
AODE. An exception is that we always use 1.0 for ˆP(xi | y,xs) when i ∈s.
The above experiments were conducted on a single CPU single core virtual Linux machine running on a Dell PowerEdge 1950 with dual quad core Intel Xeon E5410 processors
running at 2333 MHz with 32GB of RAM.1
Average values for each combination of metric, algorithm and dataset are provided in the
Appendix. Summary results are provided in the text.
4.1 Varying n within AnDE
We ﬁrst consider the relative performance of the three variants of AnDE. For each performance measure, the number of data sets for which A2DE has lower, equal or higher outcomes relative to AODE and NB are summarized into win/draw/loss records, and likewise
for AODE relative to NB. For each win/draw/loss record a binomial sign test is performed to
assess the probability of observing the given number of wins and losses if each were equally
likely. These results are presented in Table 3. As expected, we see that increasing n from
0 (NB) to 1 (AODE) to 2 (A2DE) consistently decreases bias at the cost of an increase in
variance. As we believe that different bias and variance proﬁles suit different data quantities
 , we believe that the zero-one loss and RMSE results tell us as much
about the composition of the data collection as they do about the algorithms. Speciﬁcally,
we contend that whether one algorithm or another will win on a given dataset is determined
Table 3 Win/draw/loss: AnDE, n = 0, 1 and 2, on all 62 data sets
A2DE vs AODE
A2DE vs NB
AODE vs NB
Zero-one loss
1Due to technical issues including memory leaks in the Weka implementation of Random Forests, it was not
possible to complete all 50 runs of 2-fold cross validation for RF10 on Covertype and RF100 on Covertype
and Census-Income (KDD). These experiments were instead completed on a Linux Cluster of Xeon 2.8 GHz
CPUs, an environment that does not allow reliable time measurements to be taken. For RF10 and RF100 on
Covertype, compute times were estimated by averaging over those runs that could be completed on the virtual
machine. No runs could be completed on the virtual machine for RF100 on Census-Income (KDD) and so no
time results are reported.
Mach Learn 86:233–272
Fig. 2 Zero-one loss and RMSE of NB and AnDE on Adult dataset, as function of training set size
by how well the two algorithms’ learning biases match the underlying distribution, by their
variance, and by the quantity of data. A low variance algorithm will usually have an advantage for small data while a low bias algorithm will usually be advantaged by large data.
For our datasets, both AODE and A2DE reduce both zero-one loss and RMSE signiﬁcantly
often relative to NB. While A2DE obtains lower zero-one loss and RMSE than AODE more
often than the reverse, this difference is not found to be signiﬁcant.
To investigate in greater detail our expectation that algorithms with lower variance will
be advantaged for small data and those with lower bias for larger data, we form learning
curves for Adult, replicating the method of Webb et al. . 1000 objects are selected at
random as a test set and training sets were sampled from the remaining objects. The training
set size starts from 23 and then doubles up to 47104, this being a progression that ends
with as close to all the available data as possible once the 1000 test cases are removed. This
process is repeated 50 times and each algorithm is evaluated on the resulting training-test
set pairs. The learning curves of zero-one loss and RMSE for NB, AODE, A2DE and A3DE
are presented in Fig. 2.
The plots for zero-one loss clearly show the predicted trade-off for increasing n. At the
smallest data size, where low variance is more important than low bias, zero-one loss is
minimized by n = 0 (NB) and increases as n increases. At the largest data size, where low
bias is most important, this dimensionality is reversed. A similar trend is shown with respect
to RMSE although the algorithms have not yet achieved their asymptotic rates at the largest
data sizes available.
It is interesting to see how the relative bias/variance trade-offs of increasing n play off
when NB’s attribute independence assumption holds. The LED dataset has a speciﬁc con-
ﬁguration of attribute-values for each class, making the attributes conditionally independent
given the class. Each attribute has 10% noise added. AODE and A2DE overﬁt this noise,
Mach Learn 86:233–272
Table 4 Win/draw/loss: AnDE,
n = 0, 1 and 2, on the ten largest
A2DE vs AODE
A2DE vs NB
AODE vs NB
Zero-one loss
Fig. 3 Average per-example training and classiﬁcation times for NB, AODE and A2DE
leading to increased error. NB’s zero-one loss is 0.2627, AODE’s is 0.2639 and A2DE’s
is 0.2667. These outcomes are with training set sizes of 500. Using the UCI data generator, we generated 10 LED datasets comprising 2,000 and 10 comprising 4,000 instances
and repeated the cross-validation experiments thereon. For the datasets of 2,000, the training set size is 1,000 and the mean and standard deviation of the respective zero-one loss is
NB: 0.2603±0.0099, AODE: 0.2601±0.0101 and A2DE 0.2603±0.0102. For the datasets
of 4,000, the training set size is 2,000 and the mean and standard deviation of the respective
zero-one loss is NB: 0.2597±0.0049, AODE: 0.2598±0.0051 and A2DE: 0.2603±0.0053.
It seems clear that increasing training set sizes rapidly reduces the error advantage that NB
enjoys in this context where its conditional attribute assumption is satisﬁed.
As ﬁnal conﬁrmation that higher n is best suited to larger data, on the ten largest datasets,
those with more than 8,000 examples, A2DE always achieves lower zero-one loss and
RMSE than AODE (p = 0.001), see Table 4.
As expected, both training and classiﬁcation compute times increase as n increases. Figure 3 shows the grand averages for the per-example training and classiﬁcation times for each
algorithm.
4.2 Comparison with TAN
We here explore the relative beneﬁts of discriminative search for a single best Bayesian
classiﬁer model against AnDE’s search-free approach of averaging over a class of Bayesian
classiﬁer models. To this end we compare AODE and A2DE with TAN. Table 5 presents
win/draw/loss results comparing AODE and A2DE to TAN.
Overall, TAN has an advantage in bias but a disadvantage in variance relative to AODE.
When using search to select a single 1-DBC model is compared to averaging over a class
of 2-DBCs, the bias advantage is lost but the variance disadvantage remains. The relative
bias-variance tradeoffs of AODE and TAN result in a general error advantage to AODE.
Comparing TAN to A2DE, TAN no longer has a bias advantage, and at this higher value of
n, the error advantage of the AnDE classiﬁer becomes even more consistent.
Figure 4 shows the relative training and classiﬁcation times for AODE, A2DE and TAN.
It is clear that A2DE has a considerably greater computational requirements both for training
Mach Learn 86:233–272
Table 5 Win/draw/loss: AnDE,
n = 1 and 2 vs TAN on all 62
A2DE vs TAN
AODE vs TAN
Zero-one loss
Fig. 4 Average per-example training and classiﬁcation times for AODE, A2DE and TAN. (Two values are
shown for each algorithm, the average across all datasets and the average across the ten lowest-dimensional
(4–7 attributes) datasets)
Table 6 Win/draw/loss: AnDE,
n = 1 and 2 vs MAPLMG on all
62 data sets
A2DE vs MAPLMG
AODE vs MAPLMG
Zero-one loss
and classiﬁcation. However, this disadvantage disappears when we consider only the ten
lowest dimensional datasets, also illustrated in this ﬁgure.
4.3 Comparison with MAPLMG
As discussed above, we wish to investigate the relative payoffs obtained by investing additional computation to that required by AODE by respectively using discriminative learning
of weights or, alternatively, increasing the dimensionality of the probabilities from which
the posterior probability is extrapolated. To this end, Table 6 presents win/draw/loss results
comparing A2DE and AODE to MAPLMG.
As established by previous research , MAPLMG’s approach of using discriminative learning of weights for the AODE linear combination significantly reduces bias relative to AODE at the cost of an increase in variance. However, relative to this discriminative approach to extrapolating from three-dimensional probabilities,
A2DE’s search-free approach to extrapolating from four-dimensional probabilities further
reduces bias at the cost of an increase in variance. While the resulting difference in error is
not found to be signiﬁcant across the full suite of 62 datasets, when the ten largest datasets
Mach Learn 86:233–272
Table 7 Win/draw/loss: AnDE,
n = 1 and 2 vs MAPLMG on the
ten largest data sets
A2DE vs MAPLMG
AODE vs MAPLMG
Zero-one loss
Fig. 5 Average per-example training and classiﬁcation times for AODE, A2DE and MAPLMG. (In addition
to the times for all datasets, training times are shown for the ten largest datasets)
are considered, the lower bias algorithm, A2DE, consistently achieves lower zero-one loss
and RMSE than MAPLMG (p = 0.001) (see Table 7).
MAPLMG’s Bayesian model averaging comes at considerable cost in training time.
Figure 5 shows the average per-example training and test times for AODE, A2DE and
MAPLMG. Note that MAPLMG is implemented as an external function to Weka, and hence
is likely to be inherently more efﬁcient. The training and test times include a substantial
ﬁxed overhead, and hence the per-instance training times should decrease if the complexity
is linear with respect to the training set size. However, MAPLMG’s super-linear training
complexity minimizes this effect, demonstrating that it will not be feasible to apply it to
very large data.
4.4 Comparison with Feating
To understand how the AnDE approach performs relative to Feating NB, we compare A2DE,
that calculates the mean of the joint probabilities, with a variant PA2DE, that calculates the
mean of the posterior probabilities, and another variant FA2DE, that calculates the mode of
the classes predicted by the submodels. As described in Sect. 3 and the start of Sect. 4, these
embody the two main differences between AnDE and Feating NB.
Table 8 shows the win/draw/loss results comparing A2DE to these variants. It is clear that
both variants have higher bias but lower variance than A2DE. It is straightforward to understand why Feating would have lower variance. The mode is a much more stable estimator of
central tendency than the mean, which can be greatly inﬂuenced by a single outlier. It is less
obvious why lower variance should result from averaging over the estimates of the posterior
rather than of the joint probability. Nonetheless, the result is consistent with Cerquides and
de Màntaras’ ﬁnding that a linear combination of joint probability estimates resulted
in higher accuracy than a linear combination of posterior probability estimates. This remains
an interesting unexplained phenomena worthy of further investigation.
Over the full range of datasets these differences in bias and variance proﬁles do not
result in statistically signiﬁcant differences on either measure of error, except with respect
Mach Learn 86:233–272
Table 8 Win/draw/loss: A2DE
vs PA2DE and FA2DE
A2DE vs PA2DE
A2DE vs FA2DE
Zero-one loss
Table 9 Win/draw/loss: A2DE
vs PA2DE and FA2DE on the ten
largest datasets
A2DE vs PA2DE
A2DE vs FA2DE
Zero-one loss
to RMSE for Feating. This reﬂects the manner in which Feating selects a single class rather
than producing a distribution of class probabilities.
Due to its lower bias, A2DE achieves lower zero-one loss than PA2DE on eight and
FA2DE on nine of the ten largest datasets. This outcome is statistically signiﬁcant at the
0.05 level with respect to FA2DE, but misses out on being statistically signiﬁcant with respect to PA2DE. A2DE achieves lower RMSE than both the alternatives on eight of the ten
largest datasets, and draws on one of the remaining datasets with respect to FA2DE, again
attaining statistical signiﬁcance at the 0.05 level relative to FA2DE but failing to do so relative to PA2DE. Hence, the evidence is suggestive that for large data the AnDE approach is
preferable to the selection of the mode, and calculating the mean of the joint probabilities is
preferable to calculation the mean of the posteriors.
4.5 Comparison with the state-of-the-art
In addition to the relative performance of these related algorithms, it is useful to understand
how the performance compares to well known examples of the state-of-the-art. We choose
Random Forests as the comparator algorithm because it is relatively unparameterized and hence readily produces clearly understood performance outcomes. We
use Random Forests with both the default setting of 10 trees (RF10) and with 100 trees
(RF100), allowing us to explore the relative computational/accuracy trade-offs. Table 10
shows the win/draw/loss results for each of A2DE, AODE and NB against RF10 and RF100
for each of zero-one loss, Bias, Variance and RMSE.
All three levels of AnDE have higher bias but lower variance than both levels of Random Forests. This trade-off delivers lower error signiﬁcantly more often than not for both
A2DE and AODE relative to RF10. Both deliver lower error more often than RF100, but
not signiﬁcantly so. Notably, relative to both RF10 and RF100, NB achieves higher error
almost as often as lower. This illustrates the weaknesses of such ‘bake-offs’ with respect to
error. As we have argued above, low variance algorithms such as NB will be advantaged by
the relatively small data sets used in this study. To assess this effect, we repeated the error
comparisons using only the ten largest datasets, those containing more than 8000 examples.
The results are shown in Table 11. For these larger datasets, both R10 and RF100 achieve
lower error more often than all three of A2DE, AODE and NB, signiﬁcantly so with respect
to AODE and NB and when comparing RF100 to A2DE on zero-one loss. This suggests that
Mach Learn 86:233–272
Table 10 Win/draw/loss:
AnDE, n = 0, 1 and 2, vs RF10
and RF100 on all 62 data sets
AnDE vs RF10
AnDE vs RF100
Zero-one loss
Zero-one loss
Zero-one loss
Table 11 Win/draw/loss:
AnDE, n = 0, 1 and 2, vs RF10
and RF100 on the ten largest data
AnDE vs RF10
AnDE vs RF100
Zero-one loss
Zero-one loss
Zero-one loss
for very large training data, in the absence of any prior knowledge of the nature of the multivariate probability distribution that the data embodies, Random Forests are likely to achieve
lower error than an AnDE classiﬁer, although the data quantity at which this is achieved will
be ever greater as the dimensionality of AnDE is increased.
However, Random Forests’ error advantage for large data comes at a cost in training time.
Figure 6 shows the training and classiﬁcation times for AODE, A2DE, RF10 and RF100. It
is apparent that, overall, RF100 has very high training times. While A2DE’s training time
does approach RF100’s for high dimensional data, for small data and low dimensional data
its training times are competitive with RF10. On the other hand, A2DE requires substantially
more classiﬁcation time on average than Random Forests. This requirement grows greatly
with high-dimensional data. A2DE will not be feasible for classiﬁcation of large numbers
of high-dimensional objects. In contrast, its classiﬁcation time is very competitive on lowdimensional data.
5 Conclusions and directions for future research
AnDE provides an attractive framework for developing machine learning techniques. A single parameter n controls a bias-variance trade-off such that n = a provides a classiﬁer whose
Mach Learn 86:233–272
Fig. 6 Average per-example training and classiﬁcation times for AODE, A2DE, RF10 and RF100. (Training
times are presented for all, the ten largest (excluding Census Income, for which RF100 could not be executed
on a machine for which reliable times could be obtained, thus 5,620–581,012 examples), the ten lowest
dimensional (5–7 attributes) and the ten highest dimensional (43–70 attributes) datasets. Classiﬁcation times
are presented for all, the ten lowest dimensional and the ten highest dimensional datasets)
asymptotic error is the Bayes optimal error rate. However, for high-dimensional data only
very low-dimensional forms of AnDE are feasible. Nonetheless, we have established that
higher-dimensional variants are likely to deliver greater accuracy than lower-dimensional
alternatives when the number of training examples is high. In consequence, a promising
direction for future research is to develop computationally efﬁcient techniques for approximating AnDE for high values of n.
A further unresolved issue is how to select an appropriate value of n for any speciﬁc
dataset T . Are there more computationally efﬁcient approaches than a simple wrapper-based
comparison of each possible value?
A number of techniques have been developed for extending AODE to handle numeric
data . There is a need to extend this work to the more general AnDE
framework.
We have presented a strategy for learning without ﬁtting the full multivariate probability
distribution. We do not argue, however, that ﬁtting the full multivariate probability distribution should necessarily be avoided. Indeed, it has been demonstrated that it is possible
to reduce the error of AODE both by appropriate feature selection and weighting of the submodels in order to better ﬁt the full multivariate probability distribution. Therefore, it is likely to be worthwhile to explore efﬁcient methods for
each of these strategies for higher values of n. If fast classiﬁcation is required, and time for
training is less constrained, approaches that use search to select a small number of submodels from an AnDE model are likely to be desirable. Where there is sufﬁcient training time
available, search for appropriate submodel weights is also likely to be useful.
We have developed a generative learning algorithm that generalizes the principles that
underlie AODE to ever higher levels of dimensionality. It has the following desirable features:
• both time and space complexity are linear with respect to the number of training examples;
• it learns in a single pass through the training data;
• it performs direct prediction of class probabilities;
• it has integrated handling of missing values;
• it is robust in the face of noise;
• other than the choice of which instantiation (choice of n) and choice of smoothing technique, the approach uses no tunable parameters;
• it does not perform model selection;
Mach Learn 86:233–272
• a simple mechanism controls the bias/variance trade-off;
• it supports incremental learning;
• learning and classiﬁcation can readily utilize parallel computation; and
• there is a direct theoretical basis that provides optimal prediction except insofar as clearly
speciﬁed assumptions are violated.
A single parameter n provides control over a bias-variance trade-off, such that higher values of n are appropriate for greater numbers of training cases. AnDE demonstrates that it
is possible to develop competitive learners without using search. Of further interest, this
family of algorithms show that it is possible to develop low bias algorithms in a generative
framework. Finally, A2DE proves to be a computationally tractable version of AnDE that
delivers strong classiﬁcation accuracy for large data without any parameter tuning.
Acknowledgements
Australian
DP110101427. We are grateful to Mark Carman, Joao Gama, Kevin Korb and Nayyar Zaidi for insightful
discussions on this research and feedback on drafts of this paper.
Appendix: Detailed results
Detailed results for Bias, Variance, zero-one Loss, RMSE, Training Time and Classiﬁcation
Times are presented in Tables 8 to 13. The datasets are listed in ascending order on number
of instances.
Mach Learn 86:233–272
Table 12 Bias
Contact-lenses
Lung Cancer
Labor negotiations
Postoperative Patient
Promoter Gene Sequences
Echocardiogram
Lymphography
Iris Classiﬁcation
Teaching Assistant Evaluation
Wine Recognition
Auto Imports
Sonar Classiﬁcation
Glass Identiﬁcation
New-Thyroid
Heart Disease (Cleveland)
Haberman’s Survival
Primary Tumor
Liver Disorders (Bupa)
Ionosphere
Dermatology
Horse Colic
Mach Learn 86:233–272
Table 12 (Continued)
House Votes 84
Cylinder Bands
Balance Scale
Credit Screening
Breast Cancer (Wisconsin)
Pima Indians Diabetes
Tic-Tac-Toe Endgame
Contraceptive Method Choice
Car Evaluation
Splice-junction Gene Sequences
King-rook-vs-king-pawn
Hypothyroid (Garavan)
Sick-euthyroid
SPAM E-mail
Waveform-5000
Mach Learn 86:233–272
Table 12 (Continued)
Nettalk (Phoneme)
Page Blocks
Optical Digits
Pen Digits
MAGIC Gamma Telescope
Letter Recognition
Connect-4 Opening
Census-Income (KDD)
Mach Learn 86:233–272
Table 13 Variance
Contact-lenses
Lung Cancer
Labor negotiations
Postoperative Patient
Promoter Gene Sequences
Echocardiogram
Lymphography
Iris Classiﬁcation
Teaching Assistant Evaluation
Wine Recognition
Auto Imports
Sonar Classiﬁcation
Glass Identiﬁcation
New-Thyroid
Heart Disease (Cleveland)
Haberman’s Survival
Primary Tumor
Liver Disorders (Bupa)
Ionosphere
Dermatology
Horse Colic
Mach Learn 86:233–272
Table 13 (Continued)
House Votes 84
Cylinder Bands
Balance Scale
Credit Screening
Breast Cancer (Wisconsin)
Pima Indians Diabetes
Tic-Tac-Toe Endgame
Contraceptive Method Choice
Car Evaluation
Splice-junction Gene Sequences
King-rook-vs-king-pawn
Hypothyroid (Garavan)
Sick-euthyroid
SPAM E-mail
Waveform-5000
Mach Learn 86:233–272
Table 13 (Continued)
Nettalk (Phoneme)
Page Blocks
Optical Digits
Pen Digits
MAGIC Gamma Telescope
Letter Recognition
Connect-4 Opening
Census-Income (KDD)
Mach Learn 86:233–272
Table 14 Zero-one loss
Contact-lenses
Lung Cancer
Labor negotiations
Postoperative Patient
Promoter Gene Sequences
Echocardiogram
Lymphography
Iris Classiﬁcation
Teaching Assistant Evaluation
Wine Recognition
Auto Imports
Sonar Classiﬁcation
Glass Identiﬁcation
New-Thyroid
Heart Disease (Cleveland)
Haberman’s Survival
Primary Tumor
Liver Disorders (Bupa)
Ionosphere
Dermatology
Horse Colic
Mach Learn 86:233–272
Table 14 (Continued)
House Votes 84
Cylinder Bands
Balance Scale
Credit Screening
Breast Cancer (Wisconsin)
Pima Indians Diabetes
Tic-Tac-Toe Endgame
Contraceptive Method Choice
Car Evaluation
Splice-junction Gene Sequences
King-rook-vs-king-pawn
Hypothyroid (Garavan)
Sick-euthyroid
SPAM E-mail
Waveform-5000
Nettalk (Phoneme)
Mach Learn 86:233–272
Table 14 (Continued)
Page Blocks
Optical Digits
Pen Digits
MAGIC Gamma Telescope
Letter Recognition
Connect-4 Opening
Census-Income (KDD)
Mach Learn 86:233–272
Table 15 RMSE
Contact-lenses
Lung Cancer
Labor negotiations
Postoperative Patient
Promoter Gene Sequences
Echocardiogram
Lymphography
Iris Classiﬁcation
Teaching Assistant Evaluation
Wine Recognition
Auto Imports
Sonar Classiﬁcation
Glass Identiﬁcation
New-Thyroid
Heart Disease (Cleveland)
Haberman’s Survival
Primary Tumor
Liver Disorders (Bupa)
Ionosphere
Dermatology
Horse Colic
Mach Learn 86:233–272
Table 15 (Continued)
House Votes 84
Cylinder Bands
Balance Scale
Credit Screening
Breast Cancer (Wisconsin)
Pima Indians Diabetes
Tic-Tac-Toe Endgame
Contraceptive Method Choice
Car Evaluation
Splice-junction Gene Sequences
King-rook-vs-king-pawn
Hypothyroid (Garavan)
Sick-euthyroid
SPAM E-mail
Waveform-5000
Nettalk (Phoneme)
Mach Learn 86:233–272
Table 15 (Continued)
Page Blocks
Optical Digits
Pen Digits
MAGIC Gamma Telescope
Letter Recognition
Connect-4 Opening
Census-Income (KDD)
Mach Learn 86:233–272
Table 16 Per instance training time
Contact-lenses
Lung Cancer
Labor negotiations
Postoperative Patient
Promoter Gene Sequences
Echocardiogram
Lymphography
Iris Classiﬁcation
Teaching Assistant Evaluation
Wine Recognition
Auto Imports
Sonar Classiﬁcation
Glass Identiﬁcation
New-Thyroid
Heart Disease (Cleveland)
Haberman’s Survival
Primary Tumor
Liver Disorders (Bupa)
Ionosphere
Dermatology
Horse Colic
Mach Learn 86:233–272
Table 16 (Continued)
House Votes 84
Cylinder Bands
Balance Scale
Credit Screening
Breast Cancer (Wisconsin)
Pima Indians Diabetes
Tic-Tac-Toe Endgame
Contraceptive Method Choice
Car Evaluation
Splice-junction Gene Sequences
King-rook-vs-king-pawn
Hypothyroid (Garavan)
Sick-euthyroid
SPAM E-mail
Waveform-5000
Nettalk (Phoneme)
Mach Learn 86:233–272
Table 16 (Continued)
Page Blocks
Optical Digits
Pen Digits
MAGIC Gamma Telescope
Letter Recognition
Connect-4 Opening
Census-Income (KDD)
Mean (excluding Census-Income)
Mach Learn 86:233–272
Table 17 Per instance classiﬁcation time
Contact-lenses
Lung Cancer
Labor negotiations
Postoperative Patient
Promoter Gene Sequences
Echocardiogram
Lymphography
Iris Classiﬁcation
Teaching Assistant Evaluation
Wine Recognition
Auto Imports
Sonar Classiﬁcation
Glass Identiﬁcation
New-Thyroid
Heart Disease (Cleveland)
Haberman’s Survival
Primary Tumor
Liver Disorders (Bupa)
Ionosphere
Dermatology
Horse Colic
Mach Learn 86:233–272
Table 17 (Continued)
House Votes 84
Cylinder Bands
Balance Scale
Credit Screening
Breast Cancer (Wisconsin)
Pima Indians Diabetes
Tic-Tac-Toe Endgame
Contraceptive Method Choice
Car Evaluation
Splice-junction Gene Sequences
King-rook-vs-king-pawn
Hypothyroid (Garavan)
Sick-euthyroid
SPAM E-mail
Waveform-5000
Nettalk (Phoneme)
Mach Learn 86:233–272
Table 17 (Continued)
Page Blocks
Optical Digits
Pen Digits
MAGIC Gamma Telescope
Letter Recognition
Connect-4 Opening
Census-Income (KDD)
Mean (excluding Census-Income)
Mach Learn 86:233–272