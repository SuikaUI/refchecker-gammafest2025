Decentralized Reinforcement Learning Control
of a Robotic Manipulator
Lucian Bus¸oniu
Bart De Schutter
Robert Babuˇska
Delft Center for Systems and Control
Delft University of Technology
2628 CD Delft, The Netherlands
Email: {i.l.busoniu,b.deschutter,r.babuska}@tudelft.nl
Abstract— Multi-agent systems are rapidly ﬁnding applications
in a variety of domains, including robotics, distributed control,
telecommunications, etc. Learning approaches to multi-agent
control, many of them based on reinforcement learning (RL),
are investigated in complex domains such as teams of mobile
robots. However, the application of decentralized RL to low-level
control tasks is not as intensively studied. In this paper, we
investigate centralized and decentralized RL, emphasizing the
challenges and potential advantages of the latter. These are then
illustrated on an example: learning to control a two-link rigid
manipulator. Some open issues and future research directions in
decentralized RL are outlined.
Keywords—multi-agent learning, decentralized control, reinforcement learning
I. INTRODUCTION
A multi-agent system (MAS) is a collection of interacting
agents that share a common environment (operate on a common process), which they perceive through sensors, and upon
which they act through actuators . In contrast to the classical
control paradigm, that uses a single controller acting on the
process, in MAS control is distributed among the autonomous
MAS can arise naturally as a viable representation of the
considered system. This is the case with e.g., teams of mobile
robots, where the agents are the robots and the process is
their environment , . MAS can also provide alternative
solutions for systems that are typically regarded as centralized,
e.g., resource management: each resource may be managed by
a dedicated agent or several agents may negotiate access
to passive resources . Another application ﬁeld of MAS
is decentralized, distributed control, e.g., for trafﬁc or power
Decentralized, multi-agent solutions offer several potential
advantages over centralized ones :
– Speed-up, resulting from parallel computation.
– Robustness to single-point failures, if redundancy is built
into the system.
– Scalability, resulting from modularity.
MAS also pose certain challenges, many of which do not
appear in centralized control. The agents have to coordinate
their individual behaviors, such that a coherent joint behavior
results that is beneﬁcial for the system. Conﬂicting goals, interagent communication, and incomplete agent views over the
process, are issues that may also play a role.
The multi-agent control task is often too complex to be
solved effectively by agents with pre-programmed behaviors.
Agents can do better by learning new behaviors, such that
their performance gradually improves , . Learning can
be performed either online, while the agents actually try to
solve the task, or ofﬂine, typically by using a task model to
generate simulated experience.
Reinforcement learning (RL) is a simple and general
framework that can be applied to the multi-agent learning
problem. In this framework, the performance of each agent is
rewarded by a scalar signal, that the agent aims to maximize.
A signiﬁcant body of research on multi-agent RL has evolved
over the last decade (see e.g., , , ).
In this paper, we investigate the single-agent, centralized RL
task, and its multi-agent, decentralized counterpart. We focus
on cooperative low-level control tasks. To our knowledge,
decentralized RL control has not been applied to such tasks.
We describe the challenge of coordinating multiple RL agents,
and brieﬂy mention the approaches proposed in the literature.
We present some potential advantages of multi-agent RL. Most
of these advantages extend beyond RL to the general multiagent learning setting.
We illustrate the differences between centralized and multiagent RL on an example involving learning to control a twolink rigid manipulator. Finally, we present some open research
issues and directions for future work.
The rest of the paper is organized as follows. Section II
introduces the basic concepts of RL. Cooperative decentralized
RL is then discussed in Section III. Section IV introduces
the two-link rigid manipulator and presents the results of RL
control on this process. Section V concludes the paper.
II. REINFORCEMENT LEARNING
In this section we introduce the main concepts of centralized
and multi-agent RL for deterministic processes. This presentation is based on , .
A. Centralized RL
The theoretical model of the centralized (single-agent) RL
task is the Markov decision process.
Deﬁnition 1: A
Markov decision process
⟨X, U, f, ρ⟩where: X is the discrete set of process states, U
is the discrete set of agent actions, f : X × U →X is the
1–4244–0342–1/06/$20.00 c⃝2006 IEEE
ICARCV 2006
state transition function, and ρ : X × U →R is the reward
The process changes state from xk to xk+1 as a result of
action uk, according to the state transition function f. The
agent receives (possibly delayed) feedback on its performance
via the scalar reward signal rk ∈R, according to the reward
function ρ. The agent chooses actions according to its policy
The learning goal is the maximization, at each time step k,
of the discounted return:
j=0 γjrk+j+1,
where γ ∈(0, 1) is the discount factor. The action-value
function (Q-function), Qh : X × U →R, is the expected
return of a state-action pair under a given policy: Qh(x, u) =
E {Rk | xk = x, uk = u, h}. The agent can maximize its return by ﬁrst computing the optimal Q-function, deﬁned as
Q∗(x, u) = maxh Qh(x, u), and then choosing actions by the
greedy policy h∗(x) = arg maxu Q∗(x, u), which is optimal
(ties are broken randomly).
The central result upon which RL algorithms rely is that
Q∗satisﬁes the Bellman optimality recursion:
Q∗(x, u) = ρ(x, u) + γ max
u′∈U Q∗(f(x, u), u′) ∀x, u.
Value iteration is an ofﬂine, model-based algorithm that
turns this recursion into an update rule:
Qℓ+1(x, u) = ρ(x, u) + γ max
u′∈U Qℓ(f(x, u), u′) ∀x, u.
where ℓis the iteration index. Q0 can be initialized arbitrarily.
The sequence Qℓprovably converges to Q∗.
Q-learning is an online algorithm that iteratively estimates
Q∗by interaction with the process, using observed rewards rk
and pairs of subsequent states xk, xk+1 :
Qk+1(xk, uk) = Qk(xk, uk)+
rk+1 + γ max
u′∈U Q(xk+1, u′) −Qk(xk, uk)
where α ∈(0, 1] is the learning rate. The sequence Qk
provably converges to Q∗under certain conditions, including
that the agent keeps trying all actions in all states with nonzero
probability . This means that the agent must sometimes
explore, i.e., perform other actions than those dictated by the
current greedy policy.
B. Multi-Agent RL
The generalization of the Markov decision process to the
multi-agent case is the Markov game.
Deﬁnition 2: A
⟨A, X, {Ui}i∈A , f, {ρi}i∈A⟩where: A
{1, . . . , n} is
the set of n agents, X is the discrete set of process states,
{Ui}i∈A are the discrete sets of actions available to the agents,
yielding the joint action set U = ×i∈AUi, f : X × U →X
is the state transition function, and ρi : X × U →R, i ∈A
are the reward functions of the agents.
Note that the state transitions, agent rewards ri,k, and thus
also the agent returns Ri,k, depend on the joint action uk =
1,k, . . . , uT
n,k]T, U k ∈U, ui,k ∈Ui. The policies hi : X ×
Ui → form together the joint policy h. The Q-function
of each agent depends on the joint action and is conditioned
on the joint policy, Qh
i : X × U →R.
A fully cooperative Markov game is a game where the
agents have identical reward functions, ρ1 = . . . = ρn. In
this case, the learning goal is the maximization the common
discounted return. In the general case, the reward functions
of the agents may differ. Even agents which form a team
may encounter situations where their immediate interests are
in conﬂict, e.g., when they need to share some resource. As the
returns of the agents are correlated, they cannot be maximized
independently. Formulating a good learning goal in such a
situation is a difﬁcult open problem (see e.g., – ).
III. COOPERATIVE DECENTRALIZED RL CONTROL
This section brieﬂy reviews approaches to solving the
coordination issue in decentralized RL, and then mentions
some of the potential advantages of decentralized RL.
A. The Coordination Problem
Coordination requires that all agents coherently choose their
part of a desirable joint policy. This is not trivial, even if the
task is fully cooperative. To see this, assume all agents learn in
parallel the common optimal Q-function with, e.g., Q-learning:
Qk+1(xk, uk) = Qk(xk, uk)+
rk+1 + γ max
u′∈U Q(xk+1, u′) −Qk(xk, uk)
Then, in principle, they could use the greedy policy to
maximize the common return. However, greedy action selection breaks ties randomly, which means that in the absence
of additional mechanisms, different agents may break a tie
in different ways, and the resulting joint action may be
suboptimal.
The multi-agent RL algorithms in the literature solve this
problem in various ways.
Coordination-free methods bypass the issue. For instance,
in fully cooperative tasks, the Team Q-learning algorithm 
assumes that the optimal joint actions are unique (which will
rarely be the case). Then, (5) can directly be used.
The agents can be indirectly steered toward coordination.
To this purpose, some algorithms learn empirical models of
the other agents and adapt to these models . Others use
heuristics to bias the agents toward actions that promise to
yield good reward . Yet others directly search through the
space of policies using gradient-based methods .
The action choices of the agents can also be explicitly
coordinated or negotiated:
– Social conventions and roles restrict the action
choices of the agents.
– Coordination graphs explicitly represent where coordination between agents is required, thus preventing the
agents from engaging in unnecessary coordination activities .
– Communication is used to negotiate action choices, either
alone or in combination with the above techniques.
B. Potential Advantages of Decentralized RL
If the coordination problem is efﬁciently solved, learning
speed might be higher for decentralized learners. This is
because each agent i searches an action space Ui. A centralized
learner solving the same problem searches the joint action
space U = U1 × · · · × Un, which is exponentially larger.
This difference will be even more signiﬁcant in tasks where
not all the state information is relevant to all the learning
agents. For instance, in a team of mobile robots, at a given
time, the position and velocity of robots that are far away from
the considered robot might not be interesting for it. In such
tasks, the learning agents can consider only the relevant state
components and thus further decrease the size of the problem
they need to solve .
Memory and processing time requirements will also be
smaller for smaller problem sizes.
If several learners solve similar tasks, then they could gain
further beneﬁt from sharing their experience or knowledge.
IV. EXAMPLE: TWO-LINK RIGID MANIPULATOR
A. Manipulator Model
The two-link manipulator, depicted in Fig. 1, is described
by the nonlinear fourth-order model:
M(θ)¨θ + C(θ, ˙θ) ˙θ + G(θ) = τ
where θ = [θ1, θ2]T, τ = [τ1, τ2]T. The system has two control
inputs, the torques in the two joints, τ1 and τ2, and four
measured outputs – the link angles, θ1, θ2, and their angular
speeds ˙θ1, ˙θ2.
The mass matrix M(θ), Coriolis and centrifugal forces
Schematic drawing of the two-link rigid manipulator.
matrix C(θ, ˙θ), and gravity vector G(θ), are:
P1 + P2 + 2P3 cos θ2
P2 + P3 cos θ2
P2 + P3 cos θ2
C(θ, ˙θ) =
b1 −P3 ˙θ2 sin θ2
−P3( ˙θ1 + ˙θ2) sin θ2
P3 ˙θ2 sin θ2
−g1 sin θ1 −g2 sin(θ1 + θ2)
−g2 sin(θ1 + θ2)
The meaning and values of the physical parameters of the
system are given in Table I.
Using these, the rest of the parameters in (6) can be
computed by:
P3 = m2l1c2
g1 = (m1c1 + m2l1)g
g2 = m2c2g
In the sequel, it is assumed that the manipulator operates
in a horizontal plane, leading to G(θ) = 0. Furthermore, the
following simpliﬁcations are adopted in (6):
1) Coriolis and centrifugal forces are neglected, leading to
C(θ, ˙θ) = diag[b1, b2];
2) ¨θ1 is neglected in the equation for ¨θ2;
3) the friction in the second joint is neglected in the
equation for ¨θ1.
After these simpliﬁcations, the dynamics of the manipulator
can be approximated by:
P2(P1 + P2 + 2P3 cos θ2)·
P2(τ1 −b1 ˙θ1) −(P2 + P3 cos θ2)τ2
The complete process state is given by x = [θT, ˙θT]T.
If centralized control is used, the command is u = τ; for
decentralized control with one agent controlling each joint
motor, the agent commands are u1 = τ1, u2 = τ2.
PHYSICAL PARAMETERS OF THE MANIPULATOR
gravitational acceleration
length of ﬁrst link
length of second link
mass of ﬁrst link
mass of second link
inertia of ﬁrst link
0.004 kgm2
inertia of second link
0.003 kgm2
center of mass of ﬁrst link
center of mass of second link
damping in ﬁrst joint
damping in second joint
0.02 kgs−1
maximum torque of ﬁrst joint motor
maximum torque of ﬁrst joint motor
maximum angular speed of ﬁrst link
2π rad/sec
maximum angular speed of second link
2π rad/sec
B. RL Control
The control goal is the stabilization of the system around
θ = ˙θ = 0 in minimum time, with a tolerance of ±5 · π/180
rad for the angles, and ±0.1 rad/sec for the angular speeds.
To apply RL in the form presented in Section II, the time
axis, as well as the continuous state and action components of
the manipulator, must ﬁrst be discretized. Time is discretized
with a sampling time of Ts = 0.05 sec; this gives the discrete
system dynamics f. Each state component is quantized in
fuzzy bins, and three torque values are considered for each
joint: −τi,max (maximal torque clockwise), 0, and τi,max
(maximal torque counter-clockwise).
One Q-value is stored for each combination of bin centers
and torque values. The Q-values of continuous states are then
interpolated between these center Q-values, using the degrees
of membership to each fuzzy bin as interpolation weights. If
e.g., the Q-function has the form Q(θ2, ˙θ2, τ2), the Q-values
of a continuous state [θ2,k, ˙θ2,k]T are computed by:
˜Q(θ2,k, ˙θ2,k, τ2) =
m=1,...,Nθ2
n=1,...,N ˙θ2
µθ2,m(θ2,k)µ ˙θ2,n( ˙θ2,k) · Q(m, n, τ2), ∀τ2
where e.g., µ ˙θ2,n( ˙θ2,k) is the membership degree of ˙θ2,k in
the nth bin. For triangular membership functions, this can be
computed as:
µ ˙θ2,n( ˙θ2,k) =
max(0, cn+1−˙θ2,k
cn+1−cn ), if n = 1
˙θ2,k−cn−1
cn−cn−1 , cn+1−˙θ2,k
if 1 < n < N ˙θ2
˙θ2,k−cn−1
cn−cn−1 ), if n = N ˙θ2
where cn is the center of the nth bin – see Fig. 2 for an
Example of quantization in fuzzy bins with triangular membership
functions for ˙θ2.
Such a set of bins is completely determined by a vector
of bin center coordinates. For ˙θ1 and ˙θ2, 7 bins are used,
with their centers at [−360, −180, −30, 0, 30, 180, 360]·π/180
rad/sec. For θ1 and θ2, 12 bins are used, with their centers at
[−180, −130, −80, −30, −15, −5, 0, 5, 15, 30, 80, 130]·π/180
rad; there is no ‘last’ or ‘ﬁrst’ bin, because the angles evolve
on a circle manifold [−π, π). The π point is identical to −π,
so the ‘last’ bin is a neighbor of the ‘ﬁrst’.
Algorithm 1 Fuzzy value iteration for a SISO RL controller
1: Q0(m, uj) = 0, for m = 1, . . . , NX, j = 1, . . . , NU
for m = 1, . . . , NX, j = 1, . . . , NU do
Qℓ+1(m, uj) = ρ(cm, uj)
µx, ˜m(f(cm, uj)) max
uj Qℓ( ˜m, ˜uj)
8: until ∥Qℓ−Qℓ−1∥≤δ
The optimal Q-functions for both the centralized and decentralized case are computed with a version of value iteration (3)
which is altered to accommodate the fuzzy representation of
the state. The complete algorithm is given in Alg. 1. For easier
readability, the RL controller is assumed single-input singleoutput, but the extension to multiple states and / or outputs is
straightforward. The discount factor is set to γ = 0.98, and
the threshold value to δ = 0.01.
The control action in state xk is computed as follows
(assuming as above a SISO controller):
uk = h(xk) =
µx,m(xk) arg max
Q( ˜m, ˜uj)
Centralized RL. The reward function ρ for the centralized
learner computes rewards by:
if |θi,k| ≤5 · π/180 rad
≤0.1 rad/sec, i ∈{1, 2}
The centralized policy for solving the two-link manipulator
task must be of the form:
[τ1, τ2]T = h(θ1, θ2, ˙θ1, ˙θ2)
Therefore, the centralized learner uses a Q-table of the form
Q(θ1, θ2, ˙θ1, ˙θ2, τ1, τ2).
The policy computed by value iteration is applied to the
system starting from the initial state x0 = [−1, −3, 0, 0]T.
The resulting command, state, and reward signals are given in
Fig. 3(a).
Decentralized RL. In the decentralized case, the rewards
are computed separately for the two agents:
if |θi,k| ≤5 · π/180 rad
≤0.1 rad/sec
For decentralized control, the system (11) creates an asymmetric setting. Agent 2 can choose its action τ2,k by only
considering the second link’s state, whereas agent 1 needs to
take into account θ2,k and τ2,k besides the ﬁrst link’s state. If
agent 2 is always the ﬁrst to choose its action, and agent 1
Link angles[rad]
Link velocities[rad/sec]
Cmd torque joint 1[Nm]
Cmd torque joint 2[Nm]
Reward [-]
(a) Centralized RL (thin line–link 1, thick line–link 2)
Link angles[rad]
Link velocities[rad/sec]
Cmd torque joint 1[Nm]
Cmd torque joint 2[Nm]
Reward [-]
(b) Decentralized RL (thin line–link / agent 1, thick line–link / agent 2)
State, command, and reward signals for RL control.
can learn about this action before it is actually taken (e.g., by
communication) then the two agents can learn control policies
of the following form:
τ2 = h2(θ2, ˙θ2)
τ1 = h1(θ1, θ2, ˙θ1, τ2)
Therefore, the two agents use Q-tables of the form
Q2(θ2, ˙θ2, τ2), and respectively Q1(θ1, θ2, ˙θ1, τ2, τ1). Value
iteration is applied ﬁrst for agent 2, and the resulting policy is
used in value iteration for agent 1.
The policies computed in this way are applied to the
system starting from the initial state x0 = [−1, −3, 0, 0]T.
The resulting command, state, and reward signals are given in
Fig. 3(b).
C. Discussion
Value iteration converges in 125 iterations for the centralized case, 192 iterations for agent 1, and 49 iterations
for agent 2. The learning speeds are therefore comparable
for centralized and decentralized learning in this application.
Agent 2 of course converges relatively faster, as it state-action
space is much smaller.
Both the centralized and the decentralized policies stabilize
the system in 1.2 seconds. The steady-state angle offsets are
all within the imposed 5 degrees tolerance bound. Notice
that in Fig. 3(b), the ﬁrst link is stabilized slightly faster
than in Fig. 3(a), where both links are stabilized at around
the same time. This is because decentralized learners are
rewarded separately (17), and have an incentive to stabilize
their respective links faster.
The form of coordination used by the two agents is
COMPUTATIONAL REQUIREMENTS
Q-table size
CPU time [sec]
Centralized
(12 × 7 × 3)2 = 63504
12 × 7 × 12 × 3 × 3 = 9072
12 × 7 × 3 = 252
Agent 1 + Agent 2
indirect. The second agent can safely ignore the ﬁrst (18).
The ﬁrst agent includes θ2 and τ2 in its state signal, and
in this fashion accounts for the second agent’s inﬂuence on
its task. This is visible in Fig. 3(b) around t = 0.8s, when
the ﬁrst link is pushed counterclockwise (‘up’) due to the
negative acceleration in link 2. Agent 1 counters this effect
by accelerating clockwise (’down’). A similar effect is visible
around t = 1s in Fig. 3(a).
The memory and processing time requirements1 of value
iteration for the two learning experiments are summarized in
Table II. Both memory and CPU requirements are more than
an order of magnitude higher for the centralized case. This is
mainly because, as discussed in Section III, in the decentralized
case the two agents were able to disregard state components
that were not essential in solving their task, and thus reduce
the size of their search space.
V. CONCLUSION AND FUTURE RESEARCH
We have pointed out the differences between centralized
and multi-agent cooperative RL, and we have illustrated these
differences on an example involving learning control of a twolink robotic manipulator. The decentralized solution was able
to achieve good performance while using signiﬁcantly less
computational resources than centralized learning.
As can be seen in Table II, the memory (column 2) and
time complexity (column 3) of the solutions scale poorly
with the problem size. The multi-agent RL literature has not
yet focused on the problem of scalability, although solutions
for the centralized case exist (based mainly on generalization
using function approximation to learn the value function). Such
solutions might be extended to the decentralized case.
Another issue is that RL updates assume perfect knowledge
of the task model (for model-based learning, e.g., value iteration (3)), or perfect measurements of the state (for online,
model-free learning, e.g., Q-learning (4)). Such knowledge
is often not available in real life. Studying the robustness
of solutions with respect to imperfect models or imperfect
observations is topic for future research.
ACKNOWLEDGEMENT
This research is ﬁnancially supported by Senter, Ministry
of Economic Affairs of the Netherlands within the BSIK-ICIS
project “Interactive Collaborative Information Systems” (grant
no. BSIK03024).
1The CPU times were recorded on a Centrino Dual Core 1.83 GHz machine
with 1GB of RAM. Value iteration was run on Matlab 7.1 under Windows