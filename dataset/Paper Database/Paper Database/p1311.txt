Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1085–1097
Florence, Italy, July 28 - August 2, 2019. c⃝2019 Association for Computational Linguistics
Generating Natural Language Adversarial Examples
through Probability Weighted Word Saliency
Shuhuai Ren
Huazhong University of Science and Technology
University of California, Los Angeles
shuhuai 
 
Wanxiang Che
School of Computer Science and Technology,
School of Computer Science and Technology,
Huazhong University of Science and Technology
Harbin Institute of Technology
 
 
We address the problem of adversarial attacks
on text classiﬁcation, which is rarely studied
comparing to attacks on image classiﬁcation.
The challenge of this task is to generate adversarial examples that maintain lexical correctness, grammatical correctness and semantic similarity. Based on the synonyms substitution strategy, we introduce a new word replacement order determined by both the word
saliency and the classiﬁcation probability, and
propose a greedy algorithm called probability
weighted word saliency (PWWS) for text adversarial attack. Experiments on three popular
datasets using convolutional as well as LSTM
models show that PWWS reduces the classiﬁcation accuracy to the most extent, and keeps
a very low word substitution rate. A human
evaluation study shows that our generated adversarial examples maintain the semantic similarity well and are hard for humans to perceive.
Performing adversarial training using our perturbed datasets improves the robustness of the
models. At last, our method also exhibits a
good transferability on the generated adversarial examples.
Introduction
Deep neural networks (DNNs) have exhibited vulnerability to adversarial examples primarily for
image classiﬁcation . Adversarial examples are input data that are artiﬁcially
modiﬁed to cause mistakes in models. For image
classiﬁcations, the researchers have proposed various methods to add small perturbations on images that are imperceptible to humans but can
cause misclassiﬁcation in DNN classiﬁers. Due to
the variety of key applications of DNNs in computer vision, the security issue raised by adversarial examples has attracted much attention in liter-
∗Corresponding author.
atures since 2014, and numerous approaches have
been proposed for either attack , as well as defense .
In the area of Natural Language Processing
(NLP), there is only a few lines of works done
recently that address adversarial attacks for NLP
 . This may be due to
the difﬁculty that words in sentences are discrete
tokens, while the image space is continuous to perform gradient descent related attacks or defnses. It
is also hard in human’s perception to make sense
of the texts with perturbations while for images
minor changes on pixels still yield a meaningful
image for human eyes. Meanwhile, the existence
of adversarial examples for NLP tasks, such as
span ﬁltering, fake news detection, sentiment analysis, etc., raises concerns on signiﬁcant security
issues in their applications.
In this work, we focus on the problem of generating valid adversarial examples for text classiﬁcation, which could inspire more works for NLP
attack and defense.
In the area of NLP, as the
input feature space is usually the word embedding space, it is hard to map a perturbed vector
in the feature space to a valid word in the vocabulary. Thus, methods of generating adversarial examples in the image ﬁeld can not be directly
transferred to NLP attacks. The general approach,
then, is to modify the original samples in the word
level or in the character level to achieve adversarial attacks .
We focus on the text adversarial example generation that could guarantee the lexical correctness with little grammatical error and semantic
In this way, it achieves “small per-
turbation” as the changes will be hard for humans to perceive. We introduce a new synonym
replacement method called Probability Weighted
Word Saliency (PWWS) that considers the word
saliency as well as the classiﬁcation probability.
The change value of the classiﬁcation probability
is used to measure the attack effect of the proposed substitute word, while word saliency shows
how well the original word affects the classiﬁcation. The change value of the classiﬁcation probability weighted by word saliency determines the
ﬁnal substitute word and replacement order.
Extensive experiments on three popular datasets
using convolutional as well as LSTM models
demonstrate a good attack effect of PWWS. It reduces the accuracy of the DNN classiﬁers by up to
84.03%, outperforms existing text attacking methods. Meanwhile, PWWS has a much lower word
substitution rate and exhibits a good transferability. We also do a human evaluation to show that
our perturbations are hard for humans to perceive.
In the end, we demonstrate that adversarial training using our generated examples can help improve robustness of the text classiﬁcation models.
Related Work
We ﬁrst provide a brief review on related works
for attacking text classiﬁcation models.
Liang et al. propose to ﬁnd appropriate words for insertion, deletion and replacement
by calculating the word frequency and the highest
gradient magnitude of the cost function. But their
method involves considerable human participation
in crafting the adversarial examples. To maintain
semantic similarity and avoid human detection, it
requires human efforts such as searching related
facts online for insertion.
Therefore,
subsequent research are mainly
based on the word substitution strategy so as to
avoid artiﬁcial fabrications and achieve automatic
generations. The key difference of these subsequent methods is on how they generate substitute words. Samanta and Mehta propose
to build a candidate pool that includes synonyms,
typos and genre speciﬁc keywords. They adopt
Fast Gradient Sign Method (FGSM) to pick a candidate word for replacement.
Papernot et al. perturb a word
vector by calculating forward derivative and map the perturbed word vector to
a closest word in the word embedding space. Yang
et al. derive two methods, Greedy Attack
based on perturbation, and Gumbel Attack based
on scalable learning.
Aiming to restore the interpretability of adversarial attacks based on word
substitution strategy, Sato et al. restrict the
direction of perturbations towards existing words
in the input embedding space.
As the above methods all need to calculate the
gradient with access to the model structure, model
parameters, and the feature set of the inputs, they
are classiﬁed as white-box attacks. To achieve attack under a black-box setting, which assumes no
access to the details of the model or the feature
representation of the inputs, Alzantot et al. 
propose to use a population-based optimization algorithm. Gao et al. present a DeepWord-
Bug algorithm to generate small perturbations in
the character-level for black-box attack. They sort
the tokens based on the importance evaluated by
four functions, and make random token transformations such as substitution and deletion with the
constraint of edit distance. Ebrahimi et al. 
also propose a token transformation method, and
it is based on the gradients of the one-hot input
vectors. The downside of the character-level perturbations is that they usually lead to lexical errors,
which hurts the readability and can easily be perceived by humans.
The related works have achieved good results
for text adversarial attacks, but there is still much
room for improvement regarding the percentage of
modiﬁcations, attacking success rate, maintenance
on lexical as well as grammatical correctness and
semantic similarity, etc. Based on the synonyms
substitution strategy, we propose a novel blackbox attack method called PWWS for the NLP classiﬁcation tasks and contribute to the ﬁeld of adversarial machine learning.
Text Classiﬁcation Attack
Given an input feature space X containing all possible input texts (in vector form x) and an output
space Y = {y1, y2, . . . , yK} containing K possible labels of x, the classiﬁer F needs to learn a
mapping f : X →Y from an input sample x ∈X
to a correct label ytrue ∈Y. In the following, we
ﬁrst give a deﬁnition of adversarial example for
natural language classiﬁcation, and then introduce
our word substitution strategy.
Text Adversarial Examples
Given a trained natural language classiﬁer F,
which can correctly classify the original input text
x to the label ytrue based on the maximum posterior probability.
yi∈Y P(yi|x) = ytrue.
We attack the classiﬁer by adding an imperceptible
perturbation ∆x to x to craft an adversarial example x∗, for which F is expected to give a wrong
yi∈Y P(yi|x∗) ̸= ytrue.
Eq. (2) gives the deﬁnition of the adversarial example x∗:
x∗= x + ∆x,
∥∆x∥p < ϵ,
yi∈Y P(yi|x∗) ̸= arg max
yi∈Y P(yi|x).
The original input text can be expressed as x =
w1w2 . . . wi . . . wn, where wi ∈D is a word and D
is a dictionary of words. ∥∆x∥p deﬁned in Eq. (3)
uses p-norm to represent the constraint on perturbation ∆x, and L∞, L2 and L0 are commonly
To make the perturbation small enough so that it
is imperceptible to humans, the adversarial examples need to satisfy lexical, grammatical, and semantic constraints. Lexical constraint requires that
the correct word in the input sample cannot be
changed to a common misspelled word, as a spell
check before the input of the classiﬁer can easily
remove such perturbation. The perturbed samples,
moreover, must be grammatically correct. Third,
the modiﬁcation on the original samples should
not lead to signiﬁcant changes in semantics as the
semantic constraint requires.
To meet the above constraints, we replace words
in the input texts with synonyms and replace
named entities (NEs) with similar NEs to generate
adversarial samples. Synonyms for each word can
be found in WordNet1, a large lexical database for
the English language. NE refers to an entity that
has a speciﬁc meaning in the sample text, such as
a person’s name, a location, an organization, or a
proper noun. Replacement of an NE with a similar NE imposes a slight change in semantics but
invokes no lexical or grammatical changes.
The candidate NE for replacement is picked in
1 
the following.
Assuming that the current input
sample belongs to the class ytrue and dictionary
Dytrue ⊆D contains all NEs that appear in the
texts with class ytrue, we can use the most frequently occurring named entity NEadv in the complement dictionary D−Dytrue as a substitute word.
In addition, the substitute NEadv must have the
consistent type with the original NE, e.g., they
must be both locations.
Word Substitution by PWWS
In this work, we propose a new text attacking method called Probability Weighted Word
Saliency (PWWS). Our approach is based on synonym replacement, and there are two key issues
that we resolve in the greedy PWWS algorithm:
the selection of synonyms or NEs and the decision
of the replacement order.
Word Substitution Strategy
For each word wi in x, we use WordNet to build
a synonym set Li ⊆D that contains all synonyms
of wi. If wi is an NE, we ﬁnd NEadv which has
a consistent type of wi to join Li. Then, every
i ∈Li is a candidate word for substitution of
the original wi. We select a w′
i from Li as the
proposed substitute word w∗
i if it causes the most
signiﬁcant change in the classiﬁcation probability
after replacement. The substitute word selection
method R(wi, Li) is deﬁned as follows:
i = R(wi, Li)
P(ytrue|x) −P(ytrue|x′
x = w1w2 . . . wi . . . wn,
i = w1w2 . . . w′
i . . . wn,
i is the text obtained by replacing wi with
each candidate word w′
i ∈Li. Then we replace wi
i and get a new text x∗
i = w1w2 . . . w∗
i . . . wn.
The change in classiﬁcation probability between x and x∗
i represents the best attack effect
that can be achieved after replacing wi.
i = P(ytrue|x) −P(ytrue|x∗
For each word wi ∈x, we ﬁnd the corresponding
substitute word w∗
i by Eq. (4), which solves the
ﬁrst key issue in PWWS.
Replacement Order Strategy
Furthermore, in the text classiﬁcation tasks, each
word in the input sample may have different level
of impact on the ﬁnal classiﬁcation. Thus, we incorporate word saliency into
our algorithm to determine the replacement order.
Word saliency refers to the degree of change in the
output probability of the classiﬁer if a word is set
to unknown (out of vocabulary). The saliency of a
word is computed as S(x, wi).
S(x, wi) = P(ytrue|x) −P(ytrue|ˆxi)
x = w1w2 . . . wi . . . wd,
ˆxi = w1w2 . . . unknown . . . wd.
We calculate the word saliency S(x, wi) for all
wi ∈x to obtain a saliency vector S(x) for text x.
To determine the priority of words for replacement, we need to consider the degree of change in
the classiﬁcation probability after substitution as
well as the word saliency for each word. Thus, we
score each proposed substitute word w∗
i by evaluating the ∆P ∗
i in Eq. (5) and ith value of S(x).
The score function H(x, x∗
i , wi) is deﬁned as:
i , wi) = φ(S(x))i · ∆P ∗
where φ(z)i is the softmax function
z in Eq. (8) is a vector. zi and φ(z)i indicate the
ith component of vector z and φ(z), respectively.
φ(S(x)) in Eq. (7) indicates a softmax operation
on word saliency vector S(x) and K = |S(x)|.
Eq. (7) deﬁned by probability weighted word
saliency determines the replacement order.
sort all the words wi in x in descending order
based on H(x, x∗
i , wi), then consider each word
wi under this order and select the proposed substitute word w∗
i for wi to be replaced. We greedily iterate through the process until enough words have
been replaced to make the ﬁnal classiﬁcation label
The ﬁnal PWWS Algorithm is as shown in Algorithm 1.
Empirical Evaluation
For empirical evaluation, we compare PWWS
with other attacking methods on three popular
datasets involving four neural network classiﬁcation models.
Algorithm 1 PWWS Algorithm
Input: Sample text x(0) before iteration;
Input: Length of sample text x(0): n = |x(0)|;
Input: Classiﬁer F;
Output: Adversarial example x(i)
1: for all i = 1 to n do
Compute word saliency S(x(0), wi)
Get a synonym set Li for wi
if wi is an NE then Li = Li ∪{NEadv}
if Li = ∅then continue
i = R(wi, Li);
9: end for
10: Reorder wi such that
1, w1) > · · · > H(x, x∗
12: for all i = 1 to n do
Replace wi in x(i−1) with w∗
i to craft x(i)
if F(x(i)) ̸= F(x(0)) then break
16: end for
Table 1 lists the details of the datasets, IMDB,
AG’s News, and Yahoo! Answers.
IMDB. IMDB is a large movie review dataset
consisting of 25,000 training samples and 25,000
test samples, labeled as positive or negative. We
use this dataset to train a word-based CNN model
and a Bi-directional LSTM network for sentiment
classiﬁcation .
AG’s News. This is a collection of more than
one million news articles, which can be categorized into four classes: World, Sports, Business
and Sci/Tech. Each class contains 30,000 training
samples and 1,900 testing samples.
This dataset consists of
ten topic categories: Society & Culture, Science
& Mathematics, Health, Education & Reference,
Computers & Internet, etc. Each category contains
140,000 training samples and 5,000 test samples.
Deep Neural Models
For deep neural models, we consider several classic as well as state-of-the-art models used for text
classiﬁcation. These models include both convolutional neural networks (CNN) and recurrent neural networks (RNN), for word-level or characterlevel data processing.
#Train samples
#Test samples
#Average words
IMDB Review
Sentiment analysis
News categorization
Yahoo! Answers
Topic classiﬁcation
Table 1: Statistics on the datasets. “#Average words” indicates the average number of words per sample text.
Word-based CNN consists of an
embedding layer that performs 50-dimensional
word embeddings on 400-dimensional input vectors, an 1D-convolutional layer consisting of 250
ﬁlters of kernel size 3, an 1D-max-pooling layer,
and two fully connected layers. This word-based
classiﬁcation model is used on all three datasets.
Bi-directional
128dimensional embedding layer, a Bi-directional
LSTM layer whose forward and reverse are respectively composed of 64 LSTM units, and a
fully connected layer. This word-based classiﬁcation model is used on IMDB dataset.
Char-based CNN is identical to the structure
in which includes two ConvNets. The two networks are both 9 layers deep
with 6 convolutional layers and 3 fully-connected
This char-based classiﬁcation model is
used for AG’s News dataset.
LSTM consists of a 100-dimensional embedding layer, an LSTM layer composed of 128 units,
and a fully connected layer.
This word-based
classiﬁcation model is used for Yahoo! Answers
Column 3 in Table 2 demonstrates the classiﬁcation accuracies of these models on original
(clean) examples, which almost achieves the best
results of the classiﬁcation task on these datasets.
Attacking Methods
We compare our PWWS 2 attacking method with
the following baselines.
All the baselines use
WordNet to build the candidate synonym sets L.
Random. We randomly select a synonym for
each word in the original input text to replace, and
keep performing such replacement until the classiﬁcation output changes.
FGSM ,
previously proposed for image adversarial attack:
x∗= x + ∆x
= x + ϵ · sign (∇xJ (F, ytrue)) ,
2 
where J (F, ytrue) is the cost function used for
training the neural network.
For the sake of calculation, we will use the synonym that maximizes the change of prediction output ∆F(x) as the substitute word, where ∆F(x)
is approximated by forward derivative:
This method using Eq. (10) is the main concept
introduced in .
Traversing in word order (TiWO). This
method of traversing input sample text in word order ﬁnds substitute for each word according to Eq.
Word Saliency (WS). WS sorts words in the input text based
on word saliency in Eq. (6) in descending order,
and ﬁnds substitute for each word according to Eq.
Attacking Results
We evaluate the merits of all above methods by
using them to generate 2,000 adversarial examples respectively. The more effective the attacking method is, the more the classiﬁcation accuracy
of the model drops. Table 2 shows the classiﬁcation accuracy of different models on the original
samples and the adversarial samples generated by
these attack methods.
Results show that our method reduces the classiﬁcation accuracies to the most extent. The classiﬁcation accuracies on the three datasets IMDB,
AG’s News, and Yahoo! Answers are reduced by
an average of 81.05%, 33.62%, and 38.65% respectively. The effectiveness of the attack against
multi-classiﬁcation tasks is not as good as that for
binary classiﬁcation tasks.
Our method achieves such effects by very few
word replacements. Table 3 lists the word replacement rates of the adversarial examples generated
by different methods. The rate refers to the number of substitute words divided by the total number
of words in the original clean sample texts. It indicates that PWWS replaces the fewest words while
Bi-dir LSTM
Yahoo! Answers
Table 2: Classiﬁcation accuracy of each selected model on the original three datasets and the perturbed datasets
using different attacking methods. Column 3 (Original) represents the classiﬁcation accuracy of the model for the
original samples. A lower classiﬁcation accuracy corresponds to a more effective attacking method.
Bi-dir LSTM
Yahoo! Answers
Table 3: Word replacement rate of each attacking method on the selected models for the three datasets. The lower
the word replacement rate, the better the attacking method could be in terms of retaining the semantics of the text.
Original Prediction
Adversarial Prediction
Perturbed Texts
Ah man this movie was funny (laughable) as hell, yet strange. I like
how they kept the shakespearian language in this movie, it just felt
ironic because of how idiotic the movie really was. this movie has got
to be one of troma’s best movies. highly recommended for some
senseless fun!
Conﬁdence = 96.72%
Conﬁdence = 74.78%
The One and the Only! The only really good description of the punk
movement in the LA in the early 80’s. Also, the deﬁnitive documentary
about legendary bands like the Black Flag and the X. Mainstream
Americans’ repugnant views about this ﬁlm are absolutely hilarious
(uproarious)! How can music be SO diversive in a country of
supposed liberty...even 20 years after... ﬁnd out!
Conﬁdence = 72.40%
Conﬁdence = 69.03%
Table 4: Adversarial example instances in the IMDB dataset with Bi-directional LSTM model. Columns 1 and
2 represent the category prediction and conﬁdence of the classiﬁcation model for the original sample and the
adversarial examples, respectively. In column 3, the green word is the word in the original text, while the red is the
substitution in the adversarial example.
Original Prediction
Adversarial Prediction
Perturbed Texts
site security gets a recount at rock the vote. grassroots movement to
register younger voters leaves publishing (publication) tools accessible
to outsiders.
Conﬁdence = 91.26%
Conﬁdence = 33.81%
seoul allies calm on nuclear (atomic) shock. south korea’s key allies
play down a shock admission its scientists experimented to enrich
Conﬁdence = 74.25%
Conﬁdence = 86.66%
Table 5: Adversarial example instances in the AG’s News dataset with char-based CNN model. Columns of this
table is similar to those in Table 4.
ensuring the semantic and syntactic features of the
original sample remain unchanged to the utmost
Table 4 lists some adversarial examples generated for IMDB dataset with the Bi-directional
LSTM classiﬁer.
The original positive/negative
ﬁlm reviews can be misclassiﬁed by only one synonym replacement and the model even holds a
high degree of conﬁdence. Table 5 lists some adversarial examples in AG’s News dataset with the
char-based CNN. It also requires only one synonym to be replaced for the model to be misled to
classify one type (Business) of news into another
(Sci/Tech). The adversarial examples still convey
the semantics of the original text such that humans
do not recognize any change but the neural network classiﬁers are deceived.
For more example comparisons between the ad-
Accuracy of model
Accuracy of human
Score 
Adversarial
Bi-dir LSTM
Adversarial
Adversarial
Table 6: Comparison with human evaluation. The fourth and ﬁfth columns represent the classiﬁcation accuracy of
the model and human, respectively. The last column represents how much the workers think the text is likely to be
modiﬁed by a machine. The larger the score, the higher the probability.
versarial examples generated by different methods, see details in Appendix.
Text classiﬁer based on DNNs is widely used in
NLP tasks. However, the existence of such adversarial samples exposes the vulnerability of these
models, limiting their applications in securitycritical systems like spam ﬁltering and fake news
detection.
Discussions on Previous Works
Yang et al. introduce a perturbationbased method called Greedy Attack and a scalable learning-based method called Gumbel Attack. They perform experiments on IMDB dataset
with the same word-based CNN model, and on
AG’s News dataset with a LSTM model. Their
method greatly reduces the classiﬁcation accuracy
to less than 5% after replacing 5 words . However, the semantics of the replacement words are not constrained, as antonyms
sometimes appear in their adversarial examples.
Moreover, for instance, Table 3 in shows that they change “... The plot could
give a rise a must (better) movie if the right pieces
was in the right places” to switch from negative to
positive; and they change “The premise is good,
the plot line script (interesting) and the screenplay
was OK” to switch from positive to negative. The
ﬁrst sample changes the meaning of the sentence,
while the second has grammatical errors. Under
such condition, the perturbations could be recognized by humans.
Gao et al. present a novel algorithm,
DeepWordBug, that generates small text perturbations in the character-level for black-box attack.
This method can cause a decrease of 68% on average for word-LSTM and 48% on average for
char-CNN model when 30 edit operations were allowed. However, since their perturbation exists in
the character-level, the generated adversarial examples often do not conform to the lexical constraint: misspelled words may exist in the text. For
instance, they change a positive review of “This
ﬁlm has a special place in my heart” to get a negative review of “This ﬁlm has a special plcae in
my herat”. For such adversarial examples, a spell
check on the input can easily remove the perturbation, and the effectiveness of such adversarial
attack will be removed also.
DeepWordBug is
still useful, as we could improve the robustness in
the training of classiﬁers by replacing misspelled
word with out-of-vocabulary word, or simply remove misspelled words. However, as DeepWord-
Bug can be easily defended by spell checking, we
did not consider it as a baseline in our comparison.
Further Analysis
This section provides a human evaluation to show
that our perturbation is hard for humans to perceive, and studies the transferability of the generated examples by our methods. In the end, we
show that using the generated examples for adversarial training helps improving the robustness of
the text classiﬁcation model.
Human Evaluation
To further verify that the perturbations in the adversarial examples are hard for humans to recognize, we ﬁnd six workers on Amazon Mechanical Turk to evaluate the examples generated by
PWWS. Speciﬁcally, we select 100 clean texts in
IMDB and the corresponding adversarial examples generated on word-based CNN. Then we select another 100 clean texts in IMDB and the corresponding adversarial examples generated on Bidirectional LSTM. For the third group, we select
100 clean texts from AG’s News and the corresponding adversarial examples generated on charbased CNN. For each group of date, we mix the
clean data and generated examples for the workers to classify. To evaluate the similarity, we ask
the workers to give scores from 1-5 to indicate the
likelihood that the text is modiﬁed by machine.
(a) Varying word replacement rates of the algorithms
(b) Fixed word replacement rate of 10%
Figure 1: Transferability of adversarial examples generated by different attacking methods on IMDB. The three
color bars represent the average classiﬁcation accuracies (in percentage) of the three new models on the adversarial
examples generated by word-based CNN-1. The lower the classiﬁcation accuracy, the better the transferability.
Table 6 shows the comparison with human evaluation. The generated examples can cause misclassiﬁcation on three different models, while the
classiﬁcation accuracy of humans is still very high
comparing to their judgement on clean data. Since
there are four categories for AG’s News, the classi-
ﬁcation accuracy of workers on this dataset is signiﬁcantly lower than that on IMDB (binary classiﬁcation tasks).
Thus, we did not try human
evaluation on Yahoo!
Answers as there are 10
categories to classify.
The likelihood scores of
machine perturbation on adversarial examples are
slightly higher than that on the original texts, indicating that the semantics of some synonyms are
not as accurate as the original words. Nevertheless, as the accuracy of humans on the two sets of
data are close, and the traces of machine modiﬁcations are still hard for humans to perceive.
Transferability
The transferability of adversarial attack refers to
its ability to reduce the accuracy of other models
to a certain extent when the examples are generated on a speciﬁc classiﬁcation model .
To illustrate this, we record the original wordbased CNN (described in Section 4.2) as wordbased CNN-1, and train three new proximity classiﬁcation models on the IMDB dataset, labeled
respectively as word-based CNN-2, word-based
CNN-3 and Bi-directional LSTM network. Compared to word-based CNN-1, word-based CNN-
2 has an additional fully connected layer. Wordbased CNN-3 has the same network structure as
CNN-1 except using GloVe as a pretrained word embedding. The network structure of Bi-directional LSTM is the one
introduced in Section 4.2.
When the adversarial examples generated by
our method are transferred to word-based CNN-
2 or Bi-dir LSTM, the attacking effect is slightly
inferior, as illustrated in Figure 1 (a). But note
that the word replacement rate of our method on
IMDB is only 3.81%, which is much lower than
other methods (Table 3). When we use the same
replacement ratio (say 10%) in the input text for
all methods, the transferability of PWWS is signiﬁcantly better than other methods. Figure 1 (b)
illustrates that the word substitution order determined by PWWS corresponds well to the importance of the words for classiﬁcation, and the transformation is effective across various models.
Adversarial Training
Adversarial training is
a popular technique mainly used in image classi-
ﬁcation to improve model robustness. To verify
whether incorporating adversarial training would
help improve the robustness of the test classiﬁers,
we randomly select clean samples from the training set of IMDB and use PWWS to generate 4000
adversarial examples as a set A, and train the
word-based CNN model.
We then evaluate the
classiﬁcation accuracy of the model on the original
test data and of the adversarial examples generated
using various methods. Figure 2 (a) shows that the
classiﬁcation accuracy of the model on the original
test set is improved after adversarial training. Figure 2 (a) illustrates that the robustness of the classiﬁcation model continues to improve when more
adversarial examples are added to the training set.
(a) Accuracy on the original test set
(b) Accuracy on the adversarial examples generated by various methods
Figure 2: The result of adversarial training on IMDB dataset. The x-axis represents the number of adversarial
examples selected from set A to join the original training set. The classiﬁcation accuracies are on the original test
set and the adversarial examples generated using various methods, respectively.
Conclusion
We propose an effective method called Probability
Weighted Word Saliency (PWWS) for generating
adversarial examples on text classiﬁcation tasks.
PWWS introduces a new word substitution order
determined by the word saliency and weighted by
the classiﬁcation probability. Experiments show
that PWWS can greatly reduce the text classiﬁcation accuracy with a low word substitution rate,
and such perturbation is hard for human to perceive.
Our work demonstrates the existence of adversarial examples in discrete input spaces and shows
the vulnerability of NLP models using neural networks. Comparison with existing baselines shows
the advantage of our method. PWWS also exhibits
a good transferability, and by performing adversarial training we can improve the robustness of
the models at test time. In the future, we would
like to evaluate the attacking effectiveness and ef-
ﬁciency of our methods on more datasets and models, and do elaborate human evaluation on the similarity between clean texts and the corresponding
adversarial examples.