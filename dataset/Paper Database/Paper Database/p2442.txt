Communications in
Mathematics and
Computational
mathematical sciences publishers
IMPLICIT PARTICLE FILTERS FOR DATA
ASSIMILATION
ALEXANDRE CHORIN,
MATTHIAS MORZFELD AND XUEMIN TU
COMM. APP. MATH. AND COMP. SCI.
Vol. 5, No. 2, 2010
IMPLICIT PARTICLE FILTERS FOR DATA ASSIMILATION
ALEXANDRE CHORIN, MATTHIAS MORZFELD AND XUEMIN TU
Implicit particle ﬁlters for data assimilation update the particles by ﬁrst choosing
probabilities and then looking for particle locations that assume them, guiding
the particles one by one to the high probability domain. We provide a detailed
description of these ﬁlters, with illustrative examples, together with new, more
general, methods for solving the algebraic equations and with a new algorithm
for parameter identiﬁcation.
1. Introduction
There are many problems in science, for example in meteorology and economics,
in which the state of a system must be identiﬁed from an uncertain equation
supplemented by noisy data (see, for instance, [9; 22]). A natural model of this
situation consists of an Ito stochastic differential equation (SDE):
dx = f (x, t) dt + g(x, t) dw,
where x = (x1, x2, . . . , xm) is an m-dimensional vector, f is an m-dimensional
vector function, g(x, t) is an m by m matrix, and w is Brownian motion which
encapsulates all the uncertainty in the model. In the present paper we assume for
simplicity that the matrix g(x, t) is diagonal. The initial state x(0) is given and
may be random as well.
The SDE is supplemented by measurements bn at times tn, n = 0, 1, . . . . The
measurements are related to the state x(t) by
bn = h(xn) + GW n,
where h is a k-dimensional, generally nonlinear, vector function with k ≤m, G
is a matrix, xn = x(tn), and W n is a vector whose components are independent
Gaussian variables of mean 0 and variance 1, independent also of the Brownian
motion in (1). The independence requirements can be greatly relaxed but will be
MSC2000: 60G35, 62M20.
Keywords: implicit sampling, data assimilation, particle ﬁlter.
This work was supported in part by the Director, Ofﬁce of Science, Computational and Technology
Research, United States Department of Energy under Contract no. DE-AC02-05CH11231, and by the
National Science Foundation under grants DMS-0705910 and OCE-0934298.
ALEXANDRE CHORIN, MATTHIAS MORZFELD AND XUEMIN TU
observed in the present paper. The task of a ﬁlter is to assimilate the data, that is,
estimate x on the basis of both (1) and the observations (2).
If the system (1) and the function h in (2) are linear and the data are Gaussian, the
solution can be found in principle via the Kalman–Bucy ﬁlter . In the general
case, one often estimates x as a statistic (often the mean) of a probability density
function (pdf) evolving under the combined effect of (1) and (2). The initial state
x0 being known, all one has to do is evaluate sequentially the pdfs Pn+1 of the
variables xn+1 given the equations and the data. In a “particle” ﬁlter this is done by
following “particles” (replicas of the system) whose empirical distribution at time
tn approximates Pn. One may for example [1; 9; 7; 8; 19] use the pdf Pn and (1)
to generate a prior density (in the sense of Bayes) , and then use the data bn+1 to
generate sampling weights which deﬁne a posterior density Pn+1. This can be very
expensive because in most weighting schemes, most of the weights tend to zero
fast and the number of particles needed can grow catastrophically [21; 2]; various
strategies have been proposed to ameliorate this problem.
Our remedy is implicit sampling [4; 5]. The number of particles needed in a ﬁlter
remains moderate if one can ﬁnd high probability particles; to this end, implicit
sampling works by ﬁrst picking probabilities and then looking for particles that
assume them, so that the particles are guided efﬁciently to the high probability
region one at a time, without needing a global guess of the target density. In
the present paper we provide an expository account of particle ﬁlters, separating
clearly the general principles from details of implementation; we provide general
solution algorithms for the resulting algebraic equations, in particular for nonconvex
cases which we had not considered in our previous publications, as well as a new
algorithm for parameter identiﬁcation based on an implicit ﬁlter. We also provide
examples, in particular of nonconvex problems.
Implicit ﬁlters are a special case of chainless sampling methods ; a key
connection was made in [23; 24], where it was observed that in the sampling of
stochastic differential equations, the marginals needed in Markov ﬁeld sampling
can be read off the equations and need not be estimated numerically.
2. The mathematical framework
The ﬁrst thing we do is discretize the SDE (1) by a difference scheme, so that the
equation becomes a discrete recurrence, and assume temporarily that the time step
in the dynamics equals the ﬁxed time δ between observations. For simplicity, in
this section we assume the scheme is the Euler scheme
xn+1 = xn + δf (xn, nδ) + V,
where V is a Gaussian of mean zero and variance g2(xn, nδ)δ. Higher-order schemes
IMPLICIT PARTICLE FILTERS FOR DATA ASSIMILATION
are discussed in Section 4.
The conditional probability densities Pn(x) at times tn, determined by the discretized SDE (3) given the observations (2), satisfy the recurrence relation [9,
P(xn+1) = P(xn)P(xn+1|xn)P(bn+1|xn+1)/Z,
where P(xn) = P(xn|b1, b2, . . . , bn) is the probability density at time nδ given
the data up to time nδ, P(xn+1|xn) is the probability density of xn+1 given xn
as it is determined by the dynamics, P(bn+1|xn+1) is the probability of the next
observation given the new position, as per the observation equation, and Z is a
normalization constant.
We estimate Pn+1 with the help of M particles, with positions Xn
i at time tn and
at time tn+1 (i = 1, . . . , M), which deﬁne empirical densities ˆPn, ˆPn+1 that
approximate Pn, Pn+1. We do this by requiring that, when a particle moves from
, the probability of Xn+1
given bk for k ≤n + 1 be given by
i )P(bn+1|Xn+1
where the hats have been omitted as they will be from now on, P(Xn
i ), the probability
i given bk for k ≤n, is assumed given, P(Xn+1
i ), the probability of Xn+1
i , is determined by the discretized SDE (3), P(bn+1|Xn+1
), the probability
of the observations bn+1 given the new positions Xn+1
, is determined by the
observation equaiton (2), and Z0 is an unknown normalization constant. We shall
see below that one can set P(Xn
i ) = 1 without loss of generality.
Equation (5) deﬁnes the pdf we now need to sample for each particle. One
way to do this is to pick a position Xn+1
according to some prior guess of Pn+1,
and then use weights to get the resulting pdf to agree with the true Pn+1 (the
“posterior” density); in general many of the new positions will have low probability
and therefore small weights. The idea in implicit sampling is to deﬁne probabilities
ﬁrst, and then look for particles that assume them; this is done by choosing once and
for all a ﬁxed reference random variable, say ξ, with a given pdf, say a Gaussian
exp(−ξ T ξ/2)/(2π)m/2, which one knows how to sample, and then making Xn+1
a function of ξ, a different function of each particle and each step, each function
designed so that the map ξ →Xn+1
connects highly probable values of ξ to highly
probable values of Xn+1
. To that end, write
i )P(bn+1|Xn+1
) = exp(−Fi(X)),
where on the right-hand side X is a shorthand for Xn+1
and all the other arguments
are omitted. This deﬁnes a function Fi for each particle i and each time tn. For each
i and n, Fi is an explicitly known function of X = Xn+1
. Then solve the equation
Fi(X) −φi = ξ T ξ/2,
ALEXANDRE CHORIN, MATTHIAS MORZFELD AND XUEMIN TU
where ξ is a sample of the ﬁxed reference variable and φi is an additive factor
needed to make the equation solvable. The need for φi becomes obvious if one
considers the case of a linear observation function h in (2), so that the right side
of (6) is quadratic but the left is a quadratic plus a constant. It is clear that setting
φi = min Fi will do the job, but it is sometimes convenient to perturb this choice
by a small amount (see below). In addition, and most important, with our choice
of reference variable ξ, the most likely choice of ξ is in the neighborhood of 0; if
the mapping ξ →X satisﬁes (6), this likely choice of ξ produces an X near the
minimum of Fj, hence a high probability position for the particle. We also require
that for each particle, the function Xn+1
= X = X(ξ) deﬁned by (6) be one-to-one
so that the correct pdf is sampled, in particular, it must have distinct branches for
positive values and negative values of each component of ξ. The solution of (6) is
discussed in the next section. From now on we omit the index i in both F and φ,
but it should not be forgotten that these functions vary from particle to particle and
from one time step to the next.
Once the function X = X(ξ) is determined, each value of Xn+1 = X (the subscript
i is omitted) appears with probability exp(−ξ T ξ/2)J −1/(2π)m/2, where J is the
Jacobian of the map X = X(ξ), while the product P(Xn+1|Xn)P(bn+1|Xn+1)
evaluated at Xn+1 equals exp(−ξ T ξ/2) exp(−φ). The sampling weight for the
particle is therefore exp(−φ)J(2π)m/2. If the map ξ →X is smooth near ξ = 0,
so that φ and J do not vary rapidly from particle to particle, and if there is an easy
way to compute J (see the next section), then we have an effective way to sample
Pn+1 given Pn. It is important to note that though the functions F and φ vary from
particle to particle, the probabilities of the various samples are expressed in terms
of the ﬁxed reference pdf, so that they can be compared with each other.
The weights can be eliminated by resampling. A standard resampling algorithm goes as follows : let the weight of the i-th particle be Wi, i = 1, . . . , M.
Deﬁne A = P Wi; for each of M random numbers θk, k = 1, . . . , M, drawn
from the uniform distribution on , choose a new bXn+1
j=1 W j < θk ≤A−1 Pi
j=1 W j, and then suppress the hat. This justiﬁes the
statement following (5) that one can set P(Xn) = 1.
To see what has been gained, compare our construction with the usual “Bayesian”
particle ﬁlter, where one samples P(Xn+1|Xn)P(bn+1|Xn+1) by ﬁrst ﬁnding a
“prior” density Q(Xn+1) (omitting all arguments other than Xn+1), such that the
ratio W = P(Xn+1)/Q(Xn+1) is close to a constant, and then assigning to the i-th
particle the importance weight W = Wi evaluated at the location of the particle. The
pdf deﬁned by the set of positions and weights is the density Pn+1 we are looking
for. An important special case is the choice Q(Xn+1) = P(Xn+1|Xn); the prior
is then deﬁned by the equation of motion alone and the posterior is obtained by
using the observations to weight the particles. We shall refer to this special case as
IMPLICIT PARTICLE FILTERS FOR DATA ASSIMILATION
“standard importance sampling” or “standard ﬁlter”. Of course, once the positions
and the weights of the particles have been determined, one should resample as
The catch in these earlier constructions is that the prior density Q and the desired
posterior can become nearly mutually singular, and the number of particles needed
may become catastrophically large, especially when the number of variables m is
large [2; 21]. To avoid this catch one has to make a good guess for the pdf Q, which
may not be easy because Q should approximate the unknown density Pn+1 one is
looking for — this is the basic conundrum of Monte Carlo methods, in which one
needs a good estimate to get a good estimate. In contrast, in implicit sampling one
does a separate calculation for each sample and there is no need for prior global
information. One can of course still identify the pdf deﬁned by the positions of the
particles at time tn+1 as a “prior” and the pdf deﬁned by both the positions and the
weights as a “posterior” density.
Note that one can recover standard importance sampling within our framework
by setting φ = −log P(bn+1|Xn+1), but this choice of course violates our rule for
choosing φ.
Finally, implicit sampling can be viewed as an implicit Monte Carlo scheme for
solving the Zakai equation , which describes the evolution of the unnormalized
conditional distribution for an SDE conditioned by observations. This should be
contrasted with the procedure in the popular ensemble Kalman ﬁlter , where
a Gaussian approximation of the pdf deﬁned by the SDE is extracted from a
Monte Carlo solution of the corresponding Fokker–Planck equation, a Gaussian
approximation is made for the pdf P(bn+1|xn+1) , and new particle positions
are obtained by a Kalman step. Our replacement of the Fokker–Planck equation that
corresponds to the SDE alone by a Zakai equation that describes the evolution of the
unnormalized conditional distribution does away with the need for the approximate
and expensive extraction of Gaussians and consequent Kalman step.
3. Solution of the algebraic equation that deﬁnes a new sample
We now explain how to solve (6), F(X)−φ = ξ T ξ/2, under several sets of assumptions which are met in practice. This is a well-deﬁned, deterministic, algebraic
equation for each particle. Note the great latitude that it provides in linking the
ξ variables to the X variables: it is a single equation that connects 2m variables
(the m components of ξ and the m components of X) and can be satisﬁed by many
maps ξ →X as long as (i) they are one-to-one, (ii) they map the neighborhood of 0
into a set that contains the minimum of F, (iii) they are smooth near ξ = 0 so that
the weights exp(−φ)J not vary unduly from particle to particle in the target area,
and (iv) they allow the Jacobian J to be calculated easily. The solution methods
ALEXANDRE CHORIN, MATTHIAS MORZFELD AND XUEMIN TU
presented here are far from exhaustive; further examples will be presented in the
context of speciﬁc applications.
Algorithm A (presented in [4; 5]). Assume the function F is convex upwards and
h is not too different from a linear function. For each particle, we set up an iteration,
with iterates Xn+1, j, j = 0, 1, . . . , (X j for brevity), with X0 = 0, that converge to
the next position Xn+1 of that particle. The index i that identiﬁes the particle is
omitted again. We write the equations as if the system were one-dimensional; the
multidimensional case was presented in detail in . First we sample the reference
variable ξ. The iteration is deﬁned when one knows how to ﬁnd X j+1 given X j.
Expand the observation function h in (2) around X j:
h(X j+1) = h(X j) + (Dh) j(X j+1 −X j),
where (Dh) j is the derivative of h evaluated at X j. The observation function in (2)
is now approximated as a linear function of X j+1, and the function F is the sum
of two Gaussians in X j+1. Completing a square yields a single Gaussian with a
remainder φ, i.e., F(X) = (x −¯a)2/(2¯v)+φ(X j), where the parameters φ, ¯a, ¯v are
functions of X j (this is what we called in a “pseudo-Gaussian”). The next iterate
is now X j+1 = ¯a +
¯vξ. In the multidimensional case, when each component of
the function h in (2) depends on more than one variable, ﬁnding X j+1 as a function
of ξ may require the solution of an equation of the form (X j+1)T AX j+1 = ξ T ξ/2,
where A is positive deﬁnite and symmetric. This is, as expected, a single equation
for several variables, so that the solution is not unique. We may choose, as in ,
to connect ξ to X j+1 by performing a Choleski decomposition, A = LLT , where
L is lower triangular, and then solving
2LT X j+1 = ξ. A different connection
was presented in . If the iteration converges, it converges to the exact solution
of (6), with φ the limit of the φ(X j). Its convergence can be accelerated by
Aitken’s extrapolation . The Jacobian J can be evaluated either by an implicit
differentiation of (6) or numerically, by perturbing ξ in (6) and solving the perturbed
equation (which should not require more than a single additional iteration step). It
is easy to see that this iteration, when it converges, produces a mapping ξ →X that
is one to one and onto.
An important special case occurs when the observation function h is linear in
X and there are observations at every step (see section 5 for the case of sparse
observations). It is immaterial then whether the SDE (1) is linear. In this case the
iteration converges in one step; the Jacobian J is easy to ﬁnd; if in addition the
function g(x, t) in (1) is independent of x, then J is independent of the particle and
need not be evaluated; the additive term φ can be written explicitly as a function of
the previous position Xn of the particle and of the observation bn+1. We recover an
easy implementation of optimal sequential importance sampling [1; 9; 8].
IMPLICIT PARTICLE FILTERS FOR DATA ASSIMILATION
This iteration has been used in . It may fail to converge if the function F
is not convex, as happens in particular when the observation function h is highly
nonlinear. In the latter case the value of φ it produces may also be far from the
minimum of F. If h is strongly nonlinear, the next iteration is preferable.
Algorithm B. Assume the function F is U-shaped, i.e., in the scalar case, it is at
least piecewise differentiable, F′ vanishes at a single point which is a minimum,
F is strictly decreasing on one side of the minimum and strictly increasing on the
other, with F(X) = ∞when X = ±∞. In the m-dimensional case, assume that
F has a single minimum and that each intersection of the graph of the function
y = F(X) with a vertical plane through the minimum is U-shaped in the scalar
sense (a function may be U-shaped without being convex).
Find z, the minimum of F (this is the minimum of a given real valued function,
not a minimum of a possibly multimodal pdf generated by the SDE; ﬁnding this
minimum is not equivalent to the difﬁcult problem of ﬁnding a maximum likelihood
estimate of the state of the system). The minimum z can be found by standard
minimization algorithms.
Again we are solving the equations by ﬁnding iterates X j that converge to Xn+1.
In the scalar case, given a sample of the reference variable ξ, ﬁnd ﬁrst X0 such that
X0 −z has the sign of ξ, and then ﬁnd the next iterates X j by standard tools (e.g.,
by Newton iteration), modiﬁed so that the X j are prevented from leaping over z.
In the vector case, if the observation function is diagonal — i.e., each component
of the observation is a function of a single component of the solution X — then
the scalar algorithm can be used component by component. In more complicated
situations one can take advantage of the freedom in connecting ξ to X.
Here is an interesting example of the use of this freedom, which we present in
the case of a multidimensional problem where the observation function is linear but
need not be diagonal. Set φ = min F. The function F(X)−φ can now be written as
(X −a)T A(X −a)/2, where a is a known vector, T denotes a transpose as before,
and A is a positive deﬁnite symmetric matrix. Write further y = X −a. Equation
(6) becomes
yT Ay = |ξ|2,
where |ξ| is the length of the vector ξ. Make the ansatz
where λ is a scalar, η = ξ/|ξ| is a random unit vector and ξ is a sample of the
reference density. Substitution into (8) yields
λ2(ηT Aη) = |ξ|2.
ALEXANDRE CHORIN, MATTHIAS MORZFELD AND XUEMIN TU
It is easy to see that E[ηiη j] = δi j/m, where E[ · ] denotes an expected value, the
ηi are the components of η, m is the number of variables, and δi j is the Kronecker
delta, and hence
E[ηT Aη] = trace(A)/m.
Replace (9) by
λ23 = |ξ|2.
where 3=trace(A)/m. This equation has the solution λ=|ξ|/
3, and substitution
into the ansatz leads to yi = ξi/
3, an easily implementable transformation with
Jacobian J = 3−m/2. The difference between (9) and (10) can be compensated
for by adding to φ the term λ2[(ηT Aη) −3]. Note that as m →∞, (ηT Aη) →3
provided A satisﬁes some minor requirements, so that when the number of variables
is sufﬁciently large, the perturbation one has to compensate for becomes negligible.
Detailed implementations and generalizations of this construction will be given
elsewhere in the context of speciﬁc applications.
One can readily devise algorithms also for cases where F is not U-shaped, for
example, by dividing F into monotonic pieces and sampling each of these pieces
with its predetermined probability. An alternative that is usually easier is to replace
the non-U-shaped function F by a suitable U-shaped function F0 and make up for
the bias by adding F(X) −F0(X) to the φ in the weights exp(−φ)J so that (6) is
still satisﬁed. One also has to make sure that the small ξ region is still mapped on
the high probability region for X; see the examples below.
More generally, even in convex cases, one can often change F in (6) to make
the algebraic problem easy without reducing the quality of the samples; examples
will be given in the next two sections.
4. Examples
We now present examples that illustrate the algorithms we have just described. For
more examples, see [4; 5]. For the sake of clarity, in this section we continue to
rely on an Euler discretization of the SDE, as in (3).
We begin with a response to a comment we have often heard: “This is nice, but the
construction will fail the moment you are faced with potentials with multiple wells”.
This is not so: the function F depends on the nature of the noise in the SDE and on
the function h = h(x) in the observation (2), but not on the potential. Consider, for
example, a one dimensional particle moving in the potential V (x) = 2.5(x2 −0.5)2
(see Figure 1) with the force f (x) = −∇V = −10x(x2 −1) and the resulting SDE
dx = f (x)dt +σdw, where σ = 0.1 and w is Brownian motion with unit variance;
with this choice of parameters the SDE has an invariant density concentrated in the
neighborhoods of x = ±√1/2. We consider linear observations bn = x(tn) + W,
where W is a Gaussian variable with mean zero and variance s = 0.025. We
IMPLICIT PARTICLE FILTERS FOR DATA ASSIMILATION
Figure 1. The potential in the ﬁrst example.
approximate the SDE by an Euler scheme with time step δ = 0.01, and assume
observations are available at all the points nδ. The particles all start at x = 0. We
produce data bn by running a single particle and adding to its positions errors drawn
from the assumed error density in (2), and then attempt to reconstruct this path with
our ﬁlter. For the i-th particle located at time nδ at Xn
i the function F(X) is
F(X) = (X −Xn
+ (X −bn+1)2
which is always convex. A completion of the square yields
min F = φ = (Xn
the Jacobian J is independent of the particle and need not be evaluated. In Figure 2
we display a particle run used to generate data and its reconstruction by our ﬁlter
with 50 particles.
This ﬁgure is included for completeness but both of these paths are random,
their difference varies from realization to realization, and may be large or small by
accident. To get a quantitative estimate of the performance of the ﬁlter, we repeated
this calculation 104 times and computed the mean and the variance of the difference
1 between the run that generated the data and its reconstruction at time t = 1; see
Table 1. This table shows that the ﬁlter is unbiased and that the variance of 1 is
comparable to the variance of the error in the observations s = 0.025. Even with
one single particle (and therefore no resampling) the results are still acceptable.
ALEXANDRE CHORIN, MATTHIAS MORZFELD AND XUEMIN TU
Reconstruction
Random path
Figure 2. A random path (black) and its reconstruction by our ﬁlter (gray).
We now discuss the relation between the posterior we wish to sample and the
prior in several special cases, including nonconvex situations. We want to produce
samples of the pdf P(x) = exp(−F(x))/Z, where Z is a normalization constant
2σ + (h(x) −b)2
Here h(x) is a given function of x as in (2), and σ, s, b are given parameters. This
can be viewed as a ﬁrst step in time for a ﬁltering problem where all the particles
start from the same point so that exp(−F(x))/Z = P1, or as an analysis of the
sampling for one particular particle in a general ﬁltering problem, or as an instance
of the more general problem of sampling a given pdf when the important events
may be rare. In standard Bayesian sampling one samples the variable with pdf
exp(−x2/(2σ))
Table 1. Mean and variance of the discrepancy between the observed path and the reconstructed path in Example 1 as a function
of the number of particles M, with s = 0.025.
IMPLICIT PARTICLE FILTERS FOR DATA ASSIMILATION
and then one attaches to the sample at x the weight exp(−(h(x) −b)2/(2s)); in an
implicit sampler one ﬁnds a sample x by solving F(x) −φ = ξ 2/2 for a suitable
φ and ξ and attaching to the sample the weight exp(−φ)J. For given σ, s, the
problem becomes more challenging as |b| increases.
In both the standard and the implicit ﬁlters one can view the empirical pdf
generated by the unweighted samples as a “prior” and the one generated by the
weighted samples as the “posterior”. The difﬁculty with standard ﬁlters is that
the prior and posterior densities may approach being mutually singular, so it is of
interest to estimate the Radon–Nikod´ym derivative of one of these with respect
to the other. If that derivative is a constant, we have achieved perfect importance
sampling, as every neighborhood in the sample space is visited with a frequency
proportional to its density. We estimate the Radon–Nikod´ym derivative of the prior
with respect to the posterior as follows. In this simple problem one can evaluate
the probability of any interval with respect to the posterior we wish to sample by
quadratures. We divide the interval into K pieces of equal lengths 1/K, then
ﬁnd numerically points Y1, Y2, . . . , YK−1, with YK = +∞, such that the posterior
probability of the interval [−∞, Yk] is k/K for k = 1, 2, . . . , K. We then ﬁnd
L = 105 samples of the prior and plot of a histogram of the frequencies with which
these samples fall into the posterior equal probability intervals (Yk−1, Yk). The more
this histogram departs from being a constant independent of k, the more samples
are needed to calculate the statistics of the posterior.
If h(x) is linear, the weights in the implicit ﬁlter are all equal and the histogram
is constant for all values of b. This remains true for all values of b, i.e., however
far the observation b is from what one may expect from the SDE alone. This is not
the case with a standard Bayesian ﬁlter, where some parts of the sample space that
have nonzero probability are visited very rarely.
In Table 2 we list the histogram of frequencies for a linear observation function
h(x) = x and b = 2 in a standard Bayesian ﬁlter, with K = 10. We used 104
samples; the ﬂuctuations in the implicit case measure only the accuracy with which
Table 2. Histogram of the Radon–Nikodým derivative of the prior
with respect to the posterior, standard Bayesian ﬁlter versus the
implicit ﬁlter, 10000 particles, b = 2, σ = s = 0.1, h(x) = x.
ALEXANDRE CHORIN, MATTHIAS MORZFELD AND XUEMIN TU
Table 3. Comparison of the estimates of the means, implicit vs.
standard ﬁlter, 30 particles, together with the exact results, linear
case, as explained in the text.
the histogram is computed with this number of samples. As a consequence, estimates
obtained with the implicit ﬁlter are much more reliable than the ones obtained with
the standard Bayesian ﬁlter.
In Table 3 we list the estimates of the mean position of the linear problem as
a function of b, with 30 particles, σ = s = 0.1, for the standard Bayesian and the
implicit ﬁlters, compared with the exact result. The standard deviations are not
displayed; they are all near 0.01.
The results in this one-dimensional problem mirror the situation with the example
of Bickel et al. [2; 21], designed to display the breakdown of the standard Bayesian
ﬁlter when the number of dimension is large; what happens there is that one
particle hogs almost the whole weight, so that the number of particles needed
grows catastrophically; in contrast, the implicit ﬁlter assigns equal weights to all
the particles in any number of dimensions, so that the number of particles needed
is independent of dimension; see also .
We now turn to nonlinear and nonconvex examples. Let the observation function
h be strongly nonlinear: h(x) = x3. With σ = s = 0.1, the pdf (11) becomes
non-U-shaped for |b| ≥0.77. In Figure 3 we display the function F for b = 1 (the
solid curve). To use the algorithms above we need a substitute function F0 that is
U-shaped; we also display in Figure 3 (the broken line) the function F0 we used;
the recipe here is to link a point above the local minimum on the left to the absolute
minimum on the right by a straight line. It is important to make F0 and F have the
same minimum. Many other constructions are possible (see in particular the next
section). As described above, we solve
F0(x) −φ = ξ T ξ/2
with φ = min F0, and once x has been determined, add the difference F(x)−F0(x)
to φ in the weight exp(−φ)J. This construction does not introduce any bias. The
function F0 constructed in this way is U-shaped but need not be convex, so that one
IMPLICIT PARTICLE FILTERS FOR DATA ASSIMILATION
Non−convex F
U−shaped substitute
Figure 3. A nonconvex function F (solid line) and a U-shaped
substitute (broken line).
needs Algorithm B above. In Table 4 we compare the Radon–Nikod´ym derivatives
of the prior with respect to the posterior for the resulting implicit sampling and for
standard Bayesian sampling with σ = s = 0.1, b = 1.5.
The histogram for the implicit ﬁlter is no longer perfectly balanced. The asymmetry in the histogram reﬂects that of F0 and can be eliminated by biasing ξ, but there
is no reason to do so; there is enough importance sampling without this extra step.
Table 4. Radon–Nikodým derivatives of the prior with respect to
the posterior, h(x) = x3, σ = s = 0.1, b = 1.5, 10000 samples, F0
as in the text.
ALEXANDRE CHORIN, MATTHIAS MORZFELD AND XUEMIN TU
−0.00 ± 0.01 −0.00 ± 0.01
0.109 ± 0.01
0.109 ± 0.01
0.394 ± 0.04
0.451 ± 0.02
0.775 ± 0.09
0.995 ± 0.01
0.875 ± 0.05
1.18 ± 0.01
0.895 ± 0.02
1.29 ± 0.02
Table 5. Comparison of the estimates of the means, implicit vs.
standard ﬁlter, 1000 particles, together with the exact result, when
h(x) = x3, as explained in the text.
In Table 5 we display the estimates of the means of the density for the two ﬁlters
with 1000 particles for various values of b, compared with the exact results (the
number of particles is relatively large because with h(x) = x3 and our parameter
choices the variance of the conditional density is signiﬁcant, and this number of
particles is needed for meaningful comparisons of either algorithm with the exact
As mentioned in the previous section, there are alternatives to the replacement
of F by F0; the point is that for each particle the function F is an explicitly known
nonrandom function, and this fact can be used in multiple ways.
5. Sparse observations and higher-order difference approximations
We now discuss what happens when the observations are sparse, so that there
are data only every r > 1 time steps, and how to sample when the difference
approximation is more elaborate than the Euler scheme used so far. Along the way,
we suggest additional ways to solve the algebraic equations.
Consider again the discrete SDE (3), with observations available only at times
rδ, r > 1. To simplify the presentation, let g(x, t) = 1 and assume the equation is
scalar. Write the scheme in the form xn+1 = q(xn) + δV , where V is a Gaussian
with mean zero and variance one. We have data at the points rδ, r > 1, where
bn+r = h(xn+r) + √sW and W is a Gaussian of mean zero and variance one and s
is a constant. The probability of the particle path (Xn+1
, . . . , Xn+r
now on we will suppress the index i) is
P(Xn+1, . . . , Xn+r) = exp(−F(Xn+1, . . . , Xn+r))/Z,
where Z is a normalization constant and
F(Xn+1, . . . , Xn+r) = (h(Xn+r) −bn+r)2
(Xn+i −q(Xn+i−1))2.
IMPLICIT PARTICLE FILTERS FOR DATA ASSIMILATION
The task at hand is to solve
F(Xn+1, . . . , Xn+r) −min F(Xn+1, . . . , Xn+r) = ξ T ξ/2,
where ξ is a sample of a r-dimensional Gaussian reference variable. This can be
done by the methods presented above, but we use this opportunity to present some
First, we ﬁnd the minimum of F. If F is convex, this can be easily done by
Newton’s method (note that the matrices one gets are sparse). If F is not convex,
one can try the following device: add to F the quantity αG, where α > 0 is a
parameter and G is the convex function
G = (Xn+r −bn+r)2 +
(X j −X j−1)2.
Then minimize F + αG for a suitable sequence αn →0. (This device was inspired
by the rubber band construction of computational chemistry . More generally,
it is useful to note the resemblance of the problem to the study of rare transitions
in computational chemistry [18; 10]). A minimization by a Newton’s method also
yields the Hessian H of F at the minimizer z of F.
Deﬁne F0 = φ + (1/2)(X −z)T H(X −z), where φ = min F = F(z) and X is
the vector (Xn+1, . . . , Xn+r). Solve the equation F0(X) −φ = ξ T ξ/2 and obtain
X. This is a linear problem and Choleski construction works ﬁne and also yields
the Jacobian J. Use as weight for the resulting sample X the quantity exp(−φ0)J,
with φ0 = φ + F(X) −F0(x) so that (13) is still satisﬁed and there is no bias. This
is still a high-probability sample, because the neighborhood of ξ = 0 is still mapped
on the neighborhood of the minimum of F.
As an example, consider the SDE dx = cos(5x)dt + σdW, with σ = 0.1,
discretized by Euler’s method with time step 0.01; the observations bn = xn +η are
available every 20 steps (a time interval of 0.2) and η is a Gaussian variable of mean
zero and variance 10−3. The data are generated by running the equation once and
observing its path. We used 4 particles. In Figure 4 we display the run that generated
the data path and the reconstruction; the data are used in the reconstruction only
when t = 0.2 and t = 0.4. Observe that between data the discrepancy can be quite
signiﬁcant, as is indeed unavoidable.
This last example should make it plain what one should do when one uses a
higher-order discretization of the SDE. For example, suppose one is integrating the
SDE dx = f (x)dt + dW using the second-order Klauder–Petersen scheme :
xn+1,∗= xn + δf (xn) + η1,
xn+1 = xn + (δ/2)
 f (xn) + f (xn+1,∗)
ALEXANDRE CHORIN, MATTHIAS MORZFELD AND XUEMIN TU
Observed Path
Observations
Reconstruction
Figure 4. Reconstruction with sparse data.
where η1, η2 are Gaussians with mean zero and variance δ. Observations bn =
h(xn) + η3, where η3 has mean zero and variance s, are assumed available at every
step. The probability of the pair (xn+1,∗, xn+1) is exp(−F), with
(xn+1,∗−xn−δf (xn))2
 xn+1−xn−δ
2( f (xn)+ f (xn+1,∗))
 h(xn+1)−b
All one has to do then is solve F −min F = ξ T ξ/2 for a sample ξ of a twodimensional Gaussian reference variable, along the lines suggested above.
6. Parameter identiﬁcation
One important application of particle ﬁlters is to parameter identiﬁcation, where the
SDE contains an unknown parameter and the data are used to ﬁnd this parameter’s
value. One of the standard ways of doing this [9; 14] is system augmentation: one
adds to the SDE the equation dσ = 0 for the unknown parameter σ, one offers σ a
gamut of possible values, and one relies on the resampling process that eliminates
the values that do not ﬁt the data. With the implicit ﬁlter this procedure fails, because
the particles are not eliminated fast enough. One alternative is ﬁnding the unknown
parameter σ by stochastic approximation. Speciﬁcally, ﬁnd a statistic T of the output
of the ﬁlter which is a function of σ, such that the expected value E[T ] vanishes
when σ has the right value σ ∗, and then solve the equation E[T ] = E[T (σ)] = 0
IMPLICIT PARTICLE FILTERS FOR DATA ASSIMILATION
by the Robbins–Monro iteration :
σn+1 = σn −αnT (σn),
which converges when the coefﬁcients αn are such that P αn →∞while P α2
remains bounded (for example, αn = 1/nq with 0.5 < q ≤1). Related ideas can be
found, for example, in .
As a concrete example, consider the SDE dx =dW, where W is Brownian motion
with variance σ, discretized with time steps δ, with observations bn = xn +η, where
η is a Gaussian with mean zero and variance s. Data are generated by running the
SDE once with the true value σ ∗of σ, adding the appropriate noise, and registering
the result at time nδ as bn for n = 1, 2, . . . , N. For the functional T we chose
where the summations are over i between 2 and N, 1i is the estimate of the
increment of x in the i-th step and C is a scaling constant. Clearly if the σ
used in the ﬁltering equals σ ∗then by construction the successive values of 1i
are independent and E[T ] = 0. We picked the parameters N = 100, σ = 10−2,
s = 10−4, δ = 0.01 (so that the increment of W in one step has variance 10−4).
Our algorithm is as follows: We make a guess σ1, run the ﬁlter for N steps,
evaluate T , and make a new guess for σ using (16) and α1 = 1, rerun the ﬁlter, etc.,
with the αn, the coefﬁcient in (16) at the n-th step, equal to 1/n. The scaling factor
in (17) was found by trial and error: if it is too large the iteration becomes unstable,
if it is too small the convergence is slow; we settled on C = 4.
This algorithm requires that the ﬁlter be run without resampling, because resampling introduces correlations between successive values of the 1i and bias the
values of T. In a long run, in particular in a strongly nonlinear setting, one may
need resampling for the ﬁlter to stay on track, and this can be done by segmentation:
divide the run of the ﬁlter into segments of some moderate length L, perform the
summations in the deﬁnition of T over that segment, then go back and run that
segment with resampling, then proceed to the next segment, etc.
The ﬁrst question is, how well is it possible in principle to reconstruct an unknown
value of σ from N observations; this issue was already discussed in . Given 100
samples of a Gaussian variable of mean 0 and variance σ, the variance reconstructed
from the observations is a random variable of mean σ and variance 0.16 · σ; 100
observations do not contain enough information to reconstruct σ perfectly. A good
way to estimate the best result that can be achieved is to run the algorithm with the
guess σ1 equal to the exact value σ ∗with which the data were generated. When
ALEXANDRE CHORIN, MATTHIAS MORZFELD AND XUEMIN TU
new estimate σ/σ ∗10 0.819 0.943 1.02 1.05 1.08 1.10 1.13 1.15 1.16
Table 6. Convergence of the parameter identiﬁcation algorithm.
this was done, the estimate of σ was 1.27σ ∗. This result indicates the order of
magnitude of the accuracy that can be achieved.
In Table 6 we display the result of our algorithm, run with 50 particles and
starting value σ1 = 10σ ∗. Each iteration requires running the ﬁlter once.
7. Conclusions
We have presented the implicit ﬁlter for data assimilation, together with several
algorithms for the solution of its algebraic equations, including cases with nonconvex
functions F, as well as an algorithm for parameter identiﬁcation. The key idea in
implicit sampling is to solve an algebraic equation of the form
F(X) −φ = ξ T ξ/2
for every particle, where the function F is explicitly known, X is the new position
of the particle, φ is an additive factor, and ξ is a sample of a ﬁxed reference pdf; F
varies from particle to particle and step to step. This construction makes it possible
to guide the particles to the high-probability area one by one under a wide variety
of circumstances. It is important to note that the equation that links ξ to X is
underdetermined and its solution can be adapted for each particular problem. The
effectiveness of implicit sampling depends on one’s ability to design maps ξ →X
that satisfy the criteria above and are computationally efﬁcient. The design of such
maps is problem dependent and we will present examples in the context of speciﬁc
applications.
Acknowledgements
We would like to thank Prof. Jonathan Weare for asking penetrating questions and
for making very useful suggestions, Prof. Robert Miller for his advice and encouragement, and Mr. G. Zehavi for performing some of the preliminary computations.