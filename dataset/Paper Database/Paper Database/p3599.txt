Rao, Olshausen and Lewicki: Probabilistic Models of the Brain
2001/05/02 19:33
Vision, Psychophysics and Bayes
Paul Schrater and Daniel Kersten
Introduction
Neural information processing research has progressed in two often divergent directions. On the one hand, computational neuroscience has advanced our understanding of the detailed computations of synapses, single neurons, and small networks (cf. ). Theories of this sort have beneﬁted from the scientiﬁc interplay
between testable predictions and experimental results from real neural systems.
As such, this direction is aimed most directly at the mechanisms implementing
visual function.
On the other hand, theoretical neural networks have been subsumed as special cases of statistical inference (cf. ). This direction is well-suited to
large scale systems modeling appropriate to the behavioral functions of perceptual and cognitive systems. We’ve seen considerable progress in the solution of
complex problems of perception and cognition–solutions obtained without speciﬁc reference to neural or biological mechanisms. However, in contrast to smallscale neuron-directed modeling, behavioral theories face a different experimental
challenge–namely, how can quantitative models be reﬁned by experiment? The
neural implementation of a model of statistical pattern recognition typically has
too many independent variables to test neurophysiologically, and behavioral tests
are unsatisfying because most of the parameters are unmeasureable.
The purpose of this chapter is to suggest that the development of neural information processing theories based on statistical inference is actually good thing for
psychology, and in particular for visual psychophysics. The proper level of abstraction avoids the premature introduction of unmeasureable neural parameters
that are too numerous or difﬁcult to test behaviorally. Yet, as is the case for thermodynamics and models of molecular motion, the bridge between statistical pattern
theories and neural networks can be made when required.
The principle concern of psychologists is behavior. In vision, we use the term
Rao, Olshausen and Lewicki: Probabilistic Models of the Brain
2001/05/02 19:33
Vision, Psychophysics and Bayes
behavior broadly to include perceptual psychophysics, experimental studies of
visual cognition, and visual motor control . One can also distinguish two often
divergent directions in the study of visual behavior in questions that address: 1)
neural mechanism and 2) functional tasks. These questions can often be rephrased
in the form of questions that address what the visual system can do when pushed
to extremes, in contrast to what it does do in natural circumstances.
What people can do. There is a long tradition of relating the phenomena of visual
perception and psychophysical performance to underlying neural mechanisms.
Probably the most famous and successful example is color trichromacy which was
deduced from psychophysical experiments of Helmholtz, Maxwell and others in
the 19th century. The neurophysiological basis in terms of discrete retinal receptor
types wasn’t ﬁrmly established until the middle of the 20th century. Another example is the intriguing correspondence between the wavelet-like patterns that humans detect best and the receptive ﬁeld proﬁles of simple cells in V1 . For
tests of mechanism, the behavioral tasks can be rather unnatural and the stimuli
often bear little resemblance to the kinds of images we typically see. Signal detection theory has played an important role in relating behavior to mechanism .
It provides a bridge between theory and experiment as applied to neural mechanisms. In this ﬁrst case, statistical or signal detection theories provide the means
to account for the information available in the task itself–a step often neglected in
drawing conclusions about mechanism from psychophysical measurements.
What people do do. There is also an ecological tradition in which we seek to understand how visual function is adapted to the world in which we live. Answering
the question “How well do we see the shapes, colors, locations, and identities of
objects?” is an important component in providing answers to the more general
question “How does vision work?”. The challenge we address below is arriving
at a quantitative model to answer the “How well do we see ...” question for natural visual tasks. Statistical inference theory, and in particular the speciﬁc form
we describe below as pattern inference theory, plays a role in the analysis of both
kinds of perceptual behavior–but is particularly relevant for developing quantitative predictive models of visual function.
In this chapter, we describe a framework within which to develop and test
predictive quantitative theories of human visual behavior as pertains to both
mechanism and function. What are we asking for in a quantitative theory? A
complete quantitative account of a human visual task should include models both
of function, and of the mechanisms to realize those functions.
Problems of vision: ambiguity and complexity
At the functional level, one would like a model whose input is a natural image
or sequence and whose output, shows the same performance as the human along
Rao, Olshausen and Lewicki: Probabilistic Models of the Brain
2001/05/02 19:33
Vision, Psychophysics and Bayes
some dimension. Further, the model should also predict the pattern of errors with
respect to ground truth. Below we will describe a recent study which compared
the perceived surface colors under quasi-natural lighting conditions with those of
a Bayesian estimator.
Any such modeling effort immediately runs into two well-known theoretical
problems of computer vision: ambiguity and complexity. How can a vision system draw reliable conclusions about the world from numerous locally ambiguous image measurements? The ambiguity arises because any image patch can be
produced from many different combinations of scene variables (e.g. object and illumination properties). In addition, all of the scene variables, both relevant and
irrelevant for the observer’s task, contribute to natural image measurements. In
general, image measurement space is high-dimensional, and building models that
can deal with these problems leads to complex models that are difﬁcult to analyze
and for which it is difﬁcult to determine what are the essential features. One possible solution is the historical one–take an empirical approach to human vision, and
develop models to summarize the data from the ground up.
However, the complex high-dimensional nature of the relevant scene parameters leads to an empirical problem as well: psychophysical testing of models of human perception under natural conditions could rapidly become hopelessly complex. For example, with over a dozen cues to depth, a purely empirical approach to
the problem of cue integration is combinatorially prohibitive. Thus, we are faced
with two formidable problems in vision research. First, how do we develop relevant and analyzable models for vision, and second, how do we test these models
experimentally?
Developing Theories
For the modeling problem, we will argue that it is crucial to specify the correct
level of analysis, and we distinguish modeling at the functional level from modeling at the level of mechanisms. The functional level constitutes a description
of the information required to perform for particular task without worrying about
the speciﬁc details of the computations. It addresses issues of the observer’s prior
knowledge and assumptions about scene structure, image formation, and the costs
associated with normal task demands. On the other hand, the mechanistic level is
concerned with the speciﬁc details of neural computations.
Functional Level Modeling
At the functional level, we wish to model a natural visual task, like apple counting. Given the natural task, the solution we propose is to let the ideal (Bayesian)
observer for the task serve as a default model. The modeling strategy is to hypothesize that human vision uses all the information optimally. Of course, human vision
Rao, Olshausen and Lewicki: Probabilistic Models of the Brain
2001/05/02 19:33
Vision, Psychophysics and Bayes
is not optimal, but starting from the optimal observer yields a coherent research
strategy in which models can be modiﬁed to discard the same kinds and amount
of information as the human visual system. This approach departs from the historical bottom-up approach to modeling, but we suggest that it may be ultimately
simpler than trying to determine how to “smarten up” a suboptimal model based
on experiment. Here the main focus is how often the human is ideal or “ideal-like”.
By ideal-like we mean that peformance parallels ideal in all but a small number
of dimensions (e.g. additive internal noise). Below we discuss an approach to information modeling we call “pattern inference theory”, that is an elaboration of
signal detection theory that provides the generative and decision theoretic tools to
model the informational limits to natural tasks .
Two main strategies at the functional level can be distinguished: a) model the
complexity of the visual inference, and compare the model with human performance, but don’t try to directly model the information that is present in natural
scenes ; b) model the physical information, and measure how well this model
accounts for human performance. It is this second strategy we illustrate below
with an example from color constancy . Having a general purpose model for
counting apples in trees may be in the distant future, but part of that model will
involve understanding how surface colors for apples can be distinguished from
those of leaves. Because of interreﬂections, any given point of the retinal image
will often have wavelength contributions from direct and indirect light sources
(i.e. from apples and leaves), that need to be discounted to infer the identity and
properties of objects.
Mechanism Level Modeling
At the mechanism level we are interested in how visual stimuli are actually processed, what features in the image are measured, what kinds of representations
does the visual system use. One would like an account of the neural systems, the
spatial-temporal ﬁltering, neural transformations and decision processes leading
to the output. Here the modeling methods are more diffuse, encompassing optimal image encoding ideas and more traditional analyses of the utility
of image measurements for some task (e.g. the optic ﬂow ﬁeld for inferring egomotion).
In these kinds of study the model is typically not generated through an ideal
observer analysis of a natural task. Nevertheless, we can proﬁtably use the ideal
observer approach to test mechanistic models. Here, deviations from ideal provide insight into biological limitations. The immediate focus of interest is how
much human performance differs from ideal, because it is the differences which
are diagnostic of the kinds of mechanisms used by the visual system. For example,
we know that light discrimination departs from the ideal at high light levels, long
durations, and large regions . We also know that human pattern detection competes well with ideal observers when the patterns match the receptive ﬁeld proﬁles
Rao, Olshausen and Lewicki: Probabilistic Models of the Brain
2001/05/02 19:33
Vision, Psychophysics and Bayes
of simple cells . The way in which spatial resolution departs from ideal
can be explained in large part by optical, sampling, and neural inefﬁciencies .
In addition, having an understanding of the ideal observer for the task used in
the laboratory to test the mechanistic hypotheses is crucial. Psychophysical testing hypotheses of mechanism leads to a problem in addition to that of complexity: inferring mechanisms from psychophysics must take into account how performance depends on the information for the task. In particular, opposite conclusions
can be drawn from psychophysical results depending on how the information is modeled.
(cf. ) As we illustrate below, ideal observer analysis, a part of a signal
detection theory, provides the solution to this problem of information normalization.
As an example of the difference between functional and mechanistic levels, consider the following question. What are the crucial features of leaf texture as distinct from apples that would lead to reliable segmentation? This kind of question
can be addressed at the functional level by specifying generative texture models for the task and testing whether the most statistically reliable features for
discrimination are used by the human observer. However, one could imagine a
ﬁner grain analysis to test whether the particular features hypothesized by the information model are indeed processed in terms of particular spatial ﬁlters. At this
level, laboratory manipulations can be geared towards analysing mechanisms of
visual processing. In the context of our apple counting problem, part of a complete
explanation will be to understand how motion parallax can help to segment apple
from leaf surfaces–but underneath this level, is the question of the kind of motion
mechanism human vision might use to support such functional inferences.
Testing models using ideal observers
One of the major points of this chapter is that the modeling problem naturally
breaks into two: determining how a useful signal (e.g. objects, distances, shapes,
etc) get encoded into intensity changes in the image, and second, determining the
limits to decoding the image to infer the signals. The encoding problem, which
involves modeling both the regularities and structure in the signal domain as well
as the image formation process, has been frequently neglected in studies of human
vision (but see ). We discuss this problem more completely in the section
on Pattern Inference Theory below. On the other hand, the decoding problem
involves ﬁnding decoding strategies that clearly depend on how the signals were
encoded. In the study of decoding, the fundamental object is the optimal decoder,
comparedto which all other decoders can be described in terms of the information
about the signals they discard. Thus studying the decoding problem relies on
theories of ideal observers, or more generally of optimal inference. If we describe
human perception as a process of decoding images, then the ideal observer can
be used to describe human performance in terms of deviations from optimality.
Rao, Olshausen and Lewicki: Probabilistic Models of the Brain
2001/05/02 19:33
Vision, Psychophysics and Bayes
Returning to the question of how to test our models experimentally, the preceding
suggests the strategy of comparing human to ideal performance. We will discuss
how this comparison can be used to test theories and give two examples of such
tests below.
In the next section, we outline the elements of a Bayesian approach to vision applicable to both the analysis of mechanism and function. In the following section,
we apply these elements to questions of mechanism and in particular we illustrate information normalization in the analysis of motion measurements. The last
section shows how information modeling has been used to provide a quantitative
account of an aspect of functional vision, color matching results for simple natural
Bayesian perception and Pattern Inference Theory
Because observers use their visual systems to do things, theories of visual perception cannot be built in isolation from functional visual tasks. We see the consequences of this statement as including the following principles: 1) vision is inference; 2) vision has relevant knowledge of scene structure prior to performing a
task; 3) the affordances (value of the outcomes) of a completed task determine the
costs and beneﬁts of acquiring visual knowledge. A fundamental approach that
quantiﬁes these principles is a theoretical apparatus we call Pattern Inference theory, which is a conjunction of Bayesian Decision Theory with Pattern Theory. As an
elaboration of signal detection theory, we choose the words pattern and inference
to stress the importance of modeling complex natural signals, and of considering
tasks in addition to detection, respectively.
We have elsewhere argued that Pattern Inference Theory provides the
best language for a quantitative theory of visual perception at the level of the naturally behaving (human) visual system. The term, “Pattern Theory” was coined
by Ulf Grenander to describe the mathematical study of complex natural patterns . In our usage, Pattern Inference Theory is a probabilistic model
of the observer’s world and sensory input, which has two components: the objects
of the theory, and the operations of the theory. The objects of the theory are the set
of possible image measurements
  , the set of possible scene descriptions
the joint probability distribution of
  . The operations are given by
the probability calculus, with decisions modeled as cost functionals on probabilities. The richness of the theory lies in exploiting the structure induced in
by the regularities of the world (laws of physics) and by the habits of observers.
This emphasizes a central role for the modeling of representations and their transformations. Pattern Inference theory also assumes a central role, in perception, of
generative (or synthetic) models of image patterns, as well as prior probability
Rao, Olshausen and Lewicki: Probabilistic Models of the Brain
2001/05/02 19:33
Vision, Psychophysics and Bayes
I=φ(Sprim,Ssec)
Figure 2.1: Panel A illustrates the generative model for the signal detection task.
Signal detection theory provides tools particularly appropriate for the behavioral
analysis of visual mechanisms. Panel B illustrates the general form of the generative
model for the pattern inference theory task. Pattern inference theory is an elaboration
of signal detection theory which seeks to take into account the speciﬁc generative
  of natural image formation, and the full range of natural tasks. Pattern
Inference Theory is of particular relevance for modeling natural visual function.
models of scene information. Our example below compares two generative models of color, one based on direct, and a second on direct plus indirect lighting. An
emphasis on generative models, we believe, is essential because of the inherent
complexity of the causal structure of high-dimensional image patterns. One must
model how the multitude of variables, both needed and unneeded, interact to produce image data in order to understand how to decode those patterns.
How can we describe the processes of visual inference as image decoding by
means of probability computations (i.e. from the point of view of pattern inference
theory)? To do so requires a probabilistic model of tasks. We consider a task as
specifying four ingredients: 1) the relevant or primary set of scene variables
2) the irrelevant or secondary scene variables
 , 3) the scene variables which
are presumed known
 , and 4) the type of decision to be made. Each of the
Rao, Olshausen and Lewicki: Probabilistic Models of the Brain
2001/05/02 19:33
Vision, Psychophysics and Bayes
four components of a task plays a role in determining the structure of the optimal
inference computation1.
Bayesian decision theory provides a precise language to model the costs of
errors determined by the choice of visual task . The ideal observer that
minimizes average error ﬁnds the
  which minimizes the following risk:
with respect to the posterior probability,
  . In practice, the posterior probability
 is factored into two terms (and a constant
denominator) using Bayes theorem: the likelihood,
 , which is determined by the generative model for the image measurements (see ﬁgure 2.1), and
the prior probability,
 . A simple Bayesian maxim summarizes the above
calculation: Condition the joint probability on what we know, and marginalize
over what we don’t care about2. As seen in the color constancy section below, we
have prior knowledge of the illuminant spectrum, we measure the shape and image color, we don’t care about the illumination direction, and we want to estimate
the surface color properties that minimize color judgment errors.
Pattern inference theory, ideal observers, and human vision
In order for the pattern inference theory approach to be useful, we need to be
able to construct predictive theories of visual function which are amenable to
experimental testing.
How do we formulate and test theories of vision at the functional level within
a Bayesian pattern inference theory framework? Tests of human perception can be
1. The cost or risk
 of guessing
when the image measurement is
 is deﬁned as the
expected loss:
with respect to the posterior probability,
 . The best interpretation of the image can
then be made by ﬁnding the
which minimizes the risk function. The loss function
speciﬁes the cost of guessing
when the scene variable is
. One possible loss function is
 . In this case the risk becomes
 , and then the best strategy
is to pick the most likely interpretation. This is standard maximum a posteriori estimation
(MAP). A second kind of loss function assumes that costs are constant over all guesses of a
variable. This is equivalent to marginalization of the posterior with respect to that variable.
2. This Bayesian maxim is due to James Coughlan
Rao, Olshausen and Lewicki: Probabilistic Models of the Brain
2001/05/02 19:33
Vision, Psychophysics and Bayes
based on hypotheses regarding constraints contained in: the two components of
the generative model, 1) the prior
 and 2) the likelihood
 ; 3) the image
  ; 4) the posterior
  ; or 5) the loss function. The ﬁrst four can be
called information constraints, whereas the ﬁfth can be called a decision constraint.
These levels can be translated into tests of: 1) prior knowledge and assumptions of
scene structure; 2) model of the observer’s image formation process; 3) what image
measurements and image coding does the observer do; 4) the information about
a scene available to the observer given an image; 5) the decisions and strategies
used by the observer.
It is at the level of the posterior that provides the most complete quantitative
model of the information available to an observer to perform a task. Whether this
information is used optimally or not depends on whether the observer’s loss function is matched to the particular task. Thus it is important to investigate more than
one task that uses the same posterior in order to attribute a loss of information to
the observer’s posterior rather than a loss due to an inappropriate loss function. In
all cases, we want to design our experiments to focus on the constraint hypotheses.
For example, if we are interested in testing hypotheses about the observer’s prior
distribution on light source direction, then the experimenter could focus on simple
ambiguous scenes whose interpretation depends on the light source direction. In
these scenes the prior is expected to dominate.
However, the fact that testing at the level of information constraints uses information common to many decisions has a practical side-effect: we can deﬁne
subdomains of
 that are more easily implemented in the laboratory, and
yet use the same posterior. This allows the experimenter to focus on testing the
interesting predictions of the hypotheses.
As we’ve noted above above, in psychophysical experiments, one can: a) test
at the functional or constraint level–what information does human vision avail
itself of?, or; b) test at the mechanism level–what neural subsystem can account
for performance? Because of its emphasis on modeling natural pattern representations and transformations, Pattern Inference theory is of primary relevance to
hypotheses testable at the former level, (e.g. hypotheses about the representations
appropriate for inference). Signal detection theory is of primary importance for the
latter as we will see in the motion example below, where SDT provides the tools
for rigorous tests of neural representation. In both cases, using an ideal observer
allows a simple and meaningful comparison to be made between human and ideal
performance.
The primary use of an ideal observer in an experimental setting is to provide
a measure of the information available to perform a task that can be used to
normalize the performance of any other human of model observer. Reporting
human and model performance relative to ideal performance allows a straight
forward comparison of the results from completely different tasks and visual
cues. In other words, it allows comparisons like “Is the visual system better at
Rao, Olshausen and Lewicki: Probabilistic Models of the Brain
2001/05/02 19:33
Vision, Psychophysics and Bayes
processing edges or shading information?” . A standard way of performing
this normalization is to compute efﬁciency, which is a measure of the effective
number of samples used by the observers .
This normalization function also provides a criterion by which to judge whether
two models are functionally equivalent, and when a model can be ﬁrmly rejected.
If two models produce the same efﬁciency curves for all relevant changes in task
and stimuli, then the two models are equivalent. Notice, however, that two models
can be equivalent by this criterion and yet could employ very different computations at the level of mechanism. This also allows us to construct models that are
functionally equivalent to the human visual system. Everywhere human performance deviates from ideal, we modify the ideal observer to discard equivalent
information and no more. Notice that this construction has a test of the viability of
a model for human performance built into it. Anytime a human observer can outperform a model observer on a task, that model can be eliminated as a possible
model for the human visual system.
In the next section we will look at an in-depth example of an application of the
ideal observer approach to the problem of the determining the mechanisms used
by the visual system to detect motion.
Ideal observer analysis: Mechanisms of motion measurement
Experimental studies of human perceptual behavior are often left with a crucial,
but unanswered question: To what extent is the measured performance limited by
the information in the task rather than by the perceptual system itself? Answers
to this question are critical for understanding the relationship between perceptual
behavior and its underlying biological mechanisms. Signal detection theory provided an answer through ideal observer analysis.
To show the power and limitations of this approach consider an example of a
recent application (by one of the authors) of classical signal detection theory to the
problem of visual motion measurement.
A model for motion detection
When a person moves relative to the environment, the visual image projected
onto the retina changes accordingly. Within small regions of the retina and for
short durations this image change may be approximated as a two-dimensional
translation. The set of such translations across the visual ﬁeld is termed the Optic
Flow ﬁeld. Having described a simple approximation of the otherwise complicated time-varying retinal image, the question remains whether the human visual
Rao, Olshausen and Lewicki: Probabilistic Models of the Brain
2001/05/02 19:33
Vision, Psychophysics and Bayes
Figure 2.2: A translational motion detector. a, Space-time luminance pattern of an
image translating to the right. This is a representation of the intensity information in
the retinal image (the x- y plane) over time (t). The rightward motion can be inferred
from the oriented pattern on the x-t face. b, The Fourier amplitude spectrum of the
luminance pattern, represented by the intensity of points in a three-dimensional
spatio-temporal frequency domain. Non-zero Fourier amplitudes are constrained
to lie on a plane through the origin. The orientation of this plane uniquely specify
the direction and speed of translation. c, Construction of a translation detector ,
illustrated in the Fourier domain. Pairs of balls symmetric about the origin indicate
the Fourier amplitude spectra of band-pass ﬁlters whose peak frequencies lie in the
plane. A translation detector can be constructed by summing the squared outputs of
such ﬁlters.
system uses such an approximation. A number of physiological and psychophysical experiments have established that the mammalian visual system does contain
mechanisms sensitive such local image translations , but these studies did not
specify how local image translations might be measured by the visual system. One
approach, initially due to Heeger and later reﬁned , uses fundamental properties of translating signals to derive an estimator for the velocity of the
translation. Consider an image sequence
 . If in a window of space and
 (e.g. gaussian) the image motion can be described as a translation,
  
 . It is easy to show that the
spatio-temporal (3-D) Fourier transform of one windowed region is given by
Note that the delta function term is an equation for a plane in the Fourier
domain speciﬁed by
 . Thus the equation says that local image
translations in the Fourier domain are characterized by the spatial spectrum of
the image projected onto a plane whose orientation is uniquely speciﬁed by the
velocity of the translation, which is convolved by the Fourier transform of the
windowing function. For a Gaussian windowing function, the result is easy to
state: translations are speciﬁed by blurred planes (or pancakes) in the Fourier
domain. Figure 2.2a and b illustrate this without the windowing function. Given
this description, a simple velocity detector can be constructed by pooling the
outputs of spatio-temporal ﬁlters whose peak frequencies lie on a common plane
(e.g. see ﬁgure 2.2c). Because the phase spectrum is not required for the velocity
Rao, Olshausen and Lewicki: Probabilistic Models of the Brain
2001/05/02 19:33
Vision, Psychophysics and Bayes
estimates, a noise resistant detector can be built by pooling the outputs of
ﬁlters that compute the energy within a region of spatio-temporal frequency (e.g.
like complex cells in V1). For a windowed signal
 , the output
of the detector is
denotes whose peak frequency lies on the plane speciﬁed by the signal.
Within this simple theory, we have a choice of the weights
. Given a particular
image, only some of the ﬁlter bands
will contain the signal, and responses
from ﬁlters not containing the signal will be solely due to noise. Thus a “smart”
detector can improve detection performance by adjusting its pooling weights to
match the signal. On the other hand, a good non-adaptive detector can be built by
optimizing the weights for the expected (average) signal. This leads to a detector
that pools over all spatial frequency orientations in a plane, because the expected
spatial signal is approximately isotropic. We wanted to test whether an adaptive
or ﬁxed pooling power detector is a good model of human motion detection.
Notice the putative motion detection mechanisms have not been motivated
from within signal detection theory. Rather they were motivated via a simple
approximation and signal processing issues.
Testing the model
Finding a task for which the model is ideal
Signal detection theory can be used to assess the feasibility of such a model. To do
so, we use the fact that the model we are interested in is an ideal observer for some
task and stimuli. The idea is that if we have human observers perform the optimal
task for the model, then if the model is a good description: 1) Humans should be
good at the task 2) the model should predict errors on related tasks. Schrater et.
al. have recently shown that the putative motion detectors are ideal observers
for detecting a class of novel stochastic signals added to Gaussian white noise. The
stochastic signals are produced by passing Gaussian white noise through the ﬁlters
used to construct the motion detector. In general, a detector which computes the
Fourier energy within a ﬁlter is an ideal observer for stochastic signals generated
by passing Gaussian white noise through the ﬁlter. Thus, by varying the number
and placement of ﬁlters, we can produce motion stimuli that are consistent with a
single translational velocity and have various spatial frequency spectra, or stimuli
that are consistent with multiple velocities. Examples of some ﬁlters and stimuli
are shown in ﬁgure 2.3.
So how do we go about testing our model by detecting these stochastic stimuli?
Rao, Olshausen and Lewicki: Probabilistic Models of the Brain
2001/05/02 19:33
Vision, Psychophysics and Bayes
Figure 2.3: Filter sets and examples of their corresponding signals. The top row depicts level surfaces (65% of peak response) of the three different ﬁlter sets used to
generate stimuli. The bottom row depicts space-time luminance patterns of signals
produced by passing spatio-temporal Gaussian white noise through the corresponding ﬁlter sets. (a) The “component” stimulus, constructed from a spatially and temporally band-pass ﬁlter. The x-y face of the stimulus shows structures that are spatially band-pass and oriented along the x axis. The orientation of structures on the
x-t face indicates rightward motion. (b) The “plaid” stimulus, constructed from two
“component” ﬁlters lying in a common plane. The x-y face of the stimulus shows a
mixture spatial of spatial structures with dominant orientations close to the y axis.
(c) The “planar” stimulus, constructed from a set of 10 “component” ﬁlters lying in
a common plane. The stimulus is spatially band-pass and isotropic (x-y face), and
exhibits rightward motion (x-t face).
The ideal observer analysis gives immediate simple testable predictions: 1) Under
the adaptive model as we vary the spatial structure of the motion stimulus, human
performance should be constant relative to the ideal for each stimulus. 2) Under
the ﬁxed model, we should see predictable variations in performance. 3) If the
model is false, we should be terrible at the task.
Testing the predictions
Translating these predictions into the detection task, the adaptive model predicts
that any conﬁguration of Fourier energy on the plane should be equally detectable.
To test this prediction, we had observers vary the total energy of one of the three
signals shown in ﬁgure 2.3 added to white noise until just detectable. The thresholds are plotted as signal power to noise power ratios in ﬁgure 2.4a. Note that the
thresholds are lowest for detecting the “planar” signal with energy spread equally
across a plane, followed by “plaid”, with two bands, followed by the “component”
ﬁlter, with only one band. However, unlike the non-stochastic stimuli used in previous signal detection experiments, here the signal to noise energy measure does
Rao, Olshausen and Lewicki: Probabilistic Models of the Brain
2001/05/02 19:33
Vision, Psychophysics and Bayes
Threshold SNR
Efficiency
Figure 2.4: (a) Detection performance of three subjects for the three stochastic signals
of Fig. 2.3. Threshold signal to noise ratio (SNR) for 81.1% detectability. SNR is calculated as the ratio of the signal power to the background noise power. Heavy black
lines indicate predictions for ideal summation, derived from the component condition thresholds. (b) Detection efﬁciencies for the three stimulus types. Efﬁciencies are
plotted in proportions, with 1.0 reﬂecting perfect performance; that is, performance
matching that of an ideal observer tuned to the structure of the signal in the stimulus (different for each stimulus type). The differences between the efﬁciencies of
the pattern stimuli (plaid and planar stimuli) and the component stimulus provide a
quantitative measure of summation of the pattern’s components.
not capture the subject’s relative performance on the three stimuli. Looking at the
threshold data, we might be tempted to conclude that planar stimuli are most easily detected and “component” and “plaid” stimuli are comparable in detectability.
In fact, if we correctly normalize the observer’s thresholds by the ideal observers
thresholds for each stimulus to compute an efﬁciency measure, then we ﬁnd that
the “planar” and “component” stimuli are about equally detectable, whereas the
“plaid” stimulus is much less detectable. Efﬁciencies are plotted in ﬁgure 2.4b.
The results suggest that the human visual system has band-pass ﬁlters similar
to the “component” stimulus ﬁlter (and similar to V1 complex cells), and similar
to the “planar” stimulus ﬁlter, but not to partial tilings of the plane. Thus the
predictions of the ﬁxed detector model have been conﬁrmed, while the predictions
of the adaptive detector have been contradicted. Notice also that the information
normalization provided by the ideal observers is not superﬂuous. An attempt
to compare detection performance across stimuli in terms of common stimulus
measures (e.g. Fourier energy or power spectral height) for detectability would
lead to erroneous conclusions. In addition, note that the efﬁciencies on these
stimuli are about 10%, which is not extremely high (e.g.
 50% has been found
for some other detection tasks ) but do not rule out the model either. It is likely,
Rao, Olshausen and Lewicki: Probabilistic Models of the Brain
2001/05/02 19:33
Vision, Psychophysics and Bayes
however, that the model does not capture all the important elements of motion
detectors in the visual system (e.g. opponency).
From ideal observer to human observer model
The question remains how much of human motion detection can be accounted
for by the simple ﬁxed detector that pools over an entire plane. To address this
question, we can turn from looking at the ideal observer for each stimulus and
instead try to predict the inefﬁciencies in human performance using the ﬁxed
detector model.
To do this, we compared the detectability predicted by the ﬁxed model on a set
of ﬁve stochastic stimuli, three new and the “planar” and “plaid” stimuli above.
The new stimuli were created by passing spatio-temporal white noise through
three conﬁgurations of ﬁlters, illustrated in Fig. 2.5. The ﬁrst is a plaid signal,
similar to the plaid used above. The second is a “planar triplet”, created by adding
a component band to the plaid, in the same plane as the plaid, and the third is a
non-planar triplet, created by adding a component band out of the plane of the
plaid. Detection thresholds were measured using the same method as above. The
model predicts improved summation for the planar triplet, relative to the plaid,
but no improved summation for the non-planar triplet. We computed predictions
for the detection thresholds of each of the stimuli by implementing a speciﬁc ﬁxed
power dectector model. The detector optimally summed energy over the band of
frequencies contained in the planar stimulus from experiment 1. We assumed that
the output of this detector was corrupted by the internal noise levels estimated
from subjects’ detection thresholds for the component stimulus in experiment 1.
Figure 2.5 shows observer’s thresholds compared to the model predictions for the
ﬁve pattern stimuli used. Given the assumptions built into the model concerning
the exact spatio-temporal frequency band covered by the planar power detector,
the match is surprisingly good. That is, not only do the qualitative results follow
the predictions of the planar power detector model, but the quantitative results are
well ﬁt by a pre-deﬁned instantiation of the model (without ﬁtting the parameters
of the model to the data).
Although SDT worked well in the analysis of mechanisms of motion detection,
we need a theoretical framework for which the signals can be any properties of the
world useful for the visual behavior; for example, estimates of object shape and
surface motion are crucial for actions such as recognition and navigation, but they
are not simple functions of light intensity. Natural images are high-dimensional
functions of useful signals, and arriving at decoding functions relating image
measurements to useful signals is a major theoretical challenge. However, both
of these problems are expressible in terms of Pattern theory.
So, in the next section, we focus on the ﬁrst problem: How can we model the
Rao, Olshausen and Lewicki: Probabilistic Models of the Brain
2001/05/02 19:33
Vision, Psychophysics and Bayes
Threshold SNR
Figure 2.5: Threshold SNRs for detecting the ﬁve types of pattern stimuli replotted
from experiments 1& 2, where Plaid1 in the legend denotes the plaid from the ﬁrst
experiment and Plaid2 from the second. Plaid1 differs from Plaid2 in that its energy
is more diffusely spread over frequency. Black bands indicate the predictions of a
planar ﬁlter, based on subjects’ detection thresholds for the component stimulus
used in experiment 1.
computations that have to be solved? This modeling problem can be broken down
into synthesis: a) modeling the structure of pattern information in natural images;
and analysis, b) modeling the task and extracting useful pattern structures.
Bayesian models: Color constancy
Robust object recognition relies on the estimation of object properties that are approximately invariant with respect to secondary variables such as illumination
and viewpoint. Material color is one such property, and the phenomenon of perceptual color constancy is well-established (cf. for a Bayesian analysis). For
practical and scientiﬁc reasons, most laboratory studies of human color constancy
have been limited to simple ﬂat surfaces, or to the lightness dimension , the white side is seen as a white card, slightly tinted pink
from the reﬂected light. However, if the shape of the card is made to appear
as though the sides face away from each other (convex or “roof” condition),
the white card appears magenta–i.e. more saturated towards the red3. Bloj et al.
made quantitative measurements in which observers picked a comparison surface
whose color best matched the white side of the target (see ﬁgure 2.7).
Notice that this is a simpliﬁed version of the natural task of determining the
reﬂectance and illumination of an object in the presence of other objects. The
problem is interreﬂections. The interreﬂections can be modeled pairwise. Then the
color is determined by the illuminant, the reﬂectance functions of the surfaces, and
the conﬁguration of surfaces (geometry). Let us look at how to model the physics
given a pair of surfaces.
Look at the illustration of two surfaces in ﬁgure 2.6. When two surfaces are
concave with respect to a light source, the interreﬂections off of both surfaces
provide a secondary source of illumination with different characteristics for both
surfaces. As the angle between these surfaces decreases, the amount of interreﬂected light re-reﬂected off the other surface increases, and hence the spectrum
of the reﬂected light off both surfaces changes. On the other hand, as we increase
the inter-surface angle, the amount of inter-reﬂected light decreases until it reaches
zero at 90 degrees. Because of the perspective ambiguity in interpreting a folded
card as convex or concave, there are two interesting subcases of this continuum,
one where the angle is acute and one where the angle is obtuse. These two cases
are experimentally the most interesting because they yield completely different
shape and reﬂectance attributions to the observation that one of the two surfaces
Let’s see how to model the information for optimal inference. The primary
variable of interest is the reﬂectivity (
  ) (measured in units of chroma).
3. The apparent switch in shape from corner to roof can be accomplished either by using a pseudoscope, a pair of dove prisms which effectively reverse the
stereo disparities, or by using a monocular cue, such as linear perspective (see
 
Rao, Olshausen and Lewicki: Probabilistic Models of the Brain
2001/05/02 19:33
Vision, Psychophysics and Bayes
Generative models
Roof, pink surface &
illumination without inter-reflection
Corner, white surface &
illumination with inter-reflection
Figure 2.6: The white side of the folded card appears to either be pink or magenta,
depending on the assumed shape of the card, i.e. whether the card is concave like a
corner, or convex like a roof. The illumination model for a corner involves direct as
well as inter-reﬂections, whereas the illumination model for the roof interpretation
involves only direct lighting.
The likelihood is determined by either the one-bounce (corner) or zero-bounce
model (roof condition) of mutual illumination. They assume that the shape is ﬁxed
by stereoscopic measurements, i.e. condition on shape (
  
one-bounce model yields the intensity equation for surface 1 (the “white surface”):
Rao, Olshausen and Lewicki: Probabilistic Models of the Brain
2001/05/02 19:33
Vision, Psychophysics and Bayes
where the ﬁrst term represents the direct illumination with respect to the surface and the second term represents indirect (mutual) illumination due to light
reﬂected from the red side (surface 2) .
  is the form factor describing the
extent to which surface 2 reﬂects light onto surface 1 at distance
 from the vertex . The angles
 denote the angle between the surface normal and
the light source direction for surfaces 1 and 2 respectively.
For the zero-bounce generative model (roof condition):
These generative models provide the likelihood function. Observers do not sense
, but have a measure of color called chroma, modeled by the capture of the light
by the retinal cones followed by a transformation, which we will denote as a function
 denotes the three cone action spectra, and
 denotes inner product. Thus the likelihood of observation
being due to a color patch
 in the presence of some additive measurement noise is given by:
marginalize this conditional probability with respect to illumination direction and
space (i.e.
  ) assuming uniform priors, and assume a priori (built-in)
knowledge of the illuminant spectrum
 of daylight. Matching errors to the
patch are predicted by:
assuming a uniform prior on
is determined by the matching noise.
(See for details).
Experimental and theoretical matches are shown in ﬁgure 2.7. To a ﬁrst approximation, the separation and spread of the observers’ matches are predicted well by
an observer which is ideal apart from an internal matching variability determined
 . In other words, human matches are “ideal-like”.
There are a number of important points to be made here. Note that the surfaces
and lighting were carefully chosen to provide the bare minimal model for the natural visual task of inferring surface reﬂectance in the presence of interreﬂections.
This reduction in complexity is important in that it allowed a highly controlled
experimental test of the basic question. In addition, note that the one additional
noise parameter that was used to model the observer’s deviations from ideal is in
fact not a free parameter (i.e. it was not ﬁt to the data). Instead, the noise parameter
was estimated from a separate color matching experiment.
Rao, Olshausen and Lewicki: Probabilistic Models of the Brain
2001/05/02 19:33
Vision, Psychophysics and Bayes
Number of matches
Relative normalised probability
Number of matches
Relative normalised probability
Figure 2.7: Panel A shows the distribution of human subject matches to a ﬂat card,
neither roof nor corner condition. Variability of matches is modeled as the standard
deviation,
 of “matching noise”. Panel A shows the distribution of matches under
the two experimental conditions. Figure adapted from Bloj et al. Permission from
MacMillan needed.
Summary and conclusions
We have discussed the problem of developing and testing quantitative models
of human visual behavior. To that end we distinguished modeling function from
modeling mechanism. The ideal observer plays a key role for both levels as:
 The information normalization tool for tests of visual mechanism.
 The default model for functional models of natural vision.
In testing models of vision, we emphasized the fundamental role of the ideal
observer in interpreting human and model performance. The ideal observer provides a fundamental measure of the information available to perform a task, and
thus serves to normalize human performance relative with respect to the task.
Ideal observers can be used to deﬁne a task-independent measure of performance
(efﬁciency), provide a measure of the functional equivalence of models, and serve
as a default model to be modiﬁed by experiment. We described experiments of
Schrater et al. supporting neural mechanisms specialized for the measurement lo-
Rao, Olshausen and Lewicki: Probabilistic Models of the Brain
2001/05/02 19:33
Vision, Psychophysics and Bayes
cal image velocities, which are equivalent to speciﬁc sums of sets of complex cells
in cortical area V1 .
By assuming that vision can be described as a process of decoding scene properties from images, we can use the approach of Pattern Inference Theory to develop
ideal observers that serve as the starting point and comparator for a models of
functional vision. A key point is that the importance of modeling the generative
or forward process of natural image pattern formation (or encoding). The results
of Bloj et al. showed that the visual system takes into account knowledge of
inter-reﬂected light in determining surface color. ¿From this perspective, theories
of human visual performance can be developed iteratively from the ideal observer
down (cf. ). This may be a more tractable strategy than to build models of system function bottom-up from mechanism components.
Rao, Olshausen and Lewicki: Probabilistic Models of the Brain
2001/05/02 19:33
Vision, Psychophysics and Bayes
Rao, Olshausen and Lewicki: Probabilistic Models of the Brain
2001/05/02 19:33
References
Adelson, E. H. . Lightness Perception and Lightness Illusions. In M.
Gazzaniga, M. S. (Ed.), The New Cognitive Neurosciences(pp. 339-351). Cambridge, MA: MIT Press.
reduction of
Barlow, H. B. “A method of determining the overall quantum efﬁciency of
visual discriminations.”. J. Physiol. (Lond.). 160, 155- 168. 1962.
Bishop, C. M. . Neural Networks for Pattern Recognition. Oxford:
Oxford Univeristy Press.
Bloj, M. G., Kersten, D., & Hurlbert, A. C. . Perception of threedimensional shape inﬂuences colour perception via mutual illumination. Nature, 402, 877-879.
Kraft, J. M., & Brainard, D. H. . Mechanisms of color constancy under
nearly natural viewing. Proceedings of the National Academy of Sciences USA,
96, , 307-312.
Brainard, D. H., & Freeman, W. T. . Bayesian color constancy. J Opt Soc
Am A, 14, (7), 1393-411.
Burgess, A. E., Wagner, R. F., Jennings, R. J., & Barlow, H. B. . Efﬁciency
of human visual signal discrimination. Science, 214, 93-94.
Drew, M., & Funt, B. . Calculating surface reﬂectance using a singlebounce model of mutual reﬂection. Proceedings of the 3rd International Conference on Computer Vision Osaka: 393-399.
Eagle, R. A., & Blake, A. . Two-dimensional constraints on threedimensional structure from motion tasks. Vision Res, 35, (20), 2927-41.
Eckstein, M. P. . The lower efﬁciency for conjunctions is due to noise
and not serial attentional processing. Psychological Science, 9, 111-118.
Fisher, R. A. . Statistical Methods for Research Workers, Edinburgh:
Oliver and Boyd.
Freeman, W. T. . The generic viewpoint assumption in a framework for
visual perception. Nature, 368, 542-545.
Foley, J., van Dam, A., Feiner, S., & Hughes, J. . Computer Graphics
Principles and Practice, (2nd ed.). Reading, Massachusetts: Addison-Wesley
Rao, Olshausen and Lewicki: Probabilistic Models of the Brain
2001/05/02 19:33
REFERENCES
Publishing Company.
Geisler, W. “Sequential Ideal-Observer analysis of visual discriminations”.
Psychological Review. 96,(2), 267-314. 1989.
Green, D. M., & Swets, J. A. . Signal Detection Theory and Psychophysics. Huntington, New York: Robert E. Krieger Publishing Company.
Grenander, U. .General Pattern theory, Oxford Univ Press.
Grenander, U. . Elements of Pattern theory. Baltimore: Johns Hopkins
University Press.
Grzywacz, N. M. & Yuille, A. L. A model for the estimate of local image
velocity by cells in the visual cortex. Proc. Royal Society of London A, 239, 129–
161, .
Heeger, D. J. Model for the extraction of image ﬂow. J. Opt. Soc. Am. A,
4, 1455–1471, .
Kersten, D. . Spatial summation in visual noise. Vision Research, 24,
1977-1990.
Kersten, D. . High-level vision as statistical inference. In Gazzaniga, M.
(Ed.), The New Cognitive Neurosciences Cambridge, MA: MIT Press.
Kersten, D., & Schrater, P. W. . Pattern Inference Theory: A Probabilistic
Approach to Vision. In Mausfeld, R., & Heyer, D. (Ed.), Perception and the
Physical World(pp. Chichester: John Wiley & Sons, Ltd.)
Knill, D. C. . Discrimination of planar surface slant from texture: human
and ideal observers compared. Vision Res, 38, (11), 1683-711.
Knill, D. C. . Surface orientation from texture: ideal observers, generic
observers and the information content of texture cues. Vision Res, 38, (11),
Knill, D.C., and Richards, W. (Eds). . Perception as Bayesian Inference.
Cambridge University Press. .
Knill, D. C., & Kersten, D. K. . Ideal Perceptual Observers for Computation, Psychophysics, and Neural Networks. In Watt, R. J. (Ed.), Pattern
Recognition by Man and Machine(pp. 83-97). MacMillan Press.
Koch, C., & Segev, I. . Methods in Neuronal Modeling : From Ions to
Networks. Cambridge, MA: MIT Press, 671 pages.
Liu, Z., Knill, D. C., & Kersten, D. “Object Classiﬁcation for Human and Ideal
Observers”. Vision Research. 35,(4), 549-568. 1995.
Liu, Z., & Kersten, D. . 2D observers for human 3D object recognition?
Vision Res, 38, (15-16), 2507-19.
Mumford, D. . Pattern theory: A unifying perspective. In Knill, D. C.,
& W., R. (Ed.), Perception as Bayesian Inference(pp. Chapter 2). Cambridge:
Rao, Olshausen and Lewicki: Probabilistic Models of the Brain
2001/05/02 19:33
REFERENCES
Cambridge University Press.
Nakayama, K. Biological image motion processing: a review. Vis. Res.,
25, 625–660, .
Olshausen, B. A., & Field, D. J. “Emergence of simple-cell receptive ﬁeld
properties by learning a sparse code for natural images”. Nature. 381, 607-609.
Intelligent
Richards, W. E. . Natural Computation. Cambridge, Massachusetts:
MIT Press.
B. Ripley. “Pattern Recognition and Neural Networks”. Cambridge University Press. 1996.
Schrater, P. R., Knill, D. C., & Simoncelli, E. P. . Mechanisms of visual
motion detection. Nature Neuroscience, 1, 64 - 68.
Schrater, P. . Local Motion Detection: Comparison of Human and Ideal
Model Observers. Ph.D. thesis, Philadelphia: University of Pennsylvania.
Schrater, P. R., & Kersten, D. . Statistical Structure and Task Dependence
in Visual Cue Integration. Workshop on Statistical and Computational Theories of Vision – Modeling, Learning, Computing, and Sampling, Fort Collins,
Simoncelli, E. P., Adelson, E. H., & Heeger, D. J. . Probability Distributions of Optical Flow. Mauii, Hawaii: IEEE Conf on Computer Vision and Pattern
Recognition.
Simoncelli, E. P. . Distributed Analysis and Representation of Visual
Motion. Ph.D., Cambridge, MA: Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science,
Simoncelli, E. P. . Statistical Models for Images: Compression, Restoration and Synthesis. Paciﬁc Grove, CA.: IEEE Signal Processing Society.
Simoncelli, E. P. & Heeger, D. A model of neuronal responses in visual area
MT. Vis. Res., 38, 743–761, .
Tjan, B., Braje, W., Legge, G. E., & Kersten, D. . Human efﬁciency for
recognizing 3-D objects in luminance noise. Vision Research, 35, (21), 3053-
Watson, A. B., Barlow, H. B., & Robson, J. G. . What does the eye see
best? Nature, 31,, 419-422.
Weiss, Y., & Adelson, E. H. . Slow and smooth: a Bayesian theory for the
combination of local motion signals in human vision (A.I. Memo No. 1624).
Yuille, A. L., & B¨ulthoff, H. H. . Bayesian decision theory and psychophysics. In D.C., K., & W., R. (Ed.), Perception as Bayesian Inference(pp.
Cambridge, U.K.: Cambridge University Press.
Rao, Olshausen and Lewicki: Probabilistic Models of the Brain
2001/05/02 19:33