Intl. Journal on Cyber Situational Awareness, Vol. 1, No. 1, 2016
Copyright © 2016 C-MRiC.ORG
Detecting bots using multilevel traffic analysis
Matija Stevanovic and Jens Myrup Pedersen
Department of Electronic Systems, Aalborg University
Aalborg, Denmark
Botnets, as networks of compromised “zombie” computers, represent one of
the most serious security threats on the Internet today. This paper explores
how machines compromised with bot malware can be identified at local and
enterprise networks in accurate and time-efficient manner. The paper
introduces a novel multi-level botnet detection approach that performs
network traffic analysis of three protocols widely considered as the main
carriers of botnet Command and Control (C&C) and attack traffic, i.e. TCP,
UDP and DNS. The proposed method relies on supervised machine learning
for identifying patterns of botnet network traffic. The method has been
evaluated through a series of experiments using traffic traces originating
from 40 different bot samples and diverse benign applications. The
evaluation indicates accurate and time-efficient classification of botnet
traffic for all the three protocols as well as promising performance of
identifying potentially compromised machines. The future work will be
devoted to the optimization of traffic analysis and correlation of findings
from three analysis levels in order to increase the accuracy of identifying
compromised clients within the network.
Keywords: Botnet, Botnet Detection, Traffic Analysis, Traffic Classification,
MLAs, Random Forests, Client analysis.
INTRODUCTION
Botnets are one of the most serious threats to Internet security and one of the
most challenging topics within the field of network security today. Botnets
represent a usually large collections of computers compromised with a
sophisticated malware that puts them under the control of a remote attacker
 . The compromised computers are often referred to as
“bots” while the attacker is referred to as the “botmaster”. Contrary to other
more conventional malware types, such as viruses, trojans and worms,
botnet malware has an advantage of being able to communicate with an
attacker through a Command and Control (C&C) communication channel.
Botnets deploy C&C channel using a variety of communication protocols,
such as: IRC, HTTP/HTTPS and P2P protocols. Additionally, modern
botnets use many resilience techniques that make C&C channel more
resilient to detection such as encryption, protocol obfuscation, Fast-flux and
DGA (Domain Generation Algorithm) . Using the C&C
channel, the botmaster can remotely control the behaviour of bots, turning
them into highly distributed platform for the implementation of a wide range
of malicious and illegal activities, such as: sending SPAM e-mails,
Distributed Denial of Service (DDoS) attacks, information theft and
malware distribution.
As botmasters are relying on network traffic for the communication with
bots and the implementation of attack campaigns, many detection
approaches are targeting botnets using network traffic analysis. During the
last decade an abundance of detection approaches have been proposed
relying on diverse principles of network traffic analysis . One of the latest classes of detection approaches
employs machine learning algorithms (MLAs) for identifying anomalous
botnet traffic . These approaches are often seen as
the state-of-the-art detection approaches as they promise accurate and
automatized detection of botnet traffic patterns. The contemporary machine
learning-based detection approaches are using different MLAs most
commonly supervised MLAs for classifying network traffic as malicious or
benign. These approaches target botnets at different points in the network,
they are based on different principles of traffic analysis and they are
developed and evaluated using diverse traffic data sets . In this paper we extend our previous work on network traffic
classification for botnet detection in order to
develop accurate and robust detection of compromised clients based on
network traffic classification. Our goal is to develop a detection method that
will provide identification of potentially compromised client machines while
minimizing the number of false positives thus limiting the need for
extensive operators’ involvement in the process of evaluating raised alarms.
We propose a novel multi-level botnet detection method by relying on three
traffic classification methods targeting three protocols widely considered as
the carriers of botnet network activity namely TCP, UDP and DNS. The
proposed method is developed to address some of the pitfalls of using
network traffic classification for botnet detection. First, we use supervised
machine learning as the algorithm of traffic analysis that can provide
automatized detection of botnet traffic by inferring the knowledge about the
botnet traffic patterns from already available network traces. We rely on
Random Forests classifier for providing accurate classification of botnet
traffic. Second, the proposed methods target TCP, UDP and DNS as the
main carriers of botnet C&C communication and attack traffic. Contrary to
some of the existing work we develop a classifier for each of the protocols
in order to obtain more precise analysis and ultimately more accurate
classification. Third, we propose the use of novel feature sets for
representing traffic instances within the classifiers. The traffic features are
carefully chosen in order to capture the main traits of botnet network
activity. Fourth, we evaluate the proposed method using one of the most
comprehensive data sets of botnet network traces thus providing a thorough
evaluation of classification performance and the capabilities of identifying
compromised clients. Finally, we target bots at local and enterprise networks
as we are able to obtain reliable training data on botnet traffic by relying on
honeypots and malware testing environments.
The rest of the paper is organized as follows. Section 2 presents an overview
of related work. Section 3 introduces multi-level botnet detection method
based on TCP, UDP and DNS traffic analysis. Section 4 presents the results
of performance evaluation for the proposed botnet detection method.
Section 5 discusses presented results and possibilities for future work.
Finally, Section 6 concludes the paper.
BACKGROUND
Botnet detection based on network traffic classification is one of the latest
and the most promising classes of botnet detection approaches. The main
assumption behind these approaches is that botnets create distinguishable
traffic patterns that could be accurately detected using supervised MLAs.
Over the last couple of years, a number of detection approaches that rely on
traffic classification have been proposed . Some of
the most prominent approaches were proposed by Strayer et al. ,
Masud et al. , Saad et al. , Zhao et al. , Shin et al.
 , Bilge et al. , Perdisci et al. , Haddadi et al.
 and Antonakakis et al. .
Based on the point of traffic monitoring contemporary detection methods
can be coarsely classified as ones implemented closer to client machines
 and ones implemented further away
from clients in higher network tiers . Detection approaches that
analyse traffic further away from clients are able to capture some of the
fundamental properties of botnet operation such as group behaviour and
synchronicity of compromised machines. However, this scenario also has a
number of limitations such as difficulty of processing high volume of traffic
and identifying compromised clients due to the use of NAT (Network
Address Translation). As a result, approaches implemented in the higher
network tiers usually target either sampled traffic such as NetFlow or DNS traffic that represent only a fraction of total traffic . In contrast
detection approaches implemented closer to client machines commonly
process smaller amount of traffic often providing more detailed traffic
analysis that can capture finite patterns of botnet network activity.
The contemporary detection approaches employ diverse principles of traffic
analysis thus having different detection scope and capabilities. Network
traffic is commonly analysed by observing traffic “flows” that encompass
both TCP and UDP traffic . Other approaches target
DNS traffic by analysing it either between local client and resolver or above the resolver in upper DNS hierarchies . DNS traffic is analysed
using different perspectives where some approaches classify Fully Qualified
Domain Names (FQDNs) based on the features extracted from DNS queries
and responses while others
classify domain clusters . Regarding the traffic features
used for representing TCP and UDP flows some authors such as Masud et
al. use features dependent on content of packet payload thus
violating integrity of end users’ data and being vulnerable to payload
encryption. Other approaches use
features extracted from the client machines thus requiring the access to
client machines under monitoring. Some authors consider using IP addresses as features thus violating end users’
privacy and potentially introducing bias in the data set which could lead to
over-optimistic performance. Different approaches target different botnet
network activities where some approaches identify
C&C communication, while others cover all botnet network activity.
Finally, the approaches used various supervised MLAs for the classification
task, while for the majority of the approaches tree classifiers have shown the
best performance .
The existing botnet detection methods are developed and evaluated using
various data sets of botnet and benign traffic . The
used data sets are often sparse consisting of only a handful of botnet traces
that are obtained in not-transparent way. Furthermore, the approaches often
rely on data sets that are artificially formed by overlaying and merging data
sets recorded at different monitoring points in network. Finally, some
approaches use third party labelling solutions for forming the “ground truth”
on botnet traffic thus putting the reliability of the training data in question.
In this paper we introduce a novel multi-level botnet detection approach that
is able to identify compromised machines at local and enterprise networks.
The method builds on our previous work on botnet traffic classification
 by further developing TCP, UDP and DNS traffic
classifiers and introducing client analysis entity that is able to pinpoint
malicious clients based on results of the three classifiers. The proposed
approach analyses TCP, UDP and DNS traffic separately in order to provide
more accurate detection. We believe that due to the different nature of TCP
and UDP (first connection-oriented and second connectionless) they should
be classified using separate classifiers where additional traffic features for
TCP traffic would be used. Furthermore, we believe that DNS traffic
analysis is crucial as many botnets rely on it for discovering addresses of
C&C infrastructure or victims of attack campaigns. The three methods are
based on Random Forests classifier as a capable ensemble classifier. We
choose Random Forests classifier based on good performance in classifying
botnet traffic reported by several studies and confirmed by our previous work on traffic classification for
botnet detection . For the realization of the
classifiers we defined traffic features that should successfully capture the
traits of botnet network activity. For TCP and UDP we rely only on features
extracted from the packets headers without using IP addresses as features.
This way we avoid violating privacy of the end users and over optimistic
classification performance due to the bias introduced by using IP addresses
as features. For DNS traffic analysis we classify FQDNs based on the
features extracted from DNS queries and responses. The proposed methods
do not make any assumptions about the botnet traffic, observing both C&C
and attack traffic. Finally, for the development and evaluation of the
detection methods we use data obtained by several honeypots and malware
testing environments thus having greater confidence in obtained data sets,
and the ground truth on botnet traffic used in our work. For the evaluation
we use traces from 40 botnets and numerous benign applications.
Comparing to other methods we use one of the
most extensive botnet data sets which should contribute to higher
confidence in reported classification performance.
DETECTION METHOD
In order to identify bots at local and enterprise networks we propose a multilevel detection approach that identifies compromised client machines by
classifying network traffic as malicious or benign using supervised machine
learning, as illustrated in Figure 1.
TCP, UDP and
DNS traffic
Processing
Training phase:
TCP, UDP and
DNS traffic are
modeled using
training instances.
Extraction and
selection of TCP,
UDP and DNS traffic
Classification
The models are
tested using
testing instances.
Classifier
Identification of
potentially
compromised
Classification
Training phase
Testing phase
Network tap /
Traffic trace
Labeled data set
Figure 1. A botnet detection method based on multi-level traffic analysis
The system analyses network traffic on three levels by analysing TCP, UDP
and DNS traffic produced by monitored clients. The system consists of three
main components: Processing entity, Classifier entity and Client Analysis
entity. The first entity performs processing of network traffic observed from
either live network or existing traffic trace. This entity processes traffic so
traffic instances for three analysis levels are extracted and characterized
with a set of statistical features. The extracted traffic instances are then
enriched using GEO and WHOIS information. The second entity is the
Classifier that is in charge of building the model of malicious and benign
traffic using training data and the classification of newly observed traffic
instances. The third and the final entity of the system is Client Analysis
entity that performs client analysis by correlating classification results from
three level of analysis in order to generate report on potentially
compromised clients within the monitored network. As the detection
approach relies on supervised machine learning it operates in two phases i.e.
training and test phase. During the training phase traffic models are trained
using a labelled training data while in the test phase the previously
generated models are tested by unlabelled test data.
The proposed method processes traffic in time windows, where at the end of
each consequent time window traffic instances for TCP, UDP and DNS
traffic are extracted. This way the normalization of traffic is performed by
taking “snapshots” of traffic which results in the possibility of using diverse
network traffic traces for development and evaluation of the method. The
method observes traffic between local and remote clients (R2L, L2R) as
well as local to local traffic (L2L), while all multicast and link-logical traffic
are discarded.
TCP and UDP traffic analysis
TCP and UDP traffic are analysed from the perspective of bidirectional
transport layer conversations that are defined as traffic exchanged between
source and destination IP addresses on certain source and destination ports.
For each TCP and UDP conversation we extract a set of statistical features
that capture botnet traffic heuristics. Furthermore, we perform enrichment of
the extracted features using external GEO location services. The features
extracted for TCP and UDP conversations are presented by Table 1. It
should be noted that UDP traffic analysis is realized by omitting UDP
conversation that facilitate DNS traffic (i.e. UDP port 53). This is done as
both malicious and benign DNS query-response pairs correspond to UDP
conversations with similar characteristics.
For UDP conversations we extract a series of traffic features that can be
divided in four groups i.e. Basic conversation features, Geographical
features, Time-based features and Bidirectional features (25 features in
total). For TCP conversation in addition to the four groups of features we
also extract TCP specific features (18 features in total). Basic conversation
features cover the basic statistics of TCP/UDP conversations. These features
are able to capture the traffic that uses unusual ports associated with P2P
communication. Furthermore, they can capture heavy traffic and brute force
attacks by considering the number of packets and their size. Geographical
features indicate geographical locations for remote IPs contacted by local
machines. This features should capture tendencies of some countries to be
more often associated with cyber-criminal than others. Time-based features
describe the rate of transferring the data thus being able to describe the brute
force attacks as well as the periodicity of botnet traffic. Bidirectional
features take in consideration differences in communication between the
two directions of communication indicating any unbalanced communication
that can usually be associated with the attacks and communication between
the botmaster and bots. TCP specific features capture events in regards to
establishing and maintaining TCP conversations. Keeping the track of these
events can indicate suspiciously high number of unsuccessful TCP attempts
that usually characterize botnets communication due to often interrupted and
unavailable botnet infrastructure. Also these features are able to capture
TCP-based brute force attacks such as SYN floods, SYN ACK floods, ACK
floods, ACK PUSH floods and RST and FIN floods by keeping track of
TCP flags distribution.
Table 1. TCP/UDP traffic analysis: the list of features extracted for TCP
and UDP conversations.
Basic conversation features
Port number
Layer 7 protocol
Categorical
Duration (last pkt - first pkt)
Total number of packets
Total number of Bytes
Mean of the number of Bytes per packet
Std of the number of Bytes per packet
Geographical features
Remote IP country
Categorical
Remote IP continent
Categorical
Time-based features
Number of packets per second
Number of Bytes per second
Mean of packets inter-arrival time
Std of packets inter-arrival time
Bidirectional features
Ratio of number of packets OUT/IN
Ratio of number of Bytes OUT/IN
Ratio of inter-arrival times OUT/IN
TCP specific features
Number of three way handshakes
Number of connection tear downs
Number of complete conversation
Average conversation duration
Categorical
Percentage of TCP SYN packets
Percentage of TCP SYN ACK packets
Percentage of TCP ACK packets
Percentage of TCP ACK PUSH packets
Percentage of TCP FIN packets
Percentage of TCP RST packets
DNS traffic analysis
DNS traffic analysis is implemented by observing DNS query-response
pairs for queried FQDNs. For each of the queried FQDN we extract a
number of statistical features and after the enriching process using GEO and
WHOIS services 37 features are selected, as presented in Table 2. The
selected features belong to four groups i.e. FQDN-based features, Querybased features, Response-based features and Geographical location features.
1 Some features are calculated for both directions of the conversation while
others are unique for the particular conversation.
Table 2. DNS traffic analysis: features extracted for DNS query-response
FQDN-based features
Number of tokens
Avg length of token
Length of SLD (Second Level Domain)
Length of Domain
Language of SLD
Categorical
Entropy (range of characters) for SLD
Distance from n-grams of legitimate domains
(alexa.com) for SLD
Distance from n-grams of dictionary words
domains for SLD
Number of dictionary words in SLD
Ratio of numerical characters in SLD
Ratio of vowels in SLD
Ratio of consonants in SLD
Number of dictionary words in domain
Ratio of numerical characters in domain
Ratio of vowels in domain
Ratio of consonants in domain
Query-based features
Type of query
Categorical
Number of queries
Mean of query length
Std of query length
Mean of queries inter-arrival time
Std of queries inter-arrival time
Response-based features
Number of query responses
Mean of query response length
Std of query response length
Mean of query responses inter-arrival time
Std of query response inter-arrival time
Number of NOERROR responses
Number of NXDOMAIN responses
Avg number of answers
Avg number of authority answers
Avg number of additional answers
Avg number of resolved IPs
Mean of the value of TTL (Time-To-Live) field Numerical
Std of the value of TTL field
Geographical features
Number of countries resolved IPs belong to
Number of ASs resolved IPs belong to
FQDN-based features quantify lexical properties of domain names in order
to differentiate between human-memorable domains and “unusual” domains
such as pseudorandom domains that commonly characterize Domain-Flux
(DGA). Query-based features describe the way how the FQDN was queried
capturing any irregularities such as high number of queries and periodicity
of querying certain domain. Response-based features capture characteristics
of the query responses. These features cover a number of botnet DNS
characteristics such as the number of NXDOMAIN responses that can
indicate botnet domains that have been taken down, the value of TTL field
that can characterize Fast-flux and etc. Finally, Geographical location
features capture the characteristics of IPs resolved for queried FQDNs.
These features can indicate if the IPs are hosted over a high number of
countries or Autonomous Systems (ASs) which is often associated with
malicious hosting strategies such as Fast-flux.
Classifier entity
For the task of classifying traffic within all three traffic analysis methods we
use Random Forests classifier . Random Forests represent
an ensemble learning method used for classification, that operate by
constructing a multitude of decision trees at training time and outputting the
class that is the result of majority vote from the individual trees. Random
Forests are developed in order to correct overfitting as the common
drawback of decision trees. The method combines “bagging” concept and
the random selection of features, in order to construct a collection of
decision trees with controlled variance. For the implementation of the
classifiers within the proposed traffic analysis methods we use 10 trees
where at each node 𝑙𝑜𝑔2
𝑛+ 1 features, where n is the total number of
features, that are used for growing the tree.
Client analysis entity
Client analysis entity has a goal of identifying potentially compromised
computers based on the results of the three classifiers. This entity correlates
findings of the three classifiers in order to provide a report on potentially
compromised clients within the monitored network. The client analysis
operates as illustrated in the Figure 2. Traffic instances identified as
malicious by TCP, UDP and DNS classifiers i.e. alerts are filtered in order
to eliminate false positives. The filtering is done by analysing characteristics
of alerts based on IP and domain whitelisting and geographical analysis.
Filtered alerts are then forwarded to a simple decision making process
where client is deemed malicious if there is at least one alert for it.
Filtered alerts
Preliminary decision
Malicious traffic
instances i.e. alerts
Figure 2. Client analysis: Whitelisting and Geolocation analysis.
For TCP and UDP traffic, the filtering of alerts is based on the Autonomous
Systems (AS) to which remote IPs belong to. The implemented whitelisting
excludes all conversations that involve remote IPs that are hosted by
trustworthy providers such as Microsoft, Oracle, Apple, etc., as these
providers often have tough hosting policies and there is much smaller
chance that they would be used for hosting C&C infrastructure. After
performing the filtering based on AS, we filter clients for which alerts
include conversations to remote IPs hosted in less than 2 counties. This is
done as C&C infrastructure is commonly hosted using IP addresses located
in different countries, in order to achieve resiliency.
For DNS traffic we rely on similar process of filtering alerts. First, DNS
alerts are filtered using domain whitelist. For domain whitelisting we use the
first 10,000 most popular domains according to alexa.com. Remaining
alerts are then scrutinized based on the number of countries hosting the
domain and the number of NXDOMAIN responses. We filter out clients
whose alerts include domains that are on average hosted by less than 2
countries or for which at all responses are of NOERROR type. The
reasoning behind this decision process is that cyber-criminals often rely on
Fast-flux hosting strategies that use many IP addresses typically distributed
over the world. Furthermore, botnets are commonly associated with high
number of unsuccessful queries due to C&C infrastructure often being taken
As already mentioned the proposed method will mark client malicious if
there is at least one alert after the filtering. Thereby, in order for client to be
identified as malicious the approach needs to generate at least one alert
irrelevant of type. This is because not all client would produce TCP, UDP
and DNS traffic. The preliminary decision formed this way is presented to
operator together with a report for each of the potentially compromised
clients indicating a number of TCP and UDP conversations and DNS queryresponse pairs that are classified as malicious for the particular client
machine. In addition, the method also provides overview of the basic
characteristics of traffic instances marked as malicious. The generated
reports are presented to the operator in order to make a final evaluation of
the preliminary conclusions generated by the approach.
EXPERIMENTS AND DETECTION RESULTS
In this section, we evaluate the performance of the proposed detection
method by analysing the performance of classifying TCP, UDP and DNS
traffic and capabilities of identifying potentially compromised machines.
The presented analysis methods are fully implemented in Python, by relying
on scikit-learn Python machine learning library for implementing
Random Forests classifiers. The experiments are done off-line using prerecorded data sets consisting of a number of malicious and benign traffic
traces in the form of pcap files. All experiments were done using an offthe-shelf computer with Intel Core i7 at 3.4 GHz and 16GB of RAM
memory. It should be noted that during the operation the method did not use
more than 8GB of RAM. Finally, current implementation assumes IPv4
traffic but it should be noted that there are no obstacles in using the method
on IPv6 traffic.
For the evaluation, we use several malicious and benign traffic data sets.
Benign data sets present traffic traces recorded at several LAN
environments, while malicious data sets include traffic traces recorded by
several honeypots and malware testing environments.
Benign data sets used for evaluation:
UPC data set - represents benign traffic
generated for the purpose of evaluating DPI tools. The traffic is
recorded at small local network consisting of 3 machines over the
course of 2.5 months. Data set includes traffic from various benign
applications such as web browsing, torrent clients, FTP clients etc.
ISCX data set - represents data set that is
generated in order to evaluate intrusion detection systems (IDS). The
data set represent 7 days of trace from specially deployed network
environment with 20 client machines. From this data set we use
benign traffic from the first, the fifth and the sixth day of the trace.
Malicious data sets used for evaluation:
ISOT data set - 5 network traces obtained from 3
different P2P botnets. These traces were obtained by French chapter
of Honeypot project and they include traffic that covers both C&C
communication and attack campaigns.
ISCX data set - From this data set we used trace
of traffic produced by ISCX IRC botnet. The trace includes both C&C
communication and attack phase of botnet operation.
MCFP data set - 15
traces. The data set covers traffic produced by executing different
malware for an extended period of time. The traces cover both C&C
communication and botnet attack campaigns.
Contagio data set - 14 traces. The data set covers
traffic produced by executing different bot malware in a malware
testing environment. The traces primarily cover initial bootstrapping
procedure, while some of them also include other phases of the botnet
operation.
HoneyJar data set - 5 traces generated using HoneyJar - a malware testing environment deployed by researchers at
Aalborg University with a goal of secure, automated and contained
experimenting with malware. The traces primarily cover initial
bootstrapping procedure performed by bots.
The benign and malicious data sets are summarized in Tables 3 and 4,
respectively. The presented numbers of TCP, UDP conversations and DNS
query-response pairs are taken over the total duration of the traces.
Table 3. The summary of benign traffic data sets.
Traffic trace
of packets
conversations
conversations
UPC trace #1
UPC trace #2
UPC trace #3
ISCX trace #1
ISCX trace #2
ISCX trace #3
Table 4. The summary of botnet traffic data sets.
Traffic trace2
conversations
conversations
Storm SMTP (i)
Storm UDP (i)
Waledac (i)
ZeusCnC (i)
IRC botnet (is)
Botnet 42 (m)
Botnet 43 (m)
Botnet 44 (m)
Botnet 45 (m)
Botnet 46 (m)
Botnet 47 (m)
Botnet 48 (m)
Botnet 49 (m)
Botnet 51 (m)
Botnet 52 (m)
Botnet 53 (m)
Botnet 54 (m)
Botnet 90 (m)
Botnet 91 (m)
Botnet 92 (m)
Blackhole (c)
Cutwail (c)
Pushdo (c)
Dirtjumper (c)
Kelihos (c)
Kuluoz (c)
Purplehaze (c)
Sality (c)
ZeroAccess (c)
Sirefef (c)
Gameover (c)
Agobot (h)
Batimal (h)
Palevo (h)
2 (m) - MCFP trace, (c) - Contagio trace, (h) - HoneyJar trace, (i) - ISOT
trace, (is) - ISCX trace
Evaluation procedure
As already elaborated in the previous section, the main goal of the
experiments is to evaluate the performance of traffic classification
performed by different levels of traffic analysis and to evaluate the
capabilities of identifying compromised clients using the proposed method.
4.2.1. Evaluation of traffic classification on different levels
of traffic analysis
We evaluate performance of classifying malicious and benign traffic using
“batch” analysis where we use all available data sets and evaluate the
performance of classification using 10-fold cross validation scheme. For the
classification of TCP and UDP conversations we vary the length of time
window and the number of packets processed per conversation, in order to
find out the “optimal” value of the two parameters. Similarly, for the DNS
traffic analysis we vary the length of time window examining influence of it
on classification performance. We do not vary the number of processed
DNS queries and responses within the time window as we would like to
fully capture time relations of DNS queries.
The classification performance is characterized by performance metrics that
described both accuracy and time requirements of traffic classification. The
accuracy is expressed by following metrics:
Precision: 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛=
Recall: 𝑟𝑒𝑐𝑎𝑙𝑙=
Where TP, FP and FN are number of true positives, false positives and false
negatives, respectively. Time requirements of classification are quantified
by time used for the training and the testing i.e. classification.
4.2.2. Evaluation of capabilities of identifying potentially
compromised clients
This scenario is evaluating performance of identifying compromised clients
by the proposed detection system. The goal of this scenario is to illustrate
the capabilities of the proposed approach to identify compromised
machines. In order to do so we train the proposed approach with one part of
the data set and test it against another disjoint data set. For training we used
all data sets except for the Botnet 90 trace and ISCX day #1 data sets, as
they will be used for testing. Botnet 90 data set is chosen as it is the trace
that contains the highest number of compromised clients from all malicious
traces considered i.e. 11 in total. ISCX day #1 on the other hand represent a
significant non-malicious data set that captures traffic from more than 20
non-malicious clients. This evaluation scenario is realized for the optimal
values of time window and the number of packets per conversation inferred
from the evaluation of the three classifiers.
The performance of identifying malicious clients is characterized by number
of true positives, false positives, true negatives, false negatives as well as
false positive rate (FPR), where FPR is defined as follows:
False positive rate: 𝐹𝑃𝑅=
Results of experiments
The results of the evaluation are presented by Figures 3-8 that illustrate the
results of classifying TCP, UDP and DNS traffic and Tables 5-7 that
illustrate the capabilities of identifying compromised machines.
4.3.1. TCP, UDP and DNS traffic classification
Figures 3 and 4 show the performance of TCP traffic classification while
Figures 5 and 6 show the performance of UDP traffic classification. The
results are produced by varying the length of time window and the number
of packets processed per TCP/UDP conversation. Figures 7 and 8 illustrate
the performance of DNS traffic classification, for various lengths of time
window. Within the experiments we used 4 values of the time window
length i.e. 300, 600, 1800 and 3600 seconds and 4 different values of the
maximum number of packets per TCP/UDP conversation i.e. 10, 100, 1,000,
10,000 packets.
The results presented in Figure 3 indicate that for TCP traffic classification
increasing number of packets processed per conversation and increasing the
length of the time window enhance the classification performance. While
increasing the length of time window brings modest improvements in
classification performance, increase in the number of observed packets has a
much bigger influence. Therefore, the number of packets analysed per
conversation is crucial for improving the performance of TCP classification.
Furthermore, from the results show that for more than 1000 packets and the
length of time window of more than 300 seconds performance peak. Under
these conditions the classification of malicious traffic is characterized with
precision and recall with values greater than 0.995 and 0.982 respectively,
while classification of benign traffic has even better performance with
precision and recall higher than 0.995. Results presented in Figure 4 indicate
that classifier requires a little bit less time to be trained when a longer time
window is used. This can be explained due to the fact that shorter time
window brings more training and test instances, which require more time to
be processed. However, it should be noted that the testing time is not
influenced by this. Moreover, it should be noted that used Random Forests
classifier with 10 trees performed very well in the sense of time
requirements, taking less than 120 seconds to be trained and to perform the
classification.
Figure 3. Classification results for TCP traffic.
Figure 4. Time requirements for TCP traffic classification: Training and
Testing time.
The results of UDP traffic classification are illustrated in Figure 5, showing
constant classification performance of UDP traffic for different number of
packets per conversation. Furthermore, the length of time window does not
have significant influence on classification results. For time window equal
to 3600 seconds and 10 packets per conversation precision and recall have
values of above 0.995 and 0.985 while for benign traffic we have nearly
perfect classification with precision and recall with values higher than 0.998
and 0.999 respectively. Figure 6 shows time requirements of UDP traffic
classification. The results follow the same trends as in the case of TCP
classification. The time needed to perform training and classification is less
than 5 seconds, which is significantly lower than in case of TCP traffic due
to the smaller number of UDP instances within our data set and smaller
feature set used for representing them.
Figure 5. Classification results for UDP traffic.
Figure 6. Time requirements for UDP traffic classification: Training and
Testing time.
The results for DNS traffic are presented in Figures 7 and 8. The figures
show that the performance of classification slightly degrade with increasing
the size of time window. Overall, DNS classification has shown the
performance comparable with TCP and UDP classifiers having the precision
and recall for both malicious and benign traffic higher than 0.985 and 0.975
respectively. Time requirements follow the same trend as in cases of TCP
and UDP traffic, where for training and classification the classifier requires
less than 17 seconds.
Figure 7. Classification results for DNS traffic.
Figure 8. Time requirements for DNS traffic classification: Training and
Testing time.
4.3.2. Identifying compromised clients
Tables 5, 6 and 7 illustrate the performance of the proposed system in
identifying potentially compromised clients i.e. bots based on the traffic
classification. Table 5 shows the results of identifying compromised client
based on TCP, UDP or DNS traffic analysis. Table 6 illustrates results when
all three levels of analysis are used. Finally, Table 7 represent an example of
report produced for a client in the network. The results were obtained for
using analysis window of 600 seconds and maximally 10,000 packets per
conversation as for these parameters all three classifiers perform well.
The results of identifying compromised clients using only one of the traffic
analysis levels are presented in Table 5. The table illustrates low FP and FN
for each analysis levels. The results also show that different levels of traffic
analysis are able to discover different number of clients indicating the
potential of correlating findings from different levels of analysis. Table 6 on
the other hand illustrates results of identifying compromised clients when all
three traffic analysis levels are used. In this case all malicious client
machines were identified while only 2 benign clients are deemed malicious,
accounting for false positive rate (FPR) of 0.0435. The number of falsely
identified machines is low and can be easily filtered out by a network
Table 5. Results of identifying malicious clients based on TCP, UDP and
DNS analysis.
Table 6. Results of identifying malicious clients based on all three levels of
traffic analysis.
For each client evaluated by the system i.e. for both malicious and benign
machines the proposed approach outputs a report. The report provides a
preliminary decision on the nature of client machine (i.e. Malicious or
Benign), the number of TCP, UDP and DNS traffic instances indicated as
malicious as well as a brief description of each traffic instances marked as
malicious by the three classifiers. Table 7 illustrates example of report for a
malicious client machine from the testing trace. The report provides the
operator with a sufficient amount of information so a qualified decision on
the nature of client can be made.
Table 7. An example of report provided by the method.
192.168.1.236
Classification results
Detected: 534
Total: 572
Detected: 33
Detected: 98
Total: 167
Destination IP
Protocol Country
192.168.1.236
192.168.1.236
192.168.1.236
192.168.1.236
65.55.88.22
212.135.6.24
217.12.11.64
212.108.64.65
Destination IP
Protocol Country
192.168.1.236
192.168.1.236
192.168.1.236
192.168.1.236
124.83.81.49
82.79.244.40
201.250.155.47
77.77.16.211
xnkuvnmui.ws
trlvwluyjtu.cc
dgghem.org
DISCUSSION
This section discusses the characteristics of the proposed detection method
outlining its capabilities and limitations and elaborating on possibilities for
future work.
Principles of operation
The proposed method targets machines compromised with bot malware at
local and enterprise networks by detecting if they are associated with any
malicious TCP, UDP and DNS traffic. In order to identify malicious traffic,
we develop three classifiers that capture characteristics of malicious botnet
network activity. The classifiers are based on a capable Random Forests
classifier. Results of the classification are scrutinized by the client analysis
entity in order to identify any malicious clients in the network.
The proposed method is able to identify compromised clients if they are
producing traffic on any of the three protocols. Furthermore, as the proposed
method is trained using data sets that include all phases of botnet operation
i.e. C&C communication and attack traffic, the proposed system is
independent from bot operational phase. Finally, we argue that the proposed
approach can be even more reliable if additional data sets of malicious and
benign traffic are used for training the classifiers.
For the three levels of traffic analysis we rely on feature sets that are
specially developed to encompass characteristics of botnet network activity.
We argue that chosen feature representation successfully captures a large
subset of botnet traffic characteristics, thus avoiding common pitfall of
tailoring feature representations that do not generalize well.
Traffic classifiers used for classifying of TCP, UDP and DNS traffic are
based on Random Forests classifier with only 10 trees. This opens the
potential of experimenting with much larger forests in order to further
improve classification performance. The use of additional trees leads to
approximately linear increase in training time but based on the results
presented in Section 4 there is space for increasing time consumption for the
sake of improving accuracy.
For the realization of classifiers, we have considered different lengths of
analysis window and the number of packets per conversation. The length of
analysis window directly affects how promptly can alerts be raised and
compromised clients can be identified. Therefore, for the client
identification evaluation we chose time window of 600 seconds as it
provides balance in the performance of the three classifiers. Future work
could consider experimenting with other sizes of time windows depending
on the requirements of the actual detection system.
Finally, one of the crucial elements of the proposed method is the client
analysis entity i.e. the way in which the approach implements the correlation
of findings from the three levels of traffic analysis. The current method is
based on filtering false alarms based on the fundamental traits of botnet
operation. We believe that the used client analysis entity succeeds at
automatizing a number of validation steps that would be performed by a
network operator, thus significantly reducing efforts needed for scrutinizing
the results of the approach. However, we acknowledge that the proposed
client analysis should be further developed to cover more advance
characteristics of botnet operation. Also the client analysis should be more
thoroughly evaluated using additional malicious data sets that originate from
botnets with different C&C communication mechanisms and attack
strategies.
Detection performance
The results presented in the previous section illustrate a great potential of
using the proposed method for identifying compromised client machines at
local and enterprise networks.
The three traffic classifiers are characterized with high performance for both
malicious and benign traffic. We are seeing a low number of false positives
for all three classifiers which give us confidence in relying on them for
identifying compromised clients based on TCP/UDP conversations and
DNS queries that are flagged as malicious. Furthermore, the classification
results are on par or better than the ones reported by the existing work
 . Finally, as we used one of the most comprehensive
botnet traffic data sets for development and evaluation of the proposed
classifiers, we are confident that we would see similar results on new,
previously unseen, traffic traces.
The analysis of the number of packets per TCP/UDP conversation and the
length of time window brings interesting results as well. Varying the
number of packets per TCP/UDP conversation it is obvious that the best
performance of TCP traffic classification is obtained for more than 1000
packets per conversation while the number of packets does not have an
effect on UDP classifier. It should also be mentioned that even for 10
packets per TCP conversation we are seeing good classification results and
low number of false positives. This is quite promising as not needing to
trace higher number of packets would save resources when the system is
analysing heavy traffic flows. Regarding the size of time window used when
analysing the traffic, TCP and UDP classification has better detection
performance for longer time window. On the other hand, the length on the
time-window causes slight degradation of the performance of DNS traffic
classification. Therefore, we can conclude that balanced classification
performance can be obtained for time window of length 600 seconds and
1000 packets per TCP/UDP conversation.
The results of identifying compromised clients are promising as well. For
the used evaluation scenario, we have seen only 2 falsely identified clients
which accounts for FPR of 0.0435. However, we would like to stress that
further evaluation is needed in order to make more conclusive results about
the proposed client analysis entity.
Although promising the classification performance of the three classifiers
should be further improved so the number of false positives would be
minimized, which would consequently lead to more accurate identification
of compromised clients by the client analysis entity. This could be achieved
by optimizing feature sets used for representing traffic instances so they
would better capture the heuristics of botnet traffic. This requires further
analysis of malicious botnet traffic, as well as benign traffic that have been
misclassified as malicious. Furthermore, we could obtain a certain
improvement of classification performance by using a higher number of
classification trees within the Random Forests classifier. These
improvements would come at the cost of the time-efficiency of classification
but are still worth considering. Finally, the results of traffic analysis could
be improved by using additional traffic data sets of higher quality for
training the classifiers.
Operational deployment
The proposed method is developed with operational deployment in mind
and we believe that it represents a good candidate for being deployed in
real-world operational networks. The proposed detection method could be
potentially deployed as a cloud-based solution where classification and
client analysis entities could be placed in the cloud, while traffic processing
would be implemented in routers that connect home or enterprise network to
the Internet. This application would unlock possibilities for deploying more
advanced classifiers and client analysis entities that could leverage the
computational power of the cloud. The operational deployment should also
be coupled with existing malware testing environments that would
continuously update the pool of training data with traffic traces originating
from the latest malware samples.
Future work
The future work will be devoted to further optimization of the traffic
classification so a number of false positives would be even further reduced.
This can be done through future feature optimization and feature
engineering. Furthermore, we will place special focus on optimizing client
analysis entity so compromised clients would be identified more precisely.
Evaluation of proposed method using novel traffic traces will also be
performed. Finally, we will work on operational deployment of the proposed
CONCLUSION
In this paper we proposed a novel approach for detection of bots at local and
enterprise networks. The proposed approach employed multi-level traffic
analysis by analysing TCP, UDP and DNS traffic using supervised machine
learning. The proposed method integrated three traffic classifiers based on
Random Forests classifier and novel sets of features designed to better
capture the characteristics of botnet network activity. We evaluated the
proposed method within one of the most extensive evaluation campaigns
using traffic traces from 40 bot samples and diverse benign applications.
The results of evaluation indicate the possibility of obtaining high accuracy
of botnet traffic classification for all three classification methods. The
proposed TCP classifier is characterized by precision and recall higher than
0.98 for analysing only 10 packets per conversation. The UDP classifier is
less sensitive to the number of analysed packets having precision and recall
higher than 0.995 and 0.985, respectively. The DNS classifier has also
shown overall stable performance with precision and recall higher than
0.995 and 0.976, respectively. The presented results are in the most cases
better than the results reported by the existing work thus indicating a great
potential of using the proposed classifiers. Furthermore, the proposed
method has shown the ability of identifying compromised machines with
high accuracy while producing only a small number of false positives. The
future work will be devoted to further performance evaluation and the
optimization of the traffic analysis. Finally, special emphasis will be placed
on improving the client analysis used by the method in order to ensure
reliable identification of compromised clients.