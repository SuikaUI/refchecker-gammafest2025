HAL Id: hal-03481207
 
Submitted on 15 Dec 2021
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Segmenter: Transformer for Semantic Segmentation
Robin Strudel, Ricardo Garcia, Ivan Laptev, Cordelia Schmid
To cite this version:
Robin Strudel, Ricardo Garcia, Ivan Laptev, Cordelia Schmid. Segmenter: Transformer for Semantic
Segmentation. ICCV 2021 - International Conference on Computer Vision, Oct 2021, Virtual, France.
￿hal-03481207￿
Segmenter: Transformer for Semantic Segmentation
Robin Strudel*
Ricardo Garcia*
Ivan Laptev
Cordelia Schmid
Image segmentation is often ambiguous at the level of
individual image patches and requires contextual information to reach label consensus. In this paper we introduce
Segmenter, a transformer model for semantic segmentation.
In contrast to convolution-based methods, our approach
allows to model global context already at the ﬁrst layer
and throughout the network. We build on the recent Vision
Transformer (ViT) and extend it to semantic segmentation.
To do so, we rely on the output embeddings corresponding
to image patches and obtain class labels from these embeddings with a point-wise linear decoder or a mask transformer decoder. We leverage models pre-trained for image classiﬁcation and show that we can ﬁne-tune them on
moderate sized datasets available for semantic segmentation. The linear decoder allows to obtain excellent results
already, but the performance can be further improved by a
mask transformer generating class masks. We conduct an
extensive ablation study to show the impact of the different
parameters, in particular the performance is better for large
models and small patch sizes. Segmenter attains excellent
results for semantic segmentation. It outperforms the state
of the art on both ADE20K and Pascal Context datasets and
is competitive on Cityscapes.
1. Introduction
Semantic segmentation is a challenging computer vision problem with a wide range of applications including autonomous driving, robotics, augmented reality, image editing, medical imaging and many others .
The goal of semantic segmentation is to assign each image pixel to a category label corresponding to the underlying object and to provide high-level image representations for target tasks, e.g. detecting the boundaries of people and their clothes for virtual try-on applications .
*Equal contribution.
†Inria, ´Ecole normale sup´erieure, CNRS, PSL Research University,
75005 Paris, France.
Code: 
Segmentation map
Patch attention maps
Figure 1: Our approach for semantic segmentation is purely
transformer based. It leverages the global image context at
every layer of the model. Attention maps from the ﬁrst Segmenter layer are displayed for three 8 × 8 patches and highlight the early grouping of patches into semantically meaningful categories. The original image (top-left) is overlayed
with segmentation masks produced by our method.
Despite much effort and large progress over recent years
 , image segmentation remains a
challenging problem due to rich intra-class variation, context variation and ambiguities originating from occlusions
and low image resolution.
Recent approaches to semantic segmentation typically
rely on convolutional encoder-decoder architectures where
the encoder generates low-resolution image features and the
decoder upsamples features to segmentation maps with perpixel class scores. State-of-the-art methods deploy Fully
Convolutional Networks (FCN) and achieve impressive results on challenging segmentation benchmarks . These methods rely on learnable
stacked convolutions that can capture semantically rich information and have been highly successful in computer vision.
The local nature of convolutional ﬁlters, however,
 
limits the access to the global information in the image.
Meanwhile, such information is particularly important for
segmentation where the labeling of local patches often depends on the global image context. To circumvent this issue,
DeepLab methods introduce feature aggregation
with dilated convolutions and spatial pyramid pooling. This
allows to enlarge the receptive ﬁelds of convolutional networks and to obtain multi-scale features. Following recent
progresses in NLP , several segmentation methods explore alternative aggregation schemes based on channel or
spatial attention and point-wise attention to
better capture contextual information. Such methods, however, still rely on convolutional backbones and are, hence,
biased towards local interactions. An extensive use of specialised layers to remedy this bias suggests
limitations of convolutional architectures for segmentation.
To overcome these limitations, we formulate the problem of semantic segmentation as a sequence-to-sequence
problem and use a transformer architecture to leverage
contextual information at every stage of the model. By design, transformers can capture global interactions between
elements of a scene and have no built-in inductive prior,
see Figure 1.
However, the modeling of global interactions comes at a quadratic cost which makes such methods prohibitively expensive when applied to raw image pixels . Following the recent work on Vision Transformers (ViT) , we split the image into patches and
treat linear patch embeddings as input tokens for the transformer encoder. The contextualized sequence of tokens produced by the encoder is then upsampled by a transformer
decoder to per-pixel class scores. For decoding, we consider either a simple point-wise linear mapping of patch
embeddings to class scores or a transformer-based decoding scheme where learnable class embeddings are processed
jointly with patch tokens to generate class masks. We conduct an extensive study of transformers for segmentation
by ablating model regularization, model size, input patch
size and its trade-off between accuracy and performance.
Our Segmenter approach attains excellent results while remaining simple, ﬂexible and fast. In particular, when using large models with small input patch size the best model
reaches a mean IoU of 53.63% on the challenging ADE20K
 dataset, surpassing all previous state-of-the-art convolutional approaches by a large margin of 5.3%. Such improvement partly stems from the global context captured by
our method at every stage of the model as highlighted in
In summary, our work provides the following four contributions: (i) We propose a novel approach to semantic segmentation based on the Vision Transformer (ViT) that
does not use convolutions, captures contextual information
by design and outperforms FCN based approaches. (ii) We
present a family of models with varying levels of resolution which allows to trade-off between precision and runtime, ranging from state-of-the-art performance to models
with fast inference and good performances. (iii) We propose a transformer-based decoder generating class masks
which outperforms our linear baseline and can be extended
to perform more general image segmentation tasks. (iv) We
demonstrate that our approach yields state-of-the-art results
on both ADE20K and Pascal Context datasets and
is competitive on Cityscapes .
2. Related work
Semantic segmentation.
Methods based on Fully Convolutional Networks (FCN) combined with encoder-decoder
architectures have become the dominant approach to semantic segmentation. Initial approaches 
rely on a stack of consecutive convolutions followed by spatial pooling to perform dense predictions. Consecutive approaches upsample high-level feature maps
and combine them with low-level feature maps during decoding to both capture global information and recover sharp
object boundaries. To enlarge the receptive ﬁeld of convolutions in the ﬁrst layers, several approaches have
proposed dilated or atrous convolutions. To capture global
information in higher layers, recent work employs spatial pyramid pooling to capture multi-scale contextual information. Combining these enhancements along
with atrous spatial pyramid pooling, Deeplabv3+ proposes a simple and effective encoder-decoder FCN architecture. Recent work replace coarse
pooling by attention mechanisms on top of the encoder feature maps to better capture long-range dependencies.
While recent segmentation methods are mostly focused
on improving FCN, the restriction to local operations imposed by convolutions may imply inefﬁcient processing of
global image context and suboptimal segmentation results.
Hence, we propose a pure transformer architecture that captures global context at every layer of the model during the
encoding and decoding stages.
Transformers for vision.
Transformers are now
state of the art in many Natural Language Processing (NLP)
tasks. Such models rely on self-attention mechanisms and
capture long-range dependencies among tokens (words) in
a sentence. In addition, transformers are well suited for parallelization, facilitating training on large datasets. The success of transformers in NLP has inspired several methods
in computer vision combining CNNs with forms of selfattention to address object detection , semantic segmentation , panoptic segmentation , video processing
 and few-shot classiﬁcation .
Recently, the Vision Transformer (ViT) introduced
a convolution-free transformer architecture for image classiﬁcation where input images are processed as sequences
of patch tokens.
While ViT requires training on very
large datasets, DeiT proposes a token-based distillation strategy and obtains a competitive vision transformer
trained on the ImageNet-1k dataset using a CNN as a
Figure 2: Overview of our approach Segmenter. (Left) Encoder: The image patches are projected to a sequence of embeddings and then encoded with a transformer. (Right) Decoder: A mask transformer takes as input the output of the encoder
and class embeddings to predict segmentation masks. See text for details.
teacher. Concurrent work extends this work to video classi-
ﬁcation and semantic segmentation . In more
detail, SETR uses a ViT backbone and a standard CNN
decoder. Swin Transformer uses a variant of ViT, composed of local windows, shifted between layers and Upper-
Net as a pyramid FCN decoder.
Here, we propose Segmenter, a transformer encoderdecoder architecture for semantic image segmentation. Our
approach relies on a ViT backbone and introduces a mask
decoder inspired by DETR . Our architecture does not
use convolutions, captures global image context by design
and results in competitive performance on standard image
segmentation benchmarks.
3. Our approach: Segmenter
Segmenter is based on a fully transformer-based
encoder-decoder architecture mapping a sequence of patch
embeddings to pixel-level class annotations. An overview
of the model is shown in Figure 2. The sequence of patches
is encoded by a transformer encoder described in Section
3.1 and decoded by either a point-wise linear mapping or
a mask transformer described in Section 3.2. Our model is
trained end-to-end with a per-pixel cross-entropy loss. At
inference time, argmax is applied after upsampling to obtain a single class per pixel.
3.1. Encoder
An image x ∈RH×W ×C is split into a sequence of
patches x = [x1, ..., xN] ∈RN×P 2×C where (P, P) is
the patch size, N = HW/P 2 is the number of patches
and C is the number of channels. Each patch is ﬂattened
into a 1D vector and then linearly projected to a patch
embedding to produce a sequence of patch embeddings
x0 = [Ex1, ..., ExN] ∈RN×D where E ∈RD×(P 2C).
To capture positional information, learnable position embeddings pos = [pos1, ..., posN] ∈RN×D are added to the
sequence of patches to get the resulting input sequence of
tokens z0 = x0 + pos.
A transformer encoder composed of L layers is applied to the sequence of tokens z0 to generate a sequence of
contextualized encodings zL ∈RN×D. A transformer layer
consists of a multi-headed self-attention (MSA) block followed by a point-wise MLP block of two layers with layer
norm (LN) applied before every block and residual connections added after every block:
MSA(LN(zi−1)) + zi−1,
MLP(LN(ai−1)) + ai−1,
where i ∈{1, ..., L}.
The self-attention mechanism is
composed of three point-wise linear layers mapping tokens
to intermediate representations, queries Q ∈RN×d, keys
K ∈RN×d and values V ∈RN×d. Self-attention is then
computed as follows
MSA(Q, K, V) = softmax
The transformer encoder maps the input sequence z0 =
[z0,1, ..., z0,N] of embedded patches with position encoding to zL = [zL,1, ..., zL,N], a contextualized encoding sequence containing rich semantic information used by the
decoder. In the following section we introduce the decoder.
3.2. Decoder
The sequence of patch encodings zL ∈RN×D is decoded to a segmentation map s ∈RH×W ×K where K is the
number of classes. The decoder learns to map patch-level
encodings coming from the encoder to patch-level class
scores. Next these patch-level class scores are upsampled
by bilinear interpolation to pixel-level scores. We describe
in the following a linear decoder, which serves as a baseline,
and our approach, a mask transformer, see Figure 2.
Linear. A point-wise linear layer is applied to the patch
encodings zL ∈RN×D to produce patch-level class logits
zlin ∈RN×K. The sequence is then reshaped into a 2D
feature map slin ∈RH/P ×W/P ×K and bilinearly upsampled to the original image size s ∈RH×W ×K. A softmax
is then applied on the class dimension to obtain the ﬁnal
segmentation map.
Mask Transformer.
For the transformer-based decoder,
we introduce a set of K learnable class embeddings cls =
[cls1, ..., clsK] ∈RK×D where K is the number of classes.
Each class embedding is initialized randomly and assigned
to a single semantic class. It will be used to generate the
class mask. The class embeddings cls are processed jointly
with patch encodings zL by the decoder as depicted in Figure 2. The decoder is a transformer encoder composed of M
layers. Our mask transformer generates K masks by computing the scalar product between L2-normalized patch embeddings z′
M ∈RN×D and class embeddings c ∈RK×D
output by the decoder. The set of class masks is computed
as follows
M, c) = z′
where Masks(z′
RN×K is a set of patch sequence. Each mask sequence is then reshaped into a 2D
mask to form smask
∈RH/P ×W/P ×K and bilinearly
upsampled to the original image size to obtain a feature
map s ∈RH×W ×K. A softmax is then applied on the
class dimension followed by a layer norm to obtain pixelwise class score forming the ﬁnal segmentation map. The
masks sequences are softly exclusive to each other i.e.
k=1 si,j,k = 1 for all (i, j) ∈H × W.
Our mask transformer is inspired by DETR , Max-
DeepLab and SOLO-v2 which introduce object
embeddings to produce instance masks . However, unlike our method, MaxDeep-Lab uses an hybrid approach based on CNNs and transformers and splits the pixel
and class embeddings into two streams because of computational constraints. Using a pure transformer architecture
and leveraging patch level encodings, we propose a simple approach that processes the patch and class embeddings
jointly during the decoding phase. Such approach allows to
produce dynamical ﬁlters, changing with the input. While
we address semantic segmentation in this work, our mask
transformer can also be directly adapted to perform panoptic segmentation by replacing the class embeddings by object embeddings.
Backbone Layers Token size Heads Params
Seg-B† DeiT-B
Table 1: Details of Transformer variants.
4. Experimental results
4.1. Datasets and metrics
ADE20K . This dataset contains challenging scenes
with ﬁne-grained labels and is one of the most challenging
semantic segmentation datasets. The training set contains
20,210 images with 150 semantic classes. The validation
and test set contain 2,000 and 3,352 images respectively.
Pascal Context . The training set contains 4,996 images with 59 semantic classes plus a background class. The
validation set contains 5,104 images.
Cityscapes . The dataset contains 5,000 images from
50 different cities with 19 semantic classes. There are 2,975
images in the training set, 500 images in the validation set
and 1,525 images in the test set.
Metrics. We report Intersection over Union (mIoU) averaged over all classes.
4.2. Implementation details
Transformer models. For the encoder, we build upon the
vision transformer ViT and consider ”Tiny”, ”Small”,
”Base” and ”Large” models described in Table 1. The parameters varying in the transformer encoder are the number of layers and the token size. The head size of a multiheaded self-attention (MSA) block is ﬁxed to 64, the number of heads is the token size divided by the head size and
the hidden size of the MLP following MSA is four times the
token size. We also use DeiT , a variant of the vision
transformer. We consider models representing the image at
different resolutions and use input patch sizes 8×8, 16×16
and 32×32. In the following, we use an abbreviation to
describe the model variant and patch size, for instance Seg-
B/16 denotes the ”Base” variant with 16×16 input patch
size. Models based on DeiT are denoted with a †, for instance Seg-B†/16.
ImageNet pre-training. Our Segmenter models are pretrained on ImageNet, ViT is pre-trained on ImageNet-21k
with strong data augmentation and regularization and
its variant DeiT is pre-trained on ImageNet-1k. The original ViT models have been trained with random cropping only, whereas the training procedure proposed by 
uses a combination of dropout and stochastic depth
 as regularization and Mixup and RandAugment
 as data augmentations.
This signiﬁcantly improves
the ImageNet top-1 accuracy, i.e., it obtains a gain of +2%
on ViT-B/16. We ﬁne-tuned ViT-B/16 on ADE20K with
(a) Patch size 32 × 32
(b) Patch size 16 × 16
(c) Patch size 8 × 8
(d) Ground Truth
Figure 3: Impact of the model patch size on the segmentation maps.
models from and and observe a signiﬁcant difference, namely a mIoU of 45.69% and 48.06% respectively.
In the following, all the Segmenter models will be initialized with the improved ViT models from . We use publicly available models provided by the image classiﬁcation
library timm and Google research . Both models
are pre-trained at an image resolution of 224 and ﬁne-tuned
on ImageNet-1k at a resolution of 384, except for ViT-B/8
which has been ﬁne-tuned at a resolution of 224. We keep
the patch size ﬁxed and ﬁne-tune the models for the semantic segmentation task at higher resolution depending on
the dataset. As the patch size is ﬁxed, increasing resolution results in longer token sequences. Following , we
bilinearly interpolate the pre-trained position embeddings
according to their original position in the image to match
the ﬁne-tuning sequence length. The decoders, described
in Section 3.2 are initialized with random weights from a
truncated normal distribution .
Data augmentation. During training, we follow the standard pipeline from the semantic segmentation library MM-
Segmentation , which does mean substraction, random
resizing of the image to a ratio between 0.5 and 2.0 and random left-right ﬂipping. We randomly crop large images and
pad small images to a ﬁxed size of 512×512 for ADE20K,
480×480 for Pascal-Context and 768×768 for Cityscapes.
On ADE20K, we train our largest model Seg-L-Mask/16
with a resolution of 640×640, matching the resolution used
by the Swin Transformer .
Optimization. To ﬁne-tune the pre-trained models for the
semantic segmentation task, we use the standard pixel-wise
cross-entropy loss without weight rebalancing.
stochastic gradient descent (SGD) as the optimizer
with a base learning rate γ0 and set weight decay to 0. Following the seminal work of DeepLab we adopt the
”poly” learning rate decay γ = γ0 (1 −
Ntotal )0.9 where
Niter and Ntotal represent the current iteration number and
the total iteration number. For ADE20K, we set the base
learning rate γ0 to 10−3 and train for 160K iterations with
a batch size of 8. For Pascal Context, we set γ0 to 10−3
and train for 80K iterations with a batch size of 16. For
Cityscapes, we set γ0 to 10−2 and train for 80K iterations with a batch size of 8.
The schedule is similar to
DeepLabv3+ with learning rates divided by a factor
10 except for Cityscapes where we use a factor of 1.
Stochastic Depth
45.01 45.37 45.10
42.02 42.30 41.14
36.49 36.63 35.67
Table 2: Mean IoU comparison of different regularization
schemes using Seg-S/16 on ADE20K validation set.
Backbone Patch size Im/sec ImNet acc. mIoU (SS)
Seg-B†/16 DeiT-B
Table 3: Performance comparison of different Segmenter
models with varying backbones and input patch sizes on
ADE20K validation set.
Inference. To handle varying image sizes during inference,
we use a sliding-window with a resolution matching the
training size. For multi-scale inference, following standard
practice we use rescaled versions of the image with
scaling factors of (0.5, 0.75, 1.0, 1.25, 1.5, 1.75) and leftright ﬂipping and average the results.
4.3. Ablation study
In this section, we ablate different variants of our approach on the ADE20K validation set.
We investigate
model regularization, model size, patch size, model performance, training dataset size, compare Segmenter to convolutional approaches and evaluate different decoders. Unless
stated otherwise, we use the baseline linear decoder and report results using single-scale inference.
Regularization. We ﬁrst compare two forms of regularization, dropout and stochastic depth , and show that
stochastic depth consistently improves transformer training for segmentation.
CNN models rely on batch nor-
Figure 4: Images per second and mean IoU for our approach compared to other methods on ADE20K validation
set. Segmenter models offer a competitive trade-off in terms
of performance and precision.
malization which also acts as a regularizer. In contrast, transformers are usually composed of layer normalization combined with dropout as a regularizer during training . Dropout randomly ignores tokens given as input of a block and stochastic depth randomly skips a learnable block of the model during the forward pass. We compare regularizations on Seg-S/16 based on ViT-S/16 backbone. Table 2 shows that stochastic depth set to 0.1, dropping 10% of the layers randomly, consistently improves
the performance, with 0.36% when the dropout is set to 0
compared to the baseline without regularization. Dropout
consistently hurts performances, either alone or when combined with stochastic depth. This is consistent with 
which observed the negative impact of dropout for image
classiﬁcation. From now on, all the models will be trained
with stochastic depth set to 0.1 and without dropout.
Transformer size. We now study the impact of transformers size on performance by varying the number of layers
and the tokens size for a ﬁxed patch size of 16. Table 3
shows that performance scales nicely with the backbone capacity. When doubling the token dimension, from Seg-S/16
to Seg-B/16, we get a 2.69% improvement. When doubling
the number of layers, from Seg-B/16 to Seg-L/16, we get
an improvement of 2.65%. Finally, our largest Segmenter
model, Seg-L/16, achieves a strong mIoU of 50.71% with a
simple decoding scheme on the ADE20K validation dataset
with single scale inference. The absence of tasks-speciﬁc
layers vastly used in FCN models suggests that transformer
based methods provide more expressive models, well suited
for semantic segmentation.
Patch size. Representing an image with a patch sequence
provides a simple way to trade-off between speed and accuracy by varying the patch size. While increasing the patch
size results in a coarser representation of the image, it re-
Decoder Small Medium Large mIoU (SS)
DeepLab RNeSt-101 UNet
Seg-B-Mask/32
Seg-B†-Mask/16
Seg-B-Mask/16
Seg-L-Mask/16
Evaluation with respect to the object size
on ADE20k validation set (mean IoU). Comparison of
DeepLabv3+ ResNeSt-101 to Segmenter models with a linear or a mask transformer decoder.
Dataset Size
38.31 41.87 43.42 44.61 45.37
Performance comparison of Seg-S/16 models trained with increasing dataset size and evaluated on
ADE20K validation set.
sults in a smaller sequence that is faster to process. The
third and fourth parts of Table 3 report the performance for
ViT backbones and varying patch sizes. We observe that the
patch size is a key factor for semantic segmentation performance. It is similarly important to the model size. Indeed,
going from a patch size 32 to 16 we observe an improvement of 5% for Seg-B. For Seg-B, we also report results
for a patch size of 8 and report an mIoU of 49.54%, reducing the gap from ViT-B/8 to ViT-L/16 to 1.17% while
requiring substantially fewer parameters. This trend shows
that reducing the patch size is a robust source of improvement which does not introduce any parameters but requires
to compute attention over longer sequences, increasing the
compute time and memory footprint. If it was computationally feasible, ViT-L/8 would probably be the best performing model. Going towards more computation and memory
efﬁcient transformers handling larger sequence of smaller
patches is a promising direction.
To further study the impact of patch size, we show segmentation maps generated by Segmenter models with decreasing patch size in Figure 3. We observe that for a patch
size of 32, the model learns a globally meaningful segmentation but produces poor boundaries, for example the
two persons on the left are predicted by a single blob. Reducing the patch size leads to considerably sharper boundaries as can be observed when looking at the contours of
persons. Hard to segment instances as the thin streetlight
pole in the background are only captured at a resolution
of 8. In Table 4, we report mean IoU with respect to the
object size and compare Segmenter to DeepLabv3+ with
ResNeSt backbone. To reproduce DeepLabv3+ results, we
used models from the MMSegmentation library . We
observe how Seg-B/8 improvement over Seg-B/16 comes
mostly from small and medium instances with a gain of
1.27% and 1.74% respectively. Also, we observe that overall the biggest improvement of Segmenter over DeepLab
comes from large instances where Seg-L-Mask/16 shows
an improvement of 6.39%.
Decoder variants. In this section, we compare different
decoder variants. We evaluate the mask transformer introduced in Section 3.2 and compare it to the linear baseline.
The mask transformer has 2 layers with the same token and
hidden size as the encoder. Table 4 reports the mean IoU
performance. The mask transformer provides consistent improvements over the linear baseline. The most signiﬁcant
gain of 1.6% is obtained for Seg-B†/16, for Seg-B-Mask/32
we obtain a 1.1% improvement and for Seg-L/16 a gain of
0.6%. In Table 4 we also examine the gain of different models with respect to the object size. We observe gains both
on small and large objects, showing the beneﬁt of using dynamical ﬁlters. In most cases the gain is more signiﬁcant for
large objects, i.e., 1.4% for Seg-B/32, 2.1% for Seg-B†/16
and and 1.7% for Seg-L/16. The class embeddings learned
by the mask transformer are semantically meaningful, i.e.,
similar classes are nearby, see Figure 8 for more details.
Transformer versus FCN. Table 4 and Table 6 compare
our approach to FCN models and DeepLabv3+ with
ResNeSt backbone , one of the best fully-convolutional
approaches. Our transformer approach provides a significant improvement over this state-of-the-art convolutional
approach, highlighting the ability of transformers to capture
global scene understanding. Segmenter consistently outperforms DeepLab on large instances with an improvement
of more than 4% for Seg-L/16 and 6% for Seg-L-Mask/16.
However, DeepLab performs similarly to Seg-B/16 on small
and medium instances while having a similar number of parameters. Seg-B/8 and Seg-L/16 perform best on small and
medium instances though at higher computational cost.
Performance. In Figure 4, we compare our models to several state-of-the-art methods in terms of images per seconds and mIoU and show a clear advantage of Segmenter
over FCN based models (green curve).
We also show
that our approach compares favorably to recent transformer
based approach, our largest model Seg-L-Mask/16 is onpar with Swin-L and outperforms SETR-MLA. We observe
that Seg/16 models perform best in terms of accuracy versus
compute time with Seg-B-Mask/16 offering a good tradeoff. Seg-B-Mask/16 outperforms FCN based approaches
with similar inference speed, matches SETR-MLA while
being twice faster and requiring less parameters and outperforms Swin-B both in terms of inference speed and performance. Seg/32 models learn coarser segmentation maps
as discussed in the previous section and enable fast inference with 400 images per second for Seg-B-Mask/32, four
Im/sec mIoU +MS
HRNetV2-W48
ACNet 
ResNet-101
ResNet-101
DRANet 
ResNet-101
CPNet 
ResNet-101
DeepLabv3+ 
ResNet-101
45.47 46.35
DeepLabv3+ 
ResNeSt-101
46.47 47.27
DeepLabv3+ 
ResNeSt-200
SETR-L MLA 
48.64 50.28
Swin-L UperNet Swin-L/16
52.10 53.50
47.08 48.05
Seg-B†-Mask/16
48.70 50.08
50.71 52.25
Seg-L-Mask/16
51.82 53.63
Table 6: State-of-the-art comparison on ADE20K validation set.
times faster than ResNet-50 while providing similar performances. To compute the images per second, we use a V100
GPU, ﬁx the image resolution to 512 and for each model
we maximize the batch size allowed by memory for a fair
comparison.
Dataset size. Vision Transformers highlighted the importance of large datasets to attain good performance for the
task of image classiﬁcation.
At the scale of a semantic
segmentation dataset, we analyze Seg-S/16 performance on
ADE20k dataset in Table 5 when trained with a dataset of
increasing size. We observe an important drop in performance when the training set size is below 8k images. This
shows that even during ﬁne-tuning transformers performs
best with a sufﬁcient amount of data.
4.4. Comparison with state of the art
In this section, we compare the performance of Segmenter with respect to the state-of-the-art methods on
ADE20K, Pascal Context and Cityscapes datasets.
ADE20K. Seg-B†/16 pre-trained on ImageNet-1k matches
the state-of-the-art FCN method DeepLabv3+ ResNeSt-
200 as shown in Table 6.
Adding our mask transformer, Seg-B†-Mask/16 improves by 2% and achieves
a 50.08% mIoU, outperforming all FCN methods.
best model, Seg-L-Mask/16 attains a state-of-the-art performance of 53.63%, outperforming by a margin of 5.27%
mIoU DeepLabv3+ ResNeSt-200 and the transformerbased methods SETR and Swin-L UperNet .
Pascal Context Table 7 reports the performance on Pascal Context.
Seg-B†models are competitive with FCN
methods and the larger Seg-L/16 model already provides
state-of-the-art performance, outperforming SETR-L. Performances can be further enhanced with our mask transformer, Seg-L-Mask/16, improving over the linear decoder
by 2.5% and achieving a performance of 59.04% mIoU. In
particular, we report an improvement of 2.8% over OCR
CITYSCAPES
DeepLabv3+
DeepLabv3+
DeepLabv3+
Figure 5: Qualitative comparison of Seg-L-Mask/16 performance with DeepLabV3+ ResNeSt-101. See Section C for additional qualitative results.
DeepLabv3+ 
ResNet-101
DANet 
ResNet-101
CPNet 
ResNet-101
CFNet 
ResNet-101
ACNet 
ResNet-101
APCNet 
HRNetV2-W48
DRANet 
ResNet-101
HRNetV2-W48
SETR-L MLA ViT-L/16
Seg-B†-Mask/16
Seg-L-Mask/16
Table 7: State-of-the-art comparison on Pascal Context validation set.
HRNetV2-W48 and 3.2% over SETR-L MLA.
Cityscapes. Table 8 reports the performance of Segmenter
on Cityscapes. We use a variant of mask transformer for
Seg-L-Mask/16 with only one layer in the decoder as two
layers did not ﬁt into memory due to the large input resolution of 768×768. Both Seg-B and Seg-L methods are
competitive with other state-of-the-art methods with Seg-
L-Mask/16 achieving a mIoU of 81.3%.
Qualitative results. Figure 5 shows a qualitative comparison of Segmenter and DeepLabv3+ with ResNeSt backbone, for which models were provided by the MMSegmentation library. We can observe that Deeplabv3+ tends
to generate sharper object boundaries while Segmenter provides more consistent labels on large instances and handles
partial occlusions better.
5. Conclusion
This paper introduces a pure transformer approach for
semantic segmentation. The encoding part builds up on the
PSANet 
ResNet-101
DeepLabv3+ 
Xception-71
ResNet-101
DeepLabv3+ 
ResNeSt-101
ResNet-101
CCNet 
ResNet-101
Panoptic-Deeplab Xception-71
DeepLabv3+ 
ResNeSt-200
SETR-L PUP 
Seg-B†-Mask/16
Seg-L-Mask/16
Table 8: State-of-the-art comparison on Cityscapes validation set.
recent Vision Transformer (ViT), but differs in that we rely
on the encoding of all images patches. We observe that the
transformer captures the global context very well. Applying
a simple point-wise linear decoder to the patch encodings
already achieves excellent results. Decoding with a mask
transformer further improves the performance. We believe
that our end-to-end encoder-decoder transformer is a ﬁrst
step towards a uniﬁed approach for semantic segmentation,
instance segmentation and panoptic segmentation.
6. Acknowledgements
We thank Andreas Steiner for providing the ViT-Base
model trained on 8 × 8 patches and Gauthier Izacard for
the helpful discussions. This work was partially supported
by the HPC resources from GENCI-IDRIS , the Louis Vuitton ENS Chair on Arti-
ﬁcial Intelligence, and the French government under management of Agence Nationale de la Recherche as part of
the ”Investissements d’avenir” program, reference ANR-
19-P3IA-0001 (PRAIRIE 3IA Institute).