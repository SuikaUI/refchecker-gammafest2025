Dilated Convolutions for Modeling
Long-Distance Genomic Dependencies
Ankit Gupta
School of Engineering and Applied Sciences
Harvard University
Cambridge, MA 02138
 
Alexander M. Rush
School of Engineering and Applied Sciences
Harvard University
Cambridge, MA 02138
 
We consider the task of detecting regulatory elements in the human genome directly
from raw DNA. Past work has focused on small snippets of DNA, making it
difﬁcult to model long-distance dependencies that arise from DNA’s 3-dimensional
conformation. In order to study long-distance dependencies, we develop and
release a novel dataset for a larger-context modeling task. Using this new data set
we model long-distance interactions using dilated convolutional neural networks,
and compare them to standard convolutions and recurrent neural networks. We
show that dilated convolutions are effective at modeling the locations of regulatory
markers in the human genome, such as transcription factor binding sites, histone
modiﬁcations, and DNAse hypersensitivity sites.
Introduction
Gene expression is controlled by a variety of regulatory factors that determine which genes are
expressed in which environmental conditions . Due to proteins called histones
that DNA winds around, parts of DNA are more accessible to binding than others, and so DNA
accessibility is a regulatory factor. Modiﬁcations to histones can affect the conformation of DNA, so
histone modiﬁcations are also regulatory factors. Furthermore, proteins that bind to DNA and affect
transcriptional activity are called transcription factors and are also regulatory factors. The activity
that they affect can be thousands of base pairs away .
These interactions imply that nucleotides far apart in a 1-dimensional DNA sequence may interact
in its 3-dimensional conformation, and so expression is governed by both local and long-distance
dependencies. As a result, it may be important to incorporate DNA regions that are far away in 1-D
space when modeling regulatory markers. However, past data sets for binding site prediction have
used small snippets of DNA , which limits the ability to model these interactions.
This work addresses this question by introducing a new dataset for this problem allowing for longdistance interactions, and a new model using dilated convolutions to predict the locations of regulatory
markers. We learn a mapping from a DNA region, speciﬁed as a sequence of nucleotides, to the
locations of regulatory markers in that region. Dilated convolutions can capture a hierarchical
representation of a much larger input space than standard convolutions, allowing them to scale to
large context sizes. We compare dilated convolutions to other modeling methods from deep learning
and past work, including standard convolutions and recurrent neural networks, and show that they
present an advancement over existing models. All code, data, and scripts for these experiments are
available at 
34th International Conference on Machine Learning , Workshop on Computational Biology
 
(a) Convolution
(b) Bidirectional LSTM
(c) Dilated Convolution
Figure 1: Visual representation of the models. All take a sequence of nucleotides as input. The receptive ﬁeld
is shaded. The convolution has a short path between inputs and predictions but a small receptive ﬁeld. The
Bi-LSTM has a large receptive ﬁeld, the whole sequence, but may have a long path between a nucleotide and
prediction. The dilated convolution has both a short path from the input and a large receptive ﬁeld.
Convolutional and Recurrent Neural Networks
The two most widely applied models for sequence tasks are convolutional neural networks (CNNs) and recurrent neural networks (RNNs).
CNNs consist of a series of layers each applying a linear transformation followed by a non-linearity
to a sliding kernel window of inputs , shown in Figure 1(a). RNNs apply the
same to each element of a sequence in time. An RNN with state size M processes a sequence of
inputs {xi}N by updating a state vector ci ∈RM for each timestep i such that ci+1 = R(ci, xi).
Concatenating a forward and reverse RNN gives a bidirectional RNN, as in Figure 1(b).
The interactions in a network can be quantiﬁed by the receptive ﬁeld. Formally the receptive ﬁeld of
a node is the subset R of input elements {xi} that can impact its value . For
CNNs, the size of the receptive ﬁeld is linear in the number of layers and the kernel width. Thus,
scaling the receptive ﬁeld to incorporate a large input introduces more layers, making training more
difﬁcult. Bidirectional RNNs on the other hand have a receptive ﬁeld of the whole input, but require
gradients to travel long-distances over time. Long Short-Term Memory (LSTM) cells circumvent some of these issues by using trainable gated connections that
allow the cell state to propagate more efﬁciently. However, with very long sequences, as is the case
in genetic data, LSTMs still have trouble learning very long-distance relationships.
Dilated Convolutions
Dilated convolutions offer a middle ground between
these two models with wide receptive ﬁelds and short distance gradient propagation. In CNNs
each kernel window consists of adjacent inputs, while dilated convolutions introduce gaps (dilation)
between inputs. With dilation d, the window starting at location i of size k is
xi+(k−1)·d
Yu and Koltun show that by stacking these convolutions with increasingly large d, we can
expand the receptive ﬁeld of each output exponentially. This allows them to have large receptive
ﬁelds, but still short backpropagations, as shown in Figure 1(c). We take advantage of this structure
when modeling genetic regulation. Dilated convolutions have been used for image segmentation , text-to-speech , and text classiﬁcation .
Experiments
Dataset 1: Short Sequence Prediction Benchmark 
As a preliminary experiment, we test dilated convolutions on a standard benchmark task and compare them
to CNNs and LSTMs. We use the Zhou and Troyanskaya dataset to predict the presence of
regulatory markers in short DNA sequences. Each input is a vector x ∈Vd, where d = 1000 is the
Dataset 1: Short Sequence Prediction Task
Params (Best-Case)
Test Set PR AUC
Table 1: Models and Test Set Precision-Recall Area Under Curve (PR AUC) scores for Dataset 1. We report the
number of parameters in the hyperparameter conﬁguration selected using grid search with a held-out validation
set. CNN3 is the model from Zhou and Troyanskaya , and Bi-LSTM is the bidirectional LSTM model
from Quang and Xie . Our DILATED6 model performs better than the standard convolutions on all three
types of predictions and only slightly underperforms the bidirectional LSTM model. All scores are based on
reimplementations. Model size varied across hyperparameter conﬁgurations.
sequence length, and V = {A, C, T, G}. Each output vector y ∈{0, 1}k indicates whether each of
the k = 919 regulatory markers is present in the middle 200bp of x. The markers include TFBSs,
histone modiﬁcations, and DNAse hypersensitivity sites (accessible DNA regions).
We train several architectures and report the mean PR AUC scores for each category in Table 1. The
models in this task have a ﬁnal fully-connected layer, which implies that the receptive ﬁeld of every
output contains the whole input (1000 bp). The differences between models is explained by the extent
to which each captures meaning prior to the ﬁnal layer. Note that there was dropout and batch normalization between every layer, and we select
hyperparameters using grid search. Details are given in the Supplemental Information section.
Notably the DILATED6 model performs much better than the CNN3 model from Zhou and Troyanskaya , and approaches the performance of the BI-LSTM model. The BI-LSTM is the most
effective model, which is reasonable since the sequences are short. This gives a proof-of-concept that
dilated convolutions can capture nucleotide structure better than standard convolutions.
Dataset 2: Complete Genome Labeling with Long-Range Inputs
To model long-distance
dependencies, we introduce a new dataset that models the problem as long-distance sequence labeling
instead of prediction. In particular: (1) the input sequences are longer, with each a d = 25000 bp
sequence, and (2) the outputs annotate at nucleotide-resolution, making this a dense labeling task.
We predict the presence of all regulatory markers at each nucleotide, rather than per 200bp.
We use hg19 to extract input sequences and k = 919 regulatory marker locations from ENCODE
 . Thus, we get pairs of input and output sequences {(x, y)} with x ∈Vd and
y ∈{0, 1}d×k. The inputs have vocabulary V = {N, A, C, T, G}, where the N character represents
nucleotides with high uncertainty. We remove sequences with >10% unknown nucleotides or with
>10% part of a multi-mapped sequence, meaning part of a region that maps to several genomic
locations. This left n = 93880 non-overlapping sequences that were d = 25000 in length, totaling
2.3 billion nucleotides. We train on 80% of the data, and split the rest into validation and test sets.
We train models representing the various architectures on this new dataset. We showed above with
Dataset 1 that LSTM-based models were the most effective at predicting regulatory marker locations
when given only a small number of nucleotides, and we can now study their relative success when
given more context. Since this is a base-pair level prediction task, we no longer have a fully-connected
layer for each model. We summarize the models for this task and report PR AUC scores in Table 2.
We report the best models across all hyperparameters, including number of ﬁlters, dropout, learning
rate, batch norm decay, and hidden layer size, using grid search on a held-out validation set. For the
dilated convolution, we use dilations of 1, 3, 9, 27, and 81 in sequence.
Dilated convolutional models perform the best on both TFBS and histone modiﬁcation prediction,
and do marginally worse than the best non-dilated models on predicting DNAse hypersensitivity sites.
This shows that dilated convolutions can be effective at capturing structure in the genome. In contrast,
while BI-LSTM performed better than CNN3, it did worse than the convolutions, particularly on
Dataset 2: Complete Genome Labeling
Validation PR AUC
Test PR AUC
Table 2: Models for Dataset 2 and Precision-Recall Area Under Curve (PR AUC) scores. We report the number
of parameters in the best-performing hyperparameter conﬁguration. ID-CNN is the model from Strubell et al.
 . We see much higher performance using dilated convolutions on predicting transcription factor binding
sites and histone modiﬁcations, but no improvement on predicting DNAse hypersensitivity sites. Note that this
because this is a new dense prediction task, these results should not be directly compared to those in Table 1.
TFBS prediction. This suggests that the LSTM architecture is less effective at this task, either due
to vanishing gradients or difﬁculty in learning gated recurrent connections. This suggests that even
though LSTMs are effective on short sequences, when trying to capture properties of long genetic
sequences, dilated convolutions are an important architecture to consider.
On DNAse hypersensitivity prediction, standard convolutions do well. This may be because accessible
regions locally have highly explanatory motifs and having additional context far away does not
improve the accuracy, while the other two marker types are more easily characterized with access to
distal motifs. This is also consistent with the high performance of both DILATED6 and Bi-LSTM in
predicting DNAse sites in Dataset 1 compared to TFBS and histone modiﬁcations.
To further investigate the trained models, we visualize receptive ﬁelds by sampling a validation
sequence and backpropagating an error of 1 from every positive output for a random regulatory
marker. In Figure 2, we plot the output locations (blue) and the norm of the error to the inputs (black),
which gives a visual representation of the receptive ﬁeld. We observe that the standard convolution
has a narrow receptive ﬁeld while the dilated convolution has a wider one, as the gradient is high for a
wider input in Figure 2(b). In contrast, the LSTM model in Figure 2(c) has gradient backpropagated
widely, but it usually has a low magnitude. It does not appear that the LSTM models are able to learn
the long-distance dependencies that the dilated model captures, meaning that though LSTM models
were successful on short inputs in Dataset 1, they were unable to scale to larger inputs on Dataset 2.
(b) DILATED
(c) BI-LSTM
Figure 2: We visualize the norm of the gradient to the inputs (black). This gives an indication about the actual
receptive ﬁeld that was used to make a decision at the outputs (blue). Notably, we see that CNN7 has a narrow
receptive ﬁeld, DILATED has a wide high-magnitude ﬁeld, and BI-LSTM has a wide but low-magnitude ﬁeld.
Conclusion
We introduce a new data set with larger DNA contexts and base pair level data. On this data set, we
show that dilated convolutions can outperform both CNNs and LSTMs and appear to capture longdistance relationships in DNA. This suggests that dilated convolutions are an important architecture
to consider for genetic modeling. Next, we intend to incorporate more DNA structural information,
such as Hi-C data that measures DNA conformation . We also intend to study
whether a hybrid architecture can effectively predict all marker types, particularly the DNAse sites.