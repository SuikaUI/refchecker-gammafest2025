UC San Diego
UC San Diego Previously Published Works
Supervised learning of semantic classes for image annotation and retrieval
 
IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(3)
Carneiro, Gustavo
Chan, Antoni B
Moreno, Pedro J
Publication Date
2007-03-01
Peer reviewed
eScholarship.org
Powered by the California Digital Library
University of California
Supervised Learning of Semantic Classes for
Image Annotation and Retrieval
Gustavo Carneiro, Antoni B. Chan, Pedro J. Moreno, and Nuno Vasconcelos, Member, IEEE
Abstract—A probabilistic formulation for semantic image annotation and retrieval is proposed. Annotation and retrieval are posed as
classification problems where each class is defined as the group of database images labeled with a common semantic label. It is
shown that, by establishing this one-to-one correspondence between semantic labels and semantic classes, a minimum probability of
error annotation and retrieval are feasible with algorithms that are 1) conceptually simple, 2) computationally efficient, and 3) do not
require prior semantic segmentation of training images. In particular, images are represented as bags of localized feature vectors, a
mixture density estimated for each image, and the mixtures associated with all images annotated with a common semantic label
pooled into a density estimate for the corresponding semantic class. This pooling is justified by a multiple instance learning argument
and performed efficiently with a hierarchical extension of expectation-maximization. The benefits of the supervised formulation over the
more complex, and currently popular, joint modeling of semantic label and visual feature distributions are illustrated through theoretical
arguments and extensive experiments. The supervised formulation is shown to achieve higher accuracy than various previously
published methods at a fraction of their computational cost. Finally, the proposed method is shown to be fairly robust to parameter
Index Terms—Content-based image retrieval, semantic image annotation and retrieval, weakly supervised learning, multiple instance
learning, Gaussian mixtures, expectation-maximization, image segmentation, object recognition.
INTRODUCTION
ONTENT-BASED image retrieval, the problem of searching
large image repositories according to their content, has
been the subject of a significant amount of research in the
last decade , , , , , . While early
retrieval architectures were based on the query-by-example
paradigm , , , , , , , , , ,
 , , , which formulates image retrieval as the
search for the best database match to a user-provided query
image, it was quickly realized that the design of fully
functional retrieval systems would require support for
semantic queries . These are systems where database
images are annotated with semantic labels, enabling the
user to specify the query through a natural language
description of the visual concepts of interest. This realization, combined with the cost of manual image labeling,
generated significant interest in the problem of automatically extracting semantic descriptors from images.
The two goals associated with this operation are: 1) the
automatic annotation of previously unseen images, and
2) the retrieval of database images based on semantic
queries. These goals are complementary since semantic
queries are relatively straightforward to implement once
each database image is annotated with a set of semantic
labels. Semantic image labeling can be posed as a problem
of either supervised or unsupervised learning. The earliest
efforts in the area were directed to the reliable extraction of
specific semantics, e.g., differentiating indoor from outdoor
scenes , cities from landscapes , and detecting trees
 , horses , or buildings , among others. These
efforts posed the problem of semantics extraction as one of
supervised learning: A set of training images with and
without the concept of interest was collected and a binary
classifier was trained to detect that concept. The classifier
was then applied to all database images which were, in this
way, annotated with respect to the presence or absence of
the concept. Since each classifier is trained in the “one-vsall” (OVA) mode (the concept of interest versus everything
else), we refer to this semantic labeling framework as
supervised OVA.
More recently, there has been an effort to solve the
problem in greater generality by resorting to unsupervised
learning , , , , , , , . The basic idea
is to introduce a set of latent variables that encode hidden
states of the world, where each state induces a joint
distribution on the space of semantic labels and image
appearance descriptors (local features computed over
image neighborhoods). During training, a set of labels is
assigned to each image, the image is segmented into a
collection of regions (either through a block-based decomposition , or traditional segmentation methods ,
 , , , ), and an unsupervised learning algorithm
is run over the entire database to estimate the joint density
of semantic labels and visual features. Given a new image to
annotate, visual feature vectors are extracted, the joint
probability model is instantiated with those feature vectors,
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
MARCH 2007
. G. Carneiro is with the Integrated Data Systems Department, Siemens
Corporate Research, 755 College Road East, Princeton, NJ 08540.
E-mail: .
. A.B. Chan and N. Vasconcelos are with the Department of Computer and
Electrical Engineering, University of California, San Diego, 9500 Gilman
Drive, La Jolla, CA 92093. E-mail: {abc, nuno}@ucsd.edu.
. P.J. Moreno is with Google Inc., 1440 Broadway, 21st Floor, New York,
NY 10018. E-mail: .
Manuscript received 10 Aug. 2005; revised 19 Feb. 2006; accepted 5 July
2006; published online 15 Jan. 2007.
Recommended for acceptance by B.S. Manjunath.
For information on obtaining reprints of this article, please send e-mail to:
 , and reference IEEECS Log Number TPAMI-0435-0805.
0162-8828/07/$25.00  2007 IEEE
Published by the IEEE Computer Society
state variables are marginalized, and a search for the set of
labels that maximize the joint density of text and appearance is carried out. We refer to this labeling framework as
unsupervised.
Both formulations have strong advantages and disadvantages. In generic terms, unsupervised labeling leads to
significantly more scalable (in database size and number of
concepts of interest) training procedures, places much
weaker demands on the quality of the manual annotations
required to bootstrap learning, and produces a natural
ranking of semantic labels for each new image to annotate.
On the other hand, it does not explicitly treat semantics as
image classes and, therefore, provides little guarantees that
the semantic annotations are optimal in a recognition or
retrieval sense. That is, instead of annotations that achieve
the smallest probability of retrieval error, it simply produces
the ones that have largest joint likelihood under the assumed
mixture model. Furthermore, due to the difficulties of joint
inference on sets of continuous and discrete random
variables, unsupervised learning usually requires restrictive
independence assumptions on the relationship between the
text and visual components of the annotated image data.
In this work, we show that it is possible to combine the
advantages of the two formulations through a reformulation of the supervised one. This consists of defining a
multiclass classification problem where each of the semantic
concepts of interest defines an image class. At annotation time,
these classes all directly compete for the image to annotate,
which no longer faces a sequence of independent binary
tests. This supervised multiclass labeling (SML) formulation
obviously retains the classification and retrieval optimality
of supervised OVA, as well as its ability to avoid restrictive
independence assumptions. However, it also 1) produces a
natural ordering of semantic labels at annotation time, and
2) eliminates the need to compute a “nonclass” model for
each of the semantic concepts of interest. In result, it has a
learning complexity equivalent to that of the unsupervised
formulation and, like the latter, places much weaker
requirements on the quality of manual labels than supervised OVA.
From an implementation point of view, SML requires
answers to two open questions. The first is how do we learn
the probability distribution of a semantic class from images that
are only weakly labeled with respect to that class? That is,
images labeled as containing the semantic concept of
interest, but without indication of which image regions
are observations of that concept. We rely on a multipleinstance learning , , , type of argument to
show that the segmentation problem does not have to be
solved a priori: It suffices to estimate densities from all local
appearance descriptors extracted from the images labeled with the
concept. The second is how do we learn these distributions in a
computationally efficient manner, while accounting for all data
available from each class? We show that this can be done with
recourse to a hierarchical density model proposed in for
image indexing purposes. In particular, it is shown that this
model enables the learning of semantic class densities with
a complexity equivalent to that of the unsupervised
formulation, while 1) obtaining more reliable semantic
density estimates, and 2) leading to significantly more
efficient image annotation.
Overall, the proposed implementation of SML leads to
optimal (in a minimum probability of error sense) annotation
and retrieval, and can be implemented with algorithms that
are conceptually simple, computationally efficient, and do not
require prior semantic segmentation of training images. Images
are simply represented as bags of localized feature vectors,
a mixture density estimated for each image, and the
mixtures (associated with all images annotated) with a
common semantic label pooled into a density estimate for
the corresponding semantic class. Semantic annotation and
retrieval are then implemented with a minimum probability
of error rule, based on these class densities. The overall SML
procedure is illustrated in Fig. 1.
Its efficiency and accuracy are demonstrated through an
extensive experimental evaluation, involving large-scale
databases and a number of state-of-the-art semantic image
labeling and retrieval methods. It is shown that SML
outperforms existing approaches by a significant margin,
not only in terms of annotation and retrieval accuracy, but
also in terms of efficiency. This large-scale experimental
evaluation also establishes a common framework for the
comparison of various methods that had previously only
been evaluated under disjoint experimental protocols ,
 , , , , , . This will hopefully simplify the
design of future semantic annotation and retrieval systems,
by establishing a set of common benchmarks against which
new algorithms can be easily tested. Finally, it is shown that
SML algorithms are quite robust with respect to the tuning
of their main parameters.
The paper is organized as follows: Section 2 defines the
semantic labeling and retrieval problems and reviews the
supervised OVA and unsupervised formulations. SML is
introduced in Section 3 and the estimation of semantic
densities is introduced in Section 4. In Section 5, we present
the experimental protocols used to evaluate the performance of SML. Section 6 then reports on the use of these
protocols to compare SML to the best known results from
the literature. Finally, the robustness of SML to parameter
tuning is studied in Section 6 and the overall conclusions of
this work are presented in Section 7.
SEMANTIC LABELING AND RETRIEVAL
Consider a database T ¼ fI1; . . . ; INg of images I i and a
semantic vocabulary L ¼ fw1; . . . ; wTg of semantic labels
wi. The goal of semantic image annotation is to, given an
image I, extract the set of semantic labels, or caption,1 w
that best describes I. The goal of semantic retrieval is to,
given a semantic label wi, extract the images in the
database that contain the associated visual concept. In
both cases, learning is based on a training set D ¼
fðI1; w1Þ; . . . ; ðID; wDÞg of image-caption pairs. The training set is said to be weakly labeled if 1) the absence of a
semantic label from caption wi does not necessarily mean
that the associated concept is not present in I i, and 2) it
is not known which image regions are associated with
CARNEIRO ET AL.: SUPERVISED LEARNING OF SEMANTIC CLASSES FOR IMAGE ANNOTATION AND RETRIEVAL
1. A caption is represented by a binary vector w of T dimensions whose
kth entry is 1 when wk is a member of the caption and 0 otherwise.
each label. For example, an image containing “sky” may
not be explicitly annotated with that label and, when it is,
no indication is available regarding which image pixels
actually depict sky. Weak labeling is expected in practical
retrieval scenarios, since 1) each image is likely to be
annotated with a small caption that only identifies the
semantics deemed as most relevant to the labeler, and
2) users are rarely willing to manually annotate image
regions. In the remainder of this section, we briefly
review the currently prevailing formulations for semantic
labeling and retrieval.
Supervised OVA Labeling
Under the supervised OVA formulation, labeling is
formulated as a collection of T detection problems that
determine the presence/absence of the concepts of L in the
image I. Consider the ith such problem and the random
variable Yi such that
if I contains concept wi
otherwise:
Given a collection of q feature vectors X ¼ fx1; . . . ; xqg
extracted from I, the goal is to infer the state of Yi with the
smallest probability of error, for all i. Using well-known
results from statistical decision theory , this is solved by
declaring the concept as present if
PXjYiðXj1ÞPYið1Þ  PXjYiðXj0ÞPYið0Þ;
where X is the random vector from which visual features
are drawn, PXjYiðxjjÞ is its conditional density under class
j 2 f0; 1g, and PYiðjÞ is the prior probability of that class.
Training consists of assembling a training set D1 containing all images labeled with the concept wi, a training set D0
containing the remaining images, and using some density
estimation procedure to estimate PXjYiðxjjÞ from Dj,
j 2 f0; 1g. Note that any images containing concept wi
which are not explicitly annotated with this concept are
incorrectly assigned to D0 and can compromise the
classification accuracy. In this sense, the supervised OVA
formulation is not amenable to weak labeling. Furthermore,
the set D0 is likely to be quite large when the vocabulary
size T is large and the training complexity is dominated by
the complexity of learning the conditional density for
In any case, (2) produces a sequence of labels
^wi 2 f0; 1g; i 2 f1; . . . ; Tg, and a set of posterior probabilities
PYijXð1jXÞ that can be taken as degrees of confidence on the
annotation of the image with concept wi. Note, however,
that these are posterior probabilities relative to different
classification problems and they do not establish an
ordering of importance of the semantic label wi as
descriptors of I. Nevertheless, the binary decision regarding the presence of each concept in the image is a minimum
probability of the error decision.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
MARCH 2007
Fig. 1. (a) Modeling of semantic classes. Images are represented as bags of localized features and a Gaussian mixture model (GMM) learned from
each mixture. The GMMs learned from all images annotated with a common semantic label (“mountain” in the example above) are pooled into a
density estimate for the class. (c) Semantic image retrieval and annotation are implemented with a minimum probability of error rule based on the
class densities.
Unsupervised Labeling
The basic idea underlying the unsupervised learning
formulation , , , , , , is to introduce
a variable L that encodes hidden states of the world. Each of
these states then defines a joint distribution for semantic
labels and image features. The various methods differ in the
definition of the states of the hidden variable: Some
associate a state to each image in the database , ,
while others associate them with image clusters , ,
and some model higher-level groupings, e.g., by topic .
The overall model is of the form
PX;WðX; wÞ ¼
PX;WjLðX; wjlÞPLðlÞ;
where S is the number of possible states of L, X is the set of
feature vectors extracted from I, and w is the caption of this
image. In order to avoid the difficulties of joint inference
over continuous and discrete random variables, and as
illustrated by the graphical model of Fig. 2a, the visual and
text components are usually assumed independent given
the state of the hidden variable
PX;WjLðX; wjlÞ ¼ PXjLðXjlÞPWjLðwjlÞ:
Since (3) is a mixture model, learning is usually based on the
expectation-maximization (EM) algorithm, but the details
depend on the particular definition of a hidden variable and
the probabilistic model adopted for PX;Wðx; wÞ. The simplest
model in this family , , which has also achieved the
best results in experimental trials, makes each image in the
training database a state of the latent variable,
PX;WðX; wÞ ¼
PXjLðXjlÞPWjLðwjlÞPLðlÞ;
where D is the training set size. This enables individual
estimation of PXjLðXjlÞ and PWjLðwjlÞ from each training
image, as is common in the probabilistic retrieval literature
 , , , therefore eliminating the need to iterate the
EM algorithm over the entire database (a procedure of
significant computational complexity). It follows that the
training complexity is equivalent to that of learning the
conditional densities for Yi ¼ 1 in the supervised OVA
formulation. This is significantly smaller than the learning
complexity of that formulation (which, as discussed above,
is dominated by the much more demanding task of learning
the conditionals for Yi ¼ 0). The text distribution PWjLðwjlÞ,
l 2 f1; . . . ; Dg is learned by maximum likelihood, from the
annotations of the lth training image, usually reducing to a
counting operation , . Note that, while the quality of
the estimates improves when the image is annotated with
all concepts that it includes, it is possible to compensate for
missing labels by using standard Bayesian (regularized)
estimates , . Hence, the impact of weak labeling is
not major under this formulation.
At annotation time, (3) is instantiated with the set of
feature vectors X extracted from the query I to obtain a
function of w that ranks all captions by relevance to the
latter. This function can be the joint density of (3) or the
posterior density
PWjXðwjXÞ ¼ PX;WðX; wÞ
Note that, while this can be interpreted as the Bayesian
decision rule for a classification problem with the states of
W as classes, such a class structure is not consistent with
the generative model of (3) which enforces a causal
relationship from L to W. Therefore, this formulation imposes
a mismatch between the class structure encoded in the generative
model (where classes are determined by the state of the
hidden variable) and that used for labeling (which assumes
that it is the state of W that determines the class). This
implies that the annotation decisions are not optimal in a
minimum probability of error sense.
Furthermore, when (4) is adopted, this suboptimality is
compounded by a very weak dependency between the
observation X and caption W variables, which are assumed
independent given L. The significance of the restrictions
imposed by this assumption is best understood by example.
Assume that the states of L encode topics, and one of the
topics is “bears.” Assume, further, that
the topic “bears” is associated with state L ¼ b,
there are only two types of bear images, “grizzly”
versus “polar” bears,
the two types have equal probability under the
“bears” topic, PWjLðgrizzlyjbÞ ¼ PWjLðpolarjbÞ ¼ 1=2,
the visual features are pixel colors.
Consider next the case where the images to label are those
shown in Fig. 3, and let Xi be the set of feature vectors
extracted from I i; i 2 f1; 2g. From (4), it follows that
PWjX;LðwjX i; bÞ ¼ PW;XjLðw; XijbÞ
PXjLðX ijbÞ
¼ PWjLðwjbÞ
and, for both values of i,
PWjX;LðgrizzlyjXi; bÞ ¼ PWjX;LðpolarjXi; bÞ ¼ 1=2:
CARNEIRO ET AL.: SUPERVISED LEARNING OF SEMANTIC CLASSES FOR IMAGE ANNOTATION AND RETRIEVAL
Fig. 2. Graphical model of the (a) unsupervised and (b) SML models.
Fig. 3. Two images of the “bear” topic. A grizzly bear on the left and a
polar bear on the right.
This means that, even though a mostly brown (white) image
has been observed, the labeling process still produces the
label “polar” (“grizzly”) 50 percent of the time, i.e., with the
same frequency as before the observation! Given that the
goal of semantic annotation is exactly to learn a mapping from
visual features to labels, the assumption of independence
given the hidden state is unlikely to lead to powerful
labeling systems.
SUPERVISED MULTICLASS LABELING
SML addresses the limitations of unsupervised labeling by
explicitly making the elements of the semantic vocabulary the
classes of a multiclass labeling problem. That is, by introducing
1) a random variable W, which takes values in f1; . . . ; Tg, so
that W ¼ i if and only if x is a sample from concept wi and
2) a set of class-conditional distributions PXjWðxjiÞ; i 2
f1; . . . ; Tg for the distribution of visual features given the
semantic class. The graphical model underlying SML is
shown in Fig. 2b. Using, once again, well-known results
from statistical decision theory , it is not difficult to
show that both labeling and retrieval can be implemented
with a minimum probability of error if the posterior
probabilities
PWjXðijxÞ ¼ PXjWðxjiÞPWðiÞ
are available, where PWðiÞ is the prior probability of the
ith semantic class. In particular, given a set of feature vectors
X extracted from a (previously unseen) test image I, the
minimum probability of an error label for that image is
iðXÞ ¼ arg max
PWjXðijXÞ:
Similarly, given a query concept wi, a minimum probability
of error semantic retrieval can be achieved by returning the
database image of index
jðwiÞ ¼ arg max
PXjWðX jjiÞ;
where Xj is the set of feature vectors extracted from the
jth database image, Ij. When compared to the OVA
formulation, SML relies on a single multiclass problem of
T classes instead of a sequence of T binary detection
This has several advantages. First, there is no longer a
need to estimate T nonclass distributions (Yi ¼ 0 in (1)), an
operation which, as discussed above, is the computational
bottleneck of OVA. On the contrary, as will be shown in
Section 4, it is possible to estimate all semantic densities
PXjWðxjiÞ with computation equivalent to that required to
estimate one density per image. Hence, SML has a learning
complexity equivalent to the simpler of the unsupervised
labeling approaches (5). Second, the ith semantic class
density is estimated from a training set Di containing all
feature vectors extracted from images labeled with
concept wi. While this will be most accurate if all images
that contain the concept include wi in their captions, images
for which this label is missing will simply not be considered.
If the number of images correctly annotated is large, this is
not likely to make a practical difference. If that number is
small, missing images can always be compensated for by
adopting Bayesian (regularized) estimates. In this sense,
SML is equivalent to the unsupervised formulation and,
unlike supervised OVA, not severely affected by weak
Third, at annotation time, SML produces an ordering of
the semantic classes by posterior probability PWjXðijXÞ.
Unlike OVA, these posteriors are relative to the same
classification problem, a problem where the semantic
classes compete to explain the query. This ordering is, in
fact, equivalent to that adopted by the unsupervised
learning formulation (6), but now leads to a Bayesian
decision rule that is matched to the class structure of the
underlying generative model. It is therefore optimal in a
minimum probability of error sense. Finally, by not
requiring the modeling of the joint likelihood of words
and visual features, SML does not require the independence
assumptions usually associated with the unsupervised
formulation.
ESTIMATION OF SEMANTIC CLASS DISTRIBUTIONS
Given the semantic class densities PXjWðxjiÞ; 8i, both
annotation and retrieval are relatively trivial operations.
They simply consist of the search for the solution of (8) and
(9), respectively, where PWðiÞ can be estimated by the
relative frequencies of the various classes in the database
and PXðxÞ ¼ P
i PXjWðxjiÞPWðiÞ. However, the estimation
of the class densities raises two interesting questions. The
first is whether it is possible to learn the densities of
semantic concepts in the absence of a semantic segmentation for each image in the database. This is the subject of
Section 4.1. The second is computational complexity: If the
database is large, the direct estimation of PXjWðxjiÞ from the
set of all feature vectors extracted from all images that
contain concept wi is usually infeasible. One solution is to
discard part of the data, but this is suboptimal in the sense
that important training cases may be lost. Section 4.2
discusses more effective alternatives.
Modeling Classes Without Segmentation
Many of the concepts of interest for semantic annotation or
retrieval only occupy a fraction of the images that contain
them. While objects, e.g., “bear” or “flag,” are prominent
examples of such concepts, this property also holds for
more generic semantic classes, e.g., “sky” or “grass.” Hence,
most images are a combination of various concepts and,
ideally, the assembly of a training set for each semantic
class should be preceded by 1) careful semantic segmentation, and 2) identification of the image regions containing
the associated visual feature vectors. In practice, the manual
segmentation of all database images with respect to all
concepts of interest is infeasible. On the other hand,
automated segmentation methods are usually not able to
produce a decomposition of each image into a plausible set
of semantic regions. A pressing question is then whether it
is possible to estimate the densities of a semantic class
without prior semantic segmentation, i.e., from a training
set containing a significant percentage of feature vectors
from other semantic classes.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
MARCH 2007
We approach this question from a multiple instance
learning perspective , , , , . Unlike classical
learning, which is based on sets of positive and negative
examples, multiple instance learning addresses the problem
of how to learn models from positive and negative bags of
examples. A bag is a collection of examples and is considered
positive if at least one of those examples is positive.
Otherwise, the bag is negative. The basic intuition is quite
simple: While the negative examples present in positive
bags tend to be spread all over the feature space, the
positive examples are much more likely to be concentrated
within a small region of the latter. Hence, the empirical
distribution of positive bags is well approximated by a
mixture of two components: a uniform component from
which negative examples are drawn, and the distribution of
positive examples. The key insight is that, because it must
integrate to one, the uniform component tends to have
small amplitude (in particular, if the feature space is highdimensional). It follows that, although the density of the
common concept may not be dominant in any individual
image, the consistent appearance in all images makes it
dominant over the entire positive bag.
The principle is illustrated in Fig. 4 for a hypothetical set
of images containing four semantic Gaussian concepts, each
with probability i 2 ½0; 1 (i.e., occupying i of the image
area). Introducing a hidden variable L for the image
number, the distribution of each image can be written as
PXjLðxjlÞ ¼
i¼1 i ¼ 1, ðl
iÞ are the mean and variance of
the ith Gaussian associated with the lth image, with
Gðx; ; Þ ¼
eðxÞ2=22, and the distribution of the bag
of D images is
PXjLðxjlÞPLðlÞ ¼ 1
where we have assumed that all images are equally likely.
If one of the four components (e.g., the first, for
simplicity) is always the density of concept w, e.g., l
1 ¼ w; 8l, and the others are randomly selected
from a pool of Gaussians of uniformly distributed mean
and standard deviation, then
¼ 1Gðx; w; wÞ þ
and, from the law of large numbers, as D ! 1
PXðxÞ ¼ 1Gðx; w; wÞ þ ð1  1Þ
Gðx; ; Þp;ð; Þdd;
CARNEIRO ET AL.: SUPERVISED LEARNING OF SEMANTIC CLASSES FOR IMAGE ANNOTATION AND RETRIEVAL
Fig. 4. Synthetic example of multiple instance learning of semantic class densities. Left and center columns: Probability distributions of individual
images ðPXjLðxjlÞÞ. Each image distribution is simulated by a mixture of the distribution of the concept of interest (dashed line) and three distributions
of other visual concepts present in the image (solid line). All concepts are simulated as Gaussians of different mean and variance. Right column:
empirical distribution PXðxÞ obtained from a bag of D ¼ 1; 000 simulated images, the estimated class conditional distribution (using maximum
likelihood parameter estimates under a mixture of Gaussians model) ^PXjWðxjwÞ, and the true underlying distribution PXjWðxjwÞ ¼ Gðx; w; wÞ of the
common concept w. Each row is associated with a different value of 1 in (10). (a) 1 ¼ 0:3. (b) 2 ¼ 0:4.
where p;ð; Þ is the joint distribution of the means and
variances of the components other than that associated with
w. Hence, the distribution of the positive bag for concept w
is a mixture of 1) the concept’s density and 2) the average of
many Gaussians of different mean and covariance. The
latter converges to a uniform distribution that, in order to
integrate to one, must have small amplitude, i.e.,
D!1 PXðxÞ ¼ 1Gðx; w; wÞ þ ð1  1Þ;
with   0.
Fig. 4 presents a simulation of this effect when
 2 ½100; 100,  2 ½0:1; 10, w ¼ 30, w ¼ 3:3, and the
bag contains D ¼ 1; 000 images. Fig. 5 presents a comparison between the estimate of the distribution of w,
^PXjWðxjwÞ, obtained by fitting (in the maximum likelihood
sense) a mixture of five Gaussians (using the EM
algorithm ) to the entire bag, and the true distribution
PXjWðxjwÞ ¼ Gðx; w; wÞ. The comparison is based on the
Kullback-Leibler (KL) divergence
KLð ^PXjWkPXjWÞ ¼
^PXjWðxjwÞ log
^PXjWðxjwÞ
PXjWðxjwÞ ;
and shows that, even when 1 is small (e.g., 1 ¼ 0:3), the
distribution of concept w dominates the empirical distribution of the bag, as the number D of images increases.
Fig. 6 shows that the same type of behavior is observed
in real image databases. In this example, images are
represented as a collection of independent feature vectors,
as discussed in detail in Section 4.3, and all densities are
modeled as Gaussian mixtures. Semantic densities were
learned over a set of training images from the Corel
database (see Section 6), using the method described in
Section 4.2. A set of test images were then semantically
segmented by 1) extracting a feature vector from each
location in the test image, and 2) classifying this feature
vector into one of the semantic classes present in the
image (semantic classes were obtained from the caption
provided with the image ). Fig. 6 depicts the indices of
the classes to which each image location was assigned
(class indices shown in the color bar on the right of the
image) according to
arg maxi PWjXðijXÞ;
if PWjXðijXÞ > 
otherwise;
where X is the set of feature vectors extracted from the
image to segment,  ¼ 0:5,
PWjXðijXÞ ¼ PXjWðXjiÞPWðiÞ
PXjWðXjiÞ ¼
PXjWðxkjiÞ;
PWðiÞ uniform,
PXðXÞ ¼ PXjWðXjiÞPWðiÞ þ PXjWðXj:iÞPWð:iÞ;
and the density for “no class i” ð:iÞ learned from all
training images that did not contain class i in their caption.
In order to facilitate visualization, the posterior maps were
reproduced by adding a constant, the index of the class of
largest posterior, to that posterior. Regions where all
posteriors were below threshold were declared “undecided.” Finally, the segmentation map was smoothed with a
Gaussian filter. Note that, while coarse, the segmentations
do 1) split the images into regions of different semantics,
and 2) make correct assignments between regions and
semantic descriptors. This shows that the learned densities
are close to the true semantic class densities.
Density Estimation
Given the training set Di of images containing concept wi,
the estimation of the density PXjWðxjiÞ can proceed in four
different ways: direct estimation, model averaging, naive
averaging, and hierarchical estimation.
4.2.1 Direct Estimation
Direct estimation consists of estimating the class density
from a training set containing all feature vectors from all
images in Di. The main disadvantage of this strategy is that,
for classes with a sizable number of images, the training set
is likely to be quite large. This creates a number of practical
problems, e.g., the requirement for large amounts of
memory, and makes sophisticated density estimation
techniques infeasible. One solution is to discard part of
the data, but this is suboptimal in the sense that important
training cases may be lost. We have not been able to
successfully apply this strategy.
4.2.2 Model Averaging
Model averaging exploits the idea of (3) to overcome the
computational complexity of direct estimation. It performs
the estimation of PXjWðxjiÞ in two steps. In the first step, a
density estimate is produced for each image, originating a
sequence PXjL;Wðxjl; iÞ; l 2 f1; . . . Dig, where L is a hidden
variable that indicates the image number. The class density
is then obtained by averaging the densities in this sequence
PXjWðxjiÞ ¼ 1
PXjL;Wðxjl; iÞ:
Note that this is equivalent to the density estimate obtained
under the unsupervised labeling framework if the text
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
MARCH 2007
Fig. 5. KL divergence between the estimated, ^PXjWðxjwÞ, and actual,
PXjWðxjwÞ, class conditional density of concept w as a function of the
number of training images D, for different values of 1. Error bars
illustrate the standard deviation over a set of 10 experiments for each
combination of D ¼ f1;    ; 1; 000g and 1 ¼ 0:3; 0:4.
component of the joint density of (3) is marginalized and
the hidden states are images (as in (5)). The main difference
is that, while under SML, the averaging is done only over
the set of images that belong to the semantic class, under
unsupervised labeling, it is done over the entire database.
This, once again, reflects the lack of classification optimality
of the later.
The direct application of (13) is feasible when the
densities PXjL;Wðxjl; iÞ are defined over a (common) partition of the feature space. For example, if all densities are
histograms defined on a partition of the feature space S into
Q cells fSqg; q ¼ 1;    ; Q and hq
i;l, the number of feature
vectors from class i that land on cell Sq for image l, then the
average class histogram is simply
However, when 1) the underlying partition is not the same
for all histograms or 2) more sophisticated models (e.g.,
mixture or nonparametric density estimates) are adopted,
model averaging is not as simple.
4.2.3 Naive Averaging
Consider, for example, the Gauss mixture model
PXjL;Wðxjl; iÞ ¼
i;lG x; k
i;l is a probability mass function such that
i;l ¼ 1. Direct application of (13) leads to
PXjWðxjiÞ ¼ 1
i;lG x; k
i.e., a Di-fold increase in the number of Gaussian components per mixture. Since, at annotation time, this probability
has to be evaluated for each semantic class, it is clear that
straightforward model averaging will lead to an extremely
slow annotation process.
4.2.4 Mixture Hierarchies
One efficient alternative to the complexity of model
averaging is to adopt a hierarchical density estimation
method first proposed in for image indexing. This
method is based on a mixture hierarchy where children
CARNEIRO ET AL.: SUPERVISED LEARNING OF SEMANTIC CLASSES FOR IMAGE ANNOTATION AND RETRIEVAL
Fig. 6. Original images (top row) and posterior assignments (bottom row) for each image neighborhood (Undecided means that no class has a
posterior bigger than  in (11).).
densities consist of different combinations of subsets of the
parents’ components. In the semantic labeling context,
image densities are children and semantic class densities
are their parents. As shown in , it is possible to estimate
the parameters of class mixtures directly from those
available for the individual image mixtures, using a twostage procedure. The first stage is the naive averaging of (15).
Assuming that each image mixture has K components, this
leads to a class mixture of DiK components with parameters
; j ¼ 1; . . . ; Di; k ¼ 1; . . . ; K:
The second is an extension of EM which clusters the
Gaussian components into an M-component mixture, where
M is the number of components desired at the class level.
Denoting by fm
c g; m ¼ 1; . . . ; M the parameters of
the class mixture, this algorithm iterates between the
following steps:
E-step: Compute
2tracefðm
2tracefðl
where N is a user-defined parameter (see for details)
set to N ¼ 1 in all our experiments.
M-step: Set
j; where wm
Note that the number of parameters in each image mixture
is orders of magnitude smaller than the number of feature
vectors in the image itself. Hence, the complexity of
estimating the class mixture parameters is negligible when
compared to that of estimating the individual mixture
parameters for all images in the class. It follows that the
overall training complexity is dominated by the latter task,
i.e., only marginally superior to that of naive averaging and
significantly smaller than that associated with direct
estimation of class densities. On the other hand, the
complexity of evaluating likelihoods is exactly the same as
that of direct estimation and significantly smaller than that
of naive averaging.
One final interesting property of the EM steps above is
that they enforce a data-driven form of covariance regularization. This regularization is visible in (20) where the
variances on the left-hand side can never be smaller than
those on the right-hand side. We have observed that, due to
this property, hierarchical class density estimates are much
more reliable than those obtained with direct learning .
Algorithm Description
In this section, we describe the three algorithms used in
this work, namely, training, annotation, and retrieval. We
also identify the parameters of the training algorithm that
affect the performance of the the annotation and retrieval
tasks. For the training algorithm, we assume a training
set D ¼ fðI1; w1Þ; . . . ; ðID; wDÞg of image-caption pairs,
where Ii 2 T D with T D ¼ fI1; . . . ; I Dg, and wi  L, with
L ¼ fw1; . . . ; wTg. The steps of the training algorithm are:
For each semantic class w 2 L,
Build a training image set ~
TD  T D, where w 2
wi for all I i 2 ~
For each image I 2 ~
Decompose I into a set of overlapping 8
8 regions, extracted with a sliding window
that moves by two pixels between consecutive samples (note that, in all experiments reported in this work, images were
represented in the YBR color space).
Compute a feature vector, at each location of
the three YBR color channels, by the application of the discrete cosine transform (DCT)
(see the Appendix, which can be found on
the Computer Society Digital Library at
 
for more information). Let the image be
represented by
B ¼f½xY ; xB; xR1; ½xY ; xB; xR2;
. . . ; ½xY ; xB; xRMg;
where ½xY ; xB; xRm is the concatenation of
the DCT vectors extracted from each of the
YBR color channels at image location
m 2 f1; . . . ; Mg. Note that the 192-dimensional YBR-DCT vectors are concatenated
by interleaving the values of the YBR
feature components. This facilitates the
application of dimensionality reduction
techniques due to the well-known energy
compaction properties of the DCT. To
simplify notation, we hereafter replace
½xY ; xB; xR with x.
Assuming that the feature vectors extracted
from the regions of image I are sampled
independently, find the mixture of eight
Gaussians that maximizes their likelihood
using the EM algorithm (in all experiments, the Gaussian components had diagonal covariance matrices). This produces the
following class conditional distribution for
each image:
PXjWðxjIÞ ¼
I are the maximum likelihood parameters for image I and mixture
component k.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
MARCH 2007
Fit a Gaussian mixture of 64 components by
applying the hierarchical EM algorithm of (17)-
(20) to the image-level mixtures of (21). This
leads to a conditional distribution for class w of
PXjWðxjwÞ ¼
We refer to this representation as GMM-DCT. The
parameters that may affect labeling and retrieval performance are 1) number of hierarchy levels on step (1-c),
2) number of DCT feature dimensions, and c) number of
mixture components for the class hierarchy in step (1-c).
The number of hierarchical levels in (1-c) was increased
from two to three in some experiments by adding an
intermediate level that splits the image mixtures into
groups of 250 and learns a mixture for each of these
groups. In Section 6, we provide a complete study of the
performance of our method as a function of each one of
those parameters.
The annotation algorithm processes test images It 62 T D,
executing the following steps:
Step (1-b-i) of the training algorithm.
Step (1-b-ii) of the training algorithm.
For each class wi 2 L, compute
log PWjXðwijBÞ ¼ log PXjWðBjwiÞ þ log PWðwiÞ
 log PXðBÞ;
where B is the set of DCT features extracted from
image I t,
log PXjWðBjwiÞ ¼
log PXjWðxjwiÞ;
PWðwiÞ is computed from the training set as the
proportion of images containing annotation wi, and
PXðBÞ is a constant in the computation above across
different wi 2 L.
Annotate the test image with the five classes wi of
largest posterior probability, log PWjXðwijBÞ.
Finally, the retrieval algorithm takes as inputs 1) a
semantic class wi, and 2) a database of test images T T, such
T T D ¼ ;. It consists of the following steps:
For each image I t 2 T T, perform steps 1)-4) of the
annotation algorithm.
Rank the images labeled with the query word by
decreasing PWjXðwijBÞ.
We have found, experimentally, that the restriction to the
images for which the query is a top label increases the
robustness of the ranking (as compared by the simple
ranking by label posterior, PXjWðBjwiÞ).
EXPERIMENTAL PROTOCOL
As should be clear from the discussion of the previous
sections, a number of proposals for semantic image
annotation and retrieval have appeared in the literature.
In general, it is quite difficult to compare the relative
performances of the resulting algorithms due to the lack of
evaluation on a common experimental protocol. Since the
implementation and evaluation of a labeling/retrieval
system can be quite time-consuming, it is virtually
impossible to compare results with all existing methods.
Significant progress has, however, been accomplished in the
recent past by the adoption of a “de facto” evaluation
standard, that we refer to as Corel5K, by a number of
research groups , , .
There are, nevertheless, two significant limitations
associated with the Corel5K protocol. First, because it is
based on a relatively small database, many of the semantic
labels in Corel5K have a very small number of examples.
This makes it difficult to guarantee that the resulting
annotation systems have good generalization. Second,
because the size of the caption vocabulary is also relatively
small, Corel5K does not test the scalability of annotation/
retrieval algorithms. Some of these limitations are corrected
by the Corel30K protocol, which is an extension of Corel5K
based on a substantially larger database. None of the two
protocols is, however, easy to apply to massive databases,
since both require the manual annotation of each training
image. The protocol proposed by Li and Wang (which
we refer to as PSU) is a suitable alternative for testing largescale labeling and retrieval systems.
Because each of the three protocols has been used to
characterize a nonoverlapping set of semantic labeling/
retrieval techniques, we evaluated the performance of SML
on all three. In addition to enabling a fair comparison of
SML with all previous methods, this establishes a data point
common to the three protocols that enables a unified view
of the relative performances of many previously tested
systems. This, we hope, will be beneficial to the community.
We describe the three protocols in the remainder of this
section and then present the results of our experiments in
the following.
The Corel5k and Corel30k Protocols
The evaluation of a semantic annotation/labeling and
retrieval system requires three components: an image
database with manually produced annotations, a strategy
to train and test the system, and a set of measures of
retrieval and annotation performance. The Corel5K benchmark is based on the Corel image database , , :
5,000 images from 50 Corel Stock Photo CDs were divided
into a training set of 4,000 images, a validation set of
500 images, and a test set of 500 images. An initial set of
model parameters is learned on the training set. Parameters
that require cross-validation are then optimized on the
validation set, after which, this set is merged with the
training set to build a new training set of images. Noncrossvalidated parameters are then tuned with this training set.
Each image has a caption of one to five semantic labels, and
there are 371 labels in the data set.
Image annotation performance is evaluated by comparing
the captions automatically generated for the test set, with the
human-produced ground-truth. Similarly to , , we
define the automatic annotation as the five semantic classes of
largest posterior probability, and compute the recall and
precision of every word in the test set. For a given semantic
descriptor, assuming that there are wH human annotated
images in the test set and the system annotates wauto, of which
CARNEIRO ET AL.: SUPERVISED LEARNING OF SEMANTIC CLASSES FOR IMAGE ANNOTATION AND RETRIEVAL
wC are correct, recall and precision are given by recall ¼ wC
and precision ¼
wauto , respectively. As suggested in previous
works , ,thevaluesofrecallandprecisionareaveraged
overthesetofwordsthatappearinthetestset.Finally,wealso
consider the number of words with nonzero recall (i.e., words
with wC > 0), which provides an indication of how many
words the system has effectively learned.
The performance of semantic retrieval is also evaluated
by measuring precision and recall. Given a query term and
the top n image matches retrieved from the database, recall
is the percentage of all relevant images contained in the
retrieved set, and precision is the percentage of n which are
relevant (where relevant means that the ground-truth
annotation of the image contains the query term). Once
again, we adopted the experimental protocol of ,
evaluating retrieval performance by the mean average
precision (MAP). This is defined as the average precision,
over all queries, at the ranks, where recall changes (i.e.,
where relevant items occur).
The Corel30K protocol is similar to Corel5K but substantially larger, containing 31,695 images and 5,587 words.
Of the 31,695 images, 90 percent were used for training
(28,525 images) and 10 percent for testing (3,170 images).
Only the words (950 in total) that were used as annotations
for at least 10 images were trained. Corel30K is much richer
than Corel5K in terms of number of examples per label and
database size, therefore posing a much stronger challenge to
nonscalable systems.
The PSU Protocol 
For very large image sets, it may not even be practical to
label each training image with ground-truth annotations.
An alternative approach, proposed by Li and Wang , is
to assign images to loosely defined categories, where each
category is represented by a set of words that characterize
the category as a whole, but may not accurately characterize
each individual image. For example, a collection of images
of tigers running in the wild may be annotated with the
words “tiger,” “sky,” “grass,” even though some of the
images may not actually depict sky or grass. We refer to this
type of annotation as noisy supervised annotation. While it
reduces the time required to produce ground-truth annotations, it introduces noise in the training set, where each
image in some category may contain only a subset of the
category annotations.
Li and Wang relied on noisy supervised annotation
to label very large databases by implementing a 2-step
annotation procedure, which we refer to as supervised
category-based labeling (SCBL). The image to label is first
processed with an image category classifier that identifies
the five image categories to which the image is most likely
to belong. The annotations from those categories are then
pooled into a list of candidate annotations with frequency
counts for reoccurring annotations. The candidate annotations are then ordered based on the hypothesis test that a
candidate annotation has occurred randomly in the list of
candidate annotations.
More specifically, the probability that the candidate
word appears at least j times in k randomly selected
categories is
where Ið:Þ is the indicator function, n is the total number of
image categories, and m is the number of image categories
containing the word. For n; m
k, the probability can be
approximated by
pið1  pÞki;
where p ¼ m=n is the frequency with which the word
appears in the annotation categories. A small Pðj; kÞ
indicates a low probability that the candidate word
occurred randomly (i.e., the word has high significance as
an annotation). Hence, candidate words with Pðj; kÞ below
a threshold value are selected as the annotations.
Li and Wang also proposed an experimental protocol,
based on noisy supervised annotation for the evaluation of
highly scalable semantic labeling and retrieval systems. This
protocol, which we refer to as PSU, is also based on the Corel
image set, containing 60,000 images with 442 annotations.
The image set was split into 600 image categories consisting
of 100 images each, which were then annotated with a
general description that reflected the image category as a
whole. For performance evaluation, 40 percent of the PSU
images were reserved for training (23,878 images), and the
remainder (35,817 images) were used for testing. Note that Li
and Wang only used 4,630 of the 35,817 possible test
images, whereas all the test images were used in the
experiments reported here. Annotation and retrieval performance were evaluated with the same measures used in
Corel5K and Corel30K.
EXPERIMENTAL RESULTS
In this section, we compare the performance of SML with
the previous approaches discussed above. We start with a
comparison against the less scalable unsupervised labeling
methods, using the Corel5K setup. We then compare SML
to SCBL on the larger PSU benchmark. Finally, we perform
a study of the scalability and robustness of SML. The
experiments reported here were conducted on a cluster of
3,000 state-of-the-art Linux machines. Some of these
experiments involved extensive replication of a baseline
experiment with various configurations of the free parameters of each retrieval system. In the most extreme cases,
computing time was as high as 1 hour for Corel5K and
34 hours for PSU, although these times are not definitive
since the experiments ran concurrently with, and were
preempted by, other jobs on the cluster.
Comparison of SML and Unsupervised Labeling
Table 1 presents the results obtained for SML and various
previously proposed methods (results from , ) on
Corel5K. Specifically, we considered the co-occurrence
model of , the translation model of , the continuous-space relevance model of , , and the multiple-
Bernoulli relevance model (MBRM) of . Overall, SML
achieves the best performance, exhibiting a gain of 16 percent
in recall for an equivalent level of precision when compared
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
MARCH 2007
to the previous best results (MBRM). Furthermore, the
number of words with positive recall increases by 15 percent.
Fig. 7 presents some examples of the annotations produced.
Note that, when the system annotates an image with a
descriptor not contained in the human-made caption, this
annotation is frequently plausible.
We next analyze the complexity of the annotation
process. Assuming that there are D training images and
each produces R visual feature vectors, the complexity of
CRM and MBRM is OðDRÞ. On the other hand, SML has
complexity of OðTRÞ, where T is the number of semantic
classes (or image annotations), and is usually much smaller
than D. For example, Fig. 8 presents the per-image
annotation time required by each of the methods on the
Corel data set, as a function of D. Note the much smaller
rate of increase, with database size, of the SML curve.
The performance of semantic retrieval was evaluated by
measuring precision and recall as explained in Section 5.1.
Table 2 shows that, for ranked retrieval on Corel, SML
produces results superior to those of MBRM. In particular,
it achieves a gain of 40 percent mean average precision on
the set of words that have positive recall. Fig. 9 illustrates
the retrieval results obtained with one word queries for
challenging visual concepts. Note the diversity of visual
appearance of the returned images, indicating that SML has
good generalization ability.
Comparison of SML and SCBL
To evaluate the performance of SML on large-scale retrieval
and annotation tasks, we compared its performance to that of
SCBL under the PSU protocol. For this, we started by
comparing the image categorization performance between
the GMM-DCT class representation described in Section 4.3
and the representation of . In , an image category is
represented by a two-dimensional multiresolution hidden
Markov model (2D-MHMM) defined on a feature space of
localized color and wavelet texture features at multiple
scales. An image was considered to be correctly categorized if
any of the top r categories is the true category. Table 3 shows
the accuracy of image categorization using the two class
representations. GMM-DCT outperformed the 2D-MHMM
of in all cases, with an improvement of about 0.10 (from
0.26 to 0.36). Fig. 10 shows the categorization accuracy of
GMM-DCT versus the dimension of the DCT feature space. It
can be seen that the categorization accuracy increases with
the dimension of the feature space, but remains fairly stable
over a significant range of dimensions.
CARNEIRO ET AL.: SUPERVISED LEARNING OF SEMANTIC CLASSES FOR IMAGE ANNOTATION AND RETRIEVAL
Performance Comparison of Automatic Annotation on Corel5K
Fig. 7. Comparison of SML annotations with those of a human subject.
Fig. 8. Comparison of the time complexity for the annotation of a test
image on the Corel data set.
We next compared the annotation performance of the
two steps of SCBL, using the GMM-DCT representation (we
denote this combination by SCBL-GMM-DCT) and .
Following , the performance was measured using
“mean coverage,” which is the percentage of ground-truth
annotations that match the computer annotations. Table 4
shows the mean coverage of SCBL-GMM-DCT and of 
using a threshold of 0.0649 on Pðj; kÞ, as in , and
without using a threshold. Annotations using GMM-DCT
outperform those of by about 0.13 (from 0.22 to 0.34
using a threshold, and 0.47 to 0.61 for no threshold). Fig. 11
shows the mean coverage versus the dimension of the DCT
feature space. Again, performance increases with feature
space dimension, but remains fairly stable over a large
range of dimensions.
Finally, we compared SCBL and SML when both
methods used the GMM-DCT representation. SCBL annotation was performed by thresholding the hypothesis test
(SCBL-GMM-DCT threshold), or by selecting a fixed
number annotations (SCBL-GMM-DCT fixed). SML classifiers were learned using both 2-level and 3-level hierarchies.
Fig. 12 presents the precision-recall (PR) curves produced by
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
MARCH 2007
Retrieval Results on Corel5K
Fig. 9. Semantic retrieval on Corel. Each row shows the top five matches to a semantic query. From top to bottom: “blooms,” “mountain,” “pool,”
“smoke,” and “woman.”
Accuracy of Image Categorization on the PSU Database
Fig. 10. Accuracy of image categorization on PSU using GMM-DCT
versus the dimension of the DCT feature space.
the two methods. Note that SML trained with the 3-level
hierarchy outperforms the 2-level hierarchy. This is evidence that the hierarchical EM algorithm provides some
regularization of the density estimates, which improves the
performance. The SML curve has the best overall precision
at 0.236, and its precision is clearly superior to that of SCBL
at most levels of recall. There are, however, some levels
where SCBL-GMM-DCT leads to a better precision. This is
due to the coupling of words within the same image
category and to the noise in the ground-truth annotations
Note that if the correct category is in the top five
classified categories, then the list of candidate words will
contain all of the ground-truth words for that image.
Eventually, as the image is annotated with more words
from the candidate list, these ground-truth words will be
included, regardless of whether the ground truth actually
applies to the image (i.e., when the ground truth is noisy).
As a result, recall and precision are artificially inflated as
the number of annotations increases. On the other hand, for
SML, each word class is learned separately from the other
words. Hence, images will not be annotated with the noisy
word if the concept is not present, and the precision and
recall can suffer. Finally, for SCBL-threshold, the PR curve
has an unusual shape. This is an artifact that arises from
thresholding a hypothesis test that has discrete levels.
In summary, the experimental results show that the
GMM-DCT representation substantially outperforms the
2D-MHMM of in both image categorization and
annotation using SCBL. When comparing SML and SCBL
based on the GMM-DCT representation, SML achieves the
best overall precision, but for some recall levels, SCBL can
achieve a better precision due to the coupling of annotation
words and noise in the annotation ground truth.
Robustness and Scalability of SML
We have already seen that, under the SCBL model, both the
categorization and annotation performance of the GMM-
DCT representation are quite stable with respect to the
feature space dimension. We now report on experiments
performed to evaluate the robustness of SML-GMM-DCT
method to its tunable parameters. Fig. 13a shows the PR
curves obtained for annotation on Corel5K, as a function of
the number of mixture components used to model class
conditional densities. Note that the PR curve remains fairly
CARNEIRO ET AL.: SUPERVISED LEARNING OF SEMANTIC CLASSES FOR IMAGE ANNOTATION AND RETRIEVAL
Mean Coverage for Annotation on the PSU Database
Fig. 11. Mean coverage of annotation of PSU using SCBL-GMM-DCT
versus the dimension of the DCT feature space.
Fig. 12. Precision-Recall for SCBL and SML using GMM-DCT on the
PSU database.
Fig. 13. Precision-recall curves for annotation on Corel5K using SML-GMM-DCT while varying: (a) the number of mixture components ðCÞ and
(b) the dimension of the DCT feature space ðdÞ for 64 mixture components.
stable above 64 components. Fig. 13b shows the PR curve
for annotation with 64 components while varying the
feature space dimension. In this case, stability is achieved
above 63 dimensions.
To test scalability, the SML annotation experiment was
repeated on the larger Corel30K. Fig. 14 shows the
performance obtained with 64 and 128 mixture components, learned with either the 2-level or 3-level hierarchy.
The first observation that can be made from the figure is
that a 3-level hierarchy outperforms the standard 2-level
hierarchy for both 64 and 128 components. This indicates
that the density estimates achieved with the 3-level
structure are superior to those of the standard hierarchical
organization. The differences are nevertheless not staggering, suggesting some robustness of SML with respect to this
parameter. A second interesting observation is that annotation performance on the larger database is qualitatively
similar to that obtained on the smaller Corel5K database
(e.g., compare the shape of the PR curves with those of
Fig. 13a), albeit with overall lower precision and recall
levels. This is due to the difficulty of learning specialized
annotations, and to the presence of different annotations
with the same semantic meaning, which are both more
frequent on Corel30K. It appears likely that the absolute
values of PR could be improved by complementing SML
with a language model which accounts for the multiple
labels that can usually be assigned to the same semantic
concept (e.g., “car” versus “automobile” versus “vehicle,”
etc.). These types of operations are routinely done in textual
information retrieval (e.g., through the application of query
expansion techniques ) and could be easily combined with
SML. We believe that this is an interesting topic for further
Overall, these experiments indicate that 1) SML is fairly
stable with respect to its parameter settings, and 2) results
on Corel5K are a good indication of the relative performance of different techniques on larger databases (albeit the
absolute values of PR are likely to be overestimated).
Ranked Retrieval Results
Fig. 15 presents results of ranked retrieval on Corel5K for
different numbers of mixture components and DCT
dimensions. Fig. 15a depicts the MAP for all 260 words,
while the one in the center shows the same curves for words
with nonzero recall. In both cases, the MAP increases with
the number of mixture components, stabilizing above
128 components. Fig. 15b shows the number of words with
nonzero recall, which decreases with the number of mixture
components, once again stabilizing above 128 components.
CONCLUSIONS
In this work, we have presented a unifying view of stateof-the-art techniques for semantic-based image annotation
and retrieval. This view was used to identify limitations of
the different methods and motivated the introduction of
SML. When compared with previous approaches, SML has
the advantage of combining classification and retrieval
optimality with 1) scalability in database and vocabulary
sizes, 2) ability to produce a natural ordering for semantic
labels at annotation time, and 3) implementation with
algorithms that are conceptually simple and do not require
prior semantic image segmentation. We have also presented the results of an extensive experimental evaluation,
under various previously proposed experimental protocols, which demonstrated superior performance with
respect to a sizable number of state-of-the-art methods,
for both semantic labeling and retrieval. This experimental
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
MARCH 2007
Fig. 14. Precision-recall curves for annotation on Corel30K using
SML-GMM-DCT.
Fig. 15. Ranked retrieval on Corel5K using SML-GMM-DCT with different mixture components ðCÞ: (a) MAP for all the words, (b) MAP for words with
nonzero recall, and (c) number of words with nonzero recall.
evaluation has also shown that the performance of SML is
quite robust to variability in parameters such as the
dimension of the feature space or the number of mixture
components.
ACKNOWLEDGMENTS
The authors would like to thank Kobus Barnard for
providing the Corel5K data set used in , David Forsyth
for providing the Corel30K data set, James Wang for the
PSU data set used in , and Google Inc. for providing the
computer resources for many of the experiments. This
research was partially supported by the US National
Science Foundation CAREER award IIS-0448609 and a
grant from Google Inc. Gustavo Carneiro also wishes to
acknowledge funding received from NSERC (Canada) to
support this research. Finally, they would like to thank the
reviewers for insightful comments that helped to improve
the paper. Gustavo Carneiro developed this work while he
was with the Statistical Computing Laboratory at the
University of California, San Diego.