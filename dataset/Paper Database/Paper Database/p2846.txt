Powers of Tensors and Fast Matrix Multiplication
Franc¸ois Le Gall
Department of Computer Science
Graduate School of Information Science and Technology
The University of Tokyo
 
This paper presents a method to analyze the powers of a given trilinear form (a special kind of
algebraic constructions also called a tensor) and obtain upper bounds on the asymptotic complexity
of matrix multiplication. Compared with existing approaches, this method is based on convex optimization, and thus has polynomial-time complexity. As an application, we use this method to study
powers of the construction given by Coppersmith and Winograd [Journal of Symbolic Computation,
1990] and obtain the upper bound ω < 2.3728639 on the exponent of square matrix multiplication,
which slightly improves the best known upper bound.
Introduction
Matrix multiplication is one of the most fundamental tasks in mathematics and computer science. While
the product of two n×n matrices over a ﬁeld can naturally be computed in O(n3) arithmetic operations,
Strassen showed in 1969 that O(n2.81) arithmetic operations are enough . The discovery of this
algorithm for matrix multiplication with subcubic complexity gave rise to a new area of research, where
the central question is to determine the value of the exponent of square matrix multiplication, denoted ω,
and deﬁned as the minimal value such that two n × n matrices over a ﬁeld can be multiplied using
O(nω+ε) arithmetic operations for any ε > 0. It has been widely conjectured that ω = 2 and several
conjectures in combinatorics and group theory, if true, would lead to this result . However, the
best upper bound obtained so far is ω < 2.38, as we explain below.
Coppersmith and Winograd showed in 1987 that ω < 2.3754770. Their approach can be described as follows. A trilinear form is, informally speaking, a three-dimensional array with coefﬁcients
in a ﬁeld F. For any trilinear form t one can deﬁne its border rank, denoted R(t), which is a positive
integer characterizing the number of arithmetic operations needed to compute the form. For any trilinear
form t and any real number ρ ∈ , one can deﬁne a real number Vρ(t), called the value of the trilinear form. The theory developed by Sch¨onhage shows that, for any m ≥1 and any ρ ∈ , the
following statement hold:
1/m ≥R(t) =⇒ω ≤ρ.
Here the notation t⊗m represents the trilinear form obtained by taking the n-th tensor power of t. Coppersmith and Winograd presented a speciﬁc trilinear form t, obtained by modifying a construction given
earlier by Strassen , computed its border rank R(t), and introduced deep techniques to estimate
the value Vρ(t). In particular, they showed how a lower bound ˜Vρ(t) on Vρ(t) can be obtained for any
ρ ∈ by solving an optimization problem. Solving this optimization problem, they obtained the
upper bound ω < 2.3871900, via Statement (1) with t = t and m = 1, by ﬁnding the smallest ρ such
that ˜Vρ(t) ≥R(t). They then proceeded to study the tensor power t⊗2 and showed that, despite several
new technical difﬁculties, a similar approach can be used to reduce the computation of a lower bound
˜Vρ(t⊗2) on Vρ(t⊗2) to solving another optimization problem of several variables. They discovered that
˜Vρ(t⊗2) > [ ˜Vρ(t)]2, due to the fact that the analysis of t⊗2 was ﬁner, thus giving a better upper bound
on ω via Statement (1) with t = t and m = 2. Solving numerically the new optimization problem, they
obtained the upper bound ω < 2.3754770.
In view of the improvement obtained by taking the second tensor power, a natural question was to
investigate higher powers of the construction t by Coppersmith and Winograph. Investigating the third
power was explicitly mentioned as an open problem in . More that twenty years later, Stothers showed
that, while the third power does not seem to lead to any improvement, the fourth power does give an improvement (see also ). The improvement was obtained again via Statement (1), by showing how
to reduce the computation of Vρ(t⊗4) to solving a non-convex optimization problem. The upper bound
ω < 2.3736898 was obtained in by ﬁnding numerically a solution of this optimization problem.
It was later discovered that that solution was not optimal, and the improved upper bound ω < 2.3729269
was given in by exhibiting a better solution of the same optimization problem. Independently, Vassilevska Williams constructed a powerful and general framework to analyze recursively powers of
a class of trilinear forms, including the trilinear form t by Coppersmith and Winograd, and showed how
to automatically reduce, for any form t in this class and any integer m ≥2, the problem of obtaining
lower bounds on Vρ(t⊗m) to solving (in general non-convex) optimization problems. The upper bound
ω < 2.3729 was obtained by applying this framework with t = t and m = 8, and numerically solving this optimization problem.1 A natural question is to determine what bounds on ω can be obtained
by studying t⊗m for m > 8. One may even hope that, when m goes to inﬁnity, the upper bound on ω
goes to two. Unfortunately, this question can hardly be answered by this approach since the optimization
problems are highly non-convex and become intractable even for modest values of m.
In this paper we show how to modify the framework developed in in such a way that the computation of Vρ(t⊗m) reduces to solving poly(m) instances of convex optimization problems, each having
poly(m) variables. From a theoretical point a view, since a solution of such convex problems can be
found in polynomial time, via Statement (1) we obtain an algorithm to derive an upper bound on ω from
t⊗m in time polynomial in m. From a practical point of view, the convex problems we obtain can also be
solved efﬁciently, and have several desirable properties (in particular, the optimality of a solution can be
guaranteed by using the dual problem). We use this method to analyze t⊗16 and t⊗32, and obtain the new
upper bounds on ω described in Table 1. Besides leading to an improvement for ω, these results strongly
suggest that studying powers higher than 32 will give only negligible improvements.
Our method is actually more general and can be used to efﬁciently obtain lower bounds on Vρ(t ⊗t′)
for any trilinear forms t and t′ that have a structure “similar” to t. Indeed, considering possible future
applications of our approach, we have been attentive of stating our techniques as generally as possible.
To illustrate this point, we work out in the appendix the application of our method to an asymmetric
trilinear form, originally proposed in .
Table 1: Upper bounds on ω obtained by analyzing the m-th power of the construction t by Coppersmith
and Winograd.
Upper bound
ω < 2.3871900
ω < 2.3754770
ω < 2.3729269
ω < 2.3728642
this paper (Section 6.3)
(ω < 2.3729 given in Ref. )
ω < 2.3728640
this paper (Section 6.3)
ω < 2.3728639
this paper (Section 6.3)
1Note that, while the upper bound on ω obtained for the eighth power is stated as ω < 2.3727 in the conference version ,
the statement has been corrected to ω < 2.3729 in the most recent version , since the previous bound omitted some
necessary constraints in the optimization problem. Our results conﬁrm the value of the latter bound, and increase its precision.
Algebraic Complexity Theory
This section presents the notions of algebraic complexity needed for this work. We refer to, e.g., 
for more detailed treatments. In this paper F denotes an arbitrary ﬁeld.
Trilinear forms
Let u, v and w be three positive integers, and U, V and W be three vector spaces over F of dimension u, v
and w, respectively. A trilinear form (also called a tensor) t on (U, V, W) is an element in U ⊗V ⊗W ∼=
Fu×v×w, where ⊗denotes the tensor product. If we ﬁx bases {xi}, {yj} and {zk} of U, V and W,
respectively, then t can be written as
tijk xi ⊗yj ⊗zk
for coefﬁcients tijk in F. We will usually write xi ⊗yj ⊗zj simply as xiyjzk.
Matrix multiplication of an m × n matrix with entries in F by an n × p matrix with entries in F
corresponds to the trilinear form on (Fm×n, Fn×p, Fm×p) with coefﬁcients tijk = 1 if i = (r, s), j =
(s, t) and k = (r, t) for some integers (r, s, t) ∈{1, . . . , m} × {1, . . . , n} × {1, . . . , p}, and tijk = 0
otherwise. Indeed, this form can be rewritten as
x(r,s)y(s,t)
Then, replacing the x-variables by the entries of the ﬁrst matrix and the y-variables by the entries of the
second matrix, the coefﬁcient of z(r,t) in the above expression represents the entry in the r-th row and the
t-th column of the matrix product of these two matrices. This trilinear form will be denoted by ⟨m, n, p⟩.
Another important example is the form Pn
ℓ=1 xℓyℓzℓ. This trilinear form on (Fn, Fn, Fn) is denoted ⟨n⟩and corresponds to n independent scalar products.
Given a tensor t ∈U ⊗V ⊗W, it will be convenient to denote by tC and tC2 the tensors in V ⊗W ⊗U
and W ⊗U ⊗V , respectively, obtained by permuting cyclicly the coordinates of t:
tijk yj ⊗zk ⊗xi,
tijk zk ⊗xi ⊗yj.
Given two tensors t ∈U ⊗V ⊗W and t′ ∈U ′ ⊗V ′ ⊗W ′, we can naturally deﬁne their direct sum
t ⊕t′, which is a tensor in (U ⊕U ′) ⊗(V ⊕V ′) ⊗(W ⊕W ′), and their tensor product t ⊗t′, which
is a tensor in (U ⊗U ′) ⊗(V ⊗V ′) ⊗(W ⊗W ′). For any integer c ≥1, the tensor t ⊕· · · ⊕t (with c
occurrences of t) will be denoted by c · t and the tensor t ⊗· · · ⊗t (with c occurrences of t) will be
denoted by t⊗c.
Let λ be an indeterminate and consider the extension F[λ] of F, i.e., the set of all polynomials over
F in λ. Let t ∈Fu×v×w and t′ ∈Fu′×v′×w′ be two tensors. We say that t′ is a degeneration of t, denoted
t′ ✂t, if there exist three matrices α ∈F[λ]u′×u, β ∈F[λ]v′×v, γ ∈F[λ]w′×w such that
λst′ + λs+1t′′ =
tijk α(xi) ⊗β(yj) ⊗γ(zj)
for some tensor t′′ ∈F[λ]u′×v′×w′ and some nonnegative integer s. Intuitively, the fact that a tensor t′ is
a degeneration of a tensor t means that an algorithm computing t can be converted into another algorithm
computing t′ with essentially the same complexity. The notion of degeneration can be used to deﬁne the
notion of border rank of a tensor t, denoted R(t), as follows:
R(t) = min{r ∈N | t ✂⟨r⟩}.
The border rank is submultiplicative: R(t ⊗t′) ≤R(t) × R(t′) for any two tensors t and t′.
The exponent of matrix multiplication
The following theorem, which was proven by Sch¨onhage , shows that good upper bounds on ω can
be obtained by ﬁnding a trilinear form of small border rank that can be degenerated into a direct sum of
several large matrix multiplications.
Theorem 2.1. Let e and m be two positive integers. Let t be a tensor such that e · ⟨m, m, m⟩✂t. Then
emω ≤R(t).
Our results will require a generalization of Theorem 2.1, based on the concept of value of a tensor.
Our presentation of this concept follows . Given a tensor t ∈Fu×v×w and a positive integer N, deﬁne
(e, m) ∈N × N | e·⟨m, m, m⟩✂(t ⊗tC ⊗tC2)⊗N
corresponding to all pairs (e, m) such that the tensor (t⊗tC⊗tC2)⊗N can be degenerated into a direct sum
of e tensors, each isomorphic to ⟨m, m, m⟩. Note that this set is ﬁnite. For any real number ρ ∈ ,
Vρ,N(t) = max{(emρ)
where the maximum is over all (e, m) in the set of Eq. (2). We now give the formal deﬁnition of the
value of a tensor.
Deﬁnition 2.1. For any tensor t and any ρ ∈ ,
Vρ(t) = lim
N→∞Vρ,N(t).
The limit in this deﬁnition is well deﬁned, see . Obviously, Vρ(tC) = Vρ(tC2) = Vρ(t) for any
tensor t, and Vρ(t) ≥Vρ(t′) for any tensors t, t′ such that t′ ✂t. By deﬁnition, for any positive integers
m, n and p we have
Vρ(⟨m, n, p⟩) ≥(mnp)ρ/3.
Moreover, the value is superadditive and supermultiplicative: for any two tensors t and t′, and any
ρ ∈ , the inequalities Vρ(t ⊕t′) ≥Vρ(t) + Vτ(t′) and Vρ(t ⊗t′) ≥Vρ(t) × Vτ(t′) hold. With
this concept of value, we can state the following slight generalization of Theorem 2.1, which was used
implicitly in and stated explicitly in .
Theorem 2.2. Let t be a tensor and ρ be a real number such that 2 ≤ρ ≤3. If Vρ(t) ≥R(t), then
Finally, we will need the concept of decomposition of a tensor. Our presentation of this concept
follows . Let t ∈U ⊗V ⊗W be a tensor. Suppose that the vector spaces U, V and W decompose as
where I, J and W are three ﬁnite subsets of Z. Let us call this decomposition D. We say that D is a
decomposition of t if the tensor t can be written as
(i,j,k)∈I×J×K
t(i, j, k),
where each t(i, j, k) is a tensor in Ui ⊗Vj ⊗Wk (the sum does not need to be direct). The support of t
with respect to D is deﬁned as
supp(t) = {(i, j, k) ∈I × J × K | t(i, j, k) ̸= 0},
and the nonzero t(i, j, k)’s are called the components of t.
Preliminaries and Notations
In this section S denotes a ﬁnite subset of Z × Z × Z.
Let α1, α2, α3 : S →Z be the three coordinate functions of S, which means that αℓ(s) = sℓfor each
ℓ∈{1, 2, 3} and all s = (s1, s2, s3) ∈S. We ﬁrst deﬁne the concept of tightness. The same notion was
used in .
Deﬁnition 3.1. The set S is tight if there exists an integer d such that α1(s) + α2(s) + α3(s) = d for all
s ∈S. The set S is b-tight, where b is a positive integer, if additionally αℓ(S) ⊆{0, 1, . . . , b −1} for all
ℓ∈{1, 2, 3}.
Note that if S is b-tight then |S| ≤b2.
We denote by F(S) the set of all real-valued functions on S, and by D(S) the set of all probability
distributions on S (i.e., the set of all functions f ∈F(S) such that f(s) ≥0 for each s ∈S and
s∈S f(s) = 1). Note that, with pointwise addition and scalar multiplication, F(S) forms a real vector
space of dimension |S|. Given any function f ∈F(S), we denote by f1 : α1(S) →R, f2 : α2(S) →R
and f3 : α3(S) →R the three marginal functions of f: for each ℓ∈{1, 2, 3} and each a ∈αℓ(S),
Let SS denote the group of all permutations on S. Given any function f ∈F(S) and any σ ∈SS,
we will denote by f σ the function in F(S) such that f σ(s) = f(σ(s)) for all s ∈S. We now deﬁne the
concept of invariance of a function.
Deﬁnition 3.2. Let G be a subgroup of SS. A function f ∈F(S) is G-invariant if f σ = f for all σ ∈G.
We will denote by F(S, G) the set of all G-invariant real-valued functions on S, and by D(S, G) =
D(S) ∩F(S, G) the set of all G-invariant probability distributions on S. We denote by F0(S, G) the
vector space of all functions f ∈F(S, G) such that fℓ(a) = 0 for all ℓ∈{1, 2, 3} and all a ∈αℓ(S),
χ(S, G) = dim(F0(S, G)).
We call this number χ(S, G) the compatibility degree of S with respect to G.
In our applications it will be sometimes more convenient to characterize the invariance in term of a
subgroup of permutations of the three coordinates of S, rather than in term of a subgroup of permutations
on S, as follows. Let L be a subgroup of S3, the group of permutations over {1, 2, 3}. We say that S is
L-symmetric if (sσ(1), sσ(2), sσ(3)) ∈S for all (s1, s2, s3) ∈S and all σ ∈L. If S is L-symmetric, the
subgroup L induces the subgroup LS = {πσ | σ ∈L} of SS, where πσ denotes the permutation in SS
such that πσ(s1, s2, s3) = (sσ(1), sσ(2), sσ(3)) for all (s1, s2, s3) ∈S. We will slightly abuse notation
and, when S is L-symmetric, simply write F(S, L), D(S, L), F0(S, L), χ(S, L) to represent F(S, LS),
D(S, LS), F0(S, LS) and χ(S, LS), respectively.
The entropy of a probability distribution P ∈D(S) is
P(s) log(P(s)),
with the usual convention 0 × log(0) = 0. Using the above notations, P1, P2 and P3 represent the three
marginal probability distributions of P. For each ℓ∈{1, 2, 3}, the entropy of Pℓis
Pℓ(a) log(Pℓ(a)).
It will sometimes be more convenient to represent, for each ℓ∈{1, 2, 3}, the distribution Pℓas a vector
Pℓ∈R|αℓ(S)|, by ﬁxing an arbitrary ordering of the elements in αℓ(S).
We now deﬁne the concept of compatibility of two probability distributions.
Deﬁnition 3.3. Two probability distributions P and Q in D(S) are compatible if Pℓ= Qℓfor each
ℓ∈{1, 2, 3}.
Finally, for any P ∈D(S), we deﬁne the quantity
ΓS(P) = max
Q [H(Q)] −H(P),
where the maximum is over all Q ∈D(S) compatible with P. Note that ΓS(P) is always non-negative.
General Theory
In this section we describe how to analyze the value of a trilinear form that has a decomposition with
tight support.
Derivation of lower bounds on the value
Our main tool to analyze a trilinear form that has a decomposition with tight support is the following
theorem, which shows how to reduce the computation of a lower bound on its value to solving an optimization problem. Its proof is given in the appendix.
Theorem 4.1. Let t be a trilinear form, and D be a decomposition of t with tight support S ⊆Z×Z×Z
and component set {t(s)}s∈S. Then, for any P ∈D(S) and any ρ ∈ ,
log(Vρ(t)) ≥
P(s) log(Vρ(t(s))) −ΓS(P).
This theorem can be seen as a generalized statement of the approach developed by Coppersmith
and Winograd . Several similar statements already appeared in the literature. A weaker statement,
corresponding to the simpler case where each component is isomorphic to a matrix product (which
removes the need for the term −ΓS(P) in the lower bound), can be found in . The generalization to
the case of arbitrary components stated in Theorem 4.1 was considered in , and proved implicitly,
by considering several cases (symmetric and asymmetric supports) and without reference to the entropy,
in . Theorem 4.1 aims at providing a concise statement unifying all these results, described in
terms of entropy in order to discuss the convexity of the lower bounds obtained.
Solving the optimization problem
Let t be a trilinear form with a decomposition that has a tight support, as in the statement of Theorem 4.1.
It will be convenient to deﬁne, for any ρ ∈ , the function Ψt,ρ : D(S) →R as
P(s) log(Vρ(t(s)))
for any P ∈D(S). Note that this is a concave function on the convex set D(S). In order to optimize the
upper bound on log(Vρ(t)) that is obtained from Theorem 4.1, we would like to ﬁnd, for a given value
of ρ, a probability distribution P ∈D(S) that minimizes the expression
ΓS(P) −Ψt,ρ(P).
This optimization problem is in general not convex, due to the presence of the term ΓS(P). In this
subsection we develop a method to overcome this difﬁculty and ﬁnd, using Theorem 4.1, a lower bound
on Vρ(t) in polynomial time.
Remember that ΓS(P) = maxQ[H(Q)] −H(P), where the maximum is over all Q ∈D(S) that are
compatible with P. When P is ﬁxed, these conditions on Q can be written as linear constraints. Since the
entropy is a strictly concave function, computing −ΓS(P) is then a strictly convex optimization problem
on a convex set, and in particular has a unique solution ˆQ. Note that ΓS( ˆQ) = H( ˆQ) −H( ˆQ) = 0, and
thus Ψt,ρ( ˆQ) is a lower bound on log(Vρ(t)). The tightness of this lower bound of course depends on
the initial choice of P. A natural choice is to take a probability distribution P that maximizes Ψt,ρ(P),
since ﬁnding such a probability distribution corresponds to solving a convex optimization problem. This
motivates the algorithm described in Figure 1, which we call Algorithm A.
Algorithm A
Input: • the support S ⊆Z × Z × Z of the tensor t
• a value ρ ∈ 
• the values Vρ(t(s)) for each s ∈S
1. Solve the following convex optimization problem.
minimize −Ψt,ρ(P)
subject to P ∈D(S)
2. Solve the following convex optimization problem, where ˆP denotes
the solution found at Step 1.
minimize −H(Q)
subject to Q ∈D(S)
Q compatible with ˆP
3. Output Ψt,ρ( ˆQ), where ˆQ denotes the solution found at Step 2.
Figure 1: Algorithm A computing, given a tensor t with a decomposition that has a tight support and a
value ρ ∈ , a lower bound on log(Vρ(t)).
As already mentioned, the optimization problem OPT 2 has a unique solution. While the solution of
the optimization problem OPT1 may not be unique, it can actually be shown, using the strict concavity of
the entropy function, that two solutions of OPT1 must have the same marginal probability distributions.
Since the domain of the optimization problem OPT2 depends only on the marginal distributions of ˆP,
the output of Algorithm A does not depend on which solution ˆP was found at Step 1. This output is thus
unique and, from Theorem 4.1 and the discussion above, it gives a lower bound on log(Vρ(t)). We state
this conclusion in the following theorem.
Theorem 4.2. If the support of t is tight, then Algorithm A outputs a lower bound on log(Vρ(t)).
Let us now discuss the time complexity of implementing the algorithm of Figure 1. The worstcase running time depends on the time needed to solve the two optimization problems OPT1 and OPT2
at Steps 1 and 2. Let v = Ψt,ρ( ˆQ) denote the output of an exact implementation of Algorithm A.
Theorem 4.2 shows that v ≤log(Vρ(t)). Since both OPT1 and OPT2 are convex, and since the number
of variables is upper bounded by |S|, for any ε > 0 both problems can be solved with accuracy ε in
time poly(|S|, log(1/ε)) using standard methods . Thus, for any ε′ > 0, we can compute in time
poly(|S|, log(1/ε′)) a value v′ such that |v −v′| ≤ε′ · v. In particular, we can use
1+ε′ as a lower bound
on log(Vρ(t)).
We ﬁnally explain how to exploit symmetries of the decomposition of t to reduce the number of
variables in Algorithm A. These observations will enable us to slightly simplify the exposition of our
results in the next sections. We ﬁrst deﬁne invariance of a decomposition of a tensor.
Deﬁnition 4.1. Let t be a tensor that has a decomposition D with support S and components {t(s)}s∈S.
The decomposition D is G-invariant if Ψt,ρ(P σ) = Ψt,ρ(P) for any P ∈D(S) and any σ ∈G.
With a slight abuse of language we will say, given a subgroup L of S3, that D is L-invariant if S is
L-symmetric and D is LS-invariant (see Section 3 for the deﬁnition of LS).
Assume that the decomposition D of the tensor t on which we want to apply Algorithm A is Ginvariant, where G is a subgroup of SS. Consider the optimization problem OPT1. Since the value
of its objective function is then unchanged under the action of any permutation σ ∈G on P, OPT1
has a solution that is G-invariant (see, e.g., for a discussion of symmetries in convex optimization).
Now, if ˆP is G-invariant, then the (unique) solution of the optimization problem OPT2 is G-invariant as
well, since the value of the function −H(Q) is unchanged under the action of any permutation on Q.
This means that, if the decomposition D is G-invariant, then D(S) can be replaced by D(S, G) at both
Steps 1 and 2 of Algorithm A. Note that this set of distributions can be parametrized by dim(F(S, G))
parameters, instead of |S| parameters.
Another approach
In this subsection we describe another approach to obtain lower bounds on Vρ(t) using Theorem 4.1,
which is essentially how the powers of the construction by Coppersmith and Winograd were studied in
previous works . Given any subgroup G of SS, let us consider the vector space F0(S, G)
of dimension χ(S, G) deﬁned in Section 3. It will be convenient to represent functions in this vector
space by vectors in R|S|, by ﬁxing an arbitrary ordering of the elements in S. Let R be a generating
matrix of size |S| × χ(S, G) for F0(S, G) (i.e., the columns of R form a basis of F0(S, G)). Since each
coordinate of R|S| corresponds to an element of S, we write Rsj, for s ∈S and j ∈{1, . . . , χ(S, G)}, to
represent the element in the s-th row and the j-th column of R. The approach is based on the following
proposition, which is similar to a characterization given in .
Proposition 4.1. For any P, P ′ ∈D(S, G) that are compatible, the equality ΓS(P ′) = H(P) −H(P ′)
holds if P satisﬁes the following two conditions:
(i) P(s) > 0 for any s ∈S such that R contains at least one non-zero entry in its row labeled by s,
s∈S Rsj log(P(s)) = 0 for all j ∈{1, . . . , χ(S, G)}.
Proof. We ﬁrst make the following observation: for any vector x ∈F0(S, G) ⊆R|S|, the equality
holds. This implies in particular that P
s∈S Rsj = 0 for any j ∈{1, . . . , χ(S, G)}.
Let P′ denote the vector in R|S| representing the probability distribution P ′. We have
ΓS(P ′) = max
u∈C [H(P′ + Ru)] −H(P′),
where C = {u ∈Rχ(S,G) | P′ + Ru ∈D(S)}. Note that C is a convex set. Consider the function
h: C →R deﬁned as h(u) = H(P′ + Ru) for any u ∈C. This is a concave function, differentiable on
the interior of C. Let us take an interior point u and write z = P′ + Ru. The partial derivatives at u are
Rsj(1 + log(zs)) = −
Rsj log(zs),
for each j ∈{1, . . . , χ(S, G)}.
A probability distribution P that satisﬁes the conditions in the statement of the proposition therefore
corresponds to a vanishing point (and thus a global maximum) of the function h, which implies that the
equality ΓS(P ′) = H(P) −H(P ′) holds for this distribution P, as claimed.
In particular, applying Proposition 4.1 with P ′ = P shows that, if Conditions (i) and (ii) are satisﬁed,
then ΓS(P) = 0, which implies log(Vρ(t)) ≥Ψt,ρ(P) from Theorem 4.1. This motivates the algorithm
described in Figure 2 that outputs a lower bound on log(Vρ(t)). We will call it Algorithm B.
Algorithm B
Input: • the support S ⊆Z × Z × Z for the tensor t
• a value ρ ∈ 
• the values Vρ(t(s)) for each s ∈S
• a subgroup G of SS such that the decomposition of t is G-invariant
1. Solve the following optimization problem.
minimize −Ψt,ρ(P)
subject to P ∈D(S, G)
P satisﬁes Conditions (i)-(ii) of Proposition 4.1
2. Output Ψt,ρ( ˜P), where ˜P denotes the solution found at Step 1.
Figure 2: Algorithm B computing, given a tensor t with a decomposition that has a tight support and a
value ρ ∈ , a lower bound on log(Vρ(t)).
Note that, when χ(S, G) = 0, Algorithms A and B solve exactly the same optimization problem
(since in Algorithm B Conditions (i) and (ii) are satisﬁed for any P ∈D(S, G), and ˆQ = ˆP in Algorithm A) and thus output the same value. When χ(S, G) > 0 Algorithm B usually gives better lower
bounds than Algorithm A, but at the price of introducing χ(S, G) highly nonconvex constraints, which
makes the optimization problem much harder to solve, both in theory and in practice, even for a modest
number of variables.
Powers of Tensors
Let t and t′ be two trilinear forms with decompositions D and D′, respectively. Let supp(t) ⊂Z×Z×Z
and supp(t′) ⊂Z × Z × Z denote their supports, and {t(s)}s∈supp(t) and {t′(s′)}s′∈supp(t′) denote their
component sets. Assume that both supports are tight. Fix ρ ∈ and assume that lower bounds on the
values Vρ(t(s)) and Vρ(t′(s′)) are known for each s ∈supp(t) and each s′ ∈supp(t′). In this section
we describe a method, inspired by and , and also used in , to analyze Vρ(t ⊗t′), and then
show how to use it to analyze Vρ(t⊗m) when m is a power of two.
In this section we will denote α1, α2, α3 : Z×Z×Z →Z the three coordinate functions of Z×Z×Z.
Consider the tensor
s′∈supp(t′)
t(s) ⊗t′(s′).
Consider the following decomposition of t ⊗t′: the support is
supp(t ⊗t′) =
(α1(s) + α1(s′), α2(s) + α2(s′), α3(s) + α3(s′)) | s ∈supp(t), s′ ∈supp(t′)
and, for each (a, b, c) ∈supp(t ⊗t′) the associated component is
(t ⊗t′)(a, b, c) =
t(s) ⊗t′(s′),
where the sum is over all (s, s′) ∈supp(t) × supp(t′) such that α1(s) + α1(s′) = a, α2(s) + α2(s′) = b
and α3(s) + α3(s′) = c. Note that the support of this decomposition is tight. If lower bounds on the
value of each component are known, then we can use this decomposition to obtain a lower bound on
Vρ(t ⊗t′), by using Algorithm A on t ⊗t′, which requires solving two convex optimization problems,
each having |supp(t ⊗t′)| variables.
We now explain how to evaluate the value of those components (t ⊗t′)(a, b, c). For any (a, b, c) ∈
supp(t ⊗t′), consider the following decomposition of (t ⊗t′)(a, b, c): the support is
s ∈supp(t) | (a −α1(s), b −α2(s), c −α3(s)) ∈supp(t′)
and, for each element s in this set, the corresponding component is
t(s) ⊗t′(a −α1(s), b −α2(s), c −α3(s)).
Note that the support in this decomposition is tight, and has size at most |supp(t)|. The value of each
component can be lower bounded as
 t(s) ⊗t′(a −α1(s), b −α2(s), c −α3(s))
Vρ(t(s)) × Vρ(t′(a −α1(s), b −α2(s), c −α3(s)),
from the supermultiplicativity of the value. As we supposed that the lower bounds on the values of each
component of t and t′ are known, we can use Algorithm A on each (t ⊗t′)(a, b, c) to obtain a lower
bound on Vρ((t ⊗t′)(a, b, c)), which requires solving two convex optimization problems, each having at
most |supp(t)| variables.
Let us now consider the case t′ = t. We have just shown the following result: a lower bound on
Vρ(t⊗2) can be computed by solving two convex optimization problems with |supp(t⊗2)| variables, and
2|supp(t⊗2)| convex optimization problems with at most |supp(t)| variables. An important point is that
this method additionally gives, as described in the previous paragraphs, a decomposition of t⊗2 with
tight support, and a lower bound on Vρ(t⊗2(a, b, c)) for each component t⊗2(a, b, c). This information
can then be used to analyze the trilinear form t⊗4 = t⊗2 ⊗t⊗2, by replacing t by t⊗2 in the above
analysis, giving a decomposition of t⊗4 with tight support, a lower bound on Vρ(t⊗4) and a lower bound
on the value Vρ(t⊗4(a, b, c)) of each component. By iterating this approach r times, for any r ≥1, we
can analyze the trilinear form t⊗2r, and in particular obtain a lower bound on Vρ(t⊗2r). Let us denote
by D2r the decomposition of t⊗2r obtained by this approach. Its support is
supp(t⊗2r) =
(α1(s) + α1(s′), α2(s) + α2(s′), α3(s) + α3(s′)) | s, s′ ∈supp(t⊗2r−1)
and, for any (a, b, c) ∈supp(t⊗2r), the corresponding component is
t⊗2r(a, b, c) =
t⊗2r−1(s) ⊗t⊗2r−1(s′),
where the sum is over all (s, s′) ∈supp(t⊗2r−1) such that α1(s) + α1(s′) = a, α2(s) + α2(s′) = b and
α3(s) + α3(s′) = c. This approach also gives a decomposition D2r
abc of each component t⊗2r(a, b, c). In
this decomposition the support, which we denote S2r
s ∈supp(t⊗2r−1) | (a −α1(s), b −α2(s), c −α3(s)) ∈supp(t⊗2r−1)
and, for any s ∈S2r
abc, the corresponding component of t⊗2r(a, b, c) is
t⊗2r−1(s) ⊗t⊗2r−1(a −α1(s), b −α2(s), c −α3(s)).
The overall number of convex optimization problems that need to be solved in order to analyze t⊗2r
by the above approach is upper bounded by r(2 + 2|supp(t⊗2r)|), while the number of variables in each
optimization problem is upper bounded by |supp(t⊗2r)|. In the case where supp(t) is b-tight we can
give a simple upper bound on this quantity. Indeed, when supp(t) is b-tight, our construction guarantees
that supp(t⊗2r) is (b2r)-tight, which implies that |supp(t⊗2r)| ≤(b2r)2. We thus obtain the following
Theorem 5.1. Let t be a trilinear form that has a decomposition with b-tight support supp(t) and
components {t(s)}. Fix ρ ∈ and assume that a lower bound on the value Vρ(t(s)) is known for
each s ∈supp(t). Then, for any integer r ≥1, a lower bound on Vρ(t⊗2r) can be computed by solving
poly(b, 2r) convex optimizations problems, each optimization problem having poly(b, 2r) variables.
Finally, we present two simple lemmas that show how to exploit the symmetries of the decomposition
of t to slightly reduce the number of variables in our optimization problems.
Lemma 5.1. For any r ≥1 and any (a, b, c) ∈supp(t⊗2r), the decomposition D2r
abc is {id, π}-invariant,
where id denotes the identity permutation and π is the permutation on S2r
abc such that
π(s) = (a −α1(s), b −α2(s), c −α3(s))
for all s ∈S2r
Proof. Observe that, for any probability distribution P ∈D(S2r
abc), the equality H(Pℓ) = H(P π
for all ℓ∈{1, 2, 3}, which implies that
Ψt⊗2r (a,b,c),ρ(P) = Ψt⊗2r (a,b,c),ρ(P π),
as wanted.
Lemma 5.2. Let L be a subgroup of S3. Assume that supp(t) is L-symmetric and that
Vρ(t(s1, s2, s3)) = Vρ(t(sσ(1), sσ(2), sσ(3)))
for any σ ∈L and any s = (s1, s2, s3) ∈supp(t). Then D is L-invariant and, for any r ≥1, the
decomposition D2r is L-invariant as well.
Proof. For any σ ∈L, any probability distribution P ∈D(supp(t)) and any ℓ∈{1, 2, 3}, the equality
σ(ℓ) holds, where πσ denotes the permutation such that
πσ(s1, s2, s3) = (sσ(1), sσ(2), sσ(3))
for all (s1, s2, s3) ∈supp(t). This implies that Ψt,ρ(P) = Ψt,ρ(P πσ), and thus D is L-invariant. The
same argument shows that D2r is L-invariant for any r ≥1.
From the discussion of Section 4.2, Lemma 5.1 enables us to reduce the number of variables when
computing the lower bound on Vρ(t⊗2r(a, b, c)) using Algorithm A: instead of solving an optimization
problem over D(supp(t⊗2r(a, b, c))), we only need to consider D(supp(t⊗2r(a, b, c)), {id, π}). Similarly, if the conditions of Lemma 5.2 are satisﬁed, then, instead of considering D(supp(t⊗2r)), we need
only to consider D(supp(t⊗2r), L) when computing the lower bound on Vρ(t⊗2r) using Algorithm A.
Remark. The approach described in this section can be generalized to obtain lower bounds on Vρ(t⊗m)
when m is not a power of two. For instance the third power can be analyzed by studying t ⊗t′ with
t′ = t⊗2. Another possible straightforward generalization is to allow other linear dependences in the
deﬁnition of the support, i.e., deﬁning the support of t ⊗t′ as
supp(t ⊗t′) =
(α1(s) + uα1(s′), α2(s) + uα2(s′), α3(s) + uα3(s′)) | s ∈supp(t), s′ ∈supp(t′)
where u ∈Z can be freely chosen. These two generalizations nevertheless do not seem to lead to any
improvement for ω when applied to existing constructions.
Application
In this section we apply the theory developed in the previous sections to the construction t by Coppersmith and Winograd, in order to obtain upper bounds on ω.
Construction
Let F be an arbitrary ﬁeld. Let q be a positive integer, and consider three vector spaces U, V and W
of dimension q + 2 over F. Take a basis {x0, . . . , xq+1} of U, a basis {y0, . . . , yq+1} of V , and a basis
{z0, . . . , zq+1} of W.
The trilinear form t considered by Coppersmith and Winograd is the following trilinear form on
(U, V, W):
(x0yizi + xiy0zi+xiyiz0) + x0y0zq+1 + x0yq+1z0 + xq+1y0z0.
It was shown in that R(t) = q + 2. Consider the following decomposition of U, V and W:
U = U0 ⊕U1 ⊕U2,
V = V0 ⊕V1 ⊕V2,
W = W0 ⊕W1 ⊕W2,
where U0 = span{x0}, U1 = span{x1, . . . , xq} and U2 = span{xq+1}, V0 = span{y0}, V1 =
span{y1, . . . , yq} and V2 = span{yq+1}, W0 = span{z0}, W1 = span{z1, . . . , zq} and W2 =
span{zq+1}. This decomposition induces a decomposition D of t with tight support
S = {(2, 0, 0), (1, 1, 0), (1, 0, 1), (0, 2, 0), (0, 1, 1), (0, 0, 2)}.
The components associated with (2, 0, 0) and (1, 1, 0) are
t(2, 0, 0) = xq+1y0z0 ∼= ⟨1, 1, 1⟩,
t(1, 1, 0) =
xiyiz0 ∼= ⟨1, q, 1⟩.
We have Vρ(t(2, 0, 0)) = 1 and Vρ(t(1, 1, 0)) ≥qρ/3, from the deﬁnition of the value. The other
components t(0, 2, 0) and t(0, 0, 2) are obtained by permuting the coordinates of t(2, 0, 0), while the
components t(1, 0, 1) and t(0, 1, 1) are obtained by permuting the coordinates of t(1, 1, 0).
We now use Theorem 4.1 to obtain an upper bound on ω. Let P be a probability distribution in
D(S). Let us write P(2, 0, 0) = a1, P(1, 1, 0) = a2, P(1, 0, 1) = a3, P(0, 2, 0) = a4, P(0, 1, 1) = a5
and P(0, 0, 2) = a6. The marginal distributions of P are P1 = (a1, a2 + a3, a4 + a5 + a6), P2 =
(a4, a2 + a5, a1 + a3 + a6) and P3 = (a6, a3 + a5, a1 + a2 + a4). Since the only element in D(S)
compatible with P is P, we have ΓS(P) = 0. Theorem 4.1 thus implies that
Vρ(t) ≥exp
H(P1) + H(P2) + H(P3)
× q(a2+a3+a5)ρ/3
for any ρ ∈ . Evaluating this expression with q = 6, a2 = a3 = a5 = 0.3173, a1 = a4 =
a6 = (1 −3a2)/3, and ρ = 2.38719 gives Vρ(t) > 8.00000017. Using Theorem 2.2 and the fact that
R(t) = q + 2, we conclude that ω < 2.38719. This is the same upper bound as the bound obtained in
Section 7 of .
Analyzing the powers using Algorithm A
For any r ≥1, we now consider the tensor t⊗2r and analyze it using the framework and the notations of
Section 5. The support of its decomposition D2r is the set of all triples
(a, b, c) ∈{0, . . . , 2r+1} × {0, . . . , 2r+1} × {0, . . . , 2r+1}
such that a + b + c = 2r+1. Note that the decomposition D of t satisﬁes the conditions of Lemma 5.2
for the subgroup L = S3 of S3, which implies that D2r is S3-invariant. Thus, from the discussion in
Section 4.2, when applying Algorithm A on the trilinear form t⊗2r in order to obtain a lower bound
on Vρ(t2r), we only need to consider probability distributions in D(supp(t⊗2r), S3). This set can be
parametrized by dim(F(supp(t⊗2r), S3)) parameters. Remember that we also need a lower bound on
the value of each component t⊗2r(a, b, c) before applying A on t⊗2r. Using the method described in
Section 5, these lower bounds are computed recursively by applying Algorithm A on the decomposition
abc of the component. Actually, we do not need to apply A when a = 0, b = 0 or c = 0, since a lower
bound on the value can be found analytically in this case, as stated in the following lemma (see Claim 7
in for a proof).
Lemma 6.1. For any r ≥0 and any b ∈{0, 1, . . . , 2r},
 t⊗2r(2r+1 −b, b, 0)
e∈{0,...,b}
2 )!(2r −b+e
Table 2 presents, for r ∈{1, 2, 3, 4, 5}, the number of variables in the global optimization problem,
the compatibility degree, and the best upper bound on ω we obtained by this approach. The programs
used to derive these upper bounds can be found at , and use the Matlab software CVX for convex
optimization. We work out below in details the cases r = 1 and r = 2.
Table 2: Analysis of t⊗2r using Algorithm A.
dim(F(supp(t⊗2r), S3))
χ(supp(t⊗2r), S3)
upper bound obtained
ω < 2.3754770
ω < 2.3729372
ω < 2.3728675
ω < 2.3728672
ω < 2.3728671
The case r = 1 is easy to deal with, since the compatibility degree is zero, and the values of
all components but one can be computed directly using Lemma 6.1: we have Vρ(t⊗2(4, 0, 0)) = 1,
Vρ(t⊗2(3, 1, 0)) = (2q)ρ/3 and Vρ(t⊗2(2, 2, 0)) = (q2 + 2)ρ/3. Let P be a probability distribution in
D(supp(t⊗2), S3). Write P(4, 0, 0) = a1, P(3, 1, 0) = a2, P(2, 2, 0) = a3 and P(2, 1, 1) = a4. The
partial distributions of P are
P1 = P2 = P3 = (a1, 2a2, 2a3 + a4, 2a2 + 2a4, 2a1 + 2a2 + a3).
Since χ(supp(t⊗2), S3) = 0, we have Γsupp(t⊗2)(P) = 0 and thus Theorem 4.1 gives the lower bound
(2q)6a2ρ/3(q2 + 2)3a3ρ/3× [Vρ(t⊗2(2, 1, 1))]3a4
1 (2a2)2a2(2a3 + a4)2a3+a4(2a2 + 2a4)2a2+2a4(2a1 + 2a2 + a3)2a1+2a2+a3 .
The value of the component t⊗2(2, 1, 1) is computed as described in Section 5, by considering its decomposition D2
211. This decomposition corresponds to the identity
t⊗2(2, 1, 1) =t(2, 0, 0) ⊗t(0, 1, 1) + t(1, 1, 0) ⊗t(1, 0, 1)+
t(1, 0, 1) ⊗t(1, 1, 0) + t(0, 1, 1) ⊗t(2, 0, 0),
and has (tight) support S2
211 = {(2, 0, 0), (1, 1, 0), (1, 0, 1), (0, 1, 1)}. Lemma 5.1 shows that this decomposition is {id, π}-invariant, where π is the permutation that exchanges (2, 0, 0) and (0, 1, 1) and
exchanges (1, 1, 0) and (1, 0, 1). Let P ′ be a distribution on D(S2
211, {id, π}), and write P ′(2, 0, 0) =
P ′(0, 1, 1) = b1 and P ′(1, 1, 0) = P ′(1, 0, 1) = b2. The partial distributions are P′
1 = (b1, 2b2, b1) and
3 = (b1 + b2, b1 + b2). Since ΓS2
211(P ′) = 0, we have
Vρ(t⊗2(2, 1, 1)) ≥
(1 × qρ/3)2b1(qρ/3 × qρ/3)2b2
1 (2b2)2b2(b1 + b2)4(b1+b2)
For ρ = 2.3754770 and q = 6, we obtain Vρ(t⊗2(2, 1, 1)) > 27.35608 by taking b1 = 0.01378 and b2 =
0.48622. Then, taking a1 = 0.00023, a2 = 0.01250, a3 = 0.10254 and a4 = (1 −3a1 −6a2 −3a3)/3,
Vρ(t⊗2) > 64.00000357 > (q + 2)2.
Using Theorem 2.2 and the fact that R(t⊗2) ≤(q + 2)2, we conclude that ω < 2.3754770. This is the
same upper bound as the bound found in Section 8 of .
For r = 2, we ﬁrst need to compute lower bounds on the values of the ten components t⊗4(a, b, c).
Five of them can be computed directly using Lemma 6.1, while the remaining ﬁve are computed using
Algorithm A. Table 3 gives the lower bounds obtained for ρ = 2.3729372 and q = 5. We then apply
Algorithm A on t⊗4, and obtain
Vρ(t⊗4) > 2401.00013 > (q + 2)4
for the probability distributions ˆP and ˆQ given in Table 3, which gives the upper bound ω < 2.3729372.
Values of the components and optimal probability distributions for t⊗4, with ρ = 2.3729372
and q = 5, computed using Algorithm A. In this table, d represents dim(F(S4
abc, {id, π})) and χ
represents χ(S4
abc, {id, π}), where π is deﬁned in Lemma 5.1. The symbol −means that the value is not
relevant, since lower bounds on the values can be computed directly using Lemma 6.1.
(a, b, c) d χ
lower bound on
ˆP(a, b, c)
ˆQ(a, b, c)
Vρ(t⊗4(a, b, c))
0.00000013
0.00000013
0.00001649
0.00001649
0.00000000
0.00049685
0.00164314
0.00064945
149.196694
0.01178178
0.00474178
235.605709
0.00259744
0.00963744
223.037068
0.00000000
0.01308631
472.727437
0.05233355
0.04628725
605.359824
0.08605608
0.07197608
793.438218
0.11217546
0.12526177
Analyzing the powers using both Algorithms A and B
As mentioned in the introduction, the best known upper bound on ω obtained from the fourth power
of t is ω < 2.3729269, which is slightly better than what we obtained in the previous subsection using
Algorithm A. This better bound can actually be obtained by using Algorithm B instead of Algorithm A
when computing the lower bound on Vρ(t⊗4). More precisely, in this case the optimization problem in
Algorithm B asks to minimize −Ψt,ρ(P) such that P ∈D(supp(t⊗4), S3) and P satisﬁes two additional
constraints, since χ(supp(t⊗4), S3) = 2. These two constraints (the same as in ) are:
log(P(6, 2, 0))+2 log(P(4, 3, 1)) −log(P(4, 4, 0))
−log(P(6, 1, 1)) −log(P(3, 3, 2)) = 0,
log(P(5, 3, 0))+ log(P(4, 3, 1)) + log(P(4, 2, 2))
−log(P(4, 4, 0)) −log(P(5, 2, 1)) −log(P(3, 3, 2)) = 0.
They are highly non-convex but, since their number is only two, the resulting optimization problem can
be solved fairly easily, giving the same upper bound ω < 2.3729269 as the bound reported in .
We can also use Algorithm B instead of Algorithm A to analyze t⊗8, but solving the corresponding
optimization problems in this case was delicate and required a combination of several tools. We obtained
lower bounds on the values of each component by solving the non-convex optimization problems using
the NLPSolve function in Maple, while the lower bound on Vρ(t⊗8) has been obtained by solving the
corresponding optimization problem (with 30 variables and 14 non-convex constraints) using the fmincon
function in Matlab. All the programs used are available at , and the numerical solutions are given for
ρ = 2.3728642 and q = 5 in Table 4. The probability distribution of Table 4 gives
Vρ(t⊗8) > 5764802.8 > (q + 2)8,
which shows that ω < 2.3728642.
While the non-convex optimization problems of Algorithm B seem intractable when studying higher
powers of t, these powers can be analyzed by applying Algorithm A, as in the previous subsection, but
using this time the lower bounds on the values of the components Vρ(t⊗8(a, b, c)) obtained by Algorithm B as a starting point. This strategy can be equivalently described as using Algorithm A to analyze
powers of t′, where t′ = t⊗8, with lower bounds on the values of each component of t′ computed by
Algorithm B. The lower bounds we obtain using this method for powers 16 and 32 are given in Table 5
and Figure 3. They show that ω < 2.3728640 and ω < 2.3728639, respectively.
Table 4: Values of the components and optimal probability distribution for t⊗8, with ρ = 2.3728642 and
q = 5, computed using Algorithm B. In this table, d represents dim(F(S8
abc, {id, π})) and χ represents
abc, {id, π}), where π is deﬁned in Lemma 5.1. The symbol −means that the value is not relevant,
since lower bounds on the values can be computed directly using Lemma 6.1.
lower bound on
˜P(a, b, c)
Vρ(t⊗8(a, b, c))
0.000000000049
0.000000000045
0.000000001353
0.000000001461
0.000000021882
0.000000031703
0.000000596688
0.000001088645
0.000001460028
16255.7058
0.000007710948
26646.9787
0.000017673415
35516.8360
0.000029848564
38300.8686
0.000052951447
71988.3451
0.000156556413
108961.6972
0.000332160313
124253.3641
0.000418288187
65227.2515
0.000183839144
143152.3479
0.000733913103
247939.6689
0.002008638773
320986.2915
0.003177631988
78440.0862
0.000290035432
204758.6410
0.001676299730
410936.6736
0.006194731655
608259.9214
0.012641673731
690871.1760
0.015881023028
485122.9853
0.008968430903
830558.9804
0.024712243136
1076870.7243
0.040046668103
1244849.8786
0.054072943466
1421227.6017
0.069752589222
Lower bounds on Vρ(t⊗2r) −(q + 2)2r for r = 4 and r = 5, with q = 5, computed by
performing the analysis on the second and fourth power of t⊗8 using Algorithm A, with lower bounds
of each component of t⊗8 computed by Algorithm B. Plots representing the data of this table are given
in Figure 3.
lower bound on
lower bound on
Vρ(t⊗16) −(q + 2)16
Vρ(t⊗32) −(q + 2)32
8.3460 E+08
5.7365 E+22
5.5802 E+08
3.8857 E+22
2.8110 E+08
2.0175 E+22
2.5866 E+06
3.1912 E+21
-2.4921 E+07
1.2306 E+21
-5.2465 E+07
-1.0995 E+21
-2.7407 E+08
-1.5750 E+22
-5.5031 E+08
-3.3889 E+22
-8.2774 E+08
-5.3088 E+22
-1.1051 E+09
-7.0489 E+22
(a) Power 16
(b) Power 32
Figure 3: Graphs representing the data of Table 5. The horizontal axes represent ρ, while the vertical
axes represent the lower bounds on Vρ(t⊗16) −(q + 2)16, for graph (a), and Vρ(t⊗32) −(q + 2)32, for
graph (b).
Acknowledgments
The author is grateful to Virginia Vassilevska Williams for helpful correspondence, and to Harumichi
Nishimura, Suguru Tamaki and Yuichi Yoshida for stimulating discussions. This work is supported
by the Grant-in-Aid for Young Scientists (B) No. 24700005 of the Japan Society for the Promotion of
Science and the Grant-in-Aid for Scientiﬁc Research on Innovative Areas No. 24106009 of the Ministry
of Education, Culture, Sports, Science and Technology in Japan.