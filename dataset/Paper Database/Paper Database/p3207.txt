Machine Learning, 46, 11–19, 2002
c⃝2002 Kluwer Academic Publishers. Manufactured in The Netherlands.
On a Connection between Kernel PCA
and Metric Multidimensional Scaling
CHRISTOPHER K.I. WILLIAMS
 
Division of Informatics, The University of Edinburgh, 5 Forrest Hill, Edinburgh EH1 2QL, UK
Editor: Nello Cristianini
In this note we show that the kernel PCA algorithm of Sch¨olkopf, Smola, and M¨uller (Neural
Computation, 10, 1299–1319.) can be interpreted as a form of metric multidimensional scaling (MDS) when
the kernel function k(x, y) is isotropic, i.e. it depends only on ∥x −y∥. This leads to a metric MDS algorithm
where the desired conﬁguration of points is found via the solution of an eigenproblem rather than through the
iterative optimization of the stress objective function. The question of kernel choice is also discussed.
metric multidimensional scaling, MDS, kernel PCA, eigenproblem
Introduction
Suppose we are given n objects, and for each pair (i, j) we have a measurement of the
“dissimilarity” δi j between the two objects. In multidimensional scaling (MDS) the aim
is to place n points in a low dimensional space (usually Euclidean) so that the interpoint
distances di j have a particular relationship to the original dissimilarities. In classical scaling
we would like the interpoint distances to be equal to the dissimilarities. For example,
classical scaling can be used to reconstruct a map of the locations of some cities given the
distances between them.
In metric MDS the relationship is of the form di j ≈f (δi j) where f is a speciﬁc function.
In this paper we show that the kernel PCA algorithm of Sch¨olkopf, Smola, and M¨uller
 can be interpreted as performing metric MDS if the kernel function is isotropic. In
this case the solution for the di j’s is obtained by performing classical scaling in the feature
space deﬁned by the kernel.
The structure of the remainder of this paper is as follows: In Section 2 classical and
metric MDS are reviewed, and in Section 3 the kernel PCA algorithm is described. The link
between the two methods is made in Section 4. Section 5 describes approaches to choosing
the kernel function, and we ﬁnish with a brief discussion in Section 6.
Classical and metric MDS
Classical scaling
Given n objects and the corresponding dissimilarity matrix, classical scaling is an algebraic
method for ﬁnding a set of points in space so that the dissimilarities are well-approximated
C.K.I. WILLIAMS
by the interpoint distances. The classical scaling algorithm is introduced below by starting
with the locations of n points, constructing a dissimilarity matrix based on their Euclidean
distances, and then showing how the conﬁguration of the points can be reconstructed (as
far as possible) from the dissimilarity matrix.
Let the coordinates of n points in p dimensions be denoted by xi, i = 1, . . . , n. These
can be collected together in a n × p matrix X. The dissimilarities are calculated by δ2
(xi −x j)T (xi −x j). Given these dissimilarities, we construct the matrix A such that ai j =
i j, and then set B = HAH, where H is the centering matrix H = In −1
n 11T . With
i j = (xi −x j)T (xi −x j), the construction of B leads to bi j = (xi −¯x)T (x j −¯x), where
i=1 xi. In matrix form we have B = (HX)(HX)T , and B is real, symmetric and
positive semi-deﬁnite. Let the eigendecomposition of B be B = V V T , where  is a
diagonal matrix and V is a matrix whose columns are the eigenvectors of B. If p < n,
there will be n −p zero eigenvalues.1 If the eigenvalues are ordered λ1 ≥λ2 ≥· · · ≥
λn ≥0, then B = VppV T
p , where p = diag(λ1, . . . , λp) and Vp is the n × p matrix
whose columns correspond to the ﬁrst p eigenvectors of B, with the usual normalization so
that the eigenvectors have unit length. The matrix ˆX of the reconstructed coordinates of the
points can be obtained as ˆX = Vp
2p, with B = ˆX ˆX T . Clearly from the information in the
dissimilarities one can only recover the original coordinates up to a translation, a rotation
and reﬂections of the axes; the solution obtained for ˆX is such that the origin is at the mean
of the n points, and that the axes chosen by the procedure are the principal axes of the ˆX
conﬁguration.
It may not be necessary to uses all p dimensions to obtain a reasonable approximation;
a conﬁguration ˆX in k-dimensions can be obtained by using the largest k eigenvalues so
that ˆX = Vk
k . These are known as the principal coordinates of X in k dimensions. The
fraction of the variance explained by the ﬁrst k eigenvalues is k
i=1 λi/ n
Classical scaling as explained above works on Euclidean distances as the dissimilarities. However, one can run the same algorithm with a non-Euclidean dissimilarity
matrix, although in this case there is no guarantee that the eigenvalues will be nonnegative.
Classical scaling derives from the work of Schoenberg and Young and Householder in
the 1930’s. Expositions of the theory can be found in Mardia, Kent, and Bibby and
Cox and Cox .
2.1.1. Optimality properties of classical scaling.
Mardia, Kent, and Bibby 
(Section 14.4) give the following optimality property of the classical scaling solution.
Theorem 1.
Let X denote a conﬁguration of points in Rp, with interpoint distances
i j = (xi −x j)T (xi −x j). Let L be a p × p rotation matrix and set L = (L1, L2), where
L1 is p × k for k < p. Let ˆX = XL1, the projection of X onto a k-dimensional subspace
of Rp, and let ˆd2
i j = (ˆxi −ˆx j)T (ˆxi −ˆx j). Amongst all projections ˆX = XL1, the quantity
i j) is minimized when X is projected onto its principal coordinates in k
dimensions. For all i, j we have ˆdi j ≤δi j. The value of φ for the principal coordinate
projection is φ = 2n(λk+1 + · · · + λp).
ON A CONNECTION BETWEEN KERNEL PCA AND METRIC MDS
Relationships between classical scaling and PCA
There is a well-known relationship between PCA and classical scaling; see e.g. Cox and
Cox Section 2.2.7.
Principal components analysis (PCA) is concerned with the eigendecomposition of the
samplecovariancematrix S = 1
n X T HX.ItiseasytoshowthattheeigenvaluesofnS arethe p
non-zero eigenvalues of B. To see this note that H 2 = H and thus that nS = (HX)T (HX).
Let vi be a unit-length eigenvector of B so that Bvi = λivi. Premultiplying by (HX)T
(HX)T (HX)(HX)T vi = λi(HX)T vi
so we see that λi is an eigenvalue of nS. yi = (HX)T vi is the corresponding eigenvector;
note that yT
i yi = λi. Centering X and projecting onto the unit vector ˆyi = λ−1/2
yi we obtain
HXˆyi = λ−1/2
HX(HX)T vi = λ1/2
Thus we see that the projection of X onto the eigenvectors of nS returns the classical scaling
Metric MDS
The aim of classical scaling is to ﬁnd a conﬁguration of points ˆX so that the interpoint
distances di j well approximate the dissimilarities δi j. In metric MDS this criterion is relaxed,
so that instead we require
di j ≈f (δi j),
where f is a speciﬁed (analytic) function. For this deﬁnition see, e.g. Kruskal and Wish
 (page 22), where polynomial transformations are suggested.
A straightforward way to carry out metric MDS is to deﬁne a error function (or stress)
i, j wi j(di j −f (δi j))2
where the {wi j} are appropriately chosen weights. One can then obtain derivatives of S
with respect to the coordinates of the points that deﬁne the di j’s and use gradient-based (or
more sophisticated methods) to minimize the stress. This method is known as least-squares
scaling. An early reference to this kind of method is Sammon , where wi j = 1/δi j
and f is the identity function.
Note that if f (δi j) has some adjustable parameters θ and is linear with respect to θ,2 then
the function f can also be adapted and the optimal value for those parameters given the
current di j’s can be obtained by (weighted) least-squares regression.
C.K.I. WILLIAMS
Critchley (also mentioned in Section 2.4.2 of Cox and Cox) carried out metric MDS by running the classical scaling algorithm on the transformed dissimilarities.
Critchley suggests the power transformation f (δi j) = δµ
i j (for µ > 0). If the dissimilarities are derived from Euclidean distances, we note that the kernel k(x, y) = −∥x −y∥β
is conditionally positive deﬁnite (CPD) if β ≤2 .
When the kernel is CPD, the centered matrix will be positive deﬁnite. Critchley’s use of
the classical scaling algorithm is similar to the algorithm discussed below, but crucially the
kernel PCA method ensures that the matrix B derived form the transformed dissimilarities is non-negative deﬁnite, while this is not guaranteed by Critchley’s transformation for
arbitrary µ.
A further member of the MDS family is nonmetric MDS (NMDS), also known as ordinal
scaling. Here it is only the relative rank ordering between the d’s and the δ’s that is taken
to be important; this constraint can be imposed by demanding that the function f in Eq. (3)
is monotonic. This constraint makes sense for some kinds of dissimilarity data (e.g. from
psychology) where only the rank orderings have real meaning.
Kernel PCA
In recent years there has been an explosion of work on kernel methods. For supervised learning these include support vector machines , Gaussian process prediction and spline methods . The basic idea of these
methods is to use the “kernel trick.” A point x in the original space is re-represented as a point
φ(x) in a NF-dimensional feature space3 F, where φ(x) = (φ1(x), φ2(x), . . . , φNF(x)). We
can think of each function φ j(·) as a non-linear mapping. The key to the kernel trick is to
realize that for many algorithms, the only quantities required are of the form4 φ(xi) · φ(x j)
and thus if these can be easily computed by a non-linear function k(xi, x j) = φ(xi) · φ(x j)
we can save much time and effort.
Sch¨olkopf, Smola, and M¨uller used this trick to deﬁne kernel PCA. One could
compute the covariance matrix in the feature space and then calculate its eigenvectors/eigenvalues. However, using the relationship between B and the sample covariance matrix S
described above, we can instead consider the n×n matrix K with entries Ki j = k(xi, x j) for
i, j = 1, . . . , n. If NF > n using K will be more efﬁcient than working with the covariance
matrix in feature space and anyway the latter would be singular.
The data should be centered in the feature space so that n
i=1 φ(xi) = 0. This is achieved
by carrying out the eigendecomposition of ˜K = HKH which gives the coordinates of the
approximating points as described in Section 2.2. Thus we see that the visualization of data
by projecting it onto the ﬁrst k eigenvectors is exactly classical scaling in feature space.
A relationship between kernel PCA and metric MDS
We consider two cases. In Section 4.1 we deal with the case that the kernel is isotropic
and obtain a close relationship between kernel PCA and metric MDS. If the kernel is
non-stationary a rather less close relationship is derived in Section 4.2.
ON A CONNECTION BETWEEN KERNEL PCA AND METRIC MDS
Isotropic kernels
A kernel function is stationary if k(xi, x j) depends only on the vector τ = xi −x j. A
stationary covariance function is isotropic if k(xi, x j) depends only on the distance δi j with
i j = τ.τ, so that we write k(xi, x j) =r(δi j). Assume that the kernel is scaled so that
r(0) = 1. An example of an isotropic kernel is the squared exponential or RBF (radial basis
function) kernel k(xi, x j) = exp{−θ(xi −x j)T (xi −x j)}, for some parameter θ > 0.
Consider the Euclidean distance in feature space ˜δ2
i j = (φ(xi)−φ(x j))T (φ(xi)−φ(x j)).
With an isotropic kernel this can be re-expressed as ˜δ2
i j = 2(1 −r(δi j)). Thus the matrix
A has elements ai j = r(δi j) −1, which can be written as A = K −11T . It can be easily
veriﬁed that the centering matrix H annihilates 11T , so that HAH = HKH.
We see that the conﬁguration of points derived from performing classical scaling on K
actually aims to approximate the feature-space distances computed as ˜δ2
i j = 2(1 −r(δi j)).
As the ˜δi j’s are a non-linear function of the δi j’s this procedure (kernel MDS) is an example
of metric MDS.
Kernel functions are usually chosen to be conditionally positive deﬁnite, so
that the eigenvalues of the matrix ˜K will be non-negative. Choosing arbitrary functions to
transform the dissimilarities will not give this guarantee.
In nonmetric MDS we require that di j ≈f (δi j) for some monotonic function
f . If the kernel function r is monotonically decreasing then clearly 1 −r is monotonically
increasing. However, there are valid isotropic kernel (covariance) functions which are nonmonotonic (e.g. the exponentially damped cosine r(δ) = e−θδ cos(ωδ); see Yaglom 
for details) and thus we see that f need not be monotonic in kernel MDS.
One advantage of PCA is that it deﬁnes a mapping from the original space
to the principal coordinates, and hence that if a new point x arrives, its projection onto
the principal coordinates deﬁned by the original n data points can be computed.5 The
same property holds in kernel PCA, so that the computation of the projection of φ(x)
onto the rth principal direction in feature space can be computed using the kernel trick as
i k(x, xi), where αr is the rth eigenvector of ˜K (see Eq. (4.1) in Sch¨olkopf, Smola,
and M¨uller ).
This projection property does not hold for algorithms that simply minimize the stress
objective function; for example the Sammon “mapping” algorithm does
not in fact deﬁne a mapping.
Non-stationary kernels
Sometimes non-stationary kernels (e.g. k(xi, x j) = (1+xi ·x j)m for integer m) are used. For
non-stationary kernels we proceed as before and construct ˜δ2
i j = (φ(xi) −φ(x j))T (φ(xi)−
φ(x j)). However, we now observe that ˜δ2
i j = k(xi, xi) + k(x j, x j) −2k(xi, x j) and thus the
C.K.I. WILLIAMS
matrix A can be written as
It is easily veriﬁed that constructing B = HAH annihilates the last two terms in Eq. (5), so
that again the kernel MDS procedure operates on HKH. (This is not surprising as the aim of
the construction of B is to work with the elements bi j = (φ(xi) −¯φ) · (φ(x j) −¯φ) where
¯φ is the mean vector in feature space.)
Hence the eigendecomposition of HKH can be used to carry out classical scaling is
feature space. However, the distance ˜δi j in feature space is not a function of δi j and so the
relationship of Eq. (3) does not hold. The situation can be saved somewhat if we follow
Mardia, Kent, and Bibby (Section 14.2.3) and relate similarities to dissimilarities through
i j = ˜cii + ˜cjj −2˜ci j,
where ˜ci j denotes the similarity between items i and j. Then we see that the similarity in
feature space is given by ˜ci j = φ(xi) · φ(x j) = k(xi, x j). For kernels (such as polynomial
kernels) that are functions of xi · x j (the similarity in input space), we see then that the
similarity in feature space is a non-linear function of the similarity measured in input space.
Choice of kernel
Having performed kernel MDS one can plot the scatter diagram (or Shepard diagram) of
the dissimilarities against the ﬁtted distances. We know that for each pair the ﬁtted distance
di j ≤˜δi j because of the projection property in feature space. The sum of the residuals is
given by 2n n
i=k+1 λi where the {λi} are the eigenvalues of ˜K = HKH. (See Theorem 1
above and recall that at most n of the eigenvalues of the covariance matrix in feature space
will be non-zero.)
One idea for choosing the kernel would be to ﬁx the dimensionality k and choose r(·) so
that the sum of residuals is minimized. Consider the effect of varying θ in the RBF kernel
k(xi, x j) = exp{−θ(xi −x j)T (xi −x j)}.
As θ →∞we have ˜δ2
i j = 2(1 −δ(i, j)) (where δ(i, j) is the Kronecker delta), which are
the distances corresponding to a regular simplex. Thus K →In, with trace(HKH) →n−1.
At the other extreme, letting θ →0 gives K →11T and after application of the centering
matrix trace(HKH) →0. Thus we expect that as θ →0 the summed residuals go to zero.
This can be explained by the fact that for θ →0, we have ˜δ2
i j = 2(1 −r(δi j)) →0
for all i, j, so essentially all points collapse on top of each other in feature space. Using
e−θz ≃1 −θz for small θ, we can show that Ki j = 1 −θδ2
i j as θ →0, and thus that the
classical scaling solution is obtained in this limit.
The collapse problem can be overcome by choosing a scaling of the kernel k(xi, x j; θ) =
C(θ)r(xi, x j) so that trace(HKH) = n. This normalization corresponds to setting
i=1(φ(xi) −¯φ).(φ(xi) −¯φ) = n in feature space.
ON A CONNECTION BETWEEN KERNEL PCA AND METRIC MDS
The plot shows the cumulative eigenspectrum for various values of β = θ/256 for the USPS test set.
Experiments have been run on the US Postal Service database of handwritten digits, as
used in Sch¨olkopf, Smola, and M¨uller . The test set of 2007 images was used. The
size of each image is 16×16 pixels, with the intensity of the pixels scaled so that the average
variance over all 256 dimensions is 0.5. The kernel is scaled so that trace(HKH) = n. In
ﬁgure 1 the cumulative eigenspectrum of HKH is plotted for various values of β = θ/256.
The cumulative eigenspectrum is a plot of the index j against  j
By choosing an index j one can observe from ﬁgure 1 what fraction of the variance is
explained by the ﬁrst j eigenvalues. The trend is that as θ decreases more and more variance
is explained by fewer components, which ﬁts in with the idea above that the θ →∞limit
gives rise to the regular simplex case. Thus there does not seem to be a non-trivial value of
θ which minimizes the residuals.
Critchley suggests an alternative measure of quality based on the eigenspectrum. Let the eigenvalues be scaled so that n
i=1 λi = n. Then Critchley’s criterion is
to set the parameters so that
is maximized. For the USPS experiment above this also leads to the trivial solution θ →0.
C.K.I. WILLIAMS
TheaimofCritchley’scriterionistomeasuretheunevennessoftheeigenvaluedistribution
and T (θ) is just one idea for doing this. Another might be to deﬁne pi = 1
n λi(θ) and look
to minimize the “entropy” H(θ) = −n
i=1 pi ln pi.
Discussion
The results above show that kernel PCA using an isotropic kernel function can be interpreted
asperformingakindofmetricMDS.ThemaindifferencebetweenthekernelMDSalgorithm
and other metric MDS algorithms is that kernel MDS uses the classical scaling solution in
feature space. The advantage of the classical scaling solution is that it is computed from
an eigenproblem, and avoids the iterative optimization of the stress objective function that
is used for most other MDS solutions. The classical scaling solution is unique up to the
unavoidable translation, rotation and reﬂection symmetries (assuming that there are no
repeated eigenvalues). The work of Critchley is somewhat similar to kernel MDS,
but it lacks the notion of a projection into feature space and does not always ensure that the
matrix B is non-negative deﬁnite.
We have also looked at the question of adapting the kernel so as to minimize the sum of
the residuals. However, for the case investigated this leads to a trivial solution.
Acknowledgments
I thank David Willshaw, Matthias Seeger and Amos Storkey for helpful conversations, and
the anonymous referees whose comments have helped improve the paper.
1. In fact if the points are not in “general position” the number of zero eigenvalues will be greater than n −p.
Below we assume that the points are in general position, although the arguments can easily be carried through
with minor modiﬁcations if this is not the case.
2. f can still be a non-linear function of its argument.
3. For some kernels NF = ∞.
4. We denote the inner product of two vectors as either a.b or aT b.
5. Note that this will be, in general, different to the solution found by doing PCA on the full data set of n + 1