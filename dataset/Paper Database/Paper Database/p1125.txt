An Introduction to Image Synthesis with
Generative Adversarial Nets
He Huang, Philip S. Yu and Changhu Wang
Abstract—There has been a drastic growth of research in Generative Adversarial Nets (GANs) in the past few years. Proposed in
2014, GAN has been applied to various applications such as computer vision and natural language processing, and achieves
impressive performance. Among the many applications of GAN, image synthesis is the most well-studied one, and research in this
area has already demonstrated the great potential of using GAN in image synthesis. In this paper, we provide a taxonomy of methods
used in image synthesis, review different models for text-to-image synthesis and image-to-image translation, and discuss some
evaluation metrics as well as possible future research directions in image synthesis with GAN.
Index Terms—Deep Learning, Generative Adversarial Nets, Image Synthesis, Computer Vision.
INTRODUCTION
ITH recent advances in deep learning, machine learning algorithms have evolved to such an extent that
they can compete and even defeat humans in some tasks,
such as image classiﬁcation on ImageNet , playing Go
 and Texas Hold’em poker . However, we still cannot
conclude that those algorithms have true “intelligence”,
since knowing how to do something does not necessarily
mean understanding something, and it is critical for a truly
intelligent agent to understand its tasks. “What I cannot
create, I do not understand”, said the famous physicist Richard
Feynman. To put this quote in the case of machine learning,
we can say that, for machines to understand their input data,
they need to learn to create the data. The most promising
approach is to use generative models that learn to discover
the essence of data and ﬁnd a best distribution to represent
it. Also, with a learned generative model, we can even draw
samples which are not in the training set but follow the same
distribution.
As a new framework of generative model, Generative
Adversarial Net (GAN) , proposed in 2014, is able to
generate better synthetic images than previous generative
models, and since then it has become one of the most popular research areas. A Generative Adversarial Net consists of
two neural networks, a generator and a discriminator, where
the generator tries to produce realistic samples that fool the
discriminator, while the discriminator tries to distinguish
real samples from generated ones. There are two main
threads of research on GAN. One is the theoretical thread
that tries to alleviate the instability and mode collapse
problems of GAN , or reformulate
it from different angles like information theory and
energy-based models . The other thread focuses on the
applications of GAN in computer vision (CV) , natural
language processing (NLP) and other areas.
He Huang and Philip S. Yu are with the Department of Computer Science,
University of Illinois at Chicago, USA.
Emails: {hehuang, psyu}@uic.edu
Changhu Wang is with ByteDance AI Lab, China.
Email: 
There is a great tutorial given by Goodfellow in NIPS
2016 on GAN where he describes the importance of
generative models, explains how GAN works, compares
GAN with other generative model and discusses frontier
research topics in GAN. Also, there is a recent review paper
on GAN which reviews several GAN architectures
and training techniques, and introduces some applications
of GAN. However, both of these papers are in a general
scope, without going into details of speciﬁc applications.
In this paper, we speciﬁcally focus on image synthesis,
whose goal is to generate images, since it is by far the
most studied area where GAN has been applied. Besides
image synthesis, there are many other applications of GAN
in computer vision, such as image in-painting , image
captioning , object detection and semantic
segmentation .
Research of applying GAN in natural language processing is also a growing trend, such as text modeling 
 , dialogue generation , question answering and
neural machine translation . However, training GAN
in NLP tasks is more difﬁcult and requires more techniques , which also makes it a challenging but intriguing
research area.
The main goal of this paper is to provide an overview
of the methods used in image synthesis with GAN and
point out strengths and weaknesses of current methods.
We classify the main approaches in image synthesis into
three methods, i.e. direct methods, hierarchical methods
and iterative methods. Besides these most commonly used
methods, there are also other methods which we will brieﬂy
mention. We then give a detailed discussion in two of the
most important tasks in image synthesis, i.e. text-to-image
synthesis and image-to-image translation. We also discuss
the possible reasons why GAN performs so well in certain
tasks and its role in our goal to artiﬁcial intelligence. The
goal of this paper is to provide a simple guideline for those
who want to apply GAN to their problems and help further
research in GAN.
The rest of this paper is organized as follows. In Section 2
we ﬁrst review some core concepts of GAN, as well as some
 
variants and training issues. Then in Section 3 we introduce
three main approaches and some other approaches used in
image synthesis. In Section 4, we discuss several methods
in text-to-image synthesis and some possible research directions for improvements. In Section 5, we ﬁrst introduce
supervised and unsupervised methods for image-to-image
translation, and then turn to more speciﬁc applications like
face editing, video prediction and image super-resolution. In
Section 6 we review some evaluation metrics for synthetic
images, while in Section 7 we discuss the discriminator’s
role as a learned loss function. Conclusions are given in
Section 8.
GAN PRELIMINARIES
In this section, we review some core concepts of Generative
Adversarial Nets (GANs) and some improvements.
A generative model G parameterized by θ takes as input
a random noise z and output a sample G(z; θ), so the output
can be regarded as a sample drawn from a distribution:
G(z; θ) ∼pg. Meanwhile, we have a lot of training data
x drawn from pdata, and the training objective for the
generative model G is to approximate pdata using pg.
Discriminator
Random noise
Synthetic data
Fig. 1. General structure of a Generative Adversarial Network, where
the generator G takes a noise vector z as input and output a synthetic
sample G(z), and the discriminator takes both the synthetic input G(z)
and true sample x as inputs and predict whether they are real or fake.
Generative Adversarial Net (GAN) consists of two
separate neural networks: a generator G that takes a random
noise vector z, and outputs synthetic data G(z); a discriminator D that takes an input x or G(z) and output a probability D(x) or D(G(z)) to indicate whether it is synthetic or
from the true data distribution, as shown in Figure 1. Both
of the generator and discriminator can be arbitrary neural
networks. The ﬁrst GAN uses fully connected layer
as its building block. Later, DCGAN proposes to use
fully convolutional neural networks which achieves better
performance, and since then convolution and transposed convolution layers have become the core components in many
GAN models. For more details on (transposed) convolution
arithmetic, please refer to this report .
The original way to train the generator and discriminator
is to form a two-player min-max game where the generator
G tries to generate realistic data to fool the discriminator
while discriminator D tries to distinguish between real and
synthetic data . The value function to be optimized is
shown in Equation 1, where pdata(x) denotes the true data
distribution and pz(z) denote the noise distribution.
D V (D, G) = Ex∼pdata(x)[log D(x)]
+ Ez∼pz(z)[log(1 −D(G(z)))]
However, when the discriminator is trained much better
than the generator, D can reject the samples from G with
conﬁdence close to 1, and thus the loss log(1 −D(G(z)))
saturates and G can not learn anything from zero gradient. To prevent this, instead of training G to minimize
log(1 −D(G(z))), we can train it to maximize log D(G(z))
 . Although the new loss function for G gives a different
scale of gradient than the original one, it still provides the
same direction of gradient and does not saturate.
Conditional GAN
In the original GAN, we have no control of what to be
generated, since the output is only dependent on random
noise. However, we can add a conditional input c to the
random noise z so that the generated image is deﬁned
by G(c, z) . Typically, the conditional input vector c
is concatenated with the noise vector z, and the resulting
vector is put into the generator as it is in the original GAN.
Besides, we can perform other data augmentation on c and
z, as in . The meaning of conditional input c is arbitrary,
for example, it can be the class of image, attributes of object
 or an embedding of text descriptions of the image we
want to generate .
GAN with Auxiliary Classiﬁer
In order to feed more side-information and to allow for
semi-supervised learning, one can add an additional taskspeciﬁc auxiliary classiﬁer to the discriminator, so that the
model is optimized on the original tasks as well as the
additional task . The architecture of such method is
illustrated in Figure 2, where C is the auxiliary classiﬁer.
Adding auxiliary classiﬁers allows us to use pre-trained
models (e.g. image classiﬁers trained on ImageNet), and
experiments in AC-GAN demonstrate that such method
can help generating sharper images as well as alleviate the
mode collapse problem. Using auxiliary classiﬁers can also
help in applications such as text-to-image synthesis and
image-to-image translation .
Fig. 2. Architecture of GAN with auxiliary classiﬁer, where y is the
conditional input label and C is the classiﬁer that takes the synthetic
image G(y, z) as input and predict its label ˆy
GAN with Encoder
Although GAN can transform a noise vector z into a synthetic data sample G(z), it does not allow inverse transformation. If we treat the noise distribution as a latent
feature space for data samples, GAN lacks the ability to map
data sample x into latent feature z. In order to allow such
mapping, two concurrent works BiGAN and ALI 
propose to add an encoder E in the original GAN framework, as shown in Figure 3. Let Ωx be the data space and
Ωz be the latent feature space, the encoder E takes x ∈Ωx
as input and produce a feature vector E(x) ∈Ωz as output.
The discriminator D is modiﬁed to take both a data sample
and a feature vector as input to calculate P(Y |x, z), where
Y = 1 indicates the sample is real and Y = 0 means the
data is generated by G.
Fig. 3. Architecture of BiGAN/ALI
The objective is thus deﬁned as:
D V (G, E, D) = Ex∼pdata(x) log D(x, E(x))
+ Ez∼pz(z) log(1 −D(G(z), z))
GAN with Variational Auto-Encoder
Fig. 4. Architecture of VAE-GAN
VAE-GAN proposes to combine Variational Auto-
Encoder (VAE) with GAN to exploit both of their
beneﬁts, as GAN can generate sharp images but often
miss some modes while images produced by VAE 
are blurry but have large variety. The architecture of VAE-
GAN is shown in Figure 4. The VAE part regularize the
encoder E by imposing a prior of normal distribution (e.g.
z ∼N(0, 1)), and the VAE loss term is deﬁned as:
LV AE = −Ez∼q(z|x) log[p(x|z)] + DKL(q(z|x)||p(x)),
where z ∼E(x) = q(z|x), x ∼G(z) = p(x|z) and DKL is
the Kullback-Leibler divergence.
Also, VAE-GAN proposes to represent the reconstruction loss of VAE in terms of the discriminator D. Let
Dl(x) denotes the representation of the l-th layer of the
discriminator, and a Gaussian observation model can be
deﬁned as:
p(D(x)|z) = N(D(x)|D(˜x), I),
where ˜x ∼G(z) is a sample from the generator, and I is the
identity matrix. So the new VAE loss is:
LV AE = −Ez∼q(z|x) log[p(D(x)|z)] + DKL(q(z|x)||p(x)),
which is then combined with the GAN loss deﬁned in
Equation 1. Experiments demonstrate that VAE-GAN can
generate better images than VAE or GAN alone.
Handling Mode Collapse
Although GAN is very effective in image synthesis, its
training process is very unstable and requires a lot of tricks
to get a good result, as pointed out in . Despite
its instability in training, GAN also suffers from the mode
collapse problem, as discussed in . In the original
GAN formulation , the discriminator does not need to
consider the variety of synthetic samples, but only focuses
on telling whether each sample is realistic or not, which
makes it possible for the generator to spend efforts in
generating a few samples that are good enough to fool
the discriminator. For example, although the MNIST 
dataset contains images of digits from 0 to 9, in an extreme
case, a generator only needs to learn to generate one of the
ten digits perfectly to completely fool the discriminator, and
then the generator stops trying to generate the other nine
digits. The absence of the other nine digits is an example
of inter-class mode collapse. An example of intra-class mode
collapse is, there are many writing styles for each of the
digits, but the generator only learns to generate one perfect
sample for each digit to successfully fool the discriminator.
Many methods have been proposed to address the
model collapse problem. One technique is called minibatch
features , whose idea is to make the discriminator compare
an example to a minibatch of true samples as well as a minibatch of generated samples. In this way, the discriminator
can learn to tell if a generated sample is too similar to some
other generated samples by measuring samples’ distances in
latent space. Although this method works well, as discussed
in , the performance largely depends on what features
are used in distance calculation. MRGAN proposes to
add an encoder which transforms a sample in data space
back to latent space, as in BiGAN . The combination
of encoder and generator acts as an auto-encoder, whose
reconstruction loss is added to the adversarial loss to act
as a mode regularizer. Meanwhile, the discriminator is also
trained to discriminate reconstructed samples, which acts
as another mode regularizer. WGAN proposes to use
Wasserstein distance to measure the similarity between true
data distribution and the learned distribution, instead of using Jensen-Shannon divergence as in the original GAN .
Although it theoretically avoids mode collapse, it takes a
longer time for the model to converge than previous GANs.
To alleviate this problem, WGAN-GP proposes to use
gradient penalty, instead of weight clipping in WGAN.
WGAN-GP generally produces good images and greatly
avoid mode collapse, and it is easy to apply this training
framework to other GAN models. More tips for training
GANs can be found in Soumith’s NIPS 2016 tutorial “How
to train a GAN”1.
1. 
Generator 1
Generator 2
Generator 1
Generator 2
Generator k
Discriminator
Discriminator 2
Discriminator 1
Discriminator 2
Discriminator 1
Discriminator k
(a) Direct method
(b) Hierarchical method
(c) Iterative method
Structure/Background
Texture/Foreground
(diﬀerent structures, not share weights)
(same or similar structures, may share weights)
low resolution / rough details
high resolution / sharp details
Discriminator k-1
Generator k-1
Fig. 5. Three approaches of image synthesis using Generative Adversarial Networks. The direct method does everything with only one generator
and one discriminator, while the other two methods have multiple generators and discriminators. Hierarchical method usually use two layers of
GANs, where each GAN plays a fundamentally different role than the other one. Iterative method, however, contains multiple GANs that perform
the same task but operate at different resolution.
Transposed convolution
Batch normalization
ReLU activation
Convolution
Batch normalization
Leaky ReLU activation
Discriminator
Fig. 6. Building blocks of DCGAN, where the generator uses transposed
convolution, batch-normalization and ReLU activation, while the discriminator uses convolution, batch-normalization and LeakyReLU activation
GENERAL APPROACHES OF IMAGE SYNTHESIS
In this section, we summarize the three main approaches
used in generating images, i.e. direct methods, iterative methods and hierarchical methods respectively, which form the
basis of all applications mentioned in this paper. The overall
structures of these three methods are shown in Figure 5.
Direct Methods
All methods under this category follows the philosophy of
using one generator and one discriminator in their models,
and the structures of the generator and the discriminator
are straight-forward without branches. Many of the earliest
GAN models fall into this category, like GAN , DCGAN
 , ImprovedGAN , InfoGAN , f-GAN and GAN-
INT-CLS . Among them, DCGAN is one of the most
classic ones whose structure is used by many later models
such as . The general building blocks used
in DCGAN are shown in Figure 6, where the generator uses
transposed convolution, batch-normalization and ReLU activation, while the discriminator uses convolution, batchnormalization and LeakyReLU activation.
This kind of method is relatively more straight-forward
to design and implement when compared with hierarchical
and iterative methods, and it usually achieves good results.
Hierarchical Methods
Contrary to the Direct Method, algorithms under the Hierarchical Method use two generators and two discriminators
in their models, where different generators have different
purposes. The idea behind those methods is to separate
an image into two parts, like “styles & structure” and
“foreground & background”. The relation between the two
generators may be either parallel or sequential.
SS-GAN proposes to use two GANs, a Structure-
GAN for generating a surface normal map from random
noise ˆz, and another Style-GAN that takes both the generated surface normal map as well as a noise ˜z as input and
outputs an image. The Structure-GAN uses the same building blocks as DCGAN , while the Style-GAN is slightly
different. For Style-Generator, the generated surface normal
map and the noise vector go through several convolutional
and transposed convolutional layers respectively, and then
the results are concatenated into a single tensor which will
go through the remaining layers in Style-Generator. As
for the Style-Discriminator, each surface normal map and
its corresponding image are concatenated at the channel
dimension to form a single input to the discriminator. Besides, SS-GAN assumes that, a good synthetic image should
also be used to reconstruct a good surface normal map.
Under this assumption, SS-GAN designs a fully-connected
network that transforms an image back to its surface normal
map, and uses a pixel-wise loss that enforces the reconstructed surface normal to approximate the true one. A main
limitation of SS-GAN is that it requires to use Kinect to
obtain groundtruth for surface normal maps.
As a special example, LR-GAN chooses to generate
the foreground and background content using different
generator, but only one discriminator is used to judge the
images while the recurrent image generation process is
related to the iterative method. Nonetheless, experiments of
LR-GAN demonstrate that it is possible to separate the generation of foreground and background content and produce
sharper images.
Iterative Methods
This method differentiates itself from Hierarchical Methods in
two ways. First, instead of using two different generators
that perform different roles, the models in this category use
multiple generators that have similar or even the same structures, and they generate images from coarse to ﬁne, with
each generator reﬁning the details of the results from the
previous generator. Second, when using the same structures
in the generators, Iterative Methods can use weight-sharing
among the generators , while Hierarchical Methods usually can not.
LAPGAN is the ﬁrst GAN that uses an iterative
method to generate images from coarse to ﬁne using Laplacian pyramid . The multiple generators in LAPGAN
perform the same task: takes an image from previous generator and a noise vector as input, and then outputs the
details (a residual image) that can make the image sharper
when added to the input image. The only difference in the
structures of those generators is the size of input/output
dimension, while an exception is that the generator at the
lowest level only takes a noise vector as input and outputs
an image. LAPGAN outperforms the original GAN and
shows that iterative method can generate sharper images
than direct method.
StackGAN , as an iterative method, has only two
layers of generators. First generator takes an input (z, c) and
then outputs a blurry image that can show a rough shape
and blurry details of the objects, while the second generator
takes (z, c) and the image generated by the previous generator and then output a larger image with more photo-realistic
Another example of Iterative Methods is SGAN which
stacks generators that takes lower level feature as input and
outputs higher level features, while the bottom generator
takes a noise vector as input and the top generator outputs
an image. The necessity of using separate generators for
different levels of features is that SGAN associates an encoder, a discriminator and a Q-network (which is used
to predict the posterior probability P(zi|hi) for entropy
maximization, where hi is the output feature of the i-th
layer) for each generator, so as to constrain and improve
the quality of those features.
An example of using weight-sharing is the GRAN 
model, which is an extension to the DRAW model
which is based on variational autoencoder . As in
DRAW, GRAN generates an image in a recurrent way that
feeds the output of the previous step into the model and the
output of the current step will be fed back as the input in the
next step. All steps use the same generator, so the weights
are shared among them, just like classic Recurrent Neural
Network (RNN).
Other Methods
PPGN produces impressive images in several tasks,
such as class-conditioned image synthesis , text-toimage synthesis and image inpainting . Different
from other methods mentioned earlier, PPGN uses activation
maximization to generate images, and it is based on
sampling with a prior learned with denoising autoencoder
(DAE) . To generate an image conditioned on a certain
class label y, instead of using a feed-forward way (e.g.
recurrent methods can be seen as feed-forward if unfolded
through time), PPGN runs an optimization process that
ﬁnds an input z to the generator that makes the output
image highly activate a certain neuron in another pretrained
classiﬁer (in this case, the neuron in the output layer that
corresponds to its class label y).
In order to generate better higher resolution images, ProgressiveGAN proposes to start with training a generator
and discriminator of 4×4 pixels, after which it incrementally
adds extra layers that doubles the output resolution up
to 1024 × 1024. This approach allows the model to learn
coarse structure ﬁrst and then focus on reﬁning details later,
instead of having to deal with all details at different scale
simultaneously.
TEXT-TO-IMAGE SYNTHESIS
When we apply GAN to image synthesis, it is desired to
control the content of generated images. Although there are
label-conditioned GAN models like cGAN which can
generate images belong to a speciﬁc class, it remains a great
challenge to generate images based on text descriptions.
Text-to-image synthesis is kind of the holy grail of computer
vision, since if an algorithm can generate truly realistic
images from mere text descriptions, we can have a high
conﬁdence that the algorithm actually understands what
is in the images, where computer vision is about teaching
computers to see and understand visual contents in the real
GAN-INT-CLS provides the ﬁrst attempt of using
GAN to generate images from text descriptions. The idea is
similar to conditional GAN that concatenates the condition
vector with the noise vector, but with the difference of
using the embedding of text sentences instead of class labels
or attributes. The embedding method used in GAN-INT-
CLS is from another paper that tries to learn robust embeddings of images and sentences given the imagesentence pairs. Except for the sentence embedding method,
the generator of GAN-INT-CLS follows the same architecture of DCGAN . As for the discriminator, in order to
take into account the text description, the text embedding
vector of length K is spatially replicated to become a text
embedding tensor of shape [W × H × K], where W and H
are the width and height of the generated image’s feature
tensor after going through several convolutional layers in
the discriminator. Then the text embedding tensor is combined with the image feature tensor of shape [W ×H ×C] at
the channel dimension C and forms a new tensor of shape
[W×H×(C+K)], which then goes through the rest layers of
the discriminator. The intuition behind this approach is that,
by spatial replication and depth concatenation, each “pixel”
(of shape [1 × 1 × (C + K)]) of the image’s feature tensor
contains all the information of text description, and then the
convolutional layers can learn to align the image’s content
with certain parts of the text feature by using multiple
convolution kernels.
GAN-INT-CLS also proposes to distinguish between
two sources of errors: unrealistic image with any text, and
realistic image with mismatched text. To train the discriminator
to distinguish these two kinds of errors, three types of input
are fed into the discriminator at every training step: {real
image, right text}, {real image, wrong text} and {fake image,
right text}. The experimental results in show that such
training technique is important in generating high quality
images, since it tells the model not only how to generate
realistic images, but also the correspondence between text
and images.
In addition, GAN-INT-CLS proposes to use manifold interpolation to obtain more text embeddings, by simply interpolating between captions in the training set. Since
the number of captions for each image is usually no more
than ﬁve and an image can be described in many ways,
doing such interpolation allows the model to learn from
possible text descriptions that are not in the training set.
According to the authors, the interpolated text embeddings
need not correspond to actual human-written text, so there
is no extra labeling required.
TAC-GAN is a combination of GAN-INT-CLS 
and AC-GAN . With the auxiliary classiﬁer, TAC-GAN
is able to achieve higher Inception Score than GAN-
INT-CLS and StackGAN on the Oxford-102 
Text-to-Image with Location Constraints
Although GAN-INT-CLS and StackGAN can generate images based on text description, they fail to capture the
localization constraints of the objects in images. To allow encoding spatial constraints, Reed et al. propose GAWWN 
that presents two possible solutions.
The ﬁrst approach proposed in GAWWN is to learn
a bounding box for an object by putting a spatially replicated text embedding tensor through a Spatial Transformer
Network . Here the spatial replication is the same process
mentioned in GAN-INT-CLS . The output of spatial
transformer network is a tensor with the same dimension
as input, but values outside of the bounding are all zeros.
The output tensor of the spatial transformer goes through
several convolutional layers to reduce its size back to a 1dimensional vector, which not only preserves the information of text but also provides a constraint on object’s location
by the bounding box. A beneﬁt of this approach is that it is
end-to-end, without requiring additional input.
The second approach proposed in GAWWN is to use
user-speciﬁed keypoints to constrain the different parts (e.g.
head, leg, arm, tail, etc.) of the object in the image. For each
keypoint, a mask matrix is generated where the keypoint
position is 1 and others 0, and all the matrices are combined
through depth concatenation to form a mask tensor of shape
[M × M × K], where M is the size of the mask and K
is the number of keypoints. The tensor is then ﬂattened
into a binary matrix with 1 indicating the presence of a
keypoint and 0 otherwise, and then replicated depth-wise to
become a tensor to be fed into remaining layers. Although
this approach allows more detailed constraints on the object,
it requires extra user input to specify the keypoints. Even if
GAWWN can infer unobserved keypoints from a few userspeciﬁed keypoints so that the user does not need to specify
all keypoints, the cost of extra user input is still non-trivial.
The remaining part of GAWWN is similar to GAN-
INT-CLS , with the difference of using a separate pathway to process bounding box tensor or keypoint tensor.
Although GAWWN provides two approaches that can enforce location constraints on the generated images, it only
works on images with single objects, since neither of the
proposed methods is able to handle several different objects
in an image. From the result shown in , GAWWN works
well on the CUB dataset , while the synthetic images
generated from the model trained on MPII Human Pose
(MHP) dataset are very blurry and it is hard to tell what
the content is. This may be due to the fact that the poses of a
standing bird are very similar to each other (note that birds
in the CUB dataset are at standing poses), while a human’s
poses can be of uncountable types.
The main beneﬁt of specifying the locations of each part
of the objects is that it yields more interpretable results,
and that it is desirable that the model can understand the
concepts of different parts of objects, which is one of the
ultimate goals of computer vision.
Text-to-Image with Stacked GANs
Instead of using only one generator, StackGAN proposes to use two different generators for text-to-image synthesis. The ﬁrst generator is responsible for generating lowresolution images that contain rough shapes and colors of
objects, while the second generator takes the output of the
ﬁrst generator and produces images with higher resolution
and sharper details. Each generator is associated with its
own discriminator. Besides, StackGAN also proposes a conditional data augmentation technique to produce more text
embeddings for the generator. StackGAN randomly samples
from a Gaussian distribution N(µ(φt), Σ(φt)), where the
mean vector µ(φt) and diagonal variance matrix Σ(φt)
are functions of text embedding φt. To further enforce the
smoothness over conditional manifold, N(µ(φt), Σ(φt)) is
constrained to approximate a standard Gaussian distribution N(0, 1) by adding a Kullback-Leibler divergence regularization term.
As an improved version of StackGAN, StackGAN++ 
proposes to have more pairs of generators and discriminators instead of just two, adds an unconditional image
synthesis loss to the discriminators, and uses a colorconsistency regularization term calculated by mean-square
loss of the means and variances between real and fake
AttnGAN further extends the architecture of Stack-
GAN++ by using attention mechanism over image and
text features. In AttnGAM, each sentence is embedded into a
global sentence vector and each word of the sentence is also
embedded into a word vector. The global sentence vector is
used to generate a low resolution image at the ﬁrst stage,
and then the following stages use the input image features
from the previous stage and the word vectors as input to the
attention layer and calculate a word-context vector which
will be combined with the image features and form the
input to the generator that will generate new image features.
Besides, AttnGAN also proposes a Deep Attentional Multimodal Similarity Model (DAMSM) that uses attention layers
to compute the similarity between images and text using
both global sentence vectors as well as ﬁne-grained word
vectors, which provides an additional ﬁne-grained imagetext matching loss for training the generator. Experiments of
AttnGAN not only show the effectiveness of using attention
layers in image synthesis, but also make the model more
interpretable.
generators,
GAN++ and AttnGAN produce sharper images
than GAN-INT-CLS and GAWWN on the CUB 
and Oxford-102 datasets. Although AttnGAN 
claims to have signiﬁcantly higher Inception Score than
PPGN on the COCO dataset, the examples it
provides do not visually look apparently better.
Text-to-Image by Iterative Sampling
Different from previous approaches that directly incorporate
the text information in the generation process, PPGN 
proposes to use activation maximization method to generate images in an iterative sampling way. The model
contains two separate parts: a pretrained image captioning
model, and an image generator. The image generator is a
combination of denoising auto-encoder and GAN, trained
independent of the image captioning model. Let p(x) be
the distribution of images, and p(y) be the distribution
of text descriptions, we want to sample image from the
joint distribution p(x, y). PPGN factorizes the joint distribution into two factors: p(x, y) = p(x)p(y|x), where p(x)
is modeled by the generator, and p(y|x) is modeled by the
image captioning model. According to the paper , given
a trained image generator and image captioning model, the
following iterative sampling process is performed to obtain
an image x based on the description y∗:
xt+1 = xt + ϵ1
∂log p(xt)
∂log p(y = y∗|xt)
where the ϵ1 term takes a step from current image xt to
a more realistic image (regardless of the text description),
the ϵ2 term takes a step from current image to an image
that better matches the description y∗(here the LRCN
model is used for the p(y = y∗|x) term), and the ϵ3
term adds a small noise to allow for a broader search in the
latent space.
Although such iterative sampling method takes more
time to generate an image in test phrase, it can generate
higher resolution images with better quality than previous
methods like , and its performance is among the
best in both class-conditioned and text-conditioned image
synthesis.
Limitations of Current Text-to-Image Models
Present text-to-image models perform well on datasets with
single object per image, such as human faces in CelebA ,
birds in CUB , ﬂowers in Oxford-102 , and some
objects in ImageNet . Also, they can synthesize reasonable
images for scenes like bedrooms and living rooms in the
LSUN , even though the objects in the scenes lack sharp
details. However, all present models work badly in situation
where multiple complicated objects are involved in one
image, as it is in the MS-COCO dataset .
A plausible reason why current models fail to work well
on complicated images is that the models only learn the
overall features of an image, instead of learning the concept
of each kind of objects in it. This gives an explanation why
synthetic scenes of bedrooms and living rooms lack sharp
details, since the model do not distinguish between a bed
and a desk, all it sees is that some patterns of shapes and
colors should be put somewhere in the synthetic image.
In other words, the model does not really understand the
image, but just remembers where to put some shapes and
Generative Adversarial Network certainly provides us
a promising way to do text-to-image synthesis, since it
produces sharper images than any other generative methods
so far. To take a further step in text-to-image synthesis, we
need to ﬁgure out novel ways to teach the algorithms the
concepts of things. One possible way is to train separate
models that can generate different kinds of objects, and then
train another model that learns how to combine different
objects (i.e. the reasonable relations between objects) into
one image based on the text descriptions. However, such
method requires large training sets for different objects, and
another large dataset that contains images of those different
objects, which is hard to acquire. Another possible direction
may be to make use of the Capsule idea proposed by Hinton
et al. since capsules are designed to capture the
concepts of objects, but how to efﬁciently train such capsulebased network is still a problem to be solved.
IMAGE-TO-IMAGE TRANSLATION
In this section, we discuss another type of image synthesis,
image-to-image translation, which takes images as conditional
input. Image-to-image translation is deﬁned as the problem
of translating a possible representation of one scene into
another, such as mapping BW images into RGB images,
or the other way around . This problem is related to
style transfer , which takes a content image and a
style image and output an image with the content of the
content image and the style of the style image. Image-toimage translation can be viewed as a generalization of style
transfer, since it is not limited to transferring the styles of
images, but can also manipulate attributes of objects (as in
applications of face editing). In this section, we introduce
several models that work for general image-to-image translation, from supervised methods to unsupervised ones. The
“supervision” here means that for each image in the source
domain, there is a corresponding ground-truth image in
the target domain. Later we will turn into different speciﬁc
applications such as face editing, image super resolution, image
in-painting and video prediction.
Supervised Image-to-Image Translation
Pix2Pix proposes to combine the loss of a conditional
Generative Adversarial Network (cGAN) with L1 regularization loss, so that the generator is not only trained to fool
the discriminator but also to generate images as close to
ground-truth as possible. The reason for using L1 instead of
L2 is that L1 produces less blurry images .
The conditional GAN loss is deﬁned as:
LcGAN(G, D) = Ex,y∼pdata(x,y)[log D(x, y)]+
Ex∼pdata(x),z∼pz(z)[log(1 −D(x, G(x, z))],
where x, y ∼p(x, y) are images of the same scene with
different styles, z ∼p(z) is a random noise as in the regular
The L1 loss for constraining self-similarity is deﬁned as:
LL1(G) = Ex,y∼pdata(x,y),z∼pz(z)[||y −G(x, z)||1],
The overall objective is thus given by:
G∗, D∗= arg min
D LcGAN(G, D) + λLL1(G),
where λ is a hyper-parameter to balance the two loss terms.
The authors of Pix2Pix ﬁnd that the noise z does not
have obvious effect on the output, so they provide the noise
in the form of dropout at training and test time instead of
drawing samples from a random distribution.
The generator structure for Pix2Pix is based on U-
Net , which belongs to the encoder-decoder framework
but adds skip connections from encoder to decoder so as
to allow circumventing the bottleneck for sharing low-level
information like edges of objects.
Pix2Pix proposes PatchGAN (the patch-based idea
was previously explored in MGAN ) as the discriminator, which, instead of classifying the whole image, tries to
classify each N × N path of the image and average all the
scores of patches to get the ﬁnal score for the image. The
motivation of this method is that, although L1 and L2 losses
produce blurry images and fail to capture high frequency
details, in many cases they can capture the low frequencies
quite well. In order to capture the high frequency details,
Pix2Pix argues that it is sufﬁcient to restrict the discriminator to focus only on local patches. From the experiments,
it is found that, for an 256×256 image, a patch-size of 70×70
works best.
Although Pix2Pix produces very impressive synthetic
images, the major limitation is that it must use paired
images as supervision, as is shown in Equation 8 that data
pair (x, y) is drawn from the joint distribution p(x, y).
Supervised Image-to-Image Translation with Pairwise Discrimination
PLDT proposes another method to do supervised imageto-image translation, by adding another discriminator Dpair
that learns to tell whether a pair of images from different
domains is associated with each other. The architecture
of PLDT is shown in Figure 7. Given an input image xs
from source domain, its ground-truth image xt in the target
domain, an irrelevant image x−
t in the target domain, and
the generator G transfers xs into an image ˆxt in the target
domain, the loss for Dpair can be deﬁned as:
Lpair = −t · log[Dpair(xs, x)]
+ (t −1) · log[1 −Dpair(xs, x)],
if x = ˆxt
The generator of PLDT is implemented in an
encoder-decoder fashion using (transposed) convolutions,
while the two discriminators are implemented as fully convolutional networks. As shown in the experimental results,
PLDT performs domain transfer that modiﬁes the geometric shapes of objects while trying to keep the texture
consistent among all associated images.
Channel concatenation
Associated/
Unassociated
Fig. 7. Architecture of Pixel-Level Domain Transfer (PLDT) . In this
example, the source domain is BW color space, while the target domain
is RGB space. As explained in , this model can performs geometric
modiﬁcations on the input image, while keeping the object in the output
image the same as input.
Unsupervised
Image-to-Image
Translation
Cyclic Loss
Two concurrent works CycleGAN and DualGAN 
propose to add a self-consistency (reconstruction) loss that
tries to preserve the input image after a cycle of transformation. CycleGAN and DualGAN share the same framework,
which is shown in Figure 8. As we can see, the two generators GAB and GBA are doing opposite transformations,
which can be seen as a kind of dual learning . Besides,
DiscoGAN is another model that utilizes the same cyclic
framework as Figure 8.
Here we use CycleGAN as an example. In CycleGAN,
there are two generators, GAB that transfer an image from
domain A to B and GBA that performs the opposite transformation. Also, there are also two discriminators DA and
DB that predicts whether an image belongs to that domain.
For a pair of GAB and DB, the adversarial loss function is
deﬁned as:
LGAN(GAB, DB) = Eb∼pB(b)[log DB(b)]
+ Ea∼pA(a)[1 −log(DB(GAB(a))], (11)
and similarly for the pair GBA and DA, we can deﬁne the
adversarial loss as LGAN(GBA, DA).
Besides the adversarial loss, a cycle-consistency loss is
designed to minimize the reconstruction error after we
translate an image of one domain to another and then
translate it back to the original domain, i.e. a →GAB(a) →
GBA(GAB(a)) ≈a. Since this cycle can be deﬁned from two
directions, the cycle-consistency loss is deﬁned as:
Lcyc(GAB, GBA) = Ea∼pA(a)[||a −GBA(GAB(a))||1]
+ Eb∼pB(b)[||b −GAB(GBA(b))||1].
Then the overall loss function is:
L(GAB, GBA, DA, DB) = LGAN(GAB, DB)
+ LGAN(GBA, DA)
+ λLcyc(GAB, GBA),
GAB(GBA(b))
GBA(GAB(a))
||a −GBA(GAB(a))||1
||b −GAB(GBA(b))||1
Reconstruction
Reconstruction
Fig. 8. Framework of CycleGAN and DualGAN. A and B are two different domains. There is a discriminator for each domain that judges if an image
belong to that domain. Two generators are designed to translate an image from one domain to another. There are two cycles of data ﬂow, the red
one performs a sequence of domain transfer A →B →A, while the blue one is B →A →B. L1 loss is applied on the input a (or b) and the
reconstructed input GBA(GAB(a)) (or GAB(GBA(b))) to enforce self-consistency.
where λ is a hyper-parameter to balance the losses. Then the
objective is to solve for:
GAB,GBA max
DB,DA L(GAB, GBA, DA, DB).
Although CycleGAN and DualGAN have the
same objective, they use different implementations for generators. CycleGAN uses the generator structure as proposed
in , while DualGAN follows the U-Net structure
as in . Both CycleGAN and DualGAN use the
PatchGAN with size 70 × 70 as in .
Besides different generator architectures, CycleGAN 
and DualGAN also use different techniques to stabilize the training process. DualGAN follows the training
procedure proposed in WGAN . CycleGAN applies two
techniques. First, instead of using the log loss for LGAN
in Equation 11 with a least square loss that in practice
performs more stably and produces higher quality images:
LLSGAN(GAB, DB) = Eb∼pB(b)[(DB(b) −1)2]
+ Ea∼pA(a)[(DB(GAB(a))2].
The second technique used in CycleGAN is that, in
order to reduce model oscillation , CycleGAN follows
SimGAN’s strategy and updates discriminators DA
and DB using a history of 50 previously generated images
instead of the ones produced by latest generators.
Experiments of CycleGAN demonstrate the potential of
performing high-quality image-to-image translation using
unpaired data only, even though supervised method like
Pix2Pix still outperforms CycleGAN by a noticeable
margin. CycleGAN also conducts experiments that show
the importance of using both circles in the circle-consistency
loss deﬁned in Equation 12. However, failure cases provided
in show that, just as Pix2Pix , CycleGAN does not
work in cases that necessitate geometric transformations,
such as apple ↔orange and cat ↔dog. More examples
are available on CycleGAN’s project website2.
2. 
Unsupervised Image-to-Image Translation with Distance Constraint
DistanceGAN discovers that, the distance ||xi −xj||
between two images in the source domain A is highly
positively correlated to the distance of their counterparts
||GAB(xi)−GAB(xj)|| in the target domain B, which can be
seen from Figure 1 of the paper . According to , let dk
be the distance ||xi−xj||, and d′
k be ||GAB(xi)−GAB(xj)||,
a high correlation indicates that P dkd′
k should also be high.
The pair-wise distances dk in source domain are ﬁxed, and
maximizing P dkd′
k causes dk with large value to dominate
the loss, which is undesirable. So the authors propose to
minimize P |dk −d′
k| instead.
DistanceGAN proposes to use a pair-wise distance
loss deﬁned as:
Ldist(GAB, pA) = Exi,xj∼pA| 1
(||xi −xj||1 −µA)
(||GAB(xi) −GAB(xj)||1 −µB)|,
where µA, µB (σA, σB) are the pre-computed means (standard deviations) of pair-wise distances in training sets of
domain A, B respectively.
In order to support stochastic gradient descent where
only one data sample is fed into the model at a time,
DistanceGAN proposes another self-distance constraint:
Lself-dist(GAB, pA) = Ex∼pA| 1
(||L(x) −R(x)||1 −µA)
(||GAB(L(x)) −GAB(R(x))||1 −µB)|,
where L(x) and R(x) indicate the left and right half of the
image x, and only the left (right) parts of images are taken
into account when calculating µA, σA (µB, σB).
Thus the overall loss of DistanceGAN is given by:
L = α1ALGAN(GAB, DB) + α1BLGAN(GBA, DA)
+ α2ALdist(GAB, pA) + α2BLdist(GBA, pB)
+ α3ALself-dist(GAB, pA) + α3BLself-dist(GBA, pB)
+ α4Lcyc(GAB, GBA),
where Lcyc(GAB, GBA) is deﬁned in Equation 12 and
LGAN(·) is deﬁned in Equation 11.
DistanceGAN conducts extensive experiments using different losses: Lcyc alone (DiscoGAN and CycleGAN
 ), one-sided distance loss Ldist (A →B →A or
B →A →B), the combination of Lcyc and one-sided
Ldist, and one-sided self-distance loss Lself-dist alone. The
implementation of DistanceGAN is the same as baseline
(DiscoGAN or CycleGAN ), dependent on which
one is to be compared with. Results show that the one-sided
distance loss or self-distance loss outperforms DiscoGAN
and CycleGAN in several tasks, and that the combination of
cyclic loss and distance loss achieves the best results in some
cases. However, the paper does not explore other possible
combinations, such as using both distance losses or even the
full loss function deﬁned in Equation 18.
One interesting thing is, as stated in , that Distance-
GAN computes the distances in raw RGB space and still
achieves better performance than baselines, but it may help
if the distances are calculated in images’ latent feature space
where the features can be extracted using pre-trained image
classiﬁers.
In DistanceGAN , the authors argue that the high
positive correlation between dk and d′
k implies that P dkd′
should be high. However, it is unclear how high d′
be. For example, if dk = 1 and we know that d′
be high, we still cannot deﬁnitely say that d′
more desirable than d′
k = 2. The concept of “high” is
blurry, so it may be better to use the concept “higher”. An
alternative statement could be, let x, y, z be images from
domain A, if d(x, y) > d(x, z), then d(GAB(x), GAB(y)) >
d(GAB(x), GAB(z)). And thus we can design a loss like
max(δ, d(GAB(x), GAB(y)) −d(GAB(x), GAB(z))) for all
triplets x, y, z ∈A such that d(x, y) > d(x, z).
Regardless of the objective of maximizing P dkd′
actual loss used in DistanceGAN, i.e. P |dk −d′
k|, is forcing
k to be as close to dk as possible. In other words, how
two images of a domain differ from each other should
be reﬂected in the same way when they are translated
into another domain, which can be called “equivariance”.
The distance loss and self-distance loss proposed in DistanceGAN is essentially capturing this “equivariance”
Unsupervised
Image-to-Image
Translation
Feature Constancy
As we mention previously, besides minimizing the reconstruction error at raw pixel level, we can also do this at
higher feature level, which is explored in DTN . The
architecture of DTN is shown in Figure 9, where the generator G is composed of two neural networks, a convolutional
network f and an transposed convolutional network g such
that G = g ◦f.
f(g(f(x)))
Fig. 9. Architecture of domain transfer network (DTN). As an example
here, the model transfers images from BW color space to RGB color
space. The generator is expected to be an identity matrix to images in
the target domain, so that in this example, an RGB image remains unchanged when put into the generator, while a BW image is transformed
into RGB by the generator.
Here f acts as a feature extractor, and DTN tries to
preserve high level features of an input image from source
domain after it is transferred into target domain. Let Xs
denote source domain and Xt be target domain. Given an
input image x ∈Xs, the output of generator is G(x) =
g(f(x)), then the feature reconstruction error can be deﬁned
with a distance measure d (DTN uses mean squared error
d(f(x, f(g(f(x))))).
Besides, DTN also expects the generator G to act as an
identity matrix to images from the target domain. Given a
distance measure d2 (DTN uses mean squared error (MSE)),
an identity mapping loss for target domain is deﬁned as:
d2(x, g(f(x))).
In addition, DTN use a multi-class discriminator instead
of a binary one in regular GAN. Here D is a ternary
classiﬁcation function that maps an image to one of the three
classes {1,2,3}, where class 1 means that the image is from
source domain but transformed by G, class 2 means that the
input is an image from target domain but transformed by
G, and class 3 means that the input image is from target
domain without any transformation. Let Di(x) denotes the
probability of x belongs to class i, the discriminator loss LD
is deﬁned as:
LD = −Ex∈Xs log D1(g(f(x)))
−Ex∈Xt log D2(g(f(x))) −Ex∈Xt log D3(x).
Similarly, the generator’s adversarial loss LGANG is deﬁned
LGANG = −Ex∈Xs log D3(g(f(x)))
−Ex∈Xt log D3(g(f(x))).
In order to slightly smoothen the generated images, DTN
adds an anisotropic total variation loss LT V deﬁned on
generated images z = [zij] = G(x):
LT V (z) =
((zi,j+1 −z2
ij + (zi+1,j −zij)2)
Then the overall generator loss LG is deﬁned as:
LG = LGANG + αLCONST + βLT ID + γLT V .
Experiments show that DTN produces impressive
images on face-to-emoji task, which is competitive with
some existing emoji generating programs.
Unsupervised
Image-to-Image
Translation
Auxiliary Classiﬁer
Bousmalis et. al propose to use a task-speciﬁc auxiliary
classiﬁer to help unsupervised image-to-image translation. We
denote their model as DAAC (short for “Domain Adaption
with Auxiliary Classiﬁer”) here for convenience. DAAC 
contains a task-speciﬁc classiﬁer C(x) →y that assigns a
label vector y to an image in either source domain or target
domain. This classiﬁer C, for example, can be an image
classiﬁcation model. The architecture of DAAC is similar
to Figure 2.
Given a training set Xs in which each image x ∈Xs
has a class label yx, the objective of C is to minimize the
cross-entropy loss LC:
Lc(G, C) = Ex∈Xs[−y⊺
x log[C(G(x))] −y⊺
x log[C(x)]].
Besides, DAAC also proposes a content-similarity
loss in cases with prior knowledge on what information
should be preserved after the domain adaption process.
For example, we may expect the hues of the source image
and the adapted image to be the same. In , the authors
consider the case where they render objects with black
background and expect the adapted images to have the
same objects but different backgrouds. In this case, each
image x is associated with a binary mask mx ∈Rk (k
is the number of pixels in x) to separate foreground and
background, and the content-similarity loss Ls, which is a
variation of the pairwise mean squared error (PMSE) ,
can be deﬁned as:
Ls(G) = Ex∈Xs[1
k ||(x −G(x)) ◦mx||2
k2 ((x −G(x))⊺mx)2],
where ||·||2
2 is the squared L2 norm, and ◦is the Hardamard
product. Note that here the image x is ﬂattened into a vector
The total objective is thus given by:
D αLGAN(G, D) + βLc(G, C) + γLs(G),
where LGAN(G, D) is the regular GAN objective deﬁned
in Equation 1, and α, β, γ are hyper-parameters to balance
different terms of the objective.
Although Lc(G, C) and Ls(G) help in generating better
adapted images, the amount of labeled images is limited
in real world, and ﬁne-grained pixel-level masks are even
harder to obtain. Nonetheless, using auxiliary classiﬁer can
also help to avoid mode collapse, as in AC-GAN .
zA ⇠EA(xA)
zB ⇠EB(xB)
Shared latent space
Fig. 10. Architecture of UNIT. The two encoders share weights at the
last few layers, while the two generators share weights at the ﬁrst few
layers, as indicated by the dashed lines.
Unsupervised
Image-to-Image
Translation
VAE and Weight Sharing
UNIT proposes to add VAE to CoGAN for unsupervised image-to-image translation, as illustrated in Figure
10. In addition, UNIT assumes that both encoders share the
same latent space, which means, let xA, xB be the same
image in different domains, and then the shared latent space
implies that EA(xA) = EB(BB). Based on the shared-latent
space assumption, UNIT enforces weight sharing between
the last few layers of the encoders and between the ﬁrst few
layers of the generators. The objective function for UNIT
is a combination of the objectives of GAN and VAE, with
the difference of using two sets of GANs/VAEs and adding
hyper-parameters λs to balance different loss terms. Also,
UNIT states that the shared latent space assumption implies
the cycle-consistency , so it adds another constraint to its objective which is VAE-like:
Lcc1 =λ3DKL(qA(zA|xA)||pη(z))
+ λ3DKL(qB(zB|FAB(xA))||pη(z))
−λ4EzB∼qB(zB|FAB(xA))[log pGA(xA|zB)],
where qA(zA|xA)
N(zA|EA(xA), I), qB(zA|xB)
N(zB|EA(xB), I), pη(z)
N(z|0, I), and FAB(x)
GB(EA(x)). And Lcc2 can be deﬁned similarly by reversing
subscripts of A and B.
Although UNIT performs better than models like
DTN and CoGAN on MNIST , Street View
House Number (SVHN) datasets in terms of crossdomain classiﬁcation accuracy, it does not compare with
other unsupervised methods such as CycleGAN and
DiscoGAN , nor does it use other widely adopted evaluation metrics like Inception Score.
Unsupervised Multi-domain Image-to-Image Translation
Previous models can only transform images between two
domains, but if we want to transform an image among
several domains, we need to train a separate generator for
each pair of domains, which is costly. To deal with this
problem, StarGAN proposes to use one generator that
can generate images of all domains. Instead of taking only
an image as conditional input, StarGAN also takes the label
of target domain as input, and the generator is designed to
transform the input image to the target domain indicated
by the input label. Similar to DAAC and AC-GAN ,
StarGAN uses an auxiliary domain classiﬁer which classiﬁes an image into its belonged domain. In addition, a
cycle-consistency loss is used to preserve the content
similarity between input and output images. In order to
allow StarGAN to train on multiple datasets that may have
different sets of labels, StarGAN uses an additional onehot vector to indicate the datset and concatenates all label
vectors into one vector, setting the unspeciﬁed labels as zero
for each dataset.
Summary on General Image-to-Image Translation
So far we have discussed some general image-to-image translation methods, the different losses they use are summarized
in Table 1.
The simplest loss is the pixel-wise L1 reconstruction loss
operates in the target domain, which requires paired training samples. Both one-sided and bi-directional reconstruction
loss can treated as the unsupervised version of pixel-wise
L1 reconstruction loss, since they enforce cycle-consistency
and do not require paired training samples. The additional
VAE loss is based on the assumption of shared latent space
of both source and target domain, and it also implies the
bi-directional cycle-consistency loss. The equivariance loss,
however, does not try to reconstruct images, but to preserve
the difference between images across source and target
Different from previously mentioned losses that work
on the generators directly, pair-wise discriminator loss, ternary
discriminator loss and auxiliary classiﬁer loss work on the
discriminator side and make the discriminator better at
distinguishing real and fake samples, and then the generator
can also learn better with the enhanced discriminator.
Among all mentioned models, Pix2Pix produces
sharpest images, even though the L1 loss is just a simple
add-on component to the original GAN model. It may
be interesting to combine L1 loss with the pair-wise discriminator in PLDT which may improve the model’s
performance on image-to-image translations that involve
geometric changes on images. Also, Pix2Pix may beneﬁt
from preserving similarity information between images in
source and target domains, as done in some unsupervised
methods like CycleGAN and DistanceGAN . As
for unsupervised methods, although their results are not
as sharp as supervised methods like Pix2Pix , they are
a promising research direction, since they do not require
paired data and collecting labeled data is very costly in real
Summary of losses used in image image-to-image translation, “sup” is
short for “supervision”. “√” in both “source domain” and “target domain”
columns indicates that the loss requires images from both domains (but
not necessarily paired).
Pixel-level L1
Pix2Pix 
discriminator
Bi-directional
reconstruction
CycleGAN ,
DualGAN ,
DiscoGAN ,
StarGAN 
reconstruction
discriminator
Equivariance
DistanceGAN 
DAAC ,
StarGAN 
Task-Speciﬁc Image-to-Image Translation
In this section, we are going to introduce some models
that work on more speciﬁc image-to-image translation applications.
Face Editing
Face editing is closely related to image-to-image translation,
since it also takes images as input and produces images.
However, Face editing focuses more on manipulating the
attributes of humans’ faces while image-to-image translation
is a more general scope.
IcGAN proposes to learn two separate encoders, Ez
that maps an image to its latent vector z and Ey that learns
the attribute information vector y. Attribute manipulation
is performed by tuning the attribute vector y, concatenating
it with z and then putting the combined vector as input to
the generator.
Instead of generating images directly, Shen et. al 
propose to learn residual images of attributes. To add an
attribute to an image x, a residual image G(x) is obtained by
putting it through a generator G, and then the manipulated
image is obtained by x+G(x). The framework of this model
follows the dual learning approach we discuss earlier, and the
discriminator is similar to the ternary classiﬁer in DTN ,
which distinguishes whether the image is from groundtruth or is manipulated by either of the two generators
(GAB, GBA).
Recently, Brock et. al propose Introspective Adversarial
Network (IAN) for neural photo editing, which, similar
to VAE-GAN , is also a combination of GAN and
VAE . However, unlike VAE-GAN that uses different
networks for discriminator and encoder, IAN combines
the discriminator and encoder into a single network. The
intuition behind it is that features learned by a well trained
discriminator tend to be more expressive than those learned
by maximum likelihood, which is the reason why they
are more suitable for inference. Speciﬁcally, the encoder is
implemented as a fully-connected layer on top of the last
convolution layer of the discriminator.
Besides manipulating face attributes, there are also other
forms of “face editing” such as generating the frontal view
of a person’s face based on pictures of the face’s side view.
Huang et. al propose Two-Pathway Generative Adversarial
Network (TP-GAN) that performs such task. TP-GAN
consists of two pathways, a global structure pathway and
a local texture pathway, where each pathway contains a
pair of encoder and decoder. The global pathway constructs
a blurry frontal view that captures the global structure of
the face, while the local pathway with four sub-networks
attends to local texture details around four facial landmarks,
which are left eye center, right eye center, nose tip and mouth
center. Outputs of four sub-networks in the local pathway
are concatenated with the output of global pathway, and
then the combined tensor is fed into successive convolution
layers to produce the ﬁnal synthetic image. In addition to
L1 pixel loss, TP-GAN proposes to use a symmetry loss
to constrain the symmetry of human faces, and an identity
preserving loss to preserve the identity of input face. The
identity loss is implemented as a perceptual loss .
Image Super-Resolution
Another application of GAN that is related to image-to-image
translation is image super-resolution, which is the task of
taking a low resolution image as input and outputs a high
resolution one with sharp details. SRGAN proposes
to use a residual block based generator and a fully
convolutional discriminator to do single image superresolution. Besides adversarial loss, SRGAN also combines
pixel-wise MSE loss, perceptual loss and regularization
loss . SRGAN outperforms several baselines on some
metrics by a small margin, but the difference in synthetic
images is not easy to tell without zooming in.
Video Prediction
A special kind of related application is video prediction, which
aims to predict the next frame of a video given the current
frame (or a history of frames).
VGAN proposes to use a hierarchical model for
video prediction. The generator has two data streams, a foreground stream f consisting of 3D transposed convolutions,
and a background stream b consisting of 2D transposed
convolutions. The foreground stream is also responsible for
generating a mask m that is used to merged the results of
two streams: m ⊙f + (1 −m) ⊙b. The discriminator is a set
of spatial-temporal convolution layers.
Mathieu et. al propose Adv-GDL which generates
future frames in an iterative way, from low to high resolution. Let s1, s2, ..., sk be a set of sizes and uk be the
upsampling operator from size sk−1 to sk. Let Xk, Yk
be ground-truth current and future frames of size sk and
Gk be the generator that produces images of size sk,then
the predicted future frame ˆYk is calculated by ˆYk
uk( ˆYk−1)+Gk(Xk, uk( ˆYk−1)). The discriminator is a series
of multi-scale convolutional networks with scalar output.
Although both methods produce reasonable output
videos when the time interval is short, video quality becomes worse as the time increases. Objects in synthetic
videos lose their original shapes and get morphed to indistinguishable objects in some cases, which may be due to
the models’ inability to learn legal movements of objects.
EVALUATION METRICS ON SYNTHETIC IMAGES
It is very hard to quantify the quality of synthetic images,
and metrics like RMSE are not suitable since there is no
absolute one-to-one correspondence between synthetic and
real images. A commonly used subjective metric is to use
the Amazon Mechanical Turk (AMT)3 that hires humans to
score synthetic and real images according to how realistic
they think the images are. However, people often have
different opinions of what is good or bad, so we also need
objective metrics to evaluate the quality of images.
Inception score (IS) evaluates an image based on the
entropy in class probability distribution when it is put
into a pre-trained image classiﬁer. One intuition behind
Inception score is that the better an image x is, the lower
the entropy of conditional distribution p(y|x) should be,
which means the classiﬁer have high conﬁdence of what
the image is about. Also, to encourage the model to generate various classes of images, the marginal distribution
R p(y|x = G(z))dz should have high entropy. Combining these two intuition, the Inception score is calculated
by exp(Ex∼G(z)DKL(p(y|x)||p(y)). As discussed in ,
Inception score is neither sensitive to prior distribution of
labels, nor a proper distance measure. Also, Inception score
suffers from intra-class mode collapse, since a model only
needs to generate one perfect sample for each class to get
a perfect Inception score.
Similar to Inception score, FCN-score adopts the idea
that if the synthetic images are realistic, classiﬁers trained
on real images will be able to classify the synthetic images
correctly. However, an image classiﬁer does not require the
input image to be very sharp as to give a correct classiﬁcation, which means that metrics based on image classiﬁer
may not be able to tell between two images with only small
difference in details. Worse still, research in adversarial
examples shows that the decision of a classiﬁer does
not necessarily depend on visual content of images but can
be highly inﬂuenced by noise invisible to humans, which
raises more questions on this metric.
Fr´echet Inception Distance (FID) provides a different
approach. First, generated images are embedded into a latent feature space of a chosen layer of the Inception Net. Second, embeddings of generated and real images are treated
as samples from two continuous multivariate Gaussians so
that their means and covariances can be calculated. Then
the quality of generated images can be determined by the
Fr´echet Distance between the two Gaussians:
FID(x, g) = ||µx −µg||2
2 + Tr(Σx + Σg −2(ΣxΣg)
where (µx, µg) and (Σx, Σg) are the means and covariances of the samples from the true data distribution and
generator’s learned distribution respectively. The authors of
 show that FID is consistent with human judgment and
that there is a strong negative correlation between FID and
the quality of generated images. Furthermore, FID is less
sensitive to noise than IS and can detect intra-class mode
Besides Inception score (IS), FCN-score and Fr´echet Inception Distance (FID), there are also other metrics like Gaussian
Parzen Window , Generative Adversarial Metric (GAM) 
3. 
and MODE Score . Among all these metrics, Inception score
is the most widely adopted one for quantitatively evaluating
synthetic images, and there is a recent study that uses IS
to compare several GAN models. Although FID is relatively
new, it has been shown to be better than IS .
DISCRIMINATORS
AS LEARNED LOSS FUNC-
Generative adversarial network (GAN) is powerful and
effective in that the discriminator acts as a learned loss
function instead of a ﬁxed one designed carefully for each
speciﬁc task. This is particularly important for image synthesis tasks whose loss functions are hard to be explicitly
deﬁned in math. For example, in style transfer task, it is hard
to write down a math equation that evaluates how well an
image matches a certain painting style. For image synthesis
tasks, each input may have many legal outputs, but samples
in training set cannot cover all situations. In this case, it is inappropriate to only minimize the distance between synthetic
and ground-truth images, since we want the generator to
learn the data distribution instead of remembering training
samples. Although we can design feature-based losses that
try to preserve feature consistency instead of at raw pixel
level, as done in the perceptual loss for image style, such
losses are constrained by pre-trained image classiﬁcation
models they use, and it remains a question of which layers
to pick for calculating feature loss when we switch to
another pre-trained model. A discriminator, on the other
hand, does not require explicit deﬁnition of the loss, since
it learns how to evaluate a data sample as it trains against
the generator. Thus the discriminator is able to learn a better
loss function given enough training data.
The fact that the discriminator acts as a learned loss
function has signiﬁcant meaning for general artiﬁcial intelligence. Traditional pattern recognition and machine learning
require us to deﬁne what features to be used (e.g. SIFT 
and HOG descriptors), and we design speciﬁc loss
functions and decide what optimization methods to be
applied. Deep learning free us from carefully designing
features, by learning low-level and high-level feature representations by itself during training (e.g. CNN kernels), but
we still need to work hard at designing loss functions that
work well. GAN takes us one step forward on our path towards artiﬁcial intelligence, in that it learns how to evaluate
data samples instead of being told how to do so, although
we still need to design the adversarial loss and combine
it with other auxiliary losses. In other words, previously
we design how to calculate how close an output is to the
corresponding ground-truth (L(x, ˆx)), but the discriminator
learns how to calculate how well an output matches the true
data distribution (L(x)). Such property allows models to be
more ﬂexible and more likely to generalize well. Furthermore, with learn2learn which allows neural networks
to learn to optimize themselves, there is a possibility that
we may no longer need to choose what optimizers (such
as RMSprop , Adam etc.) to use and let models
handle everything themselves.
DISCUSSION AND CONCLUSION
In this paper, we review some basics of Generative Adversarial Nets (GAN) , and classify image synthesis methods
into three main approaches, i.e. direct method, hierarchical
method and iterative method, and mention some other generation methods such as iterative sampling . We also
discuss in details two main forms of image synthesis, i.e.
text-to-image synthesis and image-to-image translation.
For text-to-image synthesis, current methods work well
on datasets where each image contains single object such
as CUB and Oxford-102 , but the performance on
complex datasets such as MSCOCO is much worse.
Although some models can produce realistic images of
rooms in LSUN , it should be noted that rooms do
not contain living things, and a living thing is certainly
much more complicated than static objects. This limitation
probably stems from the models’ inability to learn different
concepts of objects. We also propose that one possible way to
improve GAN’s performance in this task is to train different
models that generate single object well and train another
model that learns to combine different objects according to
text descriptions, and that CapsNet may be useful in
such tasks.
For image-to-image translation, we review some general methods from supervised to unsupervised settings,
such as pixel-wise loss , cyclic loss and selfdistance loss . Besides, we also introduce some taskspeciﬁc image-to-image translation models for face editing, video prediction and image super-resolution. Imageto-image translation is certainly an interesting application
of GAN, which has great potential to be incorporated into
other software products, especially mobile apps. Although
research in unsupervised methods seems more popular,
supervised methods may be more practical since they still
produce better synthetic images than unsupervised methods.
Finally, we review some evaluation metrics for synthetic
images, and discuss GAN’s role on our path towards artiﬁcial intelligence. The power of GAN largely lies in its
discriminator’s acting as a learned loss function, which
makes the model perform better on tasks whose output is
hard to evaluate by designing an explicit math equation.
ACKNOWLEDGMENTS
He Huang would like to thank Chenwei Zhang and Bokai
Cao for the valuable discussions and feedback.