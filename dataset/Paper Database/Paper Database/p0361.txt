DetectGPT: Zero-Shot Machine-Generated Text Detection
using Probability Curvature
Eric Mitchell 1 Yoonho Lee 1 Alexander Khazatsky 1 Christopher D. Manning 1 Chelsea Finn 1
The increasing fluency and widespread usage of
large language models (LLMs) highlight the desirability of corresponding tools aiding detection
of LLM-generated text. In this paper, we identify
a property of the structure of an LLMâ€™s probability function that is useful for such detection.
Specifically, we demonstrate that text sampled
from an LLM tends to occupy negative curvature regions of the modelâ€™s log probability function. Leveraging this observation, we then define
a new curvature-based criterion for judging if a
passage is generated from a given LLM. This
approach, which we call DetectGPT, does not require training a separate classifier, collecting a
dataset of real or generated passages, or explicitly watermarking generated text. It uses only
log probabilities computed by the model of interest and random perturbations of the passage
from another generic pre-trained language model
(e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model
sample detection, notably improving detection of
fake news articles generated by 20B parameter
GPT-NeoX from 0.81 AUROC for the strongest
zero-shot baseline to 0.95 AUROC for Detect-
GPT. See ericmitchell.ai/detectgpt
for code, data, and other project information.
1. Introduction
Large language models (LLMs) have proven able to generate remarkably fluent responses to a wide variety of user
queries. Models such as GPT-3 , PaLM
 , and ChatGPT 
can convincingly answer complex questions about science,
mathematics, historical and current events, and social trends.
1Stanford University.
Correspondence to: Eric Mitchell
< >.
Proceedings of the 40 th International Conference on Machine
Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright
2023 by the author(s).
Candidate passage :
â€œJoe Biden recently made a move to the White House
that included bringing along his pet German Shepherdâ€¦â€
(1) Perturb
(3) Compare
ðŸ¤– from GPT-3
â€œmade a moveâ€
Delete â€œbringing alongâ€
ðŸ¤” from other source
Figure 1. We aim to determine whether a piece of text was generated by a particular LLM p, such as GPT-3. To classify a candidate
passage x, DetectGPT first generates minor perturbations of the
passage Ëœxi using a generic pre-trained model such as T5. Then
DetectGPT compares the log probability under p of the original
sample x with each perturbed sample Ëœxi. If the average log ratio
is high, the sample is likely from the source model.
While recent work has found that cogent-sounding LLMgenerated responses are often simply wrong , the articulate nature of such generated text may still
make LLMs attractive for replacing human labor in some
contexts, notably student essay writing and journalism. At
least one major news source has released AI-written content
with limited human review, leading to substantial factual errors in some articles . Such applications of
LLMs are problematic for a variety of reasons, making fair
student assessment difficult, impairing student learning, and
proliferating convincing-but-inaccurate news articles. Unfortunately, humans perform only slightly better than chance
when classifying machine-generated vs human-written text
 , leading researchers to consider
automated detection methods that may identify signals difficult for humans to recognize. Such methods might give
teachers and news-readers more confidence in the human
origin of the text that they consume.
As in prior work , we study the
 
Zero-Shot Machine-Generated Text Detection using Probability Curvature
machine-generated text detection problem as a binary classification problem. Specifically, we aim to classify whether
a candidate passage was generated by a particular source
model. While several works have investigated methods for
training a second deep network to detect machine-generated
text, such an approach has several shortcomings, including
a tendency to overfit to the topics it was trained on as well as
the need to train a new model for each new source model that
is released. We therefore consider the zero-shot version of
machine-generated text detection, where we use the source
model itself, without fine-tuning or adaptation of any kind,
to detect its own samples. The most common method for
zero-shot machine-generated text detection is evaluating the
average per-token log probability of the generated text and
thresholding . However, this zeroth-order approach
to detection ignores the local structure of the learned probability function around a candidate passage, which we find
contains useful information about the source of a passage.
This paper poses a simple hypothesis: minor rewrites of
model-generated text tend to have lower log probability under the model than the original sample, while minor rewrites
of human-written text may have higher or lower log probability than the original sample. In other words, unlike
human-written text, model-generated text tends to lie in
areas where the log probability function has negative curvature (for example, near local maxima of the log probability).
We empirically verify this hypothesis, and find that it holds
true across a diverse body of LLMs, even when the minor
rewrites, or perturbations, come from alternative language
models. We leverage this observation to build DetectGPT,
a zero-shot method for automated machine-generated text
detection. To test if a passage came from a source model pÎ¸,
DetectGPT compares the log probability of the candidate
passage under pÎ¸ with the average log probability of several
perturbations of the passage under pÎ¸ ). If the perturbed passages tend
to have lower average log probability than the original by
some margin, the candidate passage is likely to have come
from pÎ¸. See Figure 1 for an overview of the problem and
DetectGPT. See Figure 2 for an illustration of the underlying hypothesis and Figure 3 for empirical evaluation of
the hypothesis. Our experiments find that DetectGPT is
more accurate than existing zero-shot methods for detecting machine-generated text, improving over the strongest
zero-shot baseline by over 0.1 AUROC for multiple source
models when detecting machine-generated news articles.
Contributions. Our main contributions are: (a) the identification and empirical validation of the hypothesis that the
curvature of a modelâ€™s log probability function tends to be
significantly more negative at model samples than for human text, and (b) DetectGPT, a practical algorithm inspired
by this hypothesis that approximates the trace of the log
xfake âˆ¼pÎ¸ (x)
xreal âˆ¼phuman(x)
Fake/real sample
Perturbed fake/real sample
Log likelihood
Figure 2. We identify and exploit the tendency of machinegenerated passages x âˆ¼pÎ¸(Â·) (left) to lie in negative curvature
regions of log p(x), where nearby samples have lower model
log probability on average.
In contrast, human-written text
x âˆ¼preal(Â·) (right) tends not to occupy regions with clear negative log probability curvature; nearby samples may have higher or
lower log probability.
probability functionâ€™s Hessian to detect a modelâ€™s samples.
2. Related Work
Increasingly large LLMs have led to dramatically improved performance on
many language-related benchmarks and the ability to generate convincing and on-topic text.
GROVER was the first LLM trained specifically for generating plausible news articles. Human evaluators found
GROVER-generated propaganda at least as trustworthy as
human-written propaganda, motivating the authors to study
GROVERâ€™s ability to detect its own generations by finetuning a detector on top of its features; they found GROVER
better able to detect GROVER-generated text than other pretrained models. However, Bakhtin et al. ; Uchendu
et al. note that models trained explicitly to detect
machine-generated text tend to overfit to their training distribution of data or source models.
Other works have trained supervised models for machinegenerated text detection on top of neural representations
 , bag-of-words
features , and handcrafted statistical features . Alternatively, Solaiman et al. notes the surprising efficacy
of a simple zero-shot method for machine-generated text
detection, which thresholds a candidate passage based on its
average log probability under the generative model, serving
as a strong baseline for zero-shot machine-generated text
detection in our work. In our work, we similarly use the
generating model to detect its own generations in a zero shot
manner, but through a different approach based on estimating local curvature of the log probability around the sample
rather than the raw log probability of the sample itself. See
Jawahar et al. for a complete survey on machine-
Zero-Shot Machine-Generated Text Detection using Probability Curvature
generated text detection. Other work explores watermarks
for generated text , which modify
a modelâ€™s generations to make them easier to detect. Our
work does not assume text is generated with the goal of easy
detection; DetectGPT detects text generated from publicly
available LLMs using standard LLM sampling strategies.
The widespread use of LLMs has led to much other contemporaneous work on detecting LLM output. Sadasivan
et al. show that the detection AUROC of the an detector is upper bounded by a function of the TV distance
between the model and human text. However, we find that
AUROC of DetectGPT is high even for the largest publiclyavailable models (Table 2), suggesting that TV distance may
not correlate strongly with model scale and capability. This
disconnect may be exacerbated by new training objectives
other than maximum likelihood, e.g., reinforcement learning with human feedback . Both Sadasivan et al. and Krishna et al.
 show the effectiveness of paraphrasing as a tool for
evading detection, suggesting an important area of study
for future work. Liang et al. show that multi-lingual
detection is difficult, with non-DetectGPT detectors showing bias against non-native speakers; this result highlights
the advantage of zero-shot detectors like DetectGPT, which
generalize well to any data generated by the original generating model. Mireshghallah et al. study which proxy
scoring models produce the most useful log probabilities
for detection when the generating model is not known (a
large-scale version of our Figure 6). Surprisingly (but consistent with our findings), they find that smaller models are
in fact better proxy models for performing detection with
perturbation-based methods like DetectGPT.
The problem of machine-generated text detection echoes earlier work on detecting deepfakes, artificial images or videos
generated by deep nets, which has spawned substantial efforts in detection of fake visual content . While early works in deepfake detection used relatively general-purpose model architectures
 , many deepfake detection methods
rely on the continuous nature of image data to achieve stateof-the-art performance , making direct application to text difficult.
3. The Zero-Shot Machine-Generated Text
Detection Problem
We study zero-shot machine-generated text detection, the
problem of detecting whether a piece of text, or candidate
passage x, is a sample from a source model pÎ¸. The problem
is zero-shot in the sense that we do not assume access to
human-written or generated samples to perform detection.
As in prior work, we study a â€˜white boxâ€™ setting in which the detector may evaluate the log prob-
Algorithm 1 DetectGPT model-generated text detection
1: Input: passage x, source model pÎ¸, perturbation function q,
number of perturbations k, decision threshold Ïµ
2: Ëœxi âˆ¼q(Â· | x), i âˆˆ[1..k] // mask spans, sample replacements
i log pÎ¸(Ëœxi)
// approximate expectation in Eq. 1
4: Ë†dx â†log pÎ¸(x) âˆ’ËœÂµ
// estimate d (x, pÎ¸, q)
i (log pÎ¸(Ëœxi) âˆ’ËœÂµ)2 // variance for normalization
âˆšËœÏƒx > Ïµ then
return true
// probably model sample
return false
// probably not model sample
ability of a sample log pÎ¸(x). The white box setting does
not assume access to the model architecture or parameters.
Most public APIs for LLMs (such as GPT-3) enable scoring
text, though some exceptions exist, notably ChatGPT. While
most of our experiments consider the white box setting, see
Section 5.2 for experiments in which we score text using
models other than the source model. See Mireshghallah
et al. for a comprehensive evaluation in this setting.
The detection criterion we propose, DetectGPT, also makes
use of generic pre-trained mask-filling models in order to
generate passages that are â€˜nearbyâ€™ the candidate passage.
However, these mask-filling models are used off-the-shelf,
without any fine-tuning or adaptation to the target domain.
4. DetectGPT: Zero-shot Machine-Generated
Text Detection with Random Perturbations
DetectGPT is based on the hypothesis that samples from a
source model pÎ¸ typically lie in areas of negative curvature
of the log probability function of pÎ¸, unlike human text. In
other words, if we apply small perturbations to a passage
x âˆ¼pÎ¸, producing Ëœx, the quantity log pÎ¸(x) âˆ’log pÎ¸(Ëœx)
should be relatively large on average for machine-generated
samples compared to human-written text. To leverage this
hypothesis, first consider a perturbation function q(Â· | x)
that gives a distribution over Ëœx, slightly modified versions of
x with similar meaning (we will generally consider roughly
paragraph-length texts x). As an example, q(Â· | x) might be
the result of simply asking a human to rewrite one of the
sentences of x, while preserving the meaning of x. Using
the notion of a perturbation function, we can define the
perturbation discrepancy d (x, pÎ¸, q):
d (x, pÎ¸, q) â‰œlog pÎ¸(x) âˆ’EËœxâˆ¼q(Â·|x) log pÎ¸(Ëœx)
We state our hypothesis more formally as the Local Perturbation Discrepancy Gap Hypothesis, which describes a gap
in the perturbation discrepancy for model-generated text
and human-generated text.
Perturbation Discrepancy Gap Hypothesis. If q produces
samples on the data manifold, d (x, pÎ¸, q) is positive and
large with high probability for samples x âˆ¼pÎ¸. For humanwritten text, d (x, pÎ¸, q) tends toward zero for all x.
Zero-Shot Machine-Generated Text Detection using Probability Curvature
EleutherAI/gpt-neo-2.7B
EleutherAI/gpt-j-6B
EleutherAI/gpt-neox-20b
Log Probability Change (Perturbation Discrepancy)
Figure 3. The average drop in log probability (perturbation discrepancy) after rephrasing a passage is consistently higher for modelgenerated passages than for human-written passages. Each plot
shows the distribution of the perturbation discrepancy d (x, pÎ¸, q)
for human-written news articles and machine-generated articles of equal word length. Human-written articles are a sample
of 500 XSum articles; machine-generated text, generated from
models GPT-2 (1.5B), GPT-Neo-2.7B , GPT-J
 ) and GPT-NeoX ), is generated by prompting each model with the first
30 tokens of each XSum article, sampling from the raw conditional
distribution. Discrepancies are estimated with 100 T5-3B samples.
If we define q(Â· | x) to be samples from a mask-filling model
such as T5 , rather than human rewrites,
we can empirically test the Perturbation Discrepancy Gap
Hypothesis in an automated, scalable manner. For real data,
we use 500 news articles from the XSum dataset ; for model samples, we use the output of four
different LLMs when prompted with the first 30 tokens of
each article in XSum. We use T5-3B to apply perturbations,
masking out randomly-sampled 2-word spans until 15% of
the words in the article are masked. We approximate the
expectation in Eq. 1 with 100 samples from T5.1 Figure 3
shows the result of this experiment. We find the distribution
of perturbation discrepancies is significantly different for
human-written articles and model samples; model samples
tend to have a larger perturbation discrepancy. Section 5.3
explores a relaxation of the assumption that q only produces
samples on the data manifold, finding that a gap, although
reduced, still exists in this case.
Given these results, we can detect if a piece of text was
generated by a model pÎ¸ by simply thresholding the perturbation discrepancy. In practice, we find that normalizing the
perturbation discrepancy by the standard deviation of the observed values used to estimate EËœxâˆ¼q(Â·|x) log pÎ¸(Ëœx) provides
a slightly better signal for detection, typically increasing
1We later show in Figure 8 that varying the number of samples
used to estimate the expectation effectively allows for trading off
between accuracy and speed.
AUROC by around 0.020, so we use this normalized version
of the perturbation discrepancy in our experiments. The
resulting method, DetectGPT, is summarized in Alg. 1. Having described an application of the perturbation discrepancy
to machine-generated text detection, we next provide an
interpretation of this quantity.
Interpretation of perturbation discrepancy as curvature
While Figure 3 suggests that the perturbation discrepancy
may be useful, it is not immediately obvious what it measures. In this section, we show that the perturbation discrepancy approximates a measure of the local curvature
of the log probability function near the candidate passage,
more specifically, that it is proportional to the negative trace
of the Hessian of the log probability function.2 To handle the non-differentiability of discrete data, we consider
candidate passages in a latent semantic space, where small
displacements correspond to valid edits that retain similar
meaning to the original. Because our perturbation function
(T5) models natural text, we expect our perturbations to
roughly capture such meaningful variations of the original
passage, rather than arbitrary edits.
We first invoke Hutchinsonâ€™s trace estimator , giving an unbiased estimate of the trace of matrix A:
tr(A) = EzzâŠ¤Az
provided that the elements of z âˆ¼qz are IID with E[zi] = 0
and Var(zi) = 1. To use Equation 2 to estimate the trace
of the Hessian of f at x, we must therefore compute the
expectation of the directional second derivative zâŠ¤Hf(x)z.
We approximate this expression with finite differences:
zâŠ¤Hf(x)z â‰ˆf(x + hz) + f(x âˆ’hz) âˆ’2f(x)
Combining Equations 2 and 3 and simplifying with h = 1,
we have an estimate of the negative Hessian trace
âˆ’tr (Hf(x)) â‰ˆ2f(x) âˆ’Ez [f(x + z) + f(x âˆ’z)] . (4)
If our noise distribution is symmetric, that is, p(z) = p(âˆ’z)
for all z, then we can simplify Equation 4 to
âˆ’tr (Hf(x))
â‰ˆf(x) âˆ’Ezf(x + z).
We note that the RHS of Equation 5 corresponds to the
perturbation discrepancy (1) where the perturbation function q(Ëœx | x) is replaced by the distribution qz(z) used
in Hutchinsonâ€™s trace estimator (2). Here, Ëœx is a highdimensional sequence of tokens while qz is a vector in a
2Rather than the Hessian of the log likelihood with respect to
model parameters (the Fisher Information Matrix), here we refer
to the Hessian of the log probability with respect to the sample x.
Zero-Shot Machine-Generated Text Detection using Probability Curvature
WritingPrompts
GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX
GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX
GPT-2 OPT-2.7 Neo-2.7 GPT-J NeoX
Table 1. AUROC for detecting samples from the given model on the given dataset for DetectGPT and four previously proposed criteria
(500 samples used for evaluation). From 1.5B parameter GPT-2 to 20B parameter GPT-NeoX, DetectGPT consistently provides the most
accurate detections. Bold shows the best AUROC within each column (model-dataset combination); asterisk (*) denotes the second-best
AUROC. Values in the final row show DetectGPTâ€™s AUROC over the strongest baseline method in that column.
compact semantic space. Since the mask-filling model samples sentences similar to x with minimal changes to semantic meaning, we can think of the mask-filling model as first
sampling a similar semantic embedding (Ëœz âˆ¼qz) and then
mapping this to a token sequence (Ëœz 7â†’Ëœx). Sampling in
semantic space ensures that all samples stay near the data
manifold, which is useful because we would expect the log
probability to always drop if we randomly perturb tokens.
We can therefore interpret our objective as approximating
the curvature restricted to the data manifold.
5. Experiments
We conduct experiments to better understand multiple facets
of machine-generated text detection; we study the effectiveness of DetectGPT for zero-shot machine-generated text detection compared to prior zero-shot approaches, the impact
of distribution shift on zero-shot and supervised detectors,
and detection accuracy for the largest publicly-available
models. To further characterize factors that impact detection accuracy, we also study the robustness of zero-shot
methods to machine-generated text that has been partially
revised, the impact of alternative decoding strategies on
detection accuracy, and a black-box variant of the detection task. Finally, we analyze more closely DetectGPTâ€™s
behavior as the choice of perturbation function, the number
of samples used to estimate d (x, pÎ¸, q), the length of the
passage, and the data distribution is varied.
Comparisons. We compare DetectGPT with various existing zero-shot methods for machine-generated text detection
that also leverage the predicted token-wise conditional distributions of the source model for detection. These methods
correspond to statistical tests based on token log probabilities, token ranks, or predictive entropy . The
first method uses the source modelâ€™s average token-wise log
probability to determine if a candidate passage is machinegenerated or not; passages with high average log probability
are likely to be generated by the model. The second and
third methods use the average observed rank or log-rank of
the tokens in the candidate passage according to the modelâ€™s
conditional distributions. Passages with smaller average
(log-)rank are likely machine-generated. We also evaluate an entropy-based approach inspired by the hypothesis
in Gehrmann et al. that model-generated texts will
be more â€˜in-distributionâ€™ for the model, leading to more
over-confident (thus lower entropy) predictive distributions.
Empirically, we find predictive entropy to be positively correlated with passage fake-ness more often that not; therefore, this baseline uses high average entropy in the modelâ€™s
predictive distribution as a signal that a passage is machinegenerated. While our main focus is on zero-shot detectors
as they do not require re-training for new domains or source
models, for completeness we perform comparisons to supervised detection models in Section 5.1, using OpenAIâ€™s
RoBERTa-based GPT-2 detector models,3
which are fine-tuned on millions of samples from various
GPT-2 model sizes and decoding strategies.
Datasets & metrics Our experiments use six datasets that
cover a variety of everyday domains and LLM use-cases.
We use news articles from the XSum dataset to represent fake news detection, Wikipedia paragraphs from SQuAD contexts to
represent machine-written academic essays, and prompted
stories from the Reddit WritingPrompts dataset to represent detecting machine-generated creative
writing submissions. To evaluate robustness to distribution
shift, we also use the English and German splits of WMT16
 as well as long-form answers written by
human experts in the PubMedQA dataset .
Each experiment uses between 150 and 500 examples for
evaluation, as noted in the text. For each experiment, we
generate the machine-generated text by prompting with the
first 30 tokens of the real text (or just the question tokens
for the PubMedQA experiments). We measure performance
using the area under the receiver operating characteristic
curve (AUROC), which can be interpreted as the probability
that a classifier correctly ranks a randomly-selected positive (machine-generated) example higher than a randomlyselected negative (human-written) example. All experiments
use an equal number of positive and negative examples.
3 
Zero-Shot Machine-Generated Text Detection using Probability Curvature
Likelihood
XSum GPT-2 Detection
Likelihood
WMT16-en mGPT Detection
Likelihood
PubMedQA PubMedGPT Detection
Supervised
Unsupervised
Likelihood
WMT16-de mGPT Detection
Detection Method
Detection AUROC
Figure 4. Supervised machine-generated text detection models
trained on large datasets of real and generated texts perform as
well as or better than DetectGPT on in-distribution (top row)
text. However, zero-shot methods work out-of-the-box for new
domains (bottom row) such as PubMed medical texts and German
news data from WMT16. For these domains, supervised detectors
fail due to excessive distribution shift.
Hyperparameters. The key hyperparameters of DetectGPT
are the fraction of words masked for perturbation, the length
of the masked spans, the model used for mask filling, and the
sampling hyperparameters for the mask-filling model. Using
BERT masked language modeling as
inspiration, we use 15% as the mask rate. We performed
a small sweep over masked span lengths of {2, 5, 10} on a
held-out set of XSum data, finding 2 to perform best. We
use these settings for all experiments, without re-tuning.
We use T5-3B for almost all experiments, except for GPT-
NeoX and GPT-3 experiments, where compute resources
allowed for the larger T5-11B model; we also use mT5-3B
instead of T5-3B for the WMT multilingual experiment. We
do not tune the hyperparameters for the mask filling model,
sampling directly with temperature 1.
5.1. Main Results
We first present two groups of experiments to evaluate DetectGPT along with existing methods for zero-shot and supervised detection on models from 1.5B to 175B parameters.
Zero-shot machine-generated text detection. We present
the comparison of different zero-shot detection methods in
Table 1. In these experiments, model samples are generated by sampling from the raw conditional distribution with
temperature 1. DetectGPT most improves average detection accuracy for XSum stories (0.1 AUROC improvement)
and SQuAD Wikipedia contexts (0.05 AUROC improvement). While it also performs accurate detection for Writing-
Prompts, the performance of all methods tends to increase,
0.64 / 0.58
0.92 / 0.74
0.92 / 0.81
0.71 / 0.64
0.92 / 0.88
0.91 / 0.88
0.64 / 0.55
0.76 / 0.61
0.88 / 0.67
0.84 / 0.77
0.84 / 0.84
0.87 / 0.84
Table 2. DetectGPT detects generations from GPT-3 and Jurassic-2
Jumbo (175B models from OpenAI and AI21 Labs) with average
AUROC on-par with supervised models trained specifically for
machine-generated text detection. For more â€˜typicalâ€™ text, such
as news articles, supervised methods perform strongly. The GPT-
3 AUROC appears first in each column, the Jurassic-2 AUROC
appears second (i.e., after the slash).
and the average margin of improvement is narrow.4 For 14
of the 15 combinations of dataset and model, DetectGPT
provides the most accurate detection performance, with a
0.06 AUROC improvement on average. Log-rank thresholding proves to be a consistently stronger baseline than log
probability thresholding, although it requires slightly more
information (full predicted logits), which are not always
available in public APIs.
Comparison with supervised detectors. While our experiments generally focus on zero-shot detection, some works
have evaluated the detection performance of supervised
methods (typically fine-tuned transformers) for detecting
machine-generated text. In this section, we explore several
domains to better understand the relative strengths of supervised and zero-shot detectors. The results are presented in
Figure 4, using 200 samples from each dataset for evaluation. We find that supervised detectors can provide similar
detection performance to DetectGPT on in-distribution data
like English news, but perform significantly worse than zeroshot methods in the case of English scientific writing and
fail altogether for German writing. This finding echoes past
work showing that language models trained for machinegenerated text detection overfit to their training data ; Ippolito et al. ; Jawahar et al. ).
In contrast, zero-shot methods generalize relatively easily
to new languages and domains; DetectGPTâ€™s performance
in particular is mostly unaffected by the change in language
from English to German.
While our experiments have shown that DetectGPT is effective on a variety of domains and models, it is natural to
wonder if it is effective for the largest publicly-available
LMs. Therefore, we also evaluate multiple zero-shot and supervised methods on two 175B parameter models, OpenAIâ€™s
GPT-3 and AI21 Labsâ€™ Jurassic-2 Jumbo. Because neither
API provides access to the complete conditional distribution
4The overall ease of detecting machine-generated fake writing
corroborates anecdotal reporting that machine-generated creative
writing tends to be noticeably generic, and therefore relatively easy
to detect .
Zero-Shot Machine-Generated Text Detection using Probability Curvature
Fraction of GPT-J-generated news article re-written
Detection AUROC
Likelihood
Figure 5. We simulate human edits to machine-generated text by
replacing varying fractions of model samples with T5-3B generated text (masking out random five word spans until r% of text is
masked to simulate human edits to machine-generated text). The
four top-performing methods all generally degrade in performance
with heavier revision, but DetectGPT is consistently most accurate.
Experiment is conducted on the XSum dataset.
for each token, we cannot compare to the rank, log rank, and
entropy-based prior methods. We sample 150 examples5
from the PubMedQA, XSum, and WritingPrompts datasets
and compare the two pre-trained RoBERTa-based detector
models with DetectGPT and the probability thresholding
baseline. We show in Table 2 that DetectGPT can provide
detection competitive with or better than the stronger of the
two supervised models, and it again greatly outperforms
probability thresholding on average.
5.2. Variants of Machine-Generated Text Detection
Detecting paraphrased machine-generated text. In practice, humans may manually edit or refine machine-generated
text rather than blindly use a modelâ€™s generations for their
task of interest. We therefore conduct an experiment to
simulate the detection problem for model samples that have
been increasingly heavily revised. We simulate human revision by replacing 5 word spans of the text with samples
from T5-3B until r% of the text has been replaced, and
report performance as r varies. Figure 5 shows that DetectGPT maintains detection AUROC above 0.8 even when
nearly a quarter of the text in model samples has been replaced. Unsurprisingly, almost all methods show a gradual
degradation in performance as the sample is more heavily
revised. The entropy baseline shows surprisingly robust
performance in this setting (althought it is least accurate
on average), even slightly improving detection performance
up to 24% replacement. DetectGPT shows the strongest
detection performance for all revision levels.
Impact of alternative decoding strategies on detection.
While Table 1 suggests that DetectGPT is effective for
5We reduce the number of evaluation samples from 500 in our
main experiments to reduce the API costs of these experiments.
WritingPrompts
Table 3. AUROC for zero-shot methods averaged across the five
models in Table 1 for both top-k and top-p sampling, with k =
40 and p = 0.96. Both settings enable slightly more accurate
detection, and DetectGPT consistently provides the best detection
performance. See Appendix Tables 4 and 5 for complete results.
detecting machine-generated text, prior work notes that
the decoding strategy (i.e., temperature sampling, top-k,
nucleus/top-p) can impact the difficulty of detection. We repeat the analysis from Section 5.1 using top-k sampling and
nucleus sampling. Top-k sampling truncates the sampling
distribution to only the k highest-probability next tokens;
nucleus sampling samples from only the smallest set of tokens whose combined probability exceeds p. The results
are summarized in Table 3; Appendix Tables 4 and 5 show
complete results. We use k = 40, and p = 0.96, in line with
prior work . We find that both top-k
and nucleus sampling make detection easier, on average.
Averaging across domains, DetectGPT provides the clearest
signal for zero-shot detection.
Detection when the source model is unknown. While
our experiments have focused on the white-box setting
for machine-generated text detection, in this section, we
Scoring Model
Base Model
Figure 6. DetectGPT performs
best when scoring samples
with the same model that generated them (diagonal), but
the column means suggest that
some models (GPT-Neo, GPT-
2) may be better â€˜scorersâ€™ than
others (GPT-J). White values
show mean (standard error)
AUROC over XSum, SQuAD,
and WritingPrompts; black
shows row/column mean.
explore the effect of using
a different model to score a
candidate passage (and perturbed texts) than the model
that generated the passage.
In other words, we aim
to classify between humangenerated text and text from
model A, but without access to model A to compute log probabilities. Instead, we use log probabilities computed by a surrogate model B.
We consider three models, GPT-J,
GPT-Neo-2.7, and GPT-2,
evaluating all possible combinations of source model
and surrogate model (9 total). We average the performance across 200 samples
from XSum, SQuAD, and
Zero-Shot Machine-Generated Text Detection using Probability Curvature
Detection AUROC
5 perturbations
25 perturbations
Mask filling model size (# parameters)
Figure 7. There is a clear association between capacity of maskfilling model and detection performance, across source model
scales. Random mask filling (uniform sampling from mask filling
model vocabulary) performs poorly, reinforcing the idea that the
perturbation function should produce samples on the data manifold.
Curves show AUROC scores on 200 SQuAD contexts.
WritingPrompts. The results are presented in Figure 6,
showing that when the surrogate model is different from the
source model, detection performance is reduced, indicating
that DetectGPT is most suited to the white-box setting. Yet
we also observe that if we fix the model used for scoring
and average across source models whose generations are
detected (average within column), there is significant variation in AUROC; GPT-2 and GPT-Neo-2.7 seem to be better
â€˜scorersâ€™ than GPT-J. These variations in cross-model scoring performance suggest ensembling scoring models may
be a useful direction for future research; see Mireshghallah
et al. for reference.
5.3. Other factors impacting performance of DetectGPT
In this section, we explore how factors such as the size of the
mask-filling model, the number of perturbations used to estimate the expectation in Equation 1, or the data distribution
of the text to be detected impact detection quality.
Source and mask-filling model scale. Here we study the
impact of the size of the source model and mask-filling
model on DetectGPTâ€™s performance; the results are shown
in Figure 7. In particular, the increased discrimination power
of DetectGPT for larger mask-filling models supports the
interpretation that DetectGPT is estimating the curvature
of the log probability in a latent semantic space, rather
than in raw token embedding space. Larger T5 models
better represent this latent space, where random directions
correspond to meaningful changes in the text.
Number of perturbations for DetectGPT. We evaluate the
performance of DetectGPT as a function of the number of
perturbations used to estimate the expectation in Equation 1
on three datasets. The results are presented in Figure 8.
Detection accuracy continues to improve until 100 perturbations, where it converges. Evaluations use 100 examples
from each dataset.
Data distributional properties. We study more closely
the impact of the data distribution on DetectGPT, particu-
Detection AUROC
WritingPrompts
Number of perturbations
Figure 8. Impact of varying the number of perturbations (samples
of mask and mask-fill) used by DetectGPT on AUROC for GPT-2
(left) and GPT-J (right) to estimate the perturbation discrepancy
on detection. Averaging up to 100 perturbations greatly increases
DetectGPTâ€™s reliability. Perturbations sampled from T5-large.
larly how the domain impacts the threshold separating the
perturbation discrepancy distributions of model-generated
and human texts as well as the impact of passage length on
detection. Figure 9 shows the perturbation discrepancy distributions for model-generated and human texts across four
data distributions, using GPT-Neo-2.7B to generate samples. A threshold of slightly below 0.1 separates human and
model texts across data distributions, which is important for
practical scenarios in which a passage may be analyzed without knowing its domain a priori. Finally, Figure 10 shows an
analysis of DetectGPTâ€™s performance as a function of passage length. We bin the paired human- and model-generated
sequences by their average length into three bins of equal
size (bottom/middle/top third), and plot the AUROC within
each bin. The relationship between detection performance
and passage length generally depends on the dataset and
model (or tokenizer). For very long sequences, DetectGPT
may see reduced performance because our implementation
of DetectGPT applies all T5 mask-filling perturbations at
once, and T5 may fail to track many mask tokens at once.
By applying perturbations in multiple sequential rounds of
smaller numbers of masks, this effect may be mitigated.
6. Discussion
As large language models continue to improve, they will
become increasingly attractive tools for replacing human
writers in a variety of contexts, such as education, journalism, and art. While legitimate uses of language model
technologies exist in all of these settings, teachers, readers,
and consumers are likely to demand tools for verifying the
human origin of certain content with high educational, societal, or artistic significance, particularly when factuality
(and not just fluency) is crucial.
In light of these elevated stakes and the regular emergence of
new large language models, we study the zero-shot machinegenerated text detection problem, in which we use only the
raw log probabilities computed by a generative model to
determine if a candidate passage was sampled from it. We
Zero-Shot Machine-Generated Text Detection using Probability Curvature
WritingPrompts
Log Probability Change (Perturbation Discrepancy)
Figure 9. Perturbation discrepancy distributions for GPT-Neo
(2.7B) and humans across domains. A threshold of 0.1 generally separates model- and human-generated text well, which is
important for practical scenarios where the domain is unknown.
identify a property of the log probability function computed
by a wide variety of large language models, showing that a
tractable approximation to the trace of the Hessian of the
modelâ€™s log probability function provides a useful signal
for detecting model samples. Our experiments find that
this signal is more discriminative than existing zero-shot
detection methods and is competitive with bespoke detection
models trained with millions of model samples.
DetectGPT and Watermarking. One interpretation of
the perturbation function is producing semantically similar
rephrasings of the original passage. If these rephrasings
are systematically lower-probability than the original passage, the model is exposing its bias toward the specific (and
roughly arbitrary, by human standards) phrasing used. In
other words, LLMs that do not perfectly imitate human
writing essentially watermark themselves implicitly. Under
this interpretation, efforts to manually add watermarking biases to model outputs may further improve the effectiveness of methods
such as DetectGPT, even as LLMs continue to improve.
Limitations. One limitation of probability-based methods
for zero-shot machine-generated text detection (like Detect-
GPT) is the white-box assumption that we can evaluate log
probabilities of the model(s) in question. For models behind APIs that do provide probabilities (such as GPT-3),
evaluating probabilities nonetheless costs money. Another
assumption of DetectGPT is access to a reasonable perturbation function. While in this work, we use off-the-shelf
mask-filling models such as T5 and mT5 (for non-English
languages), some domains may see reduced performance
if existing mask-filling models do not well represent the
space of meaningful rephrases, reducing the quality of the
curvature estimate. While DetectGPT provides the best
available detection performance for PubMedQA, its drop
in performance compared to other datasets may be a result
Average length
Average length
WritingPrompts
Average length
EleutherAI/gpt-j-6b
Average length
EleutherAI/gpt-neox-20b
Figure 10. DetectGPT AUROC vs passage length. The relationship between detection performance and passage length generally
depends on the dataset and model (or tokenizer). Decreases in
detection quality with increasing length may be due to T5 failing
to track many (20+) masks to fill at once; this problem may be
mitigated by applying mask-fills in a sequence of smaller batches.
of lower quality perturbations. Finally, DetectGPT is more
compute-intensive than other methods for detection, as it
requires sampling and scoring the set of perturbations for
each candidate passage, rather than just the candidate passage; a better tuned perturbation function or more efficient
curvature approximation may help mitigate these costs.
Future Work. While the methods in this work make no
assumptions about the models generating the samples, future work may explore how watermarking algorithms can be
used in conjunction with detection algorithms like Detect-
GPT to further improve detection robustness as language
models continually improve their reproductions of human
text. Separately, the results in Section 5.2 suggest that extending DetectGPT to use ensembles of models for scoring,
rather than a single model, may improve detection in the
black box setting. Another topic that remains unexplored
is the relationship between prompting and detection; that
is, can a clever prompt successfully prevent a modelâ€™s generations from being detected by existing methods? Finally,
future work may explore whether the local log probability curvature property we identify is present for generative
models in other domains, such as audio, video, or images.
We hope that the present work serves as inspiration to future work developing effective, general-purpose methods
for mitigating potential harms of machine-generated media.
Acknowledgements
EM gratefully acknowledges funding from a Knight-
Hennessy Graduate Fellowship. CF and CM are CIFAR
Fellows. The Stanford Center for Research on Foundation
Models (CRFM) provided part of the compute resources
used for the experiments in this work.
Zero-Shot Machine-Generated Text Detection using Probability Curvature