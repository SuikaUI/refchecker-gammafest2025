Multimodal Explanations: Justifying Decisions and Pointing to the Evidence
Dong Huk Park1, Lisa Anne Hendricks1, Zeynep Akata2,3, Anna Rohrbach1,3,
Bernt Schiele3, Trevor Darrell1, and Marcus Rohrbach4
1EECS, UC Berkeley, 2University of Amsterdam, 3MPI for Informatics, 4Facebook AI Research
Deep models that are both effective and explainable are
desirable in many settings; prior explainable models have
been unimodal, offering either image-based visualization of
attention weights or text-based generation of post-hoc justiﬁcations. We propose a multimodal approach to explanation, and argue that the two modalities provide complementary explanatory strengths. We collect two new datasets to
deﬁne and evaluate this task, and propose a novel model
which can provide joint textual rationale generation and attention visualization. Our datasets deﬁne visual and textual justiﬁcations of a classiﬁcation decision for activity
recognition tasks (ACT-X) and for visual question answering tasks (VQA-X). We quantitatively show that training
with the textual explanations not only yields better textual
justiﬁcation models, but also better localizes the evidence
that supports the decision. We also qualitatively show cases
where visual explanation is more insightful than textual explanation, and vice versa, supporting our thesis that multimodal explanation models offer signiﬁcant beneﬁts over
unimodal approaches.
1. Introduction
Explaining decisions is an integral part of human communication, understanding, and learning, and humans naturally provide both deictic (pointing) and textual modalities in a typical explanation. We aim to build deep learning models that also are able to explain their decisions with
similar ﬂuency in both visual and textual modalities. Previous machine learning methods for explanation were able to
provide a text-only explanation conditioned on an image in
context of a task, or were able to visualize active intermediate units in a deep network performing a task, but were
unable to provide explanatory text grounded in an image.
We propose a new model which can jointly generate visual and textual explanations, using an attention mask to
localize salient regions when generating textual rationales.
We argue that to train effective models, measure the quality
Visual Pointing
Q: Is this a healthy meal?
...because it
is a hot dog
with a lot of
Textual Justification
...because it
contains a
variety of
vegetables on
the table.
Figure 1: For a given question and an image, our Pointing
and Justiﬁcation Explanation (PJ-X) model predicts the answer and multimodal explanations which both point to the
visual evidence for a decision and provide textual justiﬁcations. We show that considering multimodal explanations
results in better visual and textual explanations.
of the generated explanations, compare with other methods,
and understand when methods will generalize, it is important to have access to ground truth human explanations. Unfortunately, there is a dearth of datasets which include examples of how humans justify speciﬁc decisions. Thus, we
collect two new datasets, ACT-X and VQA-X, which allow
us to train and evaluate our novel model, which we call the
Pointing and Justiﬁcation Explanation (PJ-X) model. PJ-X
is explicitly multimodal: it incorporates an explanatory attention step, which allows our model to both visually point
to the evidence and justify a model decision with text.
To illustrate the utility of multimodal explanations, consider Figure 1. In both examples, the question “Is this a
healthy meal?” is asked, and the PJ-X model correctly answers either “no” or “yes” depending on the visual input.
To justify why the image is not healthy, the generated textual justiﬁcation mentions the kinds of unhealthy food in the
image (“hot dog” and “toppings”). In addition to mentioning the unhealthy food, our model is able to point to the hot
dog in the image. Likewise, to justify why the image on
the right is healthy, the textual explanation model mentions
“vegetables”. Note that PJ-X model then points to the vegetables, which are mentioned in the textual explanation, but
 
not other items in the image, such as the bread.
We propose VQA and activity recognition as testbeds for
studying explanations because they are challenging and important visual tasks which have interesting properties for explanation. VQA is a widely studied multimodal task that requires visual and textual understanding as well as commonsense knowledge. The newly collected VQA v2 dataset 
includes complementary pairs of questions and answers.
Complementary VQA pairs ask the same question of two
semantically similar images which have different answers.
As the two images are semantically similar, VQA models
must employ ﬁnegrained reasoning to answer the question
correctly. Not only is this an interesting and useful setting
for measuring overall VQA performance, but it is also interesting when studying explanations. By comparing explanations from complementary pairs, we can more easily
determine whether our explanations focus on the important
factors for making a decision.
Additionally, we collect annotations for activity recognition using the MPII Human Pose (MHP) dataset . Activity recognition in still images relies on a variety of cues,
such as pose, global context, and the interaction between
humans and objects.
Though a recognition model can
potentially classify an activity correctly, it is not capable
of indicating which factors inﬂuence the decision process.
Furthermore, classifying speciﬁc activities requires understanding ﬁnegrained differences (e.g., “road biking” and
“mountain biking” include similar objects like “bike” and
“helmet,” but road biking occurs on a road whereas mountain biking occurs on a mountain path). Such ﬁnegrained
differences are interesting yet difﬁcult to capture when explaining neural network decisions.
In sum, we present ACT-X and VQA-X, two novel
datasets of human annotated multimodal explanations (visual and textual) for activity recognition and visual question answering. These datasets allow us to train the Pointing
and Justiﬁcation (PJ-X) model which goes beyond current
visual explanation systems by producing multimodal explanations, justifying the predicted answer post-hoc by visual
pointing and textual justiﬁcation. Our datasets also allow to
effectively evaluate explanation models, and we show that
the PJ-X model outperforms strong baselines, and, importantly, that by generating multimodal explanations, we outperform models which only produce visual or textual explanations. We will release our model architecture, learned
weights, and datasets upon acceptance of this paper.
2. Related Work
Explanations. Early textual explanation models span a variety of applications (e.g., medical and feedback for
teaching programs ) and are generally template
based. More recently, developed a deep network to
A man on a snowboard
is on a ramp.
Description
Q: What is the person doing?
A: Snowboarding.
Explanation
Because... they are on a
snowboard in snowboarding outfit.
A gang of biker police
riding their bikes in
formation down a street.
Q: Can these people arrest someone?
Because... they are Vancouver police.
A man in a black shirt and
blue jeans is holding a
glowing ball.
I can tell the person is juggling.
A man standing wearing
a pink shirt and grey
pants near a ball.
Because... he holds two balls in one
hand, while another ball is aloft just
above the other hand.
I can tell the person is juggling.
Because... he has two balls in his
hands while two are in the air.
Figure 2: In comparison to descriptions, our VQA-X explanations focus on the evidence that pertains to the question
and answer instead of generally describing the scene. For
ACT-X, our explanations are task speciﬁc whereas descriptions are more generic.
generate natural language justiﬁcations of a ﬁne-grained object classiﬁer. However, unlike our model, it does not provide multimodal explanations. Furthermore, could not
train on reference human explanations as no such datasets
existed. We provide two datasets with reference textual explanations to enable more research in the direction of textual
explanation generation.
A variety of work has proposed methods to visually
explain decisions. Some methods ﬁnd discriminative visual patches whereas others aim to understand
intermediate features which are important for end decisions e.g. what does a certain neuron represent.
Our model PJ-X points to visual evidence via an attention
mechanism which is an intuitive way to convey knowledge
about what is important to the network without requiring
domain knowledge. Unlike prior work, PJ-X generates multimodal explanations in the form of explanatory sentences
and attention maps pointing to the visual evidence.
Prior work has investigated how well generated visual
explanations align with human gaze . However, when
answering a question, humans do not always look at image
regions which are necessary to explain a decision. For example, given the question “What is the name of the restaurant?”, human gaze might capture other buildings before
settling on the restaurant. In contrast, when we collect our
annotations, we allow annotators to view the entire image
and ask them to point to the most relevant visual evidence
for making a decision. Furthermore, our visual explanations are collected in conjunction with textual explanations
to build and evaluate multimodal explanation models.
Visual Question Answering and Attention.
Initial approaches to VQA used full-frame representations , but
most recent approaches use some form of spatial atten-
#Imgs #Q/A Pairs #Unique Q. #Unique A. #Expl. (Avg. #w) Expl.Vocab Size #Comple. Pairs #Visual Ann.
Table 1: Dataset statistics for VQA-X (top) and ACT-X (bottom). Unique Q. = Unique questions, Unique A. = Unique answers,
Expl. = Explanations, Avg. #w = Average number of words, Comple. Pairs = Complementary pairs, Visual Ann. = Visual annotations.
tion . We base our method
on , the winner of VQA 2016 challenge, however we
use an element-wise product as opposed to compact bilinear pooling. has explored the element-wise product for
VQA just as we do in our method, however improves
performance by applying hyperbolic tangent (TanH) after
the multimodal pooling whereas we improve by applying
signed square-root and L2 normalization.
Activity Recognition. Recent work on activity recognition
in still images relies on a variety of cues, such as pose and
global context . Speciﬁcally, considers additional image regions and considers a global image
feature in addition to the region where an activity occurs.
Generally, works on the MPII Human Activities dataset provide the ground truth location of a human at test time .
In contrast, we consider a more realistic scenario and do
not make any assumptions about where the activity occurs
at test time. Our model relies on attention to focus on important parts of an image for classiﬁcation and explanation.
3. Multimodal Explanations
We propose multimodal explanation tasks with visual
and textual components, deﬁned on both visual question
answering and activity recognition testbeds. To train and
evaluate models for this task we collect two multimodal explanation datasets: Visual Question Answering Explanation
(VQA-X) and Activity Explanation (ACT-X) (see Table 1
for a summary). For each dataset we collect textual and visual explanations from human annotators.
VQA Explanation Dataset (VQA-X). The Visual Question Answering (VQA) dataset contains open-ended
questions about images which require understanding vision,
language, and commonsense knowledge to answer. VQA
consists of approximately 200K MSCOCO images ,
with 3 questions per image and 10 answers per question.
Many questions in VQA are of the sort: “What is the
color of the banana?”. It is difﬁcult for humans to explain
answers to such questions because it requires explaining a
fundamental visual property: color. Thus, we aim to provide textual explanations for questions that go beyond such
trivial cases. To do this, we consider the annotations collected in which say how old a human must be to answer
a question. We ﬁnd that questions which require humans to
be of age 9 or higher are generally interesting to explain.
Additionally, we consider complementary pairs from the
VQA v2 dataset . Complementary pairs consist of a
question and two similar images which give two different
answers. Complementary pairs are particularly interesting
for the explanation task because they allow us to understand
whether explanations name the correct evidence based on
image content, or whether they just memorize which content to consider based off speciﬁc question types.
We collect a single textual explanation for QA pairs in
the training set and three textual explanations for test/val
QA pairs. Some examples can be seen Figure 2.
Action Explanation Dataset (ACT-X). The MPII Human
Pose (MHP) dataset contains 25K images extracted from
Youtube videos. From the MHP dataset, we select all images that pertain to 397 activities, resulting in 18, 030 images total. For each image we collect three explanations.
During data annotation, we ask the annotators to complete
the sentence “I can tell the person is doing (action) because..” where the action is the ground truth activity label.
We also ask them to use at least 10 words and avoid mentioning the activity class in the sentence. MHP dataset also
comes with sentence descriptions provided by . See
Figure 2 for examples of descriptions and explanations.
Ground truth for pointing. In addition to textual justiﬁcations, we collect visual explanations from humans for both
VQA-X and ACT-X datasets in order to evaluate how well
the attention of our model corresponds to where humans
think the evidence for the answer is.
Human-annotated
visual explanations are collected via Amazon Mechanical
Turk where we use the segmentation UI interface from the
OpenSurfaces Project . Annotators are provided with an
image and an answer (question and answer pair for VQA-X,
Q: What is the person doing? A: Skiing
Q: What is the boy doing? A: Skateboarding
Q: What game are they playing? A: Baseball
Activity: Mowing Lawn
Activity: Planting, Potting
Activity: Bicycling, Mountain
Figure 3: Human annotated visual explanations. On the left: example annotations collected on VQA-X dataset. On the right:
Example annotations collected on ACT-X dataset. The visual evidence that justiﬁes the answer is segmented in yellow.
Q: What is the person holding? A: Ski Poles
Q: Are there street lights? A: Yes
Figure 4: Human visual annotations from VQA-HAT and
VQA-X. We aggregate all the annotations in each image
and normalize them to create a probability distribution. The
distribution is then visualized over the image as a heatmap.
class label for ACT-X). They are asked to segment objects
and/or regions that most prominently justify the answer. For
each dataset we randomly sample images from the test split,
and for each image we collect 3 annotations. Some examples can be seen in Figure 3.
Comparing with VQA-HAT. A thorough comparison between our dataset and VQA-HAT dataset from is currently not viable because the two datasets have different
splits and the overlap is small. However, we present qualitative comparison in Figure 4. In the ﬁrst row, our VQA-X
annotation has a ﬁner granularity since it segments out the
objects in interest more accurately than the VQA-HAT annotation. In the second row, our annotation contains less extraneous information than the VQA-HAT annotation. Since
the VQA-HAT annotations are collected by having humans
“unblur” the images, they are more likely to introduce noise
when irrelevant regions are uncovered.
4. Pointing and Justiﬁcation Model (PJ-X)
The goal of our work is to implement a multimodal explanation system that justiﬁes a decision with natural language and points to the evidence. We deliberately design
our Pointing and Justiﬁcation Model (PJ-X) to allow training these two tasks. Speciﬁcally we want to rely on natural language justiﬁcations and the classiﬁcation labels as
the only supervision. We design the model to learn how to
point in a latent way. For the pointing we rely on an attention mechanism which allows the model to focus on a
spatial subset of the visual representation.
We ﬁrst predict the answer given an image and question
using the answering model. Then given the answer, question, and image, we generate visual and textual explanations
with the multimodal explanation model. An overview of our
model is presented in Figure 5.
Answering model. In visual question answering the goal is
to predict an answer given a question and an image. For activity recognition we do not have an explicit question. Thus,
we ignore the question which is equivalent to setting the
question representation to f Q(Q) = 1, a vector of ones.
We base our answering model on the overall architecture
from the MCB model , but replace the MCB unit with a
simpler element-wise multiplication ⊙to pool multimodal
features. We found that this leads to similar performance,
but much faster training (see supplemental material).
In detail, we extract spatial image features f I(I, n, m)
from the last convolutional layer of ResNet-152 followed
by 1 × 1 convolutions ( ¯f I) giving a 2048 × N × M spatial image feature. We encode the question Q with a 2-layer
it contains a
variety of
vegetables
on the table.
Answering Model
Multimodal Explanation Model
Weighted Sum
Justification
Weighted Sum
Visual Pointing
Figure 5: Our Pointing and Justiﬁcation (PJ-X) architecture generates a multimodal explanation which includes textual
justiﬁcation (“it contains a variety of vegetables on the table”) and pointing to the visual evidence.
LSTM, which we refer to as f Q(Q). We combine this
and the spatial image feature using element-wise multiplication followed by signed square-root, L2 normalization,
and Dropout, and two more layers of 1 × 1 convolutions
with ReLU in between. This process gives us a N × M
attention map ¯αn,m. We apply softmax to produce a normalized soft attention map.
The attention map is then used to take the weighted sum
over the image features and this representation is once again
combined with the LSTM feature to predict the answer ˆy as
a classiﬁcation problem over all answers Y . We provide an
extended formalized version in the supplemental.
Multimodal explanation model.
We argue that to generate multimodal explanation, we should condition it on
question, answer, and image. For instance, to be able to
explain “Because they are Vancouver police” in Figure 2,
the model needs to see the question, i.e. “Can these people
arrest someone?”, the answer, i.e. “Yes” and the image, i.e.
the “Vancouver police” banner on the motorcycles.
We model this by pooling image, question, and answer
representations to generate attention map, our Visual Pointing. The Visual Pointing is further used to create attention
features that guide the generation of our Textual Justiﬁcation.
More speciﬁcally, the answer predictions are embedded in a d-dimensional space followed by tanh nonlinearity and a fully connected layer:
f yEmbed(ˆy)
W6(tanh(W5ˆy + b5)) + b6. To allow the model to learn
how to attend to relevant spatial location based on the answer, image, and question, we combine this answer feature
with Question-Image embedding ¯f IQ(I, Q) from the answering model. Applying 1 × 1 convolutions, element-wise
multiplication followed by signed square-root, L2 normalization, and Dropout, results in a multimodal feature.
¯f IQA(I, n, m, Q, ˆy) =(W7 ¯f IQ(I, Q, n, m) + b7)
⊙f yEmbed(ˆy))
f IQA(I, Q, ˆy) =L2(signed sqrt( ¯f IQA(I, Q, ˆy)))
with Relu ρ(x) = max(x, 0). Next we predict a N × M
attention map ¯αn,m and apply softmax to produce a normalized soft attention map, our Visual Pointing αpointX
aims to point at the evidence of the generated explanation:
¯αn,m =f pointX(I, n, m, Q, ˆy)
=W9ρ(W8f IQA(I, Q, ˆy) + b8) + b9
exp(¯αn,m)
j=1 exp(¯αn,m)
Using αpointX
, we compute the attended visual representation, and merge it with the LSTM feature that encodes
the question and the embedding feature that encodes the answer:
f X(I, Q, ˆy) =(W10
f I(I, n, m) + b10)
⊙(W11f Q(Q) + b11) ⊙f yEmbed(ˆy) (8)
This combined feature is then fed into an LSTM decoder
to generate our Textual Justiﬁcations that are conditioned
on image, question, and answer.
Justiﬁcations
[w1, w2, . . .] and our model predicts one word wt at
each time step t conditioned on the previous word and the
hidden state of the LSTM:
ht = f LST M(f X(I, Q, ˆy), wt−1, ht−1)
wt = f pred(ht) = Softmax(Wpredht + bpred)
5. Experiments
In this section, after detailing the experimental setup, we
present quantitative results on ablations done for textual justiﬁcation and visual pointing tasks, and discuss their implications. Additionally, we provide and analyze qualitative
results for both tasks.
5.1. Experimental Setup
Here, we detail our experimental setup in terms of model
training, hyperparameter settings, and evaluation metrics.
Model training and hyperparameters. For VQA, the answering model of PJ-X is pre-trained on the VQA v2 training set .
We then freeze or ﬁnetune the weights of
the answering model when training the multimodal explanation model on textual annotations as the VQA-X dataset
is signiﬁcantly smaller than the original VQA training set.
For activity recognition, answering and explanation components of PJ-X are trained jointly. The spatial feature size of
PJ-X is N = M = 14. For VQA, we limit the answer space
to the 3000 most frequently occurring answers on the training set (i.e. |Y | = 3000) whereas for activity recognition,
|Y | = 397. We set the answer embedding size as d = 300
for both tasks.
Evaluation metrics.
We evaluate our textual justiﬁcations w.r.t BLEU-4 , METEOR , ROUGE ,
CIDEr and SPICE metrics, which measure the degree of similarity between generated and ground truth sentences. We also include human evaluation since automatic
metrics do not always reﬂect human preference. We randomly choose 1000 data points each from the test splits of
VQA-X and ACT-X datasets, where the model predicts the
correct answer, and then for each data point ask 3 human
subjects to judge whether a generated explanation is better
than, worse than, or equivalent to the ground truth explanation (we note that human judges do not know what explanation is ground truth and the order of sentences is randomized). We report the percentage of generated explanations
which are equivalent to or better than ground truth human
explanations, when at least 2 out of 3 human judges agree.
For visual pointing task, we use Earth Mover’s Distance (EMD) which measures the distance between
two probability distributions over a region. We use the code
from to compute EMD. We also report on Rank Correlation which was used in . For computing Rank Correlation, we follow where we scale the generated attention map and the human ground-truth annotations from the
VQA-X/ACT-X/VQA-HAT datasets to 14 × 14, rank the
pixel values, and then compute correlation between these
two ranked lists.
5.2. Textual Justiﬁcation
We ablate PJ-X and compare with related approaches on
our VQA-X and ACT-X datasets through automatic and human evaluations for the generated explanations.
Details on compared models. We compare with the stateof-the-art using publicly available code. For fair comparison, we use ResNet features extracted from the entire
image when training . The generated sentences from
 are conditioned on both the image and the class label. uses discriminative loss, which enforces the generated sentence to contain class-speciﬁc information, to backpropagate policy gradients when training the language generator, and thus involves training a separate sentence classi-
ﬁer to generate rewards. Our model does not use discriminative loss/policy gradients and does not require deﬁning
a reward. Note that is trained with descriptions. Similarly, ”Ours on Descriptions” is an ablation in which we
train PJ-X on descriptions instead of explanations. ”Ours
w/o Attention” is similar to in the sense that there is
no attention mechanism involved when generating explanations, however, it does not use the discriminative loss and is
trained on explanations instead of descriptions.
Descriptions vs. Explanations. “Ours” signiﬁcantly outperforms “Ours with Descriptions” by a large margin on
both datasets which is expected as descriptions are insuf-
ﬁcient for the task of generating explanations. Additionally, “Ours” compares favorably to even in the case
when “Ours” generates textual justiﬁcations conditioned on
the prediction, not the ground-truth answer. These results
demonstrate the limitation of training explanation systems
with descriptions, and thus support the necessity of having datasets speciﬁcally curated for explanations.
on Descriptions” performs worse on certain metrics compared to which may be attributed to additional training
signals generated from discriminative loss and policy gradients, but further investigation is left for future work.
Unimodal explanations vs.
Multimodal explanations.
Including attention when generating textual justiﬁcations
allows us to build a multimodal explanation model. Aside
from the immediate beneﬁt of providing visual rationale
about a model’s decision, learning to point at visual evidence helps generating better textual justiﬁcations.
can be seen from Table 2, “Ours” greatly improves textual
justiﬁcations compared to “Ours w/o Attention” on both
datasets, demonstrating the value of designing multimodal
explanation systems.
5.3. Visual Pointing
We compare the visual pointing performance of PJ-X to
several baselines and report quantitative results with corresponding analysis.
GT-ans Train- Att.
Condi- ing
Automatic evaluation
Automatic evaluation
tioning Data
12.9 15.9 39.0 12.4 12.0
Ours on Descriptions Yes
6.1 12.8 26.4 36.2 12.1
6.9 12.9 28.3 20.3
Ours w/o Attention
18.0 17.6 42.4 66.3 14.3
16.9 17.0 42.0 33.3 10.6
19.8 18.6 44.0 73.4 15.4
24.5 21.5 46.9 58.7 16.0
Ours on Descriptions No
5.9 12.6 26.3 35.2 11.9
5.2 11.0 26.5 10.4
Ours w/o Attention
18.0 17.3 42.1 63.6 13.8
11.9 13.6 37.9 16.9
19.5 18.2 43.4 71.3 15.1
15.3 15.6 40.0 22.0
Table 2: Evaluation of Textual Justiﬁcations. Evaluated automatic metrics: BLEU-4 (B), METEOR (M), ROUGE (R), CIDEr
(C) and SPICE (S). Reference sentence for human and automatic evaluation is always an explanation. All in %. Our proposed
model compares favorably to baselines.
Earth Mover’s
Rank Correlation
(lower is better)
(higher is better)
VQA-X ACT-X VQA-X
ACT-X VQA-HAT
Random Point
6.59 +0.0017 +0.0003
3.25 +0.0003 -0.0001
HieCoAtt-Q 
Answering Model
4.78 +0.2211 +0.0104
2.54 +0.3423 +0.3933
Table 3: Evaluation of Visual Pointing Justiﬁcations. For
rank correlation, all results have standard error < 0.005.
Details on compared models.
We compare our model
against the following baselines. Random Point randomly
attends to a single point in a 14 × 14 grid. Uniform Map
generates attention map that is uniformly distributed over
the 14×14 grid. In addition to these baselines, we also compare PJ-X attention maps with those generated from stateof-the-art VQA systems such as .
Improved localization with textual explanations.
evaluate attention maps using the Earth Mover’s Distance
(lower is better) and Rank Correlation (higher is better) on
VQA-X and ACT-X datasets in Table 3. From Table 3, we
observe that “Ours” not only outperforms baselines Random Point and Uniform Map, but also our answering model
and on both datasets and on both metrics. The attention
maps generated from our answering model and do not
receive training signals from the textual annotations as they
are only trained to predict the correct answer, whereas the
attention maps generated from PJ-X multimodal explanation model are latently learned through supervision of textual annotations. The experiment results imply that learning to generate textual explanations helps improve visual
Q: Is this a zoo?
… because the zebras are
standing in a green field.
… because there are
animals in an enclosure.
Q: Is the water calm?
… because there are waves
… because there are no
waves and you can see the
reflection of the sun.
Figure 6: VQA-X qualitative results: For each image the
PJ-X model provides an answer and a justiﬁcation, and
points to the evidence for that justiﬁcation. We show pairs
of images from VQA v2 complementary pairs.
pointing task, and further conﬁrm the advantage of having
a multimodal explanation system.
The activity is
A: Mowing Lawn
… because he is kneeling
in the grass next to a lawn
… because he is pushing a
lawn mower over a grassy
The activity is
A: Mountain Biking
… because he is riding a
bicycle down a mountain
path in a mountainous area.
… because he is wearing a
cycling uniform and riding
a bicycle down the road.
A: Mowing Lawn
A: Road Biking
Figure 7: ACT-X qualitative results: For each image the PJ-
X model provides an answer and a justiﬁcation, and points
to the evidence for that justiﬁcation.
5.4. Qualitative Results
In this section we present our qualitative results on VQA-
X and ACT-X datasets demonstrating that our model generates high quality sentences and the attention maps point to
relevant locations in the image.
VQA-X. Figure 6 shows qualitative results on our VQA-X
dataset. We show pairs of images that form complementary
pairs in VQA v2. Our textual justiﬁcations are able to both
capture common sense and discuss speciﬁc image parts important for answering a question. For example, when asked
“Is this a zoo?”, the explanation model is able to discuss
what the concept of “zoo” represents, i.e. “animals in an
enclosure”. When determining whether the water is calm,
which requires discussing speciﬁc image regions, the textual justiﬁcation discusses foam on the waves.
Visually, we notice that our attention model is able to
point to important visual evidence. For example in the top
row of Figure 6, for the question “Is this a zoo?” the visual
explanation focuses on the ﬁeld in one case, and the fence
in another.
ACT-X. Figure 7 shows results on our ACT-X dataset. Textual explanations discuss a variety of visual cues important
for correctly classifying activities such as global context,
Q: Is the man leaning forward?
… because he is riding
… because the sky is clear
blue and there are no
Q: Is it cloudy?
Figure 8: Qualitative results comparing the insightfulness
of visual pointing and textual justiﬁcation. The left example demonstrates how visual pointing is more informative
than textual justiﬁcation whereas the right example shows
the opposite.
e.g. “a grassy lawn / a mountainous area”, and person-object
interaction, e.g. “pushing a lawn mower / riding a bicycle”
for mowing lawn and mountain biking, respectively. These
explanations require determining which of many multiple
cues are appropriate to justify a particular action.
Our model points to visual evidence important for understanding each human activity. For example to classify
“mowing lawn” in the top row of Figure 7 the model focuses both on the person, who is on the grass, as well as the
lawn mower. Our model can also differentiate between similar activities based on the context, e.g.”mountain biking” or
”road biking”.
Explanation Consistent with Incorrect Prediction. Generating reasonable explnations for correct answers is important, but it is also crucial to see how a system behaves in the
face of incorrect predictions. Such analysis would provide
insights into whether the explanation generation component
of the model is consistent with the answer prediction component or not. In Figure 9, we can see that the explanations
are consistent with the incorrectly predicted answer for both
VQA-X and ACT-X. For instance in the bottom-right example, we see that the model attends to a vacuum-like object
and textually justiﬁes the prediction ”vacuuming”. Such
consistency between the answering model and the explanation model is also shown in Table 2 where we see a drop
in performance when explanations are conditioned on predictions (bottom rows) instead of the ground-truth answers
(top rows).
5.5. Usefullness of Multimodal Explanations
In this section, we address some of the advantages of
generating multimodal explanations. In particular, we look
at cases where visual explanations are more informative
than the textual explanations, and vice versa. We also investigate how multimodal explanations can help humans diag-
Q: Does the guy look happy?
… because he has a smile
on his face.
… because the walls are
cracked and dirty.
The activity is
GT: Cello, Sitting
… because she is sitting on a
chair in front of a microphone
and strumming a guitar.
… because he is standing in
a living room and pushing a
vacuum cleaner.
Q: Does this room need to be renovated?
Pred: Guitar, Sitting
GT: Painting Inside House
Pred: Vacuuming
Figure 9: Visual and textual explanations generated by our
model conditioned on incorrect predictions.
nose the performance of an AI system.
Complementary Explanations. Multimodal explanations
can support different tasks or support each other. Interestingly, in Figure 8, we present some examples where visual
pointing is more insightful than textual justiﬁcation, and
vice versa. Looking at the left example in Figure 8, it is
rather difﬁcult to explain “leaning” with language and the
model resorts to generating a correct, yet uninsightful sentence. However, the concept is easily conveyed when looking at the visual pointing result. In contrast, the right example shows the opposite. Looking at only some patches
of the sky presented by the visual pointing result does not
necessarily conﬁrm if the scene is cloudy or not, while it
is also unclear if attending to the entire region of the sky is
a desired behavior. Yet, the textual justiﬁcation succinctly
captures the rationale. These examples clearly demonstrate
the value of generating multimodal explanations.
Diagnostic Explanations. We evaluate an auxiliary task
where humans have to guess whether the system correctly
or incorrectly answered the question. The predicted answer
is not shown; only image, question, correct answer, and textual/visual explanations. The set contains 50% correctly answered questions. We compare our model against the models used for ablations in Table 2. Table 4 indicates that explanations are better than no explanations and our model is
more helpful than models trained on descriptions and also
models trained to generate textual explanations only.
6. Conclusion
As a step towards explainable AI models, we proposed
multimodal explanations for real-world tasks. Our model is
Without explanation
Ours on Descriptions
Ours w/o Attention
Table 4: Accuracy of humans judging if the model predicted
correctly.
the ﬁrst to be capable of providing natural language justi-
ﬁcations of decisions as well as pointing to the evidence in
an image. We have collected two novel explanation datasets
through crowd sourcing for visual question answering and
activity recognition, i.e. VQA-X and ACT-X. We quantitatively demonstrated that learning to point helps achieve high
quality textual explanations. We also quantitatively show
that using reference textual explanations to train our model
helps achieve better visual pointing. Furthermore, we qualitatively demonstrated that our model is able to point to the
evidence as well as to give natural sentence justiﬁcations,
similar to ones humans give.