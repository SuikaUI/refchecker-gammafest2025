IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 17, NO. 4, APRIL 2008
Customizing Kernel Functions for SVM-Based
Hyperspectral Image Classiﬁcation
Baofeng Guo, Steve R. Gunn, R. I. Damper, Senior Member, IEEE, and James D. B. Nelson
Abstract—Previous research applying kernel methods such as
support vector machines (SVMs) to hyperspectral image classiﬁcation has achieved performance competitive with the best available
algorithms. However, few efforts have been made to extend SVMs
to cover the speciﬁc requirements of hyperspectral image classiﬁcation, for example, by building tailor-made kernels. Observation
of real-life spectral imagery from the AVIRIS hyperspectral sensor
shows that the useful information for classiﬁcation is not equally
distributed across bands, which provides potential to enhance the
SVM’s performance through exploring different kernel functions.
Spectrally weighted kernels are, therefore, proposed, and a set of
particular weights is chosen by either optimizing an estimate of
generalization error or evaluating each band’s utility level. To assess the effectiveness of the proposed method, experiments are carried out on the publicly available 92AV3C dataset collected from
the 220-dimensional AVIRIS hyperspectral sensor. Results indicate that the method is generally effective in improving performance: spectral weighting based on learning weights by gradient
descent is found to be slightly better than an alternative method
based on estimating “relevance” between band information and
ground truth.
Index Terms—Hyperspectral image processing, mutual information (MI), remote sensing, support vector machines (SVMs).
I. INTRODUCTION
YPERSPECTRAL sensors simultaneously capture hundreds of narrow and contiguous spectral images from a
wide range of the electromagnetic spectrum. For instance, the
AVIRIS hyperspectral sensor has 224 spectral bands (or images) ranging from visible light to mid-infrared areas (0.4 to
2.5 m). Such a large number of bands or images implies highdimensionality data, presenting several signiﬁcant challenges to
image classiﬁcation – . It is well known that the dimensionality of input space strongly affects performance of many
classiﬁcation methods (e.g., the Hughes phenomenon ). This
requires the careful design of new algorithms that are able to
handle hundreds of such spectral images at the same time minimizing the effects from the “curse of dimensionality.” Kernel
methods, such as support vector machines (SVMs) – , are
less sensitive to the data’s dimensionality and have already
shown superior performance in many machine learning appli-
Manuscript received March 12, 2007; revised January 14, 2008. The associate
editor coordinating the review of this manuscript and approving it for publication was Prof. Dan Schonfeld.
B. Guo, S. R. Gunn, and R. I. Damper are with the School of Electronics and
Computer Science, University of Southampton, Southampton, SO17 1BJ, U.K.
(e-mail: ; ; ).
J. D. B. Nelson is with the Department of Engineering, University of Cambridge, Cambridge CB2 1PZ, U.K. (e-mail: ).
Color versions of one or more of the ﬁgures in this paper are available online
at 
Digital Object Identiﬁer 10.1109/TIP.2008.918955
cations. Recently, SVMs have attracted increasing attention in
remote-sensed multi/hyperspectral communities – . Previous literature applying SVMs to hyperspectral image classiﬁcation , , , has shown competitive performance
with the best available classiﬁcation algorithms. However, the
full potential of SVMs—such as developing customized kernels to integrate a priori domain knowledge—has not been fully
In this paper, spectrally weighted (SW) kernels are proposed
to take better advantage of SVM techniques for hyperspectral
image classiﬁcation. We ﬁrst illustrate a well-known phenomenon in hyperspectral imagery, i.e., the nonuniform distribution
of discriminatory information across different spectral bands.
Based on the AVIRIS 92AV3C dataset, some examples regarding this application-dependent distribution are given in
Fig. 1. To address the characteristic that certain parts of the
spectrum will provide a much richer descriptor for classiﬁcation
than other parts, some approaches such as a straightforward
feature selection , or a block-based approximation
to the covariance matrix can be applied. Here, we propose a
modiﬁcation to the kernel functions that can take into account
the difference of the relative utility of each spectral band by
imposing a series of spectral weights. We subsequently show
that the spectral weights of the SW kernels can be chosen
by a gradient-descent-based automatic tuning that optimizes
the SVMs’ generalization error. By analyzing the relationship
between the automatic tuning and the “relevance” evaluation of
each band (the “relevance” can be seen as an index of importance or utility of a band to classiﬁcation), we further reveal that
the spectral weights can actually be more effectively derived
from the mutual information between the spectral bands and
the ground-truth reference map. This ﬁnding can improve the
approach by reducing computational cost and saving training
The remainder of this paper is organized as follows. After a
brief introduction to the AVIRIS 92AV3C dataset in Section II,
we discuss the nonuniform information distribution across spectral bands in Section III. In Section IV, we propose spectrally
weighted kernels for hyperspectral image classiﬁcation, and in
Section V, we investigate how to use a bound of the generalization error and mutual information to decide the spectral weights.
Experiments are carried out to assess the performance of the
proposed method, which are presented in Section VI. Finally,
we end this paper with conclusions and a proposal for future
II. AVIRIS 92AV3C DATASET
The public AVIRIS 92AV3C hyperspectral dataset has been
researched extensively. The dataset is illustrative of the problem
1057-7149/$25.00 © 2008 IEEE
GUO et al.: CUSTOMIZING KERNEL FUNCTIONS FOR SVM-BASED HYPERSPECTRAL IMAGE CLASSIFICATION
Fig. 1. Nonuniform information distribution. (a) 100 samples of spectral responses for two classes of vegetation in AVIRIS 92AV3C dataset: corn (red
in the online version; dark in the print version) and wheat (green in the online version; light in the print version); the statistical features of spectral re-
ﬂectance values in each spectral band: (b) the means, (c) the standard deviations,
and (d) the Bhattacharyya distances between the two classes.
of hyperspectral image analysis to determine land use. It can
be downloaded from ftp://ftp.ecn.purdue.edu/biehl/MultiSpec/.
Although the AVIRIS sensor collects nominally 224 bands (or
images) of data, four of these contain only zeros and so are discarded, leaving 220 bands in the 92AV3C dataset. At certain
frequencies, the spectral images are known to be adversely affected by atmospheric water absorption. This affects some 20
bands. Each image is of size 145
145 pixels. The data-cube
was collected over a test site called Indian Pine in north-western
Indiana , .
The database is accompanied by a reference map, indicating
partial ground truth, whereby pixels are labeled as belonging to
one of 16 classes of vegetation or other land types (see examples
in Table I). Not all pixels are so labeled (e.g., highway, rail track,
etc.), presumably because they correspond to uninteresting regions or were too difﬁcult to label.
III. NONUNIFORM INFORMATION DISTRIBUTION
Hyperspectral sensors capture signals in a wide spectrum, and
it can be expected that different parts of the spectrum will have
differing representative capabilities for distinguishing the objects of interest. The intrinsic spectral-distinctness of different
NUMBER OF TRAINING AND TESTING PIXELS IN EACH CLASS
objects might not necessarily coincide in the same wavelengths
or bands. In some parts of the spectrum, materials may have a
much more distinctive spectral reﬂectance than in other parts of
the spectrum. Moreover, complex transmission conditions in the
atmosphere, such as water and CO absorption, also play a role
in this phenomenon.
Fig. 1(a) shows 100 samples (pixels) of spectral reﬂectance of
corn and wheat, extracted from the AVIRIS 92AV3C hyperspectral imagery. The
-axis shows the number of spectral bands
(1–220), and the
-axis depicts the pixel value measured in the
different bands. It is seen that substantial overlap between the
two classes occurs in some bands due to the natural similarity
and the variability of spectral reﬂectance. To separate them, we
have to consider their statistical features, such as the means
[see Fig. 1(b)] and standard deviations [see Fig. 1(c)] for each
spectral band. If we ignore the second- or higher-order statistics [i.e., only using the difference between the two classes’s
means, see the dashed line in Fig. 1(b)], the two classes appear to be more separable in the bands 15–35, 80–100, and
120–140 than other heavily overlapped bands such as bands
40–80. Only considering the means of course implies a loss of
information, so a better measurement of statistical separability
is given by the Bhattacharyya distance, which takes account of
the second-order statistic, i.e., variance. The Bhattacharyya distances between the two classes in each spectral band are presented in Fig. 1(d), where the bands 110–150, 165–215 are revised as the higher-value ones due to their lower variances. In
the following discussion of customized kernels, it is not necessary to evaluate the separability of a group of bands, so the
covariances among bands are not calculated.
Fig. 1 clearly shows that irrespective of using the simplest
statistics [see Fig. 1(b) for the mean and Fig. 1(c) for the
standard deviation] or the Bhattacharyya separability measure
[see Fig. 1(d)], their values vary across bands. This indicates
that in hyperspectral imagery the discriminatory information is
nonuniformly distributed across the spectrum. Among the set of
spectral bands, some may contain more useful information for
classiﬁcation than others, and have larger separability indexes
accordingly. Considering that the separability measure gives
an estimate of the probability of correct classiﬁcation, it would
be expected that classiﬁcation performance can beneﬁt from
placing greater emphasis on the more informative bands.
Hence, two different strategies may be considered:
• to select effective spectral bands with spectral management algorithms, such as feature selection by a ﬁltering
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 17, NO. 4, APRIL 2008
• to customize directly the classiﬁer by integrating this
a priori knowledge (i.e., feature selection by an “embedding” approach).
The ﬁrst strategy has been discussed in and ; however, in this research, we focus on the second one. In the SVMbased classiﬁcation framework, a straightforward approach is
to modify the kernel functions by assigning different weights
to different bands, adaptively embedding the amount of useful
classiﬁcation-information contained within that band. To this
end, we propose the use of spectrally weighted kernels to exploit better this speciﬁc characteristic of hyperspectral imagery.
IV. SPECTRALLY WEIGHTED KERNELS
To present the proposed spectrally weighted kernels, we ﬁrst
introduce several relevant SVM formulas. A full introduction to
SVMs can be found in and .
-dimensional hyperspectral data vector
(in this research, this vector can be seen as a pixel with 220
components) with subscript denoting the example number. The
SVM classiﬁer can be represented as
are the classiﬁcation targets (labels);
are Lagrange multipliers;
number of examples; and
is a threshold. Furthermore,
is an appropriate kernel function
which has a corresponding inner product expansion,
. Commonly used functions are polynomials and Gaussian radial
basis functions (RBFs), as follows:
is the order of polynomials and
is a width parameter
characterizing the RBFs.
For a hyperspectral data vector
corresponds to the reﬂectance value of example
in the speciﬁc spectral band
. Generic kernels,
e.g., (2) and (3), regard each component
with equal emphasis
in their projection into feature space. However, Section III has
argued that it is advantageous to moderate the spectral information according to the richness of the descriptor. For example if
the component
is a reﬂectance value in the spectrum or bands
where two classes can be clearly separated (such as the regions
with the higher Bhattacharyya distance in Fig. 1), weighting this
feature to have a larger effect in feature space could improve
classiﬁcation and, similarly, reducing it when it adds little to the
description.
To modify the kernel function so as to reﬂect the above consideration, a weight vector
corresponding
to each spectral band is used to scale each feature
in the hyperspectral data vector before mapping it into feature space. To
simplify notation, we introduce a diagonal matrix
Given this weighting, the SW kernels for a polynomial and an
RBF can be written as
. In this scheme, the weights
have been designed to correspond to each feature component
and a simple diagonal matrix can achieve this goal.
The necessary and sufﬁcient condition for deciding whether
a function is a kernel is given by Mercer’s theorem. It is easy
to prove that the SW kernels still satisfy Mercer’s condition,
since they can be also interpreted as a scaling procedure in input
space, and will not change the kernels’ Mercer condition. Substituting the SW kernels into (1) gives the corresponding spectrally weighted SVMs.
Using the proposed SW kernels, a priori knowledge (e.g., the
nonuniform information distribution) can be incorporated into
the SVM learning procedure. As the weights can be considered
as the part of the SVMs’ model parameters, through tuning to
maximize the estimate of generalization error, it should be possible to achieve better performance. SW kernels can also be seen
as a form of data preprocessing, akin to feature selection. As the
approach can de-emphasize less important features, it implicitly conducts feature selection. When the weights are zero, the
corresponding features will be cut off equivalently [see the distance calculation in Gaussian (5) and polynomial kernels (4)].
The above procedure changes the measurement complexity of
the classiﬁers; then, according to (i.e., the Hughes phenomenon), it may affect classiﬁcation accuracy for over-dimensional
data, given that the number of training samples is ﬁnite (as in the
case of remote-sensing applications).
V. ESTIMATION OF SPECTRAL WEIGHTS
For SW kernels, a key problem is to choose the spectral
. On the one hand, SW kernels are motivated by
the nonuniform information distribution across bands, so the
spectral weights are expected to reﬂect the relative inﬂuence
(namely the relevance to classiﬁcation) of each band to the
the kernel values; on the other hand, the change of the kernel
value by imposing such weights should improve classiﬁcation
accuracy, i.e., minimizing the error
examples. It is known that the latter goal can be achieved by
optimizing a bound of generalization error
One of the well-known upper bounds of
is the ratio of radius
to margin,
is the radius of a sphere enclosing
mapped training examples
is the margin between the hyperplane and the closest
 , . This bound may be intuitively understood as
follows: the radius
indicates the compactness of data, and
the margin
implies the distance of two classes. It is similar to
the Bhattacharyya separability measure, which is calculated by
means (akin to the distance of two classes) and variances (akin
to the compactness of data). This theorem also justiﬁes the
idea of maximizing the SVM margin
, or equivalently
minimizing
. The factor regarding the radius may
GUO et al.: CUSTOMIZING KERNEL FUNCTIONS FOR SVM-BASED HYPERSPECTRAL IMAGE CLASSIFICATION
be implicitly implemented by the choice of kernels and their
parameters.
A. Scheme Based on Gradient Descent Algorithm
Based on the above discussion, if the spectral weights can
be chosen to increase the SVM margin, i.e., to lower the radius-margin bound, it becomes possible to improve the classiﬁcation accuracy. Thus, the ﬁrst scheme for choosing the spectral
weights is proposed as follows – .
According to SVM theory, the margin
can be derived as
-dimensional vector perpendicular to the separating hyperplane, given by
From (6), maximizing
can be achieved by minimizing
From (8), it is seen that given ﬁxed
, the derivative of
is a spectral weight corresponding to the th component of a hyperspectral data vector (i.e., the th spectral image).
Thus, the choice of spectral weights can be implemented by
using a gradient descent algorithm as follows:
controls the searching speed, and
is the iteration step.
In this scheme, weights
are updated step-by-step, and a (local)
minimum of
will be found after a number of iterations.
B. Scheme Based on Relevance Evaluation
The weighting scheme shown in Section V-A is based on the
gradient descent algorithm, which needs a time-consuming iterative updating [see (10)], and usually it will not ﬁnd the global
optimal solution. On the other hand, we know that the spectral
weights also reﬂect the relevance of each spectral band to the
classiﬁcation. Thus, an alternative weighting approach can be
conceived as follows. First, the kernel in (5) can be rewritten as
denote the number of training examples, and
is the band number. The derivative of
It can be shown that the value of (12) for different bands
is decided by the term
, given the same initialization
Combining (9) and (12), we get
including all coefﬁcients except
When the examples belong to the same class, i.e.,
will change little because
. When the examples belong to different classes, i.e.,
The expectation of variable
According to the Bhattacharyya distance, given the same difference of means, the bands with the smaller variances will tend
to have higher separability values (i.e., they are “good” bands,
probably with lower classiﬁcation errors). This assumption can
be evidenced by observing the following 1-D Bhattacharyya
coefﬁcient:
So, roughly speaking, given a certain assumption (e.g., the
same difference of means), the “good” bands should have lower
variances, i.e., the smaller
. Thus, we
is a measure of the level of “relevance” or “goodness”
of a band to classiﬁcation. Combining (14) and (17), we get
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 17, NO. 4, APRIL 2008
Equation (18) shows that in each step of automatic tuning,
the change of the spectral weight
is related to the level of its
relevance to classiﬁcation, i.e.,
. Therefore, to obtain a lower
bound of generalization error, the spectral weights can also be
found through the evaluation of relevance
, as an alternative
to the gradient descent approach discussed in Section V-A.
Considering that gradient descent is usually time-consuming, a
weighting scheme based on relevance evaluation is attractive.
Among many possible relevance measures, we propose the use
of mutual information (MI) to estimate each band’s level of relevance. The advantages of employing MI are its close relation
to Bayes classiﬁcation errors and effective implementation
 , .
Given two random variables
with marginal probability distributions
, and joint probability distribution
, the mutual information between
is deﬁned as
According to Shannon’s information theory, entropy measures information content in terms of uncertainty, and is deﬁned
From (19) and (20), it is not difﬁcult to ﬁnd that mutual information is related to entropies by the following equations:
are the entropies of
is their joint entropy; and
are the conditional entropies of
, respectively.
The joint and conditional entropies can be written as
Treating the spectral images and the corresponding reference
map as random variables, MI can be used to estimate the dependency or relevance between them. In detail, we can treat
each spectral band’s pixels as samples of the random variable
with possible continuous reﬂectance values
class category as variable
with discrete vegetation labels
. Thus, MI between
can be evaluated
as follows:
Fig. 2. Comparison of the spectral weights obtained by the two schemes:
(a) the weights found by the gradient-descent tuning; (b) the weights decided
by mutual information; (c) the derivatives of kwk in one of the iterations.
Since the reference map implicitly deﬁnes the required classi-
ﬁcation result, MI measures the relevance of each spectral band
to the classiﬁcation objective. Using (21), the mutual information between each of the 220 spectral images (or bands) and
the corresponding reference map accompanying the 92AV3C
dataset was calculated as shown in Fig. 2(b).
By comparing this MI curve to the examples of AVIRIS images, we may verify the agreement between the relevance level
and the MI values. It has been found that the bands most
similar to the reference map (i.e., with higher relevance) are
those having higher values of MI. The MI curve also reveals
clearly the effect of atmospheric water absorption, giving the
lowest MI values in bands 104–108 and 150–163 at precisely
those frequencies where absorption occurs . In this particular example, it is seen that the MI of a spectral band with
respect to the reference map is consistent with visual impressions regarding the relevance or relative importance of each
spectral band to classiﬁcation. Moreover, it can be seen that the
overall shapes of the MI curve and the Bhattacharyya distance
[Fig. 1(d)] are very similar, indicating an agreement of MI with
another commonly used (but more computationally expensive)
separability measure.
Fig. 2 further compares the spectral weights obtained by the
above two schemes; Fig. 2(a) shows the weights calculated
by the gradient-descent tuning, and Fig. 2(b) is the result
based on mutual information estimation. It is seen that the
two sets of weights have very similar overall shape across
different bands, suggesting the comparable effect of the two
schemes in band-utility evaluation. Moreover, it is found that
the derivatives of
are inversely proportional to the MIs,
in general. Fig. 2(c) shows an example of a group of derivatives
obtained in one of the iterations of (10). Recall our previous
GUO et al.: CUSTOMIZING KERNEL FUNCTIONS FOR SVM-BASED HYPERSPECTRAL IMAGE CLASSIFICATION
discussion in (11)–(18): In the gradient descent algorithm, the
weights are usually set up as the same coefﬁcients over the
whole bands in the initialization step, and will be gradually
updated by subtracting the derivatives. Given the relation
between the derivatives and the mutual information (i.e., the
inverse proportion), the weights will ﬁnally converge to a result
similar to the mutual information. Thus, Fig. 2 suggests that
MI can effectively encode the relevance of spectral bands to
classiﬁcation and be employed as a spectral weight.
VI. EXPERIMENTS
Following the two weighting schemes discussed in Section V, SVMs based on SW kernels are implemented to test the
proposed method. The performance of the proposed method is
compared with a standard SVM with no spectral weighting of
the kernel as adopted in , , , etc., on the AVIRIS
92AV3C dataset.
In the AVIRIS 92AV3C dataset, the seven most numerous
classes are chosen as the testing objects, accounting for 80.64%
of all 16-class pixels. The class labels from the reference map
accompanying the dataset were utilized for supervised training.
Among the labeled pixels, 20% of them from each class were
randomly chosen as the training set, with the remaining 80%
forming the test set on which performance was assessed (see
Table I). This was repeated ﬁve times to allow an estimate of the
error in this sampling process. The performance measurement
adopted assessed the classiﬁcation error of the proposed method
on a held-out set.
In the experiments, the seven classes are named as
respectively (see Table I). Since SVMs are inherently binary
(two-class) classiﬁers, it is more straightforward to evaluate
performance based on each class pair. Thus,
classiﬁers were constructed based on each class-pair, named as
, respectively. Moreover, it is more
effective to learn the weights by using the two-class SVMs
since the different class-pairs may have different spectral characteristics. So, in this case, the pixels from other classes will
not affect the classiﬁcation associated with the classiﬁer that
was not trained from those examples. The kernel function used
here is the Gaussian RBF [see (5)]. The kernel parameter
the penalty parameter
were tested between
a validation procedure using the training data and 0.4 and 60,
respectively, were chosen as suitable values.
Fig. 3 shows the variation of classiﬁcation accuracy as
a function of iteration step in the weight-learning scheme
discussed in Section V-A. It is seen that with each round of
weight updating, the classiﬁcation error changes accordingly,
because of the gradual optimization of the generalization error
bound. Using customized kernels, the classiﬁer tuning or data
preprocessing (e.g., scaling) required for high-dimensionality data is incorporated into the SVM learning procedure.
Thus, by this embedding approach, the weights can not only
de-emphasize features, but also work as the part of the SVMs’
model parameters. By tuning these model parameters to maximize the estimate of the generalization error, it becomes
possible to achieve a better performance. The results from
four 92AV3C-based binary trials in Fig. 3 show the reduced
classiﬁcation errors, providing empirical evidence to support
Fig. 3. Classiﬁcation accuracy as a function of iteration number in the gradient descent learning; for classiﬁers (a) C
(Corn-notill versus Corn-min),
(Corn-notill versus Grass/Trees), (c) C
(Corn-notill versus Soybeans-notill), and (d) C
(Corn-notill versus Soybeans-min), respectively.
the above argument. In practical applications, the accuracy
veriﬁcation illustrated in Fig. 3 is not necessary, and lower
classiﬁcation errors can be obtained by adjusting the threshold
in the gradient descent algorithm.
Results using the two weighting schemes are shown in
Table II. From Table II, it can be seen that the methods based
on SW kernels outperformed the unweighted method in the
majority of the 21 classiﬁers. The improvement is especially
signiﬁcant when the two classes are difﬁcult to differentiate
(i.e., their classiﬁcation errors are relatively higher). For example, the vegetation classes Corn-notill
, Corn-min
Soybeans-notill
, Soybeans-min
, and Soybean-clean
are the most similar classes in the scene, and their classiﬁcation errors are usually larger than 10% (see the numbers listed
in the rows
of Table II). From
the point of view of “spectral signature,” it can be expected
that these confusable vegetation classes will show considerable
similarity in their spectral reﬂectances at a global level. Correspondingly, the spectral difference between them will only
appear in some particular wavelengths, and the bands therein
will dominate the overall discriminatory capability. Apparently,
in this case the effect caused by the nonuniform information
distribution is quite substantial and the SW kernels, tailor-made
for this scenario, become successful.
Table II also shows that there are several exceptions, e.g.,
the classiﬁers
, where the weighting
schemes did not successfully reduce the classiﬁcation errors.
Almost all of these exceptions belong to the scenario where
two classes are relatively easier to separate. For example,
correspond to classiﬁcation of the
vegetation pairs Corn-notill versus Grass/Trees, Soybean-clean
versus Grass/Trees, and Grass/Trees versus Woods, respectively. Compared to the similar crops mentioned previously,
these vegetation classes are very distinct. Therefore, it can be
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 17, NO. 4, APRIL 2008
COMPARISON OF CLASSIFICATION ERROR (%)  SAMPLING ERROR (STD); RBF KERNEL,  = 0:4; C = 60
expected that signiﬁcant spectral differences will appear across
a wide range of the spectrum, overshadowing the necessity to
emphasize a particular subset of bands as the SW kernels are
designed to do. As a result, the proposed scheme may lose its
efﬁcacy, but also carries the risk of over-ﬁtting incurred by
introducing extra parameters. Although no improvement has
been made in these small number of exceptions, the overall
effectiveness of this method in the majority of classes is still
encouraging.
Comparing the two weighting schemes, the gradient-based
approach appears to be slightly better than that based on mutual information. This is understandable because the former uses
different sets of weights for different classiﬁers, which are individually optimized in each SVM’s learning procedure. On the
contrary, the latter scheme uses a single set of weights, i.e., the
MI values, for all 21 classiﬁers. However, the MI-based scheme
is still a useful alternative as the calculation of MI is much faster
than using gradient descent.
VII. CONCLUSION
In this paper, we have proposed an extension to the
SVM-based method for hyperspectral image classiﬁcation
using spectrally weighted (SW) kernels. This extension is
motivated by the observation that the useful information for
classiﬁcation is not evenly distributed among each spectral
band. Within the SVM framework, SW kernels can be conveniently constructed by highlighting the informative bands in the
kernel mapping. We have shown that it is possible to improve
the upper bound of classiﬁcation error through learning the
spectral weights in the customized kernels. Further research
revealed that the mutual information between a spectral band
and the ground truth can also be used to design spectral
weights, resulting in a signiﬁcant saving of computational cost.
Experimental results showed that, at least for the limited binary
trials based on the AVIRIS 92AV3C dataset, the classiﬁcation
performance can be improved to some extent by a spectral
customization of the kernels using either a gradient-descent
tuning or a mutual information criterion. Further work could
explore the possibility of avoiding the over-ﬁtting incurred by
the multiple adjustable parameters and testing the algorithms
on other labeled multiclass datasets.
ACKNOWLEDGMENT
The authors would like to thank the reviewers for their valuable comments which have resulted in a number of improvements in the paper.