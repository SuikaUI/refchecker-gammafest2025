ORIGINAL ARTICLE
Classiﬁcation of microarray cancer data using ensemble approach
Sajid Nagi • Dhruba Kr. Bhattacharyya
Received: 8 January 2013 / Revised: 20 May 2013 / Accepted: 31 May 2013 / Published online: 18 June 2013
 Springer-Verlag Wien 2013
An ensemble of classiﬁers is created by combining predictions of multiple component classiﬁers for
improving prediction performance. In this paper, we conduct experimental comparison of J48, NB, IBK on nine
microarray cancer datasets and also analyze their performance with Bagging, Boosting and Stack Generalization.
The experimental results show that all ensemble methods
outperform the individual classiﬁcation methods. We then
present a method, referred to as SD-EnClass, for combining
classiﬁers from different classiﬁcation families into an
ensemble, based on a simple estimation of each classiﬁer’s
class performance. The experimental results show that the
proposed model improves classiﬁcation accuracy, in comparison to simply selecting the best classiﬁer in the combination. In the second stage, we combine the results of our
proposed method with the results of Boosting, Bagging and
Stacking using the combining method proposed, to obtain
results which are signiﬁcantly better than using Boosting,
Bagging or Stacking alone.
Bagging  Boosting  Cancer datasets 
Classiﬁer  Ensemble  Meta-ensemble  Microarray data 
1 Introduction
Recent developments in the area of DNA microarray
technology, such as classiﬁcation, clustering, biclustering,
triclustering and feature selection techniques have
made it possible for scientists to monitor the expression
level of thousands of genes with a single experiment
 . This helps in
(i) classifying diseases according to varying expression
levels in normal and tumor cells, (ii) uncovering gene–gene
relationship, and (iii) identifying genes responsible for the
development of diseases.
Many methods have been proposed in microarray classiﬁcation, including subspace clustering and ensemble methods such as Bagging
and Boosting , for
classiﬁers
historical
microarray
expression data to be used for classifying unknown data.
An ensemble of classiﬁers is a set of classiﬁers whose
individual predictions are combined in some way to classify new examples, with an intention of improving classi-
ﬁcation accuracy over an average classiﬁer. Since it is not
known a priori which classiﬁer is best for a particular
classiﬁcation problem, an ensemble reduces the risk of
selecting a poorly performing classiﬁer.
1.1 Existing work
The study on ensemble-based classiﬁers has expanded
rapidly in recent times and researchers have used many
terms to describe the combining models involving different
learning algorithms. Elder and Pregibon used the
term ‘Blending’, Dietterich called it ‘Ensemble of
Department of Computer Science, St. Edmund’s College,
Shillong 793001, Meghalaya, India
e-mail: 
D. Kr. Bhattacharyya (&)
Department of Computer Science and Engineering, Tezpur
University, Napaam 784028, Assam, India
e-mail: 
Netw Model Anal Health Inform Bioinforma 2:159–173
DOI 10.1007/s13721-013-0034-x
Classiﬁers’, Steinberg termed it
as ‘Committee of Experts’, while Breiman referred
to it as ‘Perturb and Combine (P&C)’. Several other terms
also can be found in the literature .
However, the concept of combining models is actually
quite simple: train several models using the same dataset,
or from samples of the same dataset and combine the
output predictions, typically by voting (for classiﬁcation
problems) or by averaging output values (for estimation
problems) among the other known combining methods.
In view of the signiﬁcant improvement in the classiﬁcation accuracy through combining classiﬁers, Breiman
 introduced Bagging, which combines outputs from
decision tree models generated from bootstrap samples
(with replacement) of a training dataset. Models are combined by simple voting. Freund and Schapire 
introduced Boosting, an iterative process of weighing more
heavily the incorrectly classiﬁed cases by decision tree
models, and then combining all the models generated
during the process. ARCing is a form of
boosting that, like boosting, weighs incorrectly classiﬁed
cases more heavily, but instead of the Freund and Schapire
 formula for weighing, weighted random samples are
drawn from the training data. Wolpert used
regression to combine neural network models which was
later known as Stacking. These are just a few of the wellknown algorithms currently described in the literature, and
many more methods have been developed by researchers as
well. A survey by Kilic¸ and Tan provides an insight
into algorithms that can handle binary classiﬁcation
1.2 Discussion and motivation
Based on our limited survey it has been observed that:
In recent years, several innovative classiﬁers have been
introduced for real-life data classiﬁcation with high
detection rate. However, the performances of most of
these existing classiﬁers are application dependent.
The assumption of most classiﬁers that the real-life data
has a high resemblance to the training data may not be
true in reality.
Lack of appropriate or sufﬁcient training data is a major
cause of poor performance of most classiﬁers.
Combining the outputs of several classiﬁers may reduce
the risk of selecting a poorly performing classiﬁer.
The errors made while classifying instances by one
classiﬁer are generally averaged out by the correct
classiﬁcation of another classiﬁer, so that the overall
classiﬁcation accuracy is improved.
Base classiﬁers should be diverse in nature so that a
ﬁnal unbiased decision can be taken.
This work aims to provide an empirical study on the
pros and cons of various existing supervised classiﬁers and
their ensembles. It introduces an ensemble method, SD-
EnClass, based on the scores of the existing ensemble
approaches. The effectiveness of the proposed SD-EnClass
has been established over nine cancer datasets from Kent
Ridge Biological Dataset Repository .
1.3 Organization of the paper
The rest of the paper is organized as follows: Sect. 2 provides the background of the work, existing approaches of
ensembles and ensemble combination methods. Section 3
is on related works and Sect. 4 describes the experimental
setup. In Sect. 5, we present our proposed model of
ensemble and analyze its performance by comparing with
three other ensemble techniques. To improve the accuracy
rate further, we propose a meta-ensemble that works on our
ensemble and two other competing ensembles. The metaensemble and its experimental evaluation are presented in
Sect. 6. Finally, the concluding remarks and future research
directions are included in Sect. 7.
2 Background of the work
A classiﬁer e is a function that maps a vector of attribute
values x (also called example) to classes in C = {C1,
C2,…, Cn}. An ensemble classiﬁer consists of a set of
classiﬁers E = {e1, e2,…, ek} whose output is dependent
on the outputs of the constituent classiﬁers (Bostro¨m et al.
The performance of an ensemble mostly depends on the
individual performance of the classiﬁers present in the
ensemble. Two key requirements of the classiﬁers forming
the ensemble are:
diversity of classiﬁers in nature
accuracy in the classiﬁer predictions.
Similar classiﬁers usually make similar errors, so
forming an ensemble with similar classiﬁers would not
improve the classiﬁcation rate. Also, presence of a poorly
performing classiﬁer may cause performance deterioration
in the overall performance. Similarly, presence of a classiﬁer that performs much better than all of the other
available base classiﬁers may cause degradation in the
overall performance. Another important factor is the
amount of correlation among the incorrect classiﬁcations
made by each classiﬁer. If the consistent classiﬁers tend to
misclassify the same instances, then combining their results
will have no beneﬁt. In contrast, a greater amount of
independence among the classiﬁers can result in errors by
S. Nagi, D. K. Bhattacharyya
individual classiﬁers being overlooked when the results of
the ensemble are combined.
2.1 Construction of ensembles
The task of constructing an ensemble can be broken down
into two subtasks: (i) selection of a diverse set of base level
models or classiﬁers with consistently acceptable performance and (ii) appropriate combination of their predictions
with due weightage. Next we discuss these two subtasks
along with some other important factors.
Classiﬁer selection In supervised classiﬁcation, classiﬁers are trained to become experts in some local
area of the total feature space. For each example (say
x), a classiﬁer is identiﬁed which is most likely to
produce the correct classiﬁcation label, as shown in
Fig. 1. The output of the classiﬁers identiﬁed as the
best for a given classiﬁcation problem is selected.
Usually, the input sample space is partitioned into
smaller areas and each classiﬁer (say Hi) learns the
example in each area. It is similar to the divide and
conquer approach. Here, multiple local experts may
be nominated to make the decision. Finally, a subset
of classiﬁers performing consistently well with high
classiﬁcation accuracy for several real-life datasets is
selected as the base classiﬁers.
Classiﬁer fusion Here, the outputs of many different
classiﬁers are mixed instead of extracting a single
best classiﬁer. Each classiﬁer in the ensemble has
some knowledge of the entire feature space and tries
to solve the same classiﬁcation problem using
different methods based on different training sets,
classiﬁers or parameters. The ﬁnal output is determined by fusing the decisions of the individual
classiﬁers as shown in Fig. 2. All classiﬁers in the
ensemble are trained over the entire feature space.
2.2 Existing methods
Several methods have been developed for the construction
of ensembles. Some methods are general and they can be
applied to any learning algorithm, whereas others are
speciﬁc to particular algorithms. Next, we report some of
the basic approaches for the construction of ensembles.
Different classiﬁer models For effectiveness we can
use several types of learning algorithms from different backgrounds, e.g., decision tree, neural network,
nearest neighbor, etc. However, we can use the same
classiﬁer with a slight change in the user-deﬁned
parameters, leading to signiﬁcant variation in classi-
ﬁcation results.
Different feature subsets Classiﬁers are built using
different subsets of features of the training dataset. It
works only when some redundancy in features is
present on the training dataset. Both the deterministic
and random approaches can be used for selecting
different feature subsets of input data. In the deterministic approach, a prior knowledge about the input
data is required, whereas the random approach uses
random subspace method for selecting the different
feature subsets. Here, each classiﬁer is selected on a
random space, and a feature in the subset is selected
with a probabilistic approach. Random forests, which
uses decision trees, is an example of this method.
However, a common problem with this random
method is that some subspaces lack information and
as a result may not give good performance.
Different training sets One learning algorithm is run
on different random sub-samples of training data to
produce different classiﬁers. It works well for
unstable learners, i.e., output classiﬁer undergoes
major changes, given only small changes in training
data. Random sub-samples of the training data can be
generated using re-sampling and re-weighing. Bagging and wagging use re-sampling, while Boosting
and ARCing uses re-weighing of the training dataset.
Different combination schemes Different classiﬁers
are trained on the training data and their outputs are
combined using different combination schemes like
majority voting, algebraic combiners, etc., to get the
ﬁnal output. Next we discuss the basics of various
combination methods and their effectiveness.
Fig. 1 Classiﬁer selection
Fig. 2 Classiﬁer fusion
Ensemble approach to classify microarray cancer data
2.3 Ensemble combination method
An ensemble of classiﬁers can be trained simply on different subsets of the training data, different parameters of
the classiﬁers, or even with different subsets of features as
in random subspace models. The classiﬁers can then be
combined using one of the combination rules. Some of
these combination rules operate on class labels only,
whereas others need continuous outputs that can be interpreted as support given by the classiﬁer to each of the
classes. Xu et al. deﬁne three types of base model
outputs to be used for classiﬁer combination.
Abstract-level output, where each classiﬁer outputs a
unique class label for each input pattern.
Rank-level output, where each classiﬁer outputs a list
of ranked class labels for each input pattern.
Measurement-level output, where each classiﬁer outputs a vector of continuous-valued measures that can
represent estimates of class posterior probabilities or
class-related conﬁdence values that represent the
support for the possible classiﬁcation hypotheses.
Figure 3 shows different types of ensemble combination
methods and subsequently describes each of them brieﬂy.
2.3.1 Combining class labels
If the outputs of the combined classiﬁers are in the form of
class labels, then they can be combined using any of the
following methods .
Majority voting In this approach, each classiﬁer has
equal vote and the most popular classiﬁcation is the
one chosen by the ensemble as a whole .
The most basic variant is the plurality vote, where the
class with the most votes wins. In general, we have
three versions of majority voting as given below.
One on which all classiﬁers agree (unanimous voting).
Predicted by at least one more than half the number of
classiﬁers (simple majority).
One that receives the highest number of votes,
whether or not the sum of those votes exceeds 50 %
(plurality voting or just majority voting).
However, none of these versions is free from the common limitations of majority voting.
Weighted majority voting If we have evidence that
certain experts are more qualiﬁed than others, weighing the decisions of those qualiﬁed experts more
heavily may further improve the overall performance.
Let us denote the decision of hypothesis ht on class cj
as dt,j such that dt,j is 1 if ht selects cj and 0 otherwise.
Further, we assume that the classiﬁers are assigned
weights according to their performances such that
classiﬁer ht is assigned weight wt.
The total weights received by a class is the sum of the
product of the weights of the classiﬁers, wt and their
respective decisions, dt,j. The class receiving the highest
weighted vote is selected as the ﬁnal decision of the
ensemble. Thus, according to this assumption, an ensemble
will select class J if the following holds:
ð Þ ¼ maxC
2.3.2 Combining continuous outputs
Continuous output is interpreted as the degree of support
given to a class by a classiﬁer, usually accepted as the
value of the posterior probability for that class . Algebraic combiners are used to merge the decisions of the classiﬁers in continuous output format, following the convention as given above:
ð Þ ¼ support given by the tth classifier to the jth class
for the instance x
wj ¼ weight of the jth classifier
T ¼ total number of classifiers
ð Þ ¼ total support for the jth class for instance x
Sum rule The total support for a class is calculated as
the sum of the supports given to that class by all the
classiﬁers. After calculating the total supports for all
the classes, the class with the highest support is
selected as the ﬁnal output.
Mean rule This rule is similar to the sum rule but the
total support is normalized by 1/T (T = number of
classiﬁers).
Weighted Majority
Ensemble Combination
Combining Class
Combining Continuous
Combination
Fig. 3 Ensemble combination methods
S. Nagi, D. K. Bhattacharyya
Weighted sum rule By this rule, the support for a
class is calculated by the sum of the product of the
classiﬁers’ weight and their respective supports.
classiﬁers to a particular class are multiplied to
obtain the ﬁnal support for that class. This rule is
very sensitive to the pessimistic classiﬁers as a low
support can remove any chance of getting selected by
that class.
Maximum rule As the name suggests, this rule selects
the maximum of all the supports of the different
classiﬁers for a particular class.
ð Þ ¼ maxT
t¼1fdt;j x
Minimum rule This rule selects the minimum of all
the supports of the different classiﬁers for a particular
ð Þ ¼ minT
t¼1fdt;j x
Median rule This rule selects the median of the
supports of the different classiﬁers for a particular
ð Þ ¼ medianT
t¼1fdt;j x
Generalized mean rule Many of the above rules are
in fact special cases of the generalized mean rule
which is as follows:
The methods of combination described above are
summarized in Table 1.
2.3.3 Other combination rules
Few other combination rules not listed above are : (i) Borda count: takes the rankings of the class
consideration;
space: lists the most common correct classes in a lookup
table for all probable class combinations given by the
classiﬁers; (iii) decision templates: computes a similarity
measure between the current decision proﬁle of the
unknown instance and the average decision proﬁles of
instances from each class; and (iv) Dempster–Schafer
rule: computes the plausibility based belief measures for
each class.
2.4 Discussion
Based on our empirical study on these various combination
methods, it has been observed that:
Most classiﬁers are application dependent and inconsistent in performance over majority of datasets.
An ensemble method with an appropriate set of base
classiﬁers is found to perform better than the individual
classiﬁers.
Appropriate training samples can improve the performance of the classiﬁers signiﬁcantly.
Performance of the majority of combination methods is
affected by the presence of outliers.
The sum rule or the weighted majority voting gives
good results.
A signiﬁcant improvement in classiﬁcation accuracy
becomes possible with a meta-ensemble, i.e., ensemble
of ensembles, if the base ensemble methods perform
consistently well and the combination method is
carefully selected.
3 Related work
In the past decade, many researchers have devoted their
efforts to the study of ensemble decision tree methods for
microarray classiﬁcation. Ensemble decision tree methods
combine decision trees generated from multiple training
datasets by re-sampling the training dataset. Bagging,
Boosting and Stacking are some of the well-known
ensemble methods in the machine learning ﬁeld.
We have selected three classiﬁers, i.e., J48 (decision
tree), IBK (instance-based learner) and Naive Bayes
(probabilistic) as the base classiﬁers based on the following
All these classiﬁers performed consistently well over
several real-life UCI datasets.
They are from three different classiﬁcation algorithms
They belong to three different states, i.e., stable,
unstable and probabilistic.
In this section, we highlight the classiﬁers and the
ensembles used in our study.
Ensemble approach to classify microarray cancer data
3.1 Base classiﬁers
Decision tree J48 implements Quinlan’s C4.5 algorithm
 for generating a pruned or unpruned C4.5
tree. Decision trees are built from a set of labeled training
data using the concept of information entropy. Based on
each attribute of the data, a decision can be arrived at by
splitting the data into smaller subsets.
J48 examines the normalized information gain (difference in entropy) that results from choosing an attribute for
splitting the data. To arrive at a decision, the attribute with
the highest normalized information gain is used. Then the
algorithm recursively moves on to the smaller subsets and
the splitting procedure stops if all instances in a subset
belong to the same class. Next, a leaf node is created in the
decision tree indicating the class. In case none of the features give any information gain, then J48 creates a decision
node higher up in the tree using the expected value of the
J48 can handle both continuous and discrete attributes,
training data with missing attribute values and attributes
with differing costs. It also provides an option for pruning
trees after creation.
Instance-based classiﬁers such as the
kNN classiﬁer operate on the assumption that classiﬁcation
of unknown instances can be done by relating the unknown
to the known based on some distance/similarity function.
The probability of two instances far apart in the instance
space (as deﬁned by the appropriate distance function)
belonging to the same class is less likely than two closely
situated instances.
The algorithm computes the k closest neighbors of an
instance of an unknown class and the class is assigned by
voting among those neighbors. To prevent ties, the value of
k is taken as an odd number for binary classiﬁcation. Plurality voting or majority voting is used for multiple classes
but the latter can sometimes result in no class being
assigned to an instance, while the former can result in
classiﬁcations being made with very low support from the
neighborhood. Another option is to weigh each neighbor by
an inverse function of its distance to the instance being
classiﬁed.
3.1.3 Naı¨ve Bayes (NB)
Naı¨ve Bayes classiﬁer is a
probabilistic
assuming a strong (Naı¨ve) independence of attributes.
Naı¨ve Bayes classiﬁers take into consideration that all
attributes (features) independently contribute to the probability of a certain decision. It analyzes independently all
the attributes of the data with equal importance and hence
Naı¨ve Bayes classiﬁers can be trained very efﬁciently in a
supervised learning setting.
Table 1 Summary of combination rule
Effectiveness
Class label combination
Majority voting
ð Þ ¼ maxC
Gives average performance when majority does not give
accurate prediction
Weighted majority
ð Þ ¼ maxC
Performs well only if the weights of the classiﬁers are
assigned precisely
Continuous output
combination
Gives average performance when majority does not give
accurate prediction
Weighted sum rule
Performs well only if the weights of the classiﬁers are
assigned precisely
Performance is signiﬁcantly affected by outliers
Product rule
Sensitive to low probability value
Maximum rule
ð Þ ¼ maxT
t¼1fdt;j x
Chooses the most optimistic value
Minimum rule
ð Þ ¼ minT
t¼1fdt;j x
Performance is signiﬁcantly affected by outliers
Median rule
ð Þ ¼ medianT
t¼1fdt;j x
Performance is signiﬁcantly affected by outliers
Generalized rule
Affected by outliers
S. Nagi, D. K. Bhattacharyya
3.2 Ensemble methods
3.2.1 Bagging
Bagging is an ensemble method where
the same learning algorithm is used for different training
datasets to obtain different classiﬁers. The diversity
amongst the training datasets is achieved by a bootstrap
technique used to re-sample the training dataset. As shown
in Fig. 4, each classiﬁer is then trained on a re-sample of
instances, which then assigns a predicted class to this set of
instances. The individual classiﬁers’ predictions (having
equal weightage) are then combined by taking majority
3.2.2 Boosting
Boosting uses a re-sampling
technique different from Bagging. In this case, a new
training dataset is generated according to its sample distribution. The ﬁrst classiﬁer is constructed from the original
dataset where every sample has an equal weight (Fig. 5). In
the succeeding training dataset, the weight is reduced if the
sample has been correctly classiﬁed, otherwise it is
increased if the samples are misclassiﬁed. In the committee
decision, a weighted voting method is used so that a more
accurate classiﬁer is given greater weightage than a less
accurate classiﬁer.
3.2.3 Stacked generalization
Stacked generalization (or stacking), proposed by Wolpert
 , performs its task in two phases (Fig. 6): (i) the
layer-1 base classiﬁers are trained using bootstrapped
samples of the level-0 training dataset and (ii) the outputs
of layer-1 are then used to train a layer-2 meta-classiﬁer.
The purpose is to check whether the training data have
been properly learned. For example, if a particular classi-
ﬁer incorrectly learns a certain region of the feature space
and hence consistently misclassiﬁes instances coming from
that region, then the level-2 classiﬁer may be able to learn
this behavior, and along with the learned behaviors of other
classiﬁers, and correct such improper training. Polikar
 proposed to use class probabilities rather than class
labels as the output in the level-1 dataset, so as to improve
the stacking performance.
3.3 Discussion
The selection of the base classiﬁers has been done
based on the fact that they are diverse in nature and
have been established over the 2 class problems.
To eliminate the biasness of individual classiﬁers, the
ensemble method is adopted so that the overall
classiﬁcation accuracy is improved and also the errors
of one classiﬁer are averaged out by the correct
classiﬁcation of another classiﬁer.
Bagging and Boosting of each of the base classiﬁers
is done so as to derive the maximum beneﬁt in terms
of classiﬁcation accuracy.
The individual ensemble approaches are also not
totally free from their limitations, so an approach of
creating an ensemble of ensembles is
found to be suitable to maximize classiﬁcation
Prediction
Classifier 1
Classifier 2
Classifier K
Original Training Data
Re-Sampling
Learning Algorithm
Fig. 4 Block diagram of multiple classiﬁer systems based on
Bagging. Di class prediction by the ith classiﬁer
Original Training
Prediction
Base Classifier
Iteration 1
Weights Modified
Base Classifier
Iteration 2
Weights Modified
Base Classifier
Iteration K
Fig. 5 Block diagram of multiple classiﬁer systems based on
Boosting. Di class prediction by the ith classiﬁer
Base Classifiers
Meta Classifiers
Prediction
CLASSIFIER
(LEVEL - 0)
Fig. 6 Multiple classiﬁers based on stack generalization
Ensemble approach to classify microarray cancer data
3.4 Motivation for a new ensemble method
Based on our limited experimental study, it has been
observed that:
Algorithms from different classiﬁcation families can be
used with appropriate combination method to form an
effective ensemble.
To reduce error rates, it is a necessary condition to
combine the relatively uncorrelated output predictions.
With highly correlated output predictions, there is little
scope for the reduction in error, as the committee of
experts has no diversity to draw from.
A strong reason for combining models across different
algorithm families can be stated as—different algorithms will provide uncorrelated output estimates
because of their varied classiﬁcation functions.
Abbott showed considerable differences in classiﬁer performance class by class—information that is
clear, once classiﬁer is obscure to another. Since it is
difﬁcult to know a priori which algorithm(s) will produce
the lowest error for each domain (on unseen data),
combining models across algorithm families mitigates
that risk by including contributions from all the families.
The motivation is to devise a cost-effective ensemble
method, SD-EnClass, not inﬂuenced by the biasness of the
base classiﬁers and which shows consistently improved
detection rates compared to the base classiﬁers in the
combination (Table 2).
4 Experimental design methodology
Tenfold cross-validation is used in this experiment where the
dataset is partitioned into ten sets of equal size. Nine of these
sets are combined and used for training, while the remaining
one is used for testing. Then the process is repeated with nine
different sets combined for training and so on until all the ten
individual partitions have been used for testing. The ﬁnal
accuracy of an algorithm will be the average of the ten trials.
The datasets were obtained from Kent Ridge Biological
Dataset Repository . Table 3 shows the
summary of the characteristics of the nine datasets. The
experiments were conducted on varying proportions of the
training and test datasets and also conducted using tenfold
cross-validation on the merged original training and test data.
The proportion of the training data was consistently kept
below 60 %. Since determining how much data is needed for
training and testing is one of the key issues of data mining, we
have worked around this issue using varying proportions of
training and test datasets to avoid over-ﬁtting, without
compromising on accuracy. The performances shown in
Table 5 are an average of the experiments using varying
proportions of the training and test datasets and tenfold crossvalidation on the merged original training and test data.
4.1 Software used for comparison
All the algorithms were executed in WEKA (Weka 3.6.2)
package which is available online ( 
ac.nz/ml/weka/) with their default parameter settings in a
high-end workstation with 3.33 GHz Intel Xeon processor
and 8 GB RAM and the proposed model was implemented
in the same environment using Java (jdk1.6). Default settings are used for all compared ensemble methods as they
showed a high accuracy on an average.
5 The proposed SD-EnClass
Our model for combining classiﬁers into an ensemble is
based on a simple estimation of each classiﬁer’s expertise
Table 2 Summary of the existing ensemble approaches
Classiﬁers
Input parameter
Resampling
Unstable learner
trained over resampled sets outputs
different models
Training data, no. of
classes, no. of
dimensions, no. of
iterations
Simple and easy
to understand
Accuracy value
lower than
other ensemble
approaches
Resampling
Weak learner reweighted in every
Training data, no. of
classes, no. of
dimensions, no. of
iterations
Performance of
Degrades with
Generalization
Resampling
and kfolding
Diverse base
classiﬁers
Metaclassiﬁer
Training data, no. of
classes, no. of
dimensions, no. of
iterations
performance
Storage and time
complexity
S. Nagi, D. K. Bhattacharyya
(accuracy) on class prediction. This technique is a simple
combining method which can use any ‘n’ stronger learners
as base classiﬁers to build an effective ensemble. Predictions of the ‘n’ classiﬁers are combined to obtain the best
prediction for a given test instance. In our experimental
setup, we choose decision trees, Bayesian classiﬁer and knearest neighbor learners as the base learners based on
their consistent performance over real-life datasets such as
Our model utilizes the expertise of a classiﬁer in classifying a part of the problem better than the other classiﬁers
in the ensemble. It explores the necessary property of the
diversity among the base classiﬁers to have a good performing ensemble. It is known that different classiﬁers
handle the problem of classiﬁcation at hand differently. So
an attempt has been made to combine the results of such
diverse classiﬁers to obtain a better classiﬁcation result
than using a single classiﬁer alone.
The architecture of the proposed model is shown in
Fig. 7. It is a two-layer model, where each layer is dedicated with a deﬁnite functionality. The output of the layer-
1 is used as input by the layer-2. Layer-1 deals with the
training and selection of the base classiﬁers, while layer-2
deals with the combination of the predictions of the
selected base classiﬁers.
5.1 Distinct training sample selection
The proposed SD-EnClass uses a 2-step technique, referred
as SD-Prune-Redundant, to select a distinct subset of
training samples by discarding the redundant training
instances. This forms the input to the tenfold cross-validation. Since the performance of classiﬁers is dependent on
its training, so effort is made to create a training set which
is complete in nature, i.e., the training set should hold
instances that would represent the entire domain space.
Steps of SD-Prune-Redundant are given next.
Convert training dataset DTrain into market–basket
form DMB using Algorithm 1 (Fig. 8a);
Filter DMB using Algorithm 2 to remove duplicate
training instances (Fig. 8b).
5.2 Working of the SD-EnClass
Initially, the base classiﬁers are trained with the distinct
training dataset. Next, we evaluate the performance of the
base classiﬁers using the test dataset. The classiﬁer with the
highest class performance for a certain class out of the base
classiﬁers becomes the expert of that class. The classspeciﬁc performance of a classiﬁer is calculated as:
Class specific accuracy ¼ ðTotal no: of correctly
predicted instances for a classÞ=
ðtotal no: of predicted instances
of that classÞ:
To evaluate the class performance of a classiﬁer, a
confusion matrix as shown in Table 4 is used. The
elements in this
table characterize the classiﬁcation
behavior of a given classiﬁer. The sum of the row
elements represents the number of total instances present
in each class, whereas the sum of the column elements
gives the total number of instances predicted as that
Table 3 Basic information of the datasets used
Training instances
Test instances
Total instances
No. of attributes
No. of classes
Lung cancer
Breast cancer
PERFORMANCE STORAGE
PROPOSED SD-Enclass
FINAL PREDICTIONS
INPUT TEST
Fig. 7 Architecture of the proposed SD-EnClass
Ensemble approach to classify microarray cancer data
As shown in Table 4, the total number of instances
present for class A is (x ? p ? l), and the total number of
instances predicted for class A is (x ? y ? z). Here, we
have taken three classes in the table, for more number of
classes, the rows and columns will increase, respectively.
Example: Let the total number of predicted instances for
class A be 100, and total number of correctly predicted
instances for class A is 90, then the class-speciﬁc accuracy
(CSA) for class A for that classiﬁer is 90/100, i.e., 0.9.
We compute the CSA for each base classiﬁer for each
class and accuracy values are stored using a 2-D link list
structure. For our experimental setup, we choose three
(n = 3) base classiﬁers. In the link list structure, the nodes
in the rows correspond to the number of base classiﬁers in
the ensemble and the column nodes correspond to the
number of classes in the dataset. From this data structure,
the class expert for a given class is easily found. During
classiﬁcation of an instance, the instance is ﬁrst classiﬁed
by the base classiﬁers and the individual predictions of the
base classiﬁers are combined as follows:
For a given instance, if all the classiﬁers predict the
same class, then the ensemble goes by the same
If the predictions of majority classiﬁers (2 of 3) match,
then any of the following situations may arise:
C3 is an expert in the class it predicts whereas C1 and
C2 are not, then the prediction given by C3 is taken as
the decision of the ensemble.
C3 is an expert in the class it predicts and anyone of
the classiﬁers, C1 and C2, is also an expert in its
predictions then the ensemble looks for the class
probabilities of the respective classiﬁers and selects
the one with the highest value. If there exists a further
tie between the probability values then the ensemble
goes with the majority.
If the predictions of all the classiﬁers disagree, then
any of the following situations may arise:
One of the classiﬁers could be an expert in its
prediction then the ensemble goes by that classiﬁer’s
Table 4 Confusion matrix
Predicted A
Predicted B
Predicted C
Class-wise no. of instances
x ? p ? l (for A)
y ? q ? m (for B)
z ? r ? n (for C)
No. of predicted class instances
x ? y ? z (for A)
p ? q ? r (for B)
l ? m ? n (for C)
ALGORITHM 1:MB-Representation Algorithm
Input: Training Dataset DTrain
Output: Market-Basket representation of DTrain i.e., DMB
Step-1: Read the raw training dataset , say DTrain
Step-2: For each attribute ai of DTrain
Step-3: Compute no. of intervals, say , for ai adaptively on the distribution of data
Step-4: Assign no. of bits for ai towards market-basket representation of DTrain
Step-5: Next i
Step-6: End
(a) Market Basket Conversion
ALGORITHM 2:Find-Distinct
Input: DMB
Output: DMB
Step-1: Read DMB;
Step-2: For each pair of instances from DMB
Step-3: Compute similarity Sij by counting number of agreements, i.e. no of positions
attribute values of i th and j th instances match
Step-4: if Sij = N i.e. the total number of dimensions of DMB then select mean of these
two instances
Step-6: Next pair
Step-7: End
(b) Distinct training instance selection
Fig. 8 a Market–basket
conversion, b distinct training
instance selection
S. Nagi, D. K. Bhattacharyya
Two classiﬁers could be experts in its class predictions. In such a case, the decision of the classiﬁer
which has a higher class probability is taken as the
ﬁnal decision.
All the three classiﬁers could be experts in its class
predictions. In that case, the decision of the classiﬁer
which has the highest class probability is taken as the
ﬁnal decision.
In this manner, the class predictions of the base classi-
ﬁers are combined to get the ﬁnal prediction.
5.3 Performance analysis of our proposed model
In this phase, we test the performance of our proposed
model. We combine the outputs of J48, IBK and Naı¨ve
Bayes classiﬁers with the combination rule we proposed
earlier and the results are depicted in Table 5.
From Table 5 and graph in Fig. 9, it is clear that our
proposed model has been successful in increasing the
prediction accuracy in 4 of 9 datasets while in 2 datasets it
has been at par with the best performing classiﬁer. Thus, in
6 [i.e., (4 ? 2)] datasets, the proposed model has shown a
good performance, whereas in 4 datasets the performance
has decreased. Nonetheless, the proposed model has been
successful in more than 60 % of the cases.
Next, we apply the three basic ensemble approaches,
i.e., Bagging, Boosting and Stacking, for each of the base
classiﬁers and analyze their performance. For Stack Generalization, we used the three classiﬁers as the base learners
and use these three classiﬁers as Meta-learners one by one
and compare their performances. We notice that Bagging,
Boosting and Stacking improved the performance of J48
and NB, while Stacking with IBK slightly improved the
performance of IBK at the expense of computation time.
This is because Bagging and Boosting mainly improve the
performances of unstable learners; being a stable learner,
IBKs’ performance was not affected much.
While comparing our proposed model with the existing
ensembles, we see from Table 6 and Fig. 10 that the proposed model has not been able to outperform the existing
ensembles in most of the cases. The prediction accuracy is
slightly less than the best performing ensemble.
Notably, the new model is able to have the highest
accuracy in 1 dataset; in 2 datasets, it is at par with the best
existing ensemble and in 3 datasets it has been the second
best performer. To sum up, it cannot be concluded that the
proposed model has been the best performer, but it has
shown an average performance. To improve the prediction
accuracy further, we have thus proposed the metaensemble.
6 Meta-ensemble
From the results of the previous section, it is clear that
combining the outputs of different classiﬁers improves
classiﬁcation accuracy than the best single classiﬁer in the
combination, but it does not perform as well as boosting.
The advantage of boosting acts directly to reduce the error
cases, whereas combining works indirectly. As our proposed model works well to get the best output from the
combination, we used this method to combine the results of
our ensemble with the results of boosting, stacking and
bagging and form a meta-ensemble; the architecture is
shown in Fig. 11.
Table 5 Performance analysis of the proposed model
SD_EnClass
Lung cancer
Breast cancer
Bold values indicate best results obtained from the models
Performance
Performance analysis of Proposed Ensemble
SD_EnClass
Embryonal...
Lung Cancer
Breast Cancer
Fig. 9 Performance analysis of
the proposed model
Ensemble approach to classify microarray cancer data
The new meta-ensemble is a four-layer model where
each layer has a deﬁnite functionality and the output of the
lower layers is used by the higher layers. More summarized
results are presented while traversing toward the higher
In layer-1, any n classiﬁer models are generated. Here
n could be any integer, but in our case n is set to 3. Layer-1
could be further improved by training a larger number of
classiﬁer models and selecting a small set of good performing classiﬁer models out of them. That would deﬁnitely increase the overall accuracy of the proposed model.
Layer-2 is the ensemble layer where the individual
outputs of the consistent base classiﬁers are combined as
described in Sect. 5.2. The output of the ensemble is stored
in a ﬁle which is then fed in layer-4.
Layer-3 creates a pool of classiﬁer ensembles. Popular
methods such as Bagging, Boosting and Stack Generalization (Stacking) are implemented with the classiﬁer
performances recorded. Stacking is implemented with the
three classiﬁers as base learners and taking one of them at a
time as a meta-learner. Two best performing ensembles are
selected for layer-4 along with the proposed ensemble
Layer-4 ﬁnally combines the output of layer-2 and
layer-4, i.e., our ensemble model and the two best performing
prediction.
A comparative analysis of the meta-ensemble with the
existing ensembles is presented in Table 7. For each of the
datasets, Boosting, Bagging and Stacking performed differently. We combined the outputs of the two best performing methods out of boosting, bagging and stacking for
each of the classiﬁers with the result given by our proposed
The results of Table 7 are summarized in Fig. 12. We
observe that of the 9 datasets, our method works well for 8
of them, while performing average in one of the datasets.
Table 6 Performance analysis of the proposed ensemble with existing ensembles
Lung cancer
Breast cancer
Bold values indicate best results obtained from the models
Ovarian Prostrate
Performance of the Classifiers
Performance Analysis of the Proposed Ensemble with Existing Ensembles
Bagging J48
Bagging NB
Bagging IBK
Boosting NB
Stacking J48
Stacking NB
Stacking IBK
Fig. 10 Performance analysis of the proposed model with existing ensembles
S. Nagi, D. K. Bhattacharyya
Abstraction
META ENSEMBLE
INPUT/TEST INSTANCES
PERFORMANCE STORAGE
FINAL PREDICTIONS
PERFORMANCE STORAGE
PERFORMANCE STORAGE
PROPOSED ENSEMBLE
Fig. 11 Architecture of the
meta-ensemble
Table 7 Performance analysis of the meta-ensemble
Metaensemble
Lung cancer
Breast cancer
Bold values indicate best results obtained from the models
Ovarian Prostrate
Performance of Classifiers
Performance Analysis of Meta Ensemble
Bagging j48
Bagging NB
Bagging IBK
Stacking NB
Fig. 12 Performance analysis of the meta-ensemble
Ensemble approach to classify microarray cancer data
Of the 8 datasets it performed well, it improved classiﬁcation accuracy in 4 of them signiﬁcantly, while in
remaining 4 it performed as well as the best classiﬁer in the
combination.
Thereby we can say from the above results that given a
set of classiﬁers our proposed model combines the prediction of the classiﬁers to obtain a better prediction result
which in most of the cases is better than selecting the best
classiﬁer in the combination of classiﬁers.
7 Conclusion and future work
In this paper, we analyze the performances of popular
ensemble methods like Bagging, Boosting and Stack
Generalization. We found out that on an average, Boosting
and Stack Generalization using unstable learners (decision
trees) and probabilistic classiﬁers (Naı¨ve Bayes) work
better than applying them to stable learners (nearest
neighbor classiﬁers). We also learnt that Bagging of classiﬁers performs better than Boosting and Stack Generalization for most of the cancer datasets used in our
experiments.
We have also proposed an effective way of combining
the outputs of the classiﬁers in the ensemble, based on the
class performance of each of the classiﬁers in the combination. The experimental results show that our model
performs better than making a simple selection of the best
single classiﬁer in the combination in majority of the cases.
In doing so, we mostly tested our model on two class
datasets for which it performed well; we also tested on one
multi-class dataset on which we got good performance and
in future we can extend the combination rules so that it
works even better for multi-class problem.
While combining models across the different algorithm
families, we saw an improvement of performance in the
classiﬁcation accuracy compared to the best single model
in the combination, but when compared to Bagging, its
performance was average. So in the next stage, we combined the results of our proposed ensemble with the results
of the best performing ensemble methods for the datasets,
using our proposed combining method and obtained results
which were signiﬁcantly better than using Boosting, Bagging or Stacking alone.
In the modern era where computing technologies are
getting better every day, we can use our method in parallelization to achieve better classiﬁcation accuracy to
solve a classiﬁcation problem. There is scope to incorporate self-computing approaches (like fuzzy, roughset, ANN
or their hybridization) in the proposed model for further
enhancement of performance.