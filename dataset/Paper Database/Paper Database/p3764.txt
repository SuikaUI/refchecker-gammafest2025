Data Augmentation for Graph Neural Networks
Tong Zhao1*, Yozen Liu2, Leonardo Neves2, Oliver Woodford2, Meng Jiang1, Neil Shah2
1 University of Notre Dame, Notre Dame, IN 46556
2 Snap Inc., Santa Monica, CA 90405
{tzhao2, mjiang2}@nd.edu, {yliu2, lneves, oliver.woodford, nshah}@snap.com
Data augmentation has been widely used to improve generalizability of machine learning models. However, comparatively little work studies data augmentation for graphs. This
is largely due to the complex, non-Euclidean structure of
graphs, which limits possible manipulation operations. Augmentation operations commonly used in vision and language
have no analogs for graphs. Our work studies graph data
augmentation for graph neural networks (GNNs) in the context of improving semi-supervised node-classiﬁcation. We
discuss practical and theoretical motivations, considerations
and strategies for graph data augmentation. Our work shows
that neural edge predictors can effectively encode classhomophilic structure to promote intra-class edges and demote inter-class edges in given graph structure, and our main
contribution introduces the GAUG graph data augmentation
framework, which leverages these insights to improve performance in GNN-based node classiﬁcation via edge prediction. Extensive experiments on multiple benchmarks show
that augmentation via GAUG improves performance across
GNN architectures and datasets.
Introduction
Data driven inference has received a signiﬁcant boost in generalization capability and performance improvement in recent years from data augmentation techniques. These methods increase the amount of training data available by creating plausible variations of existing data without additional
ground-truth labels, and have seen widespread adoption in
ﬁelds such as computer vision (CV) ,
and natural language processing (NLP) . Such augmentations allow inference engines to learn to generalize better
across those variations and attend to signal over noise. At the
same time, graph neural networks (GNNs) have emerged as a rising approach for datadriven inference on graphs, achieving promising results on
*Our work was done during ﬁrst author’s internship at Snap Inc.
Copyright © 2021, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
tasks such as node classiﬁcation, link prediction and graph
representation learning.
Despite the complementary nature of GNNs and data
augmentation, few works present strategies for combining
the two. One major obstacle is that, in contrast to other
data, where structure is encoded by position, the structure
of graphs is encoded by node connectivity, which is irregular. The hand-crafted, structured, data augmentation operations used frequently in CV and NLP therefore cannot be
applied. Furthermore, this irregularity does not lend itself to
easily deﬁning new augmentation strategies. The most obvious approaches involve adding or removing nodes or edges.
For node classiﬁcation tasks, adding nodes poses challenges
in labeling and imputing features and connectivity of new
nodes, while removing nodes simply reduces the data available. Thus, edge addition and removal appears the best augmentation strategy for graphs. But the question remains,
which edges to change.
Three relevant approaches have recently been proposed.
DROPEDGE randomly removes a fraction
of graph edges before each training epoch, in an approach
reminiscent of dropout . This, in principle, robustiﬁes test-time inference, but cannot beneﬁt from
added edges. In approaches more akin to denoising or pre-
ﬁltering, ADAEDGE iteratively add (remove) edges between nodes predicted to have the same (different) labels with high conﬁdence in the modiﬁed graph.
This ad-hoc, two-stage approach improves inference in general, but is prone to error propagation and greatly depends
on training size. Similarly, BGCN iteratively trains an assortative mixed membership stochastic block model with predictions of GCN to produce multiple denoised graphs, and ensembles results from multiple
GCNs. BGCN also bears the risk of error propagation.
Present work. Our work studies new techniques for graph
data augmentation to improve node classiﬁcation. Section 3
introduces motivations and considerations in augmentation
via edge manipulation. Speciﬁcally, we discuss how facilitating message passing by removing “noisy” edges and
adding “missing” edges that could exist in the original graph
can beneﬁt GNN performance, and its relation to intra-class
and inter-class edges. Figure 1 demonstrates, on a toy dataset
(a), that while randomly modifying edges (b) can lead to
lower test-time accuracy, strategically choosing ideal edges
The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)
(a) Original graph.
O : 92.4 F1
(b) Random mod.
M : 90.4, O : 91.0 F1
(c) Proposed GAUG mod.
M : 95.7, O : 94.3 F1
(d) Omniscient mod.
M : 98.6, O : 95.6 F1
Figure 1: GCN performance (test micro-F1) on the original Zachary’s Karate Club graph in (a), and three augmented graph
variants in (b-d), evaluated on both original (O) and modiﬁed (M) graph settings. Black, solid-blue, dashed-blue edges denote original graph connectivity, newly added, and removed edges respectively. While random graph modiﬁcation (b) hurts
performance, our proposed GAUG augmentation approaches (c) demonstrate signiﬁcant relative performance improvements,
narrowing the gap to omniscient, class-aware modiﬁcations (d).
to add or remove given (unrealistic) omniscience of node
class labels (d) can substantially improve it.
Armed with this insight, Section 4 presents our major
contribution: the proposed GAUG framework for graph data
augmentation. We show that neural edge predictors like
GAE are able to latently learn
class-homophilic tendencies in existent edges that are improbable, and nonexistent edges that are probable. GAUG
leverages this insight in two approaches, GAUG-M and
GAUG-O, which tackle augmentation in settings where edge
manipulation is and is not feasible at inference time. GAUG-
M uses an edge prediction module to fundamentally modify
an input graph for future training and inference operations,
whereas GAUG-O learns to generate plausible edge augmentations for an input graph, which helps node classiﬁcation without any modiﬁcation at inference time. In essence,
our work tackles the problem of the inherent indeterminate nature of graph data and provides graph augmentations,
which can both denoise structure and also mimic variability. Moreover, its modular design allows augmentation to be
ﬂexibly applied to any GNN architecture. Figure 1(c) shows
GAUG-M and GAUG-O achieves marked performance improvements over (a-b) on the toy graph.
In Section 5, we present and discuss an evaluation of
GAUG-O across multiple GNN architectures and datasets,
demonstrating a consistent improvement over the state-ofthe-art, and quite large in some scenarios. Our proposed
GAUG-M (GAUG-O) shows up to 17% (9%) absolute F1
performance improvements across datasets and GNN architectures without augmentation, and up to 16% (9%) over
baseline augmentation strategies.
Other Related Work
As discussed above, relevant literature in data augmentation
for graph neural networks is limited . We discuss other related
works in tangent domains below.
Graph Neural Networks. GNNs enjoy widespread use in
modern graph-based machine learning due to their ﬂexibility to incorporate node features, custom aggregations and
inductive operation, unlike earlier works which were based
on embedding lookups . Many GNN
variants have been developed in recent years, following the
initial idea of convolution based on spectral graph theory
 . Many spectral GNNs have since been
developed and improved by . As spectral GNNs generally operate (expensively)
on the full adjacency, spatial-based methods which perform
graph convolution with neighborhood aggregation became
prominent , owing to their scalability and ﬂexibility . Several works propose
more advanced architectures which add residual connections
to facilitate deep GNN training . More recently, task-speciﬁc GNNs
were proposed in different ﬁelds such as behavior modeling
 .
Data Augmentation. Augmentation strategies for improving generalization have been broadly studied in contexts
outside of graph learning. Traditional point-based classiﬁcation approaches widely leveraged oversampling, undersampling and interpolation methods . In recent years, variants of such techniques are widely used in natural language processing (NLP)
and computer vision (CV). Replacement approaches involving synonym-swapping are common in NLP , as are text-variation approaches (i.e. for visual questionanswering). Backtranslation methods 
have also enjoyed success. In CV, historical image transformations in the input space, such as rotation, ﬂipping,
color space transformation, translation and noise injection
 , as well as recent methods such as cutout and random erasure have proven useful. Recently,
augmentation via photorealistic generation through adversarial networks shows promise in several applications, especially in medicine . Most-related to our work is liter-
ature on meta-learning based augmentation in CV , which aim to learn neural image transformation operations via an augmentation network, using a loss
from a target network. While our work is similar in motivation, it fundamentally differs in network structure, and tackles augmentation in the much-less studied graph context.
Graph Data Augmentation via
Edge Manipulation
In this section, we introduce our key idea of graph data augmentation by manipulating G via adding and removing edges
over the ﬁxed node set. We discuss preliminaries, practical
and theoretical motivations, and considerations in evaluation
under a manipulated-graph context.
Preliminaries
Let G = (V, E) be the input graph with node set V and edge
set E. Let N = |V| be the number of nodes. We denote
the adjacency matrix as A ∈{0, 1}N×N, where Aij = 0
indicates node i and j are not connected. We denote the node
feature matrix as X ∈RN×F , where F is the dimension of
the node features and Xi: indicates the feature vector of node
i (the ith row of X). We deﬁne D as the diagonal degree
matrix such that Dii = P
Graph Neural Networks. In this work, we use the wellknown graph convolutional network (GCN) as an example when explaining GNNs in the
following sections; however, our arguments hold straightforwardly for other GNN architectures. Each GCN layer
(GCL) is deﬁned as:
H(l+1) = fGCL(A, H(l); W(l))
2 H(l)W(l)),
where ˜A = A + I is the adjacency matrix with added selfloops, ˜D is the diagonal degree matrix ˜Dii = P
j ˜Aij, and
σ(·) denotes a nonlinear activation such as ReLU.
Motivation
Practical reasons. Graphs aim to represent an underlying
process of interest. In reality, a processed or observed graph
may not exactly align with the process it intended to model
(e.g. “which users are actually friends?” vs. “which users are
observed to be friends?”) for several reasons. Many graphs
in the real world are susceptible to noise, both adversarial
and otherwise (with exceptions, like molecular or biological
graphs). Adversarial noise can manifest via spammers who
pollute the space of observed interactions. Noise can also be
induced by partial observation: e.g. a friend recommendation system which never suggests certain friends to an enduser, thus preventing link formation. Moreover, noise can
be created in graph preprocessing, by adding/removing selfloops, removing isolated nodes or edges based on weights.
Finally, noise can occur due to human errors: in citation
networks, a paper may omit (include) citation to a highly
(ir)relevant paper by mistake. All these scenarios can produce a gap between the “observed graph” and the so-called
“ideal graph” for a downstream inference task (in our case,
node classiﬁcation).
Enabling an inference engine to bridge this gap suggests
the promise of data augmentation via edge manipulation. In
the best case, we can produce a graph Gi (ideal connectivity), where supposed (but missing) links are added, and
unrelated/insigniﬁcant (but existing) links removed. Figure
1 shows this beneﬁt realized in the ZKC graph: strategically adding edges between nodes of the same group (intraclass) and removing edges between those in different groups
(inter-class) substantially improves node classiﬁcation test
performance, despite using only a single training example
per class. Intuitively, this process encourages smoothness
over same-class node embeddings and differentiates otherclass node embeddings, improving distinction.
Theoretical reasons. Strategic edge manipulation to promote intra-class edges and demote inter-class edges makes
class differentiation in training trivial with a GNN, when
done with label omniscience. Consider a scenario of extremity where all possible intra-class edges and no possible interclass edges exists, the graph can be viewed as k fully connected components, where k is the number of classes and all
nodes in each component have the same label. Then by Theorem 1 ), GNNs can easily generate distinct node representations between distinct classes, with equivalent representations for all same-class nodes. Under this “ideal graph”
scenario, learned embeddings can be effortlessly classiﬁed.
Theorem 1. Let G = (V, E) be a undirected graph with adjacency matrix A, and node features X be any block vector
in RN×F . Let f : A, X; W →H be any GNN layer with
a permutation-invariant neighborhood aggregator over the
target node and its neighbor nodes u ∪N(u) (e.g. Eq. 1)
with any parameters W, and H = f(A, X; W) be the resulting embedding matrix. Suppose G contains k fully connected components. Then we have:
1. For any two nodes i, j ∈V that are contained in the same
connected component, Hi: = Hj:.
2. For any two nodes i, j ∈V that are contained in different
connected components Sa, Sb ⊆V, Hi: ̸= Hj: when W
is not all zeros and P
v∈Sa Xv: ̸= ε P
u∈Sb Xu:, ∀ε ∈R.
This result suggests that with an ideal, class-homophilic
graph Gi, class differentiation in training becomes trivial.
However, it does not imply such results in testing, where
node connectivity is likely to reﬂect G and not Gi. We would
expect that if modiﬁcations in training are too contrived, we
risk overﬁtting to Gi and performing poorly on G due to a
wide train-test gap. We later show techniques (Section 4) for
approximating Gi with a modiﬁed graph Gm, and show empirically that these modiﬁcations in fact help generalization,
both when evaluating on graphs akin to Gm and G.
Modiﬁed and Original Graph Settings for
Graph Data Augmentation
Prior CV literature considers image data augmentation a two-step process: (1) applying a transformation f : S →T to input images S
to generate variants T , and (2) utilizing S ∪T for model
% Edges added (by Edge Predictor)
# of total edges
Intra-class
Inter-class
(a) Learned edge +
% Edges added (by Random)
Intra-class
Inter-class
Test micro F1
(b) Random edge +
% Edges removed (by Edge Predictor)
# of total edges
Intra-class
Inter-class
(c) Learned edge –
% Edges removed (by Random)
Intra-class
Inter-class
Test micro F1
(d) Random edge –
Figure 2: GAUG-M uses an edge-predictor module to deterministically modify a graph for future inference. Neural edge-predictors (e.g. GAE) can learn class-homophilic
tendencies, promoting intra-class and demoting inter-class
edges compared to random edge additions (a-b) and removals (c-d) respectively, leading to node classiﬁcation performance (test micro-F1) improvements (green).
training. Graph data augmentation is notably different, since
typically |S| = 1 for node classiﬁcation, unlike the image
setting where |S| ≫1. However, we propose two strategies with analogous, but distinct formalisms: we can either
(1) apply one or multiple graph transformation operation
f : G →Gm, such that Gm replaces G for both training and
inference, or (2) apply many transformations fi : G →Gi
for i = 1 . . . N, such that G ∪{Gi
i=1 may be used in
training, but only G is used for inference. We call (1) the
modiﬁed-graph setting, and (2) the original-graph setting,
based on their inference scenario.
One might ask: when is each strategy preferable? We reason that the answer stems from the feasibility of applying
augmentation during inference to avoid a train-test gap. The
modiﬁed-graph setting is thus most suitable in cases where
a given graph is unchanging during inference. In such cases,
one can produce a single Gm, and simply use this graph for
both training and testing. However, when inferences must
be made on a dynamic graph (i.e. for large-scale, latencysensitive applications) where calibrating new graph connectivity (akin to G) with Gm during inference is infeasible (e.g.
due to latency constraints), augmentation in the originalgraph setting is more appropriate. In such cases, test statistics on Gm may be overly optimistic as performance indicators. In practice, these loosely align with transductive and
inductive contexts in prior GNN literature.
Proposed GAUG Framework
In this section, we introduce the GAUG framework, covering
two approaches for augmenting graph data in the aforementioned modiﬁed-graph and original-graph settings respectively. Our key idea is to leverage information inherent in
the graph to predict which non-existent edges should likely
exist, and which existent edges should likely be removed in
G to produce modiﬁed graph(s) Gm to improve model performance. As we later show in Section 5, by leveraging this
label-free information, we can consistently realize improvements in test/generalization performance in semi-supervised
node classiﬁcation tasks across augmentation settings, GNN
architectures and datasets.
GAUG-M for Modiﬁed-Graph Setting
We ﬁrst introduce GAUG-M, an approach for augmentation
in the modiﬁed-graph setting which includes two steps: (1)
we use an edge predictor function to obtain edge probabilities for all possible and existing edges in G. The role of the
edge predictor is ﬂexible and can generally be replaced with
any suitable method. (2) Using the predicted edge probabilities, we deterministically add (remove) new (existing) edges
to create a modiﬁed graph Gm, which is used as input to a
GNN node-classiﬁer.
The edge predictor can be deﬁned as any model fep :
A, X →M, which takes the graph as input, and outputs
an edge probability matrix M where Muv indicates the predicted probability of an edge between nodes u and v. In
this work, we use the graph auto-encoder (GAE) as the edge predictor module due to its simple architecture and competitive performance. GAE consists
of a two layer GCN encoder and an inner-product decoder:
, where Z = f (1)
GCL (A, X)
Z denotes the hidden embeddings learned by the encoder,
M is the predicted (symmetric) edge probability matrix produced by the inner-product decoder, and σ(·) is an elementwise sigmoid function. Let |E| denote the number of edges
in G. Then, using the probability matrix M, GAUG-M deterministically adds the top i|E| non-edges with highest edge
probabilities, and removes the j|E| existing edges with least
edge probabilities from G to produce Gm, where i, j ∈ .
This is effectively a denoising step.
Figure 2 shows the change in intra-class and interclass edges when adding/removing using GAE-learned edge
probabilities and their performance implications compared
to a random perturbation baseline on CORA: adding (removing) by learned probabilities results in a much steeper
growth (slower decrease) of intra-class edges and much
slower increase (steeper decrease) in inter-class edges
compared to random. Notably, these affect classiﬁcation
performance (micro-F1 scores, in green): random addition/removal hurts performance, while learned addition consistently improves performance throughout the range, and
learned removal improves performance over part of the
range (until ∼20%). Importantly, these results show that
while we are generally not able to produce the ideal graph
Gi without omniscience (as discussed in Section 3.2), such
capable edge predictors can latently learn to approximate
class-homophilic information in graphs and successfully
promote intra-class and demote inter-class edges to realize
performance gains in practice.
GAUG-M shares the same time and space complexity as
its associated GNN architecture during training/inference,
while requiring extra disk space to save the dense O(N 2)
,QWHUSRODWLRQDQG
*UDSK1HXUDO1HWZRUN
1RGH&ODVVLğHU
1HXUDO(GJH3UHGLFWRU
,QSXW*UDSK
Figure 3: GAUG-O is comprised of three main components: (1) a differentiable edge predictor which produces edge probability
estimates, (2) an interpolation and sampling step which produces sparse graph variants, and (3) a GNN which learns embeddings
for node classiﬁcation using these variants. The model is trained end-to-end with both classiﬁcation and edge prediction losses.
edge probability matrix M for manipulation. Note that M’s
computation can be trivially parallelized.
GAUG-O for Original-Graph Setting
To complement the above approach, we propose GAUG-O
for the original-graph setting, where we cannot beneﬁt from
graph manipulation at inference time. GAUG-O is reminiscent of the two-step approach in GAUG in that it also uses an
edge prediction module for the beneﬁt of node classiﬁcation,
but also aims to improve model generalization (test performance on G) by generating graph variants {Gi
i=1 via edge
prediction and hence improve data diversity. GAUG-O does
not require discrete speciﬁcation of edges to add/remove, is
end-to-end trainable, and utilizes both edge prediction and
node-classiﬁcation losses to iteratively improve augmentation capacity of the edge predictor and classiﬁcation capacity of the node classiﬁer GNN. Figure 3 shows the overall architecture: each training iteration exposes the node-classiﬁer
to a new augmented graph variant.
Unlike GAUG-M’s deterministic graph modiﬁcation step,
GAUG-O supports a learnable, stochastic augmentation process. As such, we again use the graph auto-encoder (GAE)
for edge prediction. To prevent the edge predictor from arbitrarily deviating from original graph adjacency, we interpolate the predicted M with the original A to derive an adjacency P. In the edge sampling phase, we sparsify P with
Bernoulli sampling on each edge to get the graph variant
adjacency A′. For training purposes, we employ a (soft,
differentiable) relaxed Bernoulli sampling procedure as a
Bernoulli approximation. This relaxation is a binary special case of the Gumbel-Softmax reparameterization trick
 .
Using the relaxed sample, we apply a straight-through (ST)
gradient estimator ,
which rounds the relaxed samples in the forward pass, hence
sparsifying the adjacency. In the backward pass, gradients
are directly passed to the relaxed samples rather than the
rounded values, enabling training. Formally,
1 + e−(log Pij+G)/τ + 1
Pij = αMij + (1 −α)Aij
where A′ is the sampled adjacency matrix, τ is the temperature of Gumbel-Softmax distribution, G ∼Gumbel(0, 1) is
a Gumbel random variate, and α is a hyperparameter mediating the inﬂuence of edge predictor on the original graph.
The graph variant adjacency A′ is passed along with node
features X to the GNN node classiﬁer. We then backpropagate using a joint node-classiﬁcation loss Lnc and edgeprediction loss Lep
L = Lnc + βLep,
Lnc = CE(ˆy, y)
Lep = BCE(σ(fep(A, X)), A)
where β is a hyperparameter to weight the reconstruction
loss, σ(·) is an elementwise sigmoid, y, ˆy denote groundtruth node class labels and predicted probabilities, and
BCE/CE indicate standard (binary) cross-entropy loss. We
train using Lep in addition to Lnc to control potentially
excessive drift in edge prediction performance. The nodeclassiﬁer GNN is then directly used for inference, on G.
During training, GAUG-O has a space complexity of
O(N 2) in full-batch setting due to backpropagation through
all entries of the adjacency matrix. Fortunately, we can easily adapt the graph mini-batch training introduced by Hamilton et al. to achieve
an acceptable space complexity of O(M 2), where M is the
batch size.
Evaluation
In this section, we evaluate the performance of GAUG-M
and GAUG-O across architectures and datasets, and over
alternative strategies for graph data augmentation. We also
showcase their abilities to approximate class-homophily via
edge prediction and sensitivity to supervision.
Experimental Setup
We evaluate using 6 benchmark datasets across domains:
citation networks ), protein-protein interactions ), social networks ), and air trafﬁc ). Statistics for each dataset
are shown in Table 1. We follow the semi-supervised setting in most GNN literature for train/validation/test splitting on
CORA and CITESEER, and a 10/20/70% split on other
datasets due to varying choices in prior work. We evaluate
BLOGCATALOG
# Features
# Training nodes
# Validation nodes
# Test nodes
Table 1: Summary statistics and experimental setup for the six evaluation datasets.
Table 2: GAUG performance across GNN architectures and six benchmark datasets.
GAUG-M and GAUG-O using 4 widely used GNN architectures: GCN , GSAGE , GAT 
and JK-NET . We compare our GAUG-
M (modiﬁed-graph) and GAUG-O (original-graph) performance with that achieved by standard GNN performance,
as well as three state-of-the-art baselines: ADAEDGE (modiﬁed-graph), BGCN 
(modiﬁed-graph), and DROPEDGE 
(original-graph) evaluating on Gm and G, respectively. We
also show results of proposed GAUG methods on large
graphs in Section D.2 to
show their ability of mini-batching. We report test micro-
F1 scores over 30 runs, employing Optuna for efﬁcient hyperparameter search. Note that for classiﬁcation tasks which every object is guaranteed to be assigned to exactly one ground truth class (all datasets except PPI), micro-F1 score is mathematically equivalent to
accuracy. Our implementation is made publicly available at
 
Experimental Results
We show comparative results against current baselines in Table 2. Table 2 is organized per architecture (row), per dataset
(column), and original-graph and modiﬁed-graph settings
(within-row). Note that results of BGCN on PPI are missing
due to CUDA out of memory error when running the code
package from the authors. We bold best-performance per architecture and dataset, but not per augmentation setting for
visual clarity. In short, GAUG-O and GAUG-M consistently
improve over GNN architectures, datasets and alternatives,
with a single exception for GAT on PPI, on which DROPE-
DGE performs the best.
Improvement across GNN architectures. GAUG achieves
improvements over all 4 GNN architectures (averaged
across datasets): GAUG-M improves 4.6% (GCN), 4.8%
(GSAGE), 10.9% (GAT) and 5.7% (JK-NET). GAUG-O
improves 4.1%, 2.1%, 6.3% and 4.9%, respectively. We note
(b) CITESEER
(c) FLICKR
(d) AIR-USA
Figure 4: Classiﬁcation (test) performance heatmaps of
GAUG-M on various datasets when adding/dropping edges.
Red-white-blue indicate outperformance, at-par, and underperformance w.r.t. GCN on G. Pixel (0, 0) indicates G, and
x (y) axes show % edges added (removed).
that augmentation especially improves GAT performance,
as self-attention based models are sensitive to connectivity.
Improvements across datasets. GAUG also achieves improvements over all 6 datasets (averaged across architectures): GAUG-M improves 2.4%, 1.0%, 3.1%, 5.5%, 19.2%,
7.9% for each dataset (left to right in Table 2). Figure 4
shows GAUG-M (with GCN) classiﬁcation performance
heatmaps on 4 datasets when adding/removing edges according to various i, j (Section 4.1). Notably, while improvements(red) over original GCN on G differ over i, j
and by dataset, they are feasible in all cases. These improvements are not necessarily monotonic with edge addition(row) or removal(column), and can encounter transitions. Empirically, we notice these boundaries correspond
to excessive class mixing (addition) or graph shattering
(removal). GAUG-O improves 1.6%, 2.5%, 11.5%, 3.6%,
2.2%, 4.7%. We note that both methods achieves large improvements in social data (BLOGCATALOG and FLICKR)
where noisy edges may be prominent due to spam or bots
(supporting intuition from Section 3.2): Figure 4(c) shows
substantial edge removal signiﬁcantly helps performance.
Improvements over alternatives. GAUG also outperforms augmentation over BGCN, ADAEDGE, and DROPE-
DGE (averaged across datasets/architectures): GAUG-M improves 9.3%, 4.8%, and 4.1% respectively, while GAUG-
O improves 4.9%, 2.7%, and 2.0% respectively. We reason that GAUG-M outperforms BGCN and ADAEDGE
by avoiding iterative error propagation, as well as directly
manipulating edges based on the graph, rather than indirectly through classiﬁcation results. GAUG-O outperforms
DROPEDGE via learned denoising via addition and removal,
rather than random edge removal. Note that some baselines
have worse performance than vanilla GNNs, as careless augmentation/modiﬁcation on the graph can hurt performance
10 12 14 16 18
Intra-class Fraction
GAug-O Sampled
Original Graph
Inter-class Fraction
GAug-O Sampled
Original graph
(a) Edge makeup
(b) Learning curve
Figure 5: GAUG-O promotes class-homophily (a), producing classiﬁcation improvements (b).
# Nodes in training
Test F1 Improv.
GCN-GAug-M
GSAGE-GAug-M
GAT-GAug-M
JKNet-GAug-M
(a) GAUG-M
# Nodes in training
Test F1 Improv.
GCN-GAug-O
GSAGE-GAug-O
GAT-GAug-O
JKNet-GAug-O
(b) GAUG-O
Figure 6: GAUG augmentation especially improves performance under weak supervision.
by removing critical edges and adding incorrect ones.
Promoting class-homophily. Figure 5a shows (on CORA)
that the edge predictor in GAUG-O learns to promote intraclass edges and demote inter-class ones, echoing results
from Figure 2 on GAUG-M, facilitating message passing
and improving performance. Figure 5b shows that Lnc decreases and validation F1 improves over the ﬁrst few epochs,
while Lep increases to reconcile with supervision from Lnc.
Later on, the Lnc continues to decrease while intra-class ratio increases (overﬁtting).
Sensitivity to supervision. Figure 6 shows that GAUG is especially powerful under weak supervision, producing large
F1 improvements with few labeled samples. Moreover, augmentation helps achieve equal performance w.r.t standard
methods with fewer training samples. Naturally, improvements shrink in the presence of more supervision. GAUG-
M has slightly larger improvements compared to GAUG-O
with more training nodes, as inference beneﬁts from persistent graph modiﬁcations in the former but not the latter.
Conclusion
Data augmentation for facilitating GNN training has unique
challenges due to graph irregularity. Our work tackles this
problem by utilizing neural edge predictors as a means of
exposing GNNs to likely (but nonexistent) edges and limiting exposure to unlikely (but existent) ones. We show that
such edge predictors can encode class-homophily to promote intra-class edges and inter-class edges. We propose
the GAUG graph data augmentation framework which uses
these insights to improve node classiﬁcation performance
in two inference settings. Extensive experiments show our
proposed GAUG-O and GAUG-M achieve up to 17% (9%)
absolute F1 performance improvements across architectures
and datasets, and 15% (8%) over augmentation baselines.