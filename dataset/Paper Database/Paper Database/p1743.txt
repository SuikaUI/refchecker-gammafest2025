Gaussian Processes for Machine Learning
Matthias Seeger∗
Department of EECS
University of California at Berkeley
485 Soda Hall, Berkeley CA 94720-1776, USA
 
February 24, 2004
Gaussian processes (GPs) are natural generalisations of multivariate Gaussian random variables to inﬁnite (countably or continuous) index sets. GPs have been applied in
a large number of ﬁelds to a diverse range of ends, and very many deep theoretical analyses of various properties are available. This paper gives an introduction to Gaussian
processes on a fairly elementary level with special emphasis on characteristics relevant
in machine learning. It draws explicit connections to branches such as spline smoothing
models and support vector machines in which similar ideas have been investigated.
Gaussian process models are routinely used to solve hard machine learning problems. They
are attractive because of their ﬂexible non-parametric nature and computational simplicity.
Treated within a Bayesian framework, very powerful statistical methods can be implemented
which oﬀer valid estimates of uncertainties in our predictions and generic model selection
procedures cast as nonlinear optimization problems. Their main drawback of heavy computational scaling has recently been alleviated by the introduction of generic sparse approximations . The mathematical literature on GPs is large and often uses deep
concepts which are not required to fully understand most machine learning applications. In
this tutorial paper, we aim to present characteristics of GPs relevant to machine learning
and to show up precise connections to other “kernel machines” popular in the community.
Our focus is on a simple presentation, but references to more detailed sources are provided.
Introduction and Overview: Gaussian Processes in a Nutshell
In this section, we introduce the basic reasoning behind non-parametric random ﬁeld and
Gaussian process models. Readers who have been exposed to these concepts may jump to
the end of the section where an overview of the remaining sections is given.
In most machine learning problems, we aim to generalise from a ﬁnite set of observed data,
in the sense that our ability to predict uncertain aspects of a problem improves after making
∗Previously at: Institute for Adaptive and Neural Computation, University of Edinburgh, UK.
the observations. This is possible only if we postulate a priori a relationship between the
variables we will observe and the ones we wish to predict. This relationship is uncertain
itself, making generalisation a non-trivial problem. For example, in spatial statistics we
observe the values of a function at certain locations and want to predict them at other
ones. In temporal statistics, we might want to predict future values of a time series from
its past. In the situations we are interested in here, the postulated relationship can be
represented by an ensemble (or a distribution) of functions. It is helpful to imagine the
observed data being “generated” by picking a function from the ensemble which gives rise
to the sample (typically, observations themselves are imperfect or “noisy”). It is important
to stress that this generative view can well be a crude abstraction of the mechanism we really
hold capable of simulating the phenomenon, as long as its probabilistic inversion leads to
satisfying predictions. This inversion is obtained by conditioning the generative ensemble
on the observed data, which leads to a new adapted ensemble pinned down at observation
points but still variable elsewhere.
In parametric statistics, we agree on a function class indexed by a ﬁnite number of parameters. A distribution over these parameters induces an ensemble over functions. Learning
from observations means to modify this distribution so to adapt the ensemble to the data.
If our a priori postulate is a very informed one (e.g. if the function class is motivated by
a physical theory of the phenomenon), the parametric approach is the method of choice,
but if many aspects of the phenomenon are unknown or hard to describe explicitly, nonparametric modelling can be more versatile and powerful. It is important to stress that our
aim is solely to obtain accurate predictions together with valid estimates of uncertainty,
not to “explain” the inner workings of the true generative process. In the latter case, nonparametric modelling is less applicable.
In non-parametric statistics, regularities of the relationship are postulated without requiring
the ensemble to be concentrated on a easily describable class. For example, we may assume
the ensemble to be stationary or isotropic (see Section 2), which allows us to infer properties
of the generative ensemble even though our observations come from a single realisation
thereof. We might also postulate smoothness so that nearby points (in space or time) have
similar values with high probability, periodicity, boundary conditions, etc. In contrast to the
parametric case, it is less clear how we can represent such a generative ensemble explicitly.
A random ﬁeld is a mapping from an input space to real-valued random variables1, a
natural generalisation of a joint distribution to an inﬁnite index set. Like a joint distribution,
we can try to describe the ﬁeld by its low-order cumulants such as mean and covariance
function, the latter being a bivariate form satisfying a positive semideﬁniteness property
akin to a covariance matrix of a joint distribution. If all cumulants above second order
vanish, the random ﬁeld is Gaussian: a Gaussian process. Importantly, properties such as
stationarity, isotropy, smoothness, periodicity, etc. can be enforced via the choice of the
covariance function. Furthermore, all ﬁnite-dimensional marginal distributions of the ﬁeld
are jointly Gaussian, and inference and prediction require little more than numerical linear
With this brief introduction, we hope to have motivated the reader to browse through the
more detailed sections to follow. Section 2 deﬁnes Gaussian processes, introduces the important subclasses of stationary and isotropic GPs and develops two diﬀerent views on GPs
1The extension to complex-valued random ﬁelds is straightforward. Since most machine learning applications require real-valued ﬁelds only, we concentrate on this case for simplicity.
prominent in machine learning. Some elementary GP models are introduced in Section 3.
Approximate inference techniques for such models are discussed in Section 4 using a generic
framework. Theoretical aspects of GPs can be understood by associating them with reproducing kernel Hilbert spaces (RKHS), as shown in Section 5. Traditionally, GP models
have been used in the context of penalised maximum likelihood and spline smoothing which
are motivated in Section 6. A variant of spline smoothing, the support vector machine has
gained large popularity in the machine learning community, its relationship to Bayesian GP
techniques is given in Section 7. GP models have been used extensively in spatial statistics,
using an estimation procedure called kriging, as described in Section 8. The ﬁnal Section 9
deals with the choice of the covariance function which is of central importance in GP modelling. We describe classes of standard kernels and their properties, show how kernels can
be constructed from elementary parts, discuss methods for learning aspects of the kernel
and ﬁnally illustrate classes of covariance functions over discrete index sets.
Readers more interested in practical machine learning aspects may want to skip over Sections 5 and 6 which contain more theoretical material not required to understand GP applications. We use notational conventions familiar to probability theorists which is introduced
in Section A.1, but are careful to motivate the formalism in the more applied sections.
Gaussian Processes: The Process and the Weight Space
Gaussian process (GP) models are constructed from classical statistical models by replacing
latent functions of parametric form (e.g. linear functions, truncated Fourier or Wavelet expansions, multi-layer perceptrons) by random processes with Gaussian prior. In this section,
we will introduce GPs and highlight some aspects which are relevant to machine learning.
We develop two simple views on GPs, pointing out similarities and key diﬀerences to distributions induced by parametric models. We follow , Chap. 1,2. A good introduction into
the concepts required to study GP prediction is given in , Chap. 2. For concepts and
vocabulary from general probability theory, we refer to .
Let X be an non-empty index set. For the main parts of this paper, X can be arbitrary, but
here we assume that X is at least a group2 (and sometimes we assume it to be Rg). In a
nutshell, a random process X →R is a collection of random variables (one for each x ∈X)
over a common probability space. The measure-theoretic deﬁnition is awkward, but basically
the same as for a single variable. It can also be viewed as a function from the probability
space and X into the reals. The functions X →R obtained for a ﬁxed atomic event are called
sample paths, and a random process can also be seen as the corresponding distribution over
sample paths. If X ⊂X is ﬁnite, we obtain a random variable ∈R|X| by evaluating the
process at the points X, its distribution is called ﬁnite-dimensional distribution (f.d.d.). If
we assume that a random process exists and consider the system of all f.d.d.’s, it is clear
that it has to be symmetric and consistent: a permutation of the components of X must
result in the distribution of an equally permuted random vector, and if X1 ∩X2 ̸= ∅, the
marginal distributions on the intersection starting from the ones for X1 and X2 must be
identical. Formally, for every n ∈N>0, x1, . . . , xn ∈X, Borel sets B1, . . . , Bn and every
2Has an addition +, an origin 0 and a negation −.
permutation π of {1, . . . , n} we must have
 π(1),...,
 π(n)(Bπ(1) × · · · × Bπ(n)) = µ
 n(B1 × · · · × Bn)
 n(B1 × · · · × Bn−1 × R) = µ
 n−1(B1 × · · · × Bn−1).
Importantly, Kolmogorov proved that symmetry and consistency are also suﬃcient
conditions for such a speciﬁcation to guarantee the existence of a random process (in the
concrete measure-theoretic sense) with these f.d.d.’s. The question about uniqueness of random processes is tricky, because two processes can be equivalent (u(x) = v(x) almost surely
for every x; equivalent processes are called versions of each other), yet diﬀer signiﬁcantly
w.r.t. almost sure properties of their sample paths. For example, one can construct a version of a smooth process whose sample paths are not diﬀerentiable at a ﬁnite number of
points almost surely. In the context of machine learning applications we are interested here,
sample path properties such as diﬀerentiability are of lesser importance, and we will focus
on m.s. properties (to be introduced shortly) which can be characterised more directly and
are invariant under change of version. In other words, we will in general identify a process
with the equivalence class of all its versions or with a particularly “nice” member of this
class,3 and the simple nature of the applications we are interested in here guarantees the
admissability of this practice. We will see that global sample path properties of a process (in
this sense) such as smoothness and average variability are directly related to corresponding
m.s. properties. See Adler for methods of studying sample path properties.
Let {Xn} be a sequence of real-valued random variables, and recall that Xn →X (n →∞)
in quadratic mean (or in mean square (m.s.)) if E[|Xn−X|2] →0. M.s. convergence is weaker
than almost sure (a.s.) convergence, but turns out to be the most useful mode for discussing
GP aspects we require here. In general, X and Y are m.s. equivalent if E[|X −Y |2] = 0. In a
nutshell, for a property which is traditionally deﬁned in terms of limits (such as continuity,
diﬀerentiability, etc.) within R we can typically deﬁne the corresponding m.s. property for
scalar random variables by substituting normal for m.s. convergence.
Suppose that u(x) is a random process. The ﬁrst and second-order statistics of u(x) are its
mean function m(x) = E[u(x)] and covariance function
K(x, x′) = E
(u(x) −m(x))(u(x′) −m(x′))
Obviously, both depend on the f.d.d.’s of the process only. The covariance function is central to studying characteristics of the process in the mean square sense. It is a positive
semideﬁnite4 function in the sense that for every n ∈N, x1, . . . , xn ∈X, z1, . . . , zn ∈R:
zizjK(xi, xj) ≥0.
This is clear because for X = P
i zi(u(xi) −m(xi)) we have E[|X|2] ≥0. Positive semidefiniteness means that for every ﬁnite set X ⊂X the symmetric matrix K(X, X) ∈R|X|,|X|
obtained by evaluating K on X × X is positive semideﬁnite. Note that this implies that
K(x, x) ≥0 for all x. K is called positive deﬁnite if (1) holds with > whenever z ̸= 0.
3As an example, a Wiener process (see Section 2.3) always has a version with continuous sample paths.
4This term is not uniquely used in the literature, it is sometimes replaced by non-negative deﬁnite or
even positive deﬁnite (which has a diﬀerent meaning here).
The positive semideﬁniteness of K leads to an important spectral decomposition which is
discussed in Section 5. A positive semideﬁnite K will also be referred to as kernel, pointing
out its role as kernel for a linear integral operator (see Section 5).
Stationary Processes
In many situations, the behaviour of the process does not depend on the location of the
observer, and under this restriction a rich theory can be developed, linking local m.s. properties of the process to the behaviour of K close to the origin. A process is called strictly
homogeneous (or strictly stationary) if its f.d.d.’s are invariant under simultaneous translation of their variables. This implies that m(x) is constant and K(x, x′) is a function of
x −x′ (we write K(x, x′) = K(x −x′) in this case). A process fulﬁlling the latter two conditions is called (weakly) homogeneous (or (weakly) stationary). For a stationary process,
the choice of the origin is not reﬂected in the statistics up to second order. If K(0) > 0,
ρ(x) = K(x)
is called correlation function. A stationary process has a spectral representation as a stochastic Fourier integral (e.g., , Chap. 2; , Chap. 9; ), based on Bochner’s theorem which
(for X = Rg) asserts that ρ(x) is positive semideﬁnite, furthermore uniformly continuous
with ρ(0) = 1, |ρ(x)| ≤1 iﬀit is the characteristic function of a variable ω, i.e.
for a probability distribution function F(ω). If F(ω) has a density f(ω) (w.r.t. Lebesgue
measure), f is called spectral density. This theorem allows to prove positive semideﬁniteness
of K by computing its Fourier transform and checking that it is non-negative. If so, it must
be proportional to the spectral density. Note that since ρ(x) is an even function, the spectral
distribution is symmetric around 0, and if f(ω) exists it is even as well.
The f.d.d.’s of a process determine its mean square properties, while this is not true in
general for almost sure properties (such as continuity or diﬀerentiability of sample paths).
Even stronger, for a zero-mean process, m.s. properties are usually determined entirely
by the covariance function K(x, x′). For stationary processes, it is merely the behaviour
of K(x) at the origin which counts: the m.s. derivative5 D
  u(x) exists everywhere iﬀ
  K(x) exists at x = 0. Thus, the smoothness of the process in the m.s. sense grows
with the degree of diﬀerentiability at 0. For example, a process with the RBF (Gaussian)
covariance function K (27) is m.s. analytic, because K is analytic (diﬀerentiable up to any
order) at 0.
Isotropic Processes
A stationary process is called isotropic if its covariance function K(x) depends on ∥x∥
only. In this case, the spectral distribution F is invariant under isotropic isomorphisms
(e.g., rotations). Loosely speaking, second-order characteristics of an isotropic process are
denotes a diﬀerential functional, such as ∂2/(∂x1∂x2).
the same from whatever position and direction they are observed. It is much simpler to
characterise isotropic correlation functions than stationary ones in general. Let ρ(τ) = ρ(x)
for τ = ∥x∥. The spectral decomposition (2) simpliﬁes to
Λg/2−1(τ ω)dF(ω)
where F(ω) =
 ∥≤ω} dF(ω) is a distribution function for ω ≥0 and
Λν(z) = Γ(ν + 1)
(z/2)ν Jν(z),
where Jν(z) is a Bessel function of the ﬁrst kind (see , Sect. 2.5). Recall that g is the dimensionality of the input space X = Rg. The right hand side in (3) is the Hankel transform
of order g/2 −1 of F (see , Sect. 2.10). Alternatively, if the spectral density f(ω) exists
and f(ω) = f(ω) for ω = ∥ω∥, then dF(ω) = Ag−1ωg−1f(ω) dω,6 so we can easily convert
to the spectral representation in terms of f(ω). Denote the set of ρ(τ) corresponding to
isotropic correlation functions in Rg by Dg. Note that (3) characterises Dg (by Bochner’s
theorem). It is clear that Dg+1 ⊂Dg, since an isotropic correlation function in Rg+1 restricted to a g-dimensional subspace is in Dg. Beware that both F(ω) and f(ω) depend
on the dimension g for which ρ(τ) is used to induce a correlation function (see (3)). Let
g≥1 Dg. Since
→e−x2 (g →∞),
one can show that ρ(τ) ∈D∞iﬀρ(τ) =
exp(−τ 2ω2) dF(ω) (this result is due to Schoenberg).
Note that the assumption of isotropy puts strong constraints on the correlation function,
especially for large g. For example, ρ(τ) ≥inf x Λg/2−1(x) ≥−1/g so large negative correlations are ruled out. If ρ(τ) ∈D∞, it must be non-negative. Furthermore, for large g ρ(τ) is
smooth on (0, ∞) while it may have a jump at 0 (additive white noise). If ρ(τ) ∈Dg and
B ∈Rg,g is nonsingular, then
  (x) = ρ(∥Bx∥)
is a correlation function as well, called anisotropic. Examples of (an)isotropic covariance
functions are given in Section 9.
Two Views on Gaussian Processes
A Gaussian process (GP) is a process whose f.d.d.’s are Gaussian. Since a Gaussian is
determined by its ﬁrst and second-order cumulants and these involve pairwise interactions
only, its f.d.d.’s are completely determined by mean and covariance function. This means
that for GPs, strong and weak stationarity are the same concept. GPs are by far the most
accessible and well-understood processes (on uncountable index sets). It is clear that for
every positive semideﬁnite function K there exists a zero-mean GP with K as covariance
function (by Kolmogorov’s theorem), so GPs as modelling tool are very ﬂexible. Importantly,
by choosing K properly we can encode properties of the function distribution implicitly as
we desired in Section 1.
6Ag−1 = 2πg/2/Γ(g/2) is the surface area of the unit sphere in
In conjunction with latent variable modelling techniques, a wide variety of non-parametric
models can be constructed (see Section 3). The fact that all f.d.d.’s are Gaussian with
covariance matrices induced by K(x, x′) can be used to obtain approximations to Bayesian
inference fairly straightforwardly (see Section 4), and these approximations often turn out
to be much more accurate than for parametric models of equal ﬂexibility (such as multilayer perceptrons). It is interesting to note that m.s. derivatives D
  u(x) of a GP are GPs
again (if they exist), and
 ′ K(x, x′),
thus derivative observations can be incorporated into a model in the same way as function
value observations (for applications, see ). Characteristics such as m.s. diﬀerentiability up to a given order can be controlled via the covariance function (see Section 2.1), an
example is given in Section 9.
One of the most thoroughly studied GPs is the Wiener process (or Brownian motion, or continuous random walk) with covariance function K(x, x′) = σ2 min{x, x′} (here, X = R≥0;
for multivariate generalisations to Brownian sheets, see , Chap. 8). It is characterised by
u(0) = 0 a.s., E[|u(x + h) −u(x)|2] = σ2h, h ≥0, and by having orthogonal7 increments:
E[(u(x1) −u(x2))(u(x3) −u(x4))] = 0, x1 ≤x2 ≤x3 ≤x4. Note that u(x) is not stationary,
a stationary version with orthogonal increments is the Ornstein-Uhlenbeck process (see Section 9.1). The Wiener process is an example for a diﬀusion process. It has a large number of
applications in mathematics, physics and mathematical ﬁnance. The property of orthogonal
increments allows to deﬁne stochastic integrals (e.g., , Chap. 13) with a Wiener process
as (random) measure. u(x) is m.s. continuous everywhere, but not m.s. diﬀerentiable at
any point. In fact, a version of the Wiener process can be constructed which has continuous
sample paths, but for every version sample paths are nowhere diﬀerentiable with probability
1. The Wiener process can be used to explicitly construct other GPs by means of stochastic
integrals, the procedure is sketched in Section 6.
We now develop two elementary views on Gaussian processes, the process and the weight
space view. While the former is usually much simpler to work with, the latter allows us
to relate GP models to parametric linear models rather directly. We follow .8 The process view on a zero-mean GP u(x) with covariance function K is in the spirit of the GP
deﬁnition given above. u(x) is deﬁned implicitly, in that for any ﬁnite subset X ⊂X it
induces a f.d.d. N(0, K(X)) over the vector u = u(X) of process values at the points X.
Here, K(X) = K(X, X) = (K(xi, xj))i,j where X = {x1, . . . , xn}. Kolmogorov’s theorem
guarantees the existence of a GP with this family of f.d.d.’s.9 In practice, many modelling
problems involving an unknown functional relationship u(x) can be formulated such that
only ever a ﬁnite number of linear characteristics of u(x) (e.g., evaluations or derivatives
of u(x)) are linked to observations or predictive queries, and in such cases the process
view boils down to dealing with the “projection” of the GP onto a multivariate Gaussian
distribution, thus to simple linear algebra of quadratic forms.10
7Orthogonality implies independence since the process is Gaussian.
8We use the term “process view” instead of “function space view” employed in . The relationship
between GPs and associated spaces of smooth functions is a bit subtle and introduced only below in Section 5.
9If K is continuous everywhere, a version exists with continuous sample paths, but we do not require this
10In practice, some knowledge of numerical mathematics is required to avoid numerically instable proce-
GPs can also be seen from a weight space viewpoint, relating them to the linear model. In
the Bayesian context this view was ﬁrst suggested by O’Hagan as a “localised regression
model” (the weight space is ﬁnite-dimensional there) while the generalisation to arbitrary
GP priors developed there uses the process view. This paper is among the ﬁrst to address GP
regression in a rigorous Bayesian context, while the equivalence between spline smoothing
and Bayesian estimation of processes was noticed earlier by Kimeldorf and Wahba (see
Section 6). Recall the linear model
y = Φ(x)T β + ε,
where Φ(x) is a feature map from the covariate x and ε is independent Gaussian noise.
Every GP whose covariance function satisﬁes weak constraints can be written as (4), albeit
with possibly inﬁnite-dimensional weight space. To develop this view, we use some facts
which are discussed in detail below in Section 5. Under mild conditions on the covariance
function K(x, x′) of u(x), we can construct a sequence
which converges to u(x) in quadratic mean (k →∞).11 Here, βν are i.i.d. N(0, 1) variables. φν are orthonormal eigenfunctions of the operator induced by K with corresponding
eigenvalues λ1 ≥λ2 ≥· · · ≥0, P
ν < ∞, in a sense made precise in Section 5.
Thus, if β = (βν)ν and Φ(x) = (λ1/2
φν(x))ν, then u(x) = Φ(x)T β in quadratic mean,
and Φ(x)T Φ(x′) = K(x, x′). This is the weight space view on GPs and allows to view a
non-parametric regression model
y = u(x) + ε
as direct inﬁnite-dimensional generalisation of the linear model (4) with spherical Gaussian
prior on β. We say that Φ(x) maps into a feature space which is typically (countably)
inﬁnite-dimensional. It is important to note that in this construction of the feature map
Φ(x) the individual components λ1/2
φν(x) do not have the same scaling, in the sense that
their norm in L2(µ) (the Hilbert space they are drawn from and that K operates on) is
→0 (ν →∞). They are comparable in a diﬀerent (RKHS) norm which scales with
the “roughness” of a function. Intuitively, as ν →∞, the graph of φν becomes rougher and
increasingly complicated, see Section 5 for details.
For all inference purposes which are concerned with f.d.d.’s of u(x) and its derivatives (or
other linear functionals) only, the process and the weight space view are equivalent: they lead
to identical results. However, we feel that often the process view is much simpler to work
with, avoiding spurious inﬁnities12 and relying on familiar Gaussian manipulations only.
On the other hand, the weight space view is more frequently used at least in the machine
learning literature, and its peculiarities may be a reason behind the perception that GP
models are diﬃcult to interpret. There is also the danger that false intuitions or conclusions
dures. Since most matrices to be dealt with are positive semideﬁnite, this is not too hard. Some reliable
techniques are mentioned in Section 4.
11We only need pointwise m.s. convergence, although much stronger statements are possible under mild
assumptions, e.g. , Sect. 3.3.
12Which seem to cancel out almost “magically” in the end from the weight space viewpoint, while inﬁnities
do not occur in the process view in the ﬁrst place.
are developed from interpolating geometrical arguments from low-dimensional Euclidean
space to the feature space.13 We should also note that a weight space representation of a
GP in terms of a feature map Φ is of course not unique. The route via eigenfunctions of the
covariance operator is only one way to establish such.14 About the only invariant is that we
always have Φ(x)T Φ(x′) = K(x, x′).
Gaussian Processes as Limit Priors of Parametric Models
We conclude this section by mentioning that one of the prime reasons for focusing current
machine learning interest on GP models was a highly original diﬀerent way of establishing
a weight space view proposed in . Consider a model
vjh(x; u(j))
which could be a multi-layer perceptron (MLP) with hidden layer functions h, weights
u(j) and output layer weights v. Suppose that u(j) have independent identical priors s.t.
the resulting h(x; u(j)) are bounded almost surely over a compact region of interest. Also,
vj ∼N(0, ω2/H) independently. Then, for H →∞, f(x) converges in quadratic mean to a
zero-mean GP with covariance function ω2E
  [h(x; u)h(x′; u)]. Stronger conditions would
assure almost sure convergence uniformly over a compact region. The bottom line is that
if we take a conventional parametric model which linearly combines the outputs of a large
number of feature detectors, and if we scale the outputs s.t. each of them in isolation has
only a negligible contribution to the response, we might just as well use the corresponding
Gaussian process model. Neal also shows that if a non-zero number of the non-Gaussian
feature outputs have a signiﬁcant impact on the response with non-zero probability, then
the limit process is typically not Gaussian.
To conclude, the weight space view seems to relate non-parametric GP models with parametric linear models fairly directly. However, there are important diﬀerences in general.
Neal showed that GPs are obtained as limit distributions of large linear combinations of
features if each feature’s contribution becomes negligible, while the output distributions of
architectures which ﬁt at least a few strong feature detectors are typically not Gaussian.
Predictions from a GP model are smoothed versions of the data (in a sense made concrete in Section 6), i.e. interpolate by minimising general smoothness constraints encoded
in the GP prior, as opposed to parametric models which predict by focusing on these functions (within the family) which are most consistent with the data. O’Hagan discusses
diﬀerences w.r.t. optimal design.
13Steinwart gives the following example. For a universal covariance function (most kernels discussed
here have this property), any two ﬁnite disjoint subsets of X can be separated by a hyperplane in feature
space, and the distances of all points to the plane can be made to lie in an interval of arbitrarily small size.
Steinwart concludes that “any ﬁnite dimensional interpretation of the geometric situation in a feature space
of a universal kernel must fail”. We strongly agree.
14For example, in Section 5 we discuss K’s role as reproducing kernel, in the sense that K(
′))K in some Hilbert space with inner product (·, ·)K. We could deﬁne Φ to map
and use the Hilbert space as weight space.
Some Gaussian Process Models
The simplest Gaussian process model is useful for regression estimation:
y = u + ε,
where u = u(x) is a priori a zero-mean Gaussian process with covariance function K and ε
is independent N(0, σ2) noise. Inference for this model is simple and analytically tractable,
because the observation process y(x) is zero-mean Gaussian with covariance K(x, x′) +
 ′.15 Given some i.i.d. data S = {(xi, yi) | i = 1, . . . , n}, let K = (K(xi, xj))i,j. Then,
P(u) = N(0, K) and
P(u|S) = N
 K(σ2I + K)−1y, σ2(σ2I + K)−1K
where u = (u(xi))i. For some test point x∗distinct from the training points, u∗=
u(x∗) ⊥y | u, so that
P(u∗|x∗, S) =
P(u∗|x∗, u)P(u|S) du
 u∗|k(x∗)T (σ2I + K)−1y, K(x∗, x∗) −k(x∗)T (σ2I + K)−1k(x∗)
Here, k(x∗) = (K(xi, x∗))i. We see that for this model, the posterior predictive process
u(x) given S is Gaussian with mean function yT (σ2I + K)−1k(x) and covariance function
K(x, x′) −k(x)T (σ2I + K)−1k(x′).
Note that the mean function used for prediction is linear in the targets y for every ﬁxed
x∗. Furthermore, the posterior covariance function does not depend on the targets at all.
In practice, if only posterior mean predictions are required, the prediction vector ξ =
(σ2I + K)−1y can be computed using a linear conjugate gradients solver which runs in
O(n2) if the eigenvalue spectrum of K shows a fast decay. If predictive variances for many
test points are required, the Cholesky decomposition16 σ2I+K = LLT should be computed,
after which each variance computation requires a single back-substitution.
The pointwise predictive variance is never larger than the corresponding prior variance, but
the shrinkage decreases with increasing noise level σ2. The same result can be derived in
the weight space view with u(x) = Φ(x)T β, applying the standard derivation of Bayesian
linear regression (e.g., ). Note that just as in parametric linear regression, the smoothed
prediction E[u|S] is a linear function of the observations y, as is the mean function of the
predictive process E[u(x)|S] (see also Section 8). Note also that if K(x, x′) →0 as ∥x −x′∥
gets big, predictive mean and variance for points x far from all data tend to prior mean
0 and prior variance K(x, x). Second-level inference problems such as selecting values for
hyperparameters (parameters of K and σ2) or integrating them out are not analytically
15In the context of this model, it is interesting to note that if K′ is stationary and continuous everywhere
except at 0, it is the sum of a continuous (stationary) covariance K and a white noise covariance ∝δ
Furthermore, Sch¨onberg conjectured that if K′ is an isotropic bounded covariance function, it must be
continuous except possibly at 0.
16A symmetric matrix is positive deﬁnite iﬀit has a (unique) Cholesky decomposition
  T , where
lower triangular with positive diagonal elements.
tractable and approximations have to be applied. Approximate model selection is discussed
in Section 4.
We can generalise this model by allowing for an arbitrary “noise distribution” P(y|u),
retaining the GP prior on u(x). The generative view is to sample the process u(·) from
the prior, then yi ∼P(yi|u(xi)) independent from each other given u(·).17 The likelihood
function factors as a product of univariate terms:
P(y|X, u(·)) = P(y|u) =
Since the likelihood depends on u(·) only via the ﬁnite set u, the predictive posterior process
can be written as
dP(u(·)|S) = P(u|S)
P(u) dP(u(·)),
i.e. P(u(X)|S) = (P(u|S)/P(u))P(u(X)) for any ﬁnite X ⊂X. The prior measure is
“shifted” by multiplication with P(u|S)/P(u) depending on the process values u at the
training points only. The predictive process is not Gaussian in general, but its mean and
covariance function can be obtained from knowledge of the posterior mean and covariance
matrix of P(u|S) as discussed in Section 4. For a test point x∗,
P(y∗|x∗, S) = E [P(y∗|u∗)]
where the expectation is over the predictive distribution of u∗= u(x∗). In this general
model, ﬁrst-level inference is not analytically tractable. In Section 4 a general approximate
inference framework is discussed. Markov Chain Monte Carlo (MCMC) methods can be
applied fairly straightforwardly, for example by Gibbs sampling from the latent variables
u . Such methods are attractive because the marginalisation over hyperparameters can
be dealt with in the same framework. However, naive realisations may have a prohibitive
running time due to the large number of correlated latent variables, and more advanced
techniques can be diﬃcult to handle in practice. While MCMC is maybe the most advanced
and widely used class of approximate inference techniques, it is not discussed in any further
detail here (see for a review).
Generalised Linear Models. Binary Classiﬁcation
A large class of models of this kind is obtained by starting from generalised linear models
(GLMs) and replacing the parametric linear function xT β by a process u(x) with
a GP prior. This can be seen as direct inﬁnite-dimensional generalisation of GLMs by
employing the weight space view (see Section 2). In the spline smoothing context, this
framework is presented in detail in . It employs noise distributions
P(y|u) = exp
 φ−1(y u −Z(u)) + c(y, φ)
i.e. P(y|u) is in an exponential family with natural parameter u, suﬃcient statistics y/φ and
log partition function φ−1Z(u). Here, φ > 0 is a scale hyperparameter. The linear model is
17This is generalised easily to allow for bounded linear functionals of the latent process u(·) instead of the
evaluation functional δ
i, as discussed in Section 5.
a special case with φ = σ2, u = µ = Eu[y] and Z(u) = (1/2)u2. A technically attractive
feature of this framework is that log P(y|u) is strictly concave in u, leading to a strictly
log-concave, unimodal posterior P(u|S). For binary classiﬁcation and y ∈{−1, +1}, the
GLM for the binomial noise distribution is logistic regression with the logit noise
P(y|u) = σ(y (u + b)), σ(t) =
Here, φ = 2 and Z(u) = 2 log cosh((u + b)/2). Another frequently used binary classiﬁcation
noise model is probit noise
P(y|u) = Φ(y (u + b)) = Eτ∼N(0,1)
I{y(u+b)+τ>0}
which can be seen as noisy Heaviside step and is not in the exponential family. Both noise
models (8), (9) are strictly log-concave.
Models with C Latent Processes
We can also allow for a ﬁxed number C ≥1 of latent variables for each case (x, y), i.e. C
processes uc(x). The likelihood factors as
P(yi|u(i)),
u(i) = (uc(xi))c.
uc(x) is zero-mean Gaussian a priori with covariance function K (c). While it is theoretically
possible to use cross-covariance functions for prior covariances between uc for diﬀerent c,
it may be hard to come up with a suitable class of such functions.18 Furthermore, the
assumption that the processes uc are independent a priori leads to large computational
savings, since the joint covariance matrix over the data assumes block-diagonal structure.
Note that in this structure, we separate w.r.t. diﬀerent c, while in block-diagonal structures
coming from the factorised likelihood we separate w.r.t. cases i.
An important example using C latent processes is C-class classiﬁcation. The likelihood
comes from a multinomial GLM (or multiple logistic regression). It is convenient to use a
binary encoding for the class labels, i.e. y = δc for class c ∈{1, . . . , C}.19 The noise is
multinomial with
µ = E[y | u] = softmax(u) =
 1T exp(u)
−1 exp(u).
u 7→µ is sometimes called softmax mapping. Note that this mapping is not invertible, since
we can add α1 to u for any α without changing µ. In other words, the parameterisation of
the multinomial by u is overcomplete, due to the linear constraint yT 1 = 1 on y, and the
corresponding GLM log partition function
Z(u) = log 1T exp(u)
is not strictly convex. The usual remedy is to constrain u by for example ﬁxing uC = 0. This
is ﬁne in the context of ﬁtting parameters by maximum likelihood, but may be problematic
18Hyperparameters may be shared between the prior processes, making them marginally dependent.
19We use vector notation for
C associated with a single case. This should not be confused with
the vector notation
n used above to group variables for all cases.
for Bayesian inference. As mentioned above, we typically use priors which are i.i.d. over the
uc, so if we ﬁx uC = 0, the induced prior on µ is not an exchangeable distribution (i.e.
component permutations of u can have diﬀerent distributions) and µC is singled out for
no other than technical reasons. We think it is preferable in the Bayesian context to retain
symmetry and accept that u 7→µ is not 1-to-1. Dealing with this non-identiﬁability during
inference approximations is not too hard since softmax is invertible on any plane orthogonal
to 1 and Z(u) is strictly convex on such. Anyway, this detail together with the two diﬀerent
blocking structures mentioned above renders implementations of approximate inference for
the C-class model somewhat more involved than the binary case (see for an example).
Other examples for C-process models are ordinal regression (“ranking”) models (see 
for likelihood suggestions) or multivariate regression.
Robust Regression
GP regression with Gaussian noise can lead to poor results if the data is prone to outliers,
due to the light tails of the noise distribution. A robust GP regression model can be obtained by using a heavy-tailed noise distribution P(y|u) such as a Laplace or even Student-t
distribution. An interesting idea is to use the fact that the latter is obtained by starting
with N(0, τ −1) and to integrate out the precision τ over a Gamma distribution (e.g., ).
Thus, a robust model can be written as
y = u + ε,
ε ∼N(0, τ −1),
where τ is drawn i.i.d. from a Gamma distribution (whose parameters are hyperparameters).
The posterior P(u|S, τ ) conditioned on the precision values τi is Gaussian and is computed
in the same way as for the case τi = σ−2 above. τ can be sampled by MCMC, or may be
chosen to maximise the posterior P(τ |S). The marginal likelihood P(y|τ ) is Gaussian and
can be computed easily. However, note that in the latter case the number of hyperparameters
grows as n which might invalidate the usual justiﬁcation of marginal likelihood maximisation
(see Section 4).
Approximate Inference and Learning
We have seen in the previous section that the posterior process for a likelihood of the general
form (6) can be written as “shifted” version (7) of the prior. About the only processes (in
this context) which can be dealt with feasibly are Gaussian ones, and a general way of
obtaining a GP approximation to the posterior process is to approximate P(u|S) by a
Gaussian Q(u),20 leading to the process
dQ(u(·)) = Q(u)
P(u)dP(u(·))
which is Gaussian (recall from Section A.1 that this is a concise way of writing that
Q(u(X)) = (Q(u)/P(u))P(u(X)) for every ﬁnite X ⊂X). An optimal way of choosing
Q would be to minimise the relative entropy (Deﬁnition 1)
D[P(u(·)|S) ∥Q(u(·))] = D[P(u|S) ∥Q(u)].
20The conditioning on S in Q(·) is omitted for notational simplicity.
The equality is intuitively clear, since Q(u(·)), P(u(·)|S) and P(u(·)) are the same conditional on u. Formally, it follows from the fact that if dP(u(·)|S) ≪dQ(u(·)), then
dP(u(·)|S) = P(u|S)
Q(u) dQ(u(·)),
and otherwise D[P(u|S) ∥Q(u)] = ∞(recall our notation from Section A.1). At the minimum point (unique w.r.t. f.d.d.’s of Q) Q and P(·|S) have the same mean and covariance
function. This is equivalent to moment matching and requires us to ﬁnd mean and covariance matrix of P(u|S). Unfortunately, this is intractable in general for large datasets
and non-Gaussian noise. Any other Gaussian approximation Q(u) leads to a GP posterior
approximation Q(u(·)), and the intractable (11) can nevertheless be valuable as guideline.
Here, we are primarily interested in approximate inference methods for GP models which
employ GP approximations (10) to posterior processes via
Q(u) = N(u | Kξ, A).
Here, ξ, A can depend on the data S, the covariance function K (often via the kernel matrix
K) and on other hyperparameters. This class contains a variety of methods proposed in
the literature. Virtually all of these have a reduced O(n) parameterisation, since A has the
restricted form
 K−1 + I·,IDII,·
with D ∈Rd,d diagonal with positive entries and I ⊂{1, . . . , n}, |I| = d. For the methods
mentioned below in this section, d = n and I ·,I = I, but for sparse GP approximations (e.g.,
 ) we have d ≪n. In the latter case, ξ\I = 0 and we use ξ ∈Rd for simplicity,
replacing ξ in (12) by I·,Iξ.
From (10), the (approximate) predictive posterior distribution of u∗= u(x∗) at a test point
x∗is determined easily as Q(u∗|x∗, S) = N(u∗|µ(x∗), σ2(x∗)), where
µ(x∗) = kI(x∗)T ξ,
σ2(x∗) = K(x∗, x∗) −kI(x∗)T D1/2B−1D1/2kI(x∗),
B = I + D1/2KID1/2.
Here, kI(x∗) = (K(xi, x∗))i∈I. More generally, the GP posterior approximation has mean
function µ(x) and covariance function
K(x, x′) −kI(x)T D1/2B−1D1/2kI(x′).
predictive
distribution
P(y∗|x∗, S)
N(u∗|µ(x∗), σ2(x∗)). If this expectation is not analytically tractable, it can be done by
Gaussian quadrature (e.g., , Sect. 4.5) if P(y∗|u∗) is smooth and does not grow faster
than polynomial.
A simple and numerically stable way to determine the predictive variances is to compute the
Cholesky decomposition B = LLT after which each variance requires one back-substitution
with L. It is important to stress that while inference approximation in GP models often
boils down to simple linear algebra, it is crucial in practice to choose representations and
procedures which are numerically stable. In the presence of positive deﬁnite matrices, techniques based on the Cholesky factorisation are known to be most stable.21 Furthermore, in
our representation B is well-conditioned since all its eigenvalues are ≥1.
We will refer to ξ as prediction vector. More generally, as mentioned in Section 2, we can
use derivative information or other bounded linear functionals of the latent process u(x) in
the likelihood and/or for the variables to be predicted, using the fact that the corresponding
ﬁnite set of scalar variables is multivariate Gaussian with prior covariance matrix derived
from the covariance function K (as discussed in more detail in Section 5).
A generalisation to the multi-process models of Section 3 is also straightforward in principle.
Here, u has dimension C n. Again A is restricted to the form (13), although D is merely
block-diagonal with n (C × C) blocks on the diagonal. Moreover, if the processes are a
priori independent, both K and K−1 consist of C (n × n) blocks on the diagonal. The
general formulae for prediction (14) have to be modiﬁed for eﬃciency. The details are more
involved and may depend on the concrete approximation method, C-process models are not
discussed in further detail here.
Some Examples
A simple and eﬃcient way of obtaining a Gaussian approximation Q(u|S) is via Laplace’s
method (also called saddle-point approximation), as proposed in for binary classiﬁcation
with logit noise (8). To this end, we have to ﬁnd the posterior mode ˆu which can be done
by a variant of Newton-Raphson (or Fisher scoring, see ). Each iteration consists of a
weighted regression problem, i.e. requires the solution of an n × n positive deﬁnite linear
system. This can be done approximately in O(n2) using a conjugate gradients solver. At
the mode, we have
ξ = Y σ(−Y ˆu),
D = (diag σ(−Y ˆu))(diag σ(Y ˆu)),
where σ is the logistic function (8) and Y = diag y. All n diagonal elements of D are
positive. Recall that the Laplace approximation replaces the log posterior by a quadratic
ﬁtted to the local curvature at the mode ˆu. For the logit noise the log posterior is strictly
concave and dominated by the Gaussian prior far out, so in general a Gaussian approximation should be fairly accurate. On the other hand, the true posterior is signiﬁcantly skewed,
meaning that the mode can be quite distant from the mean (which would be optimal) and
the covariance approximation via local curvature around the mode can be poor.
The expectation propagation (EP) algorithm for GP models can signiﬁcantly outperform the Laplace GP approximation in terms of prediction accuracy, but is also more
costly.22 It is also somewhat harder to ensure numerical stability. On the other hand, EP
is more general and can for example deal with discontinuous or non-diﬀerentiable log likelihoods. In fact, the special case of EP for Gaussian ﬁelds has been given earlier by Opper and
Winther under the name ADATAP, and EP can be seen as an iterative generalization
of older Bayesian online learning techniques.
21Matrix inversion is often recommended in the GP machine learning literature. It is well known in
numerical mathematics that inversion should be avoided whenever possible for reasons of stability, and in
the context of our GP framework using a Cholesky decomposition is even more eﬃcient.
22Partly due to its more complex iterative structure, but also because its elementary steps are smaller
than for the Laplace technique and cannot be vectorised as eﬃciently.
A range of diﬀerent variational approximations have been suggested in . Note
that for the variational method where Q(u|S) is chosen to minimise D[· ∥P(u|S)], it is easy
to see that the best Gaussian variational distribution has a covariance matrix of the form
(13) (e.g., , Sect. 5.2.1).
Sparse approximations to GP inference are developed in . While the original
application was online learning, they are understood easier as “sparsiﬁcations” of EP (or
ADATAP). While the approximations mentioned so far have training time scaling of O(n3),
sparse inference approximations reduce this scaling to O(n d2) with adjustable d ≪n. For
many problems, sparse approximations attain suﬃcient accuracy in essentially linear time
in n which allows the application in data-rich settings. The idea is to concentrate on a
subset I ⊂{1, . . . , n}, |I| = d of the training data which we call the active set, then to
approximate the true likelihood P(y|u) of the model by a likelihood approximation Q(uI)
which is a function of the components uI only. With this replacement, inference becomes
linear in n (as can be seen from the formulae in this section which allow the use of an active
set). The challenge is how to choose I and the form for Q(uI) in a way to best approximate
the moments of the true posterior P(u|y), while staying within the resource limitations of
O(n d2) time and O(n d) memory.23 Also, if P(y|u) is not Gaussian, the sparse technique
has to be embedded in an inference approximation of the kind discussed in this section.
Details on some sparse schemes can be found in , some generic schemes based on
the EP algorithm and information-theoretic selection heuristics for I are described in .
Free Matlab software has been released by Lehel Csat´o.24
Model Selection
So far we have only been concerned with ﬁrst-level inference conditioned on ﬁxed hyperparameters. A useful general method has to provide some means to select good values for
these parameters or to marginalise over them (see Section 3). The latter is the correct way
to proceed in a strict Bayesian sense and can be approximated by MCMC techniques, but
often model selection is computationally more attractive. A frequently used general empirical Bayesian method for marginalising over nuisance hyperparameters is marginal likelihood
maximisation or maximum likelihood II (also called evidence maximisation). This technique
can be applied to the generic GP approximation described in this section, leading to a
powerful generic way of adjusting hyperparameters via nonlinear optimization which scales
linearly in the number of parameters. It is important to point out that such automatic
model selection techniques are a strong advantage of Bayesian GP methods over other kernel machines such as SVMs (see Section 7) for which we do not know of selection strategies
of similar power and generality.
If we denote the hyperparameters by α, the marginal likelihood is P(S|α) = P(y|α), where
the latent “primary” parameters u have been integrated out. If S is suﬃciently large and
α of rather small ﬁxed dimension, the hyperposterior P(α|S) frequently is highly concentrated around a mode ˆα. Instead of using P(α|S) to marginalise over α, we replace the
posterior by δ ˆ
  (α), thus simply plug in ˆα for α. This is an example of a maximum a pos-
23Choosing I completely at random is possible, but performs poorly in situations such as classiﬁcation
where the inﬂuence of patterns on the posterior can be very diﬀerent.
24See 
teriori (MAP) approximation.25 Finding ˆα basically amounts to maximising the marginal
likelihood, because the hyperprior P(α) is of a simple form. Conditions under which the
hyperposterior is suﬃciently peaked are hard to come by in general and will usually be
overrestrictive for realistic models.26 Thus, while marginal likelihood maximisation does
not solve the model selection problem in general, it has been shown to work well in many
empirical studies featuring very diﬀerent models, and its description as “plug-in” approximation to Bayesian marginalisation may lead to successful extensions in cases where the
simple method fails.
Some readers might worry at this point that we propose to select α by maximising the
likelihood P(y|α), and maximum likelihood techniques are prone to overﬁtting. The key
diﬀerence is that in the marginal likelihood, the primary “parameter” u(·) has been integrated out. While choosing primary parameters so as to maximise the likelihood often
leads to overcomplicated ﬁts that generalise badly, this is not true in general for marginal
likelihood maximisation. A simple argument (yet not a proof) is that a value α(1) leading to
very complicated u(·) needs to assign mass P(u(·)|α) to many more functions than a value
α(2) leading to simple u(·) (e.g. linear or low-order polynomial), so even if the likelihood
of y is much higher for some of the complicated u(·), in the process of marginalisation
the complicated functions are downweighted stronger in the integral for P(y|α(1)) than are
the simpler functions in the integral for P(y|α(2)). This “Occam razor” eﬀect has been
analysed by MacKay . However it is obviously possible to create situations in which
marginal likelihood maximisation still leads to overﬁtting.27 As a general rule of thumb, the
dimensionality of the hyperparameters α should not scale with n,28 and the Occam razor
argument just given should intuitively apply to the situation (once more, we do not know
of a deﬁnite test separating admissable from non-admissable cases in general).
We will focus on marginal likelihood maximisation as general model selection technique.
The log marginal likelihood log P(y|α) is as diﬃcult to compute as the posterior P(u|S, α)
and has to be approximated in general.29 It is easy to see that the variational lower bound
log P(y|α) ≥EQ [log P(y|u, α) + log P(u|α)] + H[Q(u)]
= EQ [log P(y|u, α)] −D[Q(u) ∥P(u|α)].
holds for any distribution Q(u) (recall relative and diﬀerential entropy from Section A.2).
The slack in the bound is the relative entropy D[Q(u) ∥P(u|S, α)]. Note that the posterior
approximation Q(u) depends on α as well, but it is not feasible in general to obtain its exact
gradient w.r.t. α. Variational EM, an important special case of a lower bound maximisation
algorithm is iterative, in turn freezing one of Q, α and maximising the lower bound w.r.t.
the other (here, Q can be chosen from a family of variational distributions). Alternatively,
25Multimodality in the hyperposterior can arise from non-identiﬁability of the model though symmetries in
 , i.e. there exist diﬀerent
 (2) s.t. P(
 (2)) for datasets of interest. In this
case, we can just pick any of the dominant modes ˆ
 in the hyperposterior to arrive at the same predictions
as if we had chosen a peak train featuring all equivalent modes.
26Since we integrate out a variable
 of the same dimension of the training sample and the latter is
independent only conditional on the process u(·) (which is not in general a ﬁnite-dimensional variable), we
cannot use the central limit theorem directly to assert Gaussianity of P(
 ) as n gets large.
27For example, one could maliciously set
28Although in special situations the technique may still be applicable, see or Section 3.3.
29It is analytically tractable for a Gaussian likelihood, for example in the case of GP regression with
Gaussian noise discussed above it is log N(
Q can be chosen in a diﬀerent way as approximation of the posterior P(u|S) (for example
using the EP algorithm or sparse approximations). The deviation from the variational choice
of Q (i.e. the one which maximises the lower bound over a family of candidates) can be
criticised on the ground that other choices of Q can lead to decreases in the lower bound,
so the overall algorithm does not increase its criterion strictly monotonically. On the other
hand, Q chosen in a diﬀerent way may lie outside families over which the lower bound can be
maximised eﬃciently, thus may even result in a larger value than the variational maximiser
within the family.30 Furthermore, the lower bound criterion can be motivated by the fact
that its gradient
 log P(y, u|α)]
(ignoring the dependence of Q on α) approximates the true gradient
 log P(y|α) = EP (
 log P(y, u|α)]
at every point α.
We close by mentioning an interesting point in which lower bound maximisation for GP
models might deviate from the usual practice with parametric architectures. For the latter,
it is customary to maximise the lower bound w.r.t. α while keeping Q completely ﬁxed
(the gradient of Q w.r.t. α is ignored). This makes sense as long as Q is independent
of the prior distribution in the model, but in the context of approximate GP inference
methods, the dependence of Q(u) on the GP prior (thus on α) is quite explicit (for example,
the covariance of Q is (K−1 + D)−1 which depends strongly on the kernel matrix K,
since D is merely a diagonal matrix). We argue that instead of keeping all of Q ﬁxed
during the maximisation for α, we should merely ignore the dependence of the essential
parameters ξ, D on α.31 This typically leads to a more involved gradient computation
which is potentially closer to the true gradient. Alternatively, if this computation is beyond
resource limits, further indirect dependencies on α may be ignored. We remark that the
optimisation problem is slightly non-standard due to the lack of strict monotonicity, and
given optimisers have to be modiﬁed to take this into account. Details can be found in ,
Sect. 4.5.3.
Reproducing Kernel Hilbert Spaces
The theory of reproducing kernel Hilbert spaces (RKHS) can be used to characterise the
space of random variables obtained as bounded linear functionals of a GP on which any
method of prediction from ﬁnite information must be based. Apart from that, RKHS provide
a uniﬁcation of ideas from a wide area of mathematics, most of which will not be mentioned
here. The interested reader may consult . Our exposition is taken from . This section
can be skipped by readers interested primarily in practical applications.
A reproducing kernel Hilbert space (RKHS) H is a Hilbert space of functions X →R for
which all evaluation functionals δ
 are bounded. This implies that there exists a kernel
30For example, even though the bound maximiser over all Gaussians has a covariance matrix of the form
(13), ﬁnding it is prohibitively costly in practice and proposed variational schemes use restricted
subfamilies.
31There is no simple analytic formula for this dependence, so we cannot do better than ignoring it.
K(x, x′) s.t. K(·, x) ∈H for all x ∈X and
  f = (K(·, x), f)
for all f ∈H, where (·, ·) is the inner product in H. To be speciﬁc, a Hilbert space is a vector
space with an inner product which is complete, in the sense that each Cauchy sequence
converges to an element of the space. For example, a Hilbert space H can be generated
from an inner product space of functions X →R by adjoining the limits of all Cauchy
sequences to H. Note that this is a rather abstract operation and the adjoined objects need
not be functions in the usual sense. For example, L2(µ) is obtained by completing the vector
space of functions for which
f(x)2 dµ(x) < ∞
and can be shown to contain “functions” which are not deﬁned pointwise.32 For an RKHS
H such anomalies cannot occur, since the functionals δ
 are bounded:33
|f(x)| = |δ
By the Riesz representation theorem (e.g., ) there exists a unique representer K
such that (17) holds with K(·, x) = K
  . It is easy to see that the kernel K is positive
semideﬁnite. K is called reproducing kernel (RK) of H, note that
 K(·, x), K(·, x′)
= K(x, x′).
It is important to note that in a RKHS, (norm) convergence implies pointwise convergence
to a pointwise deﬁned function, since
|fm(x) −f(x)| = |(K
  , fm −f)| ≤C
  ∥fm −f∥.
On the other hand, for any positive semideﬁnite K there exists a unique RKHS H with RK
K. Namely, the set of ﬁnite linear combinations of K(·, xi), xi ∈X with
aiK(·, xi),
aibjK(xi, x′
is an inner product space which is extended to a Hilbert space H by adjoining all limits
of Cauchy sequences. Since norm convergence implies pointwise convergence in the inner
product space, all adjoined limits are pointwise deﬁned functions and H is an RKHS with
RK K. To conclude, a RKHS has properties which make it much “nicer” to work with
than a general Hilbert space. All functions are pointwise deﬁned, and the representer of the
evaluation functional δ
 is explicitly given by K(·, x).
32The existence of such functions in L2(µ) means that expressions such as (18) have to be interpreted with
some care. Each element f ∈L2(µ) can be deﬁned as the set of all equivalent Cauchy sequences which deﬁne
f (two Cauchy sequences are equivalent if the sequence obtained by interleaving them is Cauchy as well).
An expression E(f, g) should then be understood as the limit limn→∞E(fn, gn) where fn →f, gn →g,
etc. The existence of the limit has to be established independently. In the sequel, we will always use this
convention.
33Bounded functionals are also called continuous.
RKHS by Mercer Eigendecomposition. Karhunen-Loeve Expansion
We have already mentioned that L2(µ) is not a RKHS in general, but for many kernels K it
contains a (unique) RKHS as subspace. Recall that L2(µ) contains all functions f : X →R
for which (18) holds. The standard inner product is
f(x)g(x) dµ(x).
Often, µ is taken as indicator function of a compact set such as the unit hypercube. A positive semideﬁnite K(x, x′) can be regarded as kernel (or representer) of a positive semideﬁnite
linear operator K in the sense
(Kf)(x) = (K(·, x), f).
φ is an eigenfunction of K with eigenvalue λ ̸= 0 if
(Kφ)(x) = (K(·, x), φ) = λ φ(x).
For K, all eigenvalues are real and non-negative. Furthermore, suppose K is continuous and
K(x, x′)2 dµ(x)dµ(x′) < ∞.
Then, by the Mercer-Hilbert-Schmidt theorems there exists a countable orthonormal sequence of continuous eigenfunctions φν ∈L2(µ) with eigenvalues λ1 ≥λ2 ≥· · · ≥0, and K
can be expanded in terms of these:
K(x, x′) =
λνφν(x)φν(x′),
ν < ∞, thus λν →0(ν →∞). This can be seen as generalisation of the
eigendecomposition of a positive semideﬁnite Hermitian matrix. Indeed, the reproducing
property of positive semideﬁnite kernels was recognised and used by E. H. Moore to
develop the notion of general “positive Hermitian matrices”. In this case, we can characterise
the RKHS embedded in L2(µ) explicitly. For f ∈L2(µ), deﬁne the Fourier coeﬃcients
fν = (f, φν).
Consider the subspace HK of all f ∈L2(µ) with P
ν < ∞. Then, HK is a Hilbert
space with inner product
moreover the Fourier series P
ν≥1 fνφν converges pointwise to f.34 Since {λνφν(x)} are the
Fourier coeﬃcients of K(·, x) (using Equation 19), we have
(f, K(·, x))K =
fνφν(x) = f(x),
34In particular, f is deﬁned pointwise.
thus K is the RK of HK. It is important to distinguish clearly between the inner products
(·, ·) in L2(µ) and (·, ·)K in HK (see for more details about the relationship of these
inner products). While ∥·∥measures “expected squared distance” from 0 (w.r.t. dµ), ∥·∥K is
a measure of the “roughness” of a function. For example, the eigenfunctions have ∥φν∥= 1,
but ∥φν∥K = λ−1/2
thus becoming increasingly rough.35
The spectral decomposition of K leads to an important representation of a zero-mean GP
u(x) with covariance function K: the Karhunen-Loeve expansion. Namely, the sequence
where uν are independent N(0, λν) variables, converges to u(x) in quadratic mean (a
stronger statement under additional conditions can be found in ). Moreover,
u(x)φν(x) dµ(x)
which is well deﬁned in quadratic mean. We have already used this expansion in Section 2
to introduce the “weight space view”. Note that since the variances λν decay to 0, the GP
can be approximated by ﬁnite partial sums of the expansion (see ).
Duality between RKHS and Gaussian Process
If u(x) is a zero-mean GP with covariance function K, what is the exact relationship between
u(x) and the RKHS with RK K? One might think that u(x) can be seen as distribution
over HK, but this is wrong (as pointed out in , Sect. 1.1). In fact, for any version of
u(x) sample functions from the process are not in HK with probability 1! This can be seen
by noting that for the partial sums (20) we have
= k →∞(k →∞).
Roughly speaking, HK contains “smooth”, non-erratic functions from L2(µ), characteristics
we cannot expect from sample paths of a random process. A better intuition about HK is
that it will turn out to contain expected values of u(x) conditioned on a ﬁnite amount of
information, thus the posterior mean functions we are interested in.
The following duality between HK and a Hilbert space based on u(x) was noticed in 
and is important in the context of theoretical analyses. Namely, construct a Hilbert space
HGP in the same way as above starting from positive semideﬁnite K, but replace K(·, xi)
by u(xi) and use the inner product
(A, B)GP = E[AB],
aibjK(xi, x′
35In the same sense as high-frequency components in the usual Fourier transform.
HGP is a space of random variables, not functions, but it is isometrically isomorphic to HK
under the mapping u(xi) 7→K(·, xi), with
(u(x), u(x′))GP = E[u(x)u(x′)] = K(x, x′) = (K(·, x), K(·, x′))K.
For most purposes, we can regard HGP as RKHS with RK K. The space HGP is important
in the context of inference on GP models we are interested in, because it contains exactly
the random variables we condition on or would like to predict in situations where only a
ﬁnite amount of information is available (from observations which are linear functionals of
the process).
If L is a bounded linear functional on HK, it has a representer ν ∈HK with ν(x) = LK
The isometry maps ν to a random variable Z ∈HGP which we formally denote by Lu(·).
E [(Lu(·))u(x)] = (ν, K
  )K = ν(x) = LK
More generally, if L(1), L(2) are functionals with representers ν(1), ν(2) s.t. x 7→L(j)K
in HK, then
(L(1)u(·))(L(2)u(·))
= (ν(1), ν(2))K = L(1)
 (K(·, x), ν(2))K = L(1)
Again, it is clear that Lu(·) is (in general) very diﬀerent from the process obtained by
applying L to sample paths of u(x). In fact, since the latter are almost surely not in HK,
L does not even apply to them in general. The correct interpretation is in quadratic mean,
using the isometry between HGP and HK. As an example, suppose that X = Rg and L = D
 is a diﬀerential functional evaluated at x. Then, we retrieve the observations in Section 2
about derivatives of a GP.
Penalised Likelihood. Spline Smoothing
The GP models we are interested in here have their origin in spline smoothing techniques
and penalised likelihood estimation, and for low-dimensional input spaces spline kernels are
widely used due to the favourable approximation properties of splines and computational
advantages. A comprehensive account of spline smoothing and relations to Bayesian estimation in GP models is which our exposition is mainly based on. Spline smoothing
is a special case of penalised likelihood methods, giving another view on the reproducing
kernel via the Green’s function of a penalisation (or regularisation) operator which will be
introduced below. This section can be skipped by readers interested primarily in practical
applications.
In Section 5 we have discussed the duality between a Gaussian process and the RKHS of
its covariance function. Apart from the Bayesian viewpoint using GP models, a diﬀerent
and more direct approach to estimation in non-parametric models is the penalised likelihood approach, the oldest and most widely used incarnations of which are spline smoothing
methods. We will introduce the basic ideas for the one-dimensional model which leads to
the general notion of regularisation operators, penalty functionals and their connections to
RKHS. We omit all details, (important) computational issues and multidimensional generalisations, see for details. A more elementary account is .
We will only sketch the ideas, for rigorous details see . Interpolation and smoothing
by splines originates from the work of Sch¨onberg . A natural spline s(x) of order m on
 is deﬁned based on knots 0 < x1 < · · · < xn < 1. If πk denotes the set of polynomials
of order ≤k, then s(x) ∈π2m−1 on [xi, xi+1], s(x) ∈πm−1 on [0, x1] and on [xn, 1], and
s ∈C2m−2 overall. Natural cubic splines are obtained for m = 2. Deﬁne the roughness
Jm(f) penalises large derivatives of order m by a large value, for example J2 is large for
functions of large curvature. Then, for some ﬁxed function values the interpolant minimising
Jm(f) over all f for which the latter is deﬁned is a spline of order m. More precisely,
f ∈Wm , a so-called Sobolev space of all f ∈C m−1 s.t. f (m−1) is absolutely
continuous on . If we consider the related smoothing problem of minimising the penalised
empirical risk
(yi −f(xi))2 + αJm(f),
f ∈Wm ,
it is clear that the minimiser is again a natural spline s(x) of order m (any other f ∈Wm 
can be replaced by the spline with the same values at the knots, this does not change the
risk term and cannot increase Jm). Now, from Taylor’s theorem:
ν! f (ν)(0) +
Gm(x, t)f (m)(t) dt
with Gm(x, t) = (x −t)m−1
/(m −1)! (here, u+ = uI{u≥0}). If f (ν)(0) = 0, ν = 0, . . . , m −1
then (Gm(x, ·), Dmf) = f(x), thus Gm(x, t) is the Green’s function for the boundary value
problem Dmf = g. These functions f form a Hilbert space with inner product
f (m)(t)g(m)(t) dt
which is a RKHS with RK
K(x, x′) =
Gm(x, t)Gm(x′, t) dt.
It is interesting to note that a zero-mean GP with covariance function K can be obtained as
(m −1)-fold integrated Wiener process (introduced in Section 2.3). Let W(x) be a Wiener
process on with W(0) = 0 a.s. and E[W(1)2] = 1 (its covariance function is min{x, x′}).
It is possible to deﬁne a stochastic integral against a process with independent increments.36
The process u(x) deﬁned via the stochastic integral
Gm(x, t) dW(t)
36See , Sect. 9.4 for an easy derivation. It is important to note that the stochastic integral is not the
random variable arising from integrating over sample paths of the process, the latter integrals do not exist
in many cases in which the stochastic integral can be constructed.
is a zero-mean GP with covariance function K. If W is chosen s.t. its sample paths are
continuous, u(x) is in Wm and u(ν)(0) = 0 for ν < m. Since dGm/dx = Gm−1 and
G1(x, t) = I{x>t}, u(m−1) and W are m.s. equivalent. Note that u(x) can be written as
for m > 1.
The boundary values can be satisﬁed by taking the direct sum of the space with πm−1. The
latter is trivially an RKHS w.r.t. an inner product of choice: choose an orthonormal basis
and deﬁne the kernel to be the sum of outer products of the basis functions. The kernel for
the direct sum is the sum of K and the ﬁnite-dimensional kernel. Note that ∥· ∥K is only a
seminorm on the full space because ∥p∥K = 0 for p ∈πm−1.
We only sketch the general case, see for details. We make use of the following duality
between a RKHS and a regularisation (pseudodiﬀerential) operator P on L2(µ). Let H be
the Hilbert space of f s.t. Pf ∈L2(µ). For P, consider the operator37 P∗P. If this has a null
space (such as πm−1 in the example above), we restrict H to the orthogonal complement.
Now, the operator is positive deﬁnite and has an inverse (its Green’s function) whose kernel
K is the RK of H.38 The inner product is
(f, g)K = (Pf, Pg)
and the penalty functional is simply the squared RKHS norm. If G(t, u) exists s.t.
(G(t, ·), Pf) = f(t) for all f ∈H, the RK is given by
K(s, t) = (G(s, ·), G(t, ·)).
On the other hand, we can start from an RKHS with RK K and derive the corresponding
regularisation operator P. This can give additional insight into the meaning of a covariance
function (see ). In fact, if K is stationary and continuous, we can use Bochner’s theorem (2). Namely, if f(ω) is the spectral density of K, we can take f(ω)−1/2 as spectrum of
P.39 The one-dimensional example above is readily generalised to splines on the unit sphere
or to thin plate splines in X = Rg, but the details get quite involved (see , Chap. 2).
Kimeldorf and Wahba generalised this setup to a general variational problem in an
RKHS, allowing for general bounded linear functionals Lif instead of f(xi) in (21). The
minimiser is determined by n+M coeﬃcients, where M is the dimension of the null space of
the diﬀerential operator P associated with K (M = m + 1 in the spline case above). These
can be computed by direct formulae given in , Sect. 1.3. In the more general penalised
likelihood approach , n function values or linear functionals of f are used as latent
variables in a likelihood (see Section 3), to obtain for example non-parametric extensions
of GLMs . The penalised likelihood is obtained by adding the penalty functional to the
likelihood, and just as above the minimiser is determined by n + M coeﬃcients only (this
representer theorem can be proved using the same argument as in the spline case above).
In general, iterative methods are required to ﬁnd values for these coeﬃcients.
37P∗is the adjoint of P, i.e. (f, Pg) = (P∗f, g).
38This construction via Green’s functions is diﬀerent from the one above involving Gm(x, t). Without
going into details, it may help to consider the analogue of the ﬁnite-dimensional case (vectors and matrices
instead of functions and operators):
39P is not uniquely deﬁned, but only P∗P (which has spectrum f(
Bayesian View on Spline Smoothing
We close this section by reviewing the equivalence between spline smoothing and Bayesian
estimation for a GP model pointed out by Kimeldorf and Wahba . Given a positive
semideﬁnite kernel K corresponding to a pseudodiﬀerential operator with M-dimensional
null space, we can construct an RKHS H as follows. If H0 is the null space represented by
an orthonormal basis pν and H1 the RKHS for K, let H be their direct sum. Consider the
θνpν(x) + b1/2u(x),
yi = F(xi) + εi,
where u(x) is a zero-mean GP with covariance function K and εi are independent N(0, σ2).
Furthermore, θ ∼N(0, aI) a priori. On the other hand, let fλ be the minimiser in H of
the regularised risk functional
(yi −f(xi))2 + λ∥P1f∥2
where P1 is the orthogonal projection onto H1. Kimeldorf and Wahba show that fλ lies
in the span of {pν | ν = 1, . . . , M} ∪{K(·, xi) | i = 1, . . . , n} and give a numerical procedure
for computing the coeﬃcients. If we deﬁne ˆFa(x) = E[F(x) | y1, . . . , yn], then they show
ˆFa(x) = fλ(x),
for every ﬁxed x. The proof (see , Chap. 1) is a straightforward application of the duality
between the RKHS H1 and the Hilbert space based on u(x), as described in Section 5. The
procedure of dealing with H0 and the improper prior on θ is awkward but is not necessary
if the RKHS H1 induced by K is rich enough.40
Finally, we note that a parametric extension of a non-parametric GP model can be sensible
even if H1 is rich enough in principle, leading to semiparametric models (or partial splines).
For details about such models, we refer to , Chap. 4 and , Chap. 6.
Maximum Entropy Discrimination. Large Margin Classi-
We regard GPs as building blocks for statistical models in much the same way as a parametric family of distributions (see Section 3 for examples). Statistical methods to estimate
unknown parameters in such models follow diﬀerent paradigms, and in machine learning
the following have been among the most popular.
1. Probabilistic Bayesian paradigm:
This has been introduced in Section 3. As noted in Section 4, the (intractable) posterior
process is typically approximated by a GP itself.
40This is not the case for spline kernels, for which f ∈H1 is constrained by the boundary conditions.
2. Large margin (discriminative) paradigm:
Here, a “posterior” process is obtained by associating margin constraints with observed
data, then searching for a process which fulﬁls these (soft) constraints and at the
same time is close to the prior GP, in a sense made concrete in this section. Since the
constraints are linear in the latent outputs, the “posterior” process is always a GP
with the same covariance as the prior.
The relationship between Bayesian methods and penalised likelihood or generalised spline
smoothing methods has been discussed in Section 6. Large margin methods are special cases
of spline smoothing models with a particular loss function which does not correspond to a
probabilistic noise model (e.g., ). Several attempts have been made to express
large margin discrimination methods as approximations to Bayesian inference (e.g., ), but the paradigm separation suggested in seems somewhat more convincing.
The connection between these two paradigms has been formulated in , this section
is based on their exposition. The large margin paradigm has been made popular by the
empirical success of the support vector machine (SVM) (see for background material).
In the Bayesian GP setting (see Section 3), the likelihood P(y|u) of the observed data y
can be seen to impose “soft constraints” on the predictive distribution, in the sense that
functions of signiﬁcant probability under the posterior must not violate many of them
strongly. In the large margin paradigm whose probabilistic view has been called minimum
relative entropy discrimination (MRED) , such constraints are enforced more explicitly.41
We introduce a set of latent margin variables γ = (γi)i ∈Rn, one for each datapoint. Along
with the GP prior P(u(·)) on the latent function, we choose a prior P(γ) over γ. The
margin prior encourages large margins γi, as is discussed in detail below. The minimum
relative entropy distribution dQ(u(·), γ) is deﬁned as minimiser of D[Q ∥P], subject to the
soft margin constraints
  )∼Q [yiu(xi) −γi] ≥0, i = 1, . . . , n.
Just as in the case of a likelihood function, these constraints depend on the values u =
(u(xi))i of the random process u(·) only. It is well known in information theory (e.g., ,
Sect. 3.1) that the solution to this constrained problem is given by
dQ(u(·), γ) = Z(λ)−1 exp
λi (yiui −γi)
dP(u(·), γ),
Z(λ) = E(u(·),   )∼P
λi (yiui −γi)
The value for the Lagrange multipliers λ is obtained by minimising the convex function
log Z(λ) (sometimes called the dual criterion) under the constraints λ ≥0. Since the right
hand side of (24) factorises between u(·) and γ and the same holds for the prior P, we see
that Q must factorise in the same way. Furthermore, it is immediate from (24) that Q(u(·))
is again a Gaussian process with the same covariance kernel K as P(u(·)) and with mean
41For notational simplicity, we do not use a bias term b here. The modiﬁcations to do so are straightforward.
In the original SVM formulation, b can be seen to have a uniform (improper) prior.
function µ(x∗) = k(x∗)T Y λ, where Y = diag(yi)i. Due to the factorised form, we also
have Z(λ) = Zu(·)(λ)Z
Zu(·)(λ) = E
The form of Z
 depends on the choice of the prior P(γ) on the margin variables.
Jaakkola et. al. give some examples of such priors which encourage large margins.
For example, if P(γ) = Q
i P(γi), then P(γi) should drop quickly for γi < 1 in order to
penalise small and especially negative margins (empirical errors). In order for (23) to be
a “soft constraint” only w.r.t. margin violations and also to mimic the SVM situation, we
have to use P(γi) = 0 for γi > 1.42 If P(γi) ∝e−c(1−γi)I{γi≤1}, then
and the complete dual criterion is
log Z(λ) = −
(λi + log(1 −λi/c)) + 1
2λT Y KY λ,
Except for the potential term log(1 −λi/c), this is identical to the SVM dual objective (see
below).43 The so-called hard margin SVM for which margin constraints are enforced without
allowing for violations, is obtained for c →∞. It converges only if the training data is indeed
separable and is prone to over-complicated solutions. The eﬀect of the potential term on the
solution is limited (see ). It keeps λi from saturating to c exactly (which happens in SVM
for misclassiﬁed patterns). The dual criterion can be optimised using eﬃcient algorithms
such as SMO , although the nonlinear potential term introduces minor complications.44
Just like in SVM, sparsity in λ is encouraged and can be observed in practice.
To conclude, MRED gives a complete probabilistic interpretation of the SVM, or at least
of a close approximation thereof. Note that SVM classiﬁcation cannot be seen as MAP
approximation to Bayesian inference for a probabilistic model, because its loss function does
not correspond to a proper negative log likelihood . Interestingly, the MRED view
points out limitations of this framework as opposed to a Bayesian treatment of a Gaussian
process model with a proper likelihood. Recall from above that the margin constraints are
linear in the latent outputs u, leading to the fact that the MRED “posterior” process
Q(u(·)) has the same covariance kernel K as the prior. While the constraints enforce the
predictive mean to move from 0 a priori to µ(x), the “predictive variances” are simply the
prior ones, independent of the data. This suggests that if predictive variances (or error bars)
are to be estimated besides simply performing a discrimination, then SVMs or other large
margin discriminative methods may be less appropriate than probabilistic GP models. For
more details on this argument, see , Sect. 4.7.2.
More important is the lack of practical methods for model selection with SVM. For Bayesian
GP methods, a general model selection strategy is detailed in Section 4. Alternatively,
42As in the SVM setup, the choice of 1 as margin width is arbitrary, because the distance can be re-scaled
in terms of the prior variance.
43The potential term acts like a logarithmic barrier to enforce the constraints λi < c (e.g., ).
44SMO makes use of the fact that the SVM criterion is quadratic with linear constraints.
hyperparameters can be marginalised over approximately using MCMC techniques . In
contrast, model selection for SVM is typically done using variants of cross validation, which
severely limits the number of free parameters that can be adapted.
While it is often claimed that learning-theoretical foundations count as distinctive advantage
of SVM, similar or even superior guarantees can be given for approximate Bayesian GP
techniques as well .
An important and early application of Gaussian random ﬁeld models has been termed
kriging after a South-African mining engineer D. Krige who developed methods for predicting spatial ore-grade distributions from sampled ore grades . Optimal spatial linear
prediction has its roots in earlier work by Wiener and Kolmogorov (“closeness in space”
may have to be replaced by “closeness in time”, since they were mainly concerned with time
series). These fundamental ideas have been further developed in the ﬁelds of geostatistics
 as kriging and in meteorology under the name objective analysis (see , Chap. 3 for
references).
We will not go into any details, but refer to , Chap. 3 and (we follow the latter
here). The basic model is the same as for semiparametric smoothing:
z(x) = m(x)T β + ε(x)
where m(x) is a known feature map and ε(x) is a zero-mean random ﬁeld with covariance
function K. In a nutshell, kriging is a minimum mean squared error prediction method for
linear functionals of z(x) given observations z = (z(x1), . . . , z(xn))T at spatial locations
xi ∈Rg. For example, if z(x) measures ore grade at x one might be interested in predicting
over some area B ⊂Rg. Since they focus on m.s. error and m.s. properties of z(x) in
general, kriging methods typically depend on second-order properties of the process only,
and ε(x) is often assumed to be a Gaussian ﬁeld. Furthermore, we restrict ourselves to
linear predictors λ0 + λT z. The optimal predictor of z(x∗) in the m.s. error sense is the
conditional expectation which is linear in z if ε(x) is Gaussian and β is known:
 m(x∗) −M T λ
where K = (K(xi, xj))i,j, k = (K(xi, x∗))i and M = (m(x1), . . . , m(xn))T . If β is
unknown, a simple procedure is to plug in the generalised least squares estimate
−1 M T K−1z
for ˆβ. This procedure can be motivated from several angles. If we restrict our attention to
linear predictors of z(x∗) which are unbiased in the sense
= λ0 + λT M β = E[z(x∗)] = m(x∗)T β
for any β, the suggested approach minimises the m.s. error over these unbiased predictors.
It is therefore called best linear unbiased predictor (BLUP). A Bayesian motivation can be
constructed in the same way as mentioned in Section 6. Namely, β is given a Gaussian
prior whose covariance matrix scales with a > 0 and ε(x) is a priori Gaussian. Then, the
posterior mean for z(x∗) converges to the BLUP as a →∞(i.e. as the β prior becomes
uninformative).
The equations behind the BLUP have been known long before and have been rediscovered in
many areas of statistics. In practice, kriging methods are more concerned about inducing an
appropriate covariance function (under the stationarity assumption) from observed data as
well. The empirical semivariogram is a frequently used method for estimating the covariance
function close to the origin. On the theoretical side, Stein advocates the usefulness of
ﬁxed-domain asymptotics (a growing number of observations located within a ﬁxed compact
region) to understand the relationship between covariance model and behaviour of kriging
predictors.45 By Bochner’s theorem (2) a stationary covariance function is characterised
by its spectral distribution F(ω). Stein points out that ﬁxed-domain asymptotics depend
most strongly on the spectral masses for large ∥ω∥, i.e. the high frequency components,
much less so on the low frequency ones or the mean function m(x)T β (if m(x) is smooth
itself, e.g. polynomials). Let f(ω) be the spectral density, i.e. the Fourier transform of
K(x). In general, the lighter the tails of f(ω) the smoother ε(x) is in the m.s. sense.
Stein advocates this expected smoothness as a central parameter of the GP prior and
condemns the uncritical use of smooth (analytic) covariance functions such as the RBF
(Gaussian) kernel (see Section 9). Another important concept highlighted by Stein (see also
 , Chap. 3) is the one of equivalence and orthogonality of GPs.46 Essentially, GPs with
covariance functions of diﬀerent form can be equivalent in which case it is not possible to
unambiguously decide for one of them even if an inﬁnite amount of observations in a ﬁxed
region are given. On this basis, one can argue that for a parametric family of covariance
functions inducing equivalent GPs the parameters can just as well be ﬁxed a priori since
their consistent estimation is not possible. On the other hand, parameters s.t. diﬀerent
values lead to orthogonal GPs should be learned from data and not be ﬁxed a priori.
Note that kriging models are more generally concerned with intrinsic random functions
(IRF) , generalisations of stationary processes which are also frequently used in the
spline smoothing context. In a nutshell, a k-IRF u(x) is a non-stationary random ﬁeld
based on a “spectral density” whose integral diverges on any neighborhood of the origin
(e.g., has inﬁnite pointwise variance). However, if c ∈Rn is a generalised divided diﬀerence
(g.d.d.) for x1, . . . , xn in the sense that P
i cip(xi) = 0 for all polynomials p of total degree
≤k, then the variance of P
i ciu(xi) is ﬁnite and serves to deﬁne an “covariance function”
K(x) which is k-conditionally positive semideﬁnite, namely
cicjK(xi −xj) ≥0
45Stein restricts his analysis to “interpolation”, i.e. to situations where predictions are required only at
locations which are in principle supported by observations (in contrast to “extrapolation” often studied in the
time series context). This should not be confused with the distinction between interpolation and smoothing
used in Section 6. All non-trivial kriging techniques are smoothing methods.
46Two probability measures are equivalent if they have the same null sets, i.e. are mutually absolutely
continuous (see Section A.1). They are orthogonal if there is a null set of one of them which has mass 1
under the other. Gaussian measures are either orthogonal or equivalent.
for all g.d.d.’s c. In practice, one uses semi-parametric models where the latent process of
interest is the sum of a k-IRF and a polynomial of total degree ≤k whose coeﬃcients are
parametric latent variables.47
In fact, IRFs do not add more generality w.r.t. high-frequency behaviour of the process
since f(ω) must be integrable on the complement of any 0-neighborhood, so the IRF can
be written as the uncorrelated sum of a stationary and a non-stationary part, the latter
with f(ω) = 0 outside a 0-neighborhood (thus very smooth). IRFs are not discussed in any
further detail here (see ).
Choice of Kernel. Kernel Design
There is a tendency in the machine learning community to treat kernel methods as “black
box” techniques, in the sense that covariance functions are chosen from a small set of
candidates over and over again. If a family of kernels is used, it typically comes with a
very small number of free parameters so that model selection techniques such as crossvalidation can be applied. Even though such approaches work surprisingly well for many
problems of interest in machine learning, experience almost invariably has shown that much
can be gained by choosing or designing covariance functions carefully depending on known
characteristics of a problem (for an example, see , Sect. 11.4).
Establishing a clear link between kernel functions and consequences for predictions is very
non-trivial and theoretical results are typically asymptotic arguments. As opposed to ﬁnitedimensional parametric models, the process prior aﬀects predictions from a non-parametric
model even in ﬁxed-domain asymptotic situations (see Section 8). The sole aim of this section
is to introduce a range of frequently used kernel functions and some of their characteristics,
to give some methods for constructing covariance functions from simpler elements, and
to show some techniques which can be used to obtain insight into the behaviour of the
corresponding GP. Yaglom gives extensive material, an accessible review is . In the
ﬁnal part, we discuss some kernel methods over discrete spaces X.
It should be noted that positive deﬁniteness of an arbitrary symmetric form or function is
hard to establish in general. For example, the sensible approach of constructing a distance
d(x, x′) between patterns depending on prior knowledge, then proposing
K(x, x′) = e−w d(
as covariance function does not work in general because K need not be positive semideﬁnite,
moreover there is no simple general criterion to prove that K is a covariance function.48 If
d(x, x′) can be represented in an Euclidean space, K is a kernel as we will see below. Note
that if K(x, x′) of the form (26) is a kernel, so must be K(x, x′)t for any t > 0.49 Kernels
with this property are called inﬁnitely divisible. Sch¨onberg managed to characterise
inﬁnitely divisible kernels (26) by a property on d(x, x′) which unfortunately is just as hard
to handle as positive semideﬁniteness.50
47In fact,
 ) maps to a basis of πk. As mentioned above, the BLUP is obtained as posterior expectation
under an uninformative prior on the parametric coeﬃcients.
′) is stationary, one can try to compute the spectral density, but this will not be analytically
tractable in general.
49This is true in general only for t ∈
>0, see below.
′)2 must be conditionally positive semideﬁnite of degree 0 (see Section 8).
Some Standard Kernels
In the following, we provide a list of frequently used “standard kernels”. Most of these
will have a variance (scaling) parameter C > 0 in practice, sometimes an oﬀset parameter
vb > 0, thus instead of K one uses C K or C K + vb. C scales the variance of the process,
while a vb > 0 comes from the uncertainty of a bias parameter added to the process.51 In
applications where the kernel matrix K is used directly in linear systems, it is advised to
add a jitter term52 ρδ
 ′ to the kernel to improve the condition number of K. This amounts
to a small amount of additive white noise on u(x) (ρ can be chosen quite small), but should
not be confused with measurement noise which is modelled separately (see Section 3). These
modiﬁcations are omitted in the sequel for simplicity.
The Gaussian (RBF) covariance function
K(x, x′) = exp
2 ∥x −x′∥2
is isotropic for each X = Rg (i.e. D∞). w > 0 is an inverse squared length scale parameter,
in the sense that w−1/2 determines a scale on which u(x) is expected to change signiﬁcantly.
K(x) is analytic at 0, so u(x) is m.s. analytic. Stein points out that
in quadratic mean for every x (a similar formula holds for X = Rg), so that u can be predicted perfectly by knowing all its derivatives at 0 (which depend on u on an neighborhood
of 0 only). He criticises the wide-spread use of the Gaussian covariance function because
its strong smoothness assumptions are unrealistic for many physical processes, in particular
predictive variances are often unreasonably small given data. The spectral density (in R) is
f(ω) = (2πw)−1/2 exp(−ω2/(2w)) with very light tails. On the other hand, Smola et. al. 
recommend the use of the Gaussian covariance function for high-dimensional kernel classi-
ﬁcation methods because of the high degree of smoothness. It is interesting to note that in
the context of using GPs for time series prediction, Girard et. al. report problems with
unreasonably small predictive variances using the Gaussian covariance function (although
they do not consider other kernels in comparison). Figure 1 shows smoothed plots of some
sample paths. Note the eﬀect of the length scale w−1/2 and the high degree of smoothness.
We can consider the anisotropic version, called squared-exponential covariance function:
K(x, x′) = exp
2(x −x′)T W (x −x′)
Here, W is positive deﬁnite. Typically, W is a diagonal matrix with an inverse squared
length scale parameter wj for each dimension. Full matrices W have been considered in
 , and factor analysis-type matrices W are a useful intermediate (e.g., ). An
important application of the additional d.o.f.’s in (28) as compared to the Gaussian kernel
is automatic relevance determination (ARD), as discussed below. Note that the squaredexponential covariance function for diagonal W can be seen as product of g one-dimensional
51For reasons of numerical stability, vb must not become too large.
52In the context of kriging (see Section 8), adding ρδ
′ has been proposed by Math´eron to model the
so-called “nugget eﬀect” (see , Sect. 2.3.1), but other authors have criticised this practice.
Gaussian (RBF)
Figure 1: Smoothed sample paths from GP with Gaussian covariance function. All have
variance C = 1. Dash-dotted: w = 1. Solid: w = 102. Dashed: w = 502.
Gaussian kernels with diﬀerent length scales, so the corresponding RKHS is a tensor product
space built from RKHS’s for one-dimensional functions (see Section 5).
The Mat´ern class of covariance functions (also called modiﬁed Bessel covariance functions)
is given by
2ν−1Γ(ν + 1/2)α2ν (ατ)νKν(ατ),
τ = ∥x −x′∥,
where ν > 0, α > 0 and Kν(x) is a modiﬁed Bessel function (e.g., , Sect. 2.7). One can
show that zνKν(z) →2ν−1Γ(ν) for z →0, so
Γ(ν + 1/2)α2ν .
K is isotropic for each X = Rg. An important feature of this class is that the m.s. smoothness
of u(x) can be regulated directly via ν. For example, u(x) is m times m.s. diﬀerentiable
iﬀν > m. The spectral density in R is f(ω) = (α2 + ω2)−ν−1/2. For ν = 1/2 + m we
obtain a process with rational spectral density, a continuous time analogue of an AR time
series model. For ν = 1/2, K(τ) ∝e−ατ deﬁnes an Ornstein-Uhlenbeck process, a stationary
analogue to the Wiener process which also has independent increments. In general, for
ν = 1/2 + m we have K(τ) ∝e−ατp(ατ), where p(x) is a polynomial of order m (e.g., ,
Sect. 2.7). Note that if α = (w(2ν + 1))1/2, then
α2ν+1f(ω) →e−ω2/(2w) (ν →∞),
thus K(τ) converges to the Gaussian covariance function after appropriate re-scaling.
The Mat´ern class can be generalised to an anisotropic family in the same way as the Gaussian
kernel. Figure 2 show some sample function plots for values ν = 1/2, 3/2, 5/2, 10. Note the
eﬀect of ν on the roughness of the sample paths. For ν = 1/2 the paths are erratic even
though the length scale is 1, i.e. the same as the horizontal region shown. For ν = 3/2, the
process is m.s. diﬀerentiable, for ν = 5/2 twice so.
Matern (nu=1/2): Ornstein−Uhlenbeck
Matern (nu=3/2)
Matern (nu=5/2)
Matern (nu=10)
Figure 2: Smoothed sample paths from GP with Mat´ern covariance function. All have
variance C = 1. Upper left: Ornstein-Uhlenbeck (Mat´ern, ν = 1/2), α = 1. Upper right:
Mat´ern, ν = 3/2, α = 1 (dash-dotted), α = 102 (solid). Lower left: Mat´ern, ν = 5/2, α = 1
(dash-dotted), α = 102 (solid). Lower right: Mat´ern, ν = 10, α = 1 (dash-dotted), α = 102
The exponential class of covariance functions is given by
K(τ) = e−ατ δ, δ ∈(0, 2].
The positive deﬁniteness can be proved using the Mat´ern class (see , Sect. 2.7). For
δ = 1, we have the Ornstein-Uhlenbeck covariance function, for δ = 2 the Gaussian one.
Although it seems that the kernel varies smoothly in δ, the processes have quite diﬀerent
properties in the regimes δ ∈(0, 1), δ = 1, δ ∈(1, 2) and δ = 2. Continuous sample paths
can be ensured for any δ ∈(0, 2], but diﬀerentiable sample paths can only be obtained for
δ = 2 (in which case they are analytic).53 K(τ) is not positive deﬁnite for δ > 2. Figure 3
shows some sample path plots.
Exponential
Figure 3: Smoothed sample paths from GP with exponential covariance function. All have
variance C = 1 and α = 102. Solid: δ = 1.5. Dashed: δ = 1.9. Dash-dotted: δ = 2 (Gaussian).
We have derived the spline covariance function on (22) from ﬁrst principles above.
This kernel is of interest because posterior mean functions in GP models (or minimisers of
the variational problem over the RKHS) are splines of order m, i.e. piecewise polynomials
in C2m−2 (see Section 6) and associated computations are O(n) (where n is the number of
training points, or “knots”) only. On the other hand, technical complications arise because
spline kernels are RKs for subspaces of Wm only, namely of the functions which satisfy
the boundary conditions (see Section 6). The operator induced by a spline kernel has a
null space spanned by polynomials, and in practice it is necessary to adjoin the corresponding (ﬁnite-dimensional) space. The spline kernels are not stationary (they are supported
on ), but we can obtain spline kernels on the circle by imposing periodic boundary
conditions on Wm , leading to the stationary kernel
K(x, x′) =
(2πν)2m cos(2πν(x −x′)).
From this representation, it follows that the spectral density is
(2πν)2m δ2πν(|ω|)
which is discrete. Note that sample functions from u(x) are periodic with probability 1.
In Wahba , Chap. 2 it is shown how to construct splines on the sphere by using the
53All these statements hold with probability 1, as usual.
iterated Laplacian, but this becomes quite involved. An equivalent to splines (in a sense)
can be deﬁned in Rg using thin-plate spline conditionally positive deﬁnite functions (see
Section 8), see for details.
For kernel discrimination methods, polynomial covariance functions
K(x, x′) =
(xT x′ + α)m
((∥x∥2 + α)(∥x′∥2 + α))m/2 ,
α ≥0, m ∈N
are popular although they seem unsuitable for regression problems. The denominator normalises the kernel to K(x, x) = 1. Although this normalisation is not done in some applications, it seems to be recommended in general. Polynomial kernels without the normalising
denominator can be seen to induce a ﬁnite-dimensional feature space of polynomials of total
degree ≤m (if α > 0).54 It is interesting to note that this is exactly the RKHS we have to
adjoin to one for a conditionally positive deﬁnite kernel of order m such as the thin-plate
spline covariance function. On the other hand, in the spline case these polynomial parts
are usually not regularised at all. By the Karhunen-Loeve expansion (see Section 5), we
can write u(x) as expansion in all monomials of total degree ≤m with Gaussian random
coeﬃcients. The regularisation operator (see Section 6) for polynomial kernels is worked
out in . Note that K(x, x′) is not a covariance function for m ̸∈N, thus the kernel is
not inﬁnitely divisible. Figure 4 shows some sample path plots. These are polynomials and
therefore analytic.
Polynomial
Figure 4: Sample paths from GP with polynomial covariance function. All have variance
C = 1 and α = 0.05. Solid: m = 10. Dashed: m = 5.
The Euclidean inner product xT Σx′ is sometimes referred to as “linear kernel” in the
54The feature space of the normalised polynomial kernel consists of polynomials of total degree ≤m
divided by (∥
 ∥2 + α)m/2.
machine learning literature. GP models based on this kernel are nothing else than straightforward linear models (linear regression, logistic regression, etc.). It is clear from the weight
space view (see Section 2) that a linear model can always be regarded as a GP model (or
kernel technique), but this makes sense only if n < g, where n is the number of training
points.55 Furthermore, the SVM with linear kernel is a variant of the perceptron method
 with “maximal stability” studied in statistical physics.
Finally, let us give an example of a function which is not a covariance function, the so-called
“sigmoid kernel”
K(x, x′) = tanh
 axT x′ + b
K is not positive semideﬁnite for any a, b (see ), it is nevertheless shipped in most
SVM packages we know of. It springs from the desire to make kernel expansions look like
restricted one-layer neural networks. The correct link between MLPs and GP models has
been given by Neal (see Section 2), which involves taking the limit of inﬁnitely large networks. A covariance function corresponding to a one-layer MLP in the limit has been given
by Williams . In practice, it is of course possible to ﬁt expansions of kernels to data
which are not covariance functions. However, the whole underlying theory of minimisation
in a RKHS (see Sections 5 and 6) breaks down, as does the view as inference in a GP model.
On the practical side, ﬂawed results such as negative predictive variances can pop up when
least expected. Even worse, most optimisation techniques (including SVM algorithms) rely
on the positive semideﬁniteness of matrices and may break down otherwise. In fact, the
SVM optimisation problem is not convex and has local minima for general K.
Constructing Kernels from Elementary Parts
We can construct complicated covariance functions from simple restricted ones which are
easier to characterise (e.g. stationary or (an)isotropic covariance functions, see Section 2).
A large number of families of elementary covariance functions are known (e.g., ), some
of which are reviewed in Section 9.1.
A generalisation of stationary kernels to conditionally positive semideﬁnite ones (stationary
ﬁelds to IRFs) is frequently used in geostatistical models (see Section 8) but will not be
discussed here. The class of positive semideﬁnite forms has formidable closure properties.
It is closed under positive linear (so-called conic) combinations, pointwise product and
pointwise limit. If K(v, v′) is a covariance function, so is
˜K(x, x′) =
h(x; v)h(x′; v′)K(v, v′) dvdv′
(if ˜K is ﬁnite everywhere). An important special case is ˜K(x, x′) = a(x)K(x, x′) a(x′).
For example, a given kernel (with positive variance everywhere) can always be modiﬁed to
be constant on the diagonal by choosing a(x) = K(x, x)−1/2, this normalisation has been
discussed in the context of the polynomial kernel above. Note that O’Hagan’s “localised
regression model” (Section 2) is also a special case of (30). A general way of creating a
non-stationary covariance function ˜K(y, y′) from a parametric model h(y; θ) linear in θ
is to assume a GP prior on θ, then to integrate out the parameters (see for details).
Furthermore, suppose we do so with a sequence of models and priors to obtain a sequence
55Otherwise, running a kernel algorithm is wasteful and awkward due to a singular kernel matrix.
of kernels. If the priors are appropriately scaled, the pointwise limit exists and is a kernel
again. Many standard kernels can be obtained in this way (e.g., ). Neal showed that if
the model size goes to inﬁnity and the prior variances tend to 0 accordingly, layered models
with non-Gaussian priors will also tend to a GP (due to the central limit theorem; see
Section 2).
Another important modiﬁcation is embedding. If K(h, h′) is a covariance function and h(x)
is an arbitrary map, then
˜K(x, x′) = K(h(x), h(x′))
is a covariance function as well (this is a special case of (30)). For example, if we have
d(x, x′) = ∥h(x)−h(x′)∥in some Euclidean space, then (26) is a valid kernel induced from
the Gaussian (RBF) kernel (27). The Fisher kernel and mutual information kernels 
are examples. Embedding can be used to put rigid constraints on the GP. For example,
if K is stationary in (31) and h(x) = h(x′), then u(x) = u(x′) almost surely.56 For
h(x) = (cos((2π/ν)x), sin((2π/ν)x))T , sample paths of u(x) are ν-periodic functions.
Embedding can be used to create non-stationary kernels from elementary stationary ones.
A more powerful mechanism starts from viewing (30) in a diﬀerent way. Let K be the
squared-exponential kernel (28), but suppose the input x is subject to noise:
x = t + ε,
ε ∼N(0, S(t)).
Here, ε at diﬀerent observed locations t are independent, and all noise variables are independent of the process u(·). The process v(t) = u(x) = u(t + ε) is not Gaussian, but its
mean and covariance function are determined easily: E[v(t)] = 0 and
E[v(t)v(t′)] ∝E
N(t + ε | t′ + ε′, W −1)
= N(t | t′, W −1 + S(t) + S(t′))
which has the form of a squared-exponential kernel with covariance matrix which depends on
t, t′. A similar construction was used in to create non-stationary covariance functions.
This idea can be generalised considerably as shown in . Deﬁne
Q(t, t′) =
2(S(t) + S(t′))
Note that Q is not a Mahalanobis distance, because the covariance matrix depends on t, t′.
Now, if ρ(τ) is an isotropic correlation function in D∞(recall Section 2.2), it is shown in
ρQ(t, t′) = |S(t)|1/4|S(t′)|1/4
2(S(t) + S(t′))
ρ(Q(t, t′))
is a valid correlation function. The proof uses the characterisation
e−τ 2ω2 dF(ω)
56This is because the correlation ρ(u(
′)) is 1, thus u(
 ) = a u(
′)+b for ﬁxed a, b, then a = 1, b = 0
because both variables have the same mean and variance.
of D∞(see Section 2.2), thus
ρ(Q(t, t′)) =
2(S(t) + S(t′))
4ω2 (S(t) + S(t′))
The integral can now be written as
N(r | t, ˜S(t, ω))N(r | t′, ˜S(t′, ω)) dr dF(ω)
which is positive semi-deﬁnite as a special case of (30).57 Equation (32) can be used to create
many new families of non-stationary kernels from isotropic ones. Note that now there are
two ﬁelds to estimate, u(·) and t 7→S(t). In principle, the latter one can be speciﬁed via GPs
as well (see ), but inference becomes very costly. On the other hand, simpler parametric
models may be suﬃcient. If unlabelled data is abundant, it is possible to learn the second
ﬁeld from this source only (see ). It is interesting to note that if t 7→S(t) is smooth,
then m.s. properties of u(·) deducible from ρ(τ) are transferred to the GP with correlation
function ρQ (32).
Guidelines for Kernel Choice
Choosing a good kernel for a task depends on intuition and experience. On high-dimensional
tasks where no suitable prior knowledge is available, the best option may be to explore
simple combinations of the standard kernels listed above. If invariances are known, they
may be encoded using the methods described in , Sect. 11.4. With approximate Bayesian
GP inference, one can in principle use combinations of diﬀerent kernels with a lot of free
(hyper)parameters which can be adapted automatically.
For low-dimensional X, one can obtain further insight. Stein points out the usefulness of
studying ﬁxed-domain asymptotics (see Section 8). In this respect, the tail behaviour of the
spectral density (see Section 2) is important. The m.s. degree of diﬀerentiability (degree of
smoothness) of the process depends on the rate of decay of f(ω). Stein recommends kernel
families such as the Mat´ern class (29) which come with a degree of smoothness parameter
ν. He also stresses the importance of the concept of equivalence and orthogonality of GPs
(see Section 8). His arguments are of asymptotic nature, for example it is not clear whether
ν in the Mat´ern class can be learned accurately enough from a limited amount of data.
Also, predictions from equivalent processes with diﬀerent kernels can be diﬀerent.58
There are ways of “getting a feeling” for the behaviour of a process by visualisation, which
is an option if X = Rg is low-dimensional, g = 1, 2. We can draw “samples” from the
process and plot them as follows (the plots in this section have been produced in this
way). Let X ⊂X be a ﬁne grid59 over a domain of interest, n = |X| and u = u(X) ∼
57Namely, (30) applies especially to “diagonal kernels” K(
 ′) where f is positive. In our
58Stein argues (citing Jeﬀreys) that such diﬀerences cannot be important since they do not lead to consistency in the large data limit (in a ﬁxed domain).
59For ﬁne grids and smooth kernels such as the Gaussian one, the Cholesky technique described here fails
due to round-oﬀerrors. The singular value decomposition (SVD) should be used in this case, concentrating
on the leading eigendirections which can be determined reliably.
N(0, K(X)). We can sample u as u = Lv, v ∼N(0, I), where K(X) = LLT is the
Cholesky decomposition. If X is too large, u can be approximated using an incomplete
Cholesky factorisation of K(X) (see ). If g = 1, the process is isotropic and the grid
is regularly spaced, K(X) has Toeplitz structure60 and its Cholesky decomposition can be
computed in O(n2) (see ). Repeatedly sampling u and plotting (X, u) can give an idea
about degree of smoothness, average length scales (Euclidean distance in X over which u(x)
is expected to vary signiﬁcantly) or other special features of K.
Learning the Kernel
One promising approach for choosing a covariance function is to learn it from data and/or
prior knowledge. For example, given a parametric family of covariance functions, how can
we choose61 the parameters in order for the corresponding process to model the observed
data well?
Model selection from a ﬁxed family can be done by the empirical Bayesian method of
marginal likelihood maximisation, a generic approximation of which in the case of GP models is given in Section 4. Since this procedure typically scales linearly in the number of hyperparameters, elaborate and heavily parameterised families can be employed. An important
special case has been termed automatic relevance determination (ARD) by MacKay 
and Neal . The idea is to introduce a hyperparameter which determines the scale of
variability of a related variable of interesting (with prior mean 0). For example, we might
set up a linear model (4) by throwing in a host of diﬀerent features (components in Φ(x)),
then place a N(β|0, D) on the weights β where D is a diagonal matrix of positive hyperparameters. If we place a hyperprior on diag D which encourages small values, there is an
a priori incentive for di = Di,i to become very small, inducing a variance of βi close to 0
which eﬀectively switches oﬀthe eﬀect of βiφi(x) on predictions. This is balanced against
the need to use at least some of the components of the model to ﬁt the data well, leading
to an automatic discrimination between relevant and irrelevant components. In the context
of covariance functions, we can implement ARD with any anisotropic kernel (see Section 2)
of the form
K(x, x′) = ˜K((x −x′)T W (x −x′)),
where ˜K is isotropic and W is diagonal and positive deﬁnite. An example is the squaredexponential covariance function (28). Here, wi determines the scale of variability of the
(prior) ﬁeld as x moves along the i-th coordinate axis. If we imagine the ﬁeld being restricted
to a line parallel to this axis, w−1/2
is the length scale of this restriction, i.e. a distance
for which the expected change of the process is signiﬁcant. If wi ≈0, this length scale is
very large, thus the ﬁeld will be almost constant along this direction (in regions of interest).
Thus, via ARD we can discriminate relevant from irrelevant dimensions in the input variable
x automatically, and predictions will not be inﬂuenced signiﬁcantly by the latter.
In spatial statistics, semivariogram techniques (see , Sect. 2.3.1) are frequently used.
For a stationary process, the (semi)variogram is γ(x −x′) = (1/2)Var[u(x) −u(x′)]. It is
estimated by averaged squared distances over groups of datapoints which are roughly the
60A matrix is Toeplitz if all its diagonals (main and oﬀ-diagonals) are constant.
61The proper Bayesian solution would be to integrate out the parameters, but even if this can be approximated with MCMC techniques, the outcome is a mixture of covariance functions leading to expensive
predictors.
same distance apart and ﬁtted to parametric families by maximum likelihood. Stein 
criticises the use of the empirical semivariogram as single input for choosing a covariance
function and suggests a range of other techniques, including the empirical Bayesian approach
mentioned above.
For classiﬁcation models, the idea of local invariance w.r.t. certain groups of transformations
is important. For example, the recognition of handwritten digits should not be inﬂuenced by
translations or small-angle rotations of the bitmap.62 If a process is used as latent function
in a classiﬁcation problem, e.g. representing the log probability ratio between classes (see
Section 3), then starting from some x and applying small transformations from a group
w.r.t. which discrimination should remain invariant should not lead to signiﬁcant changes
in the process output (e.g. in the m.s. sense). To relate this notion to ARD above, varying x
along such invariant directions should induce a coordinate of x (non-linear in general) which
is irrelevant for prediction. Chapter 11 in gives a number of methods for modifying
a covariance function in order to incorporate invariance knowledge to some degree, also
reviewing work in that direction which we omit here.
Finally, Minka pointed out that instances of the “learning how to learn” or “prior
learning” paradigm can be seen as learning a GP prior from multi-task data (see his paper for
references). In fact, the setup is the one of a standard hierarchical model frequently used in
Bayesian statistics to implement realistic prior distributions. We have access to several noisy
samples and make the assumption that these have been sampled from diﬀerent realisations
of the latent process which in turn have been sampled i.i.d. from the process prior. Data
of this sort is very valuable for inferring aspects of the underlying covariance function. In
a simple multi-task scenario a multi-layer perceptron is ﬁt to several samples by penalised
maximum likelihood, sharing the same input-to-hidden weights but using diﬀerent sets of
hidden-to-output weights for each sample. The idea is that the hidden units might discover
features which are important in general, while the combination in the uppermost layer is
speciﬁc. If we place Gaussian priors on the hidden-to-output weights, this becomes a GP
model with a covariance function determined by the hidden units. More generally, we can
start from any parametric family of covariance functions and learn hyperparameters or even
the hyperposterior from multi-task data using marginal likelihood maximisation together
with the hierarchical sampling model. An approximate implementation of this idea has been
reported in .
Kernels for Discrete Objects
As mentioned in Section 2, in principle the input space X is not restricted to be Rg or even
a group. For example, Gaussian processes over lattices are important in vision applications
(in the form of a Gaussian Markov random ﬁeld with sparse structured inverse covariance
matrix). For Gaussian likelihoods, the posterior mean can be determined most eﬃciently
using a conjugate gradients solver63 and the embedded trees algorithm of Wainwright,
Sudderth and Willsky can be used to compute the marginal variances as well. Kernel
methods, i.e. methods which use covariance matrices over variables determined from the
“spatial” relationship of these (or associated covariates) have been proposed for a number
62Although a 180-degree rotation of a 6 results in a 9.
63Loopy belief propagation renders the correct mean as well if it converges , but is much slower and
often numerically unstable.
of problems involving discrete spaces X (ﬁnite or countably inﬁnite). Our aim in this section
is no more than to give a few selected examples.
Kernels can be deﬁned on the set of ﬁnite-length strings from a ﬁnite alphabet. Many
string kernels have been proposed recently, but we will not try to review any of this work.
Important applications of string kernels (or distance measures between sequences) arise from
problems in DNA or RNA biology where statistical models have to be built for nucleotide
sequences. Many proposed string kernels are special cases of convolution kernels introduced
by Haussler . Maybe the most interesting case discussed there is the extension of a
hidden Markov random ﬁeld (HMRF). The latter is a Markov random ﬁeld (MRF) with
observed variables x, latent variables u and clique potentials Cd(xd, ud) where xd, ud are
subsets of components of x, u, and u is marginalised over. If we replace the clique potential
by positive deﬁnite kernels Kd((xd, ud), (x′
d)) and marginalise over u, u′, the result is a
covariance kernel which can also be seen as unnormalised joint generative distribution for
(x, x′). If the original MRF has a structure which allows for tractable computation, the
same algorithm can be used to evaluate the covariance function eﬃciently. For example, a
hidden Markov model (HMM) for sequences can be extended to a pair-HMM in this way,
emitting two observed sequences sharing the same latent sequence, and many string kernels
arise as special cases of this construction.
In practice, string kernels (and more generally kernels obtained from joint probabilities
under pair-HMRFs) often suﬀer from the “ridge problem”: K(x, x) is much larger than
K(x, x′) for many x′ for which a priori we would like to attain a signiﬁcant correlation,
especially if rather long sequences are compared. For example, in models involving DNA
sequences we would like sequences to correlate strongly if they are homologous, i.e. encode
for proteins of very similar function. In a standard string kernel, two sequences are strongly
correlated if both can be obtained from a common “ancestor” latent sequence by few operations such as insertions and substitutions (this ancestor model is motivated by the evolution
of genes and gives a good example for the pair-HMM setup). However, often homologous
sequences diﬀer quite substantially in regions on which the structure of the functional part
of the protein does not depend strongly. Such “remote” homologies are the really interesting
ones, since very close homologies can often be detected using simpler statistical techniques
than process models based on string kernels. On the other hand, it may be possible to spot
such homologies by going beyond string kernels and pair-HMRF constructions, for example building on the general framework given in where kernels are obtained from ﬁnite
transducers.
A conceptually simple way to obtain a kernel on X is to embed X in some Euclidean space
Rg, then to concatenate the embedding with any of the known Rg kernels, for example
the Gaussian one (27). An example is the Fisher kernel which maps datapoints to
their “Fisher scores” under a parametric model. There has been a surge of interest recently
in automatic methods for parameterising low-dimensional non-linear manifolds (e.g., ) by local Euclidean coordinates. Although these methods are non-parametric, they can
be used to ﬁt conventional parametric mixture models in order to obtain a parametric
embedding which could then be used to obtain a kernel.
Recently, Kondor and Laﬀerty proposed kernels on discrete objects using concepts
from spectral graph theory (diﬀusion on graphs). If X is ﬁnite, a covariance function on
X is simply a positive semideﬁnite matrix. If H is a symmetric generator matrix, the
corresponding exponential kernel is deﬁned as
K(β) = exp(βH) =
We deﬁne Kβ(x, x′) = K(β)
 ′, where we use elements of X as indices into the matrix K (β).
K(β) is positive deﬁnite. In fact, it has the same eigenvectors as H, but the eigenspectrum is
transformed via λ →exp(βλ). In practice, general exponential kernels cannot be computed
feasibly if X is large, in particular there is no general eﬃcient way of computing kernel
matrices of Kβ over points of interest. It might be possible to approximate marginalisations
of K(β) by sampling. The kernel and generator matrices are linked by the heat equation
∂β K(β) = HK(β).
It is interesting to note that every inﬁnitely divisible covariance function Kβ with scale
parameter β on X has the form (33). Namely, if K is the covariance matrix for Kβ, then
H = ∂K/∂β at β = 0. Kondor and Laﬀerty are interested in diﬀusion kernels on graphs
as special cases of exponential kernels. Here, the generator is the negative of the so-called
graph Laplacian. The construction can be seen as stationary Markov chain (random walk)
in continuous time on the vertices of the graph. The kernel Kβ(x, x′) is the probability of
being at x′ at time β, given that the state at time 0 was x. This interpretation requires
that H1 = 0 which is true for the negative graph Laplacian and which implies that K (β)
is (doubly) stochastic. The same equation describes heat ﬂow or diﬀusion from an initial
distribution. The idea is to describe the structure of X (in the sense of “closeness”, i.e.
close points should be highly correlated under the covariance function) in terms of local
neighbourhood association which induce an (weighted or unweighted) undirected graph.
Then, the correlation at some x with all other points is proportional to the distribution
of a random walk started at x after time β. Similar ideas have been used very eﬀectively
for non-parametric clustering or classiﬁcation with partially labelled data . Kondor and
Laﬀerty give examples for graphs of special regular structures for which the diﬀusion kernel
can be determined eﬃciently. These include certain special cases of string kernels (here, X
is inﬁnite and the analogue to Markov chains has to be treated more carefully). In situations
where Kβ cannot be determined by known simple recursive formulae, one could represent
X by a representative sample including the training set (but also unlabelled data). If the
generator matrix of the underlying graph (projected onto the representative sample in a
sensible way) is sparse, its leading eigenvectors and eigenvalues could be approximated
by sparse eigensolvers which would lead to an approximation of K (β) which is low-rank
optimal w.r.t. the Frobenius norm. Kondor and Laﬀerty also note that on the graph given
by a regular grid in Rg, the generator matrix converges towards the usual Laplacian operator
and Kβ towards the Gaussian kernel (27) as the mesh size approaches 0.
How Useful are Uncertainty Estimates?
In this section we have highlighted a number of powerful techniques of encoding prior
knowledge in a covariance function or learning an appropriate kernel. For many problems in
machine learning (especially in classiﬁcation) one does not observe a big diﬀerence in generalisation error over a range of diﬀerent common kernels, while signiﬁcant diﬀerences arise
in the uncertainty estimates (predictive variances) for Bayesian GP techniques. Moreover,
the discussion in Section 7 suggests that much of the additional complexity in Bayesian GP
methods as compared to SVM arise exactly because such uncertainty estimates are desired
as well. It is therefore important to ask how useful these estimates are in practice.
Strictly speaking, both frequentist conﬁdence intervals and Bayesian uncertainty estimates
are tied to assumptions which are likely to be violated in non-trivial real world situations.
The former are conditioned on a null hypothesis which is certainly violated at some scale,
the latter require the data to be generated by the model. In a Bayesian setting, diﬀerent
priors and models can be compared either to conclude that the predictions enjoy a certain
robustness or to detect mismatches which should trigger a reﬁnement.
In the case of GP models, the choice of the covariance function can have a signiﬁcant eﬀect
on the uncertainty estimates. We demonstrate this fact using a simple one-dimensional
regression task. Note that in GP regression with Gaussian noise, the error bars do not
depend on the targets (this is diﬀerent for non-Gaussian likelihoods, e.g. in classiﬁcation).
Data was sampled from a noisy sine wave around π/2, (3/2)π, a single point at π, the
noise standard deviation was σ = 0.05. We compare the RBF covariance function (27) with
w = 4 against the Mat´ern kernel with diﬀerent ν and α = (w(2ν + 1))1/2, the process
variance was C = 1 in all cases. Recall that for the Mat´ern kernel, ν controls the degree of
m.s. diﬀerentiability of the process, while the RBF process is m.s. analytic. Figure 5 shows
mean predictions and one standard deviation error bars (the noise level was set to the true
As expected, for the Ornstein-Uhlenbeck prior (ν = 1/2) the mean prediction interpolates
the data, the error bars grow to the maximum value 1 very rapidly away from the data. A
Brownian motion process is not suitable as prior for a smoothing technique. The tendency
to interpolate rather than smooth the data diminishes with growing ν, as does the speed
with which the error bars grow to 1 away from data. Note also the very slim error bars for
the RBF prediction in the data-rich regions, expressing the strong (prior) belief that the
underlying function is smooth, thus close to the smooth mean prediction there. Stein 
notes that predictions using the RBF covariance function often come with unrealistically
small error bars.
In many situations, the uncertainty estimates themselves are of less importance than the
quality of the decisions based on them. In the Bayesian context, decisions are made by
substituting the predictive distribution inferred from data for the unknown truth. Utility
values can be computed as expectations over the predictive distribution and “Bayesian
optimal” decisions be made by comparing these for diﬀerent alternatives. A simple example
arises in binary classiﬁcation if the task allows us to reject a certain fraction of the test
patterns. The Bayesian optimal decision is to reject patterns for which the target predictive
distribution P(y∗|x∗, D) is most uncertain (has highest entropy). A similar setting is treated
heuristically with SVM discriminants rejecting those patterns for which the discriminant
value is closest to zero. Note that in both cases, we are interested in the order relations of the
scores over a test set rather than their numerical values. A study comparing both practices
(the GP technique is a sparse IVM approximation using the same amount of running
time) has been done in , Sect. 4.7.2. It concludes that on the example considered the
SVM reject strategy shows signiﬁcant weaknesses compared to the approximate Bayesian
IVM setup and that the additional work for obtaining uncertainty estimates can pay oﬀ.64
64It is shown that large wrong predictive means are often accompanied by large predictive variances,
Figure 5: Error bars for noisy sine regression task for diﬀerent covariance functions. Mean
prediction (solid), errors bars (dotted), true curve (dashed), data (dots). Upper left: RBF,
w = 4. Upper right: Ornstein-Uhlenbeck (Mat´ern, ν = 1/2), α = 2.8284. Lower left: Mat´ern,
ν = 3/2, α = 4. Lower right: Mat´ern, ν = 3/2, α = 4.899.
Note that these shortcomings of SVM cannot be alleviated by posthoc transformations of
the discriminant output (as suggested by ) because these leave order relations invariant.
In this paper, we described central properties of Gaussian processes and statistical models based on GPs together with eﬃcient generic ways of approximate inference and model
selection. The focus is less on giving algorithmic descriptions of concrete inference approximations and their variational optimisation problems, which may be found in the references
provided. Instead we hope to have conveyed the basic concepts of latent variables and
Gaussian random ﬁelds required to understand these non-parametric algorithms and to
have highlighted some of the essential diﬀerences to parametric statistical models. By the
explaining the superior performance of the Bayesian score which combines these two quantities.
evolution of ever more powerful computers and the development of fast sparse inference approximations, we feel that GP models will become applicable to large-data problems which
were previously restricted to parametric models. GP models are more powerful and ﬂexible
than simple linear parametric models and easier to handle than complicated ones such as
multi-layer perceptrons, and the availability of fast algorithms should remove remaining
obstacles of them becoming part of the standard toolbox of machine learning practitioners.
Acknowledgements
We thank Chris Williams for many discussions and important comments on early drafts,
David Barber and Bernhard Sch¨olkopf for corrections and improvements, Bernhard
Sch¨olkopf and the MPI T¨ubingen for their hospitality in September 2003, furthermore Neil
Lawrence, Ralf Herbrich, Lehel Csat´o, Manfred Opper, Carl Rasmussen, Amos Storkey and
Michael Tipping for discussions and comments. The author gratefully acknowledges support through a research studentship from Microsoft Research Ltd. during his postgraduate
In section A.1, we describe the notational conventions used in this paper and some concepts
from probability theory. In Section A.2 we collect some deﬁnitions.
Vectors a = (ai)i = (a1 . . . an)T (column by default) and matrices A = (ai,j)i,j are written
in bold-face. If A ∈Rm,n, I ⊂{1, . . . , m}, J ⊂{1, . . . , n} are index sets,65 then AI,J
denotes the |I| × |J| sub-matrix formed from A by selecting the corresponding entries
(i, j), i ∈I, j ∈J.
Some special vectors and matrices are deﬁned as follows: 0 = (0)i and 1 = (1)i the vectors
of all zero and all ones, δj = (δi,j)i the j-th standard unit vector. Here, δi,j = 1 if i = j,
and 0 otherwise (Kronecker symbol). Furthermore, I = (δi,j)i,j is the identity matrix.
The superscript T denotes transposition. diag a is the matrix with diagonal a and 0 elsewhere. diag A is the vector containing the diagonal of A. tr A is the sum of the diagonal
elements of A, tr A = 1T (diag A). |A| denotes the determinant of the square matrix A.
For p > 1, ∥a∥p denotes the p-norm of the vector a, ∥a∥p = (P
i |ai|p)1/p. If nothing else
is said, ∥· ∥= ∥· ∥2, the Euclidean norm. Relations are vectorised in Matlab style, as are
scalar functions: a ≥b means that ai ≥bi for all i, and f(a) = (f(ai))i.
We do not distinguish notationally between a random variable and its possible values.
Vector or matrix random variables are written in the same way as vectors or matrices. If a
distribution has a density, we generally use the same notation for the distribution and its
density function. If x is a random variable, then E[x] denotes the expectation (or expected
value) of x. If A is an event, then Pr{A} denotes its probability. The probability space will
65All index sets and sets of data points are assumed to be ordered, although we use a notation known
from unordered sets.
usually be clear from the context, but for clarity we often use an additional subscript, e.g.
PrS{A} or EP [x] (meaning that x ∼P). By IA, we denote the indicator function of an
event A, i.e. IA = 1 if A is true, IA = 0 otherwise. Note that Pr{A} = E[IA]. The delta
distribution δ
 places mass 1 onto the point x and no mass elsewhere, δ
  (B) = I{
  ∈B}. Let
X, Y, Z be sets of random variables, X, Y non-empty. We write X ⊥Y | Z to denote the
conditional independence of X and Y given Z: the conditional distribution of X given Y, Z
does not depend on Y.
log denotes the logarithm to Euler’s base e. The notation f(x) ∝g(x) means that f(x) =
cg(x) for c ̸= 0 constant w.r.t. x. We often use this notation with the left hand side being
a density. By sgn x, we denote the sign of x, i.e. sgn x = +1 for x > 0, sgn x = −1 for
x < 0, and sgn 0 = 0. The Landau O-notation is deﬁned as g(n) = O(f(n)) iﬀthere exists
a constant c ≥0 such that g(n) ≤c f(n) for almost all n.
We use some probability-theoretic concepts and notation which might be unfamiliar to the
reader. A measure is denoted by dµ(x), the Lebesgue measure in Rg is denoted by dx. If A
is a measurable set (“event”), µ(A) =
  ∈A}dµ(x) denotes its mass under µ. A measure
is ﬁnite if the mass of the whole space is ﬁnite, and a probability measure if this mass is
1. If dµ is a probability measure, we denote its distribution by µ. The events A of mass 0
are called null sets.66 For example, in Rg with Lebesgue measure (the usual “volume”) all
aﬃne spaces of dimension < g are null sets. A property is almost surely (a.s.) true if the
event of it being false is a null set. dµ1 is called absolutely continuous w.r.t. dµ2 if all null
sets of dµ1 are null sets of dµ2 (the notation is dµ1 ≪dµ2). The theorem of Radon and
Nikodym states that dµ1 has a density f(x) w.r.t. dµ2, i.e.
  ∈A}f(x) dµ2(x)
for all measurable A, iﬀdµ1 ≪dµ2. In this case,
f(x) = dµ1(x)
is called Radon-Nikodym derivative or simply density w.r.t. dµ2.
Deﬁnitions
Deﬁnition 1 (Relative Entropy) Let P, Q be two probability measures on the same
space with Q ≪P, such that the density dQ/dP exists almost everywhere. The relative
entropy is deﬁned as
D[Q ∥P] = EQ
If Q is not absolutely continuous w.r.t. P, we set D[Q ∥P] = ∞. It is always non-negative,
and equal to 0 iﬀQ = P. The function (Q, P) 7→D[Q ∥P] is strictly convex.
66In order not to run into trouble, we always assume that our probability space is complete, meaning that
its sigma-algebra contains all subsets of null sets.
If both Q and P have a density w.r.t. Lebesgue measure dw, then dQ/dP = Q(w)/P(w),
the ratio of the densities.
If we ﬁx a base measure P0 (ﬁnite, need not be a probability), the entropy can be deﬁned as
H[Q] = −D[Q ∥P0]. For continuous distributions over Rg, the uniform (Lebesgue) measure
is not ﬁnite. The usual remedy is to subtract oﬀan inﬁnite part of the entropy which does
not depend on the argument Q, ending up with the diﬀerential entropy
Q(w) log Q(w) dw.
Both entropy and diﬀerential entropy are concave functions (being the negative of convex