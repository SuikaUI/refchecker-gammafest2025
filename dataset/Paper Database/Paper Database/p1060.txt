The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)
Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks
Christopher Morris,1 Martin Ritzert,2 Matthias Fey,1 William L. Hamilton,3
Jan Eric Lenssen,1 Gaurav Rattan,2 Martin Grohe2
1TU Dortmund University
2RWTH Aachen University
3McGill University and MILA
{christopher.morris, matthias.fey, janeric.lenssen}@tu-dortmund.de,
{ritzert, rattan, grohe}@informatik.rwth-aachen.de,
 
In recent years, graph neural networks (GNNs) have emerged as
a powerful neural architecture to learn vector representations
of nodes and graphs in a supervised, end-to-end fashion. Up to
now, GNNs have only been evaluated empirically—showing
promising results. The following work investigates GNNs from
a theoretical point of view and relates them to the 1-dimensional
Weisfeiler-Leman graph isomorphism heuristic (1-WL). We
show that GNNs have the same expressiveness as the 1-WL in
terms of distinguishing non-isomorphic (sub-)graphs. Hence,
both algorithms also have the same shortcomings. Based on this,
we propose a generalization of GNNs, so-called k-dimensional
GNNs (k-GNNs), which can take higher-order graph structures
at multiple scales into account. These higher-order structures
play an essential role in the characterization of social networks
and molecule graphs. Our experimental evaluation conﬁrms
our theoretical ﬁndings as well as conﬁrms that higher-order
information is useful in the task of graph classiﬁcation and
regression.
Introduction
Graph-structured data is ubiquitous across application domains
ranging from chemo- and bioinformatics to image and social
network analysis. To develop successful machine learning
models in these domains, we need techniques that can exploit
the rich information inherent in graph structure, as well as the
feature information contained within a graph’s nodes and edges.
In recent years, numerous approaches have been proposed for
machine learning graphs—most notably, approaches based
on graph kernels or, alternatively,
using graph neural network algorithms .
Kernel approaches typically ﬁx a set of features in advance—
e.g., indicator features over subgraph structures or features
of local node neighborhoods. For example, one of the most
successful kernel approaches, the Weisfeiler-Lehman subtree
Copyright c⃝2019, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
kernel , which is based on the 1dimensional Weisfeiler-Leman graph isomorphism heuristic
 , generates node features through an
iterative relabeling, or coloring, scheme: First, all nodes are
assigned a common initial color; the algorithm then iteratively
recolors a node by aggregating over the multiset of colors
in its neighborhood, and the ﬁnal feature representation of
a graph is the histogram of the resulting node colors. By
iteratively aggregating over local node neighborhoods in this
way, the WL subtree kernel is able to effectively summarize the
neighborhood substructures present in a graph. However, while
powerful, the WL subtree kernel—like other kernel methods—
is limited because this feature construction scheme is ﬁxed (i.e.,
it does not adapt to the given data distribution). Moreover, this
approach—like the majority of kernel methods—focuses only
on the graph structure and cannot interpret continuous node and
edge labels, such as real-valued vectors which play an important
role in applications such as bio- and chemoinformatics.
Graph neural networks (GNNs) have emerged as a machine learning framework addressing the above challenges.
Standard GNNs can be viewed as a neural version of the
1-WL algorithm, where colors are replaced by continuous feature vectors and neural networks are used to aggregate over
node neighborhoods . In effect, the GNN framework can
be viewed as implementing a continuous form of graph-based
“message passing”, where local neighborhood information is
aggregated and passed on to the neighbors .
By deploying a trainable neural network to aggregate information in local node neighborhoods, GNNs can be trained
in an end-to-end fashion together with the parameters of the
classiﬁcation or regression algorithm, possibly allowing for
greater adaptability and better generalization compared to the
kernel counterpart of the classical 1-WL algorithm.
Up to now, the evaluation and analysis of GNNs has been
largely empirical, showing promising results compared to
kernel approaches, see, e.g., . However,
it remains unclear how GNNs are actually encoding graph
structure information into their vector representations, and
whether there are theoretical advantages of GNNs compared to
kernel based approaches.
Present Work. We offer a theoretical exploration of the relationship between GNNs and kernels that are based on the
1-WL algorithm. We show that GNNs cannot be more powerful than the 1-WL in terms of distinguishing non-isomorphic
(sub-)graphs, e.g., the properties of subgraphs around each
node. This result holds for a broad class of GNN architectures
and all possible choices of parameters for them. On the positive side, we show that given the right parameter initialization
GNNs have the same expressiveness as the 1-WL algorithm,
completing the equivalence. Since the power of the 1-WL has
been completely characterized, see, e.g., , we can transfer these
results to the case of GNNs, showing that both approaches have
the same shortcomings.
Going further, we leverage these theoretical relationships to
propose a generalization of GNNs, called k-GNNs, which are
neural architectures based on the k-dimensional WL algorithm
(k-WL), which are strictly more powerful than GNNs. The
key insight in these higher-dimensional variants is that they
perform message passing directly between subgraph structures,
rather than individual nodes. This higher-order form of message
passing can capture structural information that is not visible at
the node-level.
Graph kernels based on the k-WL have been proposed in
the past . However, a key
advantage of implementing higher-order message passing in
GNNs—which we demonstrate here—is that we can design
hierarchical variants of k-GNNs, which combine graph representations learned at different granularities in an end-to-end
trainable framework. Concretely, in the presented hierarchical
approach the initial messages in a k-GNN are based on the
output of lower-dimensional k′-GNN (with k′ < k), which
allows the model to effectively capture graph structures of
varying granularity. Many real-world graphs inherit a hierarchical structure—e.g., in a social network we must model
both the ego-networks around individual nodes, as well as the
coarse-grained relationships between entire communities, see,
e.g., —and our experimental results demonstrate that these hierarchical k-GNNs are able to consistently
outperform traditional GNNs on a variety of graph classiﬁcation and regression tasks. Across twelve graph regression tasks
from the QM9 benchmark, we ﬁnd that our hierarchical model
reduces the mean absolute error by 54.45% on average. For
graph classiﬁcation, we ﬁnd that our hierarchical models leads
to slight performance gains.
Key Contributions. Our key contributions are summarized as
1. We show that GNNs are not more powerful than the 1-WL
in terms of distinguishing non-isomorphic (sub-)graphs.
Moreover, we show that, assuming a suitable parameter
initialization, GNNs have the same power as the 1-WL.
2. We propose k-GNNs, which are strictly more powerful
than GNNs. Moreover, we propose a hierarchical version of
k-GNNs, so-called 1-k-GNNs, which are able to work with
the ﬁne- and coarse-grained structures of a given graph, and
relationships between those.
3. Our theoretical ﬁndings are backed-up by an experimental study, showing that higher-order graph properties are
important for successful graph classiﬁcation and regression.
Related Work
Our study builds upon a wealth of work at the intersection
of supervised learning on graphs, kernel methods, and graph
neural networks.
Historically, kernel methods—which implicitly or explicitly map graphs to elements of a Hilbert space—have been
the dominant approach for supervised learning on graphs. Important early work in this area includes random-walk based
kernels ) and kernels based on shortest paths . More recently, developments in graph
kernels have emphasized scalability, focusing on techniques
that bypass expensive Gram matrix computations by using
explicit feature maps. Prominent examples of this trend include kernels based on graphlet counting , and, most notably, the Weisfeiler-Lehman subtree
kernel as well as its higher-order
variants . Graphlet and
Weisfeiler-Leman kernels have been successfully employed
within frameworks for smoothed and deep graph kernels . Recent works focus
on assignment-based approaches , spectral approaches , and graph decomposition approaches . Graph kernels were dominant in graph
classiﬁcation for several years, leading to new state-of-the-art
results on many classiﬁcation tasks. However, they are limited
by the fact that they cannot effectively adapt their feature representations to a given data distribution, since they generally
rely on a ﬁxed set of features. More recently, a number of
approaches to graph classiﬁcation based upon neural networks
have been proposed. Most of the neural approaches ﬁt into
the graph neural network framework proposed by . Notable instances of this model include Neural
Fingerprints , Gated Graph Neural
Networks , GraphSAGE , SplineCNN , and the spectral approaches proposed in —all of
which descend from early work in and . Recent extensions and improvements to the GNN framework include approaches to
incorporate different local structures around subgraphs and novel techniques for pooling node representations in order perform graph classiﬁcation . GNNs have achieved state-of-the-art performance on several graph classiﬁcation benchmarks in recent
years, see, e.g., —as well as applications
such as protein-protein interaction prediction ,
recommender systems , and the analysis
of quantum interactions in molecules . A
survey of recent advancements in GNN techniques can be
found in .
Up to this point (and despite their empirical success) there
has been very little theoretical work on GNNs—with the no-
table exceptions of Li et al.’s work
connecting GNNs to a special form Laplacian smoothing and
Lei et al.’s work showing that the feature
maps generated by GNNs lie in the same Hilbert space as some
popular graph kernels. Moreover, Scarselli et al. investigates the approximation capabilities of GNNs.
Preliminaries
We start by ﬁxing notation, and then outline the Weisfeiler-
Leman algorithm and the standard graph neural network framework.
Notation and Background
A graph G is a pair (V, E) with a ﬁnite set of nodes V and a
set of edges E ⊆{{u, v} ⊆V | u ̸= v}. We denote the set of
nodes and the set of edges of G by V (G) and E(G), respectively. For ease of notation we denote the edge {u, v} in E(G)
by (u, v) or (v, u). Moreover, N(v) denotes the neighborhood
of v in V (G), i.e., N(v) = {u ∈V (G) | (v, u) ∈E(G)}.
We say that two graphs G and H are isomorphic if there exists
an edge preserving bijection ϕ : V (G) →V (H), i.e., (u, v)
is in E(G) if and only if (ϕ(u), ϕ(v)) is in E(H). We write
G ≃H and call the equivalence classes induced by ≃isomorphism types. Let S ⊆V (G) then G[S] = (S, ES) is the subgraph induced by S with ES = {(u, v) ∈E(G) | u, v ∈S}.
A node coloring is a function V (G) →Σ with arbitrary
codomain Σ. Then a node colored or labeled graph (G, l) is
a graph G endowed with a node coloring l: V (G) →Σ. We
say that l(v) is a label or color of v ∈V (G). We say that a
node coloring c reﬁnes a node coloring d, written c ⊑d, if
c(v) = c(w) implies d(v) = d(w) for every v, w in V (G).
Two colorings are equivalent if c ⊑d and d ⊑c, and we write
c ≡d. A color class Q ⊆V (G) of a node coloring c is a
maximal set of nodes with c(v) = c(w) for every v, w in Q.
Moreover, let [1 : n] = {1, . . . , n} ⊂N for n > 1, let S be
a set then the set of k-sets [S]k = {U ⊆S | |U| = k} for
k ≥2, which is the set of all subsets with cardinality k, and let
{{. . .}} denote a multiset.
Weisfeiler-Leman Algorithm
We now describe the 1-WL algorithm for labeled graphs. Let
(G, l) be a labeled graph. In each iteration, t ≥0, the 1-WL
computes a node coloring c(t)
: V (G) →Σ, which depends
on the coloring from the previous iteration. In iteration 0, we
= l. Now in iteration t > 0, we set
l (v)= H A S H
(v), {{c(t−1)
(u)|u ∈N(v)}}
where H A S H bijectively maps the above pair to a unique
value in Σ, which has not been used in previous iterations. To
test two graph G and H for isomorphism, we run the above
algorithm in “parallel” on both graphs. Now if the two graphs
have a different number of nodes colored σ in Σ, the 1-WL
concludes that the graphs are not isomorphic. Moreover, if the
number of colors between two iterations does not change, i.e.,
the cardinalities of the images of c(t−1)
are equal, the
algorithm terminates. Termination is guaranteed after at most
max{|V (G)|, |V (H)|} iterations. It is easy to see that the
algorithm is not able to distinguish all non-isomorphic graphs,
e.g., see . Nonetheless, it is a
powerful heuristic, which can successfully test isomorphism
for a broad class of graphs .
The k-dimensional Weisfeiler-Leman algorithm (k-WL), for
k ≥2, is a generalization of the 1-WL which colors tuples from
V (G)k instead of nodes. That is, the algorithm computes a
coloring c(t)
l,k : V (G)k →Σ. In order to describe the algorithm,
we deﬁne the j-th neighborhood
Nj(s)={(s1, . . . , sj−1, r, sj+1, . . . , sk) | r ∈V (G)} (2)
of a k-tuple s = (s1, . . . , sk) in V (G)k. That is, the j-th
neighborhood Nj(t) of s is obtained by replacing the j-th
component of s by every node from V (G). In iteration 0,
the algorithm labels each k-tuple with its atomic type, i.e.,
two k-tuples s and s′ in V (G)k get the same color if the
map si 7→s′
i induces a (labeled) isomorphism between the
subgraphs induced from the nodes from s and s′, respectively.
For iteration t > 0, we deﬁne
j (s) = H A S H
(s′) | s′ ∈Nj(s)}}
k,l(s)= H A S H
1 (s), . . . , C(t)
Hence, two tuples s and s′ with c(t−1)
(s) = c(t−1)
different colors in iteration t if there exists j in [1:k] such that
the number of j-neighbors of s and s′, respectively, colored
with a certain color is different. The algorithm then proceeds
analogously to the 1-WL. By increasing k, the algorithm
gets more powerful in terms of distinguishing non-isomorphic
graphs, i.e., for each k ≥2, there are non-isomorphic graphs
which can be distinguished by the (k + 1)-WL but not by the
k-WL . We note here that
the above variant is not equal to the folklore variant of k-WL
described in , which differs
slightly in its update rule. However, it holds that the k-WL
using Equation (4) is as powerful as the folklore (k−1)-WL
 .
WL Kernels. After running the WL algorithm, the concatenation of the histogram of colors in each iteration can be used as
a feature vector in a kernel computation. Speciﬁcally, in the
histogram for every color σ in Σ there is an entry containing
the number of nodes or k-tuples that are colored with σ.
Graph Neural Networks
Let (G, l) be a labeled graph with an initial node coloring
f (0) : V (G) →R1×d that is consistent with l. This means
that each node v is annotated with a feature f (0)(v) in R1×d
such that f (0)(u) = f (0)(v) if and only if l(u) = l(v). Alternatively, f (0)(v) can be an arbitrary real-valued feature vector
associated with v. Examples include continuous atomic properties in chemoinformatic applications where nodes correspond
to atoms, or vector representations of text in social network
applications. A GNN model consists of a stack of neural network layers, where each layer aggregates local neighborhood
information, i.e., features of neighbors, around each node and
then passes this aggregated information on to the next layer.
A basic GNN model can be implemented as follows . In each layer t > 0, we
compute a new feature
f (t)(v) = σ
f (t−1)(v)·W (t)
f (t−1)(w)·W (t)
in R1×e for v, where W (t)
are parameter matrices from Rd×e , and σ denotes a component-wise non-linear
function, e.g., a sigmoid or a ReLU.1
Following , one may also replace the
sum deﬁned over the neighborhood in the above equation by
a permutation-invariant, differentiable function, and one may
substitute the outer sum, e.g., by a column-wise vector concatenation or LSTM-style update step. Thus, in full generality a
new feature f (t)(v) is computed as
f (t−1)(v), f W2
 {{f (t−1)(w) | w ∈N(v)}}
where f W1
aggr aggregates over the set of neighborhood features
merge merges the node’s representations from step (t −1)
with the computed neighborhood features. Both f W1
aggr and f W2
may be arbitrary differentiable, permutation-invariant functions (e.g., neural networks), and, by analogy to Equation 5,
we denote their parameters as W1 and W2, respectively. In
the rest of this paper, we refer to neural architectures implementing Equation (6) as 1-dimensional GNN architectures
A vector representation fGNN over the whole graph can be
computed by summing over the vector representations computed for all nodes, i.e.,
f (T )(v),
where T > 0 denotes the last layer. More reﬁned approaches
use differential pooling operators based on sorting and soft assignments .
In order to adapt the parameters W1 and W2 of Equations (5)
and (6), to a given data distribution, they are optimized in an
end-to-end fashion (usually via stochastic gradient descent)
together with the parameters of a neural network used for
classiﬁcation or regression.
Relationship Between 1-WL and 1-GNNs
In the following we explore the relationship between the
1-WL and 1-GNNs. Let (G, l) be a labeled graph, and let
t′≤t denote the GNN parameters given
by Equation (5) or Equation (6) up to iteration t. We encode
the initial labels l(v) by vectors f (0)(v) ∈R1×d, e.g., using a
1-hot encoding.
Our ﬁrst theoretical result shows that the 1-GNN architectures do not have more power in terms of distinguishing
between non-isomorphic (sub-)graphs than the 1-WL algorithm.
More formally, let f W1
merge and f W2
aggr be any two functions chosen
in (6). For every encoding of the labels l(v) as vectors f (0)(v),
1For clarity of presentation we omit biases.
and for every choice of W(t), we have that the coloring c(t)
1-WL always reﬁnes the coloring f (t) induced by a 1-GNN
parameterized by W(t).
Theorem 1. Let (G, l) be a labeled graph. Then for all t ≥0
and for all choices of initial colorings f (0) consistent with l,
and weights W(t),
Our second result states that there exist a sequence of parameter matrices W(t) such that 1-GNNs have exactly the same
power in terms of distinguishing non-isomorphic (sub-)graphs
as the 1-WL algorithm. This even holds for the simple architecture (5), provided we choose the encoding of the initial labeling
l in such a way that different labels are encoded by linearly
independent vectors.
Theorem 2. Let (G, l) be a labeled graph. Then for all t ≥0
there exists a sequence of weights W(t), and a 1-GNN architecture such that
Hence, in the light of the above results, 1-GNNs may viewed
as an extension of the 1-WL which in principle have the same
power but are more ﬂexible in their ability to adapt to the
learning task at hand and are able to handle continuous node
Shortcomings of Both Approaches
The power of 1-WL has been completely characterized, see,
e.g., . Hence, by using Theorems 1 and 2,
this characterization is also applicable to 1-GNNs. On the other
hand, 1-GNNs have the same shortcomings as the 1-WL. For
example, both methods will give the same color to every node in
a graph consisting of a triangle and a 4-cycle, although vertices
from the triangle and the vertices from the 4-cycle are clearly
different. Moreover, they are not capable of capturing simple
graph theoretic properties, e.g., triangle counts, which are an
important measure in social network analysis .
k-dimensional Graph Neural Networks
In the following, we propose a generalization of 1-GNNs,
so-called k-GNNs, which are based on the k-WL. Due to
scalability and limited GPU memory, we consider a set-based
version of the k-WL. For a given k, we consider all k-element
subsets [V (G)]k over V (G). Let s = {s1, . . . , sk} be a k-set
in [V (G)]k, then we deﬁne the neighborhood of s as
N(s) = {t ∈[V (G)]k | |s ∩t| = k −1} .
The local neighborhood NL(s) consists of all t ∈N(s) such
that (v, w) ∈E(G) for the unique v ∈s \ t and the unique
w ∈t \ s. The global neighborhood NG(s) then is deﬁned as
N(s) \ NL(s).2
2Note that the deﬁnition of the local neighborhood is different
from the the one deﬁned in which
is a superset of our deﬁnition. Our computations therefore involve
sparser graphs.
Learning higher-order graph properties
(a) Hierarchical 1-2-3-GNN network architecture
(b) Pooling from 2- to 3-GNN.
Figure 1: Illustration of the proposed hierarchical variant of the k-GNN layer. For each subgraph S on k nodes a feature f is learned,
which is initialized with the learned features of all (k −1)-element subgraphs of S. Hence, a hierarchical representation of the input
graph is learned.
The set based k-WL works analogously to the k-WL, i.e., it
computes a coloring c(t)
s,k,l : [V (G)]k →Σ as in Equation (1)
based on the above neighborhood. Initially, c(0)
s,k,l colors each
element s in [V (G)]k with the isomorphism type of G[s].
Let (G, l) be a labeled graph. In each k-GNN layer t ≥
0, we compute a feature vector f (t)
k (s) for each k-set s in
[V (G)]k. For t = 0, we set f (0)
k (s) to f iso(s), a one-hot
encoding of the isomorphism type of G[s] labeled by l. In each
layer t > 0, we compute new features by
(s) · W (t)
u∈NL(s)∪NG(s)
(u) · W (t)
Moreover, one could split the sum into two sums ranging
over NL(s) and NG(s) respectively, using distinct parameter
matrices to enable the model to learn the importance of local
and global neighborhoods. To scale k-GNNs to larger datasets
and to prevent overﬁtting, we propose local k-GNNs, where
we omit the global neighborhood of s, i.e.,
k,L(s) = σ
(s) · W (t)
(u) · W (t)
The running time for evaluation of the above depends on |V |,
k and the sparsity of the graph (each iteration can be bounded
by the number of subsets of size k times the maximum degree).
Note that we can scale our method to larger datasets by using
sampling strategies introduced in, e.g., . We can
now lift the results of the previous section to the k-dimensional
Proposition 3. Let (G, l) be a labeled graph and let k ≥2.
Then for all t ≥0, for all choices of initial colorings f (0)
consistent with l and for all weights W(t),
s,k,l ⊑f (t)
Again the second result states that there exists a suitable
initialization of the parameter matrices W(t) such that k-GNNs
have exactly the same power in terms of distinguishing nonisomorphic (sub-)graphs as the set-based k-WL.
Proposition 4. Let (G, l) be a labeled graph and let k ≥2.
Then for all t ≥0 there exists a sequence of weights W(t),
and a k-GNN architecture such that
s,k,l ≡f (t)
Hierarchical Variant
One key beneﬁt of the end-to-end trainable k-GNN framework—compared to the discrete k-WL algorithm—is that we
can hierarchically combine representations learned at different
granularities. Concretely, rather than simply using one-hot
indicator vectors as initial feature inputs in a k-GNN, we
propose a hierarchical variant of k-GNN that uses the features
learned by a (k −1)-dimensional GNN, in addition to the
(labeled) isomorphism type, as the initial features, i.e.,
for some Tk−1 > 0, where Wk−1 is a matrix of appropriate
size, and square brackets denote matrix concatenation.
Hence, the features are recursively learned from dimensions
1 to k in an end-to-end fashion. This hierarchical model also
satisﬁes Propositions 3 and 4, so its representational capacity
is theoretically equivalent to a standard k-GNN (in terms of
its relationship to k-WL). Nonetheless, hierarchy is a natural
inductive bias for graph modeling, since many real-world
graphs incorporate hierarchical structure, so we expect this
hierarchical formulation to offer empirical utility.
Experimental Study
In the following, we want to investigate potential beneﬁts of
GNNs over graph kernels as well as the beneﬁts of our proposed k-GNN architectures over 1-GNN architectures. More
precisely, we address the following questions:
Q1 How do the (hierarchical) k-GNNs perform in comparison
to state-of-the-art graph kernels?
Q2 How do the (hierarchical) k-GNNs perform in comparison
to the 1-GNN in graph classiﬁcation and regression tasks?
Q3 How much (if any) improvement is provided by optimizing
the parameters of the GNN aggregation function, compared
to just using random GNN parameters while optimizing
the parameters of the downstream classiﬁcation/regression
algorithm?
To compare our k-GNN architectures to kernel approaches we
use well-established benchmark datasets from the graph kernel
literature . The nodes of each graph in
these dataset is annotated with (discrete) labels or no labels.
Table 1: Classiﬁcation accuracies in percent on various graph benchmark datasets.
IM DB -B I N
I MD B-M U L
PT C - M R
G R A P H L E T
S H O RT E S T- PAT H
PAT C H YS A N
1- G N N N O T U N I N G
1-2-3-G N N N O T U N I N G
1-2-3-G N N
Table 2: Mean absolute errors on the Q M9 dataset. The far-right column shows the improvement of the best k-GNN model in
comparison to the 1-GNN baseline.
D T N N M P N N 
1-2-3-G N N
To demonstrate that our architectures scale to larger datasets
and offer beneﬁts on real-world applications, we conduct experiments on the Q M9 dataset , which consists of
133 385 small molecules. The aim here is to perform regression
on twelve targets representing energetic, electronic, geometric,
and thermodynamic properties, which were computed using
density functional theory.
We use the following kernel and GNN methods as baselines
for our experiments.
Kernel Baselines. We use the Graphlet kernel , the shortest-path kernel , the Weisfeiler-Lehman subtree kernel (WL) , the Weisfeiler-Lehman Optimal Assignment kernel (WL-OA) ,
and the global-local k-WL 
with k in {2, 3} as kernel baselines. For each kernel, we computed the normalized Gram matrix. We used the C-SVM implementation of LIBSVM to compute
the classiﬁcation accuracies using 10-fold cross validation. The
parameter C was selected from {10−3, 10−2, . . . , 102, 103}
by 10-fold cross validation on the training folds.
Neural Baselines. To compare GNNs to kernels we used
the basic 1-GNN layer of Equation (5), DCNN , PatchySan ,
DGCNN . For the Q M9 dataset we used a
1-GNN layer similar to , where we replaced
the inner sum of Equation (5) with a 2-layer MLP in order
incorporate edge features (bond type and distance information).
Moreover, we compare against the numbers provided in .
Model Conﬁguration
We always used three layers for 1-GNN, and two layers for
(local) 2-GNN and 3-GNN, all with a hidden-dimension size of
64. For the hierarchical variant we used architectures that use
features computed by 1-GNN as initial features for the 2-GNN
(1-2-GNN) and 3-GNN (1-3-GNN), respectively. Moreover, us-
ing the combination of the former we componentwise concatenated the computed features of the 1-2-GNN and the 1-3-GNN
(1-2-3-GNN). For the ﬁnal classiﬁcation and regression steps,
we used a three layer MLP, with binary cross entropy and mean
squared error for the optimization, respectively. For classiﬁcation we used a dropout layer with p = 0.5 after the ﬁrst layer
of the MLP. We applied global average pooling to generate
a vector representation of the graph from the computed node
features for each k. The resulting vectors are concatenated
column-wise before feeding them into the MLP. Moreover, we
used the Adam optimizer with an initial learning rate of 10−2
and applied an adaptive learning rate decay based on validation
results to a minimum of 10−5. We trained the classiﬁcation
networks for 100 epochs and the regression networks for 200
Experimental Protocol
For the smaller datasets, which we use for comparison against
the kernel methods, we performed a 10-fold cross validation
where we randomly sampled 10% of each training fold to act
as a validation set. For the Q M9 dataset, we follow the dataset
splits described in . We randomly sampled
10% of the examples for validation, another 10% for testing,
and used the remaining for training. We used the same initial
node features as described in . Moreover,
in order to illustrate the beneﬁts of our hierarchical k-GNN
architecture, we did not use a complete graph, where edges
are annotated with pairwise distances, as input. Instead, we
only used pairwise Euclidean distances for connected nodes,
computed from the provided node coordinates. The code was
built upon the work of and is provided at https:
//github.com/chrsmrrs/k-gnn.
Results and Discussion
In the following we answer questions Q1 to Q3. Table 1 shows
the results for comparison with the kernel methods on the
graph classiﬁcation benchmark datasets. Here, the hierarchical
k-GNN is on par with the kernels despite the small dataset sizes
(answering question Q1). We also ﬁnd that the 1-2-3-GNN
signiﬁcantly outperforms the 1-GNN on all seven datasets
(answering Q2), with the 1-GNN being the overall weakest
method across all tasks.3 We can further see that optimizing
the parameters of the aggregation function only leads to slight
performance gains on two out of three datasets, and that no
optimization even achieves better results on the P ROT E I N S
benchmark dataset (answering Q3). We contribute this effect
to the one-hot encoded node labels, which allow the GNN to
gather enough information out of the neighborhood of a node,
even when this aggregation is not learned.
Table 2 shows the results for the Q M9 dataset. On eleven
out of twelve targets all of our hierarchical variants beat the 1-
GNN baseline, providing further evidence for Q2. For example,
3Note that in very recent work, GNNs have shown superior results
over kernels when using advanced pooling techniques . Note that our layers can be combined with these pooling
layers. However, we opted to use standard global pooling in order to
compare a typical GNN implementation with standard off-the-shelf
on the target H we achieve a large improvement of 98.1% in
MAE compared to the baseline. Moreover, on ten out of twelve
datasets, the hierarchical k-GNNs beat the baselines from . However, the additional structural information
extracted by the k-GNN layers does not serve all tasks equally,
leading to huge differences in gains across the targets.
It should be noted that our k-GNN models have more parameters than the 1-GNN model, since we stack two additional
GNN layers for each k. However, extending the 1-GNN model
by additional layers to match the number of parameters of the
k-GNN did not lead to better results in any experiment.
Conclusion
We presented a theoretical investigation of GNNs, showing that
a wide class of GNN architectures cannot be stronger than the
1-WL. On the positive side, we showed that, in principle, GNNs
possess the same power in terms of distinguishing between
non-isomorphic (sub-)graphs, while having the added beneﬁt of
adapting to the given data distribution. Based on this insight, we
proposed k-GNNs which are a generalization of GNNs based
on the k-WL. This new model is strictly stronger then GNNs
in terms of distinguishing non-isomorphic (sub-)graphs and is
capable of distinguishing more graph properties. Moreover, we
devised a hierarchical variant of k-GNNs, which can exploit
the hierarchical organization of most real-world graphs. Our
experimental study shows that k-GNNs consistently outperform
1-GNNs and beat state-of-the-art neural architectures on largescale molecule learning tasks. Future work includes designing
task-speciﬁc k-GNNs, e.g., devising k-GNNs layers that exploit
expert-knowledge in bio- and chemoinformatic settings.
Acknowledgments
This work is supported by the German research council (DFG)
within the Research Training Group 2236 UnRAVeL and the
Collaborative Research Center SFB 876, Providing Information
by Resource-Constrained Analysis, projects A6 and B2.