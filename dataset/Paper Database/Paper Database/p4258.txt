AID: A Benchmark Dataset for Performance Evaluation of Aerial
Scene Classiﬁcation
Gui-Song Xia1, Jingwen Hu1,2, Fan Hu1,2, Baoguang Shi3,
Xiang Bai3, Yanfei Zhong1, Liangpei Zhang1
1State Key Lab. LIESMARS, Wuhan University, Wuhan, China.
2School of Electronic Information, Wuhan University, Wuhan, China.
3Electronic Information School, Huazhong University of Science and Technology, China.
August 19, 2016
Aerial scene classiﬁcation, which aims to automatically label an aerial image with a speciﬁc semantic
category, is a fundamental problem for understanding high-resolution remote sensing imagery. In recent
years, it has become an active task in remote sensing area and numerous algorithms have been proposed
for this task, including many machine learning and data-driven approaches. However, the existing datasets
for aerial scene classiﬁcation like UC-Merced dataset and WHU-RS19 are with relatively small sizes, and
the results on them are already saturated. This largely limits the development of scene classiﬁcation
algorithms. This paper describes the Aerial Image Dataset (AID): a large-scale dataset for aerial scene
classiﬁcation. The goal of AID is to advance the state-of-the-arts in scene classiﬁcation of remote sensing
images. For creating AID, we collect and annotate more than ten thousands aerial scene images. In
addition, a comprehensive review of the existing aerial scene classiﬁcation techniques as well as recent
widely-used deep learning methods is given. Finally, we provide a performance analysis of typical aerial
scene classiﬁcation and deep learning approaches on AID, which can be served as the baseline results on
this benchmark.
Introduction
Nowadays, aerial images enable us to measure the earth surface with detail structures and are a kind of
data source of great signiﬁcance for earth observation . Due to the drastically increasing number of
aerial images and the highly complex geometrical structures and spatial patterns, to eﬀectively understand
the semantic content of them is particularly important, driven by many real-world applications in remote
sensing community. In this paper, we focus on aerial scene classiﬁcation, a key problem in aerial image
understanding, which aims to automatically assign a semantic label to each aerial image in order to know
which category it belongs to.
The problem of aerial scene classiﬁcation has received growing attention in recent years . In the
literature, primary studies have devoted to classifying aerial images at pixel level, by assigning each pixel in
an aerial image with a thematic class . However, with the increasement of the spatial resolutions, it
turns to be infeasible to interpret aerial images at pixel level , mainly due to the fact that single pixels
quickly lose their thematic meanings and discriminative eﬃciency to separate diﬀerent type of land covers.
Speciﬁcally, in 2001, Blaschke and Strobl have raised the question “What’s wrong with pixels?” and argued
that it is more eﬃcient to analyze aerial images at object-level, where the “objects” refer to local regions of
pixels sharing spectral or texture homogeneities, e.g. superpixels . This kind of approaches then have
 
dominated the analysis of high-resolution remote sensing images for decades . It is worth noticing that
both pixel- and object-level classiﬁcation methods attempt to model an aerial scene in a bottom-up manner
by aggregating extracted spectral, texture and geometrical features for training a strong classiﬁer.
However, due to the growing of image spatial resolutions, aerial scenes may often consist of diﬀerent and
distinct thematic classes and it is of great interest to reveal the context of these thematic classes, i.e.
semantic information, of aerial scenes. Aerial scene classiﬁcation aims to classify an aerial image into diﬀerent
semantic categories by directly modeling the scenes by exploiting the variations in the spatial arrangements
and structural patterns. In contrast with pixel-/object-oriented classiﬁcation, scene classiﬁcation provide a
relatively high-level interpretation of aerial images. More precisely, the item “scene” hereby usually refers to
a local area in large-scale aerial images that contain clear semantic information on the surface .
Though many exciting progresses on aerial scene classiﬁcation have been extensivly reported in recent
years, e.g.
 , there are two major issues that seriously limit the development of aerial scene
classiﬁcation.
- Lacking a comprehensive review of existing methods. Although many methods have been presented to
advance the aerial scene classiﬁcation, most of them were evaluated on diﬀerent datasets under diﬀerent
experimental settings. This somewhat makes the progress confused and may misleads the development
of the problem. Moreover, the codes of these algorithms have not been released, which brings diﬃculties
to reproduce the works for fair comparisons. Therefore, the state-of-the-art of aerial scene classiﬁcation
is not absolutely clear.
- Lacking proper benchmark datasets for performance evaluation. In order to develop robust methods
for aerial scene classiﬁcation, it is highly expected that the datasets for evaluation demonstrate all the
challenging aspects of the problem. For instance, the high diversities in the geometrical and spatial
patterns of aerial scenes. Currently, the evaluations of aerial scene classiﬁcation algorithms are typically
done on datasets containing up to two-thousands images at best, e.g. the UC-Merced dataset and
the WHU-RS19 dataset . Such limited number of images are critically insuﬃcient to approximate
the real applications, where the images are with high intra-class diversity and low inter-class variation.
Recently, the saturated results on these datasets demonstrated that the more challenging datasets are
badly required.
Due to the above issues, in this paper, we present a comprehensive review to up-to-date algorithms as
well as a new large-scale benchmark dataset of aerial images (named as AID), in order to fully advance
the task of aerial scene classiﬁcation. AID provides the research community a benchmark resource for the
development of the state-of-the-art algorithms in aerial scene classiﬁcation or other related tasks such as
aerial image search, aerial image annotation, aerial image segmentation, etc. In addition, our experiments
on AID demonstrate that it is quite helpful to reﬂect the shortcomings of existing methods. In summary, the
major contributions of this paper are as follows:
- We provide a comprehensive review on aerial scene classiﬁcation, by giving a clear summary of the
development of scene classiﬁcation approaches.
- We construct a new large-scale dataset, i.e. AID, for aerial scene classiﬁcation. The dataset is, to
our knowledge, of the largest size and the images are with high intra-class diversity and low inter-class
dissimilarity, which can provide the research community a better data resource to evaluate and advance
the state-of-the-art algorithms in aerial image analysis.
- We evaluate a set of representative aerial scene classiﬁcation approaches with various experimental
protocols on the new dataset. These can serve as baseline results for future works.
- The source codes of our implementation for all the baseline algorithms are released, will be general
tools for other researchers.
The rest of the paper is organized as follows. We ﬁrst provide a comprehensive review of related methods
in Section 2. The details of AID dataset are described in Section 3. Then, we provide the description of
baseline algorithms for benchmark evaluation in Section 4. In Section 5, the evaluation and comparison of
baseline algorithms on AID under diﬀerent experimental settings are given. Finally, some conclusion remarks
are drawn in Section 6.
The AID and the codes for reproducing all the results in this paper are downloadable at the project
webpage www.lmars.whu.edu.cn/xia/AID-project.html.
A review on aerial scene classiﬁcation
This section reviews comprehensively the existing scene classiﬁcation methods for aerial images. Distinguished
from pixel-/object-level image classiﬁcations which interpret aerial images with a bottom-up manner, scene
classiﬁcation is apt to directly model an aerial scene by developing a holistic representation of the aerial
image . One should observe that, actually, the underlying assumption of scene classiﬁcation is
that the same type of scene should share certain statistically holistic visual characteristics. This point has
been veriﬁed on natural scenes and demonstrates its eﬃciency on classifying aerial scenes. Thus, most of
the works on aerial scene classiﬁcation focus on computing such holistic and statistical visual attributes for
classiﬁcation features. In this sense, scene classiﬁcation methods can be divided into three main categories:
methods using low-level visual features, methods relying on mid-level visual representations and the methods
based on high-level vision information. In what follows, we review each category of the methods in details.
Methods using low-level visual features
With this kind of methods, it is supposed that aerial scenes can be distinguished by low-level visual features,
spectral, texture and structure, etc.
Consequently, an aerial scene image is usually described by a
feature vector extracted from such low-level visual attributes, either locally or globally . On one
hand, in order to describe the complex structures, local structure descriptors, e.g. the Scale Invariant Feature
Transform (SIFT) , have been widely used for modeling the local variations of structures in aerial images.
The classiﬁcation feature vector is usually formed by concatenating or pooling the local descriptors of the
subregions of an image. On the other hand, for depicting the spatial arrangements of aerial scenes, statistical
and global distributions of certain spatial cues such as color and texture information have also
been well investigated. For instance, Yang and Newsam compared SIFT and Gabor texture features for
classifying IKONOS satellite images by using Maximum A Posteriori (MAP) classiﬁer and found that SIFT
performs better. Santos et. al evaluated various global color descriptors and texture descriptors, e.g.
color histogram , local binary pattern (LBP) , for scene classiﬁcation.
Although single type of features work well for aerial image classiﬁcation , the combinations
of complementary features can often improve the results, see e.g. . In particular, Luo et. al 
extracted 6 diﬀerent kinds of feature descriptors, i.e. simple radiometric features, Gaussian wavelet features , Gray Level Co-Occurrence Matrix (GLCM), Gabor ﬁlters, shape features and SIFT, and
combined them to form a multiple-feature representation for indexing remote sensing images with diﬀerent
spatial resolutions, which reported that multiple features can describe aerial scenes better. Avramovic and
Risojevic integrated Gist and SIFT descriptors for aerial scene classiﬁcation. Some class-speciﬁc feature
selection methods were also developed to select a good subset of low-level visual features for aerial image
classiﬁcation .
In order to encode the global spatial arrangements and the geometrical diversities of aerial scenes, Xia
al proposed an invariant and robust shape-based scene descriptor to describe the structure distributions of aerial images. While Vladimir et. al focused on the texture information of scenes and they
successively proposed a local structural texture descriptor , an orientation diﬀerence descriptor , and
an Enhanced Gabor Texture Descriptor (EGTD) based on the Gabor ﬁlters to further improve the
performance. In , a multi-scale completed local binary patterns (MS-CLBP) was proposed for land-use
scene classiﬁcation and achieved the state-of-the-art performance among low-level methods.
It is worth noticing that scene classiﬁcation methods with low-level visual features perform well on some
aerial scenes with uniform structures and spatial arrangements, but it is diﬃcult for them to depict the
high-diversity and the non-homogeneous spatial distributions in aerial scenes .
Methods relying on mid-level visual representations
In contrast with methods relying on low-level visual attributes, mid-level aerial scene analysis approaches
attempt to develop a holistic scene representation through representing the high-order statistical patterns
formed by the extracted local visual attributes. A general pipeline is to ﬁrst extract local image attributes,
e.g. SIFT, LBP and color histograms of local image patches, and then encode these local cues for building a
holistic mid-level representation for aerial scenes.
One of the most popular mid-level approaches is the bag-of-visual-words (BoVW) model . More precisely, this method ﬁrst described local image patches by SIFT descriptors, then learned a vocabulary
of visual words (also known as dictionary or codebook) for instance by k-means clustering. Subsequently,
the local descriptors were encoded against the vocabulary by hard assignment, i.e., vector quantization, and
a global feature vector of the image could be obtained by the histogram of visual words, which is actually
counting the occurrence frequencies of each visual words in the image. Thanks to its simplicity and eﬃciency,
BoVW model and its variants have been widely adopted for computing mid-level representation for aerial
scenes, see e.g. .
In order to improve the discriminative power of BoVW model, multiple complemented low-level visual
features were combined under the framework. For instance, in various local descriptors, including SIFT,
GIST, color histogram and LBP, etc., were evaluated with the standard BoVW model for aerial scene classiﬁcation. The experiments of the concatenation of BoVW representations from diﬀerent local descriptors
proved that combining complemented features can signiﬁcantly improve the classiﬁcation accuracy. Similarly,
in , a highly discriminative texture descriptor, i.e. combined scattering feature , was incorporated
with SIFT and color histogram to extract structure and spectral information under a multi-feature extraction
scheme. Unlike the simple concatenation in , in this work, a hierarchical classiﬁcation method incorporating Extreme Value Theory (EVT)-based normalization was used to calibrate multiple features. In ,
multiple features like structure features, spectral features and texture features were extracted and encoded
in a sparse coding scheme besides BoVW. The sparse coding scheme was developed based on BoVW
but adding a sparsity constraint to the feature distributions to reduce the complexity of Support Vector Machine (SVM) meanwhile maintain good performance. In , various feature coding methods developed from
BoVW model were evaluated for scene classiﬁcation using multi-features. By applying PCA for dimension
reduction before concatenating multi-features, the Improved Fisher Vector (IFK) and Vectors of Locally
Aggregated Tensors (VLAT) methods reported to achieve the state-of-the-art performances.
Note that in BoVW models, they count the frequencies of the visual words in an image, with regardless
of the spatial distribution of the visual words.
However, the spatial arrangements of visual words, e.g.
co-occurrence, convey important information of aerial scenes. Therefore, some methods were proposed to
incorporate the spatial distribution of visual words beyond the BoVW models. For instance, Yang et al 
developed the spatial pyramid co-occurrence kernel (SPCK) to integrate the absolute and relative spatial
information ignored in the standard BoVW model setting, by relying on the idea of spatial pyramid match
kernel (SPM) and spatial co-occurrence kernel (SCK) . Later, Zhao et. al proposed another
way to incorporate the spatial information, where wavelet decomposition was utilized in the BoVW model
to combine not only the spatial information but also the texture information. Moreover, in , the authors
proposed a concentric circle-based spatial-rotation-invariant representation to encode the spatial information.
In , a pyramid-of-spatial-relatons (PSR) model was developed to capture both absolute and relative
spatial relationships of local low-level features. Unlike the conventional co-occurrence approaches that
describe pairwise spatial relationships between local features, the PSR model employed a novel concept of
spatial relation to describe relative spatial relationship between a group of local features and reported better
performance.
In addition, to encode higher-order spatial information between low-level local visual words for scene
modeling, topic models along with the BoVW scheme are developed to take into account the semantic
relationship among the visual words . Among them, Latent Dirichlet Allocation (LDA) 
model deﬁnes an intermediate variable named “topic”, which serves as a connection between the visual words
and the image. The probability distribution of the topics are estimated by Dirichlet distribution and were used
to describe an image instead of the marginal distribution of visual words with much lower dimensional features.
To combine diﬀerent features, Kusumaningrum et. al used CIELab color moments , GLCM and
edge orientation histogram (EOH) to extract spectral, texture and structure information respectively
in the LDA model. In , the probabilistic Latent Semantic Analysis (pLSA) model was adopted for
scene classiﬁcation in a multi-feature fusion manner and achieved better result than single feature. In ,
pLSA and LDA were compared using a multi-feature fusion strategy to combine three complementary
features in a semantic allocation level, and LDA demonstrated slightly better performances.
Observe that, in all the aforementioned methods, various hand-craft local image descriptors are used
to represent aerial scenes.
One main diﬃculty of such methods lies in the fact that they may lack the
ﬂexibility and adaptivity to diﬀerent scenes. In this sense, unsupervised feature learning approaches have
been developed to automatically learn adaptive feature representations from images, see e.g. .
In , a sparse coding based method was proposed to learn a holistic scene representation from raw pixel
values and other low-level features for aerial images. In , Hu et. al discovered the intrinsic space of
local image patches by applying diﬀerent manifold learning techniques and made the dictionary learning
and feature encoding more eﬀective. Also with the unsupervised feature learning scheme, Zhang et. al 
extracted the features of image patches by the sparse Auto-Encoder (SAE) and exploited the local spatial
and structural information of complex aerial scenes.
Methods based on high-level vision information
Currently, deep learning methods achieve impressive results on many computer vision tasks such as image
classiﬁcation, object and scene recognition, image retrieval, etc. This type of methods also achieve stateof-the-art performance on aerial scene classiﬁcation, see e.g. . In general, deep learning
methods use a multi-stage global feature learning architecture to adaptively learn image features and often
cast the aerial scene classiﬁcation as an end-to-end problem. Compared with low-level and mid-level methods,
deep learning methods can learn more abstract and discriminative semantic features and achieve far better
classiﬁcation performance .
It has been reported that by directly using the pre-trained deep neural network architectures on the natural
images , the extracted global features showed impressive performance on aerial scene classiﬁcation .
The two freely available pre-trained deep Convolution Neural Network (CNN) architectures are OverFeat 
and CaﬀeNet . In , another promising architecture, i.e. GoogLeNet , was considered and evaluated.
This architecture also showed astounding performance for aerial images. In , it demonstrated that a multiscale input strategy for multi-view deep learning can improve the performance of aerial scene classiﬁcation.
In contrast with directly using the features from the fully-connected layer of the pre-trained CNN architectures as the ﬁnal representation , others use the deep-CNN as local feature extractor and combine
it with feature coding techniques. For instance, Hu et. al extracted multi-scale dense CNN activations
from the last convolutional layer as local features descriptors and further coded them using feature encoding
methods like BoVW , Vector of Locally Aggregated Descriptors (VLAD) and Improved Fisher Kernel
(IFK) to generate the ﬁnal image representation. For all the deep-CNN architectures used above, either
the global or local features were obtained from the networks pre-trained on natural image datasets and were
directly used for classiﬁcation aerial images.
In addition to the above two ways using deep-learning methods, another choice is to train a new deep
network. However, as reported in , using the existing aerial scene datasets (e.g. UC-Merced dataset 
and the WHU-RS19 dataset ) to fully train the networks such as CaﬀeNet or GoogLeNet showed
a drop in accuracies compared with using the networks as global feature extractors. This can be explained by
the fact that the large scale networks usually contain millions of parameters to be trained, therefore, to train
them using the aerial datasets with only a few hundreds or thousands images will easily stick in overﬁtting and
local minimum. Thus, to better ﬁt the dataset, smaller networks for classiﬁcation were trained . In ,
a Gradient Boosting Random Convolutional Network (GBRCN) was proposed for classifying aerial images
with only two convolutional layers. In , a deep belief network (DBN) was trained on aerial images,
and the feature selection problem was formulated as a feature reconstruction problem in the DBN scheme.
By minimizing the reconstruction error over the whole feature set, the features with smaller reconstruction
errors can hold more feature intrinsics for image representation. However, the generalization ability of a
small network is often lower than that of large scale networks. It is highly demanded to train a large-scale
network with large number of annotated aerial images.
Aerial Image Datasets (AID) for
aerial scene classiﬁcation
This section ﬁrst reviews several datasets commonly used for aerial scene classiﬁcation and then described
the proposed Aerial Image Dataset (AID)1.
Existing datasets for aerial scene classiﬁcation
UC-Merced dataset 
It consists of 21 classes of land-use images selected from aerial ortho-imagery with the pixel resolution of
one foot. The original images were downloaded from the United States Geological Survey (USGS) National
Map of the following US regions: Birmingham, Boston, Buﬀalo, Columbus, Dallas, Harrisburg, Houston,
Jacksonville, Las Vegas, Los Angeles, Miami, Napa, New York, Reno, San Diego, Santa Barbara, Seattle,
Tampa, Tucson, and Ventura. They are then cropped into small regions of 256 × 256 pixels. There are
totally 2100 images manually selected and uniformly labeled into 21 classes: agricultural, airplane, baseball
diamond, beach, buildings, chaparral, dense residential, forest, freeway, golf course, harbor, intersection,
medium density residential, mobile home park, overpass, parking lot, river, runway, sparse residential, storage
tanks, and tennis courts.
It is worth noticing that UCM dataset contains a variety of spatial land-use patterns which make the
dataset more challenging. Moreover, some highly overlapped classes, e.g. dense residential, medium residential and sparse residential that mainly diﬀer in the density of structures, make the dataset diﬃcult for
classiﬁcation. This dataset is widely used for the task of aerial image classiﬁcation .
WHU-RS dataset
This dataset is collected from Google Earth imagery3. The images are with ﬁxed size of 600 × 600 pixels
with various pixel resolutions up to half a meter. This dataset has been updated to the third versions until
now. In its original version , there are 12 classes of aerial scenes including airport, bridge, river, forest,
meadow, pond, parking, port, viaduct, residential area, industrial area, and commercial area. For each class,
there were 50 samples. Later, Sheng et.al expanded the dataset to 19 classes with 7 new ones, i.e. beach,
desert, farmland, football ﬁeld, mountain, park and railway station. Thus, the dataset is composed of a total
number of 950 aerial images, which is widely used as the WHU-RS19 dataset . However,
1The AID is downloadable at www.lmars.whu.edu.cn/xia/AID-project.html.
3 
the size of this dataset is relatively small compared with UC-Merced dataset . Thus, we reorganized
and expanded WHU-RS19 to form its third version , by adding a new aerial scene type “bare land” and
increasing the number of samples in each class. In the newest version of the WHU-RS dataset, it thus has
5000 aerial images with each class containing more than 200 sample images. It is worth noticing that the
sample images of the same class in WHU-RS dataset are collected from diﬀerent regions all around the world
and the aerial scenes might appear at diﬀerent scales, orientations and with diﬀerent lighting conditions.
RSSCN7 dataset 
This dataset is also collected from Google Earth3, which contains 2800 aerial scene images labeled into 7
typical scene categories, i.e., the grassland, forest, farmland, parking lot, residential region, industrial region,
river and lake. There are 400 images in each scene type, and each image has a size of 400 × 400 pixels. It
is worth noticing that the sample images in each class are sampled on 4 diﬀerent scales with 100 images per
scale with diﬀerent imaging angles, which is the main challenge of the dataset.
Other small datasets
Besides the three public datasets mentioned before, there are also several non-public datasets, e.g., the
IKONOS Satellite Image dataset , the In-House dataset , the SPOT Image dataset , the
ORNL dataset , etc. Note that the numbers of scene types in all these datasets are less than 10, which
thus results in small intra-class diversity. Moreover, common used mid-level scene classiﬁcation methods get
saturated and have reported overall accuracies nearly 100% on these dataset. Therefore, these less challenging
datasets will severely restrict the development of aerial scene classiﬁcation algorithms.
AID: a new dataset for aerial scene classiﬁcation
To advance the state-of-the-arts in scene classiﬁcation of remote sensing images, we construct AID, a new
large-scale aerial image dataset, by collecting sample images from Google Earth imagery. Note that although
the Google Earth images are post-processed using RGB renderings from the original optical aerial images,
Hu et al. have proven that there is no signiﬁcant diﬀerence between the Google Earth images with the
real optical aerial images even in the pixel-level land use/cover mapping. Thus, the Google Earth images can
also be used as aerial images for evaluating scene classiﬁcation algorithms.
The new dataset is made up of the following 30 aerial scene types: airport, bare land, baseball ﬁeld, beach,
bridge, center, church, commercial, dense residential, desert, farmland, forest, industrial, meadow, medium
residential, mountain, park, parking, playground, pond, port, railway station, resort, river, school, sparse
residential, square, stadium, storage tanks and viaduct. All the images are labelled by the specialists in the
ﬁeld of remote sensing image interpretation, and some samples of each class are shown in Fig. 1. The numbers
of sample images varies a lot with diﬀerent aerial scene types, see Table. 1, from 220 up to 420. In all, the
AID dataset has a number of 10000 images within 30 classes.
The images in AID are actually multi-source, as Google Earth images are from diﬀerent remote imaging
sensors. This brings more challenges for scene classiﬁcation than the single source images like UC-Merced
dataset .
Moreover, all the sample images per each class in AID are carefully chosen from diﬀerent
countries and regions around the world, mainly in China, the United States, England, France, Italy, Japan,
Germany, etc., and they are extracted at diﬀerent time and seasons under diﬀerent imaging conditions, which
increases the intra-class diversities of the data.
Note that another main diﬀerence between AID and UC-Merced dataset is that AID has multi-resolutions:
the pixel-resolution changes from about 8 meters to about half a meter, and thus the size of each aerial image
is ﬁxed to be 600 × 600 pixels to cover a scene with various resolutions.
3 
baseball field
commercial
dense residential
industrial
medium residential
playground
railway station
sparse residential
storage tanks
Figure 1: Samples of AID: three examples of each semantic scene class are shown. There are 10000 images
within 30 classes.
Table 1: The diﬀerent semantic scene classes and the number of images in each class of the new dataset.
railway station
baseball ﬁeld
industrial
medium residential
sparse residential
commercial
dense residential
playground
storage tanks
Why AID is proper for aerial image classiﬁcation?
In contrast with existing remote sensing image datasets, e.g. UC-Merced dataset and WHU-RS19 dataset,
AID has following properties:
- Higher intra-class variations: In aerial images, due to the high spatial resolutions, the geometrical
structures of scenes become more clear and bring more challenges to image classiﬁcation. First, thanks
to the high complexity of the earth surface, objects in the same type of scene may appear at diﬀerent
sizes and orientations. Second, the diﬀerent imaging conditions, e.g. the ﬂying altitude and direction
and the solar elevation angles, may also vary a lot the appearance of the scene. Thus, in order to
develop robust aerial image classiﬁcation algorithms with stronger generalized capability, it is accepted
that the dataset contains high intra-class diversity. The increasing numbers of sample images per each
class in AID allow us to collect images from diﬀerent regions all over the world accompanied with
diﬀerent scales, orientations and imaging conditions, which can increase the intra-class diversities of
the dataset, see e.g. Fig. 2. In Fig. 2.(a), we illustrate two examples of the same scene with diﬀerent
scale. In Fig. 2.(b), we display examples of the same type of scene with diﬀerent building styles, as the
sample images are collected in diﬀerent regions and countries and the appearances of the same scene
varies a lot due to the cultural diﬀerences. In Fig. 2.(c), the shadow direction of the buildings vary
from west to north at diﬀerent imaging time; and a mountain varies from green to white along with
the seasonal variation.
- Smaller inter-class dissimilarity: In real cases of aerial image classiﬁcation, the dissimilarities
between diﬀerent scene classes are often small. The construction of AID well considered this point,
by adding more scene categories. As displayed in Fig. 3, AID contains scenes sharing similar objects.
e.g., both stadium and playground may contain sports ﬁeld (see Fig. 3.(a)), but the main diﬀerence
lies in whether there are stands around. Both bare land and desert are ﬁne-grained textures and share
similar colors (see Fig. 3.(b)), but bare land usually has more artiﬁcial traces. Some scene classes have
similar structural distributions, like resort and park (see Fig. 3.(c)), which may contain a lake and some
buildings, etc., however, a park is generally equipped with amusement and leisure facilities while a
resort is usually composed of villas for vacations. The AID has taken into account many these kinds of
scene classes with small inter-class dissimilarity and makes it closer to real aerial image classiﬁcation
- Relative large-scale dataset: For validating the classiﬁcation algorithm, large-scale labeled data are
often expected. However, the mannual annotation of aerial images requires expertise and is extremely
time-consuming.
AID has a total number of 10000 images which is, to our knowledge, the largest
annotated aerial image datasets. It can cover a much wider range of aerial images and better aproximate
commercial
railway station
Figure 2: Large intra-class diversity:(a). multi-scale images of the same scene; (b). diﬀerent building styles
of the same scene; (c). diﬀerent imaging conditions of the same scene.
playground
Figure 3: Small inter-class distance: (a).
similar objects between diﬀerent scenes; (b).
similar textures
between diﬀerent scenes; (c). similar structural distributions between diﬀerent scenes.
the real aerial image classiﬁcation problem than existing dataset. In contrast with our AID, both the
UC-Merced dataset and WHU-RS19 dataset contain 100 images per class and only 20 images
in each class were usually used for testing the algorithms . In such
case, the classiﬁcation accuracy will be seriously aﬀected by even one image is predicted correctly or
not and result in big standard deviation, especially when analyzing the classiﬁcation results on each
class. Therefore, our AID with relatively large-scale data can provide a better benchmark to evaluate
image classiﬁcation methods.
Baseline methods
In this section, we evaluate diﬀerent aerial scene classiﬁcation methods with low-, mid- and high-level scene
descriptions reviewed previously2. The general classiﬁcation pipeline is demonstrated in Fig. 4.
Methods with low-level scene features
Aerial image classiﬁcation methods using low-level scene features often ﬁrst partition an aerial image into
small patches, then use low-level visual features, e.g. spectral information, texture information or structure
2The codes of the baseline methods are downloadable at www.lmars.whu.edu.cn/xia/AID-project.html.
Mid-level methods
Low-level methods
High-level methods
Scene Classification
descriptors
descriptors
dictionary
classifier
classifier
classifier
Figure 4: The general pipeline of three types of scene classiﬁcation methods.
information etc., to characterize patches and ﬁnally output the distribution of the patch features as the scene
descriptor. In our tests, we choose four commonly used low-level methods in the experiment, i.e. SIFT ,
LBP , Color Histogram (CH) and GIST .
- SIFT : It describes a patch by the histograms of gradients computed over a 4 × 4 spatial grid. The
gradients are then quantized into eight bins so the ﬁnal feature vector has a dimension of 128 (4×4×8).
- LBP : Some works adopt LBP to extract texture information from aerial images, see . For a
patch, it ﬁrst compares the pixel to its 8 neighbors: when the neighbor’s value is less than the center
pixel’s, output “1”, otherwise, output “0”. This gives an 8-bit decimal number to describe the center
pixel. The LBP descriptor is obtained by computing the histogram of the decimal numbers over the
patch and results in a feature vector with 256 dimensions.
- Color histogram : Color histograms (CH) are used for extracting the spectral information of aerial
scenes . In our experiments, color histogram descriptors are computed separately in three
channels of the RGB color space. Each channel is quantized into 32 bins to form a total histogram
feature length of 96 by simply concatenation of the three channels.
- GIST : Unlike aforementioned descriptors that focus on local information, GIST represents the dominant spatial structure of a scene by a set of perceptual dimensions (naturalness, openness, roughness,
expansion, ruggedness) based on the spatial envelope model and thus widely used for describing
scenes . This descriptor is implemented by convolving the gray image with multi-scale (with the
number of S) and multi-direction (with the number of D) Gabor ﬁlters on a 4 × 4 spatial grid. By
concatenating the mean vector of each grid, we get the GIST descriptor of an image with 16 × S × D
dimensions.
Methods with mid-level scene features
In contrast with low-level methods, mid-level aerial scene classiﬁcation methods often build a scene representation by coding low-level local feature descriptors. In this paper, we evaluated 21 commonly used mid-level
features obtained by combining 3 local feature descriptors (i.e., SIFT , LBP and CH ) with 7
mid-level feature coding approaches.
- Bag of Visual Words (BoVW) model an image by leaving out the spatial information and representing it with the frequencies of local visual words . BoVW model and its variants are widely used
in scene classiﬁcation . The visual words are often produced by clustering
local image descriptors to form a dictionary (with a given size K), e.g. using k-means algorithm.
- Spatial Pyramid Matching (SPM) uses a sequence of increasingly coarser grids to build a spatial
pyramid (with L levels) coding of local image descriptors. By concatenating the weighted local image
features in each subregion at diﬀerent scales, one can get a (4L−1)×K
dimension global feature vector
which is much longer than BoVW with the same size of dictionary (K).
- Locality-constrained Linear Coding (LLC) is an eﬀective coding scheme adapted from sparse coding
methods .
It utilizes the locality constraints to code each local descriptor into its localcoordinate system by modifying the sparsity constraints . The ﬁnal feature can be generated by
max pooling of the projected coordinates with the same size of dictionary.
- Probabilistic Latent Semantic Analysis (pLSA) is a way to improve the BoVW model by topic models. A latent variable called topic is introduced and deﬁned as the conditional probability distribution
of visual words in the dictionary. It can serve as a connection between the visual words and images.
By describing an image with the distribution of topics (the number of topics is set to be T), one can
solve the inﬂuence of synonym and polysemy meanwhile reduce the feature dimension to be T.
- Latent Dirichlet allocation (LDA) is a generative topic model evolved from pLSA with the main
diﬀerence that it adds a Dirichlet prior to describe the latent variable topic instead of the ﬁxed Gaussian
distribution, and is also widely used for scene classiﬁcation . As a result, it can handel
the problem of overﬁtting and also increase the robustness. The dimension of ﬁnal feature vector is the
same with the number of topics T.
- Improved Fisher kernel (IFK) uses Gaussian Mixture Model (GMM) to encode local image features and achieves good performance in scene classiﬁcation . In essence, the feature of an
image got by Fisher vector encoding method is a gradient vector of the log-likelihood. By computing
and concatenating the partial derivatives of the mean and variance of the Gaussian functions, the ﬁnal
feature vector is obtained with the dimension of 2 × K × F (where F indicates the dimension of the
local feature descriptors and K denotes the size of the dictionary).
- Vector of Locally Aggregated Descriptors (VLAD) can be seen as a simpliﬁcation of the IFK
method which aggregates descriptors based on a locality criterion in feature space . It uses the
non-probabilistic k-means clustering to generate the dictionary by taking the place of GMM model
in IFK. When coding each local patch descriptor to its nearest neighbor in the dictionary, the diﬀerences between them in each dimension are accumulated and resulting in an image feature vector with
dimension of K × F.
Methods with high-level scene features
In recent years, learned high-level deep features have been reported to achieve impressive results on aerial
image classiﬁcation . In this work, we also compare 3 representative high-level deep-learned
scene classiﬁcation methods in our benchmark.
- CaﬀeNet: Caﬀe (Convolutional Architecture for Fast Feature Embedding) is one of the most
commonly used open-source frameworks for deep learning (deep convolutional neural networks in particular). The reference model - CaﬀeNet, which is almost a replication of ALexNet that is proposed
for the ILSVRC 2012 competition . The main diﬀerences are: (1) there is no data argumentation
during training; (2) the order of normalization and pooling are switched. Therefore, it has quite similar
performances to the AlexNet, see . For this reason, we only test CaﬀeNet in our experiment.
The architecture of CaﬀeNet comprises 5 convolutional layers, each followed by a pooling layer, and 3
fully connected layers at the end. In our work, we directly use the pre-trained model obtained using
the ILSVRC 2012 dataset , and extract the activations from the ﬁrst fully-connected layer, which
results in a vector of 4096 dimensions for an image.
- VGG-VD-16: To investigate the eﬀect of the convolutional network depth on its accuracy in the largescale image recognition setting, gives a thorough evaluation of networks by increasing depth using
an architecture with very small (3 × 3) convolution ﬁlters, which shows a signiﬁcant improvement on
the accuracies, and can be generalised well to a wide range of tasks and datasets. In our work, we use
one of its best-performing models, named VGG-VD-16, because of its simpler architecture and slightly
better results. It is composed of 13 convolutional layers and followed by 3 fully connected layers, thus
results in 16 layers. Similarly, we extract the activations from the ﬁrst fully connected layer as the
feature vectors of the images.
- GoogLeNet: This model won the ILSVRC-2014 competition .
Its main novelty lies in the
design of the ”Inception modules”, which is based on the idea of ”network in network” . By using
the Inception modules, GoogLeNet has two main advantages: (1) the utilization of ﬁlters of diﬀerent
sizes at the same layer can maintain multi-scale spatial information; (2) the reduction of the number
of parameters of the network makes it less prone to overﬁtting and allows it to be deeper and wider.
Speciﬁcally, GoogLeNet is a 22-layer architecture with more than 50 convolutional layers distributed
inside the inception modules. Diﬀerent from the above CNN models, GoogLeNet has only one fully
connected layer at last, therefore, we extract the features of the fully connected layer for testing.
Experimental studies
We evaluate all the three kinds of scene classiﬁcation methods mentioned before: methods with low-level,
mid-level and high-level scene features. For each type, we choose some representative ones as baseline for evaluation: SIFT , LBP , Color Histogram (CH) and GIST for low-level methods, BoVW ,
Spatial Pyramid Matching (SPM) , Locality-constrained Linear Coding (LLC) , Probabilistic Latent
Semantic Analysis (pLSA) , Latent Dirichlet allocation (LDA) , Improved Fisher kernel (IFK) 
and Vector of Locally Aggregated Descriptors (VLAD) combined with three local feature descriptors
(i.e., SIFT , LBP , CH ) for mid-level methods, and three representative high-level deep-learning
methods (i.e., CaﬀeNet , VGG-VD-16 and GoogLeNet ) are adopted.
Parameter Settings
In our experiment, we ﬁrstly test four kinds of low-level methods for classiﬁcation: SIFT, LBP, CH and Gist.
For the local patch descriptor SIFT, we use a ﬁxed size grid (16 × 16 pixels) with the spacing step to be 8
pixels to extract all the descriptors in the gray image plain and adopt the average pooling method for each
dimension of the descriptor so as to get the ﬁnal image feature with 128 dimensions (8 orientations with 4×4
subregions). As for the remaining three descriptors, we use them as global descriptors that can extract the
feature vectors on the whole image very eﬃciently. For LBP, we use the common used 8 neighbors to get
the binary values and convert the 8-bit binary values into a decimal value for each pixel in the gray image.
By computing the frequencies of the 256 patterns, we get the low-level LBP features of an image. For CH,
we directly use the RGB color space and quantize each channel into 32 bins. Thus, the feature of an image
is obtained by concatenation of the statistical histograms in each channel and result in 96 dimensions. For
Gist descriptor, we set the same parameters as in its original work : the number of scales is set to be 4,
the orientations are quantized into 8 bins and a 4 × 4 spatial grid is utilized for pooling, thus, it results in
512 dimensions (4 × 8 × 4 × 4).
For mid-level methods, we test afore-mentioned seven diﬀerent feature coding methods: BoVW, SPM,
LLC, pLSA, LDA, IFK and VLAD. Three local patch descriptors - SIFT, LBP and CH have been utilized for
extracting the local structure, texture and spectral features respectively. In the patch sampling procedure,
we use the grid sampling as our previous work has proven that grid sampling has better performance for
scene classiﬁcation of remote sensing imagery. Therefore, we set the patch size to be 16 × 16 pixels and the
grid spacing to be 8 pixels for all the local descriptors to balance the speed/accuracy trade-oﬀ. By combining
the three local feature descriptors and seven global feature coding methods, we can get 21 diﬀerent mid-level
features in all. As for the size of the dictionary, we set it from 16 to 8192 for the 7 coding methods when
using SIFT as local feature descriptors, and select the optimal one when using LBP and CH for describing
local patches. For some special parameters deﬁned in each coding methods, we empirically set the spatial
pyramid level to be 2 in SPM, and both the numbers of topics in pLSA and LDA are set to be a half of the
dictionary size.
For high-level methods, we just use the CNN models pre-trained on the ILSVRC 2012 dataset and
extract the features from the ﬁrst fully connected layer in each CNN model as the global features. CaﬀeNet
and VDD-VD-16 result in a vector of 4096 dimensions while GoogLeNet a 1024-dimensional feature vector
owing to the fact that GoogLeNet has only one fully connected layer, and all the features are L2 normalized
for better performance.
After getting the global features using various methods, we use the liblinear for supervised classiﬁcation because it can quickly train a linear classiﬁer on large scale datasets. More speciﬁcally, we spilt the
images in the dataset into training set and testing set. The features of the training set are used to training a linear classiﬁcation model by liblinear, and the features of the testing set are used for estimating the
performance of the trained model.
Evaluation protocols
To compare the classiﬁcation quantitatively, we compute the common used measures: overall accuracy (OA)
and Confusion matrix.
OA is deﬁned as the number of correctly predicted images divided by the total
number of predicted images. It is a direct measure to reveal the classiﬁcation performance on the whole
dataset. Confusion matrix is a speciﬁc table layout that allows direct visualization of the performance on
each class. Each column of the matrix represents the instances in a predicted class, and each row represents
the instances in an actual class, thus, each item xij in the matrix computes the proportion of images that
predicted to be the i-th type meanwhile trully belong to the j-th type.
To compute OA, we adopt two diﬀerent settings for each tested dataset in the supervised classiﬁcation
process. For the RSSCN7 dadaset and our AID dataset, we ﬁx the ratio of the number of training set to
be 20% and 50% respectively and the left for testing, while for UC-Merced dataset, the ratios are set to be
50% and 80% respectively. For the WHU-RS19 dataset, the ratios are ﬁxed at 40% and 60% respectively. To
compute the overall accuracy, we randomly split the datasets into training sets and testing sets for evaluation,
and repeat it ten times to reduce the inﬂuence of the randomness and obtain reliable results. The OA is
computed for each run, and the ﬁnal result is reported as the mean and standard deviation of OA from the
individual run.
To compute the confusion matrix, we ﬁx the training set by choosing the same images for fair comparison
on each datasets and ﬁx the ratio of the number of training set of the UC-Merced dataset, the WHU-RS19
dataset,the RSSCN7 dadaset and our AID dataset to be 50%, 40%, 20% and 20% respectively.
Experimental results
In this section, we evaluate diﬀerent methods on the common used UC-Merced dataset, WHU-RS19 dataset,
RSSCN7 dataset as well as our AID dataset, and give the corresponding results and analysis, which are
divided into four phases: results of low-level methods, results of mid-level methods, results of high-level
methods and confusion matrix.
Results with low-level methods
Table 2: Overall accuracy (OA) of diﬀerent low-level methods on the UC-Merced dataset, the WHU-RS19
dataset, the RSSCN7 dataset and our AID dataset.
UC-Merced (.5)
UC-Merced (.8)
WHU-RS19 (.4)
WHU-RS19 (.6)
RSSCN7 (.2)
RSSCN7 (.5)
28.92±0.95
32.10±1.95
25.37±1.32
27.21±1.77
28.45±1.03
32.76±1.25
13.50±0.67
16.76±0.65
34.57±1.38
36.29±1.90
40.11±1.46
44.08±2.02
57.55±1.18
60.38±1.03
26.26±0.52
29.99±0.49
42.09±1.14
46.21±1.05
48.79±2.37
51.87±3.40
57.20±1.23
60.54±1.01
34.29±0.40
37.28±0.46
44.36±1.58
46.90±1.76
45.65±1.06
48.82±3.12
49.20±0.63
52.59±0.71
30.61±0.63
35.07±0.41
Table. 2 illustrates the means and standard variances of OA using the four kinds of low-level methods
(e.g. SIFT, LBP, CH, GIST) with randomly choosing the ﬁxed percent of images to construct the training
set by repeating 10 times on the UC-Merced dataset, the WHU-RS19 dataset, the RSSCN7 dataset and our
AID dataset. Although diﬀerent features give diﬀerent performance on diﬀerent datasets, we can observe the
consistent phenomenon on all datasets that SIFT descriptor performs far less than others with about 20%
lower OA than the highest ones, which indicates that SIFT descriptor is not suitable to be as low-level feature
for directly classiﬁcation. For the other three low-level features, GIST performs the best on the UC-Merced
dataset, and CH gives the best performances on both WHU-RS19 dataset and our AID dataset, while LBP
and CH give comparable results on the RSSCN7 dataset. The diﬀerent performances can be explained by the
characteristics of the datasets, for example, both the UC-Merced dataset and our AID dataset contain various
artiﬁcial scene types, which are mainly made up of various buildings, therefore, GIST, which can extract the
dominant spatial structure of a scene, performs well on these datasets. For the RSSCN7 datasets, which
contains much natural scene types, thus, the texture feature descriptor LBP works the best. In addition, CH
gives the most robust performances on all the datasets, because most scene types are color consistent, e.g.,
grass ia mostly green and desert is dark yellow.
Results with mid-level methods
For the seven kinds of feature coding methods, the size of the dictionary has a great inﬂuence on the
classiﬁcation results, therefore, we need to ﬁrstly ﬁnd the optimal dictionary size for each coding method. To
do so, we ﬁx the local patch descriptor using SIFT, and gradually double increase the dictionary size from 16
to 8192 for each coding methods on the three datasets. The corresponding OA is shown in Fig. 5. For BoVW,
the larger the dictionary, the better the performance. However, the performance increases quite slowly when
the dictionary size becomes larger. Therefore, we ﬁx the dictionary size of BoVW to be 4096 in the following
experiment. For IFK and VLAD, the dictionary size has quite little inﬂuence on the performance for all
the datasets. But the larger dictionary will result in much higher dimensional features and thus more timeconsuming for training classiﬁcation model, therefore, the corresponding dictionary sizes are ﬁxed at 32 and
64 respectively. For LDA, pLSA and SPM, there is a drop when the size of the dictionary achieves to some
degree. Thus, we choose 1024, 1024 and 128 to be the corresponding dictionary sizes which are suitable for
all the datasets. For LLC, the performance is increasing all the way with the size of the dictionary, therefore,
we ﬁx it at 8192 in the following experiment. Note that the parameters we choose are not always the optimal
ones, which is to make a trade-oﬀbetween accuracy and speed.
After ﬁnding the proper dictionary size for each coding method, we set the corresponding values for other
local patch descriptors and evaluate the 21 kinds of mid-level features obtained by combining seven kinds
of global feature coding methods (e.g., BoVW, SPM, LLC, pLSA, LDA, IFK, VLAD) with three kinds of
local feature descriptors (e.g., SIFT, LBP, CH). Table. 3 shows the means and standard variances of OA
on each dataset. Surprisingly, when comparing the results using diﬀerent local feature descriptors, SIFT
Dictionary Size
Mean Overall Accuracy
(a) UC-Merced
Dictionary Size
Mean Overall Accuracy
(b) WHU-RS19
Dictionary Size
Mean Overall Accuracy
(c) RSSCN7
Dictionary Size
Mean Overall Accuracy
Figure 5: Overall accuracy (OA) of diﬀerent mid-level coding methods using diﬀerent dictionary size on the
UC-Merced dataset, the WHU-RS19 dataset and our AID dataset.
can give consistent the best performances, while CH performs the worst, while in the low-level features,
SIFT is the worst among the low-level methods and CH is the most robust. This indicates that SIFT is
more suitable to be encoded in the mid-level methods to generate more robust feature representation. By
comparing the results using diﬀerent global feature coding methods, BoVW and IFK account for the highest
two OA in general, pLSA, LLC and VLAD are in the middle, while the left two methods have relatively
worse performances. When comparing all the 21 mid-level methods, the features obtained by IFK with SIFT
descriptor perform the best on all the datasets, which beneﬁts from the combination of the great robustness
and invariance of SIFT when describing local patches and the generative and discriminative nature of IFK.
Results with high-level methods
Table. 4 illustrates the means and standard variances of OA using the high-level methods (i.e., the features
extracted from the ﬁrst fully connected layer using the pre-trained CNN models) on the four datasets. From
the classiﬁcation results, we can see that CaﬀeNet and VGG-VD-16 give similar performances on all the
datasets, while GoogLeNet performs slightly worse.
Note that CaﬀeNet has only 8 layers that is much
shallower than the VGG-VD-16 and GoogLeNet which has 16 and 22 layers respectively. Superﬁcially, this
phenomenon may result in the conclusion that shallower network works better, which is inconsistent with
image classiﬁcation of natural images. However, note the fact that the networks are all trained by the natural
images, we just use them as feature extractors in our experiment. Therefore, the deeper the network, the
Table 3: Overall accuracy (OA) of diﬀerent mid-level methods on the UC-Merced dataset, the WHU-RS19
dataset, the RSSCN7 dataset and our AID dataset.
UC-Merced (.5)
UC-Merced (.8)
WHU-RS19 (.4)
WHU-RS19 (.6)
RSSCN7 (.2)
RSSCN7 (.5)
BoVW (SIFT)
72.40±1.30
75.52±2.13
77.21±1.92
82.58±1.72
76.91±0.59
81.28±1.19
62.49±0.53
68.37±0.40
IFK (SIFT)
78.74±1.65
83.02±2.19
83.35±1.19
87.42±1.59
81.08±1.21
85.09±0.93
71.92±0.41
78.99±0.48
LDA (SIFT)
59.24±1.66
61.29±1.97
69.91±2.23
72.18±1.58
71.07±0.70
73.86±0.77
51.73±0.73
50.81±0.54
LLC (SIFT)
70.12±1.09
72.55±1.83
73.28±1.37
78.63±2.04
73.29±0.63
77.11±1.29
58.06±0.50
63.24±0.44
pLSA (SIFT)
67.55±1.11
71.38±1.77
73.25±1.80
77.50±1.20
75.25±1.20
79.37±0.97
56.24±0.58
63.07±0.48
SPM (SIFT)
56.50±1.00
60.02±1.06
51.82±1.63
55.82±1.95
64.97±0.79
68.45±1.01
38.43±0.51
45.52±0.61
VLAD (SIFT)
71.94±1.36
75.98±1.60
73.96±2.22
79.16±1.71
74.30±0.74
79.34±0.71
61.04±0.69
68.96±0.58
BoVW (LBP)
73.48±1.39
78.12±1.38
71.11±2.72
75.89±2.40
76.74±0.82
81.40±1.09
56.98±0.55
64.31±0.41
73.11±1.08
78.02±1.60
71.02±2.66
75.61±1.86
75.18±1.18
80.31±1.46
60.11±0.56
69.22±0.72
61.87±1.92
63.40±2.05
62.93±2.27
67.37±2.35
70.47±0.87
73.63±0.91
43.22±0.53
41.51±0.76
67.19±1.40
72.95±1.46
72.89±1.98
76.00±0.99
73.28±0.56
77.46±0.86
56.11±0.61
61.53±0.55
pLSA (LBP)
68.84±1.18
74.07±1.71
66.07±2.20
71.08±2.11
74.94±0.52
78.97±1.19
49.71±0.55
57.31±0.58
55.26±1.34
60.52±1.46
52.72±0.98
56.18±2.43
68.05±1.30
71.26±0.96
38.33±0.82
44.16±0.43
VLAD (LBP)
69.02±0.94
74.83±2.02
65.02±2.80
70.29±1.91
72.63±0.93
77.41±1.42
53.15±0.61
61.19±0.49
69.80±1.11
76.33±2.32
63.26±1.52
67.29±1.55
75.07±1.18
81.74±0.60
49.16±0.24
56.84±0.45
73.87±1.09
79.14±1.91
70.04±1.47
74.89±2.25
76.86±0.78
83.32±0.72
59.60±0.66
67.49±0.81
60.15±1.23
64.12±1.75
55.23±1.57
58.92±2.56
68.11±1.85
71.29±1.30
38.16±0.60
41.70±0.80
68.62±1.68
73.00±1.41
64.46±1.68
68.82±1.94
74.12±0.80
79.94±0.92
53.47±0.43
58.23±0.49
67.66±0.92
72.88±2.14
59.88±1.87
63.34±1.93
73.69±1.60
78.79±1.02
48.35±0.31
55.70±0.52
53.56±0.94
57.17±1.72
54.05±1.38
56.39±1.67
64.86±1.26
68.24±0.71
39.60±0.56
44.01±0.41
67.69±1.58
72.48±2.24
59.53±1.85
63.97±2.32
72.59±1.02
79.21±0.87
47.94±0.39
57.34±0.73
Table 4: Overall accuracy (OA) of high-level methods on the UC-Merced dataset, the WHU-RS19 dataset,
the RSSCN7 dataset and our AID dataset.
UC-Merced (.5)
UC-Merced (.8)
WHU-RS19 (.4)
WHU-RS19 (.6)
RSSCN7 (.2)
RSSCN7 (.5)
93.98±0.67
95.02±0.81
95.11±1.20
96.24±0.56
85.57±0.95
88.25±0.62
86.86±0.47
89.53±0.31
94.14±0.69
95.21±1.20
95.44±0.60
96.05±0.91
83.98±0.87
87.18±0.94
86.59±0.29
89.64±0.36
92.70±0.60
94.31±0.89
93.12±0.82
94.71±1.33
82.55±1.11
85.84±0.92
83.44±0.40
86.39±0.55
more likely the learned features oriented to the natural image processing task, which may result in worse
performance for classifying aerial scenes.
Compared with the above low-level and mid-level methods, high-level methods show far better performance on both datasets, which indicates that the high-level methods have the ability to learn highly discriminative features. Moreover, note that all the networks we use are pre-trained models on the ILSVRC 2012
dataset , i.e., all the parameters are trained by the natural images, which shows its great generalization
ability compared with other methods.
In addition, in all the above methods, the standard deviations of OA on our new dataset are much lower
than the others, which is mainly caused by the number of testing samples. There are only dozens of images per
class for testing of the UC-Merced dataset and WHU-RS19 dataset, thus, OA will have a greater variation
range if the numbers of right predictions in each run are inconsistent with only a few numbers. But the
number of testing images in our new dataset is more than 10 times larger than the above two, thus, it will
result in much smaller standard variances, which can help to evaluate the performances more precisely.
Confusion matrix
Besides giving the OA of various methods, we also compute the corresponding confusion matrix. For each
dataset, we choose to show the best results of the low-level, mid-level and high-level methods for each dataset.
Fig. 6 shows the confusion matrix using low-level (GIST), mid-level IFK (SIFT) and high-level (VGG-VD-16)
on the UC-Merced dataset, and Fig. 7 gives the results on WHU-RS19 dataset, and Fig. 8 gives the results
on RSSCN7 dataset, and Fig. 9 is our AID dataset.
From the confusion matrix on the UC-Merced dataset (Fig. 6), we can see that there are only 2 classes
(a) low-level (GIST)
(b) mid-level IFK (SIFT)
(b) high-level (VGG-VD-16)
Figure 6: Confusion matrix obtained by low-level (GIST), mid-level IFK (SIFT) and high-level (VGG-VD-16)
on the UC-Merced dataset.
(a) low-level (CH)
(b) mid-level IFK (SIFT)
(c) high-level (VGG-VD-16)
Figure 7: Confusion matrix obtained by low-level (CH), mid-level IFK (SIFT) and high-level (VGG-VD-16)
on the WHU-RS19 dataset.
River Lake
River Lake
(a) low-level (LBP)
River Lake
River Lake
(b) mid-level IFK (SIFT)
River Lake
River Lake
(c) high-level (CaﬀeNet)
Figure 8: Confusion matrix obtained by low-level (LBP), mid-level IFK (SIFT) and high-level (CaﬀeNet) on
the RSSCN7 dataset.
obtain the classiﬁcation accuracy above 0.8, and most classes are easily confused with others using low-level
features; when using mid-level features, the classiﬁcation accuracies of all the scene types increase and more
than a half of the classes achieve the classiﬁcation accuracy above 0.8; while for high-level features, the scene
types can be easily distinguished from others that the classiﬁcation accuracies of most classes are close to or
even equal to 1. The most notable confusion is among buildings, dense residential, medium residential and
sparse residential, for their similar structures and land cover types.
From the confusion matrix on the WHU-RS19 dataset (Fig. 7) and RSSCN7 dataset (Fig. 8), we can
observe the similar phenomenon that low-level features can not distinguish the scene types well, and midlevel features can improve the classiﬁcation accuracy a lot, while high-level features obtain a quite clean
(a) low-level (CH)
(b) mid-level IFK (SIFT)
(c) high-level (CaﬀeNet)
Figure 9: Confusion matrix obtained by low-level (CH), mid-level IFK (SIFT) and high-level (CaﬀeNet) on
our AID dataset.
confusion matrix.
When analyzing the confusion matrix on our AID dataset (Fig. 9), similarly, the low-level features give the
worst performance while high-level give the best. Although most scene types can achieve the classiﬁcation
accuracy close to 1 using high-level features, most of them are natural scene types and thus easy to be
distinguished, e.g., bareland, beach, desert, forest, mountain , etc.. Note that the most diﬃcult case in the
UC-Merced dataset, i.e., sparse residential, medium residential and dense residential areas, the classiﬁcation
accuracies in our new dataset are around 0.9, which no longer belongs to the diﬃcult ones. The most diﬃcult
scene types in our new dataset are almost newly added scene types, i.e., school (0.49), resort(0.6), square
(0.63) and center(0.65). The most notable confusion is between resort and park, which may be caused by
the fact that some parks are built for leisure, which have the similar appearances with resorts for holiday,
especially in China, they may both contain green belts, lakes, etc., and thus are easily confused. In addition,
school and dense residential are also highly confused for the densely distributed buildings.
By comparing the confusion matrix among the four datasets, we can conclude that our new dataset is more
suitable for aerial scene classiﬁcation than the others for it contains much ﬁne-grained meanwhile challenging
scene types.
Discussion
From the above experimental results, we can summarize some interesting but meaningful observations as
- By comparing various scene classiﬁcation methods, we can observe the layered performances as the
name implies: low-level methods have relatively worse performances, while high-level methods perform
better on all the datasets, which shows the great potential of high-level methods.
- By comparing diﬀerent scene classiﬁcation datasets, we can ﬁnd that our new dataset is far more
challenging than the others for it has relatively higher intra-class variations and smaller inter-class
dissimilarity. In addition, the large numbers of sample images can help to evaluate various methods
more precisely.
The above observations can provide us with very meaningful instructions for investigating more eﬀective
high-level methods on more challenging dataset to promote the progress in aerial scene classiﬁcation.
Conclusion
In this paper, we ﬁrstly give a comprehensive review on aerial scene classiﬁcation by giving a clear summary of
the existing approaches. We ﬁnd that the results on the current popular-used datasets are already saturated
and thus severely limit the progress of aerial scene classiﬁcation. In order to solve the problem, we construct
a new large-scale dataset, i.e. AID, which is the largest and most challenging one for the scene classiﬁcation
of aerial images. The purpose of the dataset is to provide the research community with a benchmark resource
to advance the state-of-the-art algorithms in aerial scene analysis. In addition, we have evaluated a set of
representative aerial scene classiﬁcation approaches with various experimental protocols on the new dataset.
These can serve as baseline results for future works. Moreover, both the dataset and the codes are public
online for freely downloading to promote the development of aerial scene classiﬁcation.
Acknowledgment
The authors would like to thank all the researchers who kindly sharing the codes used in our studies and
all the volunteers who help us constructing the dataset. This research is supported by the National Natural
Science Foundation of China under the contracts No.91338113 and No.41501462.