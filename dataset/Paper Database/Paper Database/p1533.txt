Endogeneity in Probit Response Models
by David A. Freedman
and Jasjeet S. Sekhon
U. C. Berkeley CA 94720
DRAFT 6/8/2008
Abstract. In this paper, we look at conventional methods for removing endogeneity bias in regression models, including the linear model and the probit model.
The usual Heckman two-step procedure should not be used in the probit model:
from a theoretical perspective, this procedure is unsatisfactory, and likelihood
methods are superior. However, serious numerical problems occur when standard software packages try to maximize the biprobit likelihood function, even if
the number of covariates is small. The log likelihood surface may be nearly ﬂat,
or may have saddle points with one small positive eigenvalue and several large
negative eigenvalues. We draw conclusions for statistical practice. Finally, we
describe the conditions under which parameters in the model are identiﬁable; we
believe these results are new.
1. Introduction
Suppose a linear regression model describes responses to treatment and to covariates.
If subjects self-select into treatment, the process being dependent on the error term in the
model, endogeneity bias is likely. Similarly, we may have a linear model that is to be
estimated on sample data; if subjects self-select into the sample, endogeneity becomes an
Heckman suggested a simple and ingenious two-step method for taking
care of endogeneity, which works under the conditions described in those papers. This
method is widely used. Some researchers have applied the method to probit response
models. However, the extension is unsatisfactory. The non-linearity in the probit model is
an essential difﬁculty for the two-step correction, which will often make bias worse. It is
well-known that likelihood techniques are to be preferred—although, as we show here, the
numerics are delicate.
In the balance of this article, we deﬁne models for (a) self-selection into treatment
or control, and (b) self-selection into the sample, with simulation results to delineate the
statistical issues. In the simulations, the models are correct. Thus, anomalies in the behavior
of estimators are not to be explained by speciﬁcation error. Numerical issues are explored.
We explain the motivation for the two-step estimator, and draw conclusions for statistical
practice. We derive the conditions under which parameters in the models are identiﬁable;
we believe these results are new. The literature on models for self-selection is huge, and so
is the literature on probits; we conclude with a brief review of a few salient papers.
To deﬁne the models and estimation procedures, consider n subjects, indexed by i =
1, . . . , n. Subjects are assumed to be independent and identically distributed. For each
subject, there are two manifest variables Xi, Zi and two latent variables Ui, Vi. Assume
that (Ui, Vi) are bivariate normal, with mean 0, variance 1, and correlation ρ. Assume
further that (Xi, Zi) is independent of (Ui, Vi), i.e., the manifest variables are exogenous.
For ease of exposition, we take (Xi, Zi) as bivariate normal, although that is not essential.
Until further notice, we set the means to 0, the variances to 1, the correlation between Xi
and Zi to 0.40, and sample size n to 1000.
David A. Freedman and Jasjeet S. Sekhon
2. A probit response model with an endogenous regressor
There are two equations in the model. The ﬁrst is the selection equation:
Ci = 1 if a + bXi + Ui > 0, else Ci = 0.
In application, Ci = 1 means that subject i self-selects into treatment. The second equation
deﬁnes the subject’s response to treatment:
Yi = 1 if c + dZi + eCi + Vi > 0, else Yi = 0.
Notice that Yi is binary rather than continuous.
The data are the observed values of
Xi, Zi, Ci, Yi. For example, the treatment variable Ci may indicate whether subject i
graduated from college; the response Yi, whether i has a full-time job.
Endogeneity bias is likely in (2). Indeed, Ci is endogenous due to the correlation ρ
between the latent variables Ui and Vi. A two-step correction for endogeneity is sometimes
used (although it shouldn’t be).
Step 1. Estimate the probit model (1) by likelihood techniques.
Step 2. To estimate (2), ﬁt the expanded probit model
Xi, Zi, Ci) = (c + dZi + eCi + f Mi)
to the data, where
φ(a + bXi)
(a + bXi) −(1 −Ci)
φ(a + bXi)
1 −(a + bXi).
Here,  is the standard normal distribution function with density φ = ′. In application, a and b in (4) would be unknown. These parameters are replaced by maximum
likelihood estimates obtained from Step 1. The motivation for Mi is explained in Section 6
below. Identiﬁability is discussed in Section 7: according to Proposition 1, parameters are
identiﬁable unless b = d = 0.
The operating characteristics of the two-step correction was determined in a simulation
studywhichdraws500independentsamplesofsizen = 1000. Eachsamplewasconstructed
as described above. We set a = 0.50, b = 1, and ρ = 0.60. These choices create an
environment favorable to correction.
Endogeneity is moderately strong: ρ = 0.60. So there should be some advantage to
removing endogeneity bias. The dummy variable Ci is 1 with probability about 0.64, so it
has appreciable variance. Furthermore, half the variance on the right hand side of (1) can
be explained: var(bXi) = var(Ui). The correlation between the regressors is only 0.40:
making that correlation higher exposes the correction to well-known instabilities.
The sample is large: n = 1000. Regressors are exogenous by construction. Subjects
are independent and identically distributed. Somewhat arbitrarily, we set the true value of
c in the response equation (2) to −1, while d = 0.75 and e = 0.50. As it turned out, these
choices were favorable too.
Table 1 summarizes results for three kinds of estimates:
(i) raw (ignoring endogeneity);
(ii) the two-step correction;
(iii) full maximum likelihood.
For each kind of estimate and each parameter, the table reports the mean of the estimates
across the 500 repetitions. Subtracting the true value of the parameter measures the bias in
the estimator. Similarly, the SD across the repetitions, also shown in the table, measures
the likely size of the random error.
The “raw estimates” in Table 1 are obtained by ﬁtting the probit model
Xi, Zi, Ci) = (c + dZi + eCi)
to the data, simply ignoring endogeneity. Bias is quite noticeable.
Endogeneity in Probit Response Models
Table 1. Simulation results. Correcting endogeneity bias when the response is
binary probit. There are 500 repetitions. The sample size is 1000. The correlation
between latents is ρ = 0.60. The parameters in the selection equation (1) are set
at a = 0.50 and b = 1. The parameters in the response equation (2) are set at
c = −1, d = 0.75, e = 0.50. The response equation includes the endogenous
dummy Ci deﬁned by (1). The correlation between the exogenous regressors is
0.40. MLE computed by VGAM 0.7-6.
True values
Raw estimates
The two-step estimates are obtained via (3–4), with ˆa and ˆb obtained by ﬁtting (1). We
focus on d and e, as the parameters in equation (2) that may be given causal interpretations.
Without correction, ˆd averages about 0.72; with correction, 0.83. See Table 1. Correction
doubles the bias. Without correction, ˆe averages 1.33; with correction, 0.54. Correction
helps a great deal, but some bias remains.
With the two-step correction, the SD of ˆe is about 0.21. Thus, random error in the
estimates is appreciable, even with n = 1000. On the other hand, the SE across the 500
repetitions is 0.21/
500 = 0.01. The bias in ˆe cannot be explained in terms of random
error in the simulation: increasing the number of repetitions will not make any appreciable
change in the estimated biases.
Heckman also suggested the possibility of ﬁtting the full model—equations
(1) and (2)—by maximum likelihood. The full model is a “bivariate probit” or “biprobit”
model. Results are shown in the last two lines of Table 1. The MLE is essentially unbiased.
The MLE is better than the two-step correction, although random error remains a concern.
We turn to some variations on the setup described in Table 1. The simulations reported
there generated new versions of the regressors on each repetition. Freezing the regressors
makes almost no difference in the results: standard deviations would be smaller, in the third
decimal place.
The results in Table 1 depend on ρ, the correlation between the latent variables in the
selection equation and the response equation. If ρ is increased from 0.60 to 0.80, say, the
performance of the two-step correction is substantially degraded. Likewise, increasing the
correlation between the exogenous regressors degrades the performance.
When ρ = 0.80 and the correlation between the regressors is 0.60, the bias in the
two-step correction (3–4) for ˆd is about 0.15; for ˆe, about 0.20. Figure 1 plots the bias in
ˆe against ρ, with the correlation between regressors set at 0.40 or 0.60, other parameters
being ﬁxed at their values
David A. Freedman and Jasjeet S. Sekhon
Figure 1. The two-step correction. Graph of bias in ˆe against ρ, the correlation
between the latents. The light lower line sets the correlation between regressors
to 0.40; the heavy upper line sets the correlation to 0.60. Other parameters as for
Table 1. Below 0.35, the lines criss-cross.
CORRELATION BETWEEN LATENTS
BIAS IN ESTIMATED CAUSAL EFFECT
for Table 1. The wiggles in the graph reﬂect variance in the Monte Carlo (there are “only”
500 replicates). The MLE is less sensitive to increasing correlations (data not shown).
Results are also sensitive to the distribution of the exogenous regressors. As the
variance in the regressors goes down, bias goes up—in the two-step estimates and in the
MLE. Furthermore, numerical issues become acute. There is some explanation: dividing
the SD of X by 10, say, is equivalent to dividing b by 10 in equation (1); similarly for Z
and d in (2). For small values of b and d, parameters are barely identiﬁable.
Figure 2 plots the bias in ˆe against the common SD of X and Z, which is set to values
ranging from 0.1 to 1.0. (Other parameters are set as in Table 1.) The light line represents
the MLE. Some of the “bias” in the MLE is indeed small-sample bias—when the SD is 0.1,
a sample with n = 1000 is a small sample. Some of the bias, however, reﬂects a tendency
of likelihood maximizers to quit before ﬁnding the global maximum.
The heavy line represents the two-step correction. (With an SD of 0.1, data for the
two-step correction are not shown, because there are huge outliers; even the median bias is
quite changeable from one set of 500 repetitions to another, but 0.2 may be a representative
ﬁgure.) Curiously, the two-step correction is better than the MLE when the SD of the
exogenous regressors is set to 0.2 or to 0.3. This is probably due to numerical issues in
maximizing the likelihood functions.
We believe the bias in the two-step correction (Figures 1 and 2) reﬂects the operating
characteristics of the estimator, rather than operating characteristics of the software. Beyond
1.0, the bias in the MLE seems to be negligible. Beyond 1.5, the bias in the two-step
estimator for e is minimal, but d continues to be a little problematic.
As noted above, changing the scale of X is equivalent to changing b.
Similarly,
changing the scale of Z is equivalent to changing d. See equations (1) and (2). Thus, in
Figure 2, we could leave the SDs at 1, and run through a series of (b, d) pairs:
(0.1 × b0, 0.1 × d0), (0.2 × b0, 0.2 × d), . . . ,
where b0 = 1 and d0 = 0.75 were the initial choices for Table 1.
Endogeneity in Probit Response Models
Figure 2. Graph of bias in ˆe against the common SD of the regressors X and Z.
Other parameters as for Table 1. The light line represents the MLE, as computed
by VGAM 0.7-6. The heavy line represents the two-step correction.
COMMON SD OF REGRESSORS
BIAS IN ESTIMATED CAUSAL EFFECT
The number of regressors should also be considered. With a sample size of 1000,
practitioners would often use a substantial number of covariates. Increasing the number of
regressors is likely to have a negative impact on performance.
3. A probit model with endogenous sample selection
Consider next the situation where a probit model is ﬁtted to a sample, but subjects
self-select into the sample by an endogenous process. The selection equation is
Ci = 1 if a + bXi + Ui > 0, else Ci = 0.
(Selection means, into the sample.) The response equation is
Yi = 1 if c + dZi + Vi > 0, else Yi = 0.
Equation (6) is the equation of primary interest; however, Yi and Zi are observed only when
Ci = 1. Thus, the data are the observed values of (Xi, Ci) for all i, as well as (Zi, Yi)
when Ci = 1. When Ci = 0, however, Zi and Yi remain unobserved. Notice that Yi is
binary rather than continuous. Notice too that Ci is omitted from (6); indeed, when (6) can
be observed, Ci ≡1.
Fitting (6) to the observed data raises the question of endogeneity bias. Sample subjects
have relatively high values of Ui; hence, high values of Vi. (This assumes ρ > 0.) Again,
there is a proposed solution that involves two steps.
Step 1. Estimate the probit model (5) by likelihood techniques.
Step 2. Fit the expanded probit model
Xi, Zi) = (c + dZi + f Mi)
to the data on subjects i with Ci = 1. This time,
Mi = φ(a + bXi)
(a + bXi).
David A. Freedman and Jasjeet S. Sekhon
Parameters in (8) are replaced by the estimates from Step 1. As before, this twostep correction doubles the bias in ˆd. See Table 2. The MLE removes most of the bias.
However, as for Table 1, the bias in the MLE depends on the SD of the regressors. Bias
will be noticeable if the SDs are below 0.2. Some of this is small-sample bias in the MLE,
and some reﬂects difﬁculties in numerical maximization.
Increasing the sample size from 1000 to 5000 in the simulations barely changes the
averages, but reduces the SDs by a factor of about
5, as might be expected. This comment
applies both to Table 1 and to Table 2 (data not shown), but not to the MLE results in Table 2.
Increasing n would have made the STATA code prohibitively slow to run.
Table 2. Simulation results. Correcting endogeneity bias in sample selection
when the response is binary probit. There are 500 repetitions. The sample size
is 1000. The correlation between latents is ρ = 0.60. The parameters in the
selection equation (5) are set at a = 0.50 and b = 1. The parameters in the
response equation (6) are set at c = −1, d = 0.75. Response data are observed
only when Ci = 1, as determined by the selection equation. This will occur for
about 64% of the subjects. The correlation between the exogenous regressors is
0.40. MLE computed using STATA 9.2.
True values
Raw estimates
Many applications of Heckman’s method feature a continuous response variable rather
than a binary variable. Here, the two-step correction is on ﬁrmer ground, and parallel
simulations (data not shown) indicate that the correction removes most of the endogeneity
bias when the parameters are set as in Tables 1 and 2. However, residual bias is large when
the SD of the regressors is set to 0.1 and the sample size is “only” 1000; the issues resolve
when n = 10,000. The problem with n = 1000 is created by (i) large random errors in ˆb,
coupled with (ii) poorly conditioned design matrices. In more complicated situations, there
may be additional problems.
4. Numerical issues
Exploratory computations were done in several versions of MATLAB, R, and STATA.
In the end, to avoid confusion and chance capitalization, we redid the computations in a
more uniﬁed way, with R 2.7 for the raw estimates, the two-step correction; VGAM 0.7-6
for the MLE in (1–2); and STATA 9.2 for the MLE in (5–6). Why do we focus on the
behavior of R and STATA? R is widely used in the statistical community, and STATA is
almost the lingua franca of quantitative social scientists.
Let b0 and d0 be the default values of b and d, namely, 1 and 0.75. As b and d decrease
from the defaults, VGAM in R handled the maximization less and less well (Figure 2). We
Endogeneity in Probit Response Models
believe VGAM had problems computing the Hessian, even for the base case in Table 1: its
internally-generated standard errors were too small by a factor of about two, for ˆc, ˆe, ˆρ.
By way of counterpoint, STATA did somewhat better when we used it to redo the
MLE in (1–2). However, if we multiply the default b0 and d0 by 0.3 or 0.4, bias in STATA
becomes noticeable. If we multiply by 0.1 or 0.2, many runs fail to converge, and the
runs that do converge produce aberrant estimates, particularly for a multiplier of 0.1. For
multipliers of 0.2 to 0.4, the bias in ˆe is upwards in R but downwards in STATA. In Table 2,
STATA did well. However, if we scale b0 and d0 by 0.1 or 0.2, STATA has problems. In
defense of R and STATA, we can say that they produce abundant warning messages when
they get into difﬁculties.
In multi-dimensional problems, even the best numerical analysis routines ﬁnd spurious
maxima for the likelihood function. Our models present three kinds of problems: (a) ﬂat
spots on the log likelihood surface, (b) ill-conditioned maxima, where the eigenvalues of the
Hessian are radically different in size, and (c) ill-conditioned saddle points with one small
positive eigenvalue and several large negative eigenvalues. The maximizers in VGAM and
STATA simply give up before ﬁnding anything like the maximum of the likelihood surface.
This is a major source of the biases reported above.
The model deﬁned by (1–2) is a harder challenge for maximum likelihood than (5–
6), due to the extra parameter e. Our computations suggest that most of the difﬁculty
lies in the joint estimation of three parameters, c, e, ρ. Indeed, we can ﬁx a, b, d at the
default values for Table 1, and maximize the likelihood over the remaining three parameters
c, e, ρ. VGAM and STATA still have convergence issues. The problems are the same as
with 6 parameters. For example, we found a troublesome sample where the Hessian of the
log likelihood had eigenvalues 4.7, −1253.6, −2636.9. (We parameterize the correlation
between the latents by log(1 + ρ) −log(1 −ρ) rather than ρ, since that is how binom2.rho
in VGAM does things.)
One of us (JSS) has an improved likelihood maximizer called GENOUD. See
 
GENOUD seems to do much better at the maximization, and its internally-generated SEs
are reasonably good. Results for GENOUD and STATA not reported here are available at
the URL above, along with the VGAM SEs.
5. Implications for practice
There are two main conclusions from the simulations and the analytic results.
(i) Under ordinary circumstances, the two-step correction should not be used in
probit response models. In some cases, the correction will reduce bias, but in
many other cases, the correction will increase bias.
(ii) If the bivariate probit model is used, special care should be taken with the numerics. Conventional likelihood maximization algorithms produce estimates that
are far away from the MLE. Even if the MLE has good operating characteristics,
the “MLE” found by the software package may not. Results from VGAM 0.7-6
should be treated with caution. Results from STATA 9.2 may be questionable for
various combinations of parameters.
The models analyzed here are very simple, with one covariate in each of (1–2) and (5–6).
In real examples, the number of covariates may be quite large, and numerical behavior will
be correspondingly more problematic.
Of course, there is a question more salient than the numerics: what is it that justiﬁes
probit models and like as descriptions of behavior? For additional discussion, see Freedman
 , which has further cites to the literature on this point.
David A. Freedman and Jasjeet S. Sekhon
6. Motivating the estimator
Consider (1–2). We can represent Vi as ρUi +
1 −ρ2Wi, where Wi is an N(0, 1)
random variable, independent of Ui. Then
 Xi = x, Ci = 1
 Ui > −a −bxi
 Ui > −a −bxi
(a + bxi)
= ρ φ(a + bxi)
(a + bxi)
because P {Ui > −a −bxi} = P{Ui < a + bxi} = (a + bxi). Likewise,
 Xi = x, Ci = 0
φ(a + bxi)
1 −(a + bxi).
In (2), therefore, E{Vi −ρMi
 Xi, Ci} = 0. If (2) were a linear regression equation,
then OLS estimates would be unbiased, the coefﬁcient of Mi being nearly ρ. (These remarks
take a and b as known, with the variance of the error term in the linear regression normalized
to 1.) However, (2) is not a linear regression equation: (2) is a probit model. That is the
source of the problem.
7. Identiﬁability
Identiﬁability means that parameters are determined by the joint distribution of the
observables; parameters that are not identiﬁable cannot be estimated. In the model deﬁned
by (1–2), the parameters are a, b, c, d, e and the correlation ρ between the latents; the
observables are Xi, Zi, Ci, and Yi. In the model deﬁned by (5–6), the parameters are
a, b, c, d and the correlation ρ between the latents; observables are Xi, Ci, ˜Zi, ˜Yi, where
˜Zi = Zi and ˜Yi = Yi when Ci = 1, while ˜Zi = ˜Yi = M when Ci = 0. Here, M is just a
special symbol that denotes “missing.”
Results are summarized as Propositions 1 and 2. The statements involve the sign of
d, which is +1 if d > 0, 0 if d = 0, and −1 if d < 0. Since subjects are independent and
identically distributed, only i = 1 need be considered. The variables (X1, Z1) are taken
as bivariate normal, with a correlation strictly between −1 and +1. This assumption is
discussed below.
Proposition 1. Consider the model deﬁned by (1–2). The parameters a and b in (1)
are identiﬁable, and the sign of d in (2) is identiﬁable. If b ̸= 0, the parameters c, d, e, ρ
in (2) are identiﬁable. If b = 0 but d ̸= 0, the parameters c, d, e, ρ are still identiﬁable.
However, if b = d = 0, the remaining parameters c, e, ρ are not identiﬁable.
Proposition 2. Consider the model deﬁned by (5–6). The parameters a and b in (5)
are identiﬁable, and the sign of d in (6) is identiﬁable. If b ̸= 0, the parameters c, d, ρ
in (6) are identiﬁable. If b = 0 but d ̸= 0, the parameters c, d, ρ are still identiﬁable.
However, if b = d = 0, the remaining parameters c, ρ are not identiﬁable.
Proof of Proposition 1. Clearly, the joint distribution of C1 and X1 determines a and
b, so we may consider these as given. The distributions of X1 and Z1 are determined (this
Endogeneity in Probit Response Models
is not so helpful). We can take the conditional distribution of Y1 given X1 = x and Z1 = z
as known. In other words, suppose (U, V ) are bivariate normal with mean 0, variance 1
and correlation ρ.
The joint distribution of the observables determines a, b and two functions ψ0, ψ1 of
ψ0(x, z) = P(a + bx + U < 0 & c + dz + V > 0),
ψ1(x, z) = P(a + bx + U > 0 & c + dz + e + V > 0).
There is no additional information about the parameters.
Fix x at any convenient value, and consider z > 0. Then z →ψ0(x, z) is strictly
decreasing, constant, or strictly increasing, according as d < 0, d = 0, or d > 0. The sign
of d is therefore determined. The rest of proof, alas, consists of a series of cases.
The case b ̸= 0 and d > 0. Let u = −a −bx, v = −z, ξ = U, and ζ = (V + c)/d.
Then (ξ, ζ) are bivariate normal, with unknown correlation ρ. We know ξ has mean 0 and
variance 1. The mean and variance of ζ are unknown, being c/d and 1/d2, respectively.
P(ξ < u & ζ > v)
is known for all (u, v). Does this determine ρ, c, d? Plainly so, because (12) determines
the joint distribution of ξ, ζ. We can then compute ρ, d = 1/√var(ζ), and c = dE(ζ).
Finally, ψ1 in (11) determines e. This completes the argument for the case b ̸= 0 and d > 0.
The case b ̸= 0 and d < 0 is the same, except that d = −1/√var(ζ).
The case b ̸= 0 and d = 0. Here, we know
P(U < u & c + V > 0) for all u.
Let u →∞: the marginal distribution of V determines c. Furthermore, from (13), we can
compute P (V > −c | U = u) for all u. Given U = u, we know that V is distributed as
1 −ρ2W, where W is N(0, 1). If ρ = ±1, then
 U = u) = 1 if ρu > −c
= 0 if ρu < −c
If −1 < ρ < 1, then
W > −c + ρu
So we can determine whether ρ = ±1; and if so, which sign is right. Suppose −1 < ρ < 1.
Then (14) determines (c + ρu)/
1 −ρ2. Differentiate with respect u to see that (14)
determines ρ/
1 −ρ2. This is a 1–1 function of ρ. Thus, ρ can be determined, and then
c; ﬁnally, e is obtained from ψ1 in (11). This completes the argument for the case b ̸= 0
and d = 0.
The case b = 0 and d > 0.
As above, let W be independent of U and N(0, 1);
represent V as ρU +
1 −ρ2W. Let G = {U < −a
. From ψ0 and a, we compute
V > −c −dz
1 −ρ2W > −c −dz
David A. Freedman and Jasjeet S. Sekhon
Write Ua for U conditioned so that U < −a. The right hand side of (15), as a function
of z, determines the distribution function of the sum of three terms: two independent random
variables, Ua and
1 −ρ2W/d, where W is standard normal, plus the constant c/d. This
distribution is therefore known, although it depends on the three unknowns, c, d, ρ.
Write & for the log Laplace transform of Ua. This is a known function. Now compute
the log Laplace transform of the distribution in (15). This is
Again, this function is known, although c, d, ρ are unknown. Consider the expansion of
(16) as a power series near 0, of the form κ1t + κ2t2/2! + κ3t3/3! + · · · · The κ’s are
the cumulants or semi-invariants of the distribution in (15). These are known quantities
because the function in (16) is known: κ1 is the mean of the distribution given by (15),
while κ2 is the variance and κ3 is the central third moment.
Of course, &′(0) = E(Ua) = −φ(−a)/(−a). Thus, κ1 = −φ(−a)/(−a) + c/d,
which determines c/d. Next, &′′(0) = var(Ua), so κ2 = (ρ/d)2var(Ua) + (1 −ρ2)/d2 is
determined. Finally, κ3 = &′′′(0) is the third central moment of Ua. Since Ua has a skewed
distribution, &′′′(0) ̸= 0. We can compute (ρ/d)3 from κ3, and then ρ/d. Next, we get
1/d2 from κ2, and then 1/d. (We are looking at the case d > 0.) Finally, c comes from κ1.
Thus, c, d, ρ are determined, and e comes from ψ1 in (11). This completes the argument
for the case b = 0 and d > 0.
The case b = 0 and d < 0 follows by the same argument.
The case b = d = 0. The three remaining parameters, c, e, and ρ, are not identiﬁable.
For simplicity, take a = 0, although this is not essential. Suppose
P(U < 0 & V > −c) = α
is given, with 0 < α < 1/2. Likewise,
P(U > 0 & V > −c −e) = β
is given, with 0 < β < 1/2. The joint distribution of the observables contains no further
information about the remaining parameters c, e, ρ. Choose any particular ρ with −1 ≤
ρ ≤1. Choose c so that (17) holds and e so (18) holds. The upshot: there are inﬁnitely
many c, e, ρ triplets yielding the same joint distribution for the observables. This completes
the argument for the case b = d = 0, and so for Proposition 1.
Proof of Proposition 2. Here, we know the joint distribution of (X1, C1), which
determines a, b. We also know the joint distribution of (X1, Z1, Y1) given C1 = 1; we do
not know this joint distribution given C1 = 0. As in (11), suppose (U, V ) are bivariate
normal with mean 0, variance 1 and correlation ρ. The joint distributions of the observables
determine a, b and the function
ψ1(x, z) = P(a + bx + U > 0 & c + dz + V > 0).
There is no other information in the system; in particular, we do not know the analog of ψ0.
Most of the argument is the same as before, or even a little easier. We consider in detail
only one case.
The case b = d = 0. The two remaining parameters, c, ρ are not identiﬁable. Again,
take a = 0. Fix any α with 0 < α < 1/2. Suppose
P(U > 0 & V > −c) = α
Endogeneity in Probit Response Models
is given. There is no other information to be had about c, ρ. Fix any ρ with −1 ≤ρ ≤1
and solve (20) for c. There are inﬁnitely many c, ρ pairs giving the same joint distribution
for the observables when b = d = 0. This completes our discussion of Proposition 2.
(i) The random variable Ua was deﬁned in the course of proving Proposition 1. If
desired, the moments of Ua can be obtained explicitly in terms of φ and , using repeated
integration by parts.
(ii) The Laplace transform of Ua is easily obtained by completing the square, and
2t2 (−a −t)
The third derivative of the log Laplace transform can be computed from (21), but it’s painful.
(iii) The argument for the case b = 0 and d > 0 in Proposition 1 is somewhat intricate,
but it actually covers all values of b, whether zero or non-zero. The argument shows that
for any particular real α, the values of c, d, ρ are determined by the number P(α + U < 0)
and the function
z →P(α + U < 0 & c + dz + V > 0)
(iv) Likewise, the argument for the case b ̸= 0 and d = 0 proves more. If we know
P (U < u) and P(U < u & γ + V > 0) for all real u, that determines γ and ρ.
(v) In (17), for example, if α = 1/2, then ρ = −1; but c can be anywhere in the range
(vi) The propositions can easily be extended to cover vector-valued exogenous variables.
(vii) Our proof of the propositions really does depend on the assumption of an imperfect
correlation between Xi and Zi. We hope to consider elsewhere the case where Zi ≡Xi.
The assumption of normality is not material; it is enough if the joint distributions have full
support, although positive densities are probably easier to think about.
(viii) The assumption of bivariate normality for the latent variables is critical. If this
is wrong, estimates are likely to be inconsistent.
(ix) Suppose (U, V ) are bivariate normal with correlation ρ, and −1 < ρ < 1. Then
ρ →P(U > 0 & V > 0)
is strictly monotone. This is Slepian’s theorem: see Tong . If the means are 0 and
the variances are 1, numerical calculations suggest this function is convex on (−1, 0) and
concave on (0, 1).
8. Some relevant literature
Cumulants are discussed by Rao . The ratio φ/ in (8) is usually
called the “inverse Mills ratio,” in reference to Mills — although Mills tabulates
[1 −(x)]/φ(x) for x ≥0. Heckman proposes the use of Mi to correct
for endogeneity and selection bias in the linear case, with a very clear explanation of the
issues. He also describes potential use of the MLE. Rivers and Vuong propose an
interesting alternative to the Heckman estimator. Their estimator (perhaps confusingly) is
also called a two-step procedure. It seems most relevant when the endogenous variable is
continuous; ours is binary.
David A. Freedman and Jasjeet S. Sekhon
For other estimation strategies and discussion, seeAngrist . Bhattacharya, Goldman, and McCaffrey discuss several “two-step” algorithms, including a popular
IVLS estimator that turns out to be inconsistent; they do not seem to consider the particular two-step estimator of concern in our paper. Also see Lee , Rivers and Vuong
 . Muthen discusses identiﬁability in a model with latent causal variables. The
VGAM manual notes difﬁculties in computing standard errors. According to
Stata , its maximum likelihood routine “provides consistent, asymptotically efﬁcient
estimates for all the parameters in [the] models.”
Van de Ven and Van Praag found little difference between the MLE and the
two-step correction; the difference doubtless depends on the model under consideration.
Instabilities in the two-step correction are described by Winship and Mare , Copas
and Li , and Briggs , among others. For additional citations, see Dunning
and Freedman . Ono uses the two-step correction with probit response in a
study of the Japanese labor market; X and Z are multi-dimensional. The sample size is
10,000, but only 300 subjects select into the treatment condition. Bushway, Johnson and
Slocum describe many over-enthusiastic applications of the two-step correction in
the criminology literature: binary response variables are among the least of the sins.
We do not suggest that ﬁnding the true maximum of the likelihood function guarantees
the goodness of the estimator, because there are situations where the MLE performs rather
badly. Freedman has a brief review of the literature on this topic. However, we
would suggest that spurious maxima are apt to perform even less well, particularly with the
sort of models considered here.