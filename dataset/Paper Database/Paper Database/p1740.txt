PlaNet - Photo Geolocation with Convolutional Neural Networks
Tobias Weyand
 
Ilya Kostrikov
RWTH Aachen University
 
James Philbin
 
Is it possible to build a system to determine the location where a photo was taken using just its pixels? In general, the problem seems exceptionally difﬁcult: it is trivial to
construct situations where no location can be inferred. Yet
images often contain informative cues such as landmarks,
weather patterns, vegetation, road markings, and architectural details, which in combination may allow one to determine an approximate location and occasionally an exact location. Websites such as GeoGuessr and View from
your Window suggest that humans are relatively good at
integrating these cues to geolocate images, especially enmasse. In computer vision, the photo geolocation problem
is usually approached using image retrieval methods. In
contrast, we pose the problem as one of classiﬁcation by
subdividing the surface of the earth into thousands of multiscale geographic cells, and train a deep network using millions of geotagged images. While previous approaches only
recognize landmarks or perform approximate matching using global image descriptors, our model is able to use and
integrate multiple visible cues. We show that the resulting model, called PlaNet, outperforms previous approaches
and even attains superhuman levels of accuracy in some
cases. Moreover, we extend our model to photo albums by
combining it with a long short-term memory (LSTM) architecture. By learning to exploit temporal coherence to geolocate uncertain photos, we demonstrate that this model
achieves a 50% performance improvement over the singleimage model.
1. Introduction
Photo geolocation is an extremely challenging task since
many photos offer only a few cues about their location and
these cues can often be ambiguous. For instance, an image of a typical beach scene could be taken on many coasts
across the world. Even when landmarks are present there
can still be ambiguity: a photo of the Rialto Bridge could
be taken either at its original location in Venice, Italy, or in
Las Vegas which has a replica of the bridge! In the absence
Photo CC-BY-NC by stevekc
Photo CC-BY-NC by edwin.11
Photo CC-BY-NC by jonathanfh
Figure 1. Given a query photo (left), PlaNet outputs a probability
distribution over the surface of the earth (right). Viewing the task
as a classiﬁcation problem allows PlaNet to express its uncertainty
about a photo. While the Eiffel Tower (a) is conﬁdently assigned
to Paris, the model believes that the fjord photo (b) could have
been taken in either New Zealand or Norway. For the beach photo
(c), PlaNet assigns the highest probability to southern California
(correct), but some probability mass is also assigned to places with
similar beaches, like Mexico and the Mediterranean. (For visualization purposes we use a model with a much lower spatial resolution than our full model.)
of obvious and discriminative landmarks, humans can fall
back on their world knowledge and use multiple cues to infer the location of a photo. For example, the language of
street signs or the driving direction of cars can help narrow
down possible locations. Traditional computer vision algorithms typically lack this kind of world knowledge, relying
on the features provided to them during training.
Most previous work has therefore focused on covering restricted subsets of the problem, like landmark build-
 
ings , cities where street view imagery is available , or places where coverage of internet photos is dense enough to allow building a structure-frommotion reconstruction that a query photo can be matched
against . In contrast, our goal is to localize any type
of photo taken at any location. To our knowledge, very few
other works have addressed this task .
We treat the task of geolocation as a classiﬁcation problem and subdivide the surface of the earth into a set of geographical cells which make up the target classes. We then
train a convolutional neural network (CNN) using millions of geotagged images. At inference time, our model
outputs a discrete probability distribution over the earth, assigning each geographical cell a likelihood that the input
photo was taken inside it.
The resulting model, which we call PlaNet, is capable of localizing a large variety of photos. Besides landmark buildings and street scenes, PlaNet can often predict
the location of nature scenes like mountains, waterfalls or
beaches with surprising accuracy. In cases of ambiguity, it
will often output a distribution with multiple modes corresponding to plausible locations (Fig. 1). PlaNet outperforms
the Im2GPS approach that shares a similar goal.
A small-scale experiment shows that PlaNet even reaches
superhuman performance at the task of geolocating street
view scenes. Moreover, we show that the features learned
by PlaNet can be used for image retrieval and achieve stateof-the-art results on the INRIA Holidays dataset .
Sometimes an image provides no useful cues: this is often the case with portraits, or photos of pets and common
foods. However, we could still make predictions about the
location of such photos if we also consider photos taken
at roughly the same time either before or after the query.
To this end, we have extended PlaNet to work on groups
of photos by combining it with an LSTM approach. This
method yields a 50% improvement over the single-image
model when applied to photo albums. The reason for this
improvement is that LSTMs learn to exploit temporal coherence in albums to correctly geolocate even those photos
that the single-image model would fail to annotate conﬁdently. For example, a photo of a croissant could be taken
anywhere in the world, but if it is in the same album as a
photo of the Eiffel Tower, the LSTM model will use this
cue to geolocate it to Paris.
2. Related Work
Given a query photo, Im2GPS retrieves similar
images from millions of geotagged Flickr photos and assigns the location of the closest match to the query. Image
distances are computed using a combination of global image descriptors. Im2GPS shows that with enough data, even
this simple approach can achieve surprisingly good results.
We discuss Im2GPS in detail in Sec. 3.
Because photo coverage in rural areas is sparse, 
make additional use of satellite aerial imagery. use
CNNs to learn a joint embedding for ground and aerial images and localize a query image by matching it against a
database of aerial images. take a similar approach and
use a CNN to transform ground-level features to the feature
space of aerial images.
Image retrieval based on local features, bags-of-visualwords (BoVWs) and inverted indices has been
shown to be more accurate than global descriptors at matching buildings, but requires more space and lacks the invariance to match e.g. natural scenes or articulated objects.
Most local feature based approaches therefore focus on localization within cities, either based on photos from photo
sharing websites or street view . Skyline2GPS also uses street view data, but takes
a unique approach that segments the skyline out of an image captured by an upward-facing camera and matches it
against a 3D model of the city.
While matching against geotagged images can provide
the rough location of a query photo, some applications require the exact 6-dof camera pose.
Pose estimation approaches achieve this goal using 3D models reconstructed
using structure-from-motion from internet photos. A query
image is localized by establishing correspondences between
its interest points and the points in the 3D model and solving the resulting perspective-n-point (PnP) problem to obtain the camera parameters . Because matching
the query descriptors against the 3D model descriptors is expensive, some approaches combine this technique with efﬁcient image retrieval based on inverted indices .
Instead of matching against a ﬂat collection of photos,
landmark recognition systems build a
database of landmark buildings by clustering images from
internet photo collections. The landmarks in a query image
are recognized by retrieving matching database images and
returning the landmark associated with them. Instead of using image retrieval, use SVMs trained on BoVW of
landmark clusters to decide which landmark is shown in a
query image. Instead of operating on image clusters, ,
train one exemplar SVM for each image in a dataset of street
view images.
A task related to image geolocation is scene recognition,
for which the SUN database is an established benchmark. The database consists of 131k images categorized
into 908 scene categories such as “mountain“, “cathedral“
or “staircase“. The SUN survey paper shows that Overfeat , a CNN trained on ImageNet images, consistently outperforms other approaches, including global descriptors like GIST and local descriptors like SIFT, motivating our use of CNNs for image geolocation.
In Sec. 4, we extend PlaNet to geolocate sequences of
images using LSTMs. Several previous approaches have
also realized the potential of exploiting temporal coherence
to geolocate images. ﬁrst cluster the photo collection into landmarks and then learn to predict the sequence
of landmarks in a query photo sequence. While train
a Hidden Markov Model (HMM) on a dataset of photo albums to learn popular tourist routes, train a structured
SVM that uses temporal information as an additional feature. Images2GPS also trains an HMM, but instead of
landmarks, its classes are a set of geographical cells partitioning the surface of the earth. This is similar to our approach, however we use a much ﬁner discretization.
Instead of performing geolocation, train a CNN
on a large collection of geotagged Flickr photos to predict geographical attributes like “population“, “elevation“
or “household income“. cluster street view imagery to
discover latent scene types that are characteristic for certain
geographical areas and analyze how these types correlate
with geopolitical boundaries.
In summary, most previous approaches to photo geolocation are restricted to urban areas which are densely covered
by street view imagery and tourist photos. Exceptions are
Im2GPS and , which make additional
use of satellite imagery. Prior work has shown that CNNs
are well-suited for scene classiﬁcation and geographical attribute prediction , but to our knowledge ours is
the ﬁrst method that directly takes a classiﬁcation approach
to geolocation using CNNs.
3. Image Geolocation with CNNs
We pose the task of image geolocation as a classiﬁcation problem. For this, we subdivide the earth into a set
of geographical cells. The input to our CNN are the image
pixels and the target output is a one-hot vector encoding the
cell containing the geotag of the image. Given a test image,
the output of this model is a probability distribution over
the world. The advantage of this formulation over a regression from pixels to latitude/longitude coordinates is that the
model can express its uncertainty about an image by assigning each cell a conﬁdence that the image was taken there. In
contrast, a regression model would be forced to pinpoint a
single location and would have no natural way of expressing
uncertainty about its prediction, especially in the presence
of multi-modal answers (as are expected in this task).
Adaptive partitioning using S2 Cells. We use Google’s
open source S2 geometry library12 to partition the earth’s
surface into a set of non-overlapping cells that deﬁne the
classes of our model. The S2 library deﬁnes a hierarchical
partitioning of the surface of a sphere by projecting the surfaces of an enclosing cube on it. The six sides of the cube
1 
2 jwNVHRPZTTDzXXn6Q/view
Figure 3. S2 cell quantization in 2D. The sides of the square are
subdivided recursively and projected onto the circle.
are subdivided hierarchically by six quad-trees. A node in
a quad-tree deﬁnes a region on the surface of the sphere
called an S2 cell. Fig. 3 illustrates this in 2D. We chose
this subdivision scheme over a simple subdivision of latitude/longitude coordinates, because (i) lat/lon regions get
elongated near the poles while S2 cells keep a close-toquadratic shape, and (ii) S2 cells have mostly uniform size
(the ratio between the largest and smallest S2 cell is 2.08).
A naive approach to deﬁne a tiling of the earth would be
to use all S2 cells at a certain ﬁxed depth in the hierarchy,
resulting in a set of roughly equally sized cells (see Fig. 1).
However, this would produce a very imbalanced class distribution since the geographical distribution of photos has
strong peaks in densely populated areas. We therefore perform adaptive subdivision based on the photos’ geotags:
starting at the roots, we recursively descend each quad-tree
and subdivide cells until no cell contains more than a certain
ﬁxed number t1 of photos. This way, sparsely populated areas are covered by larger cells and densely populated areas
are covered by ﬁner cells. Then, we discard all cells containing less than a minimum of t2 photos. Therefore, PlaNet
does not cover areas where photos are very unlikely to be
taken, such as oceans or poles. We remove all images from
the training set that are in any of the discarded cells. This
adaptive tiling has several advantages over a uniform one:
(i) training classes are more balanced, (ii) it makes effective use of the parameter space because more model capacity is spent on densely populated areas, (iii) the model can
reach up to street-level accuracy in city areas where cells
are small. Fig. 2 shows the S2 partitioning for our dataset.
CNN training. We train a CNN based on the Inception architecture with batch normalization . The SoftMax
output layer has one output for each S2 cells in the partitioning. We set the target output value to 1.0 at the index of
the S2 cell the respective training image belongs to and all
other outputs to 0.0. We initialize the weights of our model
with random values and use the AdaGrad stochastic
gradient descent with a learning rate of 0.045.
Our dataset consists of 126M photos with Exif geolocations mined from all over the web. We applied very little
ﬁltering, only excluding images that are non-photos (like
diagrams, clip-art, etc.) and porn. Our dataset is therefore
extremely noisy, including indoor photos, portraits, photos
of pets, food, products and other photos not indicative of
Figure 2. Left: Adaptive partitioning of the world into 26,263 S2 cells. Right: Detail views of Great Britain and Ireland and the San
Francisco bay area.
location. Moreover, the Exif geolocations may be incorrect
by several hundred meters due to noise. We split the dataset
into 91M training images and 34M validation images.
For the adaptive S2 cell partitioning (Sec. 3) we set
t1 = 10, 000 and t2 = 50. The resulting partitioning consists of 26, 263 S2 cells (Fig. 2). Our Inception model has a
total of 97,321,048 parameters. We train the model for 2.5
months on 200 CPU cores using the DistBelief framework
 until the accuracy on the validation set converges. The
long training time is due to the large variety of the training
data and the large number of classes.
We ensure that none of the test sets we use in this paper have any (near-) duplicate images in the training set.
For this, we use a CNN trained on near-duplicate images
to compute a binary embedding for each training and test
image and then remove test images whose Hamming distance to a training image is below an aggressively chosen
threshold.
Geolocation accuracy. To quantitatively measure the localization accuracy of the model, we collected a dataset of
2.3M geotagged Flickr photos from across the world. Other
than selecting geotagged images with 1 to 5 textual tags, we
did not apply any ﬁltering. Therefore, most of the images
have little to no cues about their location.
To compute localization error, we run inference and
compute the distance between the center of the predicted
S2 cell to the original location of the photo. We note that
this error measure is pessimistic, because even if the ground
truth location is within the predicted cell, the error can still
be large depending on the cell size. Fig. 4 shows what fraction of this dataset was localized with a certain geographical
distance of the ground truth locations. The blue curve shows
the performance for the most conﬁdent prediction, and the
other curves show the performance for the best of the top-
{2,3,4,5} predictions per image. Following , we added
Percentage of Flickr Dataset
Geolocation Error (log scale) [km]
Earth diameter
top-1 prediction
best of top-2 predictions
best of top-3 predictions
best of top-4 predictions
best of top-5 predictions
Figure 4. Geolocation accuracy of the top-k most conﬁdent predictions on 2.3M Flickr photos. (Lower right is best.)
approximate geographical scales of streets, cities, regions,
countries and continents. Despite the difﬁculty of the data,
PlaNet is able to localize 3.6% of the images at street-level
accuracy and 10.1% at city-level accuracy. 28.4% of the
photos are correctly localized at country level and 48.0% at
continent level. When considering the best of the top-5 predictions, the model localizes roughly twice as many images
correctly at street, city, region and country level.
Qualitative Results.
An important advantage of our
localization-as-classiﬁcation paradigm is that the model
output is a probability distribution over the globe. This way,
even if an image can not be conﬁdently localized, the model
outputs conﬁdences for possible locations. To illustrate this,
we trained a smaller model using only S2 cells at level 4 in
the S2 hierarchy, resulting in a total of only 354 S2 cells.
Fig. 1 shows the predictions of this model for test images
with different levels of geographical ambiguity.
Fig. 5 shows examples of the different types of images
Figure 5. Examples of images PlaNet localizes correctly.
model is capable of localizing photos of famous landmarks (top
row), but often yields surprisingly accurate results for images with
more subtle geographical cues. The model learns to recognize
locally typical landscapes, objects, architectural styles and even
plants and animals.
Figure 6. Examples of incorrectly localized images.
Im2GPS (orig) 
Im2GPS (new) 
8.4% 24.5%
Table 1. Comparison of PlaNet with Im2GPS. Percentages are the
fraction of images from the Im2GPS test set that were localized
within the given radius. (Numbers for the original Im2GPS are
approximate as they were extracted from a plot in the paper.)
PlaNet can localize. landmarks, which can also be recognized by landmark recognition engines , PlaNet
can often correctly localize street scenes, landscapes, buildings of characteristic architecture, locally typical objects
like red phone booths, and even some plants and animals.
Fig. 6 shows some failure modes. Misclassiﬁcations often
occur due to ambiguity, e.g., because certain landscapes or
objects occur in multiple places, or are more typical for a
certain place than the one the photo was taken (e.g., the kind
of car in the ﬁrst image is most typically found in Cuba).
To give a visual impression of the representations PlaNet
has learned for individual S2 cells, Fig. 7 shows the test images that the model assigns to a given cell with the highest
conﬁdence. The model learns a very diverse representation
of a single cell, containing the different landmarks, landscapes, or animals that a typical for a speciﬁc region.
Comparison to Im2GPS. As discussed in Sec. 2, most
image-based localization approaches focus on photos taken
Figure 7. The top-5 most conﬁdent images from the Flickr dataset
for the S2 cells on the left, showing the diverse visual representation of places that PlaNet learns.
inside cities. One of the few approaches that, like ours, aims
at geolocating arbitrary photos is Im2GPS . However, instead of classiﬁcation, Im2GPS is based on nearest
neighbor matching. The original Im2GPS approach 
matches the query image against a database of 6.5M Flickr
images and returns the geolocation of the closest matching image.
Images are represented by a combination of
six different global image descriptors. A recent extension
of Im2GPS uses both an improved image representation and a more sophisticated localization technique. It
estimates a per-pixel probability of being “ground“, “vertical“, “sky“, or “porous“ and computes color and texture
histograms for each of these classes. Additionally, bag-ofvisual-word vectors of length 1k and 50k based on SIFT
features are computed for each image. The geolocation of
a query is estimated by retrieving nearest neighbors, geoclustering them with mean shift, training 1-vs-all SVMs for
each resulting cluster, and ﬁnally picking the average GPS
coordinate of the cluster whose SVM gives the query image
the highest positive score.
We evaluate PlaNet on the Im2GPS test dataset that
consists of 237 geotagged photos from Flickr, curated such
that most photos contain at least a few geographical cues.
Tab. 1 compares the performance of PlaNet and both ver-
Percentage of GeoGuessr Questions
Geolocation Error (log scale) [km]
Earth diameter
Figure 8. Geolocation error of PlaNet vs. humans.
sions of Im2GPS. The new version is a signiﬁcant improvement over the old one. However, PlaNet outperforms even
the new version with a considerable margin. In particular, PlaNet localizes 236% more images accurately at street
level. The gap narrows at coarser scales, but even at country
level PlaNet still localizes 51% more images accurately.
A caveat of our evaluation is that PlaNet was trained on
14x more data than Im2GPS uses, which certainly gives
PlaNet an advantage. However, note that because Im2GPS
performs a nearest neighbor search, its runtime grows with
the number of images while CNN evaluation speed is independent of the amount of training data. Since the Im2GPS
feature vectors have a dimensionality of 100,000, Im2GPS
would require 8.5GB to represent our corpus of 91M training examples (assuming one byte per descriptor dimension,
not counting the space required for a search index). In contrast, our model uses only 377 MB, which even ﬁts into the
memory of a smartphone.
Comparison to human performance.
To ﬁnd out how
PlaNet compares with human intuition, we let it compete against 10 well-traveled human subjects in a game of
Geoguessr (www.geoguessr.com). Geoguessr presents the
player with a random street view panorama (sampled from
all street view panoramas across the world) and asks them
to place a marker on a map at the location the panorama
was captured. Players are allowed to pan and zoom in the
panorama, but may not navigate to adjacent panoramas. The
map is zoomable, so the location can be speciﬁed as precisely as the player wants. For this experiment, we used the
game’s “challenge mode” where two players are shown the
same set of 5 panoramas. We entered the PlaNet guesses
manually by taking a screenshot of the view presented by
the game, running inference on it and entering the center
of the highest conﬁdence S2 cell as the guess of the PlaNet
player. For a fair comparison, we did not allow the human
subjects to pan and zoom, so they did not use more information than we gave to the model. For each subject, we used a
different set of panoramas, so humans and PlaNet played a
total of 50 different rounds.
In total, PlaNet won 28 of the 50 rounds with a median
localization error of 1131.7 km, while the median human
localization error was 2320.75 km. Fig. 8 shows what percentage of panoramas were localized within which distance
by humans and PlaNet respectively. Neither humans nor
PlaNet were able to localize photos below street or city
level, showing that this task was even harder than the Flickr
dataset and the Im2GPS dataset. Fig. 9 shows some example panoramas from the game together with the guessed
locations. Most panoramas were taken in rural areas containing little to no geographical cues.
When asked what cues they used, human subjects said
they looked for any type of signs, the types of vegetation,
the architectural style, the color of lane markings and the
direction of trafﬁc on the street. Furthermore, humans knew
that street view is not available in certain countries such as
China allowing them to further narrow down their guesses.
One would expect that these cues, especially street signs,
together with world knowledge and common sense should
give humans an unfair advantage over PlaNet, which was
trained solely on image pixels and geolocations. Yet, PlaNet
was able to outperform humans by a considerable margin. For example, PlaNet localized 17 panoramas at country granularity (750 km) while humans only localized 11
panoramas within this radius. We think PlaNet has an advantage over humans because it has seen many more places
than any human can ever visit and has learned subtle cues of
different scenes that are even hard for a well-traveled human
to distinguish.
Features for image retrieval. A recent study showed
that the activations of Overfeat , a CNN trained on ImageNet can serve as powerful features for several computer vision tasks, including image retrieval. Since PlaNet
was trained for location recognition, its features should be
well-suited for image retrieval of tourist photos. To test
this, we evaluate the PlaNet features on the INRIA Holidays dataset , consisting of 1,491 personal holiday photos, including landmarks, cities and natural scenes.
extract image embeddings from the ﬁnal layer below the
SoftMax layer and rank images by
the Euclidean distance between their embedding vectors.
As can be seen in Tab. 2, the PlaNet features outperform
the Overfeat features. This is expected since our training
data is more similar to the photos from the Holidays dataset
than ImageNet. The same observation was made by 
who found that re-training on a landmark dataset improves
retrieval performance of CNN features compared to those
from a model trained on ImageNet. Their features even outperform ours, which is likely because use a carefully
crafted landmark dataset for re-training while we applied
Figure 9. Top: GeoGuessr panorama, Bottom: Ground truth location (yellow), human guess (green), PlaNet guess (blue).
Holidays mAP
Hamming Embedding
Fine Vocabulary
Overfeat+aug+ss
AlexNet+LM Retraining
PlaNet (this work)
PlaNet+aug+ss
Table 2. Image retrieval mAP using PlaNet features compared to
other methods.
Im2GPS (new)
37.4 3375.3
Table 3. Median localization error (km) by image category on the
Im2GPS test set. Manmade Landmark are landmark buildings like
the Eiffel Tower, Natural Landmark are geographical features like
the Grand Canyon, City Scene and Natural Scene are photos taken
in cities or in nature not showing any distinctive landmarks, and
Animal are photos of individual animals.
only minimal ﬁltering. Using the spatial search and augmentation techniques described in , PlaNet even outperforms state-of-the-art local feature based image retrieval
approaches on the Holidays dataset. We note that the Euclidean distance between these image embeddings is not
necessarily meaningful as PlaNet was trained for classiﬁcation. We expect Euclidean embeddings trained for image
retrieval using a triplet loss to deliver even higher mAP.
Model analysis. For a deeper analysis of PlaNet’s performance we manually classiﬁed the images of the Im2GPS
test set into different scene types. Tab. 3 shows the median
per-category error of PlaNet and Im2GPS. The results show
that PlaNet’s location discretization hurts its accuracy when
pinpointing the locations of landmarks. However, PlaNet’s
clear strength is scenes, especially city scenes, which give
it the overall advantage.
To analyze which parts of the input image are most important for the classiﬁer’s decision, we employ a method
introduced by Zeiler et al. . We plot an activation map
where the value of each pixel is the classiﬁer’s conﬁdence
in the ground truth geolocation if the corresponding part of
the image is occluded by a gray box (Fig. 10).
4. Sequence Geolocation with LSTMs
While PlaNet is capable of localizing a large variety
of images, many images are ambiguous or do not contain enough information that would allow to localize them.
However we can exploit the fact that photos naturally occur
in sequences, e.g., photo albums, that have a high geographical correlation. Intuitively, if we can conﬁdently localize
some of the photos in an album, we can use this information to also localize the photos with uncertain location. Assigning each photo in an album a location is a sequence-tosequence problem which requires a model that accumulates
a state from previously seen examples and makes the decision for the current example based on both the state and
the current example. Therefore, long-short term memory
(LSTM) architectures seem like a good ﬁt for this task.
We now explore how to address the problem of predicting
photo sequence geolocations using LSTMs.
Training Data.
For this task, we collected a dataset of
29.7M public photo albums with geotags from Google+,
which we split into 23.5M training albums (490M images)
and 6.2M testing albums (126M) images. We use the S2
quantization scheme from the previous section to assign labels to the images.
Model architecture. The basic structure of our model is as
follows (Fig. 11a): Given an image, we extract an embedding vector from the ﬁnal layer before the SoftMax layer in
PlaNet. This vector is fed into the LSTM unit. The output
vector of the LSTM is then fed into a SoftMax layer that
performs the classiﬁcation into S2 cells. We feed the images of an album into the model in chronological order. For
the Inception part, we re-use the parameters of the singleimage model. During training, we keep the Inception part
ﬁxed and only train the LSTM units and the SoftMax layer.
Results. We compare this model to the single-image PlaNet
model and a baseline that simply averages the single-image
Figure 10. Left: Input image, right: Heatmap of the probability of the correct class when sliding an occluding window over the image as
in . (a) Grand Canyon. Occluding the distinctive mountain formation makes the conﬁdence in the correct location drop the most. (b)
Norway. While the house in the foreground is fairly generic, the snowy mountain range on the left is the most important cue. (c) Shanghai.
Conﬁdence in the correct location increases if the palm trees in the foreground are covered since they are not common in Shanghai.
Figure 11. Time-unrolled diagrams of the PlaNet LSTM models. (a) Basic model. (b) Label offset. (c) Repeated sequence. The ﬁrst pass
is used to generate the state inside the LSTM, so we only use the predictions of the second pass (red box). (d) Bi-directional LSTM.
PlaNet avg
LSTM rep 25 28.3%
Table 4. Results of PlaNet LSTM on Google+ photo albums. Percentages are the fraction of images in the dataset localized within
the respective distance.
PlaNet predictions of all images in an album and assigns the
average to all images. The results are shown in Tab. 4 (ﬁrst
3 rows). Averaging within albums (’PlaNet avg’) already
yields a signiﬁcant improvement over single-image PlaNet
(45.7% relative on street level), since it transfers more conﬁdent predictions to ambiguous images. However, the LSTM
model clearly outperforms the averaging technique (50.5%
relative improvement on the street level). Visual inspection
of results showed that if an image with high location con-
ﬁdence is followed by several images with lower location
conﬁdence, the LSTM model assigns the low-conﬁdence
images locations close to the high-conﬁdence image. Thus,
while the original PlaNet model tends to “jump around”,
the LSTM model tends to predict close-by locations unless
there is strong evidence of a location change. The LSTM
model outperforms the averaging baseline because the baseline assigns all images in an album the same conﬁdences
and can thus not produce accurate predictions for albums
that include different locations (such as albums of trips).
A problem with this simple LSTM model is that many
albums contain a number of images in the beginning that
contain no helpful visual information. Due to its unidirectional nature, this model cannot ﬁx wrong predictions that
occur in the beginning of the sequence after observing a
photo with a conﬁdent location. For this reason, we now
evaluate a model where the LSTM ingests multiple photos
from the album before making its ﬁrst prediction.
Label offset.
The idea of this model is to shift the labels such that inference is postponed for several time steps
(Fig. 11b). The main motivation under this idea is that this
model can accumulate information from several images in
a sequence before making predictions. Nevertheless, we
found that using offsets does not improve localization accuracy (Tab. 4, LSTM off1, LSTM off2). We assume this
is because the mapping from input image to output labels
becomes more complex, making prediction more difﬁcult
for all photos, while improving predictions just for a limited amount of photos. Moreover, this approach does not
solve the problem universally: For example, if we offset the
label by 2 steps, but the ﬁrst image with high location con-
ﬁdence occurs only after 3 steps, the prediction for the ﬁrst
image will likely still be wrong. To ﬁx this, we now consider models that condition their predictions on all images
in the sequence instead of only previous ones.
Repeated sequences. We ﬁrst evaluate a model that was
trained on sequences that had been constructed by concatenating two instances of the same sequence (Fig. 11c). For
this model, we take predictions only for the images from the
second half of the sequence (i.e. the repeated part). Thus,
all predictions are conditioned on observations from all images. At inference time, passing the sequence to the model
for the ﬁrst time can be viewed as an encoding stage where
the LSTM builds up an internal state based on the images.
The second pass is the decoding stage where at each image,
the LSTM makes a prediction based on its state and the current image. Results show that this approach outperforms the
single-pass LSTMs (Tab. 4, LSTM rep), achieving a 7.8%
relative improvement at street level, at the cost of a twofold
increase in inference time. However, by visually inspecting the results we observed a problem with this approach:
if there are low-conﬁdence images at the beginning of the
sequence, they tend to get assigned to the last conﬁdent location in the sequence, because the model learns to rely on
its previous prediction. Therefore, predictions from the end
of the sequence get carried over to the beginning.
Bi-directional LSTM. A well-known neural network architecture that conditions the predictions on the whole
sequence are bi-directional LSTM (BLSTM) .
model can be seen as a concatenation of two LSTM models, where the ﬁrst one does a forward pass, while the second does a backward pass on a sequence (Fig. 11d). Bidirectional LSTMs cannot be trained with truncated backpropagation through time and thus require to unroll the
LSTMs to the full length of the sequence. To reduce the
computational cost of training, we had to limit the length of
the sequences to 25 images. This causes a decrease in total
accuracy since longer albums typically yield higher accuracy than shorter ones. Since our experiments on this data
are not directly comparable to the previous ones, we also
evaluate the repeated LSTM model on sequences truncated
to 25 images. As the results show (Tab. 4: LSTM rep 25,
BLSTM 25), BLSTMs clearly outperform repeated LSTMs
(16.6% relative improvement on street level). However, because they are not tractable for long sequences, the repeated
model might still be preferable in practice.
5. Conclusion
We presented PlaNet, a CNN for image geolocation. Regarding problem as classiﬁcation, PlaNet produces a probability distribution over the globe.
This allows it to express its uncertainty about the location of a photo and assign probability mass to potential locations. While previous
work mainly focused on photos taken inside cities, PlaNet is
able to localize landscapes, locally typical objects, and even
plants and animals. Our experiments show that PlaNet far
outperforms other methods for geolocation of generic photos and even reaches superhuman performance. We further
extended PlaNet to photo album geolocation by combining
it with LSTMs. Our experiments show that using contextual information for image-based localization makes it reach
50% higher performance than the single-image model.