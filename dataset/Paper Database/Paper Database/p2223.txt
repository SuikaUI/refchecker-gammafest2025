Unsupervised Embedding Learning via Invariant and Spreading
Instance Feature
Pong C. Yuen†
Shih-Fu Chang‡
† Hong Kong Baptist University, Hong Kong
‡ Columbia University, New York
{mangye,pcyuen}@comp.hkbu.edu.hk, {xu.zhang,sc250}@columbia.edu
This paper studies the unsupervised embedding learning problem, which requires an effective similarity measurement between samples in low-dimensional embedding
space. Motivated by the positive concentrated and negative separated properties observed from category-wise supervised learning, we propose to utilize the instance-wise
supervision to approximate these properties, which aims at
learning data augmentation invariant and instance spreadout features. To achieve this goal, we propose a novel instance based softmax embedding method, which directly optimizes the ‘real’ instance features on top of the softmax
function. It achieves signiﬁcantly faster learning speed and
higher accuracy than all existing methods. The proposed
method performs well for both seen and unseen testing categories with cosine similarity. It also achieves competitive
performance even without pre-trained network over samples from ﬁne-grained categories.
1. Introduction
Deep embedding learning is a fundamental task in computer vision , which aims at learning a feature embedding that has the following properties: 1) positive concentrated, the embedding features of samples belonging to the
same category are close to each other ; 2) negative separated, the embedding features of samples belonging to different categories are separated as much as possible . Supervised embedding learning methods have been studied to
achieve such objectives and demonstrate impressive capabilities in various vision tasks . However, annotated data needed for supervised methods might be difﬁcult
to obtain. Collecting enough annotated data for different
tasks requires costly human efforts and special domain expertise. To address this issue, this paper tackles the unsupervised embedding learning problem (a.k.a. unsupervised
metric learning in ), which aims at learning discriminative embedding features without human annotated labels.
Input Images
Output Features
Figure 1: Illustration of our basic idea. The features of the same
instance under different data augmentations should be invariant,
while features of different image instances should be separated.
Unsupervised embedding learning usually requires that
the similarity between learned embedding features is consistent with the visual similarity or category relations of input images. In comparison, general unsupervised feature
learning usually aims at learning a good “intermediate” feature representation from unlabelled data . The
learned feature is then generalized to different tasks by using a small set of labelled training data from the target task
to ﬁne-tune models (e.g., linear classiﬁer, object detector,
etc.) for the target task . However, the learned feature
representation may not preserve visual similarity and its
performance drops dramatically for similarity based tasks,
e.g. nearest neighbor search .
The main challenge of unsupervised embedding learning
is to discover visual similarity or weak category information
from unlabelled samples. Iscen et al. proposed to mine
hard positive and negative samples on manifolds. However,
its performance heavily relies on the quality of the initialized feature representation for label mining, which limits
the applicability for general tasks. In this paper, we propose to utilize the instance-wise supervision to approximate
the positive concentrated and negative separated properties mentioned earlier. The learning process only relies on
instance-wise relationship and does not rely on relations between pre-deﬁned categories, so it can be well generalized
to samples of arbitrary categories that have not been seen
before (unseen testing categories) .
 
For positive concentration: it is usually infeasible to
mine reliable positive information with randomly initialized
network. Therefore, we apply a random data augmentation
(e.g., transformation, scaling) to each image instance and
use the augmented image as a positive sample. In other
words, features of each image instance under different data
augmentations should be invariant. For negative separation: since unlabelled data are usually highly imbalanced
 , the number of negative samples for each image instance is much larger than that of positive samples. Therefore, a small batch of randomly selected instances can be approximately treated as negative samples for each instance.
With such assumption, we try to separate each instance from
all the other sampled instances within the batch, resulting
in a spread-out property . It is clear that such assumption may not always hold, and each batch may contain a
few false negatives. However, through our extensive experiments, we observe that the spread-out property effectively
improves the discriminability. In summary, our main idea is
to learn a discriminative instance feature, which preserves
data augmentation invariant and spread-out properties for
unsupervised embedding learning, as shown in Fig. 1.
To achieve these goals, we introduce a novel instance
feature-based softmax embedding method. Existing softmax embedding is usually built on classiﬁer weights or
memorized features , which has limited efﬁciency and
discriminability. We propose to explicitly optimize the feature embedding by directly using the inner products of instance features on top of softmax function, leading to significant performance and efﬁciency gains. The softmax function mines hard negative samples and takes full advantage
of relationships among all sampled instances to improve the
performance. The number of instance is signiﬁcantly larger
than the number of categories, so we introduce a Siamese
network training strategy.
We transform the multi-class
classiﬁcation problem to a binary classiﬁcation problem and
use maximum likelihood estimation for optimization.
The main contributions can be summarized as follows:
• We propose a novel instance feature-based softmax
embedding method to learn data augmentation invariant and instance spread-out features. It achieves significantly faster learning speed and higher accuracy than
all the competing methods.
• We show that both the data augmentation invariant
and instance spread-out properties are important for
instance-wise unsupervised embedding learning. They
help capture apparent visual similarity between samples and generalizes well on unseen testing categories.
• The proposed method achieves the state-of-the-art performances over other unsupervised learning methods
on comprehensive image classiﬁcation and embedding
learning experiments.
2. Related Work
General Unsupervised Feature Learning.
Unsupervised feature learning has been widely studied in literature. Existing works can be roughly categorized into three
categories : 1) generative models, this approach aims
at learning a parameterized mapping between images and
predeﬁned noise signals, which constrains the distribution
between raw data and noises . Bolztmann Machines
(RBMs) , Auto-encoders and generative
adversarial network (GAN) are widely studied. 2) Estimating Between-image Labels, it usually estimates between-image labels using the clustering technique
 or kNN-based methods , which provide label
information. Then label information and feature learning
process are iteratively updated. 3) Self-supervised Learning, this approach designs pretext tasks/signals to generate
“pseudo-labels” and then formulate it as a prediction task to
learn the feature representations. The pretext task could be
the context information of local patches , the position of
randomly rearranged patches , the missing pixels of an
image or the color information from gray-scale images
 . Some attempts also use video information to provide
weak supervision to learn feature representations .
As we discussed in Section 1, general unsupervised feature learning usually aims at learning a good “intermediate” feature representation that can be well generalized to
other tasks. The intermediate feature representation may
not preserve visual similar property. In comparison, unsupervised embedding learning requires additional visual similarity property of the learned features.
Deep Embedding Learning. Deep embedding learning
usually learns an embedding function by minimizing the
intra-class variation and maximizing the inter-class variation . Most of them are designed on top of
pairwise or triplet relationships . In particular, several sampling strategies are widely investigated to
improve the performance, such as hard mining , semihard mining , smart mining and so on. In comparison, softmax embedding achieves competitive performance
without sampling requirement .
Supervised learning
has achieved superior performance on various tasks, but
they still rely on enough annotated data.
Unsupervised Embedding Learning. According to the
evaluation protocol, it can be categorized into two cases,
1) the testing categories are the same with the training categories (seen testing categories), and 2) the testing categories are not overlapped with the training categories (unseen testing categories). The latter setting is more challenging. Without category-wise labels, Iscen et al. proposed to mine hard positive and negative samples on manifolds, and then train the feature embedding with triplet loss.
However, it heavily relies on the initialized representation
for label mining.
3. Proposed Method
Our goal is to learn a feature embedding network fθ(·)
from a set of unlabelled images X = {x1, x2, · · · , xn}.
fθ(·) maps the input image xi into a low-dimensional embedding feature fθ(xi) ∈Rd, where d is the feature dimension. For simplicity, the feature representation fθ(xi) of an
image instance is represented by fi, and we assume that all
the features are ℓ2 normalized, i.e. ∥fi∥2 = 1. A good feature embedding should satisfy: 1) the embedding features of
visual similar images are close to each other; 2) the embedding features of dissimilar image instances are separated.
Without category-wise labels, we utilize the instancewise supervision to approximate the positive concentrated
and negative seperated properties. In particular, the embedding features of the same instance under different data
augmentations should be invariant, while the features of different instances should be spread-out. In the rest of this
section, we ﬁrst review two existing instance-wise feature
learning methods, and then propose a much more efﬁcient
and discriminative instance feature-based softmax embedding. Finally, we will give a detailed rationale analysis and
introduce our training strategy with Siamese network.
3.1. Instance-wise Softmax Embedding
Softmax Embedding with Classiﬁer Weights. Exemplar
CNN treats each image as a distinct class.
Following the conventional classiﬁer training, it deﬁnes a matrix
W = [w1, w2, · · · , wn]T ∈Rn×d, where the j-th column
wj is called the corresponding classiﬁer weight for the j-th
instance. Exemplar CNN ensures that image instance under
different image transformations can be correctly classiﬁed
into its original instance with the learned weight. Based on
Softmax function, the probability of sample xj being recognized as the i-th instance can be represented as
k=1 exp(wT
At each step, the network pulls sample feature fi towards
its corresponding weight wi, and pushes it away from the
classiﬁer weights wk of other instances. However, classiﬁer
weights prevent explicitly comparison over features, which
results in limited efﬁciency and discriminability.
Softmax Embedding with Memory Bank. To improve the
inferior efﬁciency, Wu et al. propose to set up a memory bank to store the instance features fi calculated in the
previous step. The feature stored in the memory bank is denoted as vi, which serves as the classiﬁer weight for the
corresponding instance in the following step. Therefore,
the probability of sample xj being recognized as the i-th
instance can be written as
k=1 exp(vT
where τ is the temperature parameter controlling the concentration level of the sample distribution . vT
i fj measures the cosine similarity between the feature fj and the
i-th memorized feature vi. For instance xi at each step,
the network pulls its feature fi towards its corresponding
memorized vector vi, and pushes it away from the memorized vectors of other instances. Due to efﬁciency issue, the
memorized feature vi corresponding to instance xi is only
updated in the iteration which takes xi as input. In other
words, the memorized feature vi is only updated once per
epoch. However, the network itself is updated in each iteration. Comparing the real-time instance feature fi with the
outdated memorized feature vi would cumber the training
process. Thus, the memory bank scheme is still inefﬁcient.
A straightforward idea to improve the efﬁciency is directly optimizing over feature itself, i.e. replacing the
weight {wi} or memory {vi} with fi. However, it is implausible due to two reasons: 1) Considering the probability
P(i|xi) of recognizing xi to itself, since f T
i fi=1, i.e. the feature and “pseudo classiﬁer weight” (the feature itself) are always perfectly aligned, optimizing the network will not provide any positive concentrated property; 2) It’s impractical
to calculate the feature of all the samples (fk, k = 1, . . . , n)
on-the-ﬂy in order to calculate the denominator in Eq. (2),
especially for large-scale instance number dataset.
3.2. Softmax Embedding on ‘Real’ Instance Feature
To address above issues, we propose a softmax embedding variant for unsupervised embedding learning, which
directly optimizes the real instance feature rather than classiﬁer weights or memory bank . To achieve the goal
that features of the same instance under different data augmentations are invariant, while the features of different instances are spread-out, we propose to consider 1) both the
original image and its augmented image, 2) a small batch of
randomly selected samples instead of the full dataset.
For each iteration, we randomly sample m instances
from the dataset.
To simplify the notation, without
loss of generality, the selected samples are denoted by
{x1, x2, · · · , xm}. For each instance, a random data augmentation operation T(·) is applied to slightly modify the
original image. The augmented sample T(xi) is denoted by
ˆxi, and its embedding feature fθ(ˆxi) is denoted by ˆfi. Instead of considering the instance feature learning as a multiclass classiﬁcation problem, we solve it as binary classiﬁcation problem via maximum likelihood estimation (MLE). In
particular, for instance xi, the augmented sample ˆxi should
be classiﬁed into instance i, and other instances xj, j ̸= i
shouldn’t be classiﬁed into instance i. The probability of ˆxi
being recognized as instance i is deﬁned by
P(i|ˆxi) =
k=1 exp(f T
Embedding Space
Augmentation
Share Weights
Figure 2: The framework of the proposed unsupervised learning method with Siamese network. The input images are projected into
low-dimensional normalized embedding features with the CNN backbone. Image features of the same image instance with different data
augmentations are invariant, while embedding features of different image instances are spread-out.
On the other hand, the probability of xj being recognized
as instance i is deﬁned by
k=1 exp(f T
k fj/τ), j ̸= i
Correspondingly, the probability of xj not being recognized
as instance i is 1 −P(i|xj).
Assuming different instances being recognized as instance i are independent, the joint probability of ˆxi being
recognized as instance i and xj, j ̸= i not being classiﬁed
into instance i is
Pi = P(i|ˆxi)
(1 −P(i|xj))
The negative log likelihood is given by
Ji = −log P(i|ˆxi) −
log(1 −P(i|xj))
We solve this problem by minimizing the sum of the negative log likelihood over all the instances within the batch,
which is denoted by
log P(i|ˆxi) −
log(1 −P(i|xj)).
3.3. Rationale Analysis
This section gives a detailed rationale analysis about why
minimizing Eq. (6) could achieve the augmentation invariant and instance spread-out feature. Minimizing Eq. (6) can
be viewed as maximizing Eq. (3) and minimizing Eq. (4).
Considering Eq. (3), it can be rewritten as
P(i|ˆxi) =
i ˆfi/τ) + P
k̸=i exp(f T
Maximizing Eq. (3) requires maximizing exp(f T
i ˆfi/τ) and
minimizing exp(f T
k ˆfi/τ), k ̸= i. Since all the features are
ℓ2 normalized, maximizing exp(f T
i ˆfi/τ) requires increasing the inner product (cosine similarity) between fi and ˆfi,
resulting in a feature that is invariant to data augmentation.
On the other hand, minimizing exp(f T
k ˆfi/τ) ensures ˆfi and
other instances {fk} are separated. Considering all the instances within the batch, the instances are forced to be separated from each other, resulting in the spread-out property.
Similarly, Eq. (4) can be rewritten as,
j fj/τ) + P
k̸=j exp(f T
Note that the inner product f T
j fj is 1 and the value of τ
is generally small (say 0.1 in the experiment). Therefore,
j fj/τ) generally determines the value of the whole
denominator. Minimizing Eq. (4) means that exp(f T
should be minimized, which aims at separating fj from fi.
Thus, it further enhances the spread-out property.
3.4. Training with Siamese Network
We proposed a Siamese network to implement the proposed algorithm as shown in Fig. 2. At each iteration, m
randomly selected image instances are fed into in the ﬁrst
branch, and the corresponding augmented samples are fed
into the second branch. Note that data augmentation is also
be used in the ﬁrst branch to enrich the training samples. For
implementation, each sample has one randomly augmented
positive sample and 2N −2 negative samples to compute
Eq. (7), where N is the batch size. The proposed training strategy greatly reduces the computational cost. Meanwhile, this training strategy also takes full advantage of relationships among all instances sampled in a mini-batch .
Theoretically, we could also use a multi-branch network by
considering multiple augmented images for each instance in
the batch.
DeepCluster (10) 
DeepCluster (1000) 
Exemplar 
NPSoftmax 
Triplet (Hard)
Table 1: kNN accuracy (%) on CIFAR-10 dataset.
4. Experimental Results
We have conducted the experiments with two different
settings to evaluate the proposed method1. The ﬁrst setting
is that the training and testing sets share the same categories
(seen testing category). This protocol is widely adopted for
general unsupervised feature learning. The second setting
is that the training and testing sets do not share any common categories (unseen testing category). This setting is
usually used for supervised embedding learning . Following , we don’t use any semantic label in the training
set. The latter setting is more challenging than the former
setting and it could apparently demonstrate the quality of
learned features on unseen categories.
4.1. Experiments on Seen Testing Categories
We follow the experimental settings in to conduct
the experiments on CIFAR-10 and STL-10 datasets,
where training and testing set share the same categories.
Speciﬁcally, ResNet18 network is adopted as the backbone and the output embedding feature dimension is set to
128. The initial learning rate is set to 0.03, and it is decayed by 0.1 and 0.01 at 120 and 160 epoch. The network is
trained for 200 epochs. The temperature parameter τ is set
to 0.1. The algorithm is implemented on PyTorch with SGD
optimizer with momentum. The weight decay parameter is
5×10−4 and momentum is 0.9. The training batch size is
set to 128 for all competing methods on both datasets. Four
kinds of data augmentation methods (RandomResizedCrop,
RandomGrayscale, ColorJitter, RandomHorizontalFlip) in
PyTorch with default parameters are adopted.
Following , we adopt weighted kNN classiﬁer to
evaluate the performance. Given a test sample, we retrieve
its top-k (k = 200) nearest neighbors based on cosine similarity, then apply weighted voting to predict its label .
CIFAR-10 Dataset
CIFAR-10 datset contains 50K training images and
10K testing images from the same ten classes. The image
size are 32 × 32. Five methods are included for comparison: DeepCluster with different cluster numbers, Exem-
 
Unsupervised_Embedding_Learning
Training Epochs
kNN Accuracy (%)
DeepCluster 
Exemplar 
Figure 3: Evaluation of the training efﬁciency on CIFAR-10
dataset. kNN accuracy (%) at each epoch is reported, demonstrating the learning speed of different methods.
plar CNN , NPSoftmax , NCE and Triplet loss
with and without hard mining. Triplet (hard) is the online
hard negative sample within each batch for training ,
and the margin parameter is set to 0.5. DeepCluster and
NCE represent the state-of-the-art unsupervised feature
learning methods. The results are shown in Table 1.
Classiﬁcation Accuracy. Table 1 demonstrates that our
proposed method achieves the best performance (83.6%)
with kNN classiﬁer.
DeepCluster performs well in
learning good “intermediate” features with large-scale unlabelled data, but the performance with kNN classiﬁcation
drops dramatically. Meanwhile, it is also quite sensitive
to cluster numbers, which is unsuitable for different tasks.
Compared to Exemplar CNN which uses the classiﬁer
weights for training, the proposed method outperforms it
Compared to NPSoftmax and NCE ,
which use memorized feature for optimizing, the proposed
method outperform by 2.8% and 3.2% respectively. The
performance improvement is clear due to the idea of directly
performing optimization over feature itself. Compared to
triplet loss, the proposed method also outperforms it by a
clear margin. The superiority is due to the hard mining nature in Softmax function.
Efﬁciency. We plot the learning curves of the competing methods at different epochs in Fig. 7. The proposed
method takes only 2 epochs to get a kNN accuracy of 60%
while takes 25 epochs and takes 45 epochs to reach
the same accuracy. It is obvious that our learning speed is
much faster than the competitors. The efﬁciency is guaranteed by directly optimization on instance features rather
than classiﬁer weights or memory bank .
STL-10 Dataset
STL-10 dataset is an image recognition dataset with colored images of size 96 × 96, which is widely used in unsupervised learning. Speciﬁcally, this dataset is originally designed with three splits: 1) train, 5K labelled images in ten
k-MeansNet∗ 
Satck∗ 
Exemplar∗ 
NPSoftmax 
DeepCluster(100) 
Table 2: Classiﬁcation accuracy (%) with linear classiﬁer and
kNN classiﬁer on STL-10 dataset.
∗Results are taken from ,
the baseline network is different.
classes for training, 2) test, 8K images from the same ten
classes for testing, 3) unlabelled, 100K unlabelled images
which share similar distribution with labelled data for unsupervised learning. We follow the same experimental setting as CIFAR-10 dataset and report classiﬁcation accuracy
(%) with both Linear Classiﬁer (Linear) and kNN classier
(kNN) in Table 2. Linear classiﬁer means training a SVM
classiﬁer on the learned features and the labels of training
samples. The classiﬁer is used to predict the label of test
samples. We implement NPSoftmax , NCE and
DeepCluster (cluster number 100) under the same settings with their released code. By default, we only use 5K
training images without using labels for training. The performances of some state-of-the-art unsupervised methods
(k-MeansNet , HMP , Satck and Exemplar )
are also reported. Those results are taken from .
As shown in Table 2 , when only using 5K training images for learning, the proposed method achieves the best accuracy with both classiﬁers (kNN: 74.1%, Linear: 69.5%),
which are much better than NCE and DeepCluster 
under the same evaluation protocol. Note that kNN measures the similarity directly with the learned features and
Linear requires additional classiﬁer learning with the labelled training data. When 105K images are used for training, the proposed method also achieves the best performance for both kNN classiﬁer and linear classiﬁer. In particular, the kNN accuracy is 74.1% for 5K training images,
and it increases to 81.6% for full 105K training images. The
classiﬁcation accuracy with linear classiﬁer also increases
from 69.5% to 77.9%. This experiment veriﬁes that the proposed method can beneﬁt from more training samples.
4.2. Experiments on Unseen Testing Categories
This section evaluates the discriminability of the learned
feature embedding when the semantic categories of training
samples and testing samples are not overlapped. We follow
the experimental settings described in to conduct experiments on CUB200-2011(CUB200) , Stanford Online Product (Product) and Car196 datasets. No
semantic label is used for training. Caltech-UCSD Birds
Initial (FC)
Supervised Learning
Lifted 
Clustering 
Triplet+ 
Smart+ 
Unsupervised Learning
Cyclic 
Exemplar 
DeepCluster 
Table 3: Results (%) on CUB200 dataset.
Initial (FC)
Exemplar 
DeepCluster 
Table 4: Results (%) on Product dataset.
200 (CUB200) is a ﬁne-grained bird dataset. Following , the ﬁrst 100 categories with 5,864 images are used
for training, while the other 100 categories with 5,924 images are used for testing. Stanford Online Product (Product)
 is a large-scale ﬁne-grained product dataset. Similarly,
11,318 categories with totally 59,551 images are used for
training, while the other 11,316 categories with 60,502 images are used for testing. Cars (Car196) dataset is a
ﬁne-grained car category dataset. The ﬁrst 98 categories
with 8,054 images are used for training, while the other 98
categories with 8,131 images are used for testing.
Implementation Details. We implement the proposed
method on PyTorch. The pre-trained Inception-V1 on
ImageNet is used as the backbone network following existing methods . A 128-dim fully connected layer
with ℓ2 normalization is added after the pool5 layer as the
feature embedding layer. All the input images are ﬁrstly resized to 256 × 256. For data augmentation, the images are
randomly cropped at size 227×227 with random horizontal
ﬂipping following . Since the pre-trained network
performs well on CUB200 dataset, we randomly select the
augmented instance and its corresponding nearest instance
as positive. In testing phase, a single center-cropped image is adopted for ﬁne-grained recognition as in . We
adopt the SGD optimizer with 0.9 momentum. The initial
learning rate is set to 0.001 without decay. The temperature
parameter τ is set to 0.1. The training batch size is set to 64.
Evaluation Metrics. Following existing works on supervised deep embedding learning , the retrieval
performance and clustering quality of the testing set are
evaluated. Cosine similarity is adopted for similarity mea-
Initial (FC)
Exemplar 
DeepCluster 
Table 5: Results (%) on Car196 dataset.
surement. Given a query image from the testing set, R@K
measures the probability of any correct matching (with
same category label) occurs in the top-k retrieved ranking
list . The average score is reported for all testings samples. Normalized Mutual Information (NMI) is utilized
to measure the clustering performance of the testing set.
Comparison to State-of-the-arts. The results of all the
competing methods on three datasets are listed in Table 3,
4 and 5, respectively. MOM is the only method that
claims for unsupervised metric learning. We implement the
other three state-of-the-art unsupervised methods (Exemplar , NCE and DeepCluster ) on three datasets
with their released code under the same setting for fair comparison. Note that these methods are originally evaluated
for general unsupervised feature learning, where the training and testing set share the same categories. We also list
some results of supervised learning (originate from ) on
CUB200 dataset as shown in Table 3.
Generally, the instance-wise feature learning methods
(NCE , Examplar , Ours) outperform non-instancewise feature learning methods (DeepCluster , MOM
 ), especially on Car196 and Product datasets, which indicates instance-wise feature learning methods have good
generalization ability on unseen testing categories. Among
all the instance-wise feature learning methods, the proposed
method is the clear winner, which also veriﬁes the effectiveness of directly optimizing over feature itself. Moreover, the
proposed unsupervised learning method is even competitive
to some supervised learning methods on CUB200 dataset.
Qualitative Result. Some retrieved examples with cosine similarity on CUB200 dataset at different training
epochs are shown in Fig. 4. The proposed algorithm can
iteratively improve the quality of the learned feature and
retrieve more correct images.
Although there are some
wrongly retrieved samples from other categories, most of
the top retrieved samples are visually similar to the query.
Training from Scratch. We also evaluate the performance using a network (ResNet18) without pre-training.
The results on the large-scale Product dataset are shown in
Table 6. The proposed method is also a clear winner. Interestingly, MOM fails in this experiment. The main
reason is that the feature from randomly initialized network
provides limited information for label mining. Therefore,
MOM cannot estimate reliable labels for training.
Exemplar 
Table 6: Results (%) on Product dataset using network without
pre-trained parameters.
4.3. Ablation Study
The proposed method imposes two important properties
for instance feature learning: data augmentation invariant
and instance spread-out. We conduct ablation study to show
the effectiveness of each property on CIFAR-10 dataset.
kNN Acc (%)
Table 7: Effects of each data augmentation operation on CIFAR-
10 dataset. ’w/o’: Without. ’R’: RandomResizedCrop, ’G’: RandomGrayscale, ’C’: ColorJitter, ’F’: RandomHorizontalFlip.
kNN Acc (%)
Table 8: Different sampling strategies on CIFAR-10 dataset.
To show the importance of data augmentation invariant
property, we ﬁrstly evaluate the performance by removing
each of the operation respectively from the data augmentation set. The results are shown in Table 7. We observe
that all listed operations contribute to the remarkable performance gain achieved by the proposed algorithm. In particular, RandomResizedCrop contributes the most. We also
evaluate the performance without data augmentation (No
DA) in Table 8, and it shows that performance drops signiﬁcantly from 83.6% to 37.4%. It is because when training without data augmentation, the network does not create
any positive concentration property. The features of visually similar images are falsely separated.
To show the importance of spread-out property, we evaluated two different strategies to choose negative samples:
1) selecting the top 50% instance features that are similar
to query instance as negative (hard negative); 2) selecting
the bottom 50% instance features that are similar to query
instance as negative (easy negative). The results are shown
as “Hard” and “Easy” in Table 8. The performance drops
dramatically when only using the easy negative. In comparison, the performance almost remains the same as the
full model when only using hard negative. It shows that
separating hard negative instances helps to improve the discriminability of the learned embedding.
4.4. Understanding of the Learned Embedding
We calculate the cosine similarity between the query feature and its 5NN features from the same category (Positive)
Figure 4: 4NN retrieval results of some example queries on CUB200-2011 dataset. The positive (negative) retrieved results are framed in
green (red). The similarity is measured with cosine similarity.
(a) Random Network
(c) Exemplar 
(b) NCE 
Figure 5: The cosine similarity distributions on CIFAR-10 
as well as 5NN features from different categories (Negative). The distributions of the cosine similarity of different
methods are shown in Fig. 5. A more separable distribution
indicates a better feature embedding. It shows that the proposed method performs best to separate positive and negative samples. We could also observe that our learned feature
preserves the best spread-out property.
It is interesting to show how the learned instance-wise
feature helps the category label prediction. We report the
cosine similarity distribution based on other category definitions (attributes in ) instead of semantic label in
Fig. 6. The distribution clearly shows that the proposed
method also performs well to separate other attributes,
which demonstrates the generalization ability of the learned
5. Conclusion
In this paper, we propose to address the unsupervised
embedding learning problem by learning a data augmentation invariant and instance spread-out feature. In particular,
we propose a novel instance feature based softmax embed-
(a) Attribute “animals vs artifacts”
(b) Attribute “big vs small shape animal”
Figure 6: The cosine similarity distributions of randomly initialized network (left column) and our learned model (right column)
with different attributes on CIFAR-10 .
ding trained with Siamese network, which explicitly pulls
the features of the same instance under different data augmentations close and pushes the features of different instances away. Comprehensive experiments show that directly optimizing over instance feature leads to signiﬁcant
performance and efﬁciency gains. We empirically show that
the spread-out property is particularly important and it helps
capture the visual similarity among samples.
Acknowledgement
This work is partially supported by Research Grants
Council (RGC/HKBU12200518), Hong Kong. This work is
partially supported by the United States Air Force Research
Laboratory (AFRL) and the Defense Advanced Research
Projects Agency (DARPA) under Contract No. FA8750-16-
C-0166. Any opinions, ﬁndings and conclusions or recommendations expressed in this material are solely the responsibility of the authors and does not necessarily represent the
ofﬁcial views of AFRL, DARPA, or the U.S. Government.