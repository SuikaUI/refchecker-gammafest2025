Data augmentation for non-Gaussian regression models using
variance-mean mixtures
Nicholas G. Polson
Booth School of Business
University of Chicago
James G. Scott
University of Texas at Austin
November 26, 2024
We use the theory of normal variance-mean mixtures to derive a data-augmentation
scheme for a class of common regularization problems. This generalizes existing theory
on normal variance mixtures for priors in regression and classiï¬cation. It also allows
variants of the expectation-maximization algorithm to be brought to bear on a wider
range of models than previously appreciated. We demonstrate the method on several
examples, including sparse quantile regression and binary logistic regression. We also
show that quasi-Newton acceleration can substantially improve the speed of the algorithm without compromising its robustness.
Key words: Data augmentation; expectation-maximization; Sparsity; Varianceâ€“mean
mixture of normals
Introduction
Regularized regression and classiï¬cation
Many problems in regularized estimation involve an objective function of the form
i Î² | Ïƒ) +
g(Î²j | Ï„) .
Here yi is a response, which may be continuous, binary, or multinomial; xi is a known
p-vector of predictors; Î² = (Î²1, . . . , Î²p) is an unknown vector of coeï¬ƒcients; f and g are
the negative log likelihood and penalty function, respectively; and Ïƒ and Ï„ are scale parameters, for now assumed ï¬xed. We may phrase the problem as one of minimizing L(Î²), or
equivalently maximizing the unnormalized posterior density exp{âˆ’L(Î²)}, interpreting the
penalty as a negative log prior.
 
In this paper, we unify many disparate problems of this form into a single class of normal
variance-mean mixtures. This has two practical consequences. First, it facilitates the use of
penalty functions in models with rich structure, such as hierarchical non-Gaussian models,
discrete mixtures of generalized linear models, and missing-data problems.
Second, it can be used to derive simple expectation-maximization algorithms for non-
Gaussian models. This leads to a stable, uniï¬ed approach to estimation in many problems,
including quantile regression, logistic regression, negative-binomial models for count outcomes, support-vector machines, and robust regression.
The key result is Proposition 1, which describes a simple relationship between the
derivatives of f and g and the conditional suï¬ƒcient statistics that arise in our expectationmaximization algorithm. The expected values of these conditional suï¬ƒcient statistics can
usually be calculated in closed form, even if the full conditional distribution of the latent
variables is unknown or intractable. Proposition 3 provides the correponding result for the
posterior mean estimator, generalizing the results of Masreliez and Pericchi & Smith
One known disadvantage of expectationâ€“maximization algorithms is that they may converge slowly, particularly if the fraction of missing information is large . Our method, in its basic form, is no exception. But we ï¬nd that substantial gains
in speed are possible using quasi-Newton acceleration .
The resulting approach combines the best features of the expectationâ€“maximization and Newtonâ€“Raphson
algorithms: robustness and guaranteed ascent when far from the solution, and super-linear
convergence when close to it.
Relationship with previous work
Our work is motivated by recent Bayesian research on sparsity-inducing priors in linear
regression, where f = âˆ¥y âˆ’XÎ²âˆ¥2 is the negative Gaussian log likelihood, and g corresponds
to a normal variance-mixture prior . Examples of work in this
area include the lasso ; the bridge estimator ; the relevance vector machine of
Tipping ; the normal/Jeï¬€reys prior of Figueiredo and Bae & Mallick ;
the normal/exponential-gamma model of Griï¬ƒn & Brown ; the normal/gamma and
normal/inverse-Gaussian models ; the horseshoe prior of Carvalho et al. ; the hypergeometric invertedâ€“beta model of Polson &
Scott ; and the double-Pareto model of Armagan et al. . See Polson & Scott
 for a review.
We generalize this work by representing both the likelihood and prior as variance-mean
mixtures of Gaussians. This data-augmentation approach relies upon the following decom-
Table 1: Variance-mean mixture representations for many common loss functions. Recall
that zi = yi âˆ’xT
i Î² for regression, or zi = yixT
i Î² for binary classiï¬cation.
Error/loss function
f(zi | Î², Ïƒ)
Squared-error
Absolute-error
Exponential
Check loss
|zi| + (2q âˆ’1)zi
Generalized inverse Gaussian
Support vector machines
max(1 âˆ’zi, 0)
Generalized inverse Gaussian
log(1 + ezi)
p(Î² | Ï„, Ïƒ, y) âˆeâˆ’L(Î²)
i Î² | Ïƒ) âˆ’
p(zi | Î², Ïƒ)
ï£¾= p(z | Î², Ïƒ) p(Î² | Ï„) ,
where the working response zi is equal to yi âˆ’xT
i Î² for Gaussian regression, or yixT
binary classiï¬cation using logistic regression or support-vector machines. Here yi is coded as
Â±1. Both Ïƒ and Ï„ are hyperparameters; they are typically estimated jointly with Î², although
they may also be speciï¬ed by the user or chosen by cross-validation. In some cases, most
notably in logistic regression, the likelihood is free of hyperparameters, in which case Ïƒ does
not appear in the model. Previous studies have presented similar results for speciï¬c models, including support-vector machines
and the so-called powered-logit likelihood. Our paper characterizes the general case.
One thing we do not do is to study the formal statistical properties of the resulting
estimators, such as consistency as p and n diverge. For this we refer the reader to Fan &
Li and Griï¬ƒn & Brown , who discuss this issue from classical and Bayesian
viewpoints, respectively.
Normal variance-mean mixtures
There are two key steps in our approach. First, we use variance-mean mixtures, rather
than just variance mixtures. Second, we interweave two diï¬€erent mixture representations,
one for the likelihood and one for the prior. The introduction of latent variables {Ï‰i} and
{Î»j} in Equations (2) and (3), below, reduces exp{âˆ’L(Î²)} to a Gaussian linear model with
heteroscedastic errors:
p(zi | Î², Ïƒ) =
Ï†(zi | Âµz + ÎºzÏ‰âˆ’1
) dP(Ï‰i) ,
p(Î²j | Ï„) =
Ï†(Î²j | ÂµÎ² + ÎºÎ²Î»âˆ’1
j , Ï„ 2Î»âˆ’1
j ) dP(Î»j) ,
where Ï†(a | m, v) is the normal density, evaluated at a, for mean m and variance v.
By marginalizing over these latent variables with respect to diï¬€erent ï¬xed combinations
of (Âµz, Îºz, ÂµÎ², ÎºÎ²) and the mixing measures P(Î»j) and P(Ï‰i), it is possible to generate
many commonly used objective functions that have not been widely recognized as Gaussian
mixtures. Table 1 lists several common likelihoods in this class, along with the corresponding
ï¬xed choices for (Îºz, Âµz) and the mixing distribution P(Ï‰i).
A useful fact is that one may avoid dealing directly with conditional distributions for
these latent variables. To ï¬nd the posterior mode, it is suï¬ƒcient to use Proposition 1 to
calculate moments of these distributions. These moments in turn depend only upon the
derivatives of f and g, along with the hyperparameters.
We focus on two choices of the mixing measure: the generalized inverse-Gaussian distribution and the Polya distribution, the latter being an inï¬nite sum of exponential random
variables . These choices lead to the hyperbolic and Z distributions, respectively, for the variance-mean mixture.
The two key integral identities
eâˆ’Î±|Î¸âˆ’Âµ|+Îº(Î¸âˆ’Âµ) =
Ï† (Î¸ | Âµ + Îºv, v) pG
v | 1, 0, (Î±2 âˆ’Îº2)1/2
(1 + eÎ¸âˆ’Âµ)2(Î±âˆ’Îº) =
Ï† (Î¸ | Âµ + Îºv, v) pP(v | Î±, Î± âˆ’2Îº) dv ,
where pG and pP are the density functions of the generalized inverse-Gaussian and Polya
distributions, respectively.
For details, see Appendix A. We use Î¸ to denote a dummy
argument that could involve either data or parameters, and v to denote a latent variance.
All other terms are pre-speciï¬ed in order to represent a particular density or function. These
two expressions lead, by an application of the Fatouâ€“Lebesgue theorem, to three further
identities for the improper limiting cases of the two densities above:
âˆ’2câˆ’1 max(aÎ¸, 0)
Ï†(Î¸ | âˆ’av, cv) dv ,
âˆ’2câˆ’1Ïq(Î¸)
Ï†(Î¸ | v âˆ’2Ï„v, cv)eâˆ’2Ï„(1âˆ’Ï„)v dv ,
(1 + exp{Î¸ âˆ’Âµ})âˆ’1 =
Ï†(Î¸ | Âµ âˆ’v/2, v) pP(v | 0, 1) dv ,
where Ïq(Î¸) = |Î¸|/2 + (q âˆ’1/2) Î¸ is the check-loss function. The ï¬rst leads to the pseudolikelihood for support-vector machines, the second to quantile and lasso regression, and the
third to logistic and negative binomial regression under their canonical links. The function
pP(v | 0, 1) is an improper density corresponding to a sum of exponential random variables.
In the case of a multinomial model, the conditional posterior for a single category, given
the parameters for other categories, is a binary logistic model.
An expectation-maximization algorithm
Overview of approach
By exploiting conditional Gaussianity, models of the form (2)â€“(3) can be ï¬t using an
expectation-maximization algorithm . In the expectation step, one
computes the expected value of the log posterior, given the current estimate Î²(g):
C(Î² | Î²(g)) =
log p(Î² | Ï‰, Î», Ï„, Ïƒ, z)p(Ï‰, Î» | Î²(g), Ï„, z) dÏ‰ dÎ» .
Then in the maximization step, one maximizes the complete-data posterior as a function of
Î². The same representation can also be used to derive Markov-chain sampling algorithms,
although we do not explore this aspect of the approach.
The expectation-maximization algorithm has several advantages here compared to other
iterative methods. It is highly robust to the choice of initial value, has no user-speciï¬ed
tuning constants, and leads to a sequence of estimates {Î²(1), Î²(2), . . . } that monotonically
increase the observed-data log posterior density. The disadvantage is the potential for slow
convergence. For the models we consider, this disadvantage is real, but can be avoided using
quasi-Newton acceleration.
The complete-data log posterior can be represented in two ways. First, we have
log p(Î² | Ï‰, Î», Ï„, Ïƒ, z) = c0(Ï‰, Î», Ï„, Ïƒ, z) âˆ’
 zi âˆ’Âµz âˆ’ÎºyÏ‰âˆ’1
Î»j(Î²j âˆ’ÂµÎ² âˆ’ÎºÎ²Î»âˆ’1
for some constant c0, recalling that zi = yi âˆ’xT
i Î² for regression or zi = yixT
i Î² for classiï¬cation. Factorizing this further as a function of Î² yields a second representation:
log p(Î² | Ï‰, Î», Ï„, Ïƒ, z) = c1(Ï‰, Î», Ï„, Ïƒ, z) âˆ’
Ï‰i (zi âˆ’Âµz)2 + Îºy
Î»j(Î²j âˆ’ÂµÎ²)2 + ÎºÎ²
for some constant c1. We now derive the expectation and maximization steps, along with
the complete-data suï¬ƒcient statistics.
The expectation step
From (6), observe that the complete-data objective function is linear in both Ï‰i and Î»j.
Therefore, in the expectation step, we replace Î»j and Ï‰i with their conditional expectations
, given the data and the current Î²(g). The following result provides expressions
for these conditional moments under any model where both the likelihood and prior can be
represented by normal variance-mean mixtures.
Proposition 1. Suppose that the objective function L(Î²) in (1) can be represented by
a hierarchical variance-mean Gaussian mixture, as in Equations (2) and (3). Then the
conditional moments Ë†Î»j = E (Î»j | Î², Ï„, z) and Ë†Ï‰i = E (Ï‰i | Ïƒ, zi) are given by
(Î²j âˆ’ÂµÎ²)Ë†Î»j = ÎºÎ² + Ï„ 2 gâ€²(Î²j | Ï„) ,
(zi âˆ’Âµz)Ë†Ï‰i = Îºz + Ïƒ2 fâ€²(zi | Î², Ïƒ) ,
where fâ€² and gâ€² are the derivatives of f and g from (1).
This characterizes the required moments in terms of the likelihood f(zi) = âˆ’log p(zi |
Î², Ïƒ) and penalty function g(Î²j) = âˆ’log p(Î²j | Ï„). One caveat is that when Î²j âˆ’ÂµÎ² â‰ˆ0,
the conditional moment for Î»j in the expectation step may be numerically inï¬nite, and care
must be taken. Indeed, inï¬nite values for Î»j will arise naturally under certain sparsityinducing choices of g, and indicate that the algorithm has found a sparse solution. One way
to handle the resulting problem of numerical inï¬nities is to start the algorithm from a value
where (Î² âˆ’ÂµÎ²) has no zeros, and to remove Î²j from the model when it gets too close to its
prior mean. This conveys the added beneï¬t of hastening the matrix computations in the
maximization step. Although we have found this approach to work well in practice, it has the
disadvantage that a variable cannot re-enter the model once it has been deleted. Therefore,
once a putative zero has been found, we propose small perturbations in each component of
Î² to assess whether any variables should re-enter the model. Our method does not sidestep
the problem of optimization over a combinatorially large space. In particular, it does not
guarantee convergence to a global maximum if the penalty function is not convex, in which
case restarts from diï¬€erent initial values will be necessary to check for the presence of local
The maximization step
Returning to (5), the maximization step involves computing the posterior mode under a
heteroscedastic Gaussian error model and a conditionally Gaussian prior for Î².
Proposition 2. Suppose that the objective function L(Î²) in (1) can be represented by
variance-mean Gaussian mixtures, as in (2)-(3). Then given estimates Ë†Ï‰i and Ë†Î»j, we have
the following expressions for the conditional maximum of Î², where Ï‰ = (Ï‰1, . . . , Ï‰n) and
Î» = (Î»1, . . . , Î»p) are vectors, and where â„¦= diag(Ï‰1, . . . , Ï‰n) and Î› = diag(Î»1, . . . , Î»p).
1. In a regression problem,
 Ï„ âˆ’2Ë†Î›+XT Ë†â„¦X
âˆ’1(yâ‹†+bâ‹†) , yâ‹†= XT 
Ë†â„¦y âˆ’ÂµzÏ‰ âˆ’Îºzâƒ—1
, bâ‹†= Ï„ âˆ’2(ÂµÎ²Î»+ÎºÎ²âƒ—1n) ,
where âƒ—1n indicates a vector of ones.
2. In a binary classiï¬cation problem where yi = Â±1 and Xâ‹†has rows xâ‹†
Ï„ âˆ’2Ë†Î› + XT
Âµz Ë†Ï‰ + Îºzâƒ—1
One may also use a series of conditional maximization steps for the regression coeï¬ƒcients
Î² and for hyperparameters such as Ïƒ and Ï„. These latter steps exploit standard results on
variance components in linear models, which can be found in, for example, Gelman .
Quasi-Newton acceleration
There are many strategies for speeding up the expectationâ€“maximization algorithm.
very simple approach that requires no extra analytical work, at least in our case, is to
use quasi-Newton acceleration.
The idea is to decompose the observed-data likelihood
as L(Î²) = C(Î²) âˆ’R(Î²), where C(Î²) is the complete-data contribution and H(Î²) is an
unknown remainder term. This leads to a corresponding decomposition for the Hessian of
L(Î²): âˆ’âˆ‡2L(Î²) = âˆ’âˆ‡2C(Î²)+âˆ‡2R(Î²). In models of the form (2)â€“(3), âˆ’âˆ‡2C(Î²) is simply
the inverse covariance matrix of the conditional normal distribution for Î², already given.
The Hessian of the remainder, âˆ‡2R(Î²), can be iteratively approximated using a series
of numerically inexpensive rank-one updates. The approximate Hessian ËœH = âˆ‡2L(Î²) âˆ’
âˆ‡2R(Î²) is then used in a Newton-like step to yield the next iterate, along with the next
update to âˆ‡2R(Î²).
We omit the updating formula itself, along with the strategy by which one ensures that
âˆ’âˆ‡2L(Î²) is always positive deï¬nite and that each iteration monotonically increases the
posterior density. A full explanation for the general case can be found in Lange .
Below, however, we show that quasi-Newton acceleration can oï¬€er a dramatic speed-up for
variance-mean mixtures, compared to a naÂ¨Ä±ve implementation of expectationâ€“maximization.
Logistic regression
Suppose we wish to ï¬t a logistic regression where
Ë†Î² = arg min
log{1 + exp(âˆ’yixT
assuming that the outcomes yi are coded as Â±1, and that Ï„ is ï¬xed. To represent the binary
logistic likelihood as a mixture, let Ï‰âˆ’1 have a Polya distribution with Î± = 1, Îº = 1/2.
Proposition 1 gives the relevant conditional moment as
1 + ezi âˆ’1
Therefore, if the log prior g satisï¬es (3), then the following three updates will generate a
sequence of estimates that converges to stationary point of the posterior:
Ï„ âˆ’2Ë†Î›(g) + XT
1 + ez(g+1)
ÎºÎ² + Ï„ 2 gâ€²(Î²(g+1)
where z(g)
i Î²(g), Xâ‹†is the matrix having rows xâ‹†
i = yixi, and â„¦= diag(Ï‰1, . . . , Ï‰n)
and Î› = diag(Î»1, . . . , Î»p) are diagonal matrices.
This sequence of steps resembles iteratively re-weighted least squares due to the presence
of the diagonal weights matrix â„¦. But there are subtle diï¬€erences, even in the unpenalized
case where Î»j â‰¡0 and the solution is the maximum-likelihood estimator. In iteratively reweighted least squares, the analogous weight matrix â„¦has diagonal entries Ï‰i = Âµi(1 âˆ’Âµi),
where Âµi = 1/(1+eâˆ’xT
i Î²) is the estimated value of pr(yi = 1) at each stage of the algorithm.
These weights arise from the expected information matrix, given the current parameter
estimate. They decay to zero more rapidly, as a function of the linear predictor xT
the weights in our algorithm. This can lead to numerical diï¬ƒculties when the successes and
failures are nearly separable by a hyperplane in Rp, or when the algorithm is initialized far
from the solution.
To illustrate this point, we ran a series of numerical experiments with pure maximumlikelihood estimation as the goal. In each case we simulated a logistic-regression problem
with standard normal coeï¬ƒcients Î²j. Two diï¬€erent design matrices were used. The ï¬rst
problem had p = 100 and n = 104, and exhibited modest collinearity: each row xi was
drawn from a 10-dimensional factor model, xi = Bfi + ai. The 100 Ã— 10 factor-loadings
matrix B, the factors fi, and the idiosyncratic aij were all simulated from a standard normal
distribution. The second problem was larger, but nearly orthogonal: p = 500, n = 5 Ã— 104,
with each xij drawn independently from a standard normal distribution.
For each data set, a logistic-regression model was estimated using iteratively re-weighted
least squares; expectationâ€“maximization, both with and without quasi-Newton acceleration;
the nonlinear conjugate gradient method; and the nonlinear quasi-Newton method due to
Broyden, Fletcher, Goldfarb, and Shanno. These last two methods require the gradient of
the logistic-regression likelihood, which is available in closed form. Further details can be
found in Chapters 5 and 6 of Nocedal & Wright .
We ran the algorithms from two diï¬€erent starting values: Î²(0)
= 10âˆ’3 for all j, and
a random starting location in the hypercube [âˆ’1, 1]p. In the latter case the same random
location was used for all methods. All calculations were performed in R on a standard desktop computer with 8 processor cores running at 2Â·66 gigahertz.
We avoided the potential ineï¬ƒencies of R as much as possible by calling pre-compiled
routines for multi-threaded matrix operations, non-linear gradient-based optimization, and
iteratively re-weighted least squares. Code implementing all the experiments is available
from the authors.
Table 2 shows the run times for each method. These timings depend upon many fac-
Run times in seconds on two simulated problems for each logistic-regression
algorithm.
p = 100, n = 104
p = 500, n = 5 Ã— 104
Initial value
Accelerated EM
Quasi-Newton BFGS
Nonlinear conjugate gradient
tors speciï¬c to a particular computing environment, including the degree to which the
sub-routines of each algorithm are able to exploit a multi-core architecture, and the way
optimization and matrix operations are performed in R. Thus one should not read too much
into the precise values.
Nonetheless, some tentative conclusions may be drawn. In both the collinear and nearly
orthogonal cases, the iteratively re-weighted least-squares algorithm proved sensitive to the
choice of starting value. It converged when all components of Î² were initialized to 10âˆ’3, but
not when initialized to a random point in [âˆ’1, 1]p. This reï¬‚ects the fact that a quadratic
approximation to the logistic likelihood can be poor in regions far from the solution. This
may lead to an ill-conditioned linear system in the initial iterations of the algorithm, which
is sometimes severe enough to cause divergence unless some form of trust-region strategy is
The basic version of the expectation-maximization algorithm is slow but robust, converging in all cases. Moreoever, combining expectation-maximization with quasi-Newton
acceleration led to an algorithm that was equally robust, and faster than all other algorithms
Penalized logistic regression
We also investigated the performance of data augmentation versus iteratively re-weighted
penalized least squares. For this case we simulated data with a nontrivial correlation structure in the design matrix. Let Î£ = BBT + Î¨, where B is a 50 Ã— 4 matrix of standard
normal random entries, and Î¨ is a diagonal matrix with Ï‡2
1 random entries. The rows of
the design matrix X were simulated from a multivariate normal distribution with mean zero
and covariance matrix Î£, and the coeï¬ƒcients Î²j were standard normal random draws. The
size of the data set was p = 50 and n = 200. We used a ridge-regression penalty, along with
the generalized double-Pareto model where p(Î²j | Ï„) âˆ{1 + |Î²j|/(aÏ„)}âˆ’(1+a) . This is non-diï¬€erentiable at zero and is therefore sparsity-inducing, but has
polynomial tails. It also has a conditionally Gaussian representation, making Proposition
1 applicable.
We chose a = 2, and used each algorithm to compute a full solution path for Î² as a
function of the regularization parameter, here expressed as log(1/Ï„) in keeping with the
penalized-likelihood literature. Each solution was initially computed for Ï„1 = 10âˆ’3, thereby
constraining all coeï¬ƒcients to be zero or very small. The value of Ï„ was then increased
Solution path with double-Pareto penalty
Log of regularization parameter
Coefficients
Solution path with ridge penalty
Log of regularization parameter
Coefficients
Figure 1: The solution paths for Î² for the double-Pareto and ridge penalties, as a function
of the regularization parameter log(1/Ï„), for a simulated logistic regression problem. The
black lines show the solution for iteratively re-weighted penalized least squares; the grey
lines, for expectation-maximization. The black lines stop where iteratively re-weighted least
squares fails due to numerical instability.
along a discrete grid {Ï„1, . . . , Ï„K = 1000}, using the solution for Ï„k as the starting value for
the Ï„k+1 case.
As Figure 1 shows, iteratively re-weighted least squares failed when log(1/Ï„) became
too small, causing the linear system that must be solved at each stage of the algorithm to
be numerically singular. This happened before all coeï¬ƒcients had entered the model, and
when 20 out of 200 observations still had ï¬tted success probabilities between 0Â·05 and 0Â·95.
Sparse logistic regression via penalized likelihood is a topic of great current interest
 . This problem involves three distinct issues: how to
handle the logistic likelihood; how to choose a penalty function; and how to ï¬t the resulting
These issues interact in poorly understood ways.
For example, coordinate-wise
algorithms, including Gibbs sampling, can fare poorly in multimodal situations. Nonconvex
penalties lead to multimodal objective functions, but also, subject to certain regularity
conditions, exhibit more favorable statistical properties for estimating sparse signals . Moreover, coordinate descent is tractable only if the
chosen penalty leads to a univariate thresholding function whose solution is analytically
available . This is a fairly narrow class, and does not include most
of the penalties mentioned in the introduction.
The question of how to handle the likelihood further complicates matters. For example,
the area of detail in Figure 1 shows that, for a double-Pareto penalty, the solution paths ï¬t
by iteratively re-weighted penalized least squares diï¬€er in subtle but noticeable ways from
those ï¬t by expectation-maximization. By checking the maximized value of the objective
function under both methods, we are able to conï¬rm that iteratively re-weighted penalized
least squares does not yield the true optimum. Yet we do not entirely understand why, and
under what circumstances, the methods will diï¬€er, and how these diï¬€erences should aï¬€ect
Table 3: Results of the quantile-regression simulation study.
Unpenalized
Double Pareto
Estimation error
Out-of-sample check loss
Average model size
recommendations about what penalty function and algorithm should be used to ï¬t logistic
regression models. A full study of these issues is beyond the scope of the current paper,
but is a subject of active inquiry.
Penalized quantile regression
Next, we show how our data-augmentation scheme can be used to ï¬t penalized quantileregression models, and we compare these ï¬ts to the corresponding unpenalized estimates.
Choose p(Ï‰i) to be a generalized inverse-Gaussian prior of unit scale, where (Î±, Îº, Âµ) =
(1, 1 âˆ’2q, 0). This gives âˆ’log p(zi) = |zi| + (2q âˆ’1)zi, the pseudo-likelihood which yields
quantile regression for the qth quantile . Applying Proposition 1, we ï¬nd that Ë†Ï‰i = |yi âˆ’xT
i Î²(g)|âˆ’1 as the expected value of the conditional suï¬ƒcient
statistic needed in the expectation step of our algorithm.
To study the method, we simulated 50 data sets with p = 25, n = 50, and Î² =
(5, 4, 3, 2, 1, 0, . . . , 0)T . In each case the 90th percentile of the data was a linear function
of Î² with i.i.d. N(0, 1) design points. Noise was added by simulating errors from a normal
distribution whose 90th percentile was the linear predictor xT
i Î², and whose variance was
Ïƒ2 = 52. For each data set, we ï¬t three models: traditional quantile regression using the R
package from Koenker , along with quantile regression under the lasso penalty and
the generalized double-Pareto penalty with Î± = 3. For the second and third method, the
scale of regularization Ï„ was chosen by cross validation. Performance was measured by
squared-error loss in estimating Î², and out-of-sample check loss on a new data set of the
same size, simulated from the same model.
The results are in Table 3.
Both regularized versions of quantile regression for the
90th percentile seem to outperform the straight estimator.
No signiï¬cant diï¬€erences in
performance emerged between the lasso and double-Pareto penalties, although the double-
Pareto solutions were systematically more sparse.
Discussion
Our primary goal in this paper has been to show the relevance of the conditionally Gaussian
representation of (2)â€“(3), together with Proposition 1, for ï¬tting a wide class of regularized
estimators within a uniï¬ed variance-mean mixture framework. We have therefore focused
only on the most basic implementation of the expectation-maximization algorithm, together
with quasi-Newton acceleration.
There are many variants of the expectation-maximization algorithm, however, some of
which can lead to dramatic improvements .
These variants include parameter expansion , majorizationâ€“minimization
 , the partially collapsed Gibbs sampler ,
and simulation-based alternatives . Many of these modiï¬cations
require additional analytical work for particular choices of g and f.
One example here
includes the work of Liu on the robit model. We have not explored these options
here, and this remains a promising area for future research.
A second important fact is that, for many purposes, such as estimating Î² under squarederror loss, the relevant quantity of interest is the posterior mean rather than the mode.
Indeed, both Hans and Efron argue that, for predicting future observables, the
posterior mean of Î² is the better choice. The following proposition represents the posterior
mean for Î² in terms of the score function of the predictive distribution, generalizing the
results of Brown , Masreliez , Pericchi & Smith , and Carvalho et al.
There are a number of possible versions of such a result.
Here we consider a
variance-mean mixture prior p(Î²j) with a location likelihood p(y âˆ’Î²), but a similar result
holds the other way around.
Proposition 3. Let p(|y âˆ’Î²j|) be the likelihood for a location parameter Î²j, symmetric in
y âˆ’Î², and let p(Î²j) =
Ï†(Î²j; ÂµÎ² + ÎºÎ²/Î»j, Ï„ 2/Î»j) p(Î»âˆ’1
be a normal variance-mean
mixture prior. Deï¬ne the following quantities:
p(y âˆ’Î²j)p(Î²j) dÎ²j ,
Ï†(Î²j; Âµ + Îº/Î»j, Ï„ 2/Î»j)pâ‹†(Î»âˆ’1
p(y âˆ’Î²j)pâ‹†(Î²j) .
E(Î²j | y) = âˆ’ÎºÎ²
 âˆ‚log mâ‹†(y)
The generalization to nonorthogonal designs is straightforward, following the original
Masreliez paper. See, for example, Griï¬ƒn & Brown , along with the discussion
of the Tweedie formula by Efron .
Computing the posterior mean will typically require sampling from the full joint posterior distribution over all parameters. Our data-augmentation approach can lead to Markovchain Monte Carlo sampling schemes for just this purpose . The
key step is the identiï¬cation of the conditional posterior distributions for Î»j and Ï‰i. We
have made some initial progress for logistic and negative-binomial models, described in a
technical report available from the second authorâ€™s website. This remains an active area of
Acknowledgements
The authors wish to thank two anonymous referees, the editor, and the associate editor for
their many helpful comments in improving the paper.
Appendix A: distributional results
Generalized hyperbolic distributions
In all of the following cases, we assume that (Î¸ | v) âˆ¼N (Âµ + Îºv, v), and that v âˆ¼p(v).
Let p(v | Ïˆ, Î³, Î´) be a generalized inverse-Gaussian distribution, following the notation of
Barndorï¬€-Nielsen & Shephard . Consider the special case where Ïˆ = 1 and Î´ = 0, in
which case p(Î¸) is a hyperbolic distribution having density
p(Î¸ | Âµ, Î±, Îº) =
exp {âˆ’Î±|Î¸ âˆ’Âµ| + Îº(Î¸ âˆ’Âµ)} .
When viewed as a pseudo-likelihood or pseudo-prior, the class of generalized hyperbolic
distributions will generate many common objective functions. Choosing (Î±, Îº, Âµ) = (1, 0, 0)
leads to âˆ’log p(Î²j) = |Î²j|, and thus â„“1 regularization. Choosing (Î±, Îº, Âµ) = (1, 1âˆ’2q, 0) gives
âˆ’log p(zi) = |zi| + (2q âˆ’1)zi. This is the check-loss function, yielding quantile regression
for the qth quantile. Finally, choosing (Î±, Îº, Âµ) = (1, 1, 1) leads to the maximum operator:
âˆ’(1/2) log p(zi) = (1/2)|1 âˆ’zi| + (1/2)(1 âˆ’zi) = max(1 âˆ’zi, 0) ,
where zi = yixT
i Î². This is the objective function for support vector machines , and corresponds to the limiting case of a generalized
inverse-Gaussian prior.
Z distributions
Let pP(v | Î±, Î± âˆ’2Îº) be a Polya distribution, which can be represented as an inï¬nite
convolution of exponentials, and leads to a Z distributed marginal . The important result is the following:
pZ(Î¸ | Âµ, Î±, Îº) =
(1 + eÎ¸âˆ’Âµ)2(Î±âˆ’Îº) =
N (Âµ + Îºv, v) pP(v | Î±, Î± âˆ’2Îº) dv .
For logistic regression, choose (Î±, Îº, Âµ) = (1, 1/2, 0), leading to p(zi) = ezi/(1 + ezi)
with zi = yixT
i Î². This corresponds to a limiting improper case of the Polya distribution.
The necessary mixture representation still holds, however, by applying the Fatou-Lebesgue
theorem .
For the multinomial generalization of the logistic model, we require a slight modiï¬cation. Suppose that yi âˆˆ{1, . . . , K} is a category indicator, and that Î²k = (Î²k1, . . . , Î²kp)T
is the block of p coeï¬ƒcients for the kth category.
Let Î·ik = exp
}, where cik(Î²âˆ’k) = log
lÌ¸=k exp , writing the conditional likelihood for Î²k as
L(Î²k | Î²âˆ’k, y) âˆ
{wi(1 âˆ’Î·ik)}I(yiÌ¸=k)
where wi is independent of Î²k and I is the indicator function. Thus the conditional likelihood
for the kth block of coeï¬ƒcients Î²k, given all the other blocks of coeï¬ƒcients Î²âˆ’k, can be
written as a product of n terms, the ith term having a Polya mixture representation with
Îºik = I(yi = k) âˆ’1/2 and Âµik = cik(Î²âˆ’k). This allows regularized multinomial logistic
models to be ï¬t using the same approach of Section 4.1, with each block Î²k updated in a
conditional maximization step.
Appendix B: Proofs
Proposition 1
Since Ï† is a normal kernel,
âˆ‚Ï†(Î²j | ÂµÎ² + ÎºÎ²/Î»j, Ï„ 2/Î»j)
Î²j âˆ’ÂµÎ² âˆ’ÎºÎ²/Î»j
Ï†(Î²j | ÂµÎ² + ÎºÎ²/Î»j, Ï„ 2/Î»j).
We use this fact to diï¬€erentiate
p(Î²j | Ï„) =
Ï†(Î²j | ÂµÎ² + ÎºÎ²/Î»j, Ï„ 2/Î»j) p(Î»j | Ï„) dÎ»j
under the integral sign to obtain
p(Î²j | Ï„) =
Ï†(Î²j | ÂµÎ² + ÎºÎ²/Î»j, Ï„ 2/Î»j)
p(Î»j | Ï„) dÎ»j .
Dividing by p(Î²j | Ï„) and using (7) for the inner function, we get
p(Î²j | Ï„) =
p(Î²j | Ï„) âˆ’
E (Î»j | Î²j, Ï„) .
p(Î²j | Ï„) = ÎºÎ²
Î»j | Î²(g), Ï„, y
Equivalently, in terms of the penalty function âˆ’log p(Î²j | Ï„),
(Î²j âˆ’ÂµÎ²)E (Î»j | Î²j) = ÎºÎ² âˆ’Ï„ 2 âˆ‚
log p(Î²j | Ï„) .
By a similar argument,
(zi âˆ’Âµz)E (Ï‰i | Î², zi, Ïƒ) = Îºz âˆ’Ïƒ2 âˆ‚
log p(zi | Î², Ïƒ) .
We obtain the result using the identities
log p(zi | Î²i) = âˆ’fâ€²(zi | Î², Ïƒ)
log p(Î²j | Ï„) = âˆ’gâ€²(Î²j | Ï„) .
Proposition 2
We derive the expressions for a regression problem, with those for classiï¬cation involving
only small changes. Begin with Equation (5). Collecting terms, we can represent the log
posterior, up to an additive constant not involving Î², as a sum of quadratic forms in Î²:
log p(Î² | Ï‰, Î», Ï„, Ïƒ, z) =
{y âˆ’Âµzâƒ—1 âˆ’ÎºzÏ‰âˆ’1} âˆ’XÎ²
{y âˆ’Âµzâƒ—1 âˆ’ÎºzÏ‰âˆ’1} âˆ’XÎ²
Î² âˆ’Âµbâƒ—1 âˆ’ÎºÎ²Î»âˆ’1T
Î² âˆ’ÂµÎ²âƒ—1 âˆ’ÎºÎ²Î»âˆ’1
recalling that Ï‰âˆ’1 and Î»âˆ’1 are column vectors. This is the log posterior under a normal prior
Î² âˆ¼N(ÂµÎ²âƒ—1 + ÎºÎ²Î»âˆ’1, Ï„ 2Î›âˆ’1) after having observed the working response y âˆ’Âµzâƒ—1 âˆ’ÎºzÏ‰âˆ’1.
The identity â„¦(Âµâƒ—1 + ÎºÏ‰âˆ’1) = ÂµÏ‰ + Îºâƒ—1 then gives the result.
For classiï¬cation, on the other hand, let Xâ‹†be the matrix with rows xâ‹†
i = yixi. The
kernel of the conditionally normal likelihood then becomes (Xâ‹†Î² âˆ’Âµzâƒ—1âˆ’ÎºzÏ‰âˆ’1)T â„¦(Xâ‹†Î² âˆ’
Âµzâƒ—1 âˆ’ÎºzÏ‰âˆ’1). Hence it is as if we observe the n-dimensional working response Âµzâƒ—1 + ÎºzÏ‰âˆ’1
in a regression model having design matrix Xâ‹†.
Proposition 3
Our extension of Masreliezâ€™s theorem to variance-mean mixtures follows a similar path as
Proposition 1. Since Ï† is a normal kernel, we may apply (7), giving
Î²jÏ†(Î²j | ÂµÎ² + ÎºÎ²/Î»j, Ï„ 2/Î»j) = ÂµÎ² âˆ’ÎºÎ²
âˆ’âˆ‚Ï†(Î²j | ÂµÎ² + ÎºÎ²/Î»j, Ï„ 2/Î»j)
The rest of the argument follows the standard Masreliez approach.