Data augmentation for non-Gaussian regression models using
variance-mean mixtures
Nicholas G. Polson
Booth School of Business
University of Chicago
James G. Scott
University of Texas at Austin
November 26, 2024
We use the theory of normal variance-mean mixtures to derive a data-augmentation
scheme for a class of common regularization problems. This generalizes existing theory
on normal variance mixtures for priors in regression and classiﬁcation. It also allows
variants of the expectation-maximization algorithm to be brought to bear on a wider
range of models than previously appreciated. We demonstrate the method on several
examples, including sparse quantile regression and binary logistic regression. We also
show that quasi-Newton acceleration can substantially improve the speed of the algorithm without compromising its robustness.
Key words: Data augmentation; expectation-maximization; Sparsity; Variance–mean
mixture of normals
Introduction
Regularized regression and classiﬁcation
Many problems in regularized estimation involve an objective function of the form
i β | σ) +
g(βj | τ) .
Here yi is a response, which may be continuous, binary, or multinomial; xi is a known
p-vector of predictors; β = (β1, . . . , βp) is an unknown vector of coeﬃcients; f and g are
the negative log likelihood and penalty function, respectively; and σ and τ are scale parameters, for now assumed ﬁxed. We may phrase the problem as one of minimizing L(β), or
equivalently maximizing the unnormalized posterior density exp{−L(β)}, interpreting the
penalty as a negative log prior.
 
In this paper, we unify many disparate problems of this form into a single class of normal
variance-mean mixtures. This has two practical consequences. First, it facilitates the use of
penalty functions in models with rich structure, such as hierarchical non-Gaussian models,
discrete mixtures of generalized linear models, and missing-data problems.
Second, it can be used to derive simple expectation-maximization algorithms for non-
Gaussian models. This leads to a stable, uniﬁed approach to estimation in many problems,
including quantile regression, logistic regression, negative-binomial models for count outcomes, support-vector machines, and robust regression.
The key result is Proposition 1, which describes a simple relationship between the
derivatives of f and g and the conditional suﬃcient statistics that arise in our expectationmaximization algorithm. The expected values of these conditional suﬃcient statistics can
usually be calculated in closed form, even if the full conditional distribution of the latent
variables is unknown or intractable. Proposition 3 provides the correponding result for the
posterior mean estimator, generalizing the results of Masreliez and Pericchi & Smith
One known disadvantage of expectation–maximization algorithms is that they may converge slowly, particularly if the fraction of missing information is large . Our method, in its basic form, is no exception. But we ﬁnd that substantial gains
in speed are possible using quasi-Newton acceleration .
The resulting approach combines the best features of the expectation–maximization and Newton–Raphson
algorithms: robustness and guaranteed ascent when far from the solution, and super-linear
convergence when close to it.
Relationship with previous work
Our work is motivated by recent Bayesian research on sparsity-inducing priors in linear
regression, where f = ∥y −Xβ∥2 is the negative Gaussian log likelihood, and g corresponds
to a normal variance-mixture prior . Examples of work in this
area include the lasso ; the bridge estimator ; the relevance vector machine of
Tipping ; the normal/Jeﬀreys prior of Figueiredo and Bae & Mallick ;
the normal/exponential-gamma model of Griﬃn & Brown ; the normal/gamma and
normal/inverse-Gaussian models ; the horseshoe prior of Carvalho et al. ; the hypergeometric inverted–beta model of Polson &
Scott ; and the double-Pareto model of Armagan et al. . See Polson & Scott
 for a review.
We generalize this work by representing both the likelihood and prior as variance-mean
mixtures of Gaussians. This data-augmentation approach relies upon the following decom-
Table 1: Variance-mean mixture representations for many common loss functions. Recall
that zi = yi −xT
i β for regression, or zi = yixT
i β for binary classiﬁcation.
Error/loss function
f(zi | β, σ)
Squared-error
Absolute-error
Exponential
Check loss
|zi| + (2q −1)zi
Generalized inverse Gaussian
Support vector machines
max(1 −zi, 0)
Generalized inverse Gaussian
log(1 + ezi)
p(β | τ, σ, y) ∝e−L(β)
i β | σ) −
p(zi | β, σ)
= p(z | β, σ) p(β | τ) ,
where the working response zi is equal to yi −xT
i β for Gaussian regression, or yixT
binary classiﬁcation using logistic regression or support-vector machines. Here yi is coded as
±1. Both σ and τ are hyperparameters; they are typically estimated jointly with β, although
they may also be speciﬁed by the user or chosen by cross-validation. In some cases, most
notably in logistic regression, the likelihood is free of hyperparameters, in which case σ does
not appear in the model. Previous studies have presented similar results for speciﬁc models, including support-vector machines
and the so-called powered-logit likelihood. Our paper characterizes the general case.
One thing we do not do is to study the formal statistical properties of the resulting
estimators, such as consistency as p and n diverge. For this we refer the reader to Fan &
Li and Griﬃn & Brown , who discuss this issue from classical and Bayesian
viewpoints, respectively.
Normal variance-mean mixtures
There are two key steps in our approach. First, we use variance-mean mixtures, rather
than just variance mixtures. Second, we interweave two diﬀerent mixture representations,
one for the likelihood and one for the prior. The introduction of latent variables {ωi} and
{λj} in Equations (2) and (3), below, reduces exp{−L(β)} to a Gaussian linear model with
heteroscedastic errors:
p(zi | β, σ) =
φ(zi | µz + κzω−1
) dP(ωi) ,
p(βj | τ) =
φ(βj | µβ + κβλ−1
j , τ 2λ−1
j ) dP(λj) ,
where φ(a | m, v) is the normal density, evaluated at a, for mean m and variance v.
By marginalizing over these latent variables with respect to diﬀerent ﬁxed combinations
of (µz, κz, µβ, κβ) and the mixing measures P(λj) and P(ωi), it is possible to generate
many commonly used objective functions that have not been widely recognized as Gaussian
mixtures. Table 1 lists several common likelihoods in this class, along with the corresponding
ﬁxed choices for (κz, µz) and the mixing distribution P(ωi).
A useful fact is that one may avoid dealing directly with conditional distributions for
these latent variables. To ﬁnd the posterior mode, it is suﬃcient to use Proposition 1 to
calculate moments of these distributions. These moments in turn depend only upon the
derivatives of f and g, along with the hyperparameters.
We focus on two choices of the mixing measure: the generalized inverse-Gaussian distribution and the Polya distribution, the latter being an inﬁnite sum of exponential random
variables . These choices lead to the hyperbolic and Z distributions, respectively, for the variance-mean mixture.
The two key integral identities
e−α|θ−µ|+κ(θ−µ) =
φ (θ | µ + κv, v) pG
v | 1, 0, (α2 −κ2)1/2
(1 + eθ−µ)2(α−κ) =
φ (θ | µ + κv, v) pP(v | α, α −2κ) dv ,
where pG and pP are the density functions of the generalized inverse-Gaussian and Polya
distributions, respectively.
For details, see Appendix A. We use θ to denote a dummy
argument that could involve either data or parameters, and v to denote a latent variance.
All other terms are pre-speciﬁed in order to represent a particular density or function. These
two expressions lead, by an application of the Fatou–Lebesgue theorem, to three further
identities for the improper limiting cases of the two densities above:
−2c−1 max(aθ, 0)
φ(θ | −av, cv) dv ,
−2c−1ρq(θ)
φ(θ | v −2τv, cv)e−2τ(1−τ)v dv ,
(1 + exp{θ −µ})−1 =
φ(θ | µ −v/2, v) pP(v | 0, 1) dv ,
where ρq(θ) = |θ|/2 + (q −1/2) θ is the check-loss function. The ﬁrst leads to the pseudolikelihood for support-vector machines, the second to quantile and lasso regression, and the
third to logistic and negative binomial regression under their canonical links. The function
pP(v | 0, 1) is an improper density corresponding to a sum of exponential random variables.
In the case of a multinomial model, the conditional posterior for a single category, given
the parameters for other categories, is a binary logistic model.
An expectation-maximization algorithm
Overview of approach
By exploiting conditional Gaussianity, models of the form (2)–(3) can be ﬁt using an
expectation-maximization algorithm . In the expectation step, one
computes the expected value of the log posterior, given the current estimate β(g):
C(β | β(g)) =
log p(β | ω, λ, τ, σ, z)p(ω, λ | β(g), τ, z) dω dλ .
Then in the maximization step, one maximizes the complete-data posterior as a function of
β. The same representation can also be used to derive Markov-chain sampling algorithms,
although we do not explore this aspect of the approach.
The expectation-maximization algorithm has several advantages here compared to other
iterative methods. It is highly robust to the choice of initial value, has no user-speciﬁed
tuning constants, and leads to a sequence of estimates {β(1), β(2), . . . } that monotonically
increase the observed-data log posterior density. The disadvantage is the potential for slow
convergence. For the models we consider, this disadvantage is real, but can be avoided using
quasi-Newton acceleration.
The complete-data log posterior can be represented in two ways. First, we have
log p(β | ω, λ, τ, σ, z) = c0(ω, λ, τ, σ, z) −
 zi −µz −κyω−1
λj(βj −µβ −κβλ−1
for some constant c0, recalling that zi = yi −xT
i β for regression or zi = yixT
i β for classiﬁcation. Factorizing this further as a function of β yields a second representation:
log p(β | ω, λ, τ, σ, z) = c1(ω, λ, τ, σ, z) −
ωi (zi −µz)2 + κy
λj(βj −µβ)2 + κβ
for some constant c1. We now derive the expectation and maximization steps, along with
the complete-data suﬃcient statistics.
The expectation step
From (6), observe that the complete-data objective function is linear in both ωi and λj.
Therefore, in the expectation step, we replace λj and ωi with their conditional expectations
, given the data and the current β(g). The following result provides expressions
for these conditional moments under any model where both the likelihood and prior can be
represented by normal variance-mean mixtures.
Proposition 1. Suppose that the objective function L(β) in (1) can be represented by
a hierarchical variance-mean Gaussian mixture, as in Equations (2) and (3). Then the
conditional moments ˆλj = E (λj | β, τ, z) and ˆωi = E (ωi | σ, zi) are given by
(βj −µβ)ˆλj = κβ + τ 2 g′(βj | τ) ,
(zi −µz)ˆωi = κz + σ2 f′(zi | β, σ) ,
where f′ and g′ are the derivatives of f and g from (1).
This characterizes the required moments in terms of the likelihood f(zi) = −log p(zi |
β, σ) and penalty function g(βj) = −log p(βj | τ). One caveat is that when βj −µβ ≈0,
the conditional moment for λj in the expectation step may be numerically inﬁnite, and care
must be taken. Indeed, inﬁnite values for λj will arise naturally under certain sparsityinducing choices of g, and indicate that the algorithm has found a sparse solution. One way
to handle the resulting problem of numerical inﬁnities is to start the algorithm from a value
where (β −µβ) has no zeros, and to remove βj from the model when it gets too close to its
prior mean. This conveys the added beneﬁt of hastening the matrix computations in the
maximization step. Although we have found this approach to work well in practice, it has the
disadvantage that a variable cannot re-enter the model once it has been deleted. Therefore,
once a putative zero has been found, we propose small perturbations in each component of
β to assess whether any variables should re-enter the model. Our method does not sidestep
the problem of optimization over a combinatorially large space. In particular, it does not
guarantee convergence to a global maximum if the penalty function is not convex, in which
case restarts from diﬀerent initial values will be necessary to check for the presence of local
The maximization step
Returning to (5), the maximization step involves computing the posterior mode under a
heteroscedastic Gaussian error model and a conditionally Gaussian prior for β.
Proposition 2. Suppose that the objective function L(β) in (1) can be represented by
variance-mean Gaussian mixtures, as in (2)-(3). Then given estimates ˆωi and ˆλj, we have
the following expressions for the conditional maximum of β, where ω = (ω1, . . . , ωn) and
λ = (λ1, . . . , λp) are vectors, and where Ω= diag(ω1, . . . , ωn) and Λ = diag(λ1, . . . , λp).
1. In a regression problem,
 τ −2ˆΛ+XT ˆΩX
−1(y⋆+b⋆) , y⋆= XT 
ˆΩy −µzω −κz⃗1
, b⋆= τ −2(µβλ+κβ⃗1n) ,
where ⃗1n indicates a vector of ones.
2. In a binary classiﬁcation problem where yi = ±1 and X⋆has rows x⋆
τ −2ˆΛ + XT
µz ˆω + κz⃗1
One may also use a series of conditional maximization steps for the regression coeﬃcients
β and for hyperparameters such as σ and τ. These latter steps exploit standard results on
variance components in linear models, which can be found in, for example, Gelman .
Quasi-Newton acceleration
There are many strategies for speeding up the expectation–maximization algorithm.
very simple approach that requires no extra analytical work, at least in our case, is to
use quasi-Newton acceleration.
The idea is to decompose the observed-data likelihood
as L(β) = C(β) −R(β), where C(β) is the complete-data contribution and H(β) is an
unknown remainder term. This leads to a corresponding decomposition for the Hessian of
L(β): −∇2L(β) = −∇2C(β)+∇2R(β). In models of the form (2)–(3), −∇2C(β) is simply
the inverse covariance matrix of the conditional normal distribution for β, already given.
The Hessian of the remainder, ∇2R(β), can be iteratively approximated using a series
of numerically inexpensive rank-one updates. The approximate Hessian ˜H = ∇2L(β) −
∇2R(β) is then used in a Newton-like step to yield the next iterate, along with the next
update to ∇2R(β).
We omit the updating formula itself, along with the strategy by which one ensures that
−∇2L(β) is always positive deﬁnite and that each iteration monotonically increases the
posterior density. A full explanation for the general case can be found in Lange .
Below, however, we show that quasi-Newton acceleration can oﬀer a dramatic speed-up for
variance-mean mixtures, compared to a na¨ıve implementation of expectation–maximization.
Logistic regression
Suppose we wish to ﬁt a logistic regression where
ˆβ = arg min
log{1 + exp(−yixT
assuming that the outcomes yi are coded as ±1, and that τ is ﬁxed. To represent the binary
logistic likelihood as a mixture, let ω−1 have a Polya distribution with α = 1, κ = 1/2.
Proposition 1 gives the relevant conditional moment as
1 + ezi −1
Therefore, if the log prior g satisﬁes (3), then the following three updates will generate a
sequence of estimates that converges to stationary point of the posterior:
τ −2ˆΛ(g) + XT
1 + ez(g+1)
κβ + τ 2 g′(β(g+1)
where z(g)
i β(g), X⋆is the matrix having rows x⋆
i = yixi, and Ω= diag(ω1, . . . , ωn)
and Λ = diag(λ1, . . . , λp) are diagonal matrices.
This sequence of steps resembles iteratively re-weighted least squares due to the presence
of the diagonal weights matrix Ω. But there are subtle diﬀerences, even in the unpenalized
case where λj ≡0 and the solution is the maximum-likelihood estimator. In iteratively reweighted least squares, the analogous weight matrix Ωhas diagonal entries ωi = µi(1 −µi),
where µi = 1/(1+e−xT
i β) is the estimated value of pr(yi = 1) at each stage of the algorithm.
These weights arise from the expected information matrix, given the current parameter
estimate. They decay to zero more rapidly, as a function of the linear predictor xT
the weights in our algorithm. This can lead to numerical diﬃculties when the successes and
failures are nearly separable by a hyperplane in Rp, or when the algorithm is initialized far
from the solution.
To illustrate this point, we ran a series of numerical experiments with pure maximumlikelihood estimation as the goal. In each case we simulated a logistic-regression problem
with standard normal coeﬃcients βj. Two diﬀerent design matrices were used. The ﬁrst
problem had p = 100 and n = 104, and exhibited modest collinearity: each row xi was
drawn from a 10-dimensional factor model, xi = Bfi + ai. The 100 × 10 factor-loadings
matrix B, the factors fi, and the idiosyncratic aij were all simulated from a standard normal
distribution. The second problem was larger, but nearly orthogonal: p = 500, n = 5 × 104,
with each xij drawn independently from a standard normal distribution.
For each data set, a logistic-regression model was estimated using iteratively re-weighted
least squares; expectation–maximization, both with and without quasi-Newton acceleration;
the nonlinear conjugate gradient method; and the nonlinear quasi-Newton method due to
Broyden, Fletcher, Goldfarb, and Shanno. These last two methods require the gradient of
the logistic-regression likelihood, which is available in closed form. Further details can be
found in Chapters 5 and 6 of Nocedal & Wright .
We ran the algorithms from two diﬀerent starting values: β(0)
= 10−3 for all j, and
a random starting location in the hypercube [−1, 1]p. In the latter case the same random
location was used for all methods. All calculations were performed in R on a standard desktop computer with 8 processor cores running at 2·66 gigahertz.
We avoided the potential ineﬃencies of R as much as possible by calling pre-compiled
routines for multi-threaded matrix operations, non-linear gradient-based optimization, and
iteratively re-weighted least squares. Code implementing all the experiments is available
from the authors.
Table 2 shows the run times for each method. These timings depend upon many fac-
Run times in seconds on two simulated problems for each logistic-regression
algorithm.
p = 100, n = 104
p = 500, n = 5 × 104
Initial value
Accelerated EM
Quasi-Newton BFGS
Nonlinear conjugate gradient
tors speciﬁc to a particular computing environment, including the degree to which the
sub-routines of each algorithm are able to exploit a multi-core architecture, and the way
optimization and matrix operations are performed in R. Thus one should not read too much
into the precise values.
Nonetheless, some tentative conclusions may be drawn. In both the collinear and nearly
orthogonal cases, the iteratively re-weighted least-squares algorithm proved sensitive to the
choice of starting value. It converged when all components of β were initialized to 10−3, but
not when initialized to a random point in [−1, 1]p. This reﬂects the fact that a quadratic
approximation to the logistic likelihood can be poor in regions far from the solution. This
may lead to an ill-conditioned linear system in the initial iterations of the algorithm, which
is sometimes severe enough to cause divergence unless some form of trust-region strategy is
The basic version of the expectation-maximization algorithm is slow but robust, converging in all cases. Moreoever, combining expectation-maximization with quasi-Newton
acceleration led to an algorithm that was equally robust, and faster than all other algorithms
Penalized logistic regression
We also investigated the performance of data augmentation versus iteratively re-weighted
penalized least squares. For this case we simulated data with a nontrivial correlation structure in the design matrix. Let Σ = BBT + Ψ, where B is a 50 × 4 matrix of standard
normal random entries, and Ψ is a diagonal matrix with χ2
1 random entries. The rows of
the design matrix X were simulated from a multivariate normal distribution with mean zero
and covariance matrix Σ, and the coeﬃcients βj were standard normal random draws. The
size of the data set was p = 50 and n = 200. We used a ridge-regression penalty, along with
the generalized double-Pareto model where p(βj | τ) ∝{1 + |βj|/(aτ)}−(1+a) . This is non-diﬀerentiable at zero and is therefore sparsity-inducing, but has
polynomial tails. It also has a conditionally Gaussian representation, making Proposition
1 applicable.
We chose a = 2, and used each algorithm to compute a full solution path for β as a
function of the regularization parameter, here expressed as log(1/τ) in keeping with the
penalized-likelihood literature. Each solution was initially computed for τ1 = 10−3, thereby
constraining all coeﬃcients to be zero or very small. The value of τ was then increased
Solution path with double-Pareto penalty
Log of regularization parameter
Coefficients
Solution path with ridge penalty
Log of regularization parameter
Coefficients
Figure 1: The solution paths for β for the double-Pareto and ridge penalties, as a function
of the regularization parameter log(1/τ), for a simulated logistic regression problem. The
black lines show the solution for iteratively re-weighted penalized least squares; the grey
lines, for expectation-maximization. The black lines stop where iteratively re-weighted least
squares fails due to numerical instability.
along a discrete grid {τ1, . . . , τK = 1000}, using the solution for τk as the starting value for
the τk+1 case.
As Figure 1 shows, iteratively re-weighted least squares failed when log(1/τ) became
too small, causing the linear system that must be solved at each stage of the algorithm to
be numerically singular. This happened before all coeﬃcients had entered the model, and
when 20 out of 200 observations still had ﬁtted success probabilities between 0·05 and 0·95.
Sparse logistic regression via penalized likelihood is a topic of great current interest
 . This problem involves three distinct issues: how to
handle the logistic likelihood; how to choose a penalty function; and how to ﬁt the resulting
These issues interact in poorly understood ways.
For example, coordinate-wise
algorithms, including Gibbs sampling, can fare poorly in multimodal situations. Nonconvex
penalties lead to multimodal objective functions, but also, subject to certain regularity
conditions, exhibit more favorable statistical properties for estimating sparse signals . Moreover, coordinate descent is tractable only if the
chosen penalty leads to a univariate thresholding function whose solution is analytically
available . This is a fairly narrow class, and does not include most
of the penalties mentioned in the introduction.
The question of how to handle the likelihood further complicates matters. For example,
the area of detail in Figure 1 shows that, for a double-Pareto penalty, the solution paths ﬁt
by iteratively re-weighted penalized least squares diﬀer in subtle but noticeable ways from
those ﬁt by expectation-maximization. By checking the maximized value of the objective
function under both methods, we are able to conﬁrm that iteratively re-weighted penalized
least squares does not yield the true optimum. Yet we do not entirely understand why, and
under what circumstances, the methods will diﬀer, and how these diﬀerences should aﬀect
Table 3: Results of the quantile-regression simulation study.
Unpenalized
Double Pareto
Estimation error
Out-of-sample check loss
Average model size
recommendations about what penalty function and algorithm should be used to ﬁt logistic
regression models. A full study of these issues is beyond the scope of the current paper,
but is a subject of active inquiry.
Penalized quantile regression
Next, we show how our data-augmentation scheme can be used to ﬁt penalized quantileregression models, and we compare these ﬁts to the corresponding unpenalized estimates.
Choose p(ωi) to be a generalized inverse-Gaussian prior of unit scale, where (α, κ, µ) =
(1, 1 −2q, 0). This gives −log p(zi) = |zi| + (2q −1)zi, the pseudo-likelihood which yields
quantile regression for the qth quantile . Applying Proposition 1, we ﬁnd that ˆωi = |yi −xT
i β(g)|−1 as the expected value of the conditional suﬃcient
statistic needed in the expectation step of our algorithm.
To study the method, we simulated 50 data sets with p = 25, n = 50, and β =
(5, 4, 3, 2, 1, 0, . . . , 0)T . In each case the 90th percentile of the data was a linear function
of β with i.i.d. N(0, 1) design points. Noise was added by simulating errors from a normal
distribution whose 90th percentile was the linear predictor xT
i β, and whose variance was
σ2 = 52. For each data set, we ﬁt three models: traditional quantile regression using the R
package from Koenker , along with quantile regression under the lasso penalty and
the generalized double-Pareto penalty with α = 3. For the second and third method, the
scale of regularization τ was chosen by cross validation. Performance was measured by
squared-error loss in estimating β, and out-of-sample check loss on a new data set of the
same size, simulated from the same model.
The results are in Table 3.
Both regularized versions of quantile regression for the
90th percentile seem to outperform the straight estimator.
No signiﬁcant diﬀerences in
performance emerged between the lasso and double-Pareto penalties, although the double-
Pareto solutions were systematically more sparse.
Discussion
Our primary goal in this paper has been to show the relevance of the conditionally Gaussian
representation of (2)–(3), together with Proposition 1, for ﬁtting a wide class of regularized
estimators within a uniﬁed variance-mean mixture framework. We have therefore focused
only on the most basic implementation of the expectation-maximization algorithm, together
with quasi-Newton acceleration.
There are many variants of the expectation-maximization algorithm, however, some of
which can lead to dramatic improvements .
These variants include parameter expansion , majorization–minimization
 , the partially collapsed Gibbs sampler ,
and simulation-based alternatives . Many of these modiﬁcations
require additional analytical work for particular choices of g and f.
One example here
includes the work of Liu on the robit model. We have not explored these options
here, and this remains a promising area for future research.
A second important fact is that, for many purposes, such as estimating β under squarederror loss, the relevant quantity of interest is the posterior mean rather than the mode.
Indeed, both Hans and Efron argue that, for predicting future observables, the
posterior mean of β is the better choice. The following proposition represents the posterior
mean for β in terms of the score function of the predictive distribution, generalizing the
results of Brown , Masreliez , Pericchi & Smith , and Carvalho et al.
There are a number of possible versions of such a result.
Here we consider a
variance-mean mixture prior p(βj) with a location likelihood p(y −β), but a similar result
holds the other way around.
Proposition 3. Let p(|y −βj|) be the likelihood for a location parameter βj, symmetric in
y −β, and let p(βj) =
φ(βj; µβ + κβ/λj, τ 2/λj) p(λ−1
be a normal variance-mean
mixture prior. Deﬁne the following quantities:
p(y −βj)p(βj) dβj ,
φ(βj; µ + κ/λj, τ 2/λj)p⋆(λ−1
p(y −βj)p⋆(βj) .
E(βj | y) = −κβ
 ∂log m⋆(y)
The generalization to nonorthogonal designs is straightforward, following the original
Masreliez paper. See, for example, Griﬃn & Brown , along with the discussion
of the Tweedie formula by Efron .
Computing the posterior mean will typically require sampling from the full joint posterior distribution over all parameters. Our data-augmentation approach can lead to Markovchain Monte Carlo sampling schemes for just this purpose . The
key step is the identiﬁcation of the conditional posterior distributions for λj and ωi. We
have made some initial progress for logistic and negative-binomial models, described in a
technical report available from the second author’s website. This remains an active area of
Acknowledgements
The authors wish to thank two anonymous referees, the editor, and the associate editor for
their many helpful comments in improving the paper.
Appendix A: distributional results
Generalized hyperbolic distributions
In all of the following cases, we assume that (θ | v) ∼N (µ + κv, v), and that v ∼p(v).
Let p(v | ψ, γ, δ) be a generalized inverse-Gaussian distribution, following the notation of
Barndorﬀ-Nielsen & Shephard . Consider the special case where ψ = 1 and δ = 0, in
which case p(θ) is a hyperbolic distribution having density
p(θ | µ, α, κ) =
exp {−α|θ −µ| + κ(θ −µ)} .
When viewed as a pseudo-likelihood or pseudo-prior, the class of generalized hyperbolic
distributions will generate many common objective functions. Choosing (α, κ, µ) = (1, 0, 0)
leads to −log p(βj) = |βj|, and thus ℓ1 regularization. Choosing (α, κ, µ) = (1, 1−2q, 0) gives
−log p(zi) = |zi| + (2q −1)zi. This is the check-loss function, yielding quantile regression
for the qth quantile. Finally, choosing (α, κ, µ) = (1, 1, 1) leads to the maximum operator:
−(1/2) log p(zi) = (1/2)|1 −zi| + (1/2)(1 −zi) = max(1 −zi, 0) ,
where zi = yixT
i β. This is the objective function for support vector machines , and corresponds to the limiting case of a generalized
inverse-Gaussian prior.
Z distributions
Let pP(v | α, α −2κ) be a Polya distribution, which can be represented as an inﬁnite
convolution of exponentials, and leads to a Z distributed marginal . The important result is the following:
pZ(θ | µ, α, κ) =
(1 + eθ−µ)2(α−κ) =
N (µ + κv, v) pP(v | α, α −2κ) dv .
For logistic regression, choose (α, κ, µ) = (1, 1/2, 0), leading to p(zi) = ezi/(1 + ezi)
with zi = yixT
i β. This corresponds to a limiting improper case of the Polya distribution.
The necessary mixture representation still holds, however, by applying the Fatou-Lebesgue
theorem .
For the multinomial generalization of the logistic model, we require a slight modiﬁcation. Suppose that yi ∈{1, . . . , K} is a category indicator, and that βk = (βk1, . . . , βkp)T
is the block of p coeﬃcients for the kth category.
Let ηik = exp
}, where cik(β−k) = log
l̸=k exp , writing the conditional likelihood for βk as
L(βk | β−k, y) ∝
{wi(1 −ηik)}I(yi̸=k)
where wi is independent of βk and I is the indicator function. Thus the conditional likelihood
for the kth block of coeﬃcients βk, given all the other blocks of coeﬃcients β−k, can be
written as a product of n terms, the ith term having a Polya mixture representation with
κik = I(yi = k) −1/2 and µik = cik(β−k). This allows regularized multinomial logistic
models to be ﬁt using the same approach of Section 4.1, with each block βk updated in a
conditional maximization step.
Appendix B: Proofs
Proposition 1
Since φ is a normal kernel,
∂φ(βj | µβ + κβ/λj, τ 2/λj)
βj −µβ −κβ/λj
φ(βj | µβ + κβ/λj, τ 2/λj).
We use this fact to diﬀerentiate
p(βj | τ) =
φ(βj | µβ + κβ/λj, τ 2/λj) p(λj | τ) dλj
under the integral sign to obtain
p(βj | τ) =
φ(βj | µβ + κβ/λj, τ 2/λj)
p(λj | τ) dλj .
Dividing by p(βj | τ) and using (7) for the inner function, we get
p(βj | τ) =
p(βj | τ) −
E (λj | βj, τ) .
p(βj | τ) = κβ
λj | β(g), τ, y
Equivalently, in terms of the penalty function −log p(βj | τ),
(βj −µβ)E (λj | βj) = κβ −τ 2 ∂
log p(βj | τ) .
By a similar argument,
(zi −µz)E (ωi | β, zi, σ) = κz −σ2 ∂
log p(zi | β, σ) .
We obtain the result using the identities
log p(zi | βi) = −f′(zi | β, σ)
log p(βj | τ) = −g′(βj | τ) .
Proposition 2
We derive the expressions for a regression problem, with those for classiﬁcation involving
only small changes. Begin with Equation (5). Collecting terms, we can represent the log
posterior, up to an additive constant not involving β, as a sum of quadratic forms in β:
log p(β | ω, λ, τ, σ, z) =
{y −µz⃗1 −κzω−1} −Xβ
{y −µz⃗1 −κzω−1} −Xβ
β −µb⃗1 −κβλ−1T
β −µβ⃗1 −κβλ−1
recalling that ω−1 and λ−1 are column vectors. This is the log posterior under a normal prior
β ∼N(µβ⃗1 + κβλ−1, τ 2Λ−1) after having observed the working response y −µz⃗1 −κzω−1.
The identity Ω(µ⃗1 + κω−1) = µω + κ⃗1 then gives the result.
For classiﬁcation, on the other hand, let X⋆be the matrix with rows x⋆
i = yixi. The
kernel of the conditionally normal likelihood then becomes (X⋆β −µz⃗1−κzω−1)T Ω(X⋆β −
µz⃗1 −κzω−1). Hence it is as if we observe the n-dimensional working response µz⃗1 + κzω−1
in a regression model having design matrix X⋆.
Proposition 3
Our extension of Masreliez’s theorem to variance-mean mixtures follows a similar path as
Proposition 1. Since φ is a normal kernel, we may apply (7), giving
βjφ(βj | µβ + κβ/λj, τ 2/λj) = µβ −κβ
−∂φ(βj | µβ + κβ/λj, τ 2/λj)
The rest of the argument follows the standard Masreliez approach.