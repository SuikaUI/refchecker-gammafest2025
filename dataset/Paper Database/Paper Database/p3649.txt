BART: Denoising Sequence-to-Sequence Pre-training for Natural
Language Generation, Translation, and Comprehension
Mike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad,
Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer
Facebook AI
{mikelewis,yinhanliu,naman}@fb.com
We present BART, a denoising autoencoder
for pretraining sequence-to-sequence models.
BART is trained by (1) corrupting text with an
arbitrary noising function, and (2) learning a
model to reconstruct the original text. It uses
a standard Tranformer-based neural machine
translation architecture which, despite its simplicity, can be seen as generalizing BERT (due
to the bidirectional encoder), GPT (with the
left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, ﬁnding the best performance by both randomly shufﬂing the order of the original sentences and using a novel
in-ﬁlling scheme, where spans of text are replaced with a single mask token.
particularly effective when ﬁne tuned for text
generation but also works well for comprehension tasks. It matches the performance of
RoBERTa with comparable training resources
on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.
BART also provides a 1.1 BLEU increase over
a back-translation system for machine translation, with only target language pretraining. We
also report ablation experiments that replicate
other pretraining schemes within the BART
framework, to better measure which factors
most inﬂuence end-task performance.
Introduction
Self-supervised methods have achieved remarkable
success in a wide range of NLP tasks .
The most successful approaches have been variants of
masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out. Recent
work has shown gains by improving the distribution of
masked tokens , the order in which
masked tokens are predicted , and the
available context for replacing masked tokens . However, these methods typically focus
on particular types of end tasks (e.g. span prediction,
generation, etc.), limiting their applicability.
In this paper, we present BART, which pre-trains
a model combining Bidirectional and Auto-Regressive
Transformers. BART is a denoising autoencoder built
with a sequence-to-sequence model that is applicable
to a very wide range of end tasks.
Pretraining has
two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is
learned to reconstruct the original text. BART uses a
standard Tranformer-based neural machine translation
architecture which, despite its simplicity, can be seen as
generalizing BERT (due to the bidirectional encoder),
GPT (with the left-to-right decoder), and many other
more recent pretraining schemes (see Figure 1).
A key advantage of this setup is the noising ﬂexibility; arbitrary transformations can be applied to the original text, including changing its length. We evaluate
a number of noising approaches, ﬁnding the best performance by both randomly shufﬂing the order of the
original sentences and using a novel in-ﬁlling scheme,
where arbitrary length spans of text (including zero
length) are replaced with a single mask token. This approach generalizes the original word masking and next
sentence prediction objectives in BERT by forcing the
model to reason more about overall sentence length and
make longer range transformations to the input.
BART is particularly effective when ﬁne tuned for
text generation but also works well for comprehension tasks. It matches the performance of RoBERTa
 with comparable training resources
on GLUE and SQuAD , and achieves new state-of-the-art results
on a range of abstractive dialogue, question answering, and summarization tasks.
For example, it improves performance by 6 ROUGE over previous work
on XSum .
BART also opens up new ways of thinking about ﬁne
tuning. We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers. These layers are trained
to essentially translate the foreign language to noised
 
Bidirectional
(a) BERT: Random tokens are replaced with masks, and
the document is encoded bidirectionally. Missing tokens
are predicted independently, so BERT cannot easily be
used for generation.
Autoregressive
<s> A B C D
(b) GPT: Tokens are predicted auto-regressively, meaning
GPT can be used for generation. However words can only
condition on leftward context, so it cannot learn bidirectional interactions.
Autoregressive
Bidirectional
<s> A B C D
(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations. Here, a
document has been corrupted by replacing spans of text with mask symbols. The corrupted document (left) is encoded with
a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.
For ﬁne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the ﬁnal
hidden state of the decoder.
Figure 1: A schematic comparison of BART with BERT and GPT .
English, by propagation through BART, thereby using BART as a pre-trained target-side language model.
This approach improves performance over a strong
back-translation MT baseline by 1.1 BLEU on the
WMT Romanian-English benchmark.
To better understand these effects, we also report
an ablation analysis that replicates other recently proposed training objectives. This study allows us to carefully control for a number of factors, including data
and optimization parameters, which have been shown
to be as important for overall performance as the selection of training objectives . We ﬁnd
that BART exhibits the most consistently strong performance across the full range of tasks we consider.
BART is a denoising autoencoder that maps a corrupted
document to the original document it was derived from.
It is implemented as a sequence-to-sequence model
with a bidirectional encoder over corrupted text and a
left-to-right autoregressive decoder. For pre-training,
we optimize the negative log likelihood of the original
Architecture
BART uses the standard sequence-to-sequence Transformer architecture from , except, following GPT, that we modify ReLU activation functions to GeLUs 
and initialise parameters from N(0, 0.02).
base model, we use 6 layers in the encoder and decoder, and for our large model we use 12 layers in
each. The architecture is closely related to that used in
BERT, with the following differences: (1) each layer of
the decoder additionally performs cross-attention over
the ﬁnal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT
uses an additional feed-forward network before wordprediction, which BART does not. In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.
Pre-training BART
BART is trained by corrupting documents and then optimizing a reconstruction loss—the cross-entropy between the decoder’s output and the original document.
Unlike existing denoising autoencoders, which are tailored to speciﬁc noising schemes, BART allows us to
apply any type of document corruption. In the extreme
case, where all information about the source is lost,
BART is equivalent to a language model.
We experiment with several previously proposed and
novel transformations, but we believe there is a signiﬁcant potential for development of other new alternatives. The transformations we used are summarized
below, and examples are shown in Figure 2.
Token Masking
Following BERT , random tokens are sampled and replaced with
[MASK] elements.
Token Deletion
Random tokens are deleted from the
input. In contrast to token masking, the model must
decide which positions are missing inputs.
A B C . D E .
A . C . E .
A _ . D _ E .
A _C . _ E .
C . D E . A B
Document Rotation
Token Masking
Token Deletion
Text Inﬁlling
D E . A B C .
Sentence Permutation
Figure 2: Transformations for noising the input that we experiment with. These transformations can be composed.
Text Inﬁlling
A number of text spans are sampled,
with span lengths drawn from a Poisson distribution
(λ = 3). Each span is replaced with a single [MASK]
token. 0-length spans correspond to the insertion of
[MASK] tokens.
Text inﬁlling is inspired by Span-
BERT , but SpanBERT samples
span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of
[MASK] tokens of exactly the same length. Text inﬁlling teaches the model to predict how many tokens are
missing from a span.
Sentence Permutation
A document is divided into
sentences based on full stops, and these sentences are
shufﬂed in a random order.
Document Rotation
A token is chosen uniformly at
random, and the document is rotated so that it begins
with that token. This task trains the model to identify
the start of the document.
Fine-tuning BART
The representations produced by BART can be used in
several ways for downstream applications.
Sequence Classiﬁcation Tasks
For sequence classiﬁcation tasks, the same input is fed
into the encoder and decoder, and the ﬁnal hidden state
of the ﬁnal decoder token is fed into new multi-class
linear classiﬁer. This approach is related to the CLS
token in BERT; however we add the additional token
to the end so that representation for the token in the
decoder can attend to decoder states from the complete
input (Figure 3a).
Token Classiﬁcation Tasks
For token classiﬁcation tasks, such as answer endpoint
classiﬁcation for SQuAD, we feed the complete document into the encoder and decoder, and use the top
hidden state of the decoder as a representation for each
word. This representation is used to classify the token.
Sequence Generation Tasks
Because BART has an autoregressive decoder, it can be
directly ﬁne tuned for sequence generation tasks such
as abstractive question answering and summarization.
In both of these tasks, information is copied from the
input but manipulated, which is closely related to the
denoising pre-training objective. Here, the encoder input is the input sequence, and the decoder generates
outputs autoregressively.
Machine Translation
We also explore using BART to improve machine translation decoders for translating into English. Previous
work Edunov et al. has shown that models can
be improved by incorporating pre-trained encoders, but
gains from using pre-trained language models in decoders have been limited. We show that it is possible
to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that
are learned from bitext (see Figure 3b).
More precisely, we replace BART’s encoder embedding layer with a new randomly initialized encoder.
The model is trained end-to-end, which trains the new
encoder to map foreign words into an input that BART
can de-noise to English. The new encoder can use a
separate vocabulary from the original BART model.
We train the source encoder in two steps, in both
cases backpropagating the cross-entropy loss from the
output of the BART model. In the ﬁrst step, we freeze
most of BART parameters and only update the randomly initialized source encoder, the BART positional
embeddings, and the self-attention input projection matrix of BART’s encoder ﬁrst layer. In the second step,
we train all model parameters for a small number of
iterations.
Comparing Pre-training Objectives
BART supports a much wider range of noising schemes
during pre-training than previous work. We compare a
range of options using base-size models (6 encoder and
6 decoder layers, with a hidden size of 768), evaluated
on a representative subset of the tasks we will consider
for the full large scale experiments in §5.
Comparison Objectives
While many pre-training objectives have been proposed, fair comparisons between these have been dif-
ﬁcult to perform, at least in part due to differences in
training data, training resources, architectural differences between models, and ﬁne-tuning procedures. We
Pre-trained
Pre-trained
<s> A B C D E
(a) To use BART for classiﬁcation problems, the same
input is fed into the encoder and decoder, and the representation from the ﬁnal output is used.
Initialized Encoder
Pre-trained
Pre-trained
<s> A B C D
(b) For machine translation, we learn a small additional
encoder that replaces the word embeddings in BART. The
new encoder can use a disjoint vocabulary.
Figure 3: Fine tuning BART for classiﬁcation and translation.
re-implement strong pre-training approaches recently
proposed for discriminative and generation tasks. We
aim, as much as possible, to control for differences unrelated to the pre-training objective. However, we do
make minor changes to the learning rate and usage of
layer normalisation in order to improve performance
(tuning these separately for each objective). For reference, we compare our implementations with published
numbers from BERT, which was also trained for 1M
steps on a combination of books and Wikipedia data.
We compare the following approaches:
Language Model
Similarly to GPT , we train a left-to-right Transformer language
model. This model is equivalent to the BART decoder,
without cross-attention.
Permuted Language Model
Based on XLNet , we sample 1/6 of the tokens, and generate them in a random order autoregressively. For consistency with other models, we do not implement the
relative positional embeddings or attention across segments from XLNet.
Masked Language Model
Following BERT , we replace 15% of tokens with [MASK]
symbols, and train the model to independently predict
the original tokens.
Multitask Masked Language Model
As in UniLM
 , we train a Masked Language
Model with additional self-attention masks. Self attention masks are chosen randomly in with the follow
proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the ﬁrst 50% of tokens unmasked
and a left-to-right mask for the remainder.
Masked Seq-to-Seq
Inspired by MASS , we mask a span containing 50% of tokens,
and train a sequence to sequence model to predict the
masked tokens.
For the Permuted LM, Masked LM and Multitask
Masked LM, we use two-stream attention to efﬁciently compute likelihoods of the output
part of the sequence (using a diagonal self-attention
mask on the output to predict words left-to-right).
We experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source
input to the encoder and the target is the decoder output, or (2) adding the source as preﬁx to the target in
the decoder, with a loss only on the target part of the
sequence. We ﬁnd the former works better for BART
models, and the latter for other models.
To most directly compare our models on their ability
to model their ﬁne-tuning objective (the log likelihood
of the human text), we report perplexity in Table 1.
 a an extractive question answering task on Wikipedia paragraphs. Answers
are text spans extracted from a given document context.
Similar to BERT , we use concatenated question and context as input to the encoder of
BART, and additionally pass them to the decoder. The
model includes classiﬁers to predict the start and end
indices of each token.
 , a bitext classiﬁcation
task to predict whether one sentence entails another.
The ﬁne-tuned model concatenates the two sentences
with appended an EOS token, and passes them to both
the BART encoder and decoder. In contrast to BERT,
the representation of the EOS token is used to classify
the sentences relations.
 , a long-form abstractive question answering dataset. Models generate answers conditioned on the concatenation of a question and supporting documents.
 , a news summarization
dataset with highly abstractive summaries.
 , a dialogue response
generation task, conditioned on context and a persona.
 , a news summarization dataset. Summaries here are typically closely
related to source sentences.
Results are shown in Table 1. Several trends are clear:
BERT Base 
Masked Language Model
Masked Seq2seq
Language Model
Permuted Language Model
Multitask Masked Language Model
w/ Token Masking
w/ Token Deletion
w/ Text Inﬁlling
w/ Document Rotation
w/ Sentence Shufﬂing
w/ Text Inﬁlling + Sentence Shufﬂing
Table 1: Comparison of pre-training objectives. All models are of comparable size and are trained for 1M steps
on a combination of books and Wikipedia data. Entries in the bottom two blocks are trained on identical data
using the same code-base, and ﬁne-tuned with the same procedures. Entries in the second block are inspired by
pre-training objectives proposed in previous work, but have been simpliﬁed to focus on evaluation objectives (see
§4.1). Performance varies considerably across tasks, but the BART models with text inﬁlling demonstrate the most
consistently strong performance.
Performance of pre-training methods varies signiﬁcantly across tasks
The effectiveness of pre-training
methods is highly dependent on the task. For example, a simple language model achieves the best ELI5
performance, but the worst SQUAD results.
Token masking is crucial
Pre-training objectives
based on rotating documents or permuting sentences
perform poorly in isolation. The successful methods
either use token deletion or masking, or self-attention
Deletion appears to outperform masking on
generation tasks.
Left-to-right
pre-training
generation
The Masked Language Model and the Permuted
Language Model perform less well than others on
generation, and are the only models we consider that
do not include left-to-right auto-regressive language
modelling during pre-training.
Bidirectional encoders are crucial for SQuAD
noted in previous work , just
left-to-right decoder performs poorly on SQuAD, because future context is crucial in classiﬁcation decisions. However, BART achieves similar performance
with only half the number of bidirectional layers.
The pre-training objective is not the only important
Our Permuted Language Model performs less
well than XLNet . Some of this difference is likely due to not including other architectural
improvements, such as relative-position embeddings or
segment-level recurrence.
Pure language models perform best on ELI5
ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task
where other models outperform BART. A pure language model performs best, suggesting that BART is
less effective when the output is only loosely constrained by the input.
BART achieves the most consistently strong performance.
With the exception of ELI5, BART models
using text-inﬁlling perform well on all tasks.
Large-scale Pre-training Experiments
Recent work has shown that downstream performance
can dramatically improve when pre-training is scaled
to large batch sizes 
and corpora. To test how well BART performs in this
regime, and to create a useful model for downstream
tasks, we trained BART using the same scale as the
RoBERTa model.
Experimental Setup
We pre-train a large model with 12 layers in each of the
encoder and decoder, and a hidden size of 1024. Following RoBERTa , we use a batch size
of 8000, and train the model for 500000 steps. Documents are tokenized with the same byte-pair encoding
as GPT-2 . Based on the results in
Section §4, we use a combination of text inﬁlling and
sentence permutation. We mask 30% of tokens in each
document, and permute all sentences. Although sentence permutation only shows signiﬁcant additive gains
Table 2: Results for large models on SQuAD and GLUE tasks. BART performs comparably to RoBERTa and
XLNet, suggesting that BART’s uni-directional decoder layers do not reduce performance on discriminative tasks.
CNN/DailyMail
PTGEN 
PTGEN+COV 
BERTSUMABS 
BERTSUMEXTABS 
Table 3: Results on two standard summarization datasets. BART outperforms previous work on summarization on
two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.
on the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able
to learn from this task. To help the model better ﬁt the
data, we disabled dropout for the ﬁnal 10% of training
steps. We use the same pre-training data as Liu et al.
 , consisting of 160Gb of news, books, stories,
and web text.
Discriminative Tasks
Table 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and
GLUE tasks .
The most directly comparable baseline is RoBERTa,
which was pre-trained with the same resources, but
a different objective. Overall, BART performs similarly, with only small differences between the models
on most tasks. suggesting that BART’s improvements
on generation tasks do not come at the expense of classiﬁcation performance.
Generation Tasks
We also experiment with several text generation tasks.
BART is ﬁne-tuned as a standard sequence-to-sequence
model from the input to the output text. During ﬁnetuning we use a label smoothed cross entropy loss
 , with the smoothing parameter
set to 0.1. During generation, we set beam size as 5,
remove duplicated trigrams in beam search, and tuned
the model with min-len, max-len, length penalty on the
validation set .
Seq2Seq + Attention
Best System
Table 4: BART outperforms previous work on conversational response generation.
Perplexities are renormalized based on ofﬁcial tokenizer for ConvAI2.
Summarization
To provide a comparison with the
state-of-the-art in summarization, we present results
on two summarization datasets, CNN/DailyMail and
XSum, which have distinct properties.
Summaries in the CNN/DailyMail tend to resemble
source sentences. Extractive models do well here, and
even the baseline of the ﬁrst-three source sentences is
highly competitive. Nevertheless, BART outperforms
all existing work.
In contrast, XSum is highly abstractive, and extractive models perform poorly. BART outperforms the
best previous work, which leverages BERT, by roughly
6.0 points on all ROUGE metrics—representing a signiﬁcant advance in performance on this problem. Qualitatively, sample quality is high (see §6).
We evaluate dialogue response generation
on CONVAI2 , in which agents
must generate responses conditioned on both the previous context and a textually-speciﬁed persona. BART
outperforms previous work on two automated metrics.
Best Extractive
Language Model
Seq2Seq Multitask
BART achieves state-of-the-art results on
the challenging ELI5 abstractive question answering
dataset. Comparison models are from Fan et al. .
Fixed BART
Tuned BART
Table 6: The performance (BLEU) of baseline and
BART on WMT’16 RO-EN augmented with backtranslation data. BART improves over a strong backtranslation (BT) baseline by using monolingual English
pre-training.
Abstractive QA
We use the recently proposed ELI5
dataset to test the model’s ability to generate long freeform answers. We ﬁnd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains
a challenging, because answers are only weakly speci-
ﬁed by the question.
Translation
We also evaluated performance on WMT16 Romanian-
back-translation
from Sennrich et al. .
We use a 6-layer
transformer source encoder to map Romanian into
a representation that BART is able to de-noise into
English, following the approach introduced in §3.4.
Experiment results are presented in Table 6.
compare our results against a baseline Transformer
architecture with Transformerlarge settings (the baseline row).
We show the
performance of both steps of our model in the ﬁxed
BART and tuned BART rows.
For each row we
experiment on the original WMT16 Romanian-English
augmented with back-translation data.
beam width of 5 and a length penalty of α = 1.
Preliminary results suggested that our approach was
less effective without back-translation data, and prone
to overﬁtting—future work should explore additional
regularization techniques.
Qualitative Analysis
BART shows large improvements on summarization
metrics, of up to 6 points over the prior state-of-the-art.
To understand BART’s performance beyond automated
metrics, we analyse its generations qualitatively.
Table 7 shows example summaries generated by
BART. Examples are taken from WikiNews articles
published after the creation of the pre-training corpus,
to eliminate the possibility of the events described being present in the model’s training data.
Narayan et al. , we remove the ﬁrst sentence of
the article prior to summarizing it, so there is no easy
extractive summary of the document.
Unsurprisingly, model output is ﬂuent and grammatical English. However, model output is also highly abstractive, with few phrases copied from the input. The
output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California). In the ﬁrst example, inferring that
ﬁsh are protecting reefs from global warming requires
non-trivial inference from the text. However, the claim
that the work was published in Science is not supported
by the source.
These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.
Related Work
Early methods for pretraining were based on language
models. GPT only models leftward context, which is problematic for some tasks.
ELMo concatenates left-only and
right-only representations, but does not pre-train interactions between these features. Radford et al. 
demonstrated that very large language models can act
as unsupervised multitask models.
BERT introduced masked language modelling, which allows pre-training to learn interactions between left and right context words. Recent work has shown that very strong performance can
be achieved by training for longer ,
by tying parameters across layers ,
and by masking spans instead of words . Predictions are not made auto-regressively, reducing the effectiveness of BERT for generation tasks.
UniLM ﬁne-tunes BERT with an
ensemble of masks, some of which allow only leftward
context. Like BART, this allows UniLM to be used for
both generative and discriminative tasks. A difference
is that UniLM predictions are conditionally independent, whereas BART’s are autoregressive. BART reduces the mismatch between pre-training and generation tasks, because the decoder is always trained on uncorrupted context.
MASS is perhaps the most similar
model to BART. An input sequence where a contiguous
span of tokens is masked is mapped to a sequence consisting of the missing tokens. MASS is less effective
for discriminative tasks, because disjoint sets of tokens
are fed into the encoder and decoder.
XL-Net extends BERT by pre-
Source Document (abbreviated)
BART Summary
The researchers examined three types of coral in reefs off the
coast of Fiji ... The researchers found when ﬁsh were plentiful,
they would eat algae and seaweed off the corals, which appeared
to leave them more resistant to the bacterium Vibrio coralliilyticus, a bacterium associated with bleaching. The researchers suggested the algae, like warming temperatures, might render the
corals’ chemical defenses less effective, and the ﬁsh were protecting the coral by removing the algae.
Fisheries off the coast of Fiji are protecting coral reefs from the effects of global
warming, according to a study in the journal Science.
Sacoolas, who has immunity as a diplomat’s wife, was involved
in a trafﬁc collision ... Prime Minister Johnson was questioned
about the case while speaking to the press at a hospital in Watford. He said, “I hope that Anne Sacoolas will come back ...
if we can’t resolve it then of course I will be raising it myself
personally with the White House.”
Boris Johnson has said he will raise the issue of US diplomat Anne Sacoolas’ diplomatic immunity with the White House.
According to Syrian state media, government forces began deploying into previously SDF controlled territory yesterday. ...
On October 6, US President Donald Trump and Turkish President Recep Tayyip Erdoan spoke on the phone. Then both nations issued statements speaking of an imminent incursion into
northeast Syria ... . On Wednesday, Turkey began a military
offensive with airstrikes followed by a ground invasion.
Syrian government forces have entered
territory held by the US-backed Syrian
Democratic Forces (SDF) in response to
Turkey’s incursion into the region.
This is the ﬁrst time anyone has been recorded to run a full
marathon of 42.195 kilometers (approximately 26 miles) under
this pursued landmark time. It was not, however, an ofﬁcially
sanctioned world record, as it was not an ”open race” of the
IAAF. His time was 1 hour 59 minutes 40.2 seconds. Kipchoge
ran in Vienna, Austria. It was an event speciﬁcally designed to
help Kipchoge break the two hour barrier.
Kenyan runner Eliud Kipchoge has run a
marathon in less than two hours.
PG&E stated it scheduled the blackouts in response to forecasts
for high winds amid dry conditions. The aim is to reduce the risk
of wildﬁres. Nearly 800 thousand customers were scheduled to
be affected by the shutoffs which were expected to last through
at least midday tomorrow.
Power has been turned off to millions of
customers in California as part of a power
shutoff plan.
Table 7: Example summaries from the XSum-tuned BART model on WikiNews articles. For clarity, only relevant
excerpts of the source are shown. Summaries combine information from across the article and prior knowledge.
dicting masked tokens auto-regressively in a permuted
order. This objective allows predictions to condition on
both left and right context. In contrast, the BART decoder works left-to-right during pre-training, matching
the setting during generation.
Several papers have explored using pre-trained representations to improve machine translation.
largest improvements have come from pre-training on
both source and target languages , but this requires pretraining on all languages of interest. Other work has
shown that encoders can be improved using pre-trained
representations , but gains in decoders are more limited. We show how BART can be
used to improve machine translation decoders.
Conclusions
We introduced BART, a pre-training approach that
learns to map corrupted documents to the original.
BART achieves similar performance to RoBERTa on
discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks. Future work should explore new methods for corrupting
documents for pre-training, perhaps tailoring them to
speciﬁc end tasks.