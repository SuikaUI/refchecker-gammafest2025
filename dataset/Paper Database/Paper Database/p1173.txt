Volumetric and Multi-View CNNs for Object Classiﬁcation on 3D Data
Charles R. Qi∗
Matthias Nießner
Angela Dai
Mengyuan Yan
Leonidas J. Guibas
Stanford University
3D shape models are becoming widely available and
easier to capture, making available 3D information crucial
for progress in object classiﬁcation. Current state-of-theart methods rely on CNNs to address this problem. Recently,
we witness two types of CNNs being developed: CNNs
based upon volumetric representations versus CNNs based
upon multi-view representations.
Empirical results from
these two types of CNNs exhibit a large gap, indicating
that existing volumetric CNN architectures and approaches
are unable to fully exploit the power of 3D representations.
In this paper, we aim to improve both volumetric CNNs
and multi-view CNNs according to extensive analysis of
existing approaches. To this end, we introduce two distinct
network architectures of volumetric CNNs.
In addition,
we examine multi-view CNNs, where we introduce multiresolution ﬁltering in 3D. Overall, we are able to outperform current state-of-the-art methods for both volumetric
CNNs and multi-view CNNs. We provide extensive experiments designed to evaluate underlying design choices, thus
providing a better understanding of the space of methods
available for object classiﬁcation on 3D data.
1. Introduction
Understanding 3D environments is a vital element of
modern computer vision research due to paramount relevance in many vision systems, spanning a wide ﬁeld of
application scenarios from self-driving cars to autonomous
Recent advancements in real-time SLAM techniques and crowd-sourcing of virtual 3D models have additionally facilitated the availability of 3D data. . This development has encouraged the lifting of 2D to
3D for deep learning, opening up new opportunities with the
additional information of 3D data; e.g., aligning models is
easier in 3D Euclidean space. In this paper, we speciﬁcally
focus on the object classiﬁcation task on 3D data obtained
from both CAD models and commodity RGB-D sensors. In
* indicates equal contributions.
addition, we demonstrate retrieval results in the supplemental material.
While the extension of 2D convolutional neural networks
to 3D seems natural, the additional computational complexity (volumetric domain) and data sparsity introduces
signiﬁcant challenges; for instance, in an image, every pixel
contains observed information, whereas in 3D, a shape is
only deﬁned on its surface. Seminal work by Wu et al.
 propose volumetric CNN architectures on volumetric
grids for object classiﬁcation and retrieval.
While these
approaches achieve good results, it turns out that training a
CNN on multiple 2D views achieves a signiﬁcantly higher
performance, as shown by Su et al. , who augment their
2D CNN with pre-training from ImageNet RGB data .
These results indicate that existing 3D CNN architectures
and approaches are unable to fully exploit the power of 3D
representations. In this work, we analyze these observations
and evaluate the design choices. Moreover, we show how to
reduce the gap between volumetric CNNs and multi-view
CNNs by efﬁciently augmenting training data, introducing
new CNN architectures in 3D. Finally, we examine multiview CNNs; our experiments show that we are able to
improve upon state of the art with improved training data
augmentation and a new multi-resolution component.
Problem Statement
We consider volumetric representations of 3D point clouds or meshes as input to the 3D
object classiﬁcation problem.
This is primarily inspired
by recent advances in real-time scanning technology, which
use volumetric data representations. We further assume that
the input data is already pre-segmented by 3D bounding
boxes. In practice, these bounding boxes can be extracted
using the sliding windows, object proposals, or background
subtraction. The output of the method is the category label
of the volumetric data instance.
We provide a detailed analysis over factors that
inﬂuence the performance of volumetric CNNs, including
network architecture and volumn resolution. Based upon
our analysis, we strive to improve the performance of volumetric CNNs. We propose two volumetric CNN network
architectures that signﬁcantly improve state-of-the-art of
 
volumetric CNNs on 3D shape classiﬁcation. This result
has also closed the gap between volumetric CNNs and
multi-view CNNs, when they are provided with 3D input
discretized at 30×30×30 3D resolution. The ﬁrst network
introduces auxiliary learning tasks by classifying part of an
object, which help to scrutize details of 3D objects more
deeply. The second network uses long anisotropic kernels
to probe for long-distance interactions.
Combining data
augmentation with a multi-orientation pooling, we observe
signiﬁcant performance improvement for both networks.
We also conduct extensive experiments to study the in-
ﬂuence of volume resolution, which sheds light on future
directions of improving volumetric CNNs.
Furthermore, we introduce a new multi-resolution component to multi-view CNNs, which improves their already
compelling performance.
In addition to providing extensive experiments on 3D
CAD model datasets, we also introduce a dataset of realworld 3D data, constructed using dense 3D reconstruction
taken with . Experiments show that our networks can
better adapt from synthetic data to this real-world data than
previous methods.
2. Related Work
Shape Descriptors
A large variety of shape descriptors
has been developed in the computer vision and graphics
community.
For instance, shapes can be represented as
histograms or bag-of-feature models which are constructed
from surface normals and curvatures .
Alternatives
include models based on distances, angles, triangle areas, or
tetrahedra volumes , local shape diameters measured at
densely-sampled surface points , Heat kernel signatures
 , or extensions of SIFT and SURF feature descriptors
to 3D voxel grids . The spherical harmonic descriptor
(SPH) and the Light Field descriptor (LFD) are
other popular descriptors.
LFD extracts geometric and
Fourier descriptors from object silhouettes rendered from
several different viewpoints, and can be directly applied to
the shape classiﬁcation task. In contrast to recently developed feature learning techniques, these features are handcrafted and do not generalize well across different domains.
Convolutional Neural Networks
Convolutional Neural
Networks (CNNs) have been successfully used in different areas of computer vision and beyond.
In particular, signiﬁcant progress has been made in the context of
learning features.
It turns out that training from large
RGB image datasets (e.g., ImageNet ) is able to learn
general purpose image descriptors that outperform handcrafted features for a number of vision tasks, including
object detection, scene recognition, texture recognition and
classiﬁcation . This signiﬁcant improve-
Multi-View Sphere Rendering
Multi-View Standard Rendering
Volumetric Occupancy Grid
Figure 1. 3D shape representations.
ment in performance on these tasks has decidedly moved
the ﬁeld forward.
CNNs on Depth and 3D Data
With the introduction
of commodity range sensors, the depth channel became
available to provide additional information that could be
incorporated into common CNN architectures. A very ﬁrst
approach combines convolutional and recursive neural networks for learning features and classifying RGB-D images
Impressive performance for object detection from
RGB-D images has been achieved using a geocentric embedding for depth images that encodes height above ground
and angle with gravity for each pixel in addition to the
horizontal disparity . Recently, a CNN architecture has
been proposed where the RGB and depth data are processed
in two separate streams; in the end, the two streams are
combined with a late fusion network . All these descriptors operate on single RGB-D images, thus processing 2.5D
Wu et al. lift 2.5D to 3D with their 3DShapeNets
approach by categorizing each voxel as free space, surface
or occluded, depending on whether it is in front of, on, or
behind the visible surface (i.e., the depth value) from the
depth map.
The resulting representation is a 3D binary
voxel grid, which is the input to a CNN with 3D ﬁlter
banks. Their method is particularly relevant in the context
of this work, as they are the ﬁrst to apply CNNs on a 3D
representation. A similar approach is VoxNet , which
also uses binary voxel grids and a corresponding 3D CNN
architecture. The advantage of these approaches is that it
can process different sources of 3D data, including LiDAR
point clouds, RGB-D point clouds, and CAD models; we
likewise follow this direction.
An alternative direction is to exploit established 2D CNN
architectures; to this end, 2D data is extracted from the
3D representation. In this context, DeepPano converts
3D shapes into panoramic views; i.e., a cylinder projection
around its principle axis. Current state-of-the-art uses multiple rendered views, and trains a CNN that can process
all views jointly . This multi-view CNN (MVCNN) is
pre-trained on ImageNet and uses view-point pooling to
combine all streams obtained from each view. A similar
idea on stereo views has been proposed earlier .
3. Analysis of state-of-the-art 3D Volumetric
CNN versus Multi-View CNN
Volumetric CNN (volume 30x30x30)
Multi-View CNN (sphere-30 rendering)
Multi-View CNN (standard rendering)
Figure 2. Classiﬁcation accuracy. Yellow and blue bars: Performance drop of multi-view CNN due to discretization of CAD
models in rendering.
Blue and green bars: Volumetric CNN
is signiﬁcantly worse than multi-view CNN, even though their
inputs have similar amounts of information. This indicates that the
network of the volumetric CNN is weaker than that of the multiview CNN.
Two representations of generic 3D shapes are popularly
used for object classiﬁcation, volumetric and multi-view
(Fig 1). The volumetric representation encodes a 3D shape
as a 3D tensor of binary or real values. The multi-view representation encodes a 3D shape as a collection of renderings
from multiple viewpoints. Stored as tensors, both representations can easily be used to train convolutional neural
networks, i.e., volumetric CNNs and multi-view CNNs.
Intuitively, a volumetric representation should encode
as much information, if not more, than its multi-view
counterpart.
However, experiments indicate that multiview CNNs produce superior performance in object classiﬁcation. Fig 2 reports the classiﬁcation accuracy on the
ModelNet40 dataset by state-of-the-art volumetric/multiview architectures1.
A volumetric CNN based on voxel
occupancy (green) is 7.3% worse than a multi-view CNN
We investigate this performance gap in order to ascertain how to improve volumetric CNNs.
The gap seems
to be caused by two factors: input resolution and network architecture differences. The multi-view CNN downsamples each rendered view to 227 × 227 pixels (Multiview Standard Rendering in Fig 1); to maintain a similar
computational cost, the volumetric CNN uses a 30×30×30
occupancy grid (Volumetric Occupancy Grid in Fig 1)2. As
shown in Fig 1, the input to the multi-view CNN captures
more detail.
1We train models by replicating the architecture of for volumetric
CNNs and for multi-view CNNs. All networks are trained in an endto-end fashion. All methods are trained/tested on the same split for fair
comparison. The reported numbers are average instance accuracy. See
Sec 6 for details.
2Note that 30 × 30 × 30 ≈227 × 227.
However, the difference in input resolution is not the
primary reason for this performance gap, as evidenced by
further experiments.
We compare the two networks by
providing them with data containing similar level of detail.
To this end, we feed the multi-view CNN with renderings of
the 30 × 30 × 30 occupancy grid using sphere rendering3,
i.e., for each occupied voxel, a ball is placed at its center,
with radius equal to the edge length of a voxel (Multi-View
Sphere Rendering in Fig 1). We train the multi-view CNN
from scratch using these sphere renderings. The accuracy
of this multi-view CNN is reported in blue.
As shown in Fig 2, even with similar level of object
detail, the volumetric CNN (green) is 4.8% worse than
the multi-view CNN (blue).
That is, there is still signiﬁcant room to improve the architecture of volumetric
This discovery motivates our efforts in Sec 4 to
improve volumetric CNNs.
Additionally, low-frequency
information in 3D seems to be quite discriminative for object classiﬁcation—it is possible to achieve 89.5% accuracy
(blue) at a resolution of only 30 × 30 × 30. This discovery
motivates our efforts in Sec 5 to improve multi-view CNNs
with a 3D multi-resolution approach.
4. Volumetric Convolutional Neural Networks
4.1. Overview
We improve volumetric CNNs through three separate
means: 1) introducing new network structures; 2) data
augmentation; 3) feature pooling.
Network Architecture
We propose two network variations that signiﬁcantly improve state-of-the-art CNNs on 3D
volumetric data. The ﬁrst network is designed to mitigate
overﬁtting by introducing auxiliary training tasks, which
are themselves challenging. These auxiliary tasks encourage the network to predict object class labels from partial
subvolumes. Therefore, no additional annotation efforts are
needed. The second network is designed to mimic multiview CNNs, as they are strong in 3D shape classiﬁcation.
Instead of using rendering routines from computer graphics,
our network projects a 3D shape to 2D by convolving its
3D volume with an anisotropic probing kernel. This kernel is capable of encoding long-range interactions between
points. An image CNN is then appended to classify the 2D
projection. Note that the training of the projection module
and the image classiﬁcation module is end-to-end. This emulation of multi-view CNNs achieves similar performance
to them, using only standard layers in CNN.
In order to mitigate overﬁtting from too many parameters, we adopt the mlpconv layer from as our basic
building block in both network variations.
3It is computationally prohibitive to match the volumetric CNN resolution to multi-view CNN, which would be 227 × 227 × 227.
(48, 6, 2; 48; 48)
(512, 3, 2; 512; 512)
(160, 5, 2; 160; 160)
Prediction by
whole object
Prediction by
partial object
Figure 3. Auxiliary Training by Subvolume Supervision (Sec 4.2). The main innovation is that we add auxiliary tasks to predict class labels
that focus on part of an object, intended to drive the CNN to more heavily exploit local discriminative features. An mlpconv layer is a
composition of three conv layers interleaved by ReLU layers. The ﬁve numbers under mlpconv are the number of channels, kernel size
and stride of the ﬁrst conv layer, and the number of channels of the second and third conv layers, respectively. The kernel size and stride of
the second and third conv layers are 1. For example, mlpconv(48, 6, 2; 48; 48) is a composition of conv(48, 6, 2), ReLU, conv(48, 1, 1),
ReLU, conv(48, 1, 1) and ReLU layers. Note that we add dropout layers with rate=0.5 after fully connected layers.
Data Augmentation
Compared with 2D image datasets,
currently available 3D shape datasets are limited in scale
and variation. To fully exploit the design of our networks,
we augment the training data with different azimuth and elevation rotations. This allows the ﬁrst network to cover local
regions at different orientations, and the second network to
relate distant points at different relative angles.
Multi-Orientation Pooling
Both of our new networks are
sensitive to shape orientation, i.e., they capture different
information at different orientations.
To capture a more
holistic sense of a 3D object, we add an orientation pooling
stage that aggregates information from different orientations.
4.2. Network 1: Auxiliary Training by Subvolume
Supervision
We observe signiﬁcant overﬁtting when we train the
volumetric CNN proposed by in an end-to-end fashion
(see supplementary). When the volumetric CNN overﬁts to
the training data, it has no incentive to continue learning.
We thus introduce auxiliary tasks that are closely correlated
with the main task but are difﬁcult to overﬁt, so that learning
continues even if our main task is overﬁtted.
These auxiliary training tasks also predict the same object labels, but the predictions are made solely on a local
subvolume of the input. Without complete knowledge of
the object, the auxiliary tasks are more challenging, and
can thus better exploit the discriminative power of local
This design is different from the classic multitask learning setting of hetergenous auxiliary tasks, which
inevitably requires collecting additional annotations (e.g.,
conducting both object classiﬁcation and detection ).
We implement this design through an architecture shown
The ﬁrst three layers are mlpconv (multilayer
perceptron convolution) layers, a 3D extension of the 2D
mlpconv proposed by . The input and output of our
mlpconv layers are both 4D tensors. Compared with the
standard combination of linear convolutional layers and
max pooling layers, mlpconv has a three-layer structure and
is thus a universal function approximator if enough neurons
are provided in its intermediate layers. Therefore, mlpconv
is a powerful ﬁlter for feature extraction of local patches,
enhancing approximation of more abstract representations.
In addition, mlpconv has been validated to be more discriminative with fewer parameters than ordinary convolution
with pooling .
At the fourth layer, the network branches into two. The
lower branch takes the whole object as input for traditional
classiﬁcation.
The upper branch is a novel branch for
auxiliary tasks. It slices the 512 × 2 × 2 × 2 4D tensor (2
grids along x, y, z axes and 512 channels) into 2×2×2 = 8
vectors of dimension 512. We set up a classiﬁcation task
for each vector.
A fully connected layer and a softmax
layer are then appended independently to each vector to
construct classiﬁcation losses.
Simple calculation shows
that the receptive ﬁeld of each task is 22×22×22, covering
roughly 2/3 of the entire volume.
4.3. Network 2: Anisotropic Probing
The success of multi-view CNNs is intriguing. multiview CNNs ﬁrst project 3D objects to 2D and then make
use of well-developed 2D image CNNs for classiﬁcation.
Inspired by its success, we design a neural network archi-
Anisotropic Probing
Image-based CNN
(Network In Network)
Figure 4. CNN with Anisotropic Probing kernels. We use an elongated kernel to convolve the 3D cube and aggregate information to a 2D
plane. Then we use a 2D NIN (NIN-CIFAR10 ) to classify the 2D projection of the original 3D shape.
tecture that is also composed of the two stages. However,
while multi-view CNNs use external rendering pipelines
from computer graphics, we achieve the 3D-to-2D projection using network layers in a manner similar to ‘X-ray
scanning’.
Key to this network is the use of an elongated anisotropic
kernel which helps capture the global structure of the 3D
volume. As illustrated in Fig 4, the neural network has two
modules: an anisotropic probing module and a network in
network module. The anisotropic probing module contains
three convolutional layers of elongated kernels, each followed by a nonlinear ReLU layer. Note that both the input
and output of each layer are 3D tensors.
In contrast to traditional isotropic kernels, an anisotropic
probing module has the advantage of aggregating longrange interactions in the early feature learning stage with
fewer parameters. As a comparison, with traditional neural networks constructed from isotropic kernels, introducing long-range interactions at an early stage can only be
achieved through large kernels, which inevitably introduce
many more parameters. After anisotropic probing, we use
an adapted NIN network to address the classiﬁcation
Our anistropic probing network is capable of capturing
internal structures of objects through its X-ray like projection mechanism. This is an ability not offered by standard
rendering. Combined with multi-orientation pooling (introduced below), it is possible for this probing mechanism to
capture any 3D structure, due to its relationship with the
Radon transform.
In addition, this architecture is scalable to higher resolutions, since all its layers can be viewed as 2D. While
3D convolution involves computation at locations of cubic
resolution, we maintain quadratic compute.
4.4. Data Augmentation and Multi-Orientation
The two networks proposed above are both sensitive to
model orientation. In the subvolume supervision method,
different model orientations deﬁne different local subvolumes; in the anisotropic probing method, only voxels of
the same height and along the probing direction can have
interaction in the early feature extraction stage.
is helpful to augment the training data by varying object
orientation and combining predictions through orientation
Similar to Su-MVCNN which aggregates information from multiple view inputs through a view-pooling
layer and follow-on fully connected layers, we sample 3D
input from different orientations and aggregate them in a
multi-orientation volumetric CNN (MO-VCNN) as shown
in Fig 5. At training time, we generate different rotations
of the 3D model by changing both azimuth and elevation
angles, sampled randomly.
A volumetric CNN is ﬁrstly
trained on single rotations. Then we decompose the network to CNN1 (lower layers) and CNN2 (higher layers)
to construct a multi-orientation version. The MO-VCNN’s
weights are initialized by a previously trained volumetric
CNN with CNN1’s weights ﬁxed during ﬁne-tuning. While
a common practice is to extract the highest level features
(features before the last classiﬁcation linear layer) of multiple orientations, average/max/concatenate them, and train
a linear SVM on the combined feature, this is just a special
case of the MO-VCNN.
Compared to 3DShapeNets which only augments
data by rotating around vertical axis, our experiment shows
that orientation pooling combined with elevation rotation
Ori-Pooling
class prediction
class prediction
Figure 5. Left: Volumetric CNN (single orientation input). Right:
Multi-orientation volumetric CNN (MO-VCNN), which takes in
various orientations of the 3D input, extracts features from shared
CNN1 and then pass pooled feature through another network
CNN2 to make a prediction.
can greatly increase performance.
5. Multi-View Convolutional Neural Networks
The multi-view CNN proposed by is a strong alternative to volumetric representations.
This multi-view
representation is constructed in three steps: ﬁrst, a 3D shape
is rendered into multiple images using varying camera extrinsics; then image features (e.g. conv5 feature in VGG
or AlexNet) are extracted for each view; lastly features are
combined across views through a pooling layer, followed
by fully connected layers.
Although the multi-view CNN presented by produces compelling results, we are able to improve its performance through a multi-resolution extension with improved
data augmentation. We introduce multi-resolution 3D ﬁltering to capture information at multiple scales. We perform
sphere rendering (see Sec 3) at different volume resolutions. Note that we use spheres for this discretization as
they are view-invariant. In particular, this helps regularize
out potential noise or irregularities in real-world scanned
data (relative to synthetic training data), enabling robust
performance on real-world scans. Note that our 3D multiresolution ﬁltering is different from classical 2D multiresolution approaches, since the 3D ﬁltering respects the
distance in 3D.
Additionally, we also augment training data with variations in both azimuth and elevation, as opposed to azimuth
only. We use AlexNet instead of VGG for efﬁciency.
6. Experiments
We evaluate our volumetric CNNs and multi-view CNNs
along with current state of the art on the ModelNet
dataset and a new dataset of real-world reconstructions
of 3D objects.
For convenience in following discussions, we deﬁne 3D
resolution to be the discretization resolution of a 3D shape.
That is, a 30 × 30 × 30 volume has 3D resolution 30. The
sphere rendering from this volume also has 3D resolution
30, though it may have higher 2D image resolution.
6.1. Datasets
We use ModelNet for our training and
testing datasets. ModelNet currently contains 127, 915 3D
CAD models from 662 categories. ModelNet40, a subset
including 12, 311 models from 40 categories, is well annotated and can be downloaded from the web. The authors
also provide a training and testing split on the website, in
which there are 9, 843 training and 2, 468 test models4. We
4VoxNet uses the train/test split provided on the website and report
average class accuracy on the 2, 468 test split. 3DShapeNets and
MVCNN use another train/test split comprising the ﬁrst 80 shapes of
each category in the “train” folder (or all shapes if there are fewer than 80)
and the ﬁrst 20 shapes of each category in the “test” folder, respectively.
(a) bathtub
(d) monitor
Figure 6. Example models from our real-world dataset.
model is a dense 3D reconstruction, annotated, and segmented
from the background.
use this train/test split for our experiments.
By default, we report classiﬁcation accuracy on all models in the test set (average instance accuracy). For comparisons with previous work we also report average class
Real-world Reconstructions
We provide a new realworld scanning dataset benchmark, comprising 243 objects
of 12 categories; the geometry is captured with an ASUS
Xtion Pro and a dense reconstruction is obtained using the
publicly-available VoxelHashing framework . For each
scan, we have performed a coarse, manual segmentation
of the object of interest. In addition, each scan is aligned
with the world-up vector. While there are existing datasets
captured with commodity range sensors – e.g., 
– this is the ﬁrst containing hundreds of annotated models
from dense 3D reconstructions. The goal of this dataset is
to provide an example of modern real-time 3D reconstructions; i.e., structured representations more complete than a
single RGB-D frame but still with many occlusions. This
dataset is used as a test set.
6.2. Comparison with State-of-the-Art Methods
We compare our methods with state of the art for shape
classiﬁcation on the ModelNet40 dataset. In the following,
we discuss the results within volumetric CNN methods and
within multi-view CNN methods.
Volumetric CNNs
Fig 7 summarizes the performance of
volumetric CNNs.
Ours-MO-SubvolumeSup is the subvolume supervision network in Sec 4.2 and Ours-MO-
AniProbing is the anistropic probing network in Sec 4.3.
Data augmentation is applied as described in Sec 6.4 (azimuth and elevation rotations). For clarity, we use MOto denote that both networks are trained with an additional
multi-orientation pooling step (20 orientations in practice).
For reference of multi-view CNN performance at the same
3DShapeNets
(Wu et al.)
(Maturana et al.)
SubvolumeSup
AniProbing
Ours-MVCNN-
Average class accuracy
Average instance accuracy
Figure 7. Classiﬁcation accuracy on ModelNet40 (voxelized at resolution 30). Our volumetric CNNs have matched the performance
of multi-view CNN at 3D resolution 30 (our implementation of
Su-MVCNN , rightmost group).
(Su et al.)
HoGPyramid-
MVCNN-MultiRes
Average class accuracy
Average instance accuracy
Figure 8. Classiﬁcation acurracy on ModelNet40 (multi-view representation). The 3D multi-resolution version is the strongest. It is
worth noting that the simple baseline HoGPyramid-LFD performs
quite well.
3D resolution, we also include Ours-MVCNN-Sphere-30,
the result of our multi-view CNN with sphere rendering at
3D resolution 30. More details of setup can be found in the
supplementary.
As can be seen, both of our proposed volumetric CNNs
signiﬁcantly outperform state-of-the-art volumetric CNNs.
Moreover, they both match the performance of our multiview CNN under the same 3D resolution. That is, the gap
between volumetric CNNs and multi-view CNNs is closed
under 3D resolution 30 on ModelNet40 dataset, an issue
that motivates our study (Sec 3).
Multi-view CNNs
Fig 8 summarizes the performance of
multi-view CNNs. Ours-MVCNN-MultiRes is the result
by training an SVM over the concatenation of fc7 features
from Ours-MVCNN-Sphere-30, 60, and Ours-MVCNN.
HoGPyramid-LFD is the result by training an SVM over a
concatenation of HoG features at three 2D resolutions. Here
LFD (lightﬁeld descriptor) simply refers to extracting features from renderings. Ours-MVCNN-MultiRes achieves
state-of-the-art.
6.3. Effect of 3D Resolution over Performance
Sec 6.2 shows that our volumetric CNN and multi-view
CNN performs comparably at 3D resolution 30. Here we
Accuracy (%)
Model Voxelization Resolution
CNN-Sphere (single view)
Ours-MVCNN-Sphere
Ours-SubvolumeSup (single ori)
Ours-MO-SubvolumeSup
Figure 9. Top: sphere rendering at 3D resolution 10, 30, 60, and
standard rendering. Bottom: performance of image-based CNN
and volumetric CNN with increasing 3D resolution.
rightmost points are trained/tested from standard rendering.
study the effect of 3D resolution for both types of networks.
Fig 9 shows the performance of our volumetric CNN
and multi-view CNN at different 3D resolutions (deﬁned
at the beginning of Sec 6).
Due to computational cost,
we only test our volumetric CNN at 3D resolutions 10
The observations are: ﬁrst, the performance of
our volumetric CNN and multi-view CNN is on par at
tested 3D resolutions; second, the performance of multiview CNN increases as the 3D resolution grows up. To
further improve the performance of volumetric CNN, this
experiment suggests that it is worth exploring how to scale
volumetric CNN to higher 3D resolutions.
6.4. More Evaluations
Data Augmentation and Multi-Orientation Pooling
We use the same volumetric CNN model, the end-to-end
learning verion of 3DShapeNets , to train and test on
three variations of augmented data (Table 1). Similar trend
is observed for other volumetric CNN variations.
Data Augmentation
Single-Ori
Azimuth rotation (AZ)
AZ + translation
AZ + elevation rotation
Table 1. Effects of data augmentations on multi-orientation volumetric CNN. We report numbers of classiﬁcation accuracy on
ModelNet40, with (Multi-Ori) or without (Single-Ori) multiorientation pooling described in Sec 4.4.
When combined with multi-orientation pooling, applying both azimuth rotation (AZ) and elevation rotation (EL)
augmentations is extremely effective. Using only azimuth
augmentation (randomly sampled from 0◦to 360◦) with
orientation pooling, the classiﬁcation performance is increased by 86.1% −84.7% = 1.4%; combined with eleva-
Single-Ori
VoxNet 
Ours-SubvolumeSup
Ours-AniProbing
Table 2. Comparison of performance of volumetric CNN architectures. Numbers reported are classiﬁcation accuracy on Model-
Net40. Results from E2E- (end-to-end learning version) and
VoxNet are obtained by ourselves. All experiments are using
the same set of azimuth and elevation augmented data.
tion augmentation (randomly sampled from −45◦to 45◦),
the improvement becomes more signiﬁcant – increasing by
87.8% −83.0% = 4.8%. On the other hand, translation
jittering (randomly sampled shift from 0 to 6 voxels in each
direction) provides only marginal inﬂuence.
Comparison of Volumetric CNN Architectures
The architectures in comparison include VoxNet , E2E- 
(the end-to-end learning variation of implemented in
Caffe by ourselves), 3D-NIN (a 3D variation of Network in Network designed by ourselves as in Fig 3
without the “Prediction by partial object” branch), SubvolumeSup (Sec 4.2) and AniProbing (Sec 4.3). Data augmentation of AZ+EL (Sec 6.4) are applied.
From Table 2, ﬁrst, the two volumetric CNNs we propose, SubvolumeSup and AniProbing networks, both show
superior performance, indicating the effectiveness of our
design; second, multi-orientation pooling increases performance for all network variations.
This is especially
signiﬁcant for the anisotropic probing network, since each
orientation usually only carries partial information of the
Comparison of Multi-view Methods
We compare different methods that are based on multi-view representations
in Table 3. Methods in the second group are trained on
the full ModelNet40 train set. Methods in the ﬁrst group,
SPH, LFD, FV, and Su-MVCNN, are trained on a subset
(instance)
SPH (reported by )
LFD (reported by )
FV (reported by )
Su-MVCNN 
PyramidHoG-LFD
Ours-MVCNN
Ours-MVCNN-MultiRes
Table 3. Comparison of multi-view based methods.
reported are classiﬁcation accuracy (class average and instance
average) on ModelNet40.
Classiﬁcation
Retrieval MAP
Su-MVCNN 
Ours-MO-SubvolumeSup
Ours-MO-AniProbing
Ours-MVCNN-MultiRes
Table 4. Classiﬁcation accuracy and retrieval MAP on reconstructed meshes of 12-class real-world scans.
of ModelNet40 containing 3,183 training samples. They
are provided for reference. Also note that the MVCNNs
in the second group are our implementations in Caffe with
AlexNet instead of VGG as in Su-MVCNN .
We observe that MVCNNs are superior to methods by
SVMs on hand-crafted features.
Evaluation on the Real-World Reconstruction Dataset
We further assess the performance of volumetric CNNs and
multi-view CNNs on real-world reconstructions in Table 4.
All methods are trained on CAD models in ModelNet40 but
tested on real data, which may be highly partial, noisy, or
oversmoothed (Fig 6). Our networks continue to outperform state-of-the-art results. In particular, our 3D multiresolution ﬁltering is quite effective on real-world data,
possibly because the low 3D resolution component ﬁlters
out spurious and noisy micro-structures. Example results
for object retrieval can be found in supplementary.
7. Conclusion and Future work
In this paper, we have addressed the task of object classi-
ﬁcation on 3D data using volumetric CNNs and multi-view
CNNs. We have analyzed the performance gap between
volumetric CNNs and multi-view CNNs from perspectives
of network architecture and 3D resolution. The analysis
motivates us to propose two new architectures of volumetric
CNNs, which outperform state-of-the-art volumetric CNNs,
achieving comparable performance to multi-view CNNs at
the same 3D resolution of 30 × 30 × 30. Further evalution over the inﬂuence of 3D resolution indicates that 3D
resolution is likely to be the bottleneck for the performance
of volumetric CNNs. Therefore, it is worth exploring the
design of efﬁcient volumetric CNN architectures that scale
up to higher resolutions.
Acknowledgement.
The authors gratefully acknowledge
the support of Stanford Graduate Fellowship, NSF grants
IIS-1528025
DMS-1546206,
N00014-13-1-0341, a Google Focused Research award, the
Max Planck Center for Visual Computing and Communications and hardware donations by NVIDIA.