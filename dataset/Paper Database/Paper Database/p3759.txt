Under review as a conference paper at ICLR 2017
COMPACT DEEP CONVOLUTIONAL NEURAL
NETWORKS WITH COARSE PRUNING
Sajid Anwar, Wonyong Sung
Department of Electrical Engineering and Computer Science
Seoul National University
Gwanak-Gu, 08826, Republic of Korea
 , 
The learning capability of a neural network improves with increasing depth at
higher computational costs. Wider layers with dense kernel connectivity patterns
furhter increase this cost and may hinder real-time inference. We propose feature
map and kernel level pruning for reducing the computational complexity of a deep
convolutional neural network. Pruning feature maps reduces the width of a layer
and hence does not need any sparse representation. Further, kernel pruning converts the dense connectivity pattern into a sparse one. Due to coarse nature, these
pruning granularities can be exploited by GPUs and VLSI based implementations.
We propose a simple and generic strategy to choose the least adversarial pruning
masks for both granularities. The pruned networks are retrained which compensates the loss in accuracy. We obtain the best pruning ratios when we prune a
network with both granularities. Experiments with the CIFAR-10 dataset show
that more than 85% sparsity can be induced in the convolution layers with less
than 1% increase in the missclassiﬁcation rate of the baseline network.
INTRODUCTION
Deep and wider neural networks have the capacity to learn a complex unknown function from the
training data. The network reported in Dean et al. has 1.7 Billion parameters and is trained on
tens of thousands of CPU cores. Similarly Simonyan & Zisserman has employed 16-18 layers
and achieved excellent classiﬁcation results on the ImageNet dataset. The high computationally
complexity of wide and deep networks is a major obstacle in porting the beneﬁts of deep learning
to resource limited devices. Therefore, many researchers have proposed ideas to accelerate deep
networks for real-time inference Yu et al. ; Han et al. ; Mathieu et al. ; Anwar
et al. .
Network pruning is one promising techique that ﬁrst learns a function with a suﬁciently large sized
network followed by removing less important connections Yu et al. ; Han et al. ;
Anwar et al. . This enables smaller networks to inherit knowledge from the large sized predecessor networks and exhibit comparable level of performance. The works of Han et al. 
introduce ﬁne grained sparsity in a network by pruning scalar weights. Due to unstructured sparsity,
the authors employ compressed sparse row/column (CSR/CSC) for sparse representation. Thus the
ﬁne grained irregular sparsity cannot be easily translated into computational speedups.
Sparsity in a deep convolutional neural network (CNN) can be induced at various levels. Figure 1
shows four pruning granularities. At the coarsest level, a full hidden layer can be pruned. This is
shown with a red colored rectangle in Fig. 1(a). Layer wise pruning affects the depth of the network
and a deep network can be converted into a shallow network. Increasing the depth improves the
network performance and layer-wise pruning therefore demand intelligent techniques to mitigate
the performance degradation. The next pruning granularity is removing feature maps Polyak &
Wolf ; Anwar et al. . Feature map pruning removes a large number of kernels and is
therefore very destructive in nature. We therefore may not achieve higher pruning ratios. For the
depicted architecture in Fig. 1 (b)., pruning a single feature map, zeroes four kernels. Feature map
pruning affects the layer width and we directly obtain a thinner network and no sparse representation
 
Under review as a conference paper at ICLR 2017
(a) Layer-wise pruning
(b) Feature map pruning
(c) k × k Kernel-pruning
(d) Intra-kernel-pruning
Pruning granularity (coarse (left) to fine grained (right)
Inducible pruning ratios Inside allowable budget (increasing from left to right), (e.g., budget = ↓Accuracy)
Sparse representation (increasing complexity from left to right)
Depth reduction
Width reduction
Figure 1: (a-d) shows four possible pruning granularities. The proposed work is focussed on the (b)
feature map and (c) kernel pruning for simple sparse represenation. It can be observed that for the
depicted architecture in Fig. (b), four convolution kernels are pruned.
is needed. Kernel purning is the next pruning granularity and it prunes k × k kernels. It is neither
too ﬁne nor too coarse and is shown in Fig. 1(c). Kernel pruning is therefore a balanced choice
and it can change the dense kernel connectivity pattern to sparse one. Each convolution connection
involves W × H × k × k MAC operations where W, H and k represents the feature map width,
height and the kernel size respectively. Further the sparse representation for kernel-Pruning is also
very simple. A single ﬂag is enough to represent one convolution connection.The conventional
pruning techniques induce sparsity at the ﬁnest granularity by zeroing scalar weights. This sparsity
can be induced in much higher rates but demands sparse representation for computational beneﬁts
Han et al. . Therefore the high pruning ratios do not directly translate into computational
speedups. Figure 1(d) shows this with red colored zeroes in the kernel. Further Fig. 1 summarizes
the relationship between three related factors: the pruning granularities, the pruning ratios and the
sparse representations. Coarse pruning granularities demand very simple sparse representation but
we cannot achieve very high pruning ratios. Similarly ﬁne grained pruning granularities can achieve
higher pruning ratios but the sparse representation is more complicated.
The reference work of Anwar et al. analysed feature map pruning with intra-kernel strided
sparsity. To reduce the size of feature map and kernel matrices, they further imposed a constraint
that all the outgoing kernels from a feature map must have the same pruning mask. In this work,
we do not impose any such constraint and the pruning granularities are coarser. We argue that this
kind of sparsity is useful for VLSI and FFT based implementations. Moreover we show that the best
pruning results are obtained when we combine feature map and kernel level pruning. The selection
of feature map and kernel pruning candidates with a simple technique is another contribution of this
The rest of the paper is organized as follows. In Section 2, recent related works are revisited. Section 3 discusses the pruning candidate selection. Section 4 discusses the two pruning granularities.
Section 5 presents the experimental results while Section 6 concludes the discussion and adds the
future research dimensions of this work.
RELATED WORK
In the literature, network pruning has been studied by several researches Han et al. ; Yu
et al. ; Castellano et al. ; Collins & Kohli ; Stepniewski & Keane ; Reed
 . Collins & Kohli have proposed a technique where irregular sparsity is used to reduce the computational complexity in convolutional and fully connected layers. However they have
not discussed how the sparse representation will affect the computational beneﬁts. The works of
Han et al. introduce ﬁne-grained sparsity in a network by pruning scalar weights. If the
absolute magnitude of any weight is less than a scalar threshold, the weight is pruned. This work
therefore favours learning with small valued weights and train the network with the L1/L2 norm
augmented loss function. Due to pruning at very ﬁne scales, they achieve excellent pruning ratios.
Under review as a conference paper at ICLR 2017
N number of random combinations
MCRvalidation set
Prune Ratio (%) 87.0536
Prune Ratio (%) 77.5298
Prune Ratio (%) 58.3168
Prune Ratio (%) 38.5913
Prune Ratio (%) 18.6839
(a) Best of N random masks
(b) Selecting pruning candidates with weight sum
Figure 2: (a) MNIST for the network architecture: 1 × 6(C5) −16(C5) −120(C5) −84 −10.
This ﬁgure compares the best candidate selected out of N random combinations for various pruning
ratios. This ﬁgure is plotted for kernel level pruning but is equally applicable to feature map pruning.
(b) This Figure explains the idea presented in Li et al. and shows three layers, L1, L2 and
L3. All the ﬁlters/kernels from previous layer to a feature map constitute one group which is shown
with similr color. The S1,S2 and S3 is computed by summing the absolute value of all the weights
in this group.
However this kind of pruning results in irregular connectivity patterns and demand complex sparse
representation for computational beneﬁts. Convolutions are unrolled to matrix-matrix multiplication
in Chellapilla et al. for efﬁcient implementation. The work of Lebedev & Lempitsky 
also induce intra-kernel sparsity in a convolutional layer. Their target is efﬁcient computation by unrolling convolutions as matrx-matrix multiplication. Their sparse representation is not also simple
because each kernel has an equally sized pruning mask. A recently published work propose sparsity
at a higher granularity and induce channel level sparsity in a CNN network for deep face application
Polyak & Wolf . The work of Castellano et al. ; Collins & Kohli ; Stepniewski
& Keane ; Reed utilize unstructured ﬁne grained sparsity in a neural network. Fixed
point optimization for deep neural networks is employed by Anwar et al. ; Hwang & Sung
 ; Sung et al. for VLSI based implementations.
PRUNING CANDIDATE SELECTION
The learning capability of a network is determined by its architecture and the number of effective
learnable parameters. Pruning reduces this number and inevitably degrades the classiﬁcation performance. The pruning candidate selection is therefore of prime importance. Further the pruned
network is retrained to compensate for the pruning losses Yu et al. . For a speciﬁc pruning
ratio, we search for the best pruning masks which afﬂicts the least adversary on the pruned network. Indeed retraining can partially or fully recover the pruning losses, but the lesser the losses, the
more pluasible is the recovery. Further small performace degradation also means that the successor
student network has lost little or no knowledge of the predecessor teacher network. If there are M
potential pruning candidates, the total number of pruning masks is (2M) and an exhaustive search
is therefore infeasible even for a small sized network. We therefore propose a simple strategy for
selecting pruning candiates.
We evaluate N random combinations and compute the MCR for each one. We then choose the
best pruning mask which causes the least degradation to the network performance on the validation
set. Consider that for the depicted architecture in Fig.2b, we need to select feature map pruning
candidates in Layer L2 and L3 with 1/3 pruning ratio. If N = 4, the following N ordered pairs of
feature maps may be randomly selected for (L2, L3) : (1, 2), (2, 3), (3, 1), (1, 1). These combinations
generate random paths in the network and we evaluate the validation set MCR through these routes
in the network. However, this further raises the question of how to approximate N. We report the
relationship between pruning ratio and N in Fig. 2a. This analaysis is conducted for kernel level
Under review as a conference paper at ICLR 2017
Feature Map Pruning Ratio
MCRvalidation set
Baseline MCR 0.62%
Pruning with weight sum voting
Pruning with the best of 10 random masks
Pruning with the best of 20 random masks
Pruning with the best of 50 random masks
Pruning with the best of 100 random masks
Pruning with the best of 200 random masks
(a) Best of N random masks vs Li et al. 
Pruning Ratio
MCRTest Set After Retraining
(1). MCRBaseline = 0.79%
(2). Kernel Pruning
(3). FeatureMap Pruning
(b) MNIST feature map and kernel pruning
Figure 3: (a)This ﬁgure compares the feature map pruning candidate selection with weight sum voting and selecting the best out of N random combinations for various pruning ratios. The experiment
is conducted with the MNIST network architecture: 1 × 16(C5) −16 × 32(C5) −32 × 64(C5) −
120 −10. (b) Figure Feature map and kernel level pruning applied to the MNIST network. the
architecture: 1 × 6(C5) −6 × 16(C5) −16 × 120(C5) −84 −10.
pruning but is also applicable to feature map level pruning. From Fig. 2a., we can observe that for
higher pruning ratios, high value of N is beneﬁcial as it results in better pruning candidate selection.
For the pruning ratio of no more than 40%, N = 100 random evaluations generate good selections.
For lower pruning ratios, retraining is also more likely to compensate the losses as the non-pruned
parameters may still be in good numbers. The computational cost of this technique is not much as
the evaluation is done on the small sized validation set. By observing Fig. 2a., we propose that the
value of N can be estimated intially and later used in several pruning passes.
We further explain and compare this method with the weight sum criterion proposed in Li et al.
 and shown in Fig. 2b. The set of ﬁlters or kernels from the previous layer constitute a group.
This is shown with the similar color in Fig. 2b. According to Li et al. , the absolute sum of
weights determine the importance of a feature map. Suppose that in Fig.2b, the Layer L2 undergoes
feature map pruning. The weight sum criterion computes the absolute weight sum at S1, S2 and
S3. If we further suppose that the pruning ratio is 1/3, then the min(S1, S2, S3) is pruned. All the
incoming and outgoing kernels from the pruned feature map are also removed. We argue that the
sign of a weight in kernel plays important role in well-known feature extractors and therefore this is
not a good criterion.
We compare the performance of the two algoirthms and Fig. 3a shows the experimental results.
These results present the network status before any retraining is conducted. We report the performance degradation in the network classifcation against the pruning ratio. From Fig. 3a, we can
observe that our proposed method outperforms the weight sum method particularly for higher pruning ratios. This is attributed to evaluating pruning candidates in combinations. The criterion in Li
et al. evaluates the importance of a pruning unit in isolation while our proposed approach
evaluates several paths through the network and selects the best one. The combinations work togehter and matter more instead of individidual units. Further, our proposed technique is generic and
can be used for any pruning granularity: feature map, kernel and intra-kernel pruning.
FEATURE MAP AND KERNEL PRUNING
In this section we discuss feature map and kernel pruning granularities. For a similar sized network,
we analyse the achievable pruning ratios with feature map and kernel pruning. In terms of granularity, feature map pruning is coarser than kernel pruning. Feature map pruning does not need any
sparse representation and the pruned network can be implemented in a conventional way, convolution lowering Chellapilla et al. or convolution with FFTs Mathieu et al. . The main
Under review as a conference paper at ICLR 2017
Table 1: Speciﬁcations of the three CIFAR-10 networks
Architecture
Baseline MCR(%)
2x128C3-MP2-2x128C3-MP2-2x256C3-256FC-10Softmax
2x128C3-MP2-2x256C3-MP2-2x256C3-1x512C3-1024FC-1024FC-10Softmax
focus of the proposed work is analysing the unconstrained kernel pruning and feature map pruning. Pruning a feature map causes all the incoming and outgoing kernels to be zeroed because the
outgoing kernels are no more meaningful.
Kernel pruning is comparatively ﬁner. The dimension and connectivity pattern of 2D kernels determine the computing cost of a convolutional layer. The meshed fully connected convolution layers
increases this cost and can hinder the real-time inference. The unconstrained kernel pruning converts
this dense connectivity to sparse one. Kernel-pruning zeroes k ×k kernels and is neither too ﬁne nor
too coarse. Kernel level pruning provides a balance between ﬁne graind and coarse graind pruining.
It is coarse than intra-kernel sparsity and ﬁner than feature map pruning. Thats why good pruing
ratios can be achieved at very small sparse representation and computational cost. Each convolution
connection represents one convolution operation which involves Width × Height × k × k MAC
operations. In LeNet LeCun et al. , the second convolution layer has 6 × 16 feature maps
and the kernel connectivity has a ﬁxed sparse pattern. With kernel pruning, we learn this pattern
and achieve the best possible pruning ratios. We ﬁrst select pruning candidates with the criterion
outlined in Section 2. The pruned network is then retrained to compensate for the losses incurred
due to pruning. Figure 3b shows the feature map and kernel level pruning applied to MNIST LeCun
et al. network. When pruning ratios increase beyond 60%, feature map pruning degrades the
performance much. However the kernel level pruning can achieve higher pruning ratios due to ﬁner
scale granularity.
As the sparse granularities are coarse, a generic set of computing platform can beneﬁt from it.
One downside of the unconstrained kernel pruning is that convolutions can not be unrolled as
matrix-matrix multiplications Chellapilla et al. . However, customized VLSI implementations and FFT based convolutions do not employ convolution unrolling. Mathieu et. al., have proposed FFT based convolutions for faster CNN training and evaluation Mathieu et al. . The
GPU based parallel implementation showed very good speedups. As commonly known that the
IFFT(FFT(kerenel) × FFT(fmap)) = kernel ∗fmap, the kernel level pruning can relieve
this task. Although the kernel size is small, massive reusability enables the use of FFT. The FFT
of each kernel is computed only once and reused for multiple input vectors in a mini-batch. In a
feed forward and backward path, the summations can be carried in the FFT domain and once the
sum is available, the IFFT can be performed Mathieu et al. . Similarly, a customized VLSI
based implementation can also beneﬁt from the kernel level pruning. If the VLSI implementation
imposes a constraint on the pruning criterion, such as the ﬁxed number of convolution kernels from
the previous to the next layer, the pruning criterion can be adapted accordingly. Figure 3b shows
that the kernel pruning can be induced in much higher rates with minor increase in the MCR of the
baseline MNIST network. In the next Section, we report and discuss the experimental results in
EXPERIMENTAL RESULTS
In this section, we present detailed experimental results with the CIFAR-10 and SVHN datasets
Krizhevsky & Hinton . During training and pruning, we use the stochastic gradient descent
(SGD) with a mini-batch size of 128 and RMSProp Tieleman & Hinton . We train all the
networks with batch normalization Ioffe & Szegedy . We do not prune the network in small
steps, and instead one-shot prune the network for a given pruning ratio followed by retraining. The
experimental results are reported in the corresponding two subsections.
Under review as a conference paper at ICLR 2017
Pruning Ratio
MCRTest Set After Retraining
(1). MCRBaseline = 16.260%
(2). MCRBaseline + Tol(1.0) = 17.26%
(3). FeatureMap Pruning
(5). Kernel Pruning
(a) CNNsmall kernel and feature map pruning
Pruning Ratio
MCRTest Set After Retraining
(1). MCRBaseline = 16.260%
(2). MCRBaseline + Tol(1.0) = 17.26%
(3). FeatureMap Pruning
(4). Feature Map Followed by Kernel Pruning
(5). Kernel Pruning
(6). Kernel Prune Followed by Feature Map Pruning
(b) CNNsmall kernel and feature map pruning applied
in various combinations.
Figure 4: (a)This ﬁgure prunes the CNNsmall network with feature maps and kernels. (b) Here
we show that higher pruning ratios can be achieved if we apply pruning granularities in various
combinations.
The CIFAR-10 dataset includes samples from ten classes: airplane, automobile, bird, cat, deer,
dog, frog, horse, ship and truck. The training set consists of 50,000 RGB samples and we allocate
20% of these samples as validation set. Test set contains 10,000 samples and each sample has
32 × 32 × RGB resolution. We evaluate the proposed pruning granularities with two networks.
CNNsmall and CNNlarge. CNNsmall has six convolution and two overlapped max pooling layers.
We report the network architecture with an alphanumeric string as reported in Courbariaux et al.
 and outlined in Table 1. The (2 × 128C3) represents two convolution layers with each
having 128 feature maps and 3 × 3 convolution kernels. MP2 represents 3 × 3 overlapped maxpooling layer with a stride size of 2. We pre-process the original CIFAR-10 dataset with global
contrast normalization followed by zero component analysis (ZCA) whitening.
The CNNlarge has seven convolution and two max-pooling layers. Furthe, online data augmentations are employed to improve the classiﬁcation accurracy. We randomly crop 28 × 28 × 3 patches
from the 32 × 32 × 3 input vectors. These cropped vectors are then geometrically transformed
randomly. A vector may be ﬂipped horizontally or vertically, rotated, translated and scaled. At evaluation time, we crop patches from the four corners and the center of a 32 × 32 × 3 patch and ﬂip it
horizontally. We average the evaluation on these ten 28 × 28 × 3 patches to decide the ﬁnal label.
Due to larger width and depth, the CNNlarge achieves more than 90% accurracy on the CIFAR-10
dataset. The CNNsmall is smaller than CNNlarge and trained without any data augmentation. The
CNNsmall therefore achieves 84% accurracy.
FEATURE MAP AND KERNEL LEVEL PRUNING
After layer pruning, feature map pruning is the 2nd coarsest pruning granularity. Feature map pruning reduces the width of a convolutional layer and generates a thinner network. Pruning a single
feature map, zeroes all the incoming and outgoing weights and therefore, higher pruning ratios degrade the network classiﬁcation performance signiﬁcantly. Feature map pruning for the CNNcifar
is shown in Fig. 4a with a circle marked red colored line. The sparsity reported here is for Conv2
to Conv6. We do not pruned the ﬁrst convolution layer as it has only 3 × 128 × (3 × 3) = 3456
weights. The horizontal solid line shows the baseline MCR of 16.26% whereas the dashed line
shows the 1% tolerance bound. Training the network with batch normalization Ioffe & Szegedy
 enables us to directly prune a network for a target ratio, instead of taking small sized steps.
With a baseline performance of 16.26%, the network performance is very bad at 80% feature map
pruning. We can observe that 62% pruning ratio is possible with less than 1% increase in MCR.
The CNNcifar is reduced to (128C3 −83C3)-MP3-(83C3 −83C3)-MP3-(166C3 −166C3)-
256FC-10Softmax. As pruning is only applied in Conv2 to Conv6, therefore the Figure 4a pruning
ratios are computed only for these layers.
Under review as a conference paper at ICLR 2017
Table 2: Feature map and kernel level pruning (75%) in CNNsmall
Fmap Pruned
Pruned Kernels
Kernel Conn
Kernel Prune
(3 × 3 = 9)
C2 (128x128)
C2 (128x89), 30.5
27306/9 = 3034
3034/11392 = 26.6
C3 (128x128)
C3 (89x89), 51.5
18702/9 = 2078
2078/7921 = 26.2
C4 (128x128)
C4 (89x89), 51.5
18702/9 = 2078
2078/7921 = 26.2
C5 (128x256)
C5 (89x179), 51.4
37881/9 = 4209
4209/15931 = 26.4
C6 (266x256)
C6 (179x179), 51.1
76851/9 = 8539
8539/32041 = 26.6
For the same network, we can see that kernel level pruning performs better. We can achieve 70%
sparsity with kernel level pruning. This is attributed to the fact that kernel pruning is ﬁner and
hence it achieves higher ratios. Further kernel pruning may ultimately prune a feature map if all the
incoming kernels are pruned. However at inference time, we need to deﬁne the kernel connectivity
pattern which can simply be done with a binary ﬂag. So although the sparse representation is needed,
it is quite simple and straightforward. Experimental results conﬁrm that ﬁne grained sparsity can
be induced in higher rates. We achieved 70% kernel wise sparsity for Conv2 - Conv6 and the best
pruned network is layer wise reported in Table ??. The speedup and acceleration with these pruning
granularities is platform independent.
COMBINATIONS OF KERNEL AND FEATURE MAP PRUNING
In this section we discuss the various pruning granularities applied in different combinations. We
ﬁrst apply the feature map and kernel pruning to the CNNsmall network in different orders. With
feature map pruning, we can achieve 60% sparsity under the budget of 1% increase in MCR. But at
this pruning stage, the network learning capability is affected much. So we take a 50% feature map
pruned network, where the CNNsmall is reduced to (128C3 −89C3)-MP3-(89C3 −89C3)-MP3-
(179C3 −179C3)-256FC-10Softmax. As pruning is only applied to Conv2 −Conv6, therefore
in Fig. 4., pruning ratios are computed only for these layers. This network then undergoes kernel
level pruning. The blue rectange line in Figure 4 shows the pruning results. We achieve the best
pruning results in this case and the ﬁnal pruned network is reported in detail in Table 3. Overall we
achieve more than 75% pruning ratio in the ﬁnal pruned network.
We further conducted experiments on the CNNlarge and the corresponding plots are shown in Fig.
5. The CNNlarge is much wider and deeper than the CNNsmall as reported in Table 1. Therefore
there are more chances of redundancy and hence more room for pruning. Further we observe similar
trends as CNNsmall where the kernel pruning can be induced in higher ratios compared to the
feature map pruning. When the kernel pruning is applied to the feature map pruned network, we
can achieve more than 88% sparsity in the Conv2 −Conv7 of the CNNlarge network. This way
we show that our proposed technique has good scalability. These results are in conformity to the
resiliency analysis of ﬁxed point deep neural networks Sung et al..
The SVHN dataset consists of 32 × 32 × 3 cropped images of house numbers [Netzer et al. 2011]
and bears similarity with the MNIST handwritten digit recognition dataset [LeCun et al. 1998].
The classiﬁcation is challenging as more than one digit may appear in sample and the goal is to
identify a digit in the center of a patch. The dataset consists of 73,257 digits for training, 26,032 for
testing and 53,1131 extra for training. The extra set consists of easy samples and may augment the
training set. We generate a validation set of 6000 samples which consists of 4000 samples from the
training set and 2000 samples from the extra [Sermanet et al. 2012]. The network architecture is
reported like this: (2 × 64C3)-MP2- (2 × 128C3)-MP2-(2 × 128C3)-512FC-512FC-10Softmax.
This network is trained with batch normalization and we achieve the baseline MCR of 3.5% on the
test set. The corresponding pruning plots are reported in Fig. 6. We can observe a similar trend
where kernels can be pruned by a bigger ratio compred to feature maps. More than 70% pruning
ratio can be implemented in the reported network. Thus we show that the lessons learnt generalize
well on various datasets.
Under review as a conference paper at ICLR 2017
Prune RatioConv2-Conv7
MCRTest Set After Retraining
FeatureMap Pruning
Kernel Pruning
FeatureMap followed by Kernel Pruning
Baseline MCR = 9.39%
Baseline + Tolerance (1.0%)
Figure 5: The ﬁgure shows various pruning combinations applied to the CIFAR-10 CNNlarge.
Prune Ratio
MCRTest Set
Feature map pruning
Kernel Pruning
Baseline MCR is 3.5%
Tolerance MCR is 4.00%
Figure 6: This Figure shows pruning applied in various combinations to the SVHN CNN network.
CONCLUDING REMARKS
In this work we proposed feature map and kernel pruning for reducing the computational complexity
of deep CNN. We have discussed that the cost of sparse representation can be avoided with coarse
Under review as a conference paper at ICLR 2017
pruning granularities. We demonstrated a simple and generic algorithm for selecting the best pruning mask. We conducted experiments with several benchmarks and networks and showed that the
proposed technique has good scalability. We are exploring online pruning in future for exploiting
run-time beneﬁts.
ACKNOWLEDGMENT
This work was supported by the National Research Foundation of Korea (NRF) grant funded by the
Korean government (MSIP) (No. 2015R1A2A1A10056051).