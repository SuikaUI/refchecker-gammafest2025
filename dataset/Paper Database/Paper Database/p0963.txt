Inside-Outside Net: Detecting Objects in Context with Skip Pooling and
Recurrent Neural Networks
Sean Bell1
C. Lawrence Zitnick2
Kavita Bala1
Ross Girshick2
1Cornell University
2Microsoft Research∗
{sbell,kb}@cs.cornell.edu
 
It is well known that contextual and multi-scale representations are important for accurate visual recognition.
In this paper we present the Inside-Outside Net (ION), an
object detector that exploits information both inside and
outside the region of interest. Contextual information outside the region of interest is integrated using spatial recurrent neural networks. Inside, we use skip pooling to extract information at multiple scales and levels of abstraction. Through extensive experiments we evaluate the design
space and provide readers with an overview of what tricks
of the trade are important. ION improves state-of-the-art on
PASCAL VOC 2012 object detection from 73.9% to 76.4%
mAP. On the new and more challenging MS COCO dataset,
we improve state-of-art-the from 19.7% to 33.1% mAP. In
the 2015 MS COCO Detection Challenge, our ION model
won the Best Student Entry and ﬁnished 3rd place overall.
As intuition suggests, our detection results provide strong
evidence that context and multi-scale representations improve small object detection.
1. Introduction
Reliably detecting an object requires a variety of information, including the object’s ﬁne-grained details and the
context surrounding it.
Current state-of-the-art detection
approaches only use information near an object’s
region of interest (ROI). This places constraints on the type
and accuracy of objects that may be detected.
We explore expanding the approach of to include two
additional sources of information. The ﬁrst uses a multiscale representation that captures ﬁne-grained details
by pooling from multiple lower-level convolutional layers
in a ConvNet . These skip-layers span
multiple spatial resolutions and levels of feature abstraction.
The information gained is especially important for small objects, which require the higher spatial resolution provided
by lower-level layers.
∗Ross Girshick and C. Lawrence Zitnick are now at Facebook AI Research.
For each ROI
L2 normalize
ROI Pooling
Figure 1. Inside-Outside Net (ION). In a single pass, we extract
VGG16 features and evaluate 2000 proposed regions of interest (ROI). For each proposal, we extract a ﬁxed-size descriptor from several layers using ROI pooling . Each descriptor is
L2-normalized, concatenated, scaled, and dimension-reduced (1x1
convolution) to produce a ﬁxed-length feature descriptor for each
proposal of size 512x7x7. Two fully-connected (fc) layers process
each descriptor and produce two outputs: a one-of-K class prediction (“softmax”), and an adjustment to the bounding box (“bbox”).
Figure 2. Challenging detections on COCO 2015 test-dev using
our model trained on COCO “2014 train.”
Our second addition is the use of contextual information. It is well known in the study of human and computer
vision that context plays an important role in visual recognition . To gather contextual information we explore the use of spatial Recurrent Neural Networks (RNNs).
These RNNs pass spatially varying contextual information
both horizontally and vertically across an image. The use of
at least two RNN layers ensures information may be propagated across the entire image. We compare our approach to
other common methods for adding contextual information,
 
including global average pooling and additional convolutional layers. Global average pooling provides information
about the entire image, similar to the features used for scene
or image classiﬁcation .
Following previous approaches , we use object proposal detectors to identify ROIs in an image.
Each ROI is then classiﬁed as containing one or none of
the objects of interest. Using dynamic pooling we can
efﬁciently evaluate thousands of different candidate ROIs
with a single forwards pass of the network. For each candidate ROI, the multi-scale and context information is concatenated into a single layer and fed through several fully
connected layers for classiﬁcation.
We demonstrate that both sources of additional information, context and multi-scale, are complementary in nature. This matches our intuition that context features look
broadly across the image, while multi-scale features capture
more ﬁne-grained details. We show large improvements on
the PASCAL VOC and Microsoft COCO object detection datasets and provide a thorough evaluation of the
gains across different object types. We ﬁnd that the they
are most signiﬁcant for object types that have been historically difﬁcult. For example, we show improved accuracy
for potted plants which are often small and amongst clutter.
In general, we ﬁnd that our approach is more adept at detecting small objects than previous state-of-the-art methods.
For heavily occluded objects like chairs, gains are found
when using contextual information.
RNNs , skip-layer connections )
have precedents in the literature, we demonstrate that their
well-executed combination has an unexpectedly positive
impact on the detector’s accuracy.
As always, the devil
is in the details and thus our paper aims to provide a
thorough exploration of design choices and their outcomes.
Contributions.
We make the following contributions:
1. We introduce the ION architecture that leverages context and multi-scale skip pooling for object detection.
2. We achieve state-of-the-art results on PASCAL VOC
2007, with a mAP of 79.2%, VOC 2012, with a mAP
of 76.4%, and on COCO, with a mAP of 24.9%.
3. We conduct extensive experiments evaluating choices
like the number of layers combined, using a segmentation loss, normalizing feature amplitudes, different
IRNN architectures, and other variations.
4. We analyze the detector’s performance and ﬁnd improved accuracy across the board, but, in particular,
for small objects.
2. Prior work
ConvNet object detectors.
ConvNets with a small number of hidden layers have been used for object detection
for the last two decades (e.g., from to ).
recently, they were successful in restricted domains such
as face detection. Recently, deeper ConvNets have led to
radical improvements in the detection of more general object categories. This shift came about when the successful
application of deep ConvNets to image classiﬁcation 
was transferred to object detection in the R-CNN system of
Girshick et al. and the OverFeat system of Sermanet et
al. . Our work builds on the rapidly evolving R-CNN
(“region-based convolutional neural network”) line of work.
Our experiments are conducted with Fast R-CNN , which
is an end-to-end trainable reﬁnement of He et al.’s SPPnet . We discuss the relationship of our approach to
other methods later in the paper in the context of our model
description and experimental results.
Spatial RNNs.
Recurrent Neural Networks (RNNs) exist in various extended forms, including bidirectional
RNNs that process sequences left-to-right and rightto-left in parallel. Beyond simple sequences, RNNs exist in
full multi-dimensional variants, such as those introduced by
Graves and Schmidhuber for handwriting recognition.
As a lower-complexity alternative, explore running
an RNN spatially (or laterally) over a feature map in place
of convolutions. These papers examine spatial RNNs for
the tasks of semantic segmentation and image classiﬁcation,
respectively. We employ spatial RNNs as a mechanism for
computing contextual features for use in object detection.
Skip-layer connections.
Skip-layer connections are a
classic neural network idea wherein activations from a
lower layer are routed directly to a higher layer while bypassing intermediate layers. The speciﬁcs of the wiring and
combination method differ between models and applications. Our usage of skip connections is most closely related
to those used by Sermanet et al. (termed “multi-stage
features”) for pedestrian detection. Different from , we
ﬁnd it essential to L2 normalize activations from different
layers prior to combining them.
The need for activation normalization when combining features across layers was recently noted by Liu et al.
(ParseNet ) in a model for semantic segmentation that
makes use of global image context features. Skip connections have also been popular in recent models for semantic
segmentation, such as the “fully convolutional networks”
in , and for object instance segmentation, such as the
“hypercolumn features” in .
segmentation
(optional regularizer)
recurrent transitions
input-tohidden
transition)
input-tohidden
transition)
(hidden-tooutput
transition)
(hidden-to-hidden, equation 1)
recurrent transitions
(hidden-to-hidden, equation 1)
Figure 3. Four-directional IRNN architecture. We use “IRNN” units which are RNNs with ReLU recurrent transitions, initialized
to the identity. All transitions to/from the hidden state are computed with 1x1 convolutions, which allows us to compute the recurrence
more efﬁciently (Eq. 1). When computing the context features, the spatial resolution remains the same throughout (same as conv5). The
semantic segmentation regularizer has a 16x higher resolution; it is optional and gives a small improvement of around +1 mAP point.
3. Architecture: Inside-Outside Net (ION)
In this section we describe ION, a detector with an improved descriptor both inside and outside the ROI. An image is processed by a single deep ConvNet, and the convolutional feature maps at each stage of the ConvNet are
stored in memory. At the top of the network, a 2x stacked
4-directional IRNN (explained later) computes context features that describe the image both globally and locally. The
context features have the same dimensions as “conv5.” This
is done once per image. In addition, we have thousands
of proposal regions (ROIs) that might contain objects. For
each ROI, we extract a ﬁxed-length feature descriptor from
several layers (“conv3”, “conv4”, “conv5”, and “context
features”).
The descriptors are L2-normalized, concatenated, re-scaled, and dimension-reduced (1x1 convolution)
to produce a ﬁxed-length feature descriptor for each proposal of size 512x7x7. Two fully-connected (FC) layers
process each descriptor and produce two outputs: a one-of-
K object class prediction (“softmax”), and an adjustment to
the proposal region’s bounding box (“bbox”).
The rest of this section explains the details of ION and
motivates why we chose this particular architecture.
3.1. Pooling from multiple layers
Recent successful detectors such as Fast R-CNN, Faster
R-CNN , and SPPnet, all pool from the last convolutional layer (“conv5 3”) in VGG16 . In order to extend
this to multiple layers, we must consider issues of dimensionality and amplitude.
Since we know that pre-training on ImageNet is important to achieve state-of-the-art performance , and
we would like to use the previously trained VGG16 network , it is important to preserve the existing layer
shapes. Therefore, if we want to pool out of more layers,
the ﬁnal feature must also be shape 512x7x7 so that it is
the correct shape to feed into the ﬁrst fully-connected layer
(fc6). In addition to matching the original shape, we must
also match the original activation amplitudes, so that we can
feed our feature into fc6.
To match the required 512x7x7 shape, we concatenate
each pooled feature along the channel axis and reduce the
dimension with a 1x1 convolution. To match the original
amplitudes, we L2 normalize each pooled ROI and re-scale
back up by an empirically determined scale. Our experiments use a “scale layer” with a learnable per-channel scale
initialized to 1000 (measured on the training set). We later
show in Section 5.2 that a ﬁxed scale works just as well.
As a ﬁnal note, as more features are concatenated together, we need to correspondingly decrease the initial
weight magnitudes of the 1x1 convolution, so we use
“Xavier” initialization .
3.2. Context features with IRNNs
Our architecture for computing context features in ION
is shown in more detail in Figure 3. On top of the last convolutional layer (conv5), we place RNNs that move laterally
across the image. Traditionally, an RNN moves left-to-right
along a sequence, consuming an input at every step, updating its hidden state, and producing an output. We extend
this to two dimensions by placing an RNN along each row
and along each column of the image. We have four RNNs in
total that move in the cardinal directions: right, left, down,
up. The RNNs sit on top of conv5 and produce an output
with the same shape as conv5.
There are many possible forms of recurrent neural networks that we could use: gated recurrent units (GRU) ,
long short-term memory (LSTM) , and plain tanh recurrent neural networks. In this paper, we explore RNNs
composed of rectiﬁed linear units (ReLU). Le et al. 
recently showed that these networks are easy to train and
are good at modeling long-range dependencies, if the recurrent weight matrix is initialized to the identity matrix. This
means that at initialization, gradients are propagated backwards with full strength. Le et al. call a ReLU RNN
initialized this way an “IRNN,” and show that it performs
almost as well as an LSTM for a real-world language modeling task, and better than an LSTM for a toy memory problem. We adopt this architecture because it is very simple to
implement and parallelize, and is much faster than LSTMs
or GRUs to compute.
For our problem, we have four independent IRNNs that
move in four directions. To implement the IRNNs as efﬁciently as possible, we split the internal IRNN computations
into separate logical layers. Viewed this way, we can see
that the input-to-hidden transition is a 1x1 convolution, and
that it can be shared across different directions. Sharing this
transition allows us to remove 6 conv layers in total with a
negligible effect on accuracy (−0.1 mAP). The bias can be
shared in the same way, and merged into the 1x1 conv layer.
The IRNN layer now only needs to apply the recurrent matrix and apply the nonlinearity at each step. The output from
the IRNN is computed by concatenating the hidden state
from the four directions at each spatial location.
This is the update for an IRNN that moves to the right;
similar equations exist for the other directions:
i,j−1 + hright
Notice that the input is not explicitly shown in the equation,
and there is no input-to-hidden transition. This is because
it was computed as part of the 1x1 convolution, and then
copied in-place to each hidden layer. For each direction, we
can compute all of the independent rows/columns in parallel, stepping all IRNNs together with a single matrix multiply. On a GPU, this results in large speedups compared to
computing each RNN cell one at a time.
We also explore using semantic segmentation labels to
regularize the IRNN output. When using these labels, we
add the deconvolution and crop layer as implemented by
Long et al. . The deconvolution upsamples by 16x with
a 32x32 kernel, and we add an extra softmax loss layer with
a weight of 1. This is evaluated in Section 5.3.
Variants and simpliﬁcations.
We explore several further
simpliﬁcations.
1. We ﬁxed the hidden transition matrix to the identity
hh = I, which allows us to entirely remove it:
i,j−1 + hright
This is like an accumulator, but with ReLU after each
step. In Section 5.5 we show that removing the recurrent matrix has a surprisingly small impact.
“Right of me”
“Left of me”
“Above me”
“Below me”
(a) Output of ﬁrst IRNN
(b) Slice of single cell
Figure 4. Interpretation of the ﬁrst IRNN output. Each cell in the
output summarizes the features to the left/right/top/bottom.
2. To prevent overﬁtting, we include dropout layers (p =
0.25) after each concat layer in all experiments. We
later found that in fact the model is underﬁtting and
there is no need for dropout anywhere in the network.
3. Finally, we trained a separate bias b0 for the ﬁrst step
in the RNN in each direction. However, since it tends
to remain near zero after training, this component is
not really necessary.
Interpretation.
After the ﬁrst 4-directional IRNN (out of
the two IRNNs), we obtain a feature map that summarizes
nearby objects at every position in the image. As illustrated
in Figure 4, we can see that the ﬁrst IRNN creates a summary of the features to the left/right/top/bottom of every
cell. The subsequent 1x1 convolution then mixes this information together as a dimension reduction.
After the second 4-directional IRNN, every cell on the
output depends on every cell of the input. In this way, our
context features are both global and local. The features vary
by spatial position, and each cell is a global summary of the
image with respect to that speciﬁc spatial location.
4. Results
We train and evaluate our dataset on three major datasets:
PASCAL VOC 2007, VOC 2012, and on MS COCO. We
demonstrate state-of-the-art results on all three datasets.
4.1. Experimental setup
All of our experiments use Fast R-CNN built on the
Caffe framework, and the VGG16 architecture ,
all of which are available online.
As is common practice, we use the publicly available weights pre-trained on
ILSVRC2012 downloaded from the Caffe Model Zoo.1
We make some changes to Fast R-CNN, which give a
small improvement over the baseline. We use 4 images per
mini-batch, implemented as 4 forward/backward passes of
single image mini-batches, with gradient accumulation. We
sample 128 ROIs per image leading to 512 ROIs per model
update. We measure the norm of the parameter gradient
vector and rescale it if its L2 norm is above 20 (80 when
accumulating over 4 images).
1 
77.0 78.1 69.3 59.4 38.3 81.6 78.6 86.7 42.8 78.8 68.9 84.7 82.0 76.6
69.9 31.8 70.1 74.8 80.4 70.4
76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5
76.7 38.8 73.6 73.9 83.0 72.6
MR-CNN 
80.3 84.1 78.5 70.8 68.5 88.0 85.9 87.8 60.3 85.2 73.7 87.2 86.5 85.0
76.4 48.5 76.3 75.5 85.0 81.0
ION [ours]
78.2 79.1 76.8 61.5 54.7 81.9 84.3 88.3 53.1 78.3 71.6 85.9 84.8 81.6
74.3 45.6 75.3 72.1 82.6 81.4
ION [ours]
79.2 83.1 77.6 65.6 54.9 85.4 85.1 87.0 54.4 80.6 73.8 85.3 82.2 82.2
74.4 47.1 75.8 72.7 84.2 80.4
ION [ours]
79.2 79.2 77.4 69.8 55.7 85.2 84.2 89.8 57.5 78.5 73.8 87.8 85.9 81.3
75.3 49.7 76.9 74.6 85.2 82.1
ION [ours]
80.2 84.7 78.8 72.4 61.9 86.2 86.7 89.5 59.1 84.1 74.7 88.9 86.9 81.3
80.0 50.9 80.4 74.1 86.6 83.3
ION [ours]
80.2 85.2 78.8 70.9 62.6 86.6 86.9 89.8 61.7 86.9 76.5 88.4 87.5 83.4
80.5 52.4 78.1 77.2 86.9 83.5
Table 1. Detection results on VOC 2007 test. Legend: 07+12: 07 trainval + 12 trainval, 07+12+S: 07+12 plus SBD segmentation
labels , R: include 2x stacked 4-dir IRNN (context features), S: regularize with segmentation labels, W: two rounds of bounding box
regression and weighted voting , D: remove all dropout layers.
82.3 78.4 70.8 52.3 38.7 77.8 71.6 89.3 44.2 73.0 55.0 87.5 80.5 80.8
72.0 35.1 68.3 65.7 80.4 64.2
84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9
79.6 40.1 72.6 60.9 81.2 61.5
FRCN+YOLO 
83.0 78.5 73.7 55.8 43.1 78.3 73.0 89.2 49.1 74.3 56.6 87.2 80.5 80.5
74.7 42.1 70.8 68.3 81.5 67.0
84.2 78.5 73.6 55.6 53.7 78.7 79.8 87.7 49.6 74.9 52.1 86.0 81.7 83.3
81.8 48.6 73.5 59.4 79.9 65.7
MR-CNN 
85.5 82.9 76.6 57.8 62.7 79.4 77.2 86.6 55.0 79.1 62.2 87.0 83.4 84.7
78.9 45.3 73.4 65.8 80.3 74.0
ION [ours]
87.5 84.7 76.8 63.8 58.3 82.6 79.0 90.9 57.8 82.0 64.7 88.9 86.5 84.7
82.3 51.4 78.2 69.2 85.2 73.5
Table 2. Detection results on VOC 2012 test (comp4). Legend: 07+12: 07 trainval + 12 trainval, 07++12: 07 trainvaltest + 12 trainval,
07+12+S: 07+12 plus SBD segmentation labels , R: include 2x stacked 4-dir IRNN (context features), S: regularize with segmentation
labels, W: two rounds of bounding box regression and weighted voting , D: remove all dropout layers.
Avg. Precision, IoU:
Avg. Precision, Area:
Avg. Recall, # Dets:
Avg. Recall, Area:
ION [ours]
ION [ours]
ION [ours]
ION comp.†
trainval35k
ION post.†
trainval35k
Table 3. Detection results on COCO 2015 test-dev. Legend: R: include 2x stacked 4-dir IRNN (context features), S: regularize with
segmentation labels, W: two rounds of bounding box regression and weighted voting , D: remove all dropout layers. *We use a longer
training schedule, resulting in a higher score than the preliminary numbers in .
†test-dev scores for our submission to the 2015 MS
COCO Detection competition, and post-competition improvements, trained on “trainval35k”, described in the Appendix.
To accelerate training, we use a two-stage schedule. As
noted by Girshick , it is not necessary to ﬁne-tune all
layers, and nearly the same performance can be achieved
by ﬁne-tuning starting from conv3 1. With this in mind, we
ﬁrst train for 40k iterations with conv1 1 through conv5 3
frozen, and then another 100k iterations with only conv1 1
through conv2 2 frozen.
All other layers are ﬁne-tuned.
When training for COCO, we use 80k and 320k iterations
respectively. We found that shorter training schedules are
not enough to fully converge.
We also use a different learning rate (LR) schedule. The
LR exponentially decays from 5 · 10−3 to 10−4 in the ﬁrst
stage, and from 10−3 to 10−5 in the second stage. To reduce the effect of random variation, we ﬁx the random seed
so that all variants see the same images in the same order.
For PASCAL VOC we use the same pre-computed selective
search boxes from Fast R-CNN, and for COCO we use the
boxes precomputed by Hosang et al. . Finally, we modiﬁed the test thresholds in Fast R-CNN so that we keep only
boxes with a softmax score above 0.05, and keep at most
100 boxes per images.
When re-running the baseline Fast R-CNN using the
above settings, we see a +0.8 mAP improvement over the
original settings on VOC 2007 test. We compare against
the baseline using our improved settings where possible.
4.2. PASCAL VOC 2007
As shown in Table 1, we evaluate our detector (ION) on
PASCAL VOC 2007, training on the VOC 2007 trainval
dataset merged with the 2012 trainval dataset, a common
practice. Applying our method described above, we obtain
a mAP of 76.5%. We then make some simple modiﬁcations,
as described below, to achieve a higher score of 79.2%.
MR-CNN introduces a bounding box regression
Extra-small Objects
Medium Objects
Extra-large Objects
Figure 5. VOC 2007 normalized AP by size. Left to right: increasing complexity. Left-most bar in each group: Fast R-CNN;
right-most bar: our best model that achieves 79.2% mAP on VOC
2007 test. Our detector has a particularly large improvement for
small objects. See Hoiem for details on these metrics.
scheme to improve results on VOC, where bounding boxes
are evaluated twice: (1) the initial proposal boxes are evaluated and regressed to improved locations and then (2) the
improved locations are passed again through the network.
All boxes are accumulated together, and non-max supression is applied. Finally, a weighted vote is computed for
each kept box (over all boxes, including those suppressed),
where boxes that overlap a kept box by at least 0.5 IoU contribute to the average. For our method, we use the softmax scores as the weights. When adding this scheme to our
method, our mAP rises from 76.5% to 78.5%. Finally, we
observed that our models are underﬁtting and we remove
dropout from all layers to get a further gain up to 79.2%.
MR-CNN also uses context and achieves 78.2%. However, we note that their method requires that pieces are evaluated individually, and thus has a test runtime around 30
seconds per image, while our method is signiﬁcantly faster,
taking 0.8s per image on a Titan X GPU (excluding proposal
generation) without two-stage bounding box regression and
1.15s per image with it.
4.3. PASCAL VOC 2012
We also evaluate on the slightly more challenging VOC
2012 dataset, submitting to the public evaluation server.2 In
Table 2, we show the top methods on the public leaderboard
as of the time of submission. Our detector obtains a mAP
of 76.4%, which is several points higher than the next best
submission, and is the most accurate for most categories.
2Anonymous URL: 
4.4. MS COCO
Microsoft has recently released the Common Objects in
Context dataset, which contains 80k training images and 40k validation images . There is
an associated MS COCO challenge with a new evaluation
metric, that averages mAP over different IoU thresholds,
from 0.5 to 0.95 (written as “0.5:0.95”). This places a signiﬁcantly larger emphasis on localization compared to the
PASCAL VOC metric which only requires IoU of 0.5.
We are only aware of one baseline performance number for this dataset, as published in the Fast R-CNN paper,
which cites a mAP of 19.7% on the 2015 test-dev set .
We trained our own Fast R-CNN model on “2014 train” using our longer training schedule and obtained a higher mAP
of 20.5% mAP on the same set, which we use as a baseline.
As shown in Table 3, when trained on the same images with
the same schedule, our method obtains a large improvement
over the baseline with a mAP of 24.9%.
We tried applying the same bounding box voting
scheme to COCO, but found that performance decreases
on the COCO metric (IOU 0.5:0.95, second row of Table 3).
Interestingly, the scheme increases performance at IoU 0.5
(the PASCAL metric). Since the scheme heuristically blurs
together box locations, it can ﬁnd the general location of
objects, but cannot predict precise box locations, which is
important for the new COCO metric. As described in the
Appendix, we ﬁxed this for our competition submission by
raising the voting IoU threshold from 0.5 to ∼0.85.
We submitted ION to the 2015 MS COCO Detection
Challenge and won the Best Student Entry with 3rd place
overall. Using only a single model (no ensembling), our
submission achieved 31.0% on test-competition score and
31.2% on test-dev score (Table 3). After the competition,
we further improved our test-dev score to 33.1% by adding
left-right ﬂipping and adjusting training parameters. See the
Appendix for details on our challenge submission.
4.5. Improvement for small objects
In general, small objects are challenging for detectors:
there are fewer pixels on the object, they are harder to localize, and there can be many more of them per image.
Small objects are even more challenging for proposal methods. For all experiments, we are using selective search 
for object proposals, which performs very poorly on small
objects in COCO with an average recall under 10% .
We ﬁnd that our detector shows a large relative improvement in this category. For COCO, if we look at small3 objects, average precision and average recall improve from
4.1% to 7.0% and from 7.3% to 10.7% respectively. We
highlight that this is even higher than the baseline proposal
method, which is only possible because we perform bound-
3“Small” means area ≤322 px; about 40% of COCO is “small.”
ROI pooling from:
Merge features using:
L2+Scale+1x1
Table 4. Combining features from different layers. Metric: Detection mAP on VOC07 test. Training set: 07 trainval + 12 trainval. 1x1: combine features from different layers using a 1x1 convolution. L2+Scale+1x1: use L2 normalization, scaling (initialized to 1000), and 1x1 convolution, as described in section 3.1.
These results do not include “context features.” *This entry is the
same as Fast R-CNN , but trained with our hyperparameters.
L2 Normalization method
Sum across channels
Sum over all entries
Table 5. Approaches to normalizing feature amplitude. Metric:
detection mAP on VOC07 test. All methods are regularized with
loss from predicting segmentation.
ing box regression to predict improved box locations. Similarly, we show a size breakdown for VOC2007 test in Figure 5 using Hoiem’s toolkit for diagnosing errors , and
see similarly large improvements on this dataset as well.
5. Design evaluation
In this section, we explore changes to our architecture
and justify our design choices with experiments on PAS-
CAL VOC 2007. All numbers in this section are VOC 2007
test mAP, trained on 2007 trainval + 2012 trainval, with the
settings described in Section 4.1. Note that for this section, we use dropout in all networks, and a single round of
bounding box regression at test time.
5.1. Pool from which layers?
As described in Section 3.1, our detector pools regions
of interest (ROI) from multiple layers and combines the result. A straightforward approach would be to concatenate
the ROI from each layer and reduce the dimensionality using a 1x1 convolution. As shown in Table 4 (left column),
this does not work. In VGG16, the convolutional features at
different layers can have very different amplitudes, so that
naively combining them leads to unstable learning. While it
is possible in theory to learn a model with inputs of very different amplitude, this is ill-conditioned and does not work
well in practice. It is necessary to normalize the amplitude
such that the features being pooled from all layers have similar magnitude. Our method’s normalization scheme ﬁxes
this problem, as shown in Table 4 (right column).
ROI pooling from:
Use seg. loss?
C2 C3 C4 C5 IRNN
Table 6. Effect of segmentation loss. Metric: detection mAP on
VOC07 test. Adding segmentation loss tends to improve detection
performance by about 1 mAP, with no test-time penalty.
5.2. How should we normalize feature amplitude?
When performing L2 normalization, there are a few
choices to be made: do you sum over channels and perform
one normalization per spatial location (as in ParseNet ),
or should you sum over all entries in each pooled ROI and
normalize it as a single blob. Further, when re-scaling the
features back to an fc6-compatible magnitude, should you
use a ﬁxed scale or should you learn a scale per channel?
The reason why you might want to learn a scale per channel
is that you get more sharing than you would if you relied on
the 1x1 convolution to model the scale. We evaluate this in
Table 5, and ﬁnd that all of these approaches perform about
the same, and the distinction doesn’t matter for this problem. The important aspect is whether amplitude is taken
into account; the different schemes we explored in Table 5
are all roughly equivalent in performance.
To determine the initial scale, we measure the mean scale
of features pooled from conv5 on the training set, and use
that as the ﬁxed scale. Using Fast R-CNN, we measured the
mean norm to be approximately 1000 when summing over
all entries, and 130 when summing across channels.
5.3. How much does segmentation loss help?
Although our target task is object detection, many
datasets also have semantic segmentation labels, where the
object class of every pixel is labeled. Many images in PAS-
CAL VOC and every image in COCO has these labels. This
is valuable information that can be incorporated into a training algorithm to improve performance.
As shown in Figure 3, when adding stacked IRNNs it is
possible to have them also predict a semantic segmentation
output—a multitask setup. In Table 6, we see that these
extra labels consistently provide about a +1 point boost in
mAP for object detection. This is because we are training
the network with more bits of supervision, so even though
we are adding extra labels that we do not care about during inference, the features inside the network are trained to
contain more information than they would have otherwise if
only trained on object detection. Since this is an extra layer
used only for training, we can drop the layer at test time and
get a +1 mAP point boost with no change in runtime.
(a) two stacked 3x3 convolution layers
(d) two 4-direction IRNN layers
4-dir IRNN
4-dir IRNN
(b) two stacked 5x5 convolution layers
(c) global averaging and unpooling
unpool (tiling)
Figure 6. Receptive ﬁeld of different layer types. When considering a single cell in the input, what output cells depend on it? (a)
If we add two stacked 3x3 convolutions on top of conv5, then a
cell in the input inﬂuences a 5x5 window in the output. (b) Similarly, for a 5x5 convolution, one cell inﬂuences a 9x9 window in
the output. (c) For global average pooling, every cell in the output depends on the entire input, but the output is the same value
repeated. (d) For IRNNs, every cell in the output depends on the
entire input, but also varies spatially.
Context method
(a) 2x stacked 512x3x3 conv
(b) 2x stacked 256x5x5 conv
(c) Global average pooling
(d) 2x stacked 4-dir IRNN
(a) 2x stacked 512x3x3 conv
(d) 2x stacked 4-dir IRNN
Table 7. Comparing approaches to adding context. All rows
also pool out of conv3, conv4, and conv5. Metric: detection mAP
on VOC07 test. Seg: if checked, the top layer received extra supervision from semantic segmentation labels.
5.4. How should we incorporate context?
While RNNs are a powerful mechanism of incorporating context, they are not the only method. For example, one
could simply add more convolutional layers on top of conv5
and then pool out of the top convolutional layer. As shown
in Figure 6, stacked 3x3 convolutions add two cells worth of
context, and stacked 5x5 convolutions add 6 cells. Alternatively, one could use a global average and unpool (tile or repeat spatially) back to the original shape as in ParseNet .
We compared these approaches on VOC 2007 test,
shown in Table 7. The 2x stacked 4-dir IRNN layers have
fewer parameters than the alternatives, and perform better
on the test set (both with and without segmentation labels).
Therefore, we use this architecture to compute “context features” for all other experiments.
5.5. Which IRNN architecture?
When designing the IRNN for incorporating context,
there are a few basic decisions to be made, namely how
many layers and how many hidden units per layer. In ad-
ROI pooling from:
# IRNN layers
C2 C3 C4 C5 IRNN
Table 8. Varying the number of IRNN layers. Metric: mAP on
VOC07 test. Segmentation loss is used to regularize the top IRNN
layer. All IRNNs use 512 hidden units.
ROI pooling from:
Include Whh?
C3 C4 C5 IRNN
Table 9. Varying the hidden transition. We vary the number of
units and try either learning recurrent transition Whh initialized
to the identity, or entirely removing it (same as setting Whh = I).
dition, we explore the idea of entirely removing the recurrent transition (equivalent to replacing it with the identity
matrix), so that the IRNN consist of repeated steps of: accumulate, ReLU, accumulate, etc. Note that this is not the
same as an integral/area image, since each step has ReLU.
As shown in Table 8, using 2 IRNN layers performs the
best on VOC 2007 test. While stacking more convolution
layers tends to make ConvNets perform better, the same is
not always true for RNNs . We also found that the number of hidden units did not have a strong effect on the performance (Table 9), and chose 512 as the baseline size for
all other experiments.
Finally, we were surprised to discover that removing
the recurrent Whh transition performs almost as well as
learning it (Table 9). It seems that the input-to-hidden and
hidden-to-output connections contain sufﬁcient context that
the recurrent transition can be removed and replaced with
an addition, saving a large matrix multiply.
5.6. Other variations
There are some other variations on our architecture
that perform almost as well, which we summarize in Table 10.
For example, (a) the ﬁrst IRNN only processes
two directions left/right and the second IRNN only processes up/down. This kind of operation was explored in
ReNet and performs the same as modeling all four directions in both IRNN layers. We also explored (b) pooling
out of both IRNNs, and (c) pooling out of both stacked convolutions and the IRNNs. None of these variations perform
better than our main method.
Our method
(a) Left-right then up-down
(b) Pool out of both IRNNs
(c) Combine 2x stacked 512x3x3 conv and IRNN
Table 10. Other variations. Metric: VOC07 test mAP. We list
some other variations that all perform about the same.
6. Conclusion
This paper introduces the Inside-Outside Net (ION), an
architecture that leverages context and multi-scale knowledge for object detection. Our architecture uses a 2x stacked
4-directional IRNN for context, and multi-layer ROI pooling with normalization for improved object description. To
justify our design choices, we conducted extensive experiments evaluating choices like the number of layers combined, using segmentation loss, normalizing feature amplitudes, different IRNN architectures, and other variations.
We achieve state-of-the-art results on both PASCAL VOC
and COCO, and ﬁnd our proposed architecture is particularly effective at improving detection of small objects.
Acknowledgements
This project was the result of an internship at Microsoft
Research (MSR). We would like to thank Abhinav Shrivastava and Ishan Misra for helpful discussions while at MSR.
We thank NVIDIA for the donation of K40 GPUs.