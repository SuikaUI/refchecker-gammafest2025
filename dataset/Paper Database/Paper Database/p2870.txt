Convolutional MKL Based Multimodal
Emotion Recognition and Sentiment Analysis
Soujanya Poria
Temasek Laboratories
Nanyang Technological University
 
Iti Chaturvedi, Erik Cambria
School of Computer Science and Engineering
Nanyang Technological University
{iti,cambria}@ntu.edu.sg
Amir Hussain
School of Natural Sciences
University of Stirling
United Kingdom
 
Abstract—Technology has enabled anyone with an Internet
connection to easily create and share their ideas, opinions and
content with millions of other people around the world. Much
of the content being posted and consumed online is multimodal.
With billions of phones, tablets and PCs shipping today with
built-in cameras and a host of new video-equipped wearables like
Google Glass on the horizon, the amount of video on the Internet
will only continue to increase. It has become increasingly difﬁcult
for researchers to keep up with this deluge of multimodal content,
let alone organize or make sense of it. Mining useful knowledge
from video is a critical need that will grow exponentially, in
pace with the global growth of content. This is particularly
important in sentiment analysis, as both service and product
reviews are gradually shifting from unimodal to multimodal.
We present a novel method to extract features from visual and
textual modalities using deep convolutional neural networks. By
feeding such features to a multiple kernel learning classiﬁer,
we signiﬁcantly outperform the state of the art of multimodal
emotion recognition and sentiment analysis on different datasets.
Index Terms—Multimodal sentiment analysis; Deep learning;
Convolutional neural networks; Multiple kernel learning
I. INTRODUCTION
Sentiment extraction from text has made considerable
progress in the past few years , . People, however, are
gradually shifting from text to video to express their opinion
about a product or service, as it is now much easier and faster
for them to produce and share multimodal content . For the
same reasons, potential customers are now more inclined to
browse for video reviews of the product they are interested in,
rather than looking for lengthy written reviews . Another
reason for doing this is that, while trustable written reviews
are quite hard to ﬁnd, searching for good video reviews is
as easy as typing the name of the product on YouTube and
choosing the clips with more views .
This leads to the need for identifying sentiment and emotions from video as a source of multimodal information. However, there are major challenges which need to be overcome,
e.g., expressiveness of opinion varies widely from person to
person. Some people express their opinions more vocally,
some more visually and others rely exclusively on logic and
express little emotion. Furthermore, plenty of research has
been conducted in the ﬁeld of audio-visual emotion recognition.
Some recent work has also been conducted on fusing different modalities to detect emotions and polarity from videos
 , , . This paper conducts extensive research on the
different facets of this topic and aims to solve the following
two questions:
1) Is a common framework useful for both multimodal
emotion recognition and multimodal sentiment analysis?
2) Can audio, visual and textual features jointly enhance
the performance of sentiment analysis classiﬁers?
In this paper, we propose a temporal convolutional neural
network (CNN) where each pair of images at time t and t + 1
are combined into a single image. Such a model is sensitive
to sequence of images and learns a dictionary of features that
are portable across languages. In a deep CNN, each hidden
layer is obtained by convolving a matrix of weights with the
matrix of activations at the layer below and the weights are
trained using back propagation .
Furthermore, we have additional layers of recurrent neurons
in the deep model. Recurrent neural networks (RNN) have
feedback connections among neurons that can model dependencies in time sequences. Here, each hidden layer state is a
function of the previous state, which can be further expanded
as a function of all the previous states. In , the authors
proposed convolutional RNNs to capture spatial structure
information in static images. In contrast, our model uses RNN
to capture spatial and temporal patterns that are inherent in
video sequence. Our experiments showed that while using
only RNN or deep CNN does not provide good classiﬁcations,
combining the two models results in tremendous speed up and
Multiple kernel learning (MKL) is a feature selection
method where features are organized into groups and each
group has its own kernel function . MKL further improved
our results, as it is able to combine data from different
modalities effectively. Figure 1 illustrates the convolutional
recurrent multiple kernel learning (CRMKL) model, which
combines sentiment features in audio, video and text. In ,
the authors propose the use of a multi-resolution CNN to
capture temporal features in YouTube videos. However, to our
knowledge, this type of temporal CNN has not been previously
used for sentiment analysis.
© 2016 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or
future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for
resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.
n x k representation of
sentence with static and
non-static channels
Convolutional layer with
multiple filter widths and
feature maps
Max-over-time
Fully connected layer
with dropout and
softmax output
Hidden Neurons
Interconnected
RNN Time-delayed
2D Features
2D Features
Up Sampled
2D Features
Transformed 2D feature
Kernel 1 = n1x×n1y
Logistic Layer
1D Features
Kernel 2 = n2x×n2y
Kernel 3 = n3x×n3y
Video Sequence
Fig. 1. The CRMKL model combining sentiment features in audio, video and text.
The organization of the paper is as follows: Section II
reviews related works and datasets on multi-modal sentiment
detection; Section III introduces the datasets used in our
evaluation; Section IV provides the preliminary concepts necessary to understand the present work; Section V describes in
detail the convolutional recurrent MKL framework for feature
extraction and fusion from different modalities; ﬁnally, in
Section VI we validate our method on different datasets.
II. RELATED WORK AND CONTRIBUTIONS
Traditional methods could only classify text into different
topics irrespective of user interests. In contrast, sentiment is
an ordinal variable that can rank user interests in a sequential
order. Sentiment prediction requires understanding of the
sentence context and, hence, is a much more difﬁcult task than
topic classiﬁcation. While sentiment prediction only identiﬁes
positive or negative customer experience, emotion recognition
accounts for speciﬁc emotions, which can be used to create
resonance among reviews for a certain product.
Recent work on text modality has used CNN for sentimentrelated tasks such as sarcasm detection and aspect-based
opinion mining . developed a facial expression coding
system (FACS) and six facial expressions that are able to
provide sufﬁcient clues to detect emotions. Recent studies on
speech-based emotion analysis have focused on identifying
several acoustic features. One of the earliest works on fusing
audio-visual emotion recognition showed that a bimodal
system yields higher accuracy than any unimodal system.
More recent research in audio-visual emotion recognition
has been conducted at either feature level or decision
level . Though there are plenty of research articles on
audio-visual emotion recognition, only a few research works
have been carried out on multimodal emotion recognition or
sentiment analysis using textual clues with visual and audio
modality. and fused information from audio, visual
and textual modalities to extract emotion and sentiment.
 and fused audio and textual modality at feature
level for emotion recognition. fused audio and textual
clues at decision level. Similar to , this paper targets
the classiﬁcation of each sentence or utterance instead of the
entire review. However, in the authors generate userdeﬁned feature-based summaries, which is not scalable in large
datasets. Instead, we consider a deep neural network, where
each layer automatically learns features in an unsupervised
manner. This is followed by ﬁne-tuning using a subset of
known labels. In this way, the model is able to learn the
lexicon of each new dataset during training. The following is
a summary of the signiﬁcance and contributions of this paper:
●The paper combines video, audio and text modality
in order to effectively detect sentiment in a subjectindependent manner. Our ﬁrst contribution is that we
have used MKL to fuse the three modalities. While the
state of the art uses a single kernel support vector
machine (SVM) classiﬁer to fuse all three modalities, we
use multiple kernels to adapt to different modalities and,
hence, achieve higher accuracy.
●Our second contribution is the novel integration of CNN
with a low dimensional RNN, which is computationally
much faster on large video data compared to baselines.
In particular, for better modeling overlaps among features learned during temporal convolution, we consider
distributed time-delayed features in the video. This can
be achieved by initializing the weights of RNN with the
covariance matrix of output feature vectors learned by the
III. DATASETS USED
In this section, we describe the datasets used in the multimodal sentiment analysis and multimodal emotion recognition
experiments. Our method can be easily used for the multi-class
problem of neutral, positive, and negative sentiments. In this
paper, we have followed previous authors on the benchmarks
and excluded neutral reviews so that a simple comparison is
possible. The method can also be used if one or two of the
different modalities namely video, audio and text are present.
A. Multimodal Sentiment Analysis Dataset
We validated our method on three benchmark multimodal
sentiment analysis datasets. The multimodal opinion utterances
dataset (MOUD) was used to train the multimodal sentiment
analysis module. The aim of the experiment was to predict the
target label of each utterance in a video as positive or negative,
where an utterance is a video segment of about 5 seconds.
For our experiment, we use the dataset developed by
Morency et al. . They collected videos from popular social
media (e.g., YouTube) using several keywords (e.g., “favorite
products”) to produce search results consisting of videos of
either product reviews or recommendation.
On average, each video has 6 utterances and each utterance
is 5 seconds long. Each utterance in a video is annotated
separately as positive or negative. Hence, sentiment can change
during the course of a product review. The dataset contains 498
utterances labeled either positive, negative or neutral. In our
experiment, we do not consider neutral labels, that leads to
the ﬁnal dataset consisting of 448 utterances.
Apart from the MOUD dataset, the trained model was
then validated on YouTube and ICT-MMMO dataset. The
former contains 110 negative and 87 positive videos of product
reviews, the latter contains 230 positive and 119 negative
videos. ICT-MMMO dataset, however, is not a utterance-level
dataset. Hence, we manually split the videos into utterances.
B. Multimodal Emotion Recognition Dataset
The USC IEMOCAP database was collected for studying multimodal expressive dyadic interactions. This dataset
contains 12 hours of video data split into 5 minutes of dyadic
interaction between professional male and female actors. Each
interaction session was split into spoken utterances and labeled
by at least 3 annotators into one emotion category, i.e., happy,
sad, angry, surprised, excited, frustration, disgust, fear and
other. The dataset contains 1,083 angry, 1,630 happy, 1,083
sad, and 1,683 neutral videos.
IV. PRELIMINARIES
A. Deep Convolutional Neural Networks
A deep neural network can be viewed as a composite of
simple, unsupervised models such as restricted Boltzmann
machines (RBMs) where each hidden layer serves as the
visible layer for the next RBM. RBM is a bipartite graph
comprising two layers of neurons (a visible and a hidden
layer), where the connections among neurons in the same
layer are not allowed. Such a model can be ﬁrst trained in an
unsupervised manner, followed by ﬁne-tuning using a subset
of the data with known labels.
To train such a multi-layer system, we must compute the
gradient of the total energy function E with respect to the
weights in all the layers. To learn such weights and maximize the global energy function, the approximate maximum
likelihood contrastive divergence approach can be used. This
method employs each training sample to initialize the visible
layer. Next, it uses the Gibbs sampling algorithm to update
the hidden layer and then reconstruct the visible layer consecutively, until convergence. As an example, here we use a
logistic regression model to learn the binary hidden neurons
and each visible unit is assumed to be a sample from a normal
distribution. The continuous state ˆhj of the hidden neuron j,
with bias bj, is a weighted sum over all continuous visible
nodes v and is given by:
ˆhj = bj + ∑
where wij is the connection weight to hidden neuron j from
visible node vi. The binary state hj of the hidden neuron can
be deﬁned by a sigmoid activation function:
1 + e−ˆhj ,
Similarly, in the next iteration, the continuous state of each
visible node vi is reconstructed. Here, we determine the state
of visible node i, with bias ci, as a random sample from the
normal distribution where the mean is a weighted sum over
all binary hidden neurons and is given by:
vi = ci + ∑
where wij is the connection weight to hidden neuron j from
visible node i. This continuous state is a random sample from
N(vi,σ), where σ is the variance of all visible nodes.
Unlike hidden neurons, visible nodes can take continuous
values in a Gaussian RBM. Lastly, the weights are updated as
the difference between the original and reconstructed visible
layer labeled as the vector vrecon using:
△wij = α(< vihj >data −< vihj >recon),
where α is the learning rate and < vihj > is the expected
frequency with which visible unit i and hidden unit j are active
together when the visible vectors are sampled from the training
set and the hidden units are determined by (1).
Finally, the energy of a deep neural network can be determined in the ﬁnal layer using:
To extend the deep neural network to a convolutional deep
neural network, we simply partition the hidden layer into Z
groups . Each of the Z groups is associated with a nx×ny
ﬁlter where nx is the height of the kernel and ny is the width of
the kernel. Let us assume that the input image has dimension
Lx × Ly. Then, the convolution will result in a hidden layer
of Z groups each of dimension (Lx −nx + 1)×(Ly −ny + 1).
These learned kernel weights are shared among all hidden
units in a particular group. The energy function of layer l is
now a sum over the energy of individual blocks given by:
(Lx−nx+1),(Ly−ny+1)
vi+r−1,j+s−1hz
Hence, each layer of a deep CNN is referred to as a convolution RBM (CRBM). In such a model, the lower layers learn
abstract concepts and the higher layers learn complex features
for subjective sentences.
B. Recurrent Neural Networks
The standard RNN output, xl(t), at time step t for each
layer l is calculated using the following equations :
xl(t) = f(W l
R.xl(t −1) + Wl.xl−1(t))
t−kxl(t)dt
where WR is the interconnection matrix among hidden neurons
and Wl is the weight matrix of connections between hidden
neurons and the input nodes, xl−1(t) is the input vector at time
step t from layer l −1, vectors xl(t) and xl(t −1) represent
hidden neuron activation at time steps t and t−1, respectively,
and f is the non-linear activation function.
Furthermore, the distributed delays between output hidden
features in each layer can be modeled via WC. Unlike discrete
time delays that can be learned separately for each hidden
neuron, the distributed time delays are continuously changing
due to the combined effect of different outputs and, hence, we
use integration with respect to time to compute them.
In this paper, we propose to learn distributed time-delayed
dependence using CNNs. Hence, a kernel of dimension k × k
is able to capture distributed delays of up to k time points in
the video sequence and can be approximated by the covariance
matrix of features learned in the penultimate layer using (7). To
learn the weights WR of the RNN, back propagation through
time is used where the hidden layer is unfolded in time using
duplicate hidden neurons.
C. Multiple Kernel Learning
Consider a sequence of utterances s(1),s(2),...,s(T). The
corresponding features for each utterance from audio, video
and text data are denoted by x(t)a, x(t)v and x(t)t. MKL uses
the corresponding target labels y(t) ∈{+ve,−ve} to optimize
a dual form objective function with both min and max terms:
αiαjy(i)y(j)⎛
m(x(i)a,x(j)a)
m(x(i)v,x(j)v) +
m(x(i)t,x(j)t)⎞
αiy(i) = 0,
βm = 1, 0 ≤αi ≤C∀i.
where M is the total number of positive deﬁnite Gaussian
m(x(i)a,x(j)a),
m(x(i)v,x(j)v)
m(x(i)t,x(j)t) in each modality with a set of different
parameters and αi, b and βm ≥0 are coefﬁcients to be
learned simultaneously from the training data using quadratic
programming.
V. CRMKL MODEL
In order to integrate RNN with CNN and MKL and to
create the proposed CRMKL model, we extract features from
audio, video and text and combine them using MKL. In
particular, for video reviews we perform three steps: ﬁrstly,
in order to capture temporal dependencies, we transform
each pair of consecutive images at time t and t + 1 into a
single image; secondly, we include additional hidden layers of
recurrent neurons in the deep CNN model; lastly, we initialize
the distributed time-delay weight matrix of RNN with the
covariance of CNN output.
A. Extracting Features from Visual Data
Sentiment analysis of large scale visual content can help
to correctly extract sentiment of a topic. Deep CNNs have
good accuracy on topic classiﬁcation of videos, however
they can get stuck in local minima on ﬁne-grained problems
such as sentiment and emotion detection , . They are
also extremely slow. Hence, we propose a layer of recurrent
neurons to optimize the learning of features from video data.
Video sentiment detection faces two main challenges: ﬁrstly,
it is an extremely computationally-expensive task; secondly,
training datasets are weakly labeled and, hence, the trained
model may not generalize well on new data. Since the video
data is very large, we only consider every 10th frame in our
training videos. The constrained local model (CLM) is used
to ﬁnd the outline of the face in each frame . The cropped
frame size is further reduced by scaling down to a lower
resolution. In this way, we can drastically reduce the amount
of training video data.
Hidden Neurons
Interconnected
RNN Time-delayed
2D Features
2D Features
Up Sampled
2D Features
Transformed 2D feature
Kernel 1 = n1x×n1y
Logistic Layer
1D Features
Kernel 2 = n2x×n2y
Kernel 3 = n3x×n3y
Video Sequence
Fig. 2. Convolutional neural network for visual sentiment detection.
Figure 2 illustrates a convolutional RNN for visual sentiment detection. The input is a sequence of images in a video.
To capture the temporal dependence, we transform each pair
of consecutive images at t and t + 1 into a single image. We
use kernels of varying dimensions illustrated as Kernel 1, 2
and 3 to learn Layer 1 2D features from the transformed input.
Similarly, the second layer also uses kernels of varying
dimensions to learn 2D features. Up sampling layer will
transform features of different kernel sizes into uniform 2D
features. Next, a logistic layer of neurons is used to prepare
input for a RNN. Here, we have an inter-connected layer of
neurons that can model long time delays using delay states.
The ﬁnal output layer classiﬁes each video image as ‘Positive’
or ‘Negative’.
In order to generalize the model to other domains, we train it
using faces of different shapes and sizes. In order to validate
it in a speaker-independent manner, moreover, we train the
model on videos of product reviews in one domain and test
on videos from a completely different domain. Pre-processing
involved scaling all video frames to half the resolution. Each
pair of consecutive video frames were converted into a single
frame so as to achieve temporal convolution features. All the
frames were standardized to 250 × 500 pixels by padding with
The ﬁrst convolution layer contains 100 kernels of size
10×20, the next convolution layer had 100 kernels of size
20×30, this was followed by a logistic layer of 300 neurons
and a recurrent layer of 50 neurons. The convolution layers
were interleaved with pooling layers of dimension 2×2.
B. Extracting Features from Audio Data
We automatically extracted audio features from each annotated segment of the videos. Audio features were also extracted
in 30Hz frame-rate and we used a sliding window of 100ms.
To compute the features, we used the open source software
openSMILE . Speciﬁcally, this toolkit automatically extracts pitch and voice intensity. Z-standardization was used to
perform voice normalization. Basically, voice normalization
was performed and voice intensity was thresholded to identify
samples with and without voice. The features extracted by
openSMILE consist of several Low Level Descriptors (LLD)
and statistical functionals of them. Some of the functionals are
amplitude mean, arithmetic mean, root quadratic mean, standard deviation, ﬂatness, skewness, kurtosis, quartiles, interquartile ranges, linear regression slope, etc. So, counting all
functionals of each LLD, we obtained 6,373 features.
C. Extracting Features from Textual Data
We used a CNN as a trainable feature extractor to extract
features from the textual data. Each utterance in the original
dataset is in Spanish. While it is usually better to work directly
with the source language, in this work we translated each
utterance from Spanish to English using Google Translate.
Without the translation into English, 68.56% accuracy was
obtained on the MOUD dataset. The choice of CNN for feature
extraction is justiﬁed by the following considerations: the CNN
sentence model uses convolution as an operator to combine
semantically-related word vectors and the convolution layers
extract features in a hierarchical manner.
Each RBM layer is trained in an unsupervised manner and
then the complete deep model can be ﬁne-tuned using a subset
of the dataset with known labels. The features learned in an
unsupervised manner in each layer may not be the best for
classiﬁcation but can be used to train state-of-the-art classiﬁers
such as SVM or Na¨ıve Bayes.
Depending on the length of a sentence, the higher-order
features can be short and focused or long and global spanning
the entire sentence. CNNs form local features for each word
and combine them to produce a global feature vector for the
whole text using several hidden layers.
In this way, we can model semantic relations between words
that may not be syntactically related in a parse tree. These
features that the CNN builds internally can be extracted and
used as input for another, more advanced classiﬁer. In other
words, this turns CNN, originally a supervised classiﬁer, into
a trainable feature extractor. To form the input for the CNN
feature extractor, for each word in the text we constructed a
306-dimensional vector by concatenating two parts:
●Word embeddings: We used a publicly available word2vec
dictionary , which has been trained on a 100-million
word corpus from Google News using the continuous
bag-of-words architecture. This dictionary provides a
300-dimensional vector for each word. For words not
found in this dictionary, we used random vectors.
●Part of speech: We used 6 basic parts of speech (noun,
verb, adjective, adverb, preposition, conjunction) encoded
as a 6-dimensional binary vector. We used Stanford
Tagger as a part of speech tagger .
If an instance s has n words then we represent the input vector
for that instance s1∶n = s1 ⊕s2 ⊕...⊕sn. Here, si ∈Rk is a
k dimensional feature vector for word si (in this case k=306).
In our experiments, all texts were very short, consisting of one
sentence, the longest one being of 65 words. Thus, all input
vectors were of dimension 306 × (2 + 65 + 2) = 21,114. The
CNN we used consisted of 7 layers:
●Input layer of 21,114 neurons.
●Convolution layer with a kernel size of 3,4 and 50 feature
maps each. The output of the layer was computed with a
non-linear function; we used the ReLU.
●Max-pool layer with max-pool size of 2. Max pooling
operation over the feature map will take the maximum
value as the feature corresponding to a particular kernel
vector and, hence, discard highly similar features during
convolution.
●Convolution layer of kernel size of 2, 100 feature maps,
also using the ReLU.
●Max-pool layer with max-pool size of 2.
●Fully-connected layer of 500 neurons. The values of these
neurons were later used as the extracted features. For
regularization, we employ dropout on the penultimate
layer with a constraint on L2-norms of the weight vectors.
●Output softmax layer of 2 neurons. The ﬁnal layer which
outputs two labels: positive or negative.
The features were extracted from the penultimate fullyconnected layer of the CNN. In this way, we used the last output layer of the CNN only for training, but for actual decisionmaking, we replaced it with more sophisticated classiﬁers such
as SVM or MKL.
On MOUD dataset, using only CNN as a classiﬁer, 75.50%
was obtained which is in fact lower than the result (79.77%)
obtained when CNN was used to extract trainable features
for the SVM classiﬁer (Table I). We also tried other word
vectors having different dimensions, e.g., Glove word vectors
and Collobart’s word vectors.
However, the best accuracy was obtained using Google
word2vec. We would like to clarify that we only translate
the text form of utterances into English. The audio and video
data is however in Spanish. The purpose of translation is to
leverage on the lexical resources in English and to interpret
the emotions in the video and audio with text.
D. Feature Selection and Fusion
We signiﬁcantly reduced the number of features using
feature selection. We used two different feature selectors: one
based on the cyclic correlation-based feature subset selection
(CFS) and another based on principal component analysis
(PCA). The main idea of CFS is that useful feature subsets should contain features that are highly correlated with
the target class while being uncorrelated with each other.
PCA is a slightly different method, that uses an orthogonal
transformation to convert the data into a set of variables that
are linearly uncorrelated called principal components. The
components can be ranked by their magnitude in the data. By
discarding smaller (less meaningful) components, PCA allows
for dimensionality reduction and analogical reasoning .
Here, we select top K features from each method, where K
was experimentally determined by trial. For example, in case
of audio, visual and textual fusion, K was set to 300.
Feature selection for multimodal sentiment and emotion
analysis is done using MOUD and IEMOCAP training dataset,
respectively. However, for each unimodal, each bimodal, and
the multimodal experiment, feature selection is done separately. Feature-level fusion is achieved by concatenation of
the feature vectors obtained for each of the three modalities.
Clearly, the combined feature vectors from different modalities are heterogeneous in nature. Hence, the resulting vectors,
along with the corresponding sentiment polarity labels from
the training set, were used to train a classiﬁer with a MKL
algorithm; we used the SPF-GMKL implementation ,
which is designed to deal with heterogeneous data.
The parameters of the classiﬁer were found by crossvalidation. We chose a conﬁguration with 8 kernels: 5 RBF
with gamma from 0.01 to 0.05 and 3 polynomial with powers
2, 3, 4. We also tried Simple-MKL; it gave slightly lower
E. Computational Complexity
The computational complexity for a convolutional layer l is
given by O(nl−1.s2
l ), where nl−1 and nl are the number
of input and output feature maps, sl = nl−1
y are the dimensions of the input and output feature
maps. The computational complexity of a layer of recurrent
hidden neurons is only O(R × n2), where R is the maximum
time delay considered and n is the number of neurons.
We can hence conclude that the computational complexity
of RNN is much lower than CNN for each iteration of training.
Therefore, in this paper we ﬁrst train CNN for a limited
number of epochs and then the partially learned features are
further evolved using a low dimensional RNN for video data.
ACCURACY OF STATE-OF-THE-ART METHOD COMPARED WITH OUR METHOD WITH FEATURE-LEVEL FUSION ON MOUD DATASET. THE NUMBER OF
FEATURES REFERS TO OUR EXPERIMENTS, NOT TO .
Our method
without feature
with feature
# features, without selection
# features,
with selection
Multimodal
VI. EXPERIMENTAL RESULTS AND DISCUSSION
We used a common framework for both sentiment and emotion detection. For multimodal sentiment analysis, following
Perez et al., we have used the entire set of 448 utterances
in MOUD dataset and run ten-fold cross-validation using
CRMKL. In addition, to test the generalization ability of the
model on new datasets, we have also shown results on test
data from YouTube and ICT-MMMO. For comparison with
unimodal datasets such as only video or only text, we have
used SVM as a baseline classiﬁer. For the case of emotion
recognition, that is a much more ﬁne-grained problem than
sentiment detection, we evaluate our model via ten-fold crossvalidation on IEMOCAP dataset. Feature selection was not
done for visual modality as the deep CNN module in CRMKL
automatically learns the best features. Our experiments showed
that feature selection on visual data can lead to reduction in
Table I shows the 10-fold cross-validation results obtained
on MOUD dataset. The visual module of CRMKL, obtained
27% higher accuracy than the state of the art. When all modalities were used, 96.55% accuracy was obtained outperforming
the state of the art by more than 20%. Next, to assess the
accuracy of the model on an unknown dataset, we trained
the model on MOUD dataset and tested on ICT-MMMO
and YouTube dataset. On both of these datasets, the model
performed notably well.
The visual classiﬁer trained on the MOUD obtained 93.60%
accuracy. Other unimodal classiﬁers did not perform well like
the visual classiﬁer in the cross-domain analysis. As ICT-
MMMO dataset is a video-level sentiment dataset, utterancelevel sentiment evaluation is not possible. Hence, after the
model generates sentiment labels of all utterances for a video,
we took the majority sentiment label of these utterances in
order to label the video by its sentiment.
We got 85.30% accuracy on the ICT-MMMO dataset using
the trained visual sentiment model on the MOUD dataset. The
obtained accuracy on the ICT-MMMO dataset was lower than
the other two datasets. This is because ICT-MMMO dataset
was manually segmented into utterances and, hence, it is likely
to have more noise compared to other datasets.
Not only the visual features, textual features are also novel
as they indeed boosted the accuracy of the experiments where
textual modality was involved. The unimodal experiment with
only textual features outperformed the performance of the state
of the art as shown in Table I. On all three datasets, the visual
and textual modalities when combine together produced better
accuracy than other bimodal experiments.
For multimodal emotion analysis, we used the same framework as we employed for multimodal sentiment analysis. The
accuracy for all unimodal, bimodal and trimodal experiments
are signiﬁcantly better than the state of the art. However,
the performance is not as good as the multimodal sentiment
analysis experiments. One of the possible reasons for this is
the use of same CNN conﬁgurations for both visual and textual
sentiment feature extraction.
This raises the question of using larger number of neurons
and layers in CRMKL for visual and textual emotion feature
extraction. This is of course a fundamental task of our future
work. The following observations were made from the multimodal emotion analysis experiments:
●We realized that the textual classiﬁer recognized angry,
happy and neutral instances well. However, angry and
sad instances are very tough to distinguish from each
other using textual clues. One of the possible reasons is
that both classes are negative and many similar words are
used to express them.
●In the case of audio modality, we observed better accuracy
than textual modality for sad and neutral classes but not
for happy and angry classes. The classiﬁer misclassiﬁed
many happy instances into angry. However, the classiﬁer
performed very well to discriminate between sadness and
anger. We also observed that some happy instances were
classiﬁed as neutral.
●Visual modality produced the best accuracy compared to
the other two modalities. Though angry and sad faces
can be effectively classiﬁed, the classiﬁer showed some
confusion between angry and sad faces. Neutral classes
were also separated more accurately in respect to other
classes though high confusion was observed between
happy and neutral faces.
ACCURACY ON TEXTUAL (T), VISUAL (V), AUDIO (A) MODALITY AND COMPARISON WITH THE STATE OF THE ART.
Modalities
Emotion, on IEMOCAP
our results
state of the art
our results
state of the art
our results
state of the art
our results
state of the art
our results
state of the art
our results
state of the art
A + V + T our results
state of the art
When we fused the modalities using feature-level fusion
strategy, higher accuracy was obtained as compared to unimodal classiﬁers, as expected. Although the identiﬁcation
accuracy has been improved for every emotion, the confusion
between sad and angry face was still high.
The comparison with the state of the art (Table II) in
terms of accuracy shows that the proposed method performed
signiﬁcantly better. For sad and neutral emotion the proposed
method outperformed the state of the art by a margin of 8%
and 17%, respectively. However, for angry and happy the
performance is just slightly better.
Paired t-test showed statistical signiﬁcance of all experiments with conﬁdence level 95%. It can be found from the
Table II that visual and textual modalities performed notably
better than the state of the art. With the help of these two
modalities, the proposed method outperformed the state of the
In this paper, we proposed the novel integration of CNN
with a low dimensional RNN that can converge to the global
maxima much faster than baselines. Hence, in Table II, for
the emotion sad, the performance of visual modality and the
bimodal combinations of visual modality with text and audio,
respectively, is over 70%.
This is due to the superior performance of the proposed
video classiﬁer. In contrast, the bimodal combination of audio
and text has a 10% lower accuracy that is similar to the
baseline. The combined multimodal classiﬁer of audio, video
and text is slightly better than visual modality. This is because
the video classiﬁer dominates over the other two modalities.
Deep CNN have recently shown good performance on
audio, video and text classiﬁcation. Instead of using a single
large hidden layer of neurons, deep models have several small
layers of hidden neurons. Since each layer is independent,
this results in tremendous reduction in complexity. Therefore,
in this paper we construct a deep CNN for each modality,
namely: audio, video, and text.
The groups of features learned by each of the three deep
CNN are combined using MKL. In this way, we can reduce
the number of input dimensions and group the features for
A. Effect of Number of Hidden Layers
Deep learning is able to approximate very long time-delays
in video data via a hierarchy of hidden layers, where the
features learned in one layer become the input data to the next
layer. To determine the number of hidden layers of recurrent
neurons, we consider the root mean square error (MSE) on
training data. MSE is the cost function that the deep model is
trying to minimize while learning the weights. Hence, this is
a suitable metric for the improvement made by each hidden
layer in a deep model.
Figure 3 reports the decrease in MSE with increasing
number of hidden layers for the YouTube test dataset. It was
also observed that the variance over 10-fold cross-validation
reduces with increasing number of hidden layers. Hence, we
can conclude that deep learning is suitable for extracting
sentiments and emotions from video data. Since each layer
is learned independent of the previous layer, the number of
parameters is small and overﬁtting is avoided.
Number of Hidden Layers
Fig. 3. MSE with respect to number of hidden layers.
B. Tuning of Hyper-parameters
As a performance measure, we adopted the F-score. Each
dataset is split into training set, validation set, and test set.
For all three modalities and for each hidden layer we consider
different number of hidden units (i.e., n=50/200/500/700)
and 5000 epochs of CNN training using the Theano based
stochastic gradient descent. The number of hidden neurons in
each layer is gradually increased until performance saturates
due to overﬁtting. In particular, early overﬁtting occurs for the
MOUD dataset.
Our best results are obtained with an ensemble of CNNs
by 10-fold cross-validation that differ in their random initialization and mini-batches of 100 samples. Results on CNNs of
various depths and sizes shows that deep CNN outperforms
single-layer CNN with approximately the same number of
parameters, which quantitatively validates the beneﬁts of deep
networks over shallow ones.
We see a consistent improvement as we use deeper models. Following previous authors, the word vector length was
empirically set to 300, and unknown words were randomly
initialized to vectors from Gaussian distributions. The 6 dimensional vector corresponds to 6 different parts of speech
such as noun and verb.
C. Visualization of Features
The deep temporal CNN model automatically learns features from the training data, so that each neuron learns a
speciﬁc feature such as eyes or mouth. In the ﬁrst layer, the
features learned are parts of the face and their sentiments,
and the higher layers will combine these emotional features to
learn the complete face and corresponding positive or negative
sentiment. We visualize the feature detectors in the ﬁrst layer
of the network trained on the MOUD sentiment data.
We rank all image segments in the training data according to
the activation of each detector. Figure 4 shows the top image
segments activated at two feature detectors in the ﬁrst layer
of a deep CNN. We ﬁnd that similar features such as eyes or
mouth are expressed at the same hidden neuron. The feature
detectors learn to recognize not only the part of the face but
also the sentiment associated with it.
VII. CONCLUSION
Communication across the World Wide Web is rapidly
shifting from unimodal data, i.e., text, to multimodal data, i.e.,
video. Extracting emotions and polarity from videos is hence
becoming increasingly important for tasks such as social media
marketing, brand positioning, and ﬁnancial prediction.
In this paper, we proposed the fusion of speech, voice tone,
and facial expressions for multimodal emotion recognition
and sentiment analysis. In particular, we described a novel
temporal deep convolutional neural network for visual and
textual feature extraction and used multiple kernel learning
fuse heterogeneous
features extracted
modalities, namely: audio, video, and text.
In the future, we will focus on improving the accuracy of
emotion detection via different neural network conﬁgurations.
We will also consider annotation of ICT-MMMO dataset at
utterance level for smoother training of the model.
Neuron with Highly Activated Features of Forehead and Mouth
Neuron with Highly Activated Features of Eyes and Ear
Top image segments activated at two feature detectors in the ﬁrst
layer of deep CNN