JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
Domain Adaptive Ensemble Learning
Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang.
Abstract—The problem of generalizing deep neural networks
from multiple source domains to a target one is studied under
two settings: When unlabeled target data is available, it is a
multi-source unsupervised domain adaptation (UDA) problem,
otherwise a domain generalization (DG) problem. We propose a
uniﬁed framework termed domain adaptive ensemble learning
(DAEL) to address both problems. A DAEL model is composed
of a CNN feature extractor shared across domains and multiple
classiﬁer heads each trained to specialize in a particular source
domain. Each such classiﬁer is an expert to its own domain
but a non-expert to others. DAEL aims to learn these experts
collaboratively so that when forming an ensemble, they can
leverage complementary information from each other to be more
effective for an unseen target domain. To this end, each source
domain is used in turn as a pseudo-target-domain with its own expert providing supervisory signal to the ensemble of non-experts
learned from the other sources. To deal with unlabeled target
data under the UDA setting where real expert does not exist,
DAEL uses pseudo labels to supervise the ensemble learning.
Extensive experiments on three multi-source UDA datasets and
two DG datasets show that DAEL improves the state of the art
on both problems, often by signiﬁcant margins.
Index Terms—Domain adaptation, domain generalization, collaborative ensemble learning
I. INTRODUCTION
EEP neural networks trained with sufﬁcient labeled data
typically perform well when the test data follows a
similar distribution as the training data. However, when the
test data distribution is different, neural networks often suffer
from signiﬁcant performance degradation. Such a problem
is common to machine learning models and is often referred to as domain shift (or distribution shift). To overcome the domain shift problem, two related areas have been
studied extensively, namely unsupervised domain adaptation
(UDA) and domain generalization
(DG) . UDA aims to adapt a model
from a labeled source domain to an unlabeled target domain.
In contrast, DG aims to learn a model only from source data
typically gathered from multiple distinct but related domains,
and the model is directly deployed in a target domain without
any ﬁne-tuning or adaptation steps.
Early UDA work focuses on single-source scenarios. Recently, multi-source UDA has started to attract
more attention, thanks to the introduction of large-scale multidomain datasets such as DomainNet . In contrast, having
multiple source domains has been the default setting for most
DG methods from much early on . This is understandable:
without the guidance from target domain data, DG models
rely on the diversity of source domain to learn generalizable
K. Zhou is with Nanyang Technological University, Singapore.
Y. Yang and T. Xiang are with the University of Surrey, UK.
Y. Qiao is with the Shenzhen Institutes of Advanced Technology, Chinese
Academy of Sciences, China.
knowledge. This paper focuses on the multi-source setting for
both problems.
How can multiple source domains be exploited to help
generalization? Many DG methods aim to learn a
domain-invariant feature representation or classiﬁer across the
source domains, in the hope that it would also be invariant
to domain shift brought by the target domain. However,
there is an intrinsic ﬂaw in this approach, that is, when the
source domains become more diverse, learning a domaininvariant model becomes more difﬁcult. This is because each
domain now contains much domain-speciﬁc information. Simply removing the information may be detrimental to model
generalization because such information could potentially be
useful for a target domain, especially when combined across
different source domains. An example can be found in Fig. 1(a)
where the only thing in common of the ﬁve source domains
for the airplane class seems to be shape. However, texture
information is also useful for object recognition in the target
sketch domain, which we want to maintain in the learned
classiﬁer. Existing multi-source UDA methods, on the other
hand, attempt to align the data distribution of the target domain
with each source domain individually or by
means of a hard or soft domain selector. Again,
Fig. 1(a) suggests that aligning the target domain to each
individual source domain is not only difﬁcult but could also
be counterproductive due to drastic variations among source
In this paper, we propose a novel uniﬁed framework for both
multi-source DG and UDA based on the idea of collaborative
ensemble learning. Our framework, termed domain adaptive
ensemble learning (DAEL), takes a very different approach
from previous work. Speciﬁcally, each domain is used to learn
a model that is specialized in that domain (see Fig. 1(a)).
We call it a domain expert—a relative term as an expert to
a speciﬁc source domain would be a non-expert to all other
source domains as well as the target domain. The key idea
of DAEL is to learn these experts collaboratively so that
when forming an ensemble, they can leverage complementary
information to better tackle the target domain.
To realize the DAEL framework for a UDA or DG model,
a number of issues need to be addressed. (1) Scalability:
Training an ensemble of models instead of a single model
means higher computational cost. To solve this problem,
we design a DAEL model as a deep multi-expert network
consisting of a shared convolutional neural network (CNN)
feature extractor and multiple classiﬁer heads. Each head is
trained to classify images from a particular source domain.
Therefore, different heads learn different patterns from the
shared features for classiﬁcation. (2) Training: Since the target
domain data is either non-existent (for DG) or has no label (for
UDA), there is no target domain expert to provide supervisory
 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
Expert group
Non-expert group
Soft probability
as supervision
Most confident
Expert group
Non-expert group
Labeled source data
Unlabeled target data
Pseudo-label
as supervision
Supervised learning
(a) Learning domain-specific experts
(b) Collaborative ensemble learning for labeled source
(c) Collaborative ensemble learning for unlabeled target
Fig. 1: Overview of domain adaptive ensemble learning (DAEL).
signal for the source domain expert ensemble. To overcome
this, each source domain is used in turn as a pseudo-targetdomain with its own expert providing supervisory signal to the
ensemble of non-experts learned from the other sources (see
Fig. 1(b)). For unlabeled target data under the UDA setting
where real expert does not exist, DAEL uses as pseudo-label
the most conﬁdent estimation among all experts and train the
ensemble to ﬁt the pseudo-label (see Fig. 1(c)). (3) How to
measure the effectiveness of a non-expert ensemble w.r.t. an
expert: Inspired by consistency regularization (CR) 
used in semi-supervised learning, the ensemble’s effectiveness
is measured by how close its prediction is to that of an expert
when both are fed with a data point from the expert’s domain.
To amplify the regularization effect brought by CR, we use
weak and strong augmentation for input to an expert and a nonexpert ensemble respectively. Such a strategy has been shown
useful in recent semi-supervised learning methods .
Once these three issues are addressed, we have a simple but effective solution to both UDA and DG. By sending supervisory
signal to an ensemble rather than each individual, different
domain-speciﬁc experts are allowed to exploit complementary
domain-speciﬁc information from each other, resulting in a
more domain-generalizable ensemble.
We summarize our contributions as follows. (1) We present
a novel framework called domain adaptive ensemble learning
(DAEL), which improves the generalization of a multi-expert
network by explicitly training the ensemble to solve the
target task. (2) A realization of DAEL is formulated which
provides a simple yet effective solution for both multi-source
UDA and DG, unlike previous methods that only tackle one
of them. (3) We deﬁne miniDomainNet, a reduced version
of DomainNet to allow fast prototyping and experimentation. For benchmarking, a uniﬁed implementation and
evaluation platform of all compared methods is created, called
Dassl.pytorch, which has been made publicly available.1
1 
(4) We demonstrate the effectiveness of DAEL on three multisource UDA datasets and two DG datasets where DAEL
outperforms the current state of the art by a large margin (see
Table 1 & 2).
II. RELATED WORK
Unsupervised domain adaptation. Motivated by the seminal theory work by Ben-David et al. , numerous UDA
methods seek to reduce distribution discrepancy between
source and target features using some distance metrics, such
as maximum mean discrepancy , optimal transport , and graph matching . Inspired
by generative adversarial network (GAN) , several methods additionally train a domain discriminator for feature alignment. GAN has also been exploited
for pixel-level domain adaptation where target images are
synthesized via image translation/generation . Instead
of aligning the coarse marginal distribution, recent alignment
methods have shown that ﬁne-grained alignment such as
aligning class centroids or using task-speciﬁc
classiﬁers can give a better adaptation performance.
The multi-source UDA methods are more related to our
work because of the same problem setting. Several works extend the domain alignment idea to multi-source
UDA by considering all possible source-target pairs. Kang
et al. propose contrastive adaptation network where a
contrastive domain discrepancy loss is minimized for samples
from the same class but of different domains while maximized
for samples from different classes. Relationships between each
source and the target are learned by Li et al. and only the
target-related sources are kept for model learning. Hoffman
et al. compute distribution-based weights for combining
source classiﬁers. Our model architecture—a shared feature
extractor and multiple domain-speciﬁc classiﬁers—is similar
to M3SDA . However, DAEL is very different in that
different domain-speciﬁc classiﬁers are learned collaboratively
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
where each source domain is used in turn as a pseudo-targetdomain to train the ensemble.
Domain generalization. Many DG methods follow the idea
of distribution alignment originated from the UDA community to learn domain-invariant features through minimizing
in-between-source distances . Data augmentation is another popular research direction where the motivation is to avoid overﬁtting to source data. This can be
achieved by, for example, adding adversarial gradients to the
input , learning data generation networks ,
or mixing instance-level feature statistics . Meta-learning
has also been investigated for learning domain-generalizable
neural networks . Different from existing DG
methods that mostly train a single classiﬁer, our work for the
ﬁrst time introduces collaborative ensemble learning to mine
domain-speciﬁc information using domain-speciﬁc classiﬁers.
Although our pseudo-target-domain idea is related in spirit
to meta-learning, no episodic training is required in DAEL,
which makes the training procedure much simpler. We refer
readers to Zhou et al. for a comprehensive survey in DG.
Ensemble methods have been extensively researched in the
machine learning community . The principle is to train
multiple learners for the same problem and combine them for
inference. Such technique has also been widely used in competitions like ILSVRC where multiple CNNs are trained
and combined to improve the test performance . In
this work, to prompt the emergence of generalizable features,
we learn an ensemble of classiﬁers (experts) in a collaborative
way—using each individual expert to supervise the learning
of the non-expert ensemble.
III. METHODOLOGY
Problem deﬁnition. Given a labeled training dataset collected from K source domains, DS = {D1, ..., DK}, we
aim to learn a model that can generalize well to a target
domain DT . If the unlabeled target data is available during
training, it is a multi-source unsupervised domain adaptation
(UDA) problem , otherwise a domain generalization (DG)
problem . Our method addresses these two problems in a
uniﬁed framework.
Model. We aim to learn a multi-expert model, denoted
i=1, with each expert Ei specializing in a particular
source domain Di. For clarity, Ei is called an expert to Di
but a non-expert to {Dj}j̸=i. The ensemble prediction for an
image x is used at test time, i.e. p(y|x) =
i=1 Ei(x).
In implementation, the multi-expert model shares a CNN
backbone for feature extraction, followed by domain-speciﬁc
classiﬁcation heads. To allow the ensemble to better exploit
complementary information between experts, we propose domain adaptive ensemble learning (DAEL). The main idea
of DAEL is to strengthen the ensemble’s generalizability
by simulating how it is tested—using an expert’s output to
supervise the learning of ensemble of non-experts. This is
realized by consistency regularization (CR) training, as shown
in Fig. 2. To amplify the regularization effect, we follow Sohn
et al. to use weak and strong augmentations, denoted by
a(·) and A(·) respectively. Speciﬁcally, weak augmentation,
which corresponds to simple ﬂip-and-shift transformations, is
used for pseudo-label generation; strong augmentation, which
induces stronger noises like rotation and shearing, is used for
ensemble prediction.
Domain-speciﬁc expert learning. Next, we detail the
DAEL training procedure, starting with how each expert is
trained to be domain-speciﬁc. Let H(·, ·) denote cross-entropy
between two probability distributions, the loss function for
domain-speciﬁc expert learning is
Exi,y(xi)∼Di[H(y(xi), Ei(a(xi)))],
where y(xi) is the one-hot label of xi; the expectation is
implemented by mini-batch sampling (same for the following
equations).
Collaborative ensemble learning using source domain
data. Given an image xi from the i-th source domain (treated
as a pseudo-target-domain), the idea is to use as target the
corresponding expert’s prediction for the weakly augmented
image, Ei(a(xi)), and encourage the ensemble prediction
of non-experts from other source domains for the strongly
augmented image,
j̸=i Ej(A(xi)), to be close to the
target. Such a design explicitly teaches the ensemble how
to handle data from unseen domains (mimicked by strong
augmentation and guided by a pseudo-target-domain expert),
thus improving the robustness to domain shift. Formally, the
loss is deﬁned as the mean-squared error (MSE) between the
two outputs:2
∥Ei(a(xi))−
Ej(A(xi))∥2
Collaborative ensemble learning using unlabeled target
data. Given a weakly augmented target domain image a(xt),
we ﬁrst ask each source-expert to produce a class probability
distribution, pi(y|a(xt)) = Ei(a(xt)), and select as pseudolabel the most conﬁdent expert’s prediction based on their
maximum probability, arg max(pi∗), where i∗is the index of
the most conﬁdent expert. This is inspired by the observation
that correct predictions are usually conﬁdent with peaked value
on the predicted class . Then, we force the ensemble
prediction of all source-experts for the strongly augmented
image, ¯E(A(xt)) =
i=1 Ei(A(xt)), to ﬁt the one-hot
pseudo-label ˆy(xt) = arg max(pi∗).3 The loss is deﬁned as
Lu = Ext∼DT [1(max(pi∗) ≥ϵ)H(ˆy(xt), ¯E(A(xt)))],
where ϵ is a conﬁdence threshold (ﬁxed to 0.95 in this paper).
Eq. (3) can be viewed as a combination of CR and entropy
minimization because the conversion from soft probability
to one-hot encoding essentially reduces the entropy of the class
distribution. The conﬁdence threshold provides a curriculum
for ﬁltering out less conﬁdent (unreliable) pseudo labels in the
early training stages .
2We chose MSE over KL divergence because the former led to a slightly
higher performance.
3For simplicity we assume arg max converts soft probability to one-hot
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
pi(y|a(xi))
{Expert-j}j6=i
p1(y|a(xt))
pK(y|a(xt))
pensemble(y|A(xi))
pensemble(y|A(xt))
OneHot(pi⇤(y|a(xt)))
xi: data from i-th source domain
xt: data from target domain
a(·): weak augmentation
A(·): strong augmentation
{Expert-i}K
{Expert-i}K
i⇤is the index of the
most conﬁdent expert
Consistency
Consistency
Fig. 2: Illustration of domain adaptive ensemble learning. Left: collaborative learning using source domains. Right: collaborative
learning using unlabeled target domain via pseudo-labeling. The ensemble of all experts is used for testing. Gradients are only
back-propagated through the ensemble prediction path.
The full learning objective is a weighted sum of Eq. (1),
(2) and (3),
L = Lce + Lcr + λuLu,
where λu is a hyper-parameter for balancing the weighting
between Lu and the losses for the labeled source domains.
For multi-source UDA, DAEL uses Eq. (4). For DG, Lu is
removed due to the absence of target domain data. DAEL not
only provides a uniﬁed solution to the two problems, but is also
very easy to implement (see Appendix B for pseudo-code).
Gradient analysis. To understand the beneﬁt of collaborative learning (i.e., ∥1
i pi−p∗∥2) against individual learning
i ∥pi−p∗∥2) where p∗denotes the target, we analyze
their gradients with respect to a single expert’s output pi. For
collaborative learning, we have ∆pi = 2
p∗). For individual learning, we have ∆pi =
K (pi −p∗).
It is clear that collaborative learning updates an expert by
combining information from other experts, which facilitates
the exploitation of complementary information.4 Table 3a
further conﬁrms the advantage of collaborative learning.
Relation to knowledge distillation. DAEL is similar to
knowledge distillation (KD) in the sense that the teacherstudent training is used. However, in DAEL the boundary
between teacher and student is blurred because each student
can become a teacher when the input data come from its
domain of expertise. Moreover, the collaborative ensemble
learning strategy is speciﬁcally designed for dealing with
multi-domain data—it encourages different experts to learn
complementary information such that the ensemble is more
generalizable to unseen domains.
IV. EXPERIMENTS
A. Experiments on Domain Adaptation
Datasets. (1) Digit-5 consists of ﬁve different digit recognition datasets, which are MNIST , MNIST-M , USPS,
SVHN and SYN . We follow the same setting as
4The same conclusion can be drawn when using KL divergence as the
objective.
in M3SDA for experimentation. See Fig. 3 left for
example images. (2) DomainNet is a recently introduced
benchmark for large-scale multi-source domain adaptation. It
has six domains (Clipart, Infograph, Painting, Quickdraw, Real
and Sketch) and 0.6M images of 345 classes.5 See Fig. 3
right for example images. (3) The full DomainNet requires
considerable computing resources for training,6 preventing
wide deployment and extensive ablative studies. Inspired by
the miniImageNet dataset that has been widely used in the
few-shot learning community, we propose miniDomainNet,
which takes a subset of DomainNet and uses a smaller image
size (96 × 96). As noted by Saito et al. that the labels
of some domains and classes are very noisy in the original
DomainNet, we follow them to select four domains and 126
classes. As a result, miniDomainNet contains 18,703 images
of Clipart, 31,202 images of Painting, 65,609 images of Real
and 24,492 images of Sketch. In general, miniDomainNet
maintains the complexity of the original DomainNet, reduces
the requirements for computing resources and allows fast
prototyping and experimentation.
The implementation details (of all experiments in this paper) are provided in Appendix A. To ensure the results are
convincing, we run each experiment three times and report the
mean accuracy and standard deviation. The results of baseline
models are from either their papers (if reported) or our reimplementation (only when their source code is available).
Results. Following the standard test protocol , one domain is used as target and the rest as sources, and classiﬁcation
accuracy on the target domain test set is reported. Table 1
shows the results on the multi-source UDA datasets. We
summarize our ﬁndings as follows. (1) In terms of the overall
performance (the rightmost Avg column), DAEL achieves the
best results on all three datasets, outperforming the secondbest methods by large margins: 3.51% on Digit-5, 2.2% on
5We have noticed that the ‘t-shirt’ class (index 327) is excluded from
Painting’s training set (see the ofﬁcial painting_train.txt ﬁle), but
is included in the test set, which could affect the performance.
6It usually takes several GPU days for training a deep model on the full
DomainNet.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
Fig. 3: Example images from Digit-5 and DomainNet.
Accuracy (%)
Accuracy (%)
miniDomainNet
Fig. 4: Ablation study for evaluating each component in Eq. (4).
DomainNet and 6.21% on miniDomainNet.
(2) On the small Digit-5 dataset, DAEL achieves near-oracle
performance (our 96.47% vs. oracle’s 97.00%). In particular,
MNIST-M and SVHN are the two most difﬁcult domains as
can be seen in Fig. 3—MNIST-M has complex backgrounds
while SVHN contains blurred and cluttered digits. Those
distinctive features make MNIST-M and SVHN drastically
different from other domains and thus make the adaptation
task harder. Nonetheless, DAEL obtains the highest accuracy
which beats M3SDA—the 2nd best method—by 11.62% on
MNIST-M and 4.06% on SVHN.
(3) On the large-scale DomainNet/miniDomainNet, DAEL
achieves the best performance among all methods. Notably,
compared with the latest state of the art on DomainNet, i.e.
CMSS, DAEL obtains a clear margin of 2.2% on average,
which demonstrates the advantage of exploiting complementarity between source domains for ensemble prediction.
(4) Compared with M3SDA, the most related method to
ours that also has domain-speciﬁc classiﬁers, DAEL is superior
on all three datasets. This is because aligning distributions
between the target and each individual source, as in M3SDA,
is difﬁcult due to large domain variations between sources.
B. Experiments on Domain Generalization
Datasets. (1) PACS is a commonly used DG dataset
with four domains: Photo (1,670 images), Art Painting (2,048
images), Cartoon (2,344 images) and Sketch (3,929 images).
There are seven object categories: dog, elephant, giraffe,
guitar, horse, house and person. (2) Ofﬁce-Home contains
around 15,500 images of 65 categories, which are related
to ofﬁce and home objects. Similar to PACS, there are four
domains: Artistic, Clipart, Product and Real World. For evaluation, we follow the prior works to use the
leave-one-domain-out protocol, i.e. choosing one domain as
the (unseen) test domain and using the remaining three as
source domains for model training.
Results. The comparison with the state-of-the-art DG methods is shown in Table 2. Overall, DAEL achieves the best
results on both datasets with clear margins against all competitors. We provide a more detailed discussion as follows.
(1) DAEL is clearly better than the distribution alignment
methods, i.e. CCSA and MMD-AAE, with ≥4% improvement
on PACS and ≥1.2% improvement on Ofﬁce-Home. This is
not surprising because the distribution alignment theory 
developed for DA does not necessarily work for DG (which
does not have access to target data). (2) Compared with the
recent self-supervised method JiGen, DAEL obtains a clear
improvement of 2.9% on PACS. The gap is further increased
to 4.9% on Ofﬁce-Home. When it comes to CrossGrad, a stateof-the-art data augmentation method, DAEL achieves clear improvements as well. (3) The recently proposed Epi-FCR shares
a similar design choice with DAEL—to simulate domain shift
during training. Again, DAEL is clearly superior thanks to
the design of collaborative ensemble learning. Further, Epi-
FCR requires domain-speciﬁc feature extractors, as well as
additional domain-agnostic feature extractors and classiﬁers,
incurring much higher computational cost.
C. Analysis
Ablation study. We start from the baseline ensemble model
trained by Lce only and progressively add Lcr and Lu (see
Eq. (4)). The results are shown in Fig. 4. Each of Lcr and
Lu contributes positively to the performance. Combining Lcr
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
TABLE 1: Comparing DAEL with state of the art on multi-source UDA datasets.
(a) Digit-5.
95.36±0.15
99.50±0.08
99.18±0.09
92.28±0.14
98.69±0.04
Source-only
68.08±0.39
99.06±0.05
97.20±0.48
84.56±0.36
89.87±0.32
76.20±0.51
99.38±0.06
94.39±0.58
86.37±0.54
86.78±0.31
83.44±0.12
98.46±0.07
94.19±0.31
84.08±0.60
92.91±0.23
75.30±0.57
99.00±0.08
97.70±0.13
88.40±0.54
93.70±0.21
80.65±0.51
99.22±0.08
98.32±0.07
81.87±0.72
95.42±0.04
80.16±0.48
99.41±0.06
98.87±0.08
86.15±0.76
96.44±0.34
83.07±0.57
99.35±0.03
98.64±0.17
86.40±0.41
95.78±0.15
M3SDA 
82.15±0.49
99.38±0.07
98.71±0.12
88.44±0.72
96.10±0.10
DAEL (ours)
93.77±0.12
99.45±0.02
98.69±0.79
92.50±0.15
97.91±0.03
(b) DomainNet.
Oracle 
69.3±0.37 34.5±0.42 66.3±0.67 66.8±0.51 80.1±0.59 60.7±0.48 63.0
Source-only 47.6±0.52 13.0±0.41 38.1±0.45 13.3±0.39 51.9±0.85 33.7±0.54 32.9
45.5±0.59 13.1±0.72 37.0±0.69 13.2±0.77 48.9±0.65 31.8±0.62 32.6
48.6±0.73 23.5±0.59 48.8±0.63
53.5±0.56 47.3±0.47 38.2
54.3±0.64 22.1±0.70 45.7±0.63
58.4±0.65 43.5±0.57 38.5
M3SDA 
58.6±0.53 26.0±0.89 52.3±0.55
62.7±0.51 49.5±0.76 42.6
64.2±0.18 28.0±0.20 53.6±0.39 16.0±0.12 63.4±0.21 53.8±0.35 46.5
DAEL (ours)
70.8±0.14 26.5±0.13 57.4±0.28 12.2±0.70 65.0±0.23 60.6±0.25 48.7
(c) miniDomainNet.
72.59±0.30
60.53±0.74
80.47±0.34
63.44±0.15
Source-only
63.44±0.76
49.92±0.71
61.54±0.08
44.12±0.31
62.91±0.67
45.77±0.45
57.57±0.33
45.88±0.67
62.06±0.60
48.79±0.52
58.85±0.55
48.25±0.32
65.55±0.34
46.27±0.71
58.68±0.64
47.88±0.54
M3SDA 
64.18±0.27
49.05±0.16
57.70±0.24
49.21±0.34
68.09±0.16
47.14±0.32
63.33±0.16
43.50±0.47
DAEL (ours)
69.95±0.52
55.13±0.78
66.11±0.14
55.72±0.79
Accuracy (%)
Accuracy (%)
miniDomainNet
Fig. 5: Sensitivity of λu.
and Lu gives the best performance, which conﬁrms their
complementarity.
Sensitivity of λu. Fig. 5 shows that the performance soars
from λu = 0 to λu = 0.5 and remains relatively stable
between λu = 0.5 and λu = 1.0. The overall results suggest
that the model’s performance is in general insensitive to λu
around 0.5.
Collaborative ensemble or individual expert training?
As discussed in the gradient analysis part in Methodology,
collaborative learning aggregates gradients from different experts, which can better exploit the complementarity between
different sources. We justify this design in Table 3a where collaborative learning shows clear improvements over individual
Learning an ensemble of classiﬁers or a single classiﬁer?
The motivation for the former is to enable the model to better
handle complicated source data distributions—as discussed
before, learning a single classiﬁer forces the model to erase
domain-speciﬁc knowledge that could otherwise be useful for
recognition in the target domain. To justify this design, we
switch from the ensemble classiﬁers to a single classiﬁer
while keeping other designs unchanged. Table 3b conﬁrms
that learning an ensemble of classiﬁers is essential.
Using expert’s prediction or real label in Lcr? Table 3c
shows that using real label (Y ) is slightly worse than using
expert’s prediction (Ei). This is because expert’s prediction
automatically encodes the relations between classes (reﬂected
in the soft probability distribution ), thus providing better
supervisory signal.
Most conﬁdent expert’s prediction vs. ensemble predic-
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
TABLE 2: Comparing DAEL with state of the art on DG datasets PACS (left) and Ofﬁce-Home (right).
Ofﬁce-Home
77.0 75.9 96.0 69.2 79.5 58.9 49.4 74.3 76.2 64.7
MMD-AAE 75.2 72.7 96.0 64.2 77.0 56.5 47.3 72.1 74.8 62.7
80.5 76.9 93.6 66.8 79.4 59.9 49.9 74.1 75.7 64.9
JiGen 
79.4 75.3 96.0 71.6 80.5 53.0 47.5 71.5 72.8 61.2
CrossGrad 
79.8 76.8 96.0 70.2 80.7 58.4 49.4 73.9 75.8 64.4
Epi-FCR 
82.1 77.0 93.9 73.0 81.5
76.9 80.4 93.4 75.2 81.5
DAEL (ours)
84.6 74.4 95.6 78.9 83.4 59.4 55.1 74.0 75.7 66.1
TABLE 3: Evaluation of design choices in DAEL. D-5: Digit-5. miniDN: miniDomainNet.
Collaborative
vs. individual expert learning.
Col. 96.47
Ind. 93.07
(b) Learning an ensemble of classi-
ﬁers vs. a single classiﬁer.
Ensemble 96.47
prediction
vs. real label (see Lcr).
(d) Expert’s prediction vs. ensemble prediction (see Lu).
tion for Lu? Table 3d suggests that using the most conﬁdent
expert’s output (Ei∗) is better. A plausible explanation is that
ensembling smooths out the overall probability distribution
when experts have disagreements, which may lead to potentially correct instances discarded due to weak conﬁdence
(i.e. probability less than the conﬁdence threshold ϵ).
Diagnosis into individual experts. Fig. 6 shows the performance of each individual source expert versus the ensemble
on miniDomainNet trained with different losses. For Lce (blue
bars), the variance between E1−3 is large and each individual’s
performance is low, indicating that the experts are themselves
biased (overﬁtting). Comparing Lce + Lcr (orange bars) with
Lce, we observe that the variance between E1−3 is reduced
and each individual’s performance is signiﬁcantly improved,
leading to a much stronger ensemble. By adding Lu (green
bars), each individual’s performance is further boosted, and
hence the ensemble. To better understand how the ensemble
helps prediction, we visualize the top-3 classes predicted by
each expert and the ensemble in Fig. 7. In Fig. 7(a) top, expert-
3 mis-recognizes the bear as dog but the ensemble prediction is
dominated by the correct predictions made by expert-1 and -2.
A similar pattern can be observed in Fig. 7(a) bottom. Fig. 7(b)
provides examples of incorrect predictions where we observe
that the model struggles to differentiate between classes of
similar features. For example, the kangaroo in Fig. 7(b) top
looks indeed similar to a dog due to the similarity in the face
and skin texture—without looking at the body structure and
the pouch. Such mistakes might be avoided by using neural
networks that can extract features at multiple scales .
Augmentation strategy. Recall that we use weak and strong
augmentations for pseudo-label generation and prediction respectively. The rationales behind this design are: 1) We need
the expert to provide accurate supervision (pseudo labels) to
the non-expert ensemble so we feed the expert with weakly
augmented data; 2) We apply strong augmentation to the nonexpert ensemble in order to reduce overﬁtting to noisy pseudo
TABLE 4: Comparison with baseline models trained using
strong augmentation on miniDomainNet.
Clipart Painting
Source-only
Source-only+A(·)
labels. If we swap these two augmentations, i.e. using weak
augmentation for prediction while strong augmentation for
pseudo-label generation, the accuracy decreases from 61.73%
to 54.32% on miniDomainNet. In addition, if strong augmentation is applied to both branches, the accuracy declines to
58.05%, which suggests the weak-strong augmentation strategy is essential. These observations have also been reported
by Sohn et al. .
To justify that using strong augmentation is not the sole
contributor to our approach, we compare with top-performing
baselines that also use strong augmentation in Table 4. We can
see that the improvements brought by strong augmentation for
the baselines are rather limited, and the gap with our method
remains large. This result conﬁrms that collaborative ensemble
learning is the key to our superior performance.
Visualization of features. We use t-SNE to visualize the features learned by Source-only and our DAEL.
Figure 8 shows that the target features learned by Sourceonly are poorly aligned—the model cannot clearly differentiate
between “1”, “0”, “5”, “3”, and “8” (zoom-in to see the class
labels). In contrast, the target features learned by DAEL have
a much smaller domain discrepancy with the source features
and exhibit clearer class-based clustering patterns.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
Accuracy (%)
Fig. 6: Individual experts (E1−3) vs. ensemble ( ¯E) on miniDomainNet.
(a) Correct predictions
(b) Incorrect predictions
Fig. 7: Visualization of predicted classes (top-3) and the corresponding conﬁdence by each expert and the ensemble.
(a) Source-only
Fig. 8: Visualization of features from Digit-5 using t-SNE .
V. CONCLUSION
Our approach, domain adaptive ensemble learning (DAEL),
takes the ﬁrst step toward a general framework for generalizing neural networks from multiple source domains to a
target domain. When target data are not provided (the DG
problem), DAEL shows promising out-of-distribution generalization performance on PACS and Ofﬁce-Home. When
unlabeled target data are accessible (the UDA problem), DAEL
leverages pseudo labels and follows the same collaborative
ensemble learning strategy as used in the DG setting to prompt
the emergence of domain-generalizable features. Currently,
to avoid overﬁtting to noisy pseudo labels, advanced data
augmentation methods are used. However, the design of data
augmentation is often task-speciﬁc, e.g., for digit recognition
we cannot use random ﬂip; for ﬁne-grained recognition, color
distortion might be discarded. Future work can focus on new
algorithmic designs to mitigate the overﬁtting problem in a
more ﬂexible way.
APPENDIX A
IMPLEMENTATION DETAILS
Experiments on domain adaptation. SGD with momentum is used as the optimizer, and the cosine annealing rule 
is adopted for learning rate decay. For Digit-5, the CNN
backbone is constructed with three convolution layers and two
fully connected layers . For each mini-batch, we sample
from each domain 64 images. The model is trained with an
initial learning rate of 0.05 for 30 epochs. For DomainNet, we
use ResNet101 as the CNN backbone and sample from
each domain 6 images to form a mini-batch. The model is
trained with an initial learning rate of 0.002 for 40 epochs. For
miniDomainNet, we use ResNet18 as the CNN backbone.
Similarly, we sample 64 images from each domain to form
a mini-batch and train the model for 60 epochs with an
initial learning rate of 0.005. For all UDA experiments, we
set λu = 0.5 in all datasets. The sensitivity of λu = 0.5 to
performance is investigated in Fig. 5.
Experiments on domain generalization. ResNet18 is used
as the CNN backbone as in previous works . SGD
with momentum is used to train the model for 40 epochs
with an initial learning rate of 0.002. The learning rate is
further decayed by the cosine annealing rule. Each mini-batch
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
contains 30 images (10 per source domain). Note that the Lu
term in Eq.(4) is discarded here as no target data is available
for training.
APPENDIX B
PSEUDO-CODE
The full algorithm of domain adaptive ensemble learning is
presented in Alg. 1.
Algorithm 1 Pseudo-code for loss computation in DAEL.
1: Require: labeled source mini-batches {(Xi, Y i)}K
i=1, unlabeled target mini-batch Xt, source experts {Ei}K
i=1, weak/strong augmentation
a(·)/A(·), hyper-parameter λu.
2: Return: loss L.
3: Lce = 0
// Initialize Lce
4: Lcr = 0
// Initialize Lcr
5: for i = 1 to K do
// Domain-speciﬁc expert learning
Xi = a(Xi)
// Apply weak augmentation to Xi
˜Y i = Ei( ˜
// Compute prediction for expert-i
Lce = Lce + CrossEntropy( ˜Y i, Y i)
// Compute cross-entropy
loss for expert-i
// Collaborative ensemble learning for source data
Xi = A(Xi)
// Apply strong augmentation to Xi
j̸=i Ej( ˆ
// Compute ensemble prediction of
non-experts
Lcr = Lcr + MSE( ˆY i, ˜Y i)
// Compute consistency loss for nonexperts
14: end for
15: Lce = Lce/K
16: Lcr = Lcr/K
17: L = Lce + Lcr
18: if Xt is available then
// Collaborative ensemble learning for unlabeled target data
Xt = a(Xt)
// Apply weak augmentation to Xt
˜Y t, M = PseudoLabel({Ei( ˜
// Get pseudo labels and
instance masks
Xt = A(Xt)
// Apply strong augmentation to Xt
// Compute ensemble prediction of all
Lu = CrossEntropy( ˆY t, ˜Y t, M)
// Compute cross-entropy loss
for all experts
L = L + λuLu
26: end if