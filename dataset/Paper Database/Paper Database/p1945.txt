Machine Learning 108:1421–1441
 
Temporal pattern attention for multivariate time series
forecasting
Shun-Yao Shih1
· Fan-Keng Sun1 · Hung-yi Lee1
Received: 26 November 2018 / Revised: 1 May 2019 / Accepted: 29 May 2019 / Published online: 11 June 2019
© The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature 2019
Forecasting of multivariate time series data, for instance the prediction of electricity consumption, solar power production, and polyphonic piano pieces, has numerous valuable
applications. However, complex and non-linear interdependencies between time steps and
series complicate this task. To obtain accurate prediction, it is crucial to model long-term
dependency in time series data, which can be achieved by recurrent neural networks (RNNs)
with an attention mechanism. The typical attention mechanism reviews the information at
each previous time step and selects relevant information to help generate the outputs; however,
it fails to capture temporal patterns across multiple time steps. In this paper, we propose using
a set of ﬁlters to extract time-invariant temporal patterns, similar to transforming time series
data into its “frequency domain”. Then we propose a novel attention mechanism to select
relevant time series, and use its frequency domain information for multivariate forecasting.
We apply the proposed model on several real-world tasks and achieve state-of-the-art performance in almost all of cases. Our source code is available at 
Keywords Multivariate time series · Attention mechanism · Recurrent neural network ·
Convolutional neural network · Polyphonic music generation
Shun-Yao Shih and Fan-Keng Sun have contributed equally to this study.
Editors: Karsten Borgwardt, Po-Ling Loh, Evimaria Terzi, Antti Ukkonen.
B Shun-Yao Shih
 
Fan-Keng Sun
 
Hung-yi Lee
 
National Taiwan University, Taipei, Taiwan
Machine Learning 108:1421–1441
Fig. 1 Historical prices of crude oil, gasoline, and lumber. Units are omitted and scales are normalized for
simplicity
1 Introduction
In everyday life, time series data are everywhere. We observe evolving variables generated
from sensors over discrete time steps and organize them into time series data. For example, household electricity consumption, road occupancy rate, currency exchange rate, solar
power production, and even music notes can all be seen as time series data. In most cases,
the collected data are often multivariate time series (MTS) data, such as the electricity consumption of multiple clients, which are tracked by the local power company. There can exist
complex dynamic interdependencies between different series that are signiﬁcant but difﬁcult
to capture and analyze.
Analysts often seek to forecast the future based on historical data. The better the interdependencies among different series are modeled, the more accurate the forecasting can be. For
instance, as shown in Fig. 1,1 the price of crude oil heavily inﬂuences the price of gasoline,
but has a smaller inﬂuence on the price of lumber. Thus, given the realization that gasoline
is produced from crude oil and lumber is not, we can use the price of crude oil to predict the
price of gasoline.
In machine learning, we want the model to automatically learn such interdependencies
fromdata.Machinelearninghasbeenappliedtotimeseriesanalysisforbothclassiﬁcationand
forecasting . In classiﬁcation,
the machine learns to assign a label to a time series, for instance evaluating a patient’s
diagnostic categories by reading values from medical sensors. In forecasting, the machine
predicts future time series based on past observed data. For example, precipitation in the next
days, weeks, or months can be forecast according to historical measurements. The further
ahead we attempt to forecast, the harder it is.
When it comes to MTS forecasting using deep learning, recurrent neural networks
(RNNs) are often used. However, one
disadvantage in using RNNs in time series analysis is their weakness on managing long-term
dependencies, for instance yearly patterns in a daily recorded sequence .
The attention mechanism , originally utilized in
encoder–decoder networks, somewhat alleviates this problem, and
thus boosts the effectiveness of RNN .
In this paper, we propose the temporal pattern attention, a new attention mechanism for
MTS forecasting, where we use the term “temporal pattern” to refer to any time-invariant
pattern across multiple time steps. The typical attention mechanism identiﬁes the time steps
relevant to the prediction, and extracts the information from these time steps, which poses
1 Source: and 
Machine Learning 108:1421–1441
obvious limitations for MTS prediction. Consider the example in Fig. 1. To predict the
value of gasoline, the machine must learn to focus on “crude oil” and ignore “lumber”. In
our temporal pattern attention, instead of selecting the relevant time steps as in the typical
attention mechanism, the machine learns to select the relevant time series.
In addition, time series data often entails noticeable periodic temporal patterns, which are
criticalforprediction.However,theperiodicpatternsspanningmultipletimestepsaredifﬁcult
for the typical attention mechanism to identify, as it usually focuses only on a few time steps.
In temporal pattern attention, we introduce a convolutional neural network (CNN) to extract temporal pattern information from each
individual variable.
The main contributions of this paper are summarized as follows:
– We introduce a new attention concept in which we select the relevant variables as opposed
to the relevant time steps. The method is simple and general to apply on RNN.
– We use toy examples to verify that our attention mechanism enables the model to extract
temporal patterns and focus on different time steps for different time series.
– Attested by experimental results on real-world data ranging from periodic and partially
linear to non-periodic and non-linear tasks, we show that the proposed attention mechanism achieves state-of-the-art results across multiple datasets.
– The learned CNN ﬁlters in our attention mechanism demonstrate interesting and interpretable behavior.
The remainder of this paper is organized as follows. In Sect. 2 we review related work
and in Sect. 3 we describe background knowledge. Then, in Sect. 4 we describe the proposed
attention mechanism. Next, we present and analyze our attention mechanism on toy examples
inSect.5,andonMTSandpolyphonicmusicdatasetinSect.6.Finally,weconcludeinSect.7.
2 Related work
The most well-known model for linear univariate time series forecasting is the autoregressive integrated moving average (ARIMA) , which encompasses other
autoregressive time series models, including autoregression (AR), moving average (MA),
and autoregressive moving average (ARMA). Additionally, linear support vector regression
(SVR) treats the forecasting problem as a typical regression problem with time-varying parameters. However, these models are mostly limited to
linear univariate time series and do not scale well to MTS. To forecast MTS data, vector
autoregression (VAR), a generalization of AR-based models, was proposed. VAR is probably the most well-known model in MTS forecasting. Nevertheless, neither AR-based nor
VAR-based models capture non-linearity. For that reason, substantial effort has been put into
non-linear models for time series forecasting based on kernel methods ,
ensembles , Gaussian processes or regime switching . Still, these approaches apply predetermined
non-linearities and may fail to recognize different forms of non-linearity for different MTS.
Recently,deepneuralnetworkshavereceivedagreatamountofattentionduetotheirability
to capture non-linear interdependencies. Long short-term memory (LSTM) , a variant of recurrent neural network, has shown promising results in
several NLP tasks and has also been employed for MTS forecasting. Work in this area began
with using naive RNN , improved with hybrid models that combined
ARIMA and multilayer perceptrons ,
Machine Learning 108:1421–1441
Fig. 2 Proposed attention mechanism. ht represents the hidden state of the RNN at time step t. There are k
1-D CNN ﬁlters with length w, shown as different colors of rectangles. Then, each ﬁlter convolves over m
features of hidden states and produces a matrix HC with m rows and k columns. Next, the scoring function
calculates a weight for each row of HC by comparing with the current hidden state ht. Last but not least, the
weights are normalized and the rows of HC is weighted summed by their corresponding weights to generate
Vt. Finally, we concatenate Vt, ht and perform matrix multiplication to generate h′t, which is used to create
the ﬁnal forecast value (Color ﬁgure online)
and then most recently progressed to dynamic Boltzmann machines with RNN . Although these models can be applied to MTS, they mainly target univariate
or bivariate time series.
To the best of our knowledge, the long- and short-term time-series network (LSTNet) is the ﬁrst model designed speciﬁcally for MTS forecasting with up to hundreds of time series. LSTNet uses CNNs to capture short-term patterns, and LSTM or GRU
for memorizing relatively long-term patterns. In practice, however, LSTM and GRU cannot memorize very long-term interdependencies due to training instability and the gradient
vanishing problem. To address this, LSTNet adds either a recurrent-skip layer or a typical
attention mechanism. Also part of the overall model is traditional autoregression, which helps
to mitigate the scale insensitivity of neural networks. Nonetheless, LSTNet has three major
shortcomings when compared to our proposed attention mechanism: (1) the skip length of the
recurrent-skip layer must be manually tuned in order to match the period of the data, whereas
our proposed approach learns the periodic patterns by itself; (2) the LSTNet-Skip model is
speciﬁcally designed for MTS data with periodic patterns, whereas our proposed model, as
shown in our experiments, is simple and adaptable to various datasets, even non-periodic
ones; and (3) the attention layer in LSTNet-Attn model selects a relevant hidden state as
in typical attention mechanism, whereas our proposed attention mechanism selects relevant
time series which is a more suitable mechanism for MTS data.
3 Preliminaries
In this section, we brieﬂy describe two essential modules related to our proposed model: the
RNN module, and the typical attention mechanism.
3.1 Recurrent neural networks
Given a sequence of information {x1, x2, . . . , xt}, where xi ∈Rn, an RNN generally deﬁnes
a recurrent function, F, and calculates ht ∈Rm for each time step, t, as
ht = F(ht−1, xt)
Machine Learning 108:1421–1441
where the implementation of function F depends on what kind of RNN cell is used.
Long short-term memory (LSTM) cells are widely
used, which have a slightly different recurrent function:
ht, ct = F(ht−1, ct−1, xt),
which is deﬁned by the following equations:
it = sigmoid(Wxi xt + Whi ht−1)
ft = sigmoid(Wx f xt + Wh f ht−1)
ot = sigmoid(Wxoxt + Whoht−1)
ct = ft ⊙ct−1 + it ⊙tanh(Wxg xt + Whght−1)
ht = ot ⊙tanh(ct)
where it, ft, ot, and ct ∈Rm, Wxi , Wx f , Wxo and Wxg ∈Rm×n, Whi , Wh f , Who and
Whg ∈Rm×m, and ⊙denotes element-wise multiplication.
3.2 Typical attention mechanism
In the typical attention mechanism in an RNN,
given the previous states H = {h1, h2, . . . , ht−1}, a context vector vt is extracted from the
previous states. vt is a weighted sum of each column hi in H, which represents the information
relevant to the current time step. vt is further integrated with the present state ht to yield the
prediction.
Assume a scoring function f : Rm × Rm →R which computes the relevance between
its input vectors. Formally, we have the following formula to calculate the context vector vt:
exp( f (hi, ht))
j=1 exp( f (h j, ht))
4 Temporal pattern attention
While previous work focuses mainly on changing the network architecture of the attentionbased models via different settings to improve performance on various tasks, we believe there
is a critical defect in applying typical attention mechanisms on RNN for MTS forecasting.
The typical attention mechanism selects information relevant to the current time step, and
the context vector vt is the weighted sum of the column vectors of previous RNN hidden
states, H = {h1, h2, . . . , ht−1}. This design lends itself to tasks in which each time step
contains a single piece of information, for example, an NLP task in which each time step
corresponds to a single word. If there are multiple variables in each time step, it fails to ignore
variables which are noisy in terms of forecasting utility. Moreover, since the typical attention
mechanism averages the information across multiple time steps, it fails to detect temporal
patterns useful for forecasting.
The overview of the proposed model is shown in Fig. 2. In the proposed approach, given
previous RNN hidden states H ∈Rm×(t−1), the proposed attention mechanism basically
Machine Learning 108:1421–1441
attends to its row vectors. The attention weights on rows select those variables that are
helpful for forecasting. Since the context vector vt is now the weighted sum of the row vectors
containing the information across multiple time steps, it captures temporal information.
4.1 Problem formulation
In MTS forecasting, given an MTS, X = {x1, x2, . . . , xt−1}, where xi ∈Rn represents
the observed value at time i, the task is to predict the value of xt−1+Δ, where Δ is a ﬁxed
horizon with respect to different tasks. We denote the corresponding prediction as yt−1+Δ,
and the ground-truth value as ˆyt−1+Δ = xt−1+Δ. Moreover, for every task, we use only
{xt−w, xt−w+1, . . . , xt−1} to predict xt−1+Δ, where w is the window size. This is a common
practice , because the assumption is that there is no useful
information before the window and the input is thus ﬁxed.
4.2 Temporal pattern detection using CNN
CNN’s success lies in no small part to its ability to capture various important signal patterns;
as such we use a CNN to enhance the learning ability of the model by applying CNN ﬁlters
on the row vectors of H. Speciﬁcally, we have k ﬁlters Ci ∈R1×T , where T is the maximum
length we are paying attention to. If unspeciﬁed, we assume T = w. Convolutional operations
yield HC ∈Rn×k where HC
i, j represents the convolutional value of the ith row vector and
the jth ﬁlter. Formally, this operation is given by
Hi,(t−w−1+l) × C j,T−w+l.
4.3 Proposed attention mechanism
We calculate vt as a weighted sum of row vectors of HC. Deﬁned below is the scoring
function f : Rk × Rm →R to evaluate relevance:
i , ht) = 108:1421–1441
Fig. 3 Visualization of the ﬁrst type of toy examples without interdependencies (left) and the second type
of toy examples with interdependencies (right) for D = 6, which means that there are 6 time series in each
yt−1+Δ = Wh′h′
where ht, h′
t ∈Rm, Wh ∈Rm×m, Wv ∈Rm×k, and Wh′ ∈Rn×m and yt−1+Δ ∈Rn.
5 Analysis of proposed attention on toy examples
In order to elaborate the failure of traditional attention mechanisms and the inﬂuence of
interdependencies, we study the performance of different attention mechanisms on two types
of artiﬁcially constructed toy examples.
In the ﬁrst type of toy examples, the tth time step of the ith time series is deﬁned as
64 ), that is, each time series is a sine wave with different periods. Notice that any two
time series are mutually independent in the ﬁrst type, so there are no interdependency.
The second type of toy examples adds interdependencies to the ﬁrst type by mixing time
series, and thus the tth time step of the ith time series is formulated as:
where D is the number of time series. Both types of toy examples are visualized in Fig. 3 for
All models in the following analyses are trained with window size w = 64, horizon
Δ = 1, and similar amount of parameters. In this setup, each of our toy examples consists of
64 samples. Each time series in the ﬁrst sample comprises values of Eq. 16 from t = 0 to 63,
and we can shift one time step to get the second sample with values from t = 1 to 64. For the
Machine Learning 108:1421–1441
Fig. 4 Mean absolute loss and the range of standard deviation in log10 of the ﬁrst type of toy examples without
interdependencies (left) and the second type of toy examples with interdependencies (right), both in ten runs.
The baseline indicates the loss if all predicted values are zero
last sample, we use values from t = 63 to 126 as the input series correspondingly. Note that
values from t = 64 to 127 are equal to those from t = 0 to 63. We trained the models for 200
epochs on two types of toy examples for D = {1, 6, 11, . . . , 56} and record mean absolute
loss in training. There is no validation and testing data because the intent of this section is to
demonstrate the greater capability of our attention over typical attention to ﬁt MTS data not
the generalizability of our attention. The results are shown in Fig. 4.
5.1 Failure of traditional attention mechanisms
Intuitively, for the ﬁrst toy example, the model can accurately predict the next value by
memorizing the value that appears exactly one period before. However, we know that different
time series have different periods, which means to have a good prediction, the model should
be able to look back different numbers of time steps for different series. From this point, it
is clear that the failure of traditional attention mechanisms comes from extracting only one
previous time step while discounting the information in other time steps. On the other hand,
our attention mechanism attends on the features extracted from row vectors of RNN hidden
states by CNN ﬁlters, which enables the model to select relevant information across multiple
time steps.
The aforementioned explanation is veriﬁed by the left plot in Fig. 4, where we observe
that the performance of the LSTM with Luong attention is poor when D ≫1, compared to
the others. Notice that all models have similar amount of parameters, which implies that the
LSTM without attention has a larger hidden size when compared to the LSTM with Luong
attention. Consequently, the LSTM without attention outperforms the LSTM with Luong
attention when D ≫1, because the larger hidden size helps the model to make prediction
while the Luong attention is nearly useless. On the contrary, our attention is useful, so the
Machine Learning 108:1421–1441
LSTM with our attention is better than the LSTM without attention on average, even though
its hidden size is smaller. Also, removing the CNN from our attention, which results in the
same model as the “Sigmoid - W/o CNN” cell in Table 4, does not affect the performance,
which implies that our feature-wise attention is indispensable.
5.2 Influence of interdependencies
When there are interdependencies in MTS data, it is desirable to leverage the interdependencies to further improve forecasting accuracy. The right plot in Fig. 4 shows that both the
LSTM with Luong attention and the LSTM without attention do not beneﬁt from the added
interdependencies, since the loss values remain the same. On the other hand, the loss of the
LSTM with the proposed attention is lower when there are interdependencies, which suggests
that our attention successfully utilized the interdependencies to facilitate MTS forecasting.
Again, removing the CNN from our attention does not affect the performance in this case.
6 Experiments and analysis
In this section, we ﬁrst describe the datasets upon which we conducted our experiments. Next,
we present our experimental results and a visualization of the prediction against LSTNet.
Then, we discuss the ablation study. Finally, we analyze in what sense the CNN ﬁlters
resemble the bases in DFT.
6.1 Datasets
To evaluate the effectiveness and generalization ability of the proposed attention mechanism,
we used two dissimilar types of datasets: typical MTS datasets and polyphonic music datasets.
The typical MTS datasets are published by Lai et al. ; there are four datasets:
– Solar Energy2: the solar power production data from photovoltaic plants in Alabama
State in 2006.
– Trafﬁc3: two years of data provided by the California Department of Transportation that describes the road occupancy rate (between 0 and 1) on San Francisco Bay
area freeways.
– Electricity4: a record of the electricity consumption of 321 clients in kWh.
– Exchange Rate: the exchange rates of eight foreign countries (Australia, British, Canada,
China, Japan, New Zealand, Singapore, and Switzerland) from 1990 to 2016.
These datasets are real-world data that contains both linear and non-linear interdependencies.
Moreover, the Solar Energy, Trafﬁc, and Electricity datasets exhibit strong periodic patterns
indicating daily or weekly human activities. According to the authors of LSTNet, each time
series in all datasets have been split into training (60%), validation (20%), and testing set
(20%) in chronological order.
In contrast, the polyphonic music datasets introduced below are much more complicated,
in the sense that no apparent linearity or repetitive patterns exist:
2 
3 
4 
Machine Learning 108:1421–1441
Table 1 Statistics of all datasets,
where L is the length of the time
series, D is the number of time
series, S is the sampling spacing,
and B is size of the dataset in
Solar Energy
Electricity
Exchange Rate
216–102,552
LPD-5-Cleansed
1072–1,917,952 128
MuseData and LPD-5-Cleansed both have various-length time series
since the length of music pieces varies
– MuseData Boulanger-Lewandowski et al. : a collection of musical pieces from
various classical music composers in MIDI format.
– LPD-5-Cleansed Dong et al. ; Raffel : 21, 425 multi-track piano-rolls that
contain drums, piano, guitar, bass, and strings.
To train models on these datasets, we consider each played note as 1 and 0 otherwise (i.e., a
musical rest), and set one beat as one time step as shown in Table 1. Given the played notes
of 4 bars consisting of 16 beats, the task is to predict whether each pitch at the next time step
is played or not. For training, validation, and testing sets, we follow the original MuseData
separation, which is divided into 524 training pieces, 135 validation pieces, and 124 testing
pieces. LPD-5-Cleansed, however, was not split in previous work ; thus we randomly split it into training (80%), validation (10%), and testing (10%)
sets. The size of LPD-5-Cleansed dataset is much larger than others, so we decided to use a
smaller validation and testing set.
The main difference between typical MTS datasets and polyphonic music datasets is that
scalars in typical MTS datasets are continuous but scalars in polyphonic music datasets are
discrete (either 0 or 1). The statistics of both the typical MTS datasets and polyphonic music
datasets are summarized in Table 1.
6.2 Methods for comparison
We compared the proposed model with the following methods on the typical MTS datasets:
– AR: standard autoregression model.
– LRidge: VAR model with L2-regularization: the most popular model for MTS forecasting.
– LSVR: VAR model with SVR objective function .
– GP: Gaussian process model .
– SETAR: Self-exciting threshold autoregression model, a classical univariate non-linear
model .
– LSTNet-Skip: LSTNet with recurrent-skip layer.
– LSTNet-Attn: LSTNet with attention layer.
AR, LRidge, LSVR, GP and SETAR are traditional baseline methods, whereas LSTNet-Skip
and LSTNet-Attn are state-of-the-art methods based on deep neural networks.
However, as both traditional baseline methods and LSTNet are ill-suited to polyphonic
music datasets due to their non-linearity and the lack of periodicity, we use LSTM and LSTM
Machine Learning 108:1421–1441
with Luong attention as the baseline models to evaluate the proposed model on polyphonic
music datasets:
– LSTM: RNN cells as introduced in Sect. 3.
– LSTM with Luong attention: LSTM with an attention mechanism scoring function of
which f (hi, ht) = (hi)⊤Wht, where W ∈Rm×m .
6.3 Model setup and parameter settings
For all experiments, we used LSTM units in our RNN models, and ﬁxed the number of CNN
ﬁlters at 32. Also, inspired by LSTNet, we included an autoregression component in our
model when training and testing on typical MTS datasets.
For typical MTS datasets, we conducted a grid search over tunable parameters as done
with LSTNet. Speciﬁcally, on Solar Energy, Trafﬁc, and Electricity, the range for window
size w was {24, 48, 96, 120, 144, 168}, the range for the number of hidden units m was
{25, 45, 70}, and the range for the step of the exponential learning rate decay with a rate of
0.995 was {200, 300, 500, 1000}. On Exchange Rate, these three parameters were {30, 60},
{6, 12}, and {120, 200}, respectively. Two types of data normalization were also viewed as
part of the grid search: one normalized each time series by the maximum value in itself, and
the other normalized every time series by the maximum value over the whole dataset. Lastly,
we used the absolute loss function and Adam with a 10−3 learning rate on Solar Energy,
Trafﬁc, and Electricity, and a 3 · 10−3 learning rate on Exchange Rate. For AR, LRidge,
LSVR and GP, we followed the parameter settings as reported in the LSTNet paper . For SETAR, we searched the embedding dimension over {24,48,96,120,144,168} for
Solar Energy, Trafﬁc, and Electricity, and ﬁxed the embedding dimension to 30 for Exchange
Rate. The two different setups between our method and LSTNet are (1)we have two data
normalization methods to choose from, whereas LSTNet only used the ﬁrst type of data
normalization; and (2) the grid search over the window size w is different.
For models used for the polyphonic music datasets, including the baselines and proposed
models in the following subsections, we used 3 layers for all RNNs, as done in Chuan and
Herremans , and ﬁxed the trainable parameters to around 5 · 106 by adjusting the
number of LSTM units to fairly compare different models. In addition, we used the Adam
optimizer with a 10−5 learning rate and a cross entropy loss function.
6.4 Evaluation metrics
On typical MTS datasets, since we compared the proposed model with LSTNet, we followed
the same evaluation metrics: RAE, RSE and CORR. The ﬁrst metric is the relative absolute
error (RAE), which is deﬁned as
i=1 |(yt,i −ˆyt,i)|
i=1 | ˆyt,i −ˆyt0:t1,1:n|
The next metric is the root relative squared error (RSE):
i=1(yt,i −ˆyt,i)2
i=1( ˆyt,i −ˆyt0:t1,1:n)2
Machine Learning 108:1421–1441
Table 2 Results on typical MTS datasets using RAE, RSE and CORR as metrics
Solar Energy
LSTNet-Skip
LSTNet-Attn
0.0918 ± 0.0005
0.1296 ± 0.0008
0.1902 ± 0.0021
0.2727 ± 0.0045
0.2901 ± 0.0095
0.2999 ± 0.0022
0.3112 ± 0.0015
0.3118 ± 0.0034
Electricity
Exchange Rate
LSTNet-Skip
LSTNet-Attn
0.0463 ± 0.0007
0.0491 ± 0.0007
0.0541 ± 0.0006
0.0544 ± 0.0007
0.0139 ± 0.0001
0.0192 ± 0.0002
0.0280 ± 0.0006
0.0372 ± 0.0005
Solar Energy
Machine Learning 108:1421–1441
Table 2 continued
Solar Energy
LSTNet-Skip
LSTNet-Attn
0.1803 ± 0.0008
0.2347 ± 0.0017
0.3234 ± 0.0044
0.4389 ± 0.0084
0.4487 ± 0.0180
0.4658 ± 0.0053
0.4641 ± 0.0034
0.4765 ± 0.0068
Electricity
Exchange Rate
LSTNet-Skip
LSTNet-Attn
0.0823 ± 0.0012
0.0916 ± 0.0018
0.0964 ± 0.0015
0.1006 ± 0.0015
0.0174 ± 0.0001
0.0241 ± 0.0004
0.0341 ± 0.0011
0.0444 ± 0.0006
Solar Energy
LSTNet-Skip
LSTNet-Attn
0.9850 ± 0.0001
0.9742 ± 0.0003
0.9487 ± 0.0023
0.9081 ± 0.0151
0.8812 ± 0.0089
0.8717 ± 0.0034
0.8717 ± 0.0021
0.8629 ± 0.0027
Machine Learning 108:1421–1441
Table 2 continued
Solar Energy
Electricity
Exchange Rate
LSTNet-Skip
LSTNet-Attn
0.9429 ± 0.0004
0.9337 ± 0.0011
0.9250 ± 0.0013
0.9133 ± 0.0008
0.9790 ± 0.0003
0.9709 ± 0.0003
0.9564 ± 0.0005
0.9381 ± 0.0008
Best performance in boldface; second best performance is underlined. We report the mean and standard deviation of our model in ten runs. All numbers besides the results of our
model is referenced from the paper of LSTNet Lai et al. 
Machine Learning 108:1421–1441
Fig. 5 Prediction results for proposed model and LSTNet-Skip on Trafﬁc testing set with 3-hour horizon.
Proposed model clearly yields better forecasts around the ﬂat line after the peak and in the valley
Fig. 6 Validation loss under different training epochs on MuseData (left), and LPD-5-Cleansed (right)
and ﬁnally the third metric is the empirical correlation coefﬁcient (CORR):
t=t0(yt,i −yt0:t1,i)( ˆyt,i −ˆyt0:t1,i)
t=t0(yt,i −yt0:t1,i)2 t1
t=t0( ˆyt,i −ˆyt0:t1,i)2
where y, ˆy is deﬁned in Sect. 4.1, ˆyt, ∀t ∈[t0, t1] is the ground-truth value of the testing
data, and y denotes the mean of set y. RAE and RSE both disregards data scale and is
a normalized version of the mean absolute error (MAE) and the root mean square error
(RMSE), respectively. For RAE and RSE, the lower the better, whereas for CORR, the
higher the better.
To decide which model is better on polyphonic music datasets, we use validation loss
(negative log-likelihood), precision, recall, and F1 score as measurements which are widely
used in work on polyphonic music generation .
6.5 Results on typical MTS datasets
OntypicalMTSdatasets,wechosethebestmodelonthevalidationsetusingRAE/RSE/CORR
as the metric for the testing set. The numerical results are tabulated in Table 2, where the
metric of the ﬁrst two tables are RAE, followed by two tables of RSE metric, and ended
by another two tables using CORR metric. Both tables show that the proposed model outperforms almost all other methods on all datasets, horizons, and metrics. Also, our models
are able to deal with a wide range of dataset size, from the smallest 534 KB Exchange Rate
dataset to the largest 172 MB Solar Energy dataset. In these results, the proposed model
consistently demonstrates its superiority for MTS forecasting.
In the comparison to LSTNet-Skip and LSTNet-Attn, the previous state-of-the-art methods, the proposed model exhibits superior performance, especially on Trafﬁc and Electricity,
Machine Learning 108:1421–1441
Table 3 Precision, recall, and F1
score of different models on
polyphonic music datasets
W/o attention
W/ Luong attention
W/ proposed attention
LPD-5-Cleansed
W/o attention
W/ Luong attention
W/ proposed attention
Bold values indicate best performance
which contain the largest amount of time series. Moreover, on Exchange Rate, where no
repetitive pattern exists, the proposed model is still the best overall; the performance of
LSTNet-Skip and LSTNet-Attn fall behind traditional methods, including AR, LRidge,
LSVR, GP, and SETAR. In Fig. 5 we also visualize and compare the prediction of the
proposed model and LSTNet-Skip.
In summary, the proposed model achieves state-of-the-art performance on both periodic
and non-periodic MTS datasets.
6.6 Results on polyphonic music datasets
In this subsection, to further verify the efﬁcacy and generalization ability of the proposed
model to discrete data, we describe experiments conducted on polyphonic music datasets;
the results are shown in Fig. 6 and Table 3. We compared three RNN models: LSTM, LSTM
with Luong attention, and LSTM with the proposed attention mechanism. Figure 6 shows
the validation loss across training epochs, and in Table 3, we use the models with the lowest
validation loss to calculate precision, recall, and F1 score on the testing set.
From the results, we ﬁrst verify our claim that the typical attention mechanism does not
work on such tasks, as under similar hyperparameters and trainable weights, LSTM and the
proposed model outperform such attention mechanisms. In addition, the proposed model also
learns more effectively compared to LSTM throughout the learning process and yields better
performance in terms of precision, recall, and F1 score.
6.7 Analysis of CNN filters
DFT is a variant of the Fourier transform (FT) which handles equally-spaced samples of a
signal in time. In the ﬁeld of time series analysis, there is a wide body of work that utilizes
FT or DFT to reveal important characteristics in time series . In our case, since the MTS data is also equally-spaced and discrete, we could apply
DFT to analyze it. However, in MTS data, there is more than one time series, so we naturally
average the magnitude of the frequency components of every time series, and arrive at a single
frequency domain representation. We denote this the average discrete Fourier transform
(avg-DFT). The single frequency-domain representation reveals the prevailing frequency
components of the MTS data. For instance, it is reasonable to assume a notable 24-hour
oscillation in Fig. 5, which is veriﬁed by the avg-DFT of the Trafﬁc dataset shown in Fig. 7.
Machine Learning 108:1421–1441
Fig. 7 Magnitude comparison of (1) DFT of CNN ﬁlters trained on Trafﬁc with a 3-hour horizon, and (2)
every window of the Trafﬁc dataset. To make the ﬁgure more intuitive, the unit of the horizontal axis is the
Fig. 8 Two different CNN ﬁlters trained on Trafﬁc with a 3-hour horizon, which detect different periods of
temporal patterns
Since we expect our CNN ﬁlters to learn temporal MTS patterns, the prevailing frequency
components in the average CNN ﬁlters should be similar to that of the training MTS data.
Hence, we also apply avg-DFT on the k = 32 CNN ﬁlters that are trained on Trafﬁc with
a 3-hour horizon; in Fig. 7 we plot the result alongside with the avg-DFT of every window
of Trafﬁc dataset. Impressively, the two curves reach peaks at the same periods most of the
time, which implies that the learned CNN ﬁlters resemble bases in DFT. At the 24, 12, 8, and
6-hour periods, not only is the magnitude of the Trafﬁc dataset at its peak, but the magnitude
of CNN ﬁlters also tops out. Moreover, in Fig. 8, we show that different CNN ﬁlters behave
differently. Some specialize at capturing long-term (24-hour) temporal patterns, while others
are good at recognizing short-term (8-hour) temporal patterns. As a whole, we suggest that
the proposed CNN ﬁlters play the role of bases in DFT. As demonstrated in the work by
Rippel et al. , such a “frequency domain” serves as a powerful representation for CNN
to use in training and modeling. Thus, LSTM relies on the frequency-domain information
extracted by the proposed attention mechanism to accurately forecast the future.
Machine Learning 108:1421–1441
Table 4 Ablation study
Solar Energy (horizon = 24)
Trafﬁc (horizon = 24)
0.4397 ± 0.0089
0.4414 ± 0.0093
0.4502 ± 0.0099
0.4696 ± 0.0062
0.4832 ± 0.0109
0.4810 ± 0.0083
0.4389 ± 0.0084
0.4598 ± 0.0011
0.4639 ± 0.0101
0.4765 ± 0.0068
0.4785 ± 0.0069
0.4803 ± 0.0104
0.4431 ± 0.0100
0.4454 ± 0.0093
0.4851 ± 0.0049
0.4812 ± 0.0082
0.4783 ± 0.0077
0.4779 ± 0.0073
Electricity (horizon = 24)
0.0997 ± 0.0012
0.1007 ± 0.0013
0.1010 ± 0.0011
0.04923 ± 0.0037
0.04929 ± 0.0031
0.04951 ± 0.0041
0.1006 ± 0.0015
0.1022 ± 0.0009
0.1013 ± 0.0011
0.04882 ± 0.0031
0.04958 ± 0.0028
0.04979 ± 0.0027
0.1021 ± 0.0017
0.1065 ± 0.0029
0.1012 ± 0.0008
0.05163 ± 0.0040
0.05179 ± 0.0036
0.05112 ± 0.0027
Evaluation metric for Solar Energy, Trafﬁc, and Electricity is RSE, and negative log-likelihood for MuseData. We report the mean and standard deviation in ten runs. On each
corpus, bold text represents the best and underlined text represents second best
Machine Learning 108:1421–1441
6.8 Ablation study
In order to verify that the above improvement comes from each added component rather
than a speciﬁc set of hyperparameters, we conducted an ablation study on the Solar Energy,
Trafﬁc, Electricity, and MuseData datasets. There were two main settings: one controlling
how we attend to hidden states, H, of RNN and the other controlling how we integrate the
scoring function f into the proposed model, or even disable the function. First, in the proposed method, we let the model attend to values of various ﬁlters on each position (HC
can also consider attending to values of the same ﬁlters at various positions ((HC)⊤
i ) or row
vectors of H (H⊤
i ). These three different approaches correspond to the column headers in
Table 4: “Position”, “Filter”, and “Without CNN”. Second, whereas in the typical attention
mechanism, softmax is usually used on the output value of scoring function f to extract the
most relevant information, we use sigmoid as our activation function. Therefore, we compare
these two different functions. Another possible structure for forecasting is to concatenate all
previous hidden states and let the model automatically learn which values are important. Taking these two groups of settings into consideration, we trained models with all combinations
of possible structures on these four datasets.
The MuseData results show that the model with sigmoid activation and attention on HC
(position) is clearly the best, which suggests that the proposed model is reasonably effective
for forecasting. No matter which proposed component is removed from the model, performancedrops.Forexample,usingsoftmaxinsteadofsigmoidraisesthenegativelog-likelihood
from 0.04882 to 0.04923; we obtain a even worse model with a negative log-likelihood of
0.4979 if we do not use CNN ﬁlters. In addition, we note no signiﬁcant improvement between
the proposed model and that model using softmax on the ﬁrst three datasets in Table 4: Solar
Energy, Trafﬁc, and Electricity. This is not surprising, given our motivation for using sigmoid, as explained in Sect. 4.3. Originally, we expected CNN ﬁlters to ﬁnd basic patterns
and expected the sigmoid function to help the model to combine these patterns into one that
helps. However, due to the strongly periodic nature of these three datasets, it is possible that
using a small number of basic patterns is sufﬁcient for good prediction. Overall, however,
the proposed model is more general and yields stable and competitive results across different
7 Conclusions
In this paper, we focus on MTS forecasting and propose a novel temporal pattern attention
mechanism which removes the limitation of typical attention mechanisms on such tasks.
We allow the attention dimension to be feature-wise in order for the model learn interdependencies among multiple variables not only within the same time step but also across all
previous times and series. Our experiments on both toy examples and real-world datasets
strongly support this idea and show that the proposed model achieves state-of-the-art results.
In addition, the visualization of ﬁlters also veriﬁes our motivation in a more understandable
way to human beings.
Acknowledgements This work was ﬁnancially supported by the Ministry of Science and Technology of
Machine Learning 108:1421–1441