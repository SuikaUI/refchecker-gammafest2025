DELTACON: A Principled Massive-Graph Similarity Function
Danai Koutra∗
 
Joshua T. Vogelstein†
 
Christos Faloutsos∗
 
How much did a network change since yesterday? How different is the wiring between Bob’s brain (a lefthanded male) and Alice’s brain (a right-handed female)? Graph similarity with known node correspondence,
i.e. the detection of changes in the connectivity of graphs, arises in numerous settings. In this work, we
formally state the axioms and desired properties of the graph similarity functions, and evaluate when state-ofthe-art methods fail to detect crucial connectivity changes in graphs. We propose DELTACON, a principled,
intuitive, and scalable algorithm that assesses the similarity between two graphs on the same nodes (e.g.
employees of a company, customers of a mobile carrier). Experiments on various synthetic and real graphs
showcase the advantages of our method over existing similarity measures. Finally, we employ DELTACON to
real applications: (a) we classify people to groups of high and low creativity based on their brain connectivity
graphs, and (b) do temporal anomaly detection in the who-emails-whom Enron graph.
Introduction
Graphs arise naturally in numerous situations; social, trafﬁc, collaboration and computer networks, images,
protein-protein interaction networks, brain connectivity graphs and web graphs are only a few examples. A
problem that comes up often in all those settings is the following: how much do two graphs or networks differ in
terms of connectivity?
Graph similarity (or comparison) is a core task for sense-making: abnormal changes in the network trafﬁc
may indicate a computer attack; differences of big extent in a who-calls-whom graph may reveal a national
celebration, or a telecommunication problem. Besides, network similarity can give insights into behavioral
patterns: is the Facebook message graph similar to the Facebook wall-to-wall graph? Tracking changes in
networks over time, spotting anomalies and detecting events is a research direction that has attracted much interest
(e.g., , , ).
Long in the purview of researchers, graph similarity is a well-studied problem and several approaches have
been proposed to solve variations of the problem. However, graph comparison still remains an open problem,
while, with the passage of time, the list of requirements increases: the exponential growth of graphs, both in
number and size, calls for methods that are not only accurate, but also scalable to graphs with billions of nodes.
In this paper, we address two main questions: How to compare two networks efﬁciently? How to evaluate
their similarity score? Our main contributions are the following:
1. Axioms/Properties: we formalize the axioms and properties that a similarity measure must conform to.
2. Algorithm: we propose DELTACON for measuring connectivity differences between two graphs, and show
that it is: (a) principled, conforming to all the axioms presented in Section 2, (b) intuitive, giving similarity
scores that agree with common sense and can be easily explained, and (c) scalable, able to handle largescale graphs.
∗Computer Science Department, Carnegie Mellon University.
†Department of Statistical Science, Duke University.
 
(a) Connectome: neural network of brain.
(b) Dendogram representing the hierarchical clustering of the
DELTACON similarities between the 114 connectomes.
Figure 1: (a) Brain network (connectome). Different colors correspond to each of the 70 cortical regions, whose centers
are depicted by vertices. Connections between the regions are shown by edges. DELTACON is used for clustering and
classiﬁcation. (b) The connectomes are nicely classiﬁed in two big clusters by hierarchical clustering. The classiﬁcation is
based on the pairwise DELTACON similarities between the 114 connectomes that we study. Elements in red correspond to
high artistic score - thus, DELTACON shows that artistic brains seem to have different wiring than the rest.
3. Experiments: we report experiments on synthetic and real datasets, and compare DELTACON to six stateof-the-art methods that apply to our setting.
4. Applications: We use DELTACON for real-world applications, such as temporal anomaly detection and
clustering/classiﬁcation. In Fig. 1, DELTACON is used for clustering brain graphs corresponding to 114
individuals; the two big clusters which differ in terms of connectivity correspond to people with high and
low creativity. More details are given in Sec. 5.
The paper is organized as follows: Section 2 presents the intuition behind our method, and the axioms and
desired properties of a similarity measure; Sec. 3 has the proposed algorithms; experiments on synthetic and big
real networks are in Sec. 4; Sec. 5 presents two real-world applications; the related work and the conclusions are
in Sec. 6 and 7 respectively. Finally, Table 1 presents the major symbols we use in the paper and their deﬁnitions.
Proposed Method: Intuition
How can we ﬁnd the similarity in connectivity between two graphs, or more formally how can we solve the
following problem?
PROBLEM 1. DELTACONnectivity
Given: (a) two graphs, G1(V, E1) and G2(V, E2) with the same node set1, V, and different edge sets E1 and E2,
and (b) the node correspondence.
Find: a similarity score, sim(G1, G2) ∈ , between the input graphs. Similarity score of value 0 means
totally different graphs, while 1 means identical graphs.
The obvious way to solve this problem is by measuring the overlap of their edges. Why does this often not
work in practice? Consider the following example: according to the overlap method, the pairs of barbell graphs
shown in Fig. 2 of p. 8, (B10, mB10) and (B10, mmB10), have the same similarity score. But, clearly, from
the aspect of information ﬂow, a missing edge from a clique (mB10) does not play as important role in the graph
connectivity as the missing “bridge” in mmB10. So, could we instead measure the differences in the 1-step
1If the graphs have different, but overlapping, node sets V1 and V2, we assume that V = V1 ∪V2, and the extra nodes are treated as singletons.
Table 1: Symbols and Deﬁnitions. Bold capital letters: matrices, lowercase letters with arrows: vectors, plain font: scalars.
Description
set of nodes, number of nodes
set of edges, number of edges
sim(G1, G2)
similarity between graphs G1 and G2
distance between graphs G1 and G2
n × n identity matrix
n × n adjacency matrix with elements aij
n × n diagonal degree matrix, dii = P
= D −A laplacian matrix
n × n matrix of ﬁnal scores with elements sij
n × g reduced matrix of ﬁnal scores
n × 1 unit vector with 1 in the ith element
n × 1 vector of seed scores for group k
n × 1 vector of ﬁnal afﬁnity scores to node i
number of groups (node partitions)
= 1/(1 + maxi (dii)) positive constant (< 1)
encoding the inﬂuence between neighbors
DELTACON0, DELTACON
Vertex/Edge Overlap
Graph Edit Distance 
Signature Similarity 
λ-D ADJ. / LAP
λ-distance on A / L /
normalized L
away neighborhoods, 2-step away neighborhoods etc.? If yes, with what weight? It turns out (Intuition 1) that
our method does exactly this in a principled way.
Fundamental Concept The ﬁrst conceptual step of our proposed method is to compute the pairwise node
afﬁnities in the ﬁrst graph, and compare them with the ones in the second graph. For notational compactness, we
store them in a n × n similarity matrix2 S. The sij entry of the matrix indicates the inﬂuence node i has on node
j. For example, in a who-knows-whom network, if node i is, say, republican and if we assume homophily (i.e.,
neighbors are similar), how likely is it that node j is also republican? Intuitively, node i has more inﬂuence/afﬁnity
to node j if there are many, short, heavily weighted paths from node i to j.
The second conceptual step is to measure the differences in the corresponding node afﬁnity scores of the two
graphs and report the result as their similarity score.
How to measure node afﬁnity? Pagerank, personalized Random Walks with Restarts (RWR), lazy RWR,
and the “electrical network analogy” technique are only a few of the methods that compute node afﬁnities. We
could have used Personalized RWR: [I −(1 −c)AD−1]⃗si = c ⃗ei,where c is the probability of restarting the
random walk from the initial node, ⃗ei the starting (seed) indicator vector (all zeros except 1 at position i), and
⃗si the unknown Personalized Pagerank column vector. Speciﬁcally, sij is the afﬁnity of node j w.r.t. node
i. For reasons that we explain next, we chose to use a more recent and principled method, the so-called Fast
Belief Propagation (FABP), which is identical to Personalized RWR under speciﬁc conditions (see Theorem 2 in
Appendix A.2 of ). We use a simpliﬁed form of it (see Appendix A.1 in ) given by:
[I + ϵ2D −ϵA]⃗si = ⃗ei
where ⃗si = [si1, ...sin]T is the column vector of ﬁnal similarity/inﬂuence scores starting from the ith node, ϵ is
a small constant capturing the inﬂuence between neighboring nodes, I is the identity matrix, A is the adjacency
matrix and D is the diagonal matrix with the degree of node i as the dii entry.
An equivalent, more compact notation, is to use a matrix form, and to stack all the ⃗si vectors (i = 1, . . . , n)
into the n × n matrix S. We can easily prove that
S = [sij] = [I + ϵ2D −ϵA]−1 .
Why use Belief Propagation? The reasons we choose BP and its fast approximation with Eq. (2.2) are: (a)
it is based on sound theoretical background (maximum likelihood estimation on marginals), (b) it is fast (linear
on the number of edges), and (c) it agrees with intuition, taking into account not only direct neighbors, but also
2-, 3- and k-step-away neighbors, with decreasing weight. We elaborate on the last reason, next:
INTUITION 1. [Attenuating Neighboring Inﬂuence]
By temporarily ignoring the term ϵ2D in (2.2), we can expand the matrix inversion and approximate the n × n
matrix of pairwise afﬁnities, S, as
S ≈[I −ϵA]−1 ≈I + ϵA + ϵ2A2 + . . . .
As we said, our method captures the differences in the 1-step, 2-step, 3-step etc. neighborhoods in a weighted
way; differences in long paths have smaller effect on the computation of the similarity measure than differences
in short paths. Recall that ϵ < 1, and that Ak has information about the k-step paths. Notice that this is just the
intuition behind our method; we do not use this simpliﬁed formula to ﬁnd matrix S.
2In reality, we don’t measure all the afﬁnities (see Section 3.2 for an efﬁcient approximation).
Which properties should a similarity measure satisfy? Let G1(V, E1) and G2(V, E2) be two graphs, and
sim(G1, G2) ∈ denote their similarity score. Then, we want the similarity measure to obey the following
A1. Identity property: sim(G1, G1) = 1
A2. Symmetric property: sim(G1, G2) = sim(G2, G1)
A3. Zero property: sim(G1, G2) →0 for n →∞, where G1 is the clique graph (Kn), and G2 is the empty
graph (i.e., the edge sets are complementary).
Moreover, the measure must be:
(a) intuitive. It should satisfy the following desired properties:
P1. [Edge Importance] Changes that create disconnected components should be penalized more than changes
that maintain the connectivity properties of the graphs.
P2. [Weight Awareness] In weighted graphs, the bigger the weight of the removed edge is, the greater the
impact on the similarity measure should be.
P3. [Edge-“Submodularity”] A speciﬁc change is more important in a graph with few edges than in a much
denser, but equally sized graph.
P4. [Focus Awareness] Random changes in graphs are less important than targeted changes of the same extent.
(b) scalable. The huge size of the generated graphs, as well as their abundance require a similarity measure
that is computed fast and handles graphs with billions of nodes.
Proposed Method: Details
Now that we have described the high level ideas behind our method, we move on to the details.
Algorithm Description Let the graphs we compare be G1(V, E1) and G2(V, E2).
If the graphs have
different node sets, say V1 and V2, we assume that V = V1 ∪V2, where some nodes are disconnected.
As mentioned before, the main idea behind our proposed similarity algorithm is to compare the node afﬁnities
in the given graphs. The steps of our similarity method are:
Step 1. By eq. (2.2), we compute for each graph the n × n matrix of pairwise node afﬁnity scores (S1 and
S2 for graphs G1 and G2 respectively).
Step 2. Among the various distance and similarity measures (e.g., Euclidean distance (ED), cosine similarity,
correlation) found in the literature, we use the root euclidean distance (ROOTED, a.k.a. Matusita distance)
d = ROOTED(S1, S2) =
(√s1,ij −√s2,ij)2.
We use the ROOTED distance for the following reasons:
1. it is very similar to the Euclidean distance (ED), the only difference being the square root of the pairwise
similarities (sij),
2. it usually gives better results, because it “boosts” the node afﬁnities3 and, therefore, detects even small
changes in the graphs (other distance measures, including ED, suffer from high similarity scores no matter
how much the graphs differ), and
3. satisﬁes the desired properties P1-P4. As discussed in the Appendix A.5 of , at least P1 is not satisﬁed
by the ED.
3The node afﬁnities are in , so the square root makes them bigger.
Step 3. For interpretability, we convert the distance (d) to similarity measure (sim) via the formula
1+d. The result is bounded to the interval , as opposed to being unbounded [0,∞). Notice that
the distance-to-similarity transformation does not change the ranking of results in a nearest-neighbor query.
The straightforward algorithm, DELTACON0 (Algorithm 1), is to compute all the n2 afﬁnity scores of matrix
S by simply using equation (2.2). We can do the inversion using the Power Method or any other efﬁcient method.
Algorithm 1 DELTACON0
INPUT: edge ﬁles of G1(V, E1) and G2(V, E2)
// V = V1 ∪V2, if V1 and V2 are the graphs’ node sets
S1 = [I + ϵ2D1 −ϵA1]−1
// s1,ij: afﬁnity/inﬂuence of
S2 = [I + ϵ2D2 −ϵA2]−1
//node i to node j in G1
d(G1, G2) =ROOTED (S1, S2)
return sim(G1, G2) =
Scalability Analysis DELTACON0 satisﬁes all the properties in Section 2, but it is quadratic (n2 afﬁnity
scores sij - using power method for the inversion of sparse matrix) and thus not scalable. We present a faster,
linear algorithm, DELTACON (Algorithm 2), which approximates DELTACON0 and differs in the ﬁrst step. We
still want each node to become a seed exactly once in order to ﬁnd the afﬁnities of the rest of the nodes to it; but,
here, we have multiple seeds at once, instead of having one seed at a time. The idea is to randomly divide our
node-set into g groups, and compute the afﬁnity score of each node i to group k, thus requiring only n×g scores,
which are stored in the n × g matrix S′ (g ≪n). Intuitively, instead of using the n × n afﬁnity matrix S, we add
up the scores of the columns that correspond to the nodes of a group, and obtain the n × g matrix S′ (g ≪n).
The score s′
ik is the afﬁnity of node i to the kth group of nodes (k = 1, . . . , g).
LEMMA 3.1. The time complexity of computing the reduced afﬁnity matrix, S′, is linear on the number of edges.
Proof. We can compute the n × g “skinny” matrix S′ quickly, by solving [I + ϵ2D −ϵA]S′ = [⃗s01 . . .⃗s0g],
where ⃗s0k = P
i∈groupk ⃗ei is the membership n × 1 vector for group k (all 0’s, except 1’s for members of the
Thus, we compute g ﬁnal scores per node, which denote its afﬁnity to every group of seeds, instead of every seed
node that we had in eq. (2.2). With careful implementation, DELTACON is linear on the number of number of
edges and groups g. As we show in section 4.2, it takes ∼160sec, on commodity hardware, for a 1.6-million-node
Once we have the reduced afﬁnity matrices S′
2 of the two graphs, we use the ROOTED, to ﬁnd the
similarity between the n × g matrices of ﬁnal scores, where g ≪n.
LEMMA 3.2. The time complexity of DELTACON, when applied in parallel to the input graphs, is linear on the
number of edges in the graphs, i.e. O(g · max{m1, m2}).
Proof. Based on lemma 3.1. See Appendix A.3 in .
THEOREM 1. DELTACON’s similarity score between any two graphs G1, G2 upper bounds the actual
DELTACON0’s similarity score, i.e. simDC−0(G1, G2) ≤simDC(G1, G2).
Proof. Intuitively, grouping nodes blurs the inﬂuence information and makes the nodes seem more similar than
originally. For more details, see Appendix A.3 of .
In the following section we show that DELTACON (which includes DELTACON0 as a special case for g = n)
satisﬁes the axioms and properties, while in the Appendix (A.4 and A.5 in ) we provide the proofs.
Algorithm 2 DELTACON
INPUT: edge ﬁles of G1(V, E1) and G2(V, E2) and
g (groups: # of node partitions)
j=1 = random partition(V, g)
//g groups
// estimate afﬁnity vector of nodes i = 1, . . . , n to group k
for k = 1 →g do
solve [I + ϵ2D1 −ϵA1]⃗s′
solve [I + ϵ2D2 −ϵA2]⃗s′
12 . . . ⃗s′
22 . . . ⃗s′
// compare afﬁnity matrices S′
d(G1, G2) =ROOTED (S′
return sim(G1, G2) =
Table 2: Real and Synthetic Datasets
Description
Brain Graphs
connectome
Enron Email 
who-emails-whom
Epinions 
who-trusts-whom
Email EU 
who-sent-to-whom
Web Google 
site-to-site
AS skitter 
11,095,298
Kronecker 1
Kronecker 2
Kronecker 3
Kronecker 4
Kronecker 5
16,777,216
Kronecker 6
67,108,864
Experiments
We conduct several experiments on synthetic and real data (undirected, unweighted graphs, unless stated
otherwise - see Table 2) to answer the following questions:
Q1. Does DELTACON agree with our intuition and satisfy the axioms/properties? Where do other methods fail?
Q2. Is DELTACON scalable?
The implementation is in Matlab and we ran the experiments on AMD Opteron Processor 854 @3GHz, RAM
Intuitiveness of DELTACON. To answer Q1, for the ﬁrst 3 properties (P1-P3), we conduct experiments
on small graphs of 5 to 100 nodes and classic topologies (cliques, stars, circles, paths, barbell and wheel-barbell
graphs, and “lollipops” shown in Fig. 2), since people can argue about their similarities. For the name conventions
see Table 3. For our method we used 5 groups (g), but the results are similar for other choices of the parameter.
In addition to the synthetic graphs, for the last property (P4), we use real networks with up to 11 million edges
(Table 2).
We compare our method, DELTACON, to the 6 best state-of-the-art similarity measures that apply to our
Figure 2: Small synthetic graphs – K: clique, C: cycle, P: path, S: star, B:
barbell, L: lollipop, WhB: wheel-barbell
clique of size n
path of size n
cycle of size n
star of size n
lollipop of size n
barbell of size n
wheel barbell of size n
missing X edges
missing X “bridge” edges
weight of “bridge” edge
Table 3: Name Conventions for small
synthetic graphs. Missing number after
the preﬁx implied X = 1.
1. Vertex/Edge Overlap (VEO) : For two graphs G1(V1, E1) and G2(V2, E2):
simV EO(G1, G2) = 2 |E1 ∩E2| + |V1 ∩V2|
|E1| + |E2| + |V1| + |V2|.
2. Graph Edit Distance (GED) : GED has quadratic complexity in general, so they consider the case
where only insertions and deletions are allowed.
simGED(G1, G2)
|V1| + |V2| −2|V1 ∩V2|
|E1| + |E2| −2|E1 ∩E2|.
V2 and unweighted graphs, simGED is equivalent to hamming distance(A1, A2)
sum(A1 XOR A2).
3. Signature Similarity (SS) : This is the best performing similarity measure studied in . It is based on
the SimHash algorithm (random projection based method).
4. The last 3 methods are variations of the well-studied spectral method “λ-distance” ( , , ). Let
i=1 and {λ2i}|V2|
i=1 be the eigenvalues of the matrices that represent G1 and G2. Then, λ-distance is
dλ(G1, G2) =
(λ1i −λ2i)2,
where k is max(|V1|, |V2|) (padding is required for the smallest vector of eigenvalues). The variations
of the method are based on three different matrix representations of the graphs: adjacency (λ-d Adj.),
laplacian (λ-d Lap.) and normalized laplacian matrix (λ-d N.L.).
Table 4: “Edge Importance” (P1). Highlighted entries violate
∆s = sim(A, B) −sim(A, C)
∆d = d(A, C) −d(A, B)
-0.30 -0.43 -8.23
WhB10 m2WhB10 mm2WhB10
Table 6: “Edge-Submodularity” (P3).
Highlighted entries
violate P3.
∆s = sim(A, B) −sim(C, D)
∆d = d(C, D) −d(A, B)
-0.24 -0.59
-0.55 -0.39
-1.16 -1.69
-0.08 -0.06
K100 m10K100
C100 m10C100
-3.48 -4.52 -1089
“Weight Awareness” (P2).
Highlighted entries
violate P2.
∆s = sim(A, B) −sim(C, D)
∆d = d(C, D) −d(A, B)
Figure 3: “Focus-Awareness” (P4).
Tables 4-6/Figure 3: DELTACON0 and DELTACON (in bold) obey all the required properties (P1-P4). Tables 4-6: Each
row of the tables corresponds to a comparison between the similarities (or distances) of two pairs of graphs; pairs (A,B)
and (A,C) for (P1); and pairs (A,B) and (C,D) for (P2) and (P3): Non-positive values of ∆s = sim(A, B) −sim(C, D) and
∆d = d(C, D) −d(A, B) - depending on whether the corresponding method computes similarity or distance - are highlighted
and mean violation of the property of interest. Figure 3: Targeted changes hurt more than random. Plot of DELTACON
similarity scores for random changes (y axis) vs. DELTACON similarity scores for targeted changes (x axis) for 4 real-world
networks. For each graph we create 8 “corrupted” versions with 10% to 80% fewer edges than the initial graphs. Notice
that all the points are above the diagonal.
The results for the ﬁrst 3 properties are presented in the form of tables 4-6. For property P1 we compare
the graphs (A,B) and (A,C) and report the difference between the pairwise similarities/distances of our proposed
methods and the 6 state-of-the-art methods. We have arranged the pairs of graphs in such way that (A,B) are
more similar than (A,C). Therefore, table entries that are non-positive mean that the corresponding method does
not satisfy the property. Similarly, for properties P2 and P3, we compare the graphs (A,B) and (C,D) and report
the difference in their pairwise similarity/distance scores.
P1. Edge Importance : “Edges whose removal creates disconnected components are more important than
other edges whose absence does not affect the graph connectivity. The more important an edge is, the more it
should affect the similarity or distance measure.”
For this experiment we use the barbell, “wheel barbell” and “lollipop” graphs, since it is easy to argue
about the importance of the individual edges. The idea is that edges in a highly connected component (e.g.
clique, wheel) are not very important from the information ﬂow viewpoint, while edges that connect (almost
uniquely) dense components play a signiﬁcant role in the connectivity of the graph and the information ﬂow.
The importance of the “bridge” edge depends on the size of the components that it connects; the bigger the
components the more important is the role of the edge.
OBSERVATION 1. Only DELTACON succeeds in distinguishing the importance of the edges (P1) w.r.t. connectivity, while all the other methods fail at least once (Table 4).
P2. Weight Awareness : “The absence of an edge of big weight is more important than the absence of a
smaller weighted edge; this should be reﬂected in the similarity measure.”
The weight of an edge deﬁnes the strength of the connection between two nodes, and, in this sense, can
be viewed as a feature that relates to the importance of the edge in the graph. For this property, we study the
weighted versions of the barbell graph, where we assume that all the edges except the “bridge” have unit weight.
OBSERVATION 2. All the methods are weight-aware (P2), except VEO and GED which compute just the overlap
in edges and vertices between the graphs (Table 5).
P3. “Edge-Submodularity” : “Let A(V, E1) and B(V, E2) be two graphs with the same node set, and
|E1| > |E2| edges. Also, assume that mxA(V, E1) and mxB(V, E2) are the respective derived graphs after
removing x edges. We expect that sim(A, mxA) > sim(B, mxB), since the fewer the edges in a constant-sized
graph, the more “important” they are.”
The results for different graph topologies and 1 or 10 removed edges (preﬁxes ’m’ and ’m10’ respectively)
are given compactly in Table 6. Recall that non-positive values denote violation of the “edge-submodularity”
OBSERVATION 3. Only DELTACON complies to the “edge-submodularity” property (P3) in all cases examined.
P4. Focus Awareness : At this point, all the competing methods have failed in satisfying at least one of the
desired properties. To test whether DELTACON is able to distinguish the extent of a change in a graph, we analyze
real datasets with up to 11 million edges (Table 2) for two different types of changes. For each graph we create
corrupted instances by removing: (i) edges from the original graph randomly, and (ii) the same number of edges
in a targeted way (we randomly choose nodes and remove all their edges, until we have removed the appropriate
fraction of edges).
In Fig. 3, for each of the 4 real networks -Email EU, Enron, Google web and AS Skitter-, we give the pair
(sim DELTACON random, sim DELTACON targeted) for each of the different levels of corruption (10%, 20%,
. . . , 80%). That is, for each corruption level and network, there is a point with coordinates the similarity score
between the original graph and the corrupted graph when the edge removal is random, and the score when the
edge removal is targeted. The line y = x corresponds to equal similarity scores for both ways of removing edges.
OBSERVATION 4.
• “Targeted changes hurt more.” DELTACON is focus-aware (P4). Removal of edges in a
targeted way leads to smaller similarity of the derived graph to the original one than removal of the same
number of edges in a random way.
• “More changes: random ≈targeted.” As the corruption level increases, the similarity score for random
changes tends to the similarity score for targeted changes (in Fig. 3, all lines converge to the y = x line
for greater level of corruption).
This is expected as the random and targeted edge removal tend to be equivalent when a signiﬁcant fraction of
edges is deleted.
General Remarks. All in all, the baseline methods have several non-desirable properties. The spectral
methods, as well as SS fail to comply to the “edge importance” (P1) and “edge submodularity” (P3) properties.
Moreover, λ-distance has high computational cost when the whole graph spectrum is computed, cannot
distinguish the differences between co-spectral graphs, and sometimes small changes lead to big differences in
the graph spectra. As far as VEO and GED are concerned, they are oblivious on signiﬁcant structural properties
of the graphs; thus, despite their straightforwardness and fast computation, they fail to discern various changes
in the graphs. On the other hand, DELTACON gives tangible similarity scores and conforms to all the desired
properties.
Scalability of DELTACON. In Section 2 we demonstrated that DELTACON is linear on the number of
edges, and here we show that this also holds in practice. We ran DELTACON on Kronecker graphs (Table 2),
which are known to share many properties with real graphs.
OBSERVATION 5. As shown in Fig. 4, DELTACON scales linearly with the number of edges in the graph.
Notice that the algorithm can be trivially parallelized by ﬁnding the node afﬁnity scores of the two graphs in
parallel instead of sequential. Moreover, for each graph the computation of the similarity scores of the nodes
to each of the g groups can be parallelized. However, the runtime of our experiments refer to the sequential
implementation.
Scalability of DELTACON
number of edges
time (sec)
Figure 4: DELTACON is linear on the number of edges (time in sec. vs. number of edges). The exact number of edges is
annotated.
DELTACON at Work
In this section we present two applications of graph similarity measures; we use DELTACON and report our
Enron. First, we analyze the time-evolving ENRON graph. Figure 5 depicts the similarity scores between
consecutive daily who-emailed-whom graphs. By applying Quality Control with Individual Moving Range, we
obtain the lower and upper limits of the in-control similarity scores. These limits correspond to median ±3σ (The
median is used instead of the mean, since appropriate hypothesis tests demonstrate that the data does not follow
the normal distribution. Moving range mean is used to estimate σ.). Using this method we were able to deﬁne the
threshold (lower control limit) below which the corresponding days are anomalous, i.e. they differ “too much”
from the previous and following days. Note that all the anomalous days relate to crucial events in the company’s
history during 2001 (points marked with red boxes in Fig. 5): (2) 8/21, Lay emails all employees stating he wants
“to restore investor conﬁdence in Enron.”; (3) 9/26, Lay tells employees that the accounting practices are “legal
and totally appropriate”, and that the stock is “an incredible bargain.”; (4) 10/5, Just before Arthur Andersen
hired Davis Polk & Wardwell law ﬁrm to prepare a defense for the company; (5) 10/24-25, Jeff McMahon takes
over as CFO. Email to all employees states that all the pertinent documents should be preserved; (6) 11/8, Enron
announces it overstated proﬁts by 586 million dollars over 5 years.
Although high similarities between consecutive days do not consist anomalies, we found that mostly
weekends expose high similarities.
For instance, the ﬁrst two points of 100% similarity correspond to the
weekend before Christmas in 2000 and a weekend in July, when only two employees sent emails to each other.
It is noticeable that after February 2002 many consecutive days are very similar; this happens because, after the
Figure 5: Graph Anomaly Detection with DELTACON. The marked days correspond to anomalies and coincide with major
events in the history of Enron. The blue points are similarity scores between consecutive instances of the daily email activity
between the employees, and the marked days are 3σ units away from the median similarity score.
collapse of Enron, the email exchange activity was rather low and often between certain employees.
Brain Connectivity Graph Clustering. We also use DELTACON for clustering and classiﬁcation. For this
purpose we study conectomes -brain graphs-, which are obtained by Multimodal Magnetic Resonance Imaging
In total we study the connectomes of 114 people; each consists of 70 cortical regions (nodes), and connections
(weighted edges) between them. We ignore the strength of connections and derive an undirected, unweighted
brain graph per person. In addition to the connectomes, we have attributes for each person (e.g., age, gender, IQ).
We ﬁrst get the DELTACON pairwise similarities between the brain graphs, and then perform hierarchical
clustering using Ward’s method (Fig. 1(b)). As shown in the ﬁgure, there are two clearly separable groups of
brain graphs. Applying t-test on the available attributes for the two groups created by the clusters, we found
that the latter differ signiﬁcantly (p-value=0.0057) in the Composite Creativity Index (CCI), which is related to
the person’s performance on a series of creativity tasks. Moreover, the two groups correspond to low and high
openness index (p-value=0.0558), one of the “Big Five Factors”; that is, the brain connectivity is different in
people that are inventive and people that are consistent. Exploiting analysis of variance (ANOVA: generalization
of t-test when more than 2 groups are analyzed), we tested whether the various clusters that we obtain from
hierarchical clustering reﬂect the structural differences in the brain graphs. However, in the dataset we studied
there is no sufﬁcient statistical evidence that age, gender, IQ etc. are related to the brain connectivity.
Related Work
Graph Similarity. The problems are divided in two main categories: (1) With Known Node Correspondence.
Papadimitriou et al. propose 5 similarity measures for directed web graphs. Among them the best is the
Signature Similarity (SS), which is based on the SimHash algorithm, while the Vertex/Edge Overlap similarity
(VEO) performs very well. Bunke presents techniques used to track sudden changes in communications
networks for performance monitoring. The best approaches are the Graph Edit Distance and Maximum Common
Subgraph. Both are NP-complete, but the former approach can be simpliﬁed given the application and it becomes
linear on the number of nodes and edges in the graphs. (2) With Unknown Node Correspondence. Two approaches
can be used: (a) feature extraction and similarity computation, (b) graph matching and application of techniques
from the ﬁrst category , (c) graph kernels . The research directions in this category include: λ-distance
( , , ), a spectral method that has been studied thoroughly; algebraic connectivity ; an SVM-based
approach on global feature vectors ; social networks similarity ; computing edge curvatures under heat
kernel embedding ; comparison of the number of spanning trees ; fast random walk graph kernel .
Both research directions are important, but apply in different settings; if the node correspondence is available,
the algorithms that make use of it can perform only better than methods that omit it. Here we tackle the former
Node afﬁnity algorithms.
There are numerous node afﬁnity algorithms; Pagerank , Personalized
Random Walks with Restarts , the electric network analogy , SimRank , and Belief Propagation 
are only some examples of the most successful techniques. Here we focus on the latter method, and speciﬁcally
a fast variation which is also intuitive. All the techniques have been used successfully in many tasks, such
as ranking, classiﬁcation, malware and fraud detection ( , ), and recommendation systems .
Conclusions
In this work, we tackle the problem of graph similarity when the node correspondence is known (e.g., similarity
in time-evolving phone networks). Our contributions are:
• Axioms/Properties: we formalize the problem of graph similarity by providing axioms, and desired
properties.
• Algorithm: We propose DELTACON, an algorithm that is (a) principled (axioms A1-A3, in Sec. 2), (b)
intuitive (properties P1-P4, in Sec. 4), and (c) scalable, needing on commodity hardware ˜160 seconds for
a graph with over 67 million edges.
• Experiments: We evaluate the intuitiveness of DELTACON, and compare it to 6 state-of-the-art measures.
• Applications: We use DELTACON for temporal anomaly detection (ENRON), and clustering & classiﬁcation (brain graphs).
Future work includes parallelizing our algorithm, as well as trying to partition the graphs in a more informative
way (e.g., using elimination tree) than random.
Acknowledgements
The authors would like to thank Aaditya Ramdas, Aarti Singh, Elijah Mayﬁeld, Gary Miller, and Jilles Vreeken for their helpful comments and suggestions.
Funding was provided by the U.S. Army Research Ofﬁce (ARO) and Defense Advanced Research Projects Agency (DARPA) under Contract Number
W911NF-11-C-0088. Research was also sponsored by the Army Research Laboratory and was accomplished under Cooperative Agreement Number
W911NF-09-2-0053. It was also partially supported by an IBM Faculty Award. The views and conclusions contained in this document are those of
the authors and should not be interpreted as representing the ofﬁcial policies, either expressed or implied, of the Army Research Laboratory or the U.S.
Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation