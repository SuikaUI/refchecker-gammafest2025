Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 687–696,
Beijing, China, July 26-31, 2015. c⃝2015 Association for Computational Linguistics
Knowledge Graph Embedding via Dynamic Mapping Matrix
Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu and Jun Zhao
National Laboratory of Pattern Recognition (NLPR)
Institute of Automation Chinese Academy of Sciences, Beijing, 100190, China
{guoliang.ji,shizhu.he,lhxu,kliu,jzhao}@nlpr.ia.ac.cn
Knowledge graphs are useful resources for
numerous AI applications, but they are far
from completeness. Previous work such as
TransE, TransH and TransR/CTransR regard a relation as translation from head entity to tail entity and the CTransR achieves
state-of-the-art performance.
In this paper, we propose a more ﬁne-grained model
named TransD, which is an improvement
of TransR/CTransR. In TransD, we use
two vectors to represent a named symbol object (entity and relation). The ﬁrst
one represents the meaning of a(n) entity
(relation), the other one is used to construct mapping matrix dynamically. Compared with TransR/CTransR, TransD not
only considers the diversity of relations,
but also entities. TransD has less parameters and has no matrix-vector multiplication operations, which makes it can be
applied on large scale graphs. In Experiments, we evaluate our model on two typical tasks including triplets classiﬁcation
and link prediction.
Evaluation results
show that our approach outperforms stateof-the-art methods.
Introduction
Knowledge Graphs such as WordNet , Freebase and Yago
 have been playing a pivotal role in many AI applications, such as relation
extraction(RE), question answering(Q&A), etc.
They usually contain huge amounts of structured
data as the form of triplets (head entity, relation,
tail entity)(denoted as (h, r, t)), where relation
models the relationship between the two entities.
As most knowledge graphs have been built either
collaboratively or (partly) automatically, they often suffer from incompleteness. Knowledge graph
completion is to predict relations between entities
based on existing triplets in a knowledge graph. In
the past decade, much work based on symbol and
logic has been done for knowledge graph completion, but they are neither tractable nor enough convergence for large scale knowledge graphs. Recently, a powerful approach for this task is to encode every element (entities and relations) of a
knowledge graph into a low-dimensional embedding vector space. These methods do reasoning
over knowledge graphs through algebraic operations (see section ”Related Work”).
Among these methods, TransE is simple and effective, and also achieves
state-of-the-art prediction performance. It learns
low-dimensional embeddings for every entity and
relation in knowledge graphs. These vector embeddings are denoted by the same letter in boldface. The basic idea is that every relation is regarded as translation in the embedding space. For
a golden triplet (h, r, t), the embedding h is close
to the embedding t by adding the embedding r,
that is h + r ≈t. TransE is suitable for 1-to-1
relations, but has ﬂaws when dealing with 1-to-
N, N-to-1 and N-to-N relations. TransH is proposed to solve these issues.
TransH regards a relation as a translating operation on a relation-speciﬁc hyperplane, which is
characterized by a norm vector wr and a translation vector dr.
The embeddings h and t are
ﬁrst projected to the hyperplane of relation r to
obtain vectors h⊥= h −w⊤
r hwr and t⊥=
r twr, and then h⊥+ dr ≈t⊥.
in TransE and TransH, the embeddings of entities and relations are in the same space.
However, entities and relations are different types objects, it is insufﬁcient to model them in the same
space. TransR/CTransR set a
mapping matrix Mr and a vector r for every relation r. In TransR, h and t are projected to the
aspects that relation r focuses on through the ma-
Entity Space
Relation Space
Simple illustration of TransD. Each
shape represents an entity pair appearing in a
triplet of relation r. Mrh and Mrt are mapping
matrices of h and t, respectively. hip, tip(i =
1, 2, 3), and rp are projection vectors. hi⊥and
ti⊥(i = 1, 2, 3) are projected vectors of entities.
The projected vectors satisfy hi⊥+ r ≈ti⊥(i =
trix Mr and then Mrh + r ≈Mrt. CTransR is
an extension of TransR by clustering diverse headtail entity pairs into groups and learning distinct
relation vectors for each group. TransR/CTransR
has signiﬁcant improvements compared with previous state-of-the-art models. However, it also has
several ﬂaws: (1) For a typical relation r, all entities share the same mapping matrix Mr. However, the entities linked by a relation always contains various types and attributes. For example, in
triplet (friedrich burklein, nationality, germany),
friedrich burklein and germany are typical different types of entities. These entities should be projected in different ways; (2) The projection operation is an interactive process between an entity
and a relation, it is unreasonable that the mapping matrices are determined only by relations;
and (3) Matrix-vector multiplication makes it has
large amount of calculation, and when relation
number is large, it also has much more parameters than TransE and TransH. As the complexity, TransR/CTransR is difﬁcult to apply on largescale knowledge graphs.
In this paper, we propose a novel method named
TransD to model knowledge graphs.
shows the basic idea of TransD. In TransD, we de-
ﬁne two vectors for each entity and relation. The
ﬁrst vector represents the meaning of an entity or
a relation, the other one (called projection vector)
represents the way that how to project a entity embedding into a relation vector space and it will
be used to construct mapping matrices.
Therefore, every entity-relation pair has an unique mapping matrix. In addition, TransD has no matrixby-vector operations which can be replaced by
vectors operations. We evaluate TransD with the
task of triplets classiﬁcation and link prediction.
The experimental results show that our method has
signiﬁcant improvements compared with previous
Our contributions in this paper are: (1)We propose a novel model TransD, which constructs a
dynamic mapping matrix for each entity-relation
pair by considering the diversity of entities and relations simultaneously. It provides a ﬂexible style
to project entity representations to relation vector space; (2) Compared with TransR/CTransR,
TransD has fewer parameters and has no matrixvector multiplication.
It is easy to be applied
on large-scale knowledge graphs like TransE and
TransH; and (3) In experiments, our approach
outperforms previous models including TransE,
TransH and TransR/CTransR in link prediction
and triplets classiﬁcation tasks.
Related Work
Before proceeding, we deﬁne our mathematical
notations. We denote a triplet by (h, r, t) and their
column vectors by bold lower case letters h, r, t;
matrices by bold upper case letters, such as M;
tensors by bold upper case letters with a hat, such
M. Score function is represented by fr(h, t).
For a golden triplet (h, r, t) that corresponds to a
true fact in real world, it always get a relatively
higher score, and lower for an negative triplet.
Other notations will be described in the appropriate sections.
TransE, TransH and TransR/CTransR
As mentioned in Introduction section, TransE
 regards the relation r as translation from h to t for a golden triplet (h, r, t).
Hence, (h+r) is close to (t) and the score function
fr(h, t) = −∥h + r −t∥2
TransE is only suitable for 1-to-1 relations, there
remain ﬂaws for 1-to-N, N-to-1 and N-to-N relations.
To solve these problems, TransH proposes an improved model named translation on a hyperplane. On hyperplanes of different relations, a given entity has different representations. Similar to TransE, TransH has the score
function as follows:
fr(h, t) = −∥h⊥+ r −t⊥∥2
#Parameters
# Operations (Time complexity)
Unstructured 
SE 
O(Nem + 2Nrn2)(m = n)
SME(linear) 
O(Nem + Nrn + 4mk + 4k)(m = n)
SME (bilinear) 
O(Nem + Nrn + 4mks + 4k)(m = n)
LFM 
O(Nem + Nrn2)(m = n)
O((m2 + m)Nt)
SLM 
O(Nem + Nr(2k + 2nk))(m = n)
O((2mk + k)Nt)
NTN 
O(Nem + Nr(n2s + 2ns + 2s))(m = n)
O(((m2 + m)s + 2mk + k)Nt)
TransE 
O(Nem + Nrn)(m = n)
TransH 
O(Nem + 2Nrn)(m = n)
TransR 
O(Nem + Nr(m + 1)n)
CTransR 
O(Nem + Nr(m + d)n)
TransD (this paper)
O(2Nem + 2Nrn)
Table 1: Complexity (the number of parameters and the number of multiplication operations in an epoch)
of several embedding models. Ne and Nr represent the number of entities and relations, respectively.
Nt represents the number of triplets in a knowledge graph. m is the dimension of entity embedding
space and n is the dimension of relation embedding space. d denotes the average number of clusters of a
relation. k is the number of hidden nodes of a neural network and s is the number of slice of a tensor.
In order to ensure that h⊥and t⊥are on the hyperplane of r, TransH restricts ∥wr∥= 1.
Both TransE and TransH assume that entities
and relations are in the same vector space. But
relations and entities are different types of objects, they should not be in the same vector space.
TransR/CTransR is proposed
based on the idea. TransR set a mapping matrix
Mr for each relation r to map entity embedding
into relation vector space. Its score function is:
fr(h, t) = −∥Mrh + r −Mrt∥2
where Mr ∈Rm×n, h, t ∈Rn and r ∈Rm.
CTransR is an extension of TransR. As head-tail
entity pairs present various patterns in different relations, CTransR clusters diverse head-tail entity
pairs into groups and sets a relation vector for each
Other Models
Unstructured. Unstructured model ignores relations, only models entities
as embeddings. The score function is
fr(h, t) = −∥h −t∥2
It’s a simple case of TransE. Obviously, Unstructured model can not distinguish different relations.
Structured Embedding (SE). SE model sets two separate matrices Mrh and
Mrt to project head and tail entities for each relation. Its score function is deﬁned as follows:
fr(h, t) = −∥Mrhh −Mrtt∥1
Semantic Matching Energy (SME). SME model
 encodes each named
symbolic object (entities and relations) as a vector.
Its score function is a neural network that captures
correlations between entities and relations via matrix operations. Parameters of the neural network
are shared by all relations. SME deﬁnes two semantic matching energy functions for optimization, a linear form
gη = Mη1eη + Mη2r + bη
and a bilinear form
gη = (Mη1eη) ⊗(Mη2r) + bη
where η = {left, right}, eleft = h, eright = t
and ⊗is the Hadamard product. The score function is
fr(h, t) = gleft⊤gright
In , matrices of the bilinear
form are replaced by tensors.
Latent Factor Model (LFM). LFM model encodes each entity into a vector and sets a matrix for every relation. It deﬁnes a score function
fr(h, t) = h⊤Mrt, which incorporates the interaction of the two entity vectors in a simple and
effecitve way.
Single Layer Model (SLM). SLM model is designed as a baseline of Neural Tensor Network
 . The model constructs a nonlinear neural network to represent the score function deﬁned as follows.
fr(h, t) = u⊤
r f(Mr1h + Mr2t + br)
where Mr1, Mr2 and br are parameters indexed
by relation r, f() is tanh operation.
Neural Tensor Network (NTN). NTN model
 extends SLM model by considering the second-order correlations into nonlinear neural networks. The score function is
fr(h, t) = u⊤
+ br) (10)
Wr represents a 3-way tensor, Mr denotes
the weight matrix, br is the bias and f() is tanh
operation. NTN is the most expressive model so
far, but it has so many parameters that it is difﬁcult
to scale up to large knowledge graphs.
Table 1 lists the complexity of all the above
models. The complexity (especially for time) of
TransD is much less than TransR/CTransR and is
similar to TransE and TransH. Therefore, TransD
is effective and train faster than TransR/CTransR.
Beyond these embedding models, there is other related work of modeling multi-relational data, such
as matrix factorization, recommendations, etc. In
experiments, we refer to the results of RESCAL
presented in and compare with it.
Our Method
We ﬁrst deﬁne notations. Triplets are represented
as (hi, ri, ti)(i = 1, 2, . . . , nt), where hi denotes
a head entity, ti denotes a tail entity and ri denotes a relation. Their embeddings are denoted by
hi, ri, ti(i = 1, 2, . . . , nt). We use ∆to represent
golden triplets set, and use ∆
′ to denote negative
triplets set. Entities set and relations set are denoted by E and R, respectively. We use Im×n to
denote the identity matrix of size m × n.
Multiple Types of Entities and Relations
Considering the diversity of relations, CTransR
segments triplets of a speciﬁc relation r into
several groups and learns a vector representation for each group.
However, entities also
have various types.
Figure 2 shows several
kinds of head and tail entities of relation location.location.partially containedby in FB15k. In
both TransH and TransR/CTransR, all types of entities share the same mapping vectors/matrices.
However, different types of entities have different attributes and functions, it is insufﬁcient to let
them share the same transform parameters of a relation. And for a given relation, similar entities
should have similar mapping matrices and otherwise for dissimilar entities. Furthermore, the mapping process is a transaction between entities and
relations that both have various types. Therefore,
we propose a more ﬁne-grained model TransD,
which considers different types of both entities
and relations, to encode knowledge graphs into
embedding vectors via dynamic mapping matrices
produced by projection vectors.
Figure 2: Multiple types of entities of relation location.location.partially containedby.
Model In TransD, each named symbol object (entities and relations) is represented by two vectors.
The ﬁrst one captures the meaning of entity (relation), the other one is used to construct mapping
For example, given a triplet (h, r, t),
its vectors are h, hp, r, rp, t, tp, where subscript
p marks the projection vectors, h, hp, t, tp ∈Rn
and r, rp ∈Rm. For each triplet (h, r, t), we
set two mapping matrices Mrh, Mrt ∈Rm×n to
project entities from entity space to relation space.
They are deﬁned as follows:
Mrh = rph⊤
Mrt = rpt⊤
Therefore, the mapping matrices are determined
by both entities and relations, and this kind of
operation makes the two projection vectors interact sufﬁciently because each element of them can
meet every entry comes from another vector. As
we initialize each mapping matrix with an identity
matrix, we add the Im×n to Mrh and Mrh. With
the mapping matrices, we deﬁne the projected vectors as follows:
Then the score function is
fr(h, t) = −∥h⊥+ r −t⊥∥2
In experiments, we enforce constrains as ∥h∥2 ≤
1, ∥t∥2 ≤1, ∥r∥2 ≤1, ∥h⊥∥2 ≤1 and ∥t⊥∥2 ≤
Training Objective We assume that there are
nt triplets in training set and denote the ith triplet
by (hi, ri, ti)(i = 1, 2, . . . , nt). Each triplet has a
label yi to indicate the triplet is positive (yi = 1)
or negative (yi = 0). Then the golden and negative triplets are denoted by ∆= {(hj, rj, tj) |
yj = 1} and ∆
′ = {(hj, rj, tj) | yj = 0}, respectively. Before training, one important trouble is
that knowledge graphs only encode positive training triplets, they do not contain negative examples.
Therefore, we obtain ∆from knowledge graphs
and generate ∆
′ as follows: ∆
′ = {(hl, rk, tk) |
hl ̸= hk ∧yk = 1} ∪{(hk, rk, tl) | tl ̸= tk ∧yk =
1}. We also use two strategies “unif” and “bern”
described in to replace the head
or tail entity.
Let us use ξ and ξ
′ to denote a golden triplet
and a corresponding negative triplet, respectively.
Then we deﬁne the following margin-based ranking loss as the objective for training:
′) −fr(ξ)]+
where [x]+ ≜max (0, x), and γ is the margin separating golden triplets and negative triplets. The
process of minimizing the above objective is carried out with stochastic gradient descent (SGD).
In order to speed up the convergence and avoid
overﬁtting, we initiate the entity and relation embeddings with the results of TransE and initiate all
the transfer matrices with identity matrices.
Connections with TransE, TransH and
TransR/CTransR
TransE is a special case of TransD when the dimension of vectors satisﬁes m = n and all projection vectors are set zero.
TransH is related to TransD when we set m =
n. Under the setting, projected vectors of entities
can be rewritten as follows:
h⊥= Mrhh = h + h⊤
t⊥= Mrtt = t + t⊤
Hence, when m = n, the difference between
TransD and TransH is that projection vectors are
determinded only by relations in TransH, but
TransD’s projection vectors are determinded by
both entities and relations.
As to TransR/CTransR, TransD is an improvement of it.
TransR/CTransR directly deﬁnes a
mapping matrix for each relation, TransD consturcts two mapping matrices dynamically for
each triplet by setting a projection vector for each
entity and relation.
In addition, TransD has no
matrix-vector multiplication operation which can
be replaced by vector operations. Without loss of
generality, we assume m ≥n, the projected vectors can be computed as follows:
h⊥= Mrhh = h⊤
h⊤, 0⊤⊤(18)
t⊥= Mrtt = t⊤
Therefore,
TransD has less calculation than
TransR/CTransR, which makes it train faster and
can be applied on large-scale knowledge graphs.
Experiments and Results Analysis
We evaluate our apporach on two tasks: triplets
classiﬁcation and link prediction. Then we show
the experiments results and some analysis of them.
Triplets classiﬁcation and link prediction are implemented on two popular knowledge graphs:
WordNet and Freebase . WordNet is a large lexical knowledge
graph. Entities in WordNet are synonyms which
express distinct concepts. Relations in WordNet
are conceptual-semantic and lexical relations. In
this paper, we use two subsets of WordNet: WN11
 and WN18 . Freebase is a large collaborative knowledge base consists of a large number of the world
facts, such as triplets (anthony asquith, location,
london) and (nobuko otowa, profession, actor).
We also use two subsets of Freebase: FB15k and FB13 .
Table 2 lists statistics of the 4 datasets.
Table 2: Datesets used in the experiments.
Triplets Classiﬁcation
Triplets classiﬁcation aims to judge whether a
given triplet (h, r, t) is correct or not, which is a
binary classiﬁcation task. Previous work 
had explored this task. In this paper ,we use three
datasets WN11, FB13 and FB15k to evaluate our
approach. The test sets of WN11 and FB13 provided by contain golden and
negative triplets. As to FB15k, its test set only
contains correct triplets, which requires us to construct negative triplets. In this parper, we construct
negative triplets following the same setting used
for FB13 .
For triplets classiﬁcation, we set a threshold δr
for each relation r. δr is obtained by maximizing
the classiﬁcation accuracies on the valid set. For a
given triplet (h, r, t), if its score is larger than δr,
it will be classiﬁed as positive, otherwise negative.
We compare our model with several previous
embedding models presented in Related Work section. As we construct negative triplets for FB15k
by ourselves, we use the codes of TransE, TransH
and TransR/CTransR provied by 
to evaluate the datasets instead of reporting the results of directly.
In this experiment, we optimize the objective
with ADADELTA SGD . We select
the margin γ among {1, 2, 5, 10}, the dimension of entity vectors m and the dimension of relation vectors n among {20, 50, 80, 100}, and
the mini-batch size B among {100, 200, 1000,
4800}. The best conﬁguration obtained by valid
set are:γ = 1, m, n = 100, B = 1000 and taking L2 as dissimilarity on WN11; γ = 1, m, n =
100, B = 200 and taking L2 as dissimilarity on
FB13; γ = 2, m, n = 100, B = 4800 and taking L1 as dissimilarity on FB15k.
For all the
three datasets, We traverse to training for 1000
As described in Related Work section,
TransD trains much faster than TransR (On our
PC, TransR needs 70 seconds and TransD merely
spends 24 seconds a round on FB15k).
Table 3 shows the evaluation results of triplets
classiﬁcation. On WN11, we found that there are
570 entities appearing in valid and test sets but
not appearing in train set, we call them ”NULL
In valid and test sets, there are 1680
(6.4%) triplets containing ”NULL Entity”.
NTN(+E), these entity embeddings can be obtained by word embedding.
In TransD, how-
SME(bilinear)
TransE(unif)
TransE(bern)
TransH(unif)
TransH(bern)
TransR(unif)
TransR(bern)
CTransR(bern)
TransD(unif)
TransD(bern)
Table 3: Experimental results of Triplets Classiﬁcation(%). “+E” means that the results are combined with word embedding.
ever, they are only initialized randomly. Therefore, it is not fair for TransD, but we also achieve
the accuracy 86.4% which is higher than that of
NTN(+E) (86.2%). From Table 3, we can conclude that: (1) On WN11, TransD outperforms any
other previous models including TransE, TransH
and TransR/CTransR, especially NTN(+E); (2)
On FB13, the classiﬁcation accuracy of TransD
achieves 89.1%, which is signiﬁcantly higher than
that of TransE, TransH and TransR/CTransR and
is near to the performance of NTN(+E) (90.0%);
and (3) Under most circumstances, the ”bern”
sampling method works better than ”unif”.
Figure 3 shows the prediction accuracy of different relations. On the three datasets, different
relations have different prediction accuracy: some
are higher and the others are lower. Here we focus on the relations which have lower accuracy.
On WN11, the relation similar to obtains accuracy
51%, which is near to random prediction accuracy.
In the view of intuition, similar to can be inferred
from other information. However, the number of
entity pairs linked by relation similar to is only
1672, which accounts for 1.5% in all train data,
and prediction of the relation needs much information about entities. Therefore, the insufﬁcient
of train data is the main cause.
On FB13, the
accuracies of relations cuase of death and gender
are lower than that of other relations because they
are difﬁcult to infer from other imformation, especially cuase of death. Relation gender may be inferred from a person’s name ,
but we learn a vector for each name, not for the
words included in the names, which makes the
has_instance
similar_to
member_meronym
domain_region
subordinate_instance_of
domain_topic
member_holonym
synset_domain_topic
Accuracy(%)
cause_of_death
profession
nationality
institution
Accuracy(%)
Accuracy(%) of "bern"
Accuracy(%) of "unif"
Figure 3: Classiﬁcation accuracies of different relations on the three datasets. For FB15k, each triangle
represent a relation, in which the red triangles represent the relations whose accuracies of “bern” or
“unif” are lower than 50% and the blacks are higher than 50%. The red line represents the function
y = x. We can see that the most relations are in the lower part of the red line.
names information useless for gender. On FB15k,
accuracies of some relations are lower than 50%,
for which some are lack of train data and some are
difﬁcult to infer. Hence, the ability of reasoning
new facts based on knowledge graphs is under a
certain limitation, and a complementary approach
is to extract facts from plain texts.
Link Prediction
Link prediction is to predict the missing h or t for
a golden triplet (h, r, t). In this task, we remove
the head or tail entity and then replace it with all
the entities in dictionary in turn for each triplet in
test set. We ﬁrst compute scores of those corrupted
triplets and then rank them by descending order;
the rank of the correct entity is ﬁnally stored. The
task emphasizes the rank of the correct entity instead of only ﬁnding the best one entity. Similar to , we report two measures as our evaluation metrics: the average rank
of all correct entites (Mean Rank) and the proportion of correct entities ranked in top 10 (Hits@10).
A lower Mean Rank and a higher Hits@10 should
be achieved by a good embedding model. We call
the evaluation setting ”Raw’. Noting the fact that
a corrupted triplet may also exist in knowledge
graphs, the corrupted triplet should be regard as
a correct triplet.
Hence, we should remove the
corrupted triplets included in train, valid and test
sets before ranking. We call this evaluation setting
”Filter”. In this paper, we will report evaluation
results of the two settings .
In this task, we use two datasets: WN18 and
As all the data sets are the same, we
refer to their experimental results in this paper.
On WN18, we also use ADADELTA SGD for optimization. We select the margin γ
among {0.1, 0.5, 1, 2}, the dimension of entity
vectors m and the dimension of relation vectors n
among {20, 50, 80, 100}, and the mini-batch size
B among {100, 200, 1000, 1400}. The best con-
ﬁguration obtained by valid set are:γ = 1, m, n =
50, B = 200 and taking L2 as dissimilarity. For
both the two datasets, We traverse to training for
1000 rounds.
Experimental results on both WN18 and FB15k
are shown in Table 4.
From Table 4, we can
conclude that:
(1) TransD outperforms other
baseline embedding models (TransE, TransH and
TransR/CTransR), especially on sparse dataset,
i.e., FB15k; (2) Compared with CTransR, TransD
is a more ﬁne-grained model which considers the
multiple types of entities and relations simultaneously, and it achieves a better performance. It indicates that TransD handles complicated internal
correlations of entities and relations in knowledge
graphs better than CTransR; (3) The “bern” sampling trick can reduce false negative labels than
For the comparison of Hits@10 of different
kinds of relations, Table 5 shows the detailed
results by mapping properties of relations1 on
FB15k. From Table 5, we can see that TransD
outperforms TransE, TransH and TransR/CTransR
signiﬁcantly in both “unif” and “bern” settings.
TransD achieves better performance than CTransR
in all types of relations (1-to-1, 1-to-N, N-to-1 and
N-to-N). For N-to-N relations in predicting both
head and tail, our approach improves the Hits@10
by almost 7.4% than CTransR. In particular, for
1Mapping properties of relations follows the same rules in
 
Unstructured 
RESCAL 
SE 
SME (linear) 
SME (Bilinear) 
LFM 
TransE 
TransH (unif) 
TransH (bern) 
TransR (unif) 
TransR (bern) 
CTransR (unif) 
CTransR (bern) 
TransD (unif)
TransD (bern)
Table 4: Experimental results on link prediction.
Prediction Head (Hits@10)
Prediction Tail (Hits@10)
Relation Category
Unstructured 
SE 
SME (linear) 
SME (Bilinear) 
TransE 
TransH (unif) 
TransH (bern) 
TransR (unif) 
TransR (bern) 
CTransR (unif) 
CTransR (bern) 
TransD (unif)
TransD (bern)
Table 5: Experimental results on FB15K by mapping properities of relations (%).
N-to-1 relations (predicting head) and 1-to-N relations (predicting tail), TransD improves the accuracy by 9.0% and 14.7% compared with previous
state-of-the-art results, respectively.
Therefore,
the diversity of entities and relations in knowledge grahps is an important factor and the dynamic
mapping matrix is suitable for modeling knowledge graphs.
Properties of Projection Vectors
As mentioned in Section ”Introduction”, TransD
is based on the motivation that each mapping matrix is determined by entity-relation pair dynamically.
These mapping matrices are constructed
with projection vectors of entities and relations.
Here, we analysis the properties of projection vectors. We seek the similar objects (entities and relations) for a given object (entities and relations) by
projection vectors. As WN18 has the most entities (40,943 entities which contains various types
of words. FB13 also has many entities, but the
most are person’s names) and FB15k has the most
relations (1,345 relations), we show the similarity
of projection vectors on them. Table 6 and 7 show
that the same category objects have similar projection vectors. The similarity of projection vectors
of different types of entities and relations indicates
the rationality of our method.
Conclusions and Future Work
We introduced a model TransD that embed knowledge graphs into continues vector space for their
completion. TransD has less complexity and more
ﬂexibility than TransR/CTransR. When learning
embeddings of named symbol objects (entities or
relations), TransD considers the diversity of them
Extensive experiments show that TransD
outperforms TrasnE, TransH and TransR/CTransR
on two tasks including triplets classiﬁcation and
link prediction.
As shown in Triplets Classiﬁcation section, not
all new facts can be deduced from the exist-
Entities and Deﬁnitions
upset VB 4
cause to overturn from an upright or
normal position
srbija NN 1
a historical region in central and
northern Yugoslavia
Similar Entities and
Deﬁnitions
cause to move back and forth
montenegro NN 1
a former country bordering on the
Adriatic Sea
shift VB 2
change place or direction
constantina NN 1
a Romanian resort city on the Black
move with a thrashing motion
lappland NN 1
a region in northmost Europe inhabited by Lapps
ﬂuctuate VB 1
cause to ﬂuctuate or move in a wavelike pattern
plattensee NN 1
a large shallow lake in western Hungary
leaner NN 1
(horseshoes) the throw of a horseshoe so as to lean against (but not encircle) the stake
brasov NN 1
a city in central Romania in the
foothills of the Transylvanian Alps
Table 6: Entity projection vectors similarity (in descending order) computed on WN18. The similarity
scores are computed with cosine function.
/location/statistical region/rent50 2./measurement unit/dated money value/currency
Similar relations
/location/statistical region/rent50 3./measurement unit/dated money value/currency
/location/statistical region/rent50 1./measurement unit/dated money value/currency
/location/statistical region/rent50 4./measurement unit/dated money value/currency
/location/statistical region/rent50 0./measurement unit/dated money value/currency
/location/statistical region/gdp nominal./measurement unit/dated money value/currency
/sports/sports team/roster./soccer/football roster position/player
Similar relations
/soccer/football team/current roster./sports/sports team roster/player
/soccer/football team/current roster./soccer/football roster position/player
/sports/sports team/roster./sports/sports team roster/player
/basketball/basketball team/historical roster./sports/sports team roster/player
/sports/sports team/roster./basketball/basketball historical roster position/player
Table 7: Relation projection vectors similarity computed on FB15k. The similarity scores are computed
with cosine function.
ing triplets in knowledge graphs, such as relations gender, place of place, parents and children. These relations are difﬁcult to infer from all
other information, but they are also useful resource
for practical applications and incomplete, i.e. the
place of birth attribute is missing for 71% of all
people included in FreeBase .
One possible way to obtain these new triplets is
to extract facts from plain texts.
We will seek
methods to complete knowledge graphs with new
triplets whose entities and relations come from
plain texts.
Acknowledgments
This work was supported by the National Basic
Research Program of China (No. 2014CB340503)
and the National Natural Science Foundation of
China (No. 61272332 and No. 61202329).