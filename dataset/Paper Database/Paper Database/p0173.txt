Learning from History and Present: Next-item Recommendation
via Discriminatively Exploiting User Behaviors
Zhi Li1,2, Hongke Zhao1, Qi Liu1,∗, Zhenya Huang1, Tao Mei3, Enhong Chen1
1Anhui Province Key Laboratory of Big Data Analysis and Application, University of Science and Technology of China;
2School of Software Engineering, University of Science and Technology of China; 3JD AI Research
{zhili03,zhhk,huangzhy}@mail.ustc.edu.cn; {qiliuql,cheneh}@ustc.edu.cn; 
In the modern e-commerce, the behaviors of customers contain
rich information, e.g., consumption habits, the dynamics of preferences. Recently, session-based recommendations are becoming
popular to explore the temporal characteristics of customers’ interactive behaviors. However, existing works mainly exploit the
short-term behaviors without fully taking the customers’ longterm stable preferences and evolutions into account. In this paper,
we propose a novel Behavior-Intensive Neural Network (BINN)
for next-item recommendation by incorporating both users’ historical stable preferences and present consumption motivations.
Specifically, BINN contains two main components, i.e., Neural Item
Embedding, and Discriminative Behaviors Learning. Firstly, a novel
item embedding method based on user interactions is developed for
obtaining an unified representation for each item. Then, with the
embedded items and the interactive behaviors over item sequences,
BINN discriminatively learns the historical preferences and present
motivations of the target users. Thus, BINN could better perform
recommendations of the next items for the target users. Finally,
for evaluating the performances of BINN, we conduct extensive
experiments on two real-world datasets, i.e., Tianchi and JD. The
experimental results clearly demonstrate the effectiveness of BINN
compared with several state-of-the-art methods.
Next-item Recommendation, Sequential Behaviors, Item Embedding, Recurrent Neural Networks
INTRODUCTION
Recommender system, as an essential component of modern ecommerce websites, tries to predict what the most suitable products
or services are of users, based on the users’ preferences . With
the mechanism development of e-commerce, a massive amount of
customer interactions (e.g., browse, click, collect, cart, purchase)
have been logged, which imply luxuriant consumption patterns.
∗The corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from .
KDD’18, August 19–23, 2018, London, United Kingdom
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5552-0/18/08...$15.00
 
Session-based Recommender
Preference Behaviors
Session Behaviors
General Recommender
Figure 1: Recommending by integrating preference behaviors and session behaviors.
These information-rich logs provide opportunities for understanding customers’ historical stable preferences and also their present
consumption motivations, which may further contribute to smarter
recommendations.
Along this line, there is a particular interest in understanding interactive behaviors of customers. Existing works can be concluded
into two main paradigms. The first paradigm is the general recommenders. These works focus on mining the static relevancy between
users and items from interactions, which are represented by the
traditional collaborative filtering models . For example,
Zhang et al. made recommendations through a factorization model
with different item semantic representations from the knowledge
base . However, most of these works have taken user-item
specific relationships into consideration from the static views but
neglect the dynamics and evolutions of users’ preferences implied
in sequential interactions. The other paradigm is recommending
next items based on sequential pattern mining or transition
modeling . Along this line, researchers recently show more
interest in an e-commerce scenario where user profiles are invisible
so that recommender systems are developed based on the user interactions in short sessions . These session-based models
have provided the comprehension about users’ decision-making
process in a short term, but the dynamics of preferences and
how to perfectly integrate both the historical stable preferences
with present consumption motivations are still largely unexplored.
Actually, as a user’s interactive behaviors naturally form a behavioral sequence over time, the user’s historical preferences from
the long-term view and present motivations or demands from the
short-term view can be dynamically revealed. For instance, Figure 1
illustrates a typical online shopping scenario. The user’s historical
interactions imply that this user might be a “Star Wars” fan since
 
KDD’18, August 19–23, 2018, London, United Kingdom
KDD 2018 Research Paper
that the user has bought or collected various spin-off products of the
“Star Wars”. Moreover, we infer that this user would like to buy dark
T-shirts because a black shirt is included in the personal cart and the
user has browsed many short sleeve shirts. However, following the
general collaborative filtering approaches, another spin-off product
may be recommended since all the preference behaviors of the
user’s entire history are exploited in the static manners as shown
in the blue chart of Figure 1. By contraries, if we only consider the
current session behaviors of this user in accordance with what the
session-based models do, another similar or popular shirt would be
recommended as shown in the green chart of Figure 1. Actually, by
exploiting both the historical stable preferences and present consumption motivations of this user, more attention should be paid
to short sleeve shirts and the “Star Wars” graphic T-shirts perfectly
match user’s tastes. Therefore, we can conclude that a smarter recommender system should not only consider users’ historical stable
preferences but also take into account the present consumption
motivations by discriminatively exploiting different terms or types
of user behaviors.
Based on the intuition and observations, we propose a novel solution framework called Behavior-Intensive Neural Network (BINN)
to address the next-item recommendation problem. Our BINN
framework contains two main components: Neural Item Embedding and Discriminative Behaviors Learning. Specially, we propose a
novel neural item embedding method to obtain a unified item representation space for learning latent vectors which could capture the
sequential similarities between items. Different from the traditional
item embedding methods which are based on inherent features
such as item images or text descriptions, our neural item embedding method generates item representations by means of exploiting users’ collaborative sequential interactions over items directly.
Then, with the item embedded, we design two different behavior
alignments, i.e., Session Behaviors Learning and Preference Behaviors
Learning to respectively model users’ present consumption motivations and historical stable preferences by discriminatively exploring
interactive behaviors of users. Specific to the alignments, we respectively develop two deep neural network architectures to jointly
learn the session behaviors and preference behaviors. Finally, by
matching the potentially preferred items in the latent space, BINN
generates recommendations for the target users. For evaluating the
performances of BINN, we conduct extensive experiments on two
real-world datasets. The experimental results clearly demonstrate
the effectiveness of BINN compared with several state-of-the-art
methods. In summary, the main contributions of this study can be
summarized as follows.
• We propose to make item-recommendation by integrating both
the historical preferences and present motivations of users, which
are all learned from the users’ interactive behaviors.
• We propose a novel Behavior-Intensive Neural Network (BINN)
which includes embedding items by users’ interactions and discriminative behavior alignments accompanied by two applicable
neural network architectures.
• We conduct extensive experiments on two real-world datasets.
The results show that BINN model outperforms other state-ofthe-art methods from various aspects.
RELATED WORKS
Recommender systems play an essential role in many Internetbased services , such as e-commerces, and have arouse the
great attention from both industry and academia. The relevant
works of this study can be concluded into two main paradigms: the
General Recommenders and the Sequential Recommenders.
General Recommenders
Most of the general recommenders focus on mining the static relevancy between users and items from their interactions, which are
represented by the traditional collaborative filtering models . More specifically, the most common approaches of collaborative filtering are the neighborhood methods and
the factorization models .
Neighborhood Methods are based on the similarities of entities (always users or items) and recommend the nearest neighbor
items through the precomputed similarities . For example, Bell et al. proposed a new method to simultaneously derive
interpolation weights for all nearest neighbors, leading to a substantial improvement of prediction accuracy . Zhao et al. considered
the expertise of investors to improve the neighborhood methods for
the personalized investment recommendations in P2P lending .
Wang et al. presented a basic probabilistic framework which formalized the learning similarity as a regression problem . Moreover,
they introduced a novel multi-layer similarity descriptor which
modeled the joint influences of different features to improve the
neighborhood methods .
Factorization Models treat the recommendation as a user-item
matrix reconstruction problem and model the user-item interactions by dot product of latent vectors . Many previous
works focused on better representing users or items to improve the
qualities of recommender systems. For example, Liu et al. considered
users’ indecisiveness when the customers chose among competing
product options and then proposed IMF method to mine the indecisiveness in customer behaviors to improve the performance of
recommendation . Zhao et al. introduced the Nash Equilibrium
into the matrix factorization method to solve the group recommendation problem . In recent years, deep learning has also been
applied successfully to the classical collaborative filtering user-item
matrix reconstruction problems from different perspectives. For
example, many researches incorporated deep learning models for
extracting and fusing side features, such as image features ,
text information and multimode data . Moreover, some
works developed the deep neural networks to learn the relationships between the users and items .
However, most of these works provide recommendations by mining the static relevancy between users and items. The dynamics and
evolutions of users’ preferences, and also their present consumption
motivations are usually not given special attentions.
Sequential Recommenders
In recent years, researchers start to focus on various sequential
recommendation scenarios, such as next-basket , sessionbased and next-item recommendations . Among
them, session-based ones are increasingly attractive.
KDD 2018 Research Paper
KDD’18, August 19–23, 2018, London, United Kingdom
Early works on sequential recommenders are almost based on
the sequential pattern mining and the transition modeling . Due to the tremendous success of deep neural networks
in the past few years, approaches to sequence data modeling have
made significant strides and benefit a broad range of applications,
such as NLP , social media and recommendations .
For example, Hidasi et al. firstly applied recurrent neural networks
(RNNs) by modeling the whole session for session-based recommendations, which outperformed item-based methods significantly .
Quadrana et al. focused on some session-based recommendation
scenarios where user profiles were readily available and developed
a hierarchical recurrent neural networks with cross-session information transfer . Wang et al. used a hierarchical representation
model to capture both sequential behaviors and users’ general tastes
by involving transaction and user representations in next-basket
prediction . Donkers et al. extended RNNs by representing the
individual users for the purpose of generating personalized next
item recommendations .
Although these models have taken the users’ sequential information into consideration, there are still largely unexploited in the
coherence of customers’ sequential behaviors and the dynamics
of historical preferences. Comparatively, in this paper, we propose
to make item-recommendation by integrating both the historical
preferences and the present motivations of users, which are all
learned from the users’ rich interactive behaviors over items.
BINN: BEHAVIOR-INTENSIVE NEURAL
In this section, we introduce our proposed model for addressing the
personalized next-item recommendations. We first formally define
the next-item recommendation task and overview the proposed
Behavior-Intensive Neural Network (BINN). Then we describe the
two main components of BINN in detail, i.e., Neural Item Embedding
and Discriminative Behaviors Learning.
Preliminaries
Next-item recommendation is the task of predicting what a user
would like to access next based on her historical interactions. Here,
we give a formulation for the next-item recommendation.
As user interactions naturally form a sequence over time, a log
history H of the information system is a set of sequential interactions, i.e., H = {S1,S2, ...,Sn}, where |H| = n denotes the number of
users. Each useru has a corresponding interaction sequence Su ∈H
which can be represented as Su = {(x1,b1), (x2,b2), ..., (xT ,bT )},
where xj denotes the j-th item that user u operates and bj denotes
the behavior type (e.g., click, cart, purchase). Then, the personalized
next-item recommendation task can be defined as follow:
Definition 1 (Personalized Next-item Recommendation).
Given a target user u with her sequential of interactive behaviors
over items Su = {(x1,b1), (x2,b2), ..., (xT ,bT )} and also all users’
sequential interactions H, the personalized next-item recommendation task is to predict item xT +1 that the target user u is most likely
to access in her next visit.
In this paper, we address this task with a novel personalized nextitem recommendation framework, i.e., Behavior-Intensive Neural
Network (BINN). As shown in Figure 2, BINN contains two main
components: Neural Item Embedding and Discriminative Behaviors
Learning. Specifically, with a neural embedding model, we obtain
a unified item representation space for learning the latent vectors
that capture sequential similarities between items. Then we design
two interactive behaviors assignments, named Session Behaviors
Learning (SBL) and Preference Behaviors learning (PBL), to exploit
the users’ present consumption motivations and historical stable
preferences over time. Finally, we jointly learn these two interactive alignments on the latent space of item representations and
recommend top-k potentially preferred items to each target user.
Neural Item Embedding
In the first stage of BINN, Neural Item Embedding aims to generate a
unified representation for each item by learning the item similarities
from a large number of sequential behaviors over items. Previous
works of sequential recommenders always use 1-of-N encoding or
add an additional embedding layer in the deep learning architecture
to represent items . However, for a superb collection of items
in the big e-commerce platforms, on one hand, the 1-of-N encoding
networks may cost unaffordable time and always cannot to be
optimized well because of the high sparsity . On the other hand,
adding an additional embedding layer may make networks lose
performances to some extend . What’s more, both two methods
cannot reveal item sequential similarities which is implied in the
user interactions. In this case, it is necessary to find an effective
representation method to directly learn high-quality item vectors
from the users’ interaction sequences, with the result that items
implied similar attractions tend to be close to each other.
In recent years, the progress in neural embedding has achieved
tremendous advances in many domains, such as NLP , social
networks and recommendations . Among these works,
item2vec is one of the significant extensions of Skip-gram with
Negative Sampling to produce item embedding for item-based
collaborative filtering recommendations.
In this paper, we propose an improved item2vec to capture item
similarities and generate item representations by the means of
exploiting users’ collaborative interactions over items directly. Different from the words in sentences, some items in user interactions
have often been frequently accessed. The reason for this phenomenon is that users are usually indecisive in their decision-making
process , causing a lot of repetitive actions on the same item.
In addition, these frequently-operated items also indicate users’
main motivations, and other items interspersed these repeats may
be very similar or competing to these items. On the other hand,
items with low frequency may be aimlessly clicked , which will
bring noise to the item embedding. Along this line, we take one
step further to capture the characteristic of interaction sequences
and propose a novel item embedding method, called w-item2vec,
which considers the frequency of items as a weighted factor.
Inspired by item2vec , w-item2vec also uses a Skip-gram
model with Negative Sampling method . Specifically, given
an item sequence S
u = {x1,x2, ...,xN } of user u from the interactive sequence Su, the Skip-gram of w-item2vec aims at maximizing
the following objective:
KDD’18, August 19–23, 2018, London, United Kingdom
KDD 2018 Research Paper
A. Neural Item Embedding
Predicting
B. Discriminative Behaviors Learning
Interaction Sequence
Session Behaviors Learning
Preference Behaviors Learning
w-item2vec
Figure 2: The overview of Behavior-Intensive Neural Network (BINN). A. Neural Item Embedding converts sequential items
into a unified embedding space by w-item2vec. B. Discriminative Behaviors Learning constructs two alignments of user behaviors and discriminatively learns behavior information based on two LSTM-based architectures.
arg max opt = 1
log p(xj |xi),
where K is the length of sequence Su, and p(xj |xi) is defined as the
softmax function:
p(xj |xi) =
where wi and vi are the latent vectors that correspond to the target
and context representations for item xi. For alleviating computational complexity of the gradient ▽(xj |xi), Eq. (2) is always replaced
by negative sampling:
p(xj |xi) = σ(wT
where σ(x) = 1/(1 +exp(−x)), E is the number of negative samples
to be drawn per a positive sample.
Then, we improve the negative sampling model of Eq. (3)1 by
considering the item frequency within interaction sequences as the
weight of negative sampling process:
p(xj |xi) = (σΘ(xi)(wT
i ∗vk))Θ(xi),
where Θ(xi) is the frequency of item xi in the sequence. Consequently, the Skip-gram objective in Eq. (1) can be redefined as:
1There is a mistake in the version of ACM Digital Library, we correct it here.
arg max opt = 1
log p(xj |xi)
Θ(xi)(Θ(xi) ∗log σ(wT
Finally, we train the w-item2vec by gradient descent, and obtain
high-quality distributed vector representations for all the items.
With w-item2vec, we can capture item similarities with the help
of user interactions and generate an unified item representation
space, in which the representation vectors can reveal similarities
and sequential relationships of items. And for each user u, we
can generate an interaction sequence with embedding items as
Pu = {v1,v2, ...,vT }, where vj denotes the d-dimensions latent
vector of item xj.
Discriminative Behaviors Learning
After obtaining item embeddings, Discriminative Behaviors Learning (DBL) could explore sequential behaviors as prior knowledge
to recommend items that the target user is most likely to access in
her next visit.
As illustrated in Figure 1, a user’s decision-making process is
mainly influenced by two factors: her present motivations and historical preferences. More specifically, users’ present consumption
motivations are dynamic in a short term and the recent fluctuations are also important to reflect the short-term characteristics.
Considering that all the recent behaviors (e.g., click, collect, cart,
purchase) may imply users’ present motivations in a short term,
we use all types of recent behaviors to represent the present consumption motivations. On the other hand, as for exploiting users’
KDD 2018 Research Paper
KDD’18, August 19–23, 2018, London, United Kingdom
historical preferences, not all types of behaviors could depict users’
preferences. For example, we can imply that the user do not prefer
an item if she just clicks on it without purchasing at last. Therefore, for modeling users’ historical preferences, we only remain the
behaviors which can clearly depict users’ underlying preferences
from interaction histories, i.e., purchase behaviors.
In fact, the interaction process of a user is the series of implicit
feedbacks over time. Thus, different from traditional recommender
systems which explore the user-item interactions from a static
manner, we tackle the next-item recommendation by sequential
modeling. Specifically, we design two discriminative behavior alignments: Session Behaviors Learning (SBL) and Preference Behaviors
Learning (PBL) to discriminatively learn users’ present consumption motivations and historical stable preferences. Further, on this
basis, we develop two separate deep neural architectures based on
LSTM to jointly learn the motivations and preferences from
these two alignments of behaviors.
Session Behaviors Learning. As illustrated in the green
chart of Figure 1, the session behaviors in a short term can reveal
the users’ present consumption motivations. The Session Behaviors
Learning (SBL) is to model the short-term session behaviors of the
target user u. More formally, suppose we already have the previous
interaction sequence Su = {(x1,b1), (x2,b2), ..., (xt−1,bt−1)} with
embedding items Pu = {v1,v2, ...,vt−1}. The behavior bi can be
represented as an one-hot vector and the length of the vector is the
number of interaction types, e.g., click. For determining whether a
certain item xi would be a possible element of the session behaviors,
the SBL discrimination function DSBL can be defined as follows:
DSBL(xi,xt ) = Φ((t −i) ≤ts),
where function Φ(a) is to compute a discrimination signal that
equals to 1 if a is true and equals to 0 otherwise, xt−1 is the previous
item of the prediction and ts is a controlling indicator to control
the length of SBL. Specifically, as for a session-based scenario, ts is
the length of the session. And for the non-session scenarios, ts is
artificially specified. In this paper, we set ts to 10 as the default.
Aiming at the alignment of session behaviors, we develop a
Contextual LSTM (CLSTM) to learn users’ present consumption
motivations. After the initialization, at j-th interaction step, the
hidden statehj of each interaction is updated by the previous hidden
state hj−1, the current item embedding vj, and the current behavior
vector bj as:
ij = δ(Wvivj +Whihj−1 +Wcicj−1 +Wbibj + bbi),
fj = δ(Wvf vj +Whf hj−1 +Wcf cj−1 +Wbf bj + bbf ),
cj = fjcj−1 + ijtanh(Wvcvj +Whchj−1 +Wbcbj + bbc),
oj = δ(Wvovj +Whohj−1 +Wcocj +Wbobj + bbo),
hj = ojtanh(cj),
where ij, fj and oj are the input gate, forget gate and output gate
at j-th step respectively, vj is the embedding item vector, bj is the
behavior vector, cj is the cell memory, bb is the bias term, and hj is
the output at j-th step.
We essentially use the final out state ht−1 as the representation
of the present consumption motivations of user u, i.e., ΨSBL =
ht−1. With the above network structures, SBL can naturally model
fluctuations of users’ session behaviors to obtain representations
of present consumption motivations.
Preference Behaviors Learning. As mentioned above, a
smarter recommender system should not only consider users’ present
consumption motivations but also take into account the historical
stable preferences. Therefore, in addition to exploiting motivations
by SBL in a short term, PBL is used to learn users’ stable historical
preferences from the preference behaviors in a long term. Actually,
only part of behaviors imply users’ preferences. Thus, for determining whether a certain interaction (vi,bi) ∈Su would be a possible
element of the preference behaviors, the discrimination DPBL can
be defined as:
DPBL(vi,bi) = Φ(bi ∈P),
where P is the preference behavior set which contains the types of
preference behaviors i.e., collect, cart and purchase.
Different from SBL, PBL is a global representation of historical
preferences with less fluctuations. That may make the architecture
of SBL can not work well to obtain users’ historical preferences.
Inspired by Bidirectional RNN , we adapt the CLSTM to a bidirectional architecture, named Bi-CLSTM, to make full use of the
contextual long-term representation in both forward and backward
directions. Specifically, the cell of Bi-CLSTM is the same as Eq. (7)
and it can principally be trained with the same algorithms as a
regular unidirectional CLSTM. At each time step s of PBL, the
forward layer with hidden state
hPs is computed based on both
the previous hidden state
s−1 and the current item-behavior pair
(vis,bis); while the backward layer updates hidden state
the future state
s+1 and the current item-behavior pair (vis,bis).
Therefore, each PBL hidden representation hPs can be calculated
with the concatenation of the forward state and backward state, i.e.,
hPs = concatenate(
After that, we can generate the unified representation of preference behaviors ΨPBL for user u through an average pooling layer:
ΨPBL = averaдe(hP
2 , ...,hP
Particularly, taking embedding preference interactions as inputs
of above networks, PBL is able to learn and depict the profile of
each user. That can help BINN to make a good understanding of
users’ historical stable preferences.
So far, from Discriminative Behaviors Learning (DBL), we have
modeled two behavior alignments: session behaviors learning ΨSBL
and preference behaviors learning ΨPBL. Then, after an fully connected layer, we can generate the d-dimensions representation bvt
of the next item.
Model Learning and Test Stage
Taking embeddings of sequential items as inputs of networks, DBL
is able to learn both users’ present motivations and historical preferences by controlling recurrent states update of the two network
KDD’18, August 19–23, 2018, London, United Kingdom
KDD 2018 Research Paper
Training Set
Sequential Interaction
Cold Starts
Figure 3: Dataset divided with a cut time.
architectures. After DBL, we can generate the prediction of next
item representation bvt , which is ad-dimensions vector. In the global
learning stage, we use Mean Squared Error (MSE) loss function
to learn two behavior alignments jointly from the whole set of
sequential interactions H, which can be defined as:
(|Su | −ts −1)
ζ (bvt,vt ),
whereζ is MSE function,vt is the item representation that the target
user u is access in the next visit, ts is the controlling indicator, |Su |
denotes the length of the interaction sequence Su ∈H and |H| is
the number of users.
The Eq. (10) is minimized using Adagrad optimization . More
details of settings will be specified in experiments.
So far, we have discussed the whole training stage of BINN. After
obtaining the trained BINN model, in the testing stage, given an individual interaction history Su = {(x1,b1), (x2,b2), ..., (xt−1,bt−1)},
we could predict item xt that user u is most likely to access in next
visit by the following steps: (1) apply model BINN to fit user interaction process Su to get the user’s states ΨSBL and ΨPBL at t −1
step for prediction; (2) generate the next-item embedding vector
bvt and compute the similarities to all the item candidates in the
latent space which we have obtained in Section 3.2; (3) then we
can recommend the top-k potentially preferred items in the unified
representation space.
EXPERIMENTS
In this section, we first describe the experimental setups. Then,
we demonstrate the effectiveness of proposed framework from
the following aspects: (1) the visualization of embedding comparisons between w-item2vec in BINN and traditional item2vec, (2)
the comparisons of overall recommendation performances, (3) the
analysis on cold-start scenarios and (4) parameter sensitiveness of
user interactions.
Specifically, we conduct experiments on two real-world datasets,
i.e., Tianchi dataset and JD dataset.
Table 1: Statistics of datasets after preprocessing.
Statistics
# of users
# of items
# of behaviors
37,061,992
# of behavior types
Avg. behaviors per user
Avg. behaviors per item
# of behaviors in training set
31,811,364
# of behaviors in test set
• Tianchi2 is a public dataset by Alibaba’s competition of Ali
Mobile Recommendation Algorithm, which is based on the real
users-commodities behavior data on Alibaba’s M-Commerce platforms. It provides 23,291,027 interactions of 20,000 customers on
4,758,484 items within a month. In this dataset, customer behaviors include click, collect, cart and purchase, the corresponding
values are 1, 2, 3 and 4, respectively.
• JD is provided by a Chinese e-commerce company Jingdong3,
which is one of the top two largest B2C online retailers in China
by transaction volume and revenue. Specially, it provides 37,087,895
interactions of 105,180 customers on 28,710 items within 75 days
based on the real log data of users-commodities behaviors. In this
dataset, customer behaviors include browse, cart, delete-to-cart,
purchase, collect and click with the corresponding values 1, 2, 3,
4, 5 and 6, respectively.
For the reliability of experimental results, we make the necessary
preprocessing as follows. First, we filter the users whose interaction
lengths are less than 10 and items that appear less than 5 times.
Then we respectively divide two datasets into training sets and test
sets according to cut time, where 90% interactions are chosen into
the training set and the remaining interactions are used for testing.
Figure 3 illustrates strategy of dataset partitioning. In particular, for
Tianchi dataset, we use 27 days data for training and the rest 3 days
as test set while for JD dataset, we use 68 days data for training and
the rest for test. Considering that collaborative filtering methods
can not recommend an item which has not appeared before ,
we filter out interactions from test set where items do not appear
in the training set. In the same way, we also remove users from test
set who do not appear in the training set, but we take special use
of this part for analysis on the cold-start scenarios. The statistics of
two datasets after preprocessing are shown in Table 1.
Baseline Methods
We compare BINN with three traditional methods (i.e., S-POP, BPR-
MF, Item-KNN) and two state-of-the-art RNN-based models (i.e.,
GRU4Rec, HRNN) each of which contains two specific implements
(i.e., GRU4Rec, GRU4Rec Concat and HRNN Init, HRNN All).
• S-POP recommends the item with the largest number of interactions by the target user. This method works well in the context
with high repetitiveness, and the recommendation list changes
along with user interactions.
2 
933N6Rr&raceId=231522
3 
KDD 2018 Research Paper
KDD’18, August 19–23, 2018, London, United Kingdom
(a) W-item2vec.
(b) Item2vec.
Figure 4: T-SNE embedding for item vectors produced by witem2vec (a), item2vec (b) on Tianchi dataset. The items are
colored according to the categories.
• BPR-MF is one of widely used matrix factorization methods, which optimizes a pairwise ranking objective function via
stochastic gradient descent.
• Item-KNN selects the items which are similar to the previously accessed items to users.
• GRU4Rec uses the basic GRU with a TOP1 loss function
and session-parallel minibatching.
• GRU4Rec Concat is similar with GRU4Rec. Differently,
we do not use the session-partition, and the users’ interaction
sequences are fed to the GRU4Rec independently as a whole.
• HRNN Init is a hierarchical RNN for personalized crosssession recommendations, which is based on GRU4Rec and adds
an additional GRU layer to model information across the user’s
sessions for tracking the evolution of the user’s interest. It is a
state-of-the-art method in next-item recommendations.
• HRNN All is similar with HRNN, but the user representation generated by an additional GRU layer is used for initialization
and propagated in input as each step of the next session.
For fair comparisons, we set all the hidden units in the RNNbased models as 100, their dropout probabilities and learning rate as
0.1. The embedding vector for each item is 64-dimensional in BINN
model. The BINN and all the compared methods are defined and
trained on a Linux server with two 2.20 GHz Intel Xeon E5-2650 v4
CPUs and four Tesla K80 GPUs.
Evaluation Metrics
As recommender systems can suggest few items each time, and the
relevant items should be ranked first in the recommendation list.
We therefore evaluate the personalized next-item recommendation
quality with the following two evaluation metrics.
• Recall@20. It is the primary evaluation metric that is the proportion of cases having the desired item amongst the top-20 items
in all test cases . Note that Recall@20 does not discriminate between items with different rankings as long as they are
amongst the recommended list. In other word, the rank of items
in top-20 candidate set do not make a difference.
• MRR@20. Another used metric is Mean Reciprocal Rank (MRR),
which is the average of reciprocal ranks of the desire items. The
same with Recall metric, we set 20 as contributing value, that
(a) W-item2vec.
(b) Item2vec.
Figure 5: T-SNE embedding for item vectors produced by witem2vec (a), item2vec (b) on JD dataset. The items are colored according to the categories.
means the reciprocal rank is set to zero if the rank is above 20 . Considering the the order of recommendations matters, MRR
takes the rank of each recommended item into consideration.
In summary, the higher both two evaluate metrics are, the better
performances the results have.
Experimental Results
We first visualize the embedding of our proposed w-item2vec competing with item2vec, and then we show performances on next-item
recommendation task. Finally, we discuss the cold-start problem of
new users in recommendations and we analyze influences of the
interaction lengths.
Item Embedding Visualization. We apply w-item2vec for
generating the item embedding vj for each item xj from Eq. (5), in
which item similarities and sequential-behavior relationships over
items can be revealed simultaneously. We run the algorithm for 10
epochs and set the negative sampling value E = 10 and compare
our method with item2vec on both datasets. We apply the same
settings for them.
Since an effective representation method can make the items
implied similar attractions tend to be close to each other, we use
the item categories to visualize whether the latent representations
can reveal the similarities of the items. This is motivated by the
assumption that a useful representation would cluster similar items
in accordance with their category. To this end, we generate embeddings for 3,000 items which are randomly selected from three
categories. We apply t-SNE with a squared euclidean kernel
to reduce the dimensionality of item embedding vectors to 2. Then
we color each item point according to its category.
Figure 4 and 5 present the 2D embedding that are produced by
t-SNE, for w-item2vec and item2vec, respectively. As we can see,
w-item2vec provides a significantly better clustering on Tianchi
and also shows better performance than item2vec on JD since the
clustering boundaries in Figure 4(a) and Figure 5(a) are more clear.
One possible reason is that w-item2vec takes account of the item
frequencies, which makes w-item2vec can generate better representation of items than item2vec. Interestingly, both two methods have
shown remarkable results on JD dataset, one possible explanation
could be that JD dataset is more dense. As shown in Table 1, JD
dataset log much more behavioral interactions on a smaller amount
KDD’18, August 19–23, 2018, London, United Kingdom
KDD 2018 Research Paper
Table 2: Performance comparisons of BINN with baseline methods on two datasets (The improvements of RNN-based models
over the best traditional method have been marked).
0.2025(-10.48%)
0.0861(-2.49%)
0.7034(+20.16%)
0.4198(+92.92%)
GRU4Rec Concat
0.2287(+1.11%)
0.0859(-2.72%)
0.7934(+35.53%)
0.5932(+172.61%)
0.2305(+1.9%)
0.0897(+1.59%)
0.8073(+37.91%)
0.6098(+180.23%)
0.2167(-4.20%)
0.0893(+1.13%)
0.7762(+32.59%)
0.4335(+99.22%)
0.2376(+5.04%)
0.0936(+6.00%)
0.8430(+44.00%)
0.7082(+225.46%)
(a) Recall@20 on Tianchi.
(b) MRR@20 on Tianchi.
(c) Recall@20 on JD.
(d) MRR@20 on JD.
Figure 6: Recommendation performances of new users cold-start on two datasets.
of items and the average behaviors per item is 1,497.82, which is
much larger than Tianchi dataset 13.05 behaviors per item. That
makes the model could be trained better. We further observe some
outlier items that because many items on either dataset in the same
category are not similar to each others.
Recommendation Performances. To demonstrate the practical significance of our proposed model, we compare BINN with
the other methods on the next-item recommendation task. The
results of all methods on both Tianchi and JD datasets are shown
in Table 2. For the convenience of comparing differences between
traditional and RNN-based models, we highlight the improvements
of RNN-based models over the best traditional method. From the
overall views, our BINN model has achieved the best performances
on both two datasets.
Firstly, for the results on Tianchi dataset, we have some interesting observations. BINN performs significantly better than all
the other methods on both Recall@20 and MRR@20. The result
indicates that BINN framework is good at dealing with personalized
sequential information from the user interactions. Comparing with
the RNN-based models, we can note that the traditional methods
provide more competitive results. We guess a possible reason is that
users’ interactions on Tianchi have a high degree of repetitiveness
and this dateset has a large amount of item candidates when making
recommendations. That fact makes the generation of “non-trivial”
personalized recommendations (i.e., P-POP) very challenging .
The comparison among the five RNN-based models highlights the
effectiveness to track customers’ long-term preferences for next
item recommendations, because models with consideration of personalized information (i.e., BINN, HRNN Init, HRNN All) outperform those methods without that (i.e., GRU4Rec, GRU4Rec Concat)
on MRR@20.
Next, we turn to the experiments on JD dataset, which exhibits
some different results from those on Tianchi dataset. All the RNNbased models (i.e., GRU, HRNN and BINN) significantly outperform
the traditional methods, which indicates that RNN-based models do
have better abilities to model users’ sequential interactions for nextitem recommendations than traditional methods. In addition, the
non-personalized RNN-based models GRU4Rec Concat outperforms
HRNN All, which indicates that improper personalizing strategy
might even make the recommendation performances worse and
reveals the importance of the community trends from the shortterm sequential behaviors.
Then, we notice that RNN-based models perform much better
on JD dataset than that on Tianchi dataset. One possible explanation could be that JD has more interactions of more users but less
merchandises than Tianchi, and that may be a strong sequential
recommendation scenario. Moreover, the number of users is much
larger than the amount of items on JD dataset (i.e., the statistic of
items on Tianchi dataset is 674,326, but 24,744 on JD dataset), that
naturally makes the predicting candidate set smaller, and therefore,
leads to more accurate result. Actually, this scenario is much common in the real world, such as online retailers and B2C platforms.
In summary, BINN achieves the best performances on both two
datasets, followed by HRNN Init. Both two methods take user-level
representations into account. That clearly demonstrates users’ interactive behaviors may follow general short-term community trends
KDD 2018 Research Paper
KDD’18, August 19–23, 2018, London, United Kingdom
(a) Recall@20 on Tianchi.
(b) MRR@20 on Tianchi.
(c) Recall@20 on JD.
(d) MRR@20 on JD.
Figure 7: Results of next-item recommendation over different history lengths.
and reveal stable long-term preferences. Our BINN discriminatively
models the users’ historical preferences and present motivations,
that leads to superior personalized recommendation quality.
Cold Start of New Users. Cold start is a common problem
of recommender systems that new users or items have not yet gathered sufficient information to recommend or be recommended .
As we have removed users from test set that are not in the training
set on the above experiments, which is shown in Figure 3 shown,
here we focus on these users and examine the performances of our
model on cold-start problem of new users.
Indeed, new users have no interactions to be pretrained and
recommender systems cannot generate user profiles. That makes
many user profile-based recommendation methods cannot work
well, especially factorization models. However, for the RNN-based
next-item recommendations, we can use a trained neural network
to fit new users and predict from their second interactions one-byone and check item rank of the next interaction. Here, we test the
recommendation results on fifty items from the beginning of the
second ones in the interactions of the target new users. Please note
that, we do not change any training process and just select coldstart users for testing, thus all the testing do not need retraining. For
better illustration, we report the results of all RNN-based models,
using both metrics, respectively.
The results are shown as Figure 6. In most cases, BINN performs
better than the other models on both datasets. At the beginning of
the user interactions, our model BINN has deteriorated to CLSTM
because of the absence of personalized preferences. Then, with
the number of users’ interactions growing, our model shows great
improvement on recommendation performances. That can indicate
the effectiveness of modeling the long-term historical preferences
in BINN. What’s more, all the deep learning models have shown
strong capacity to face the cold-start challenges of new users. Thus,
we can conclude that all the RNN-based models can work well for
new users. Consequently, the results indicate the effectiveness of
BINN structure.
Analysis on the User History Length. Here, we take further
analysis on performances of our BINN model and other RNN-based
models. This allows to evaluate personalized recommendation methods under different amounts of historical information and reveal
the capacity of users’ long-term preference representations. Since
we argue the length of the user history has an impact on the recommender system performances, we divide the evaluation by the
length of user interactions. Specially, we use the both datasets to
make the analysis and partition users into three groups: the historical interactions less than 300, between 300 and 500, and more than
500. On account of our purpose for measuring the impact of the
complex long-term preference dynamics used in BINN and other
RNN-based models, we respectively record performances on these
three user groups.
Figure 7 shows the performances on both datasets. Firstly, we pay
attention to improvements over the length of user behaviors grows
on Tianchi dataset. We can notice that our proposed BINN has
a significantly improvement on Recall@20 as the history lengths
grow. Then, we turn to the results on JD dataset. For users with
largest amount of interactions, our proposed BINN performs best
with at least 3.92% improvement compared to other RNN-based
models. Interestingly, BINN and both two HRNN models have an
improvement with the history length growing, but GRU4Rec and
GRU4Rec Concat do not show continuous improvement between
300-500 and larger than 500 scales. In summary, the length of the
user interactions does have an impact on the performances of recommender systems. These results clearly demonstrate the effectiveness of exploiting personalized strategies, i.e., users’ historical
stable preferences, to improve the recommendation performances.
CONCLUSIONS AND FUTURE WORKS
In this paper, we proposed a novel solution framework, the Behavior-
Intensive Neural Network, to address the problem of personalized
next-item recommendations. As a user’s behaviors naturally form
a interactive sequence over time, the user’s historical preferences
from the long-term view and present motivations from the shortterm view can be dynamically revealed. Along this line, we first
introduced a w-item2vec method to generate item representations
by considering the sequential similarities of the superb items. Then
we discriminatively exploited user behaviors and proposed two
alignments of the behaviors. Specific to each alignment, we respectively developed LSTM-based neural networks to learn personal historical preferences and present consumption motivations. Finally,
we conducted extensive experiments on two industrial datasets.
The experimental results clearly demonstrated the effective of our
proposed model in personalized next-item recommendations.
In the future, we plan to study the impact of different types of
user behaviors to generate user representations and improve the
next-item recommendation even further. We also plan to investigate
our model in other domain, such as advertisements.
KDD’18, August 19–23, 2018, London, United Kingdom
KDD 2018 Research Paper
ACKNOWLEDGMENTS
This research was partially supported by grants from the National Key
Research and Development Program of China (No. 2016YFB1000904),
and the National Natural Science Foundation of China (No.s 61672483
and U1605251). Qi Liu gratefully acknowledges the support of the
Young Elite Scientist Sponsorship Program of CAST and the Youth
Innovation Promotion Association of CAS (No. 2014299).