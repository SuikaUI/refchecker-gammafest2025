S3-Rec: Self-Supervised Learning for Sequential
Recommendation with Mutual Information Maximization
Kun Zhou1†, Hui Wang1†, Wayne Xin Zhao2,3∗, Yutao Zhu5, Sirui Wang4,
Fuzheng Zhang4, Zhongyuan Wang4 and Ji-Rong Wen2,3
1School of Information, Renmin University of China
2Gaoling School of Artificial Intelligence, Renmin University of China
3Beijing Key Laboratory of Big Data Management and Analysis Methods
4Meituan-Dianping Group
5UniversitÃľ de MontrÃľal, MontrÃľal, QuÃľbec, Canada
 , {hui.wang, batmanfly, jrwen}@ruc.edu.cn, ,
 , 
Recently, significant progress has been made in sequential recommendation with deep learning. Existing neural sequential recommendation models usually rely on the item prediction loss to learn
model parameters or data representations. However, the model
trained with this loss is prone to suffer from data sparsity problem.
Since it overemphasizes the final performance, the association or
fusion between context data and sequence data has not been well
captured and utilized for sequential recommendation.
To tackle this problem, we propose the model S3-Rec, which
stands for Self-Supervised learning for Sequential Recommendation,
based on the self-attentive neural architecture. The main idea of
our approach is to utilize the intrinsic data correlation to derive
self-supervision signals and enhance the data representations via
pre-training methods for improving sequential recommendation.
For our task, we devise four auxiliary self-supervised objectives
to learn the correlations among attribute, item, subsequence, and
sequence by utilizing the mutual information maximization (MIM)
principle. MIM provides a unified way to characterize the correlation between different types of data, which is particularly suitable
in our scenario. Extensive experiments conducted on six real-world
datasets demonstrate the superiority of our proposed method over
existing state-of-the-art methods, especially when only limited
training data is available. Besides, we extend our self-supervised
learning method to other recommendation models, which also improve their performance.
CCS CONCEPTS
• Information systems →Recommender systems.
†Equal contribution.
∗Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from .
CIKM ’20, October 19–23, 2020, Virtual Event, Ireland
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6859-9/20/10...$15.00
 
Self-Supervised Learning, Sequential Recommendation, Mutual Information Maximization
ACM Reference Format:
Kun Zhou1†, Hui Wang1†, Wayne Xin Zhao2,3∗, Yutao Zhu5, Sirui Wang4,
and Fuzheng Zhang4, Zhongyuan Wang4 and Ji-Rong Wen2,3. 2020. S3-
Rec: Self-Supervised Learning for Sequential Recommendation with Mutual
Information Maximization. In The 29th ACM International Conference on
Information and Knowledge Management (CIKM ’20), October 19–23, 2020,
Virtual Event, Ireland. ACM, New York, NY, USA, 10 pages. 
10.1145/3340531.3411954
INTRODUCTION
Recent years have witnessed the great success of many online platforms, such as Amazon and Taobao. Within online platforms, users’
behaviors are dynamic and evolving over time. Thus it is critical
to capture the dynamics of sequential user behaviors for making
appropriate recommendations. In order to accurately characterize user interests and provide high-quality recommendations, the
task of sequential recommendation has been widely studied in the
literature .
Typically, sequential recommendation methods capture useful sequential patterns from users’ historical behaviors.
Such motivation has been extensively explored with deep learning.
Various methods using recurrent neural networks (RNNs) , convolutional neural networks (CNNs) , and self-attention mechanisms have been proposed to learn good representations of user
preference and characterize sequential user-item interactions.
Furthermore, researchers have incorporated rich contextual information (such as item attributes) to neural sequential recommenders . It has been demonstrated that contextual information is important to consider for improving the performance of
sequential recommender systems.
Although existing methods have been shown effective to some
extent, there are two major shortcomings that are likely to affect
the recommendation performance. First, they rely on the item prediction loss to learn the entire model. When context data is incorporated, the involved parameters are also learned through the only
optimization objective. It has been found that such an optimization way is easy to suffer from issues such as data sparsity .
 
Second, they overemphasize the final performance, while the association or fusion between context data and sequence data has not
been well captured in data representations. As shown in increasing
evidence from various fields , effective data representation (e.g., pre-trained contextualized embedding) has been a key
factor to improve the performance of existing models or architectures. Therefore, there is a need to rethink the learning paradigm
to develop more effective sequential recommender systems.
To address the above issues, we borrow the idea of self-supervised
learning for improving sequential recommendation. Self-supervised
learning is a newly emerging paradigm, which aims to let the
model learn from the intrinsic structure of the raw data. A general
framework of self-supervised learning is to first construct training
signals directly from the raw data and then pre-train the model
parameters with additionally devised optimization objectives. As
previously discussed, limited supervision signals and ineffective
data representations are the two major learning issues with existing
neural sequential methods. Fortunately, self-supervised learning
seems to provide a promising solution to both problems: it utilizes
the intrinsic data correlation to devise auxiliary training objectives
and enhances the data representations via pre-trained methods
with rich self-supervised signals. However, for sequential recommendation, the context information exists in different forms or
with varying intrinsics, including item, attribute, subsequence, or
sequence. It is not easy to develop a unified approach to characterizing such data correlations. For this problem, we are inspired
by the recently proposed mutual information maximization (MIM)
method . It has been shown to be particularly effective
to capture the correlation between different views (or parts) of the
original input by maximizing the mutual information between the
encoded representations of these views.
To this end, in this paper, we propose a novel Self-Supervised
learning approach to improve Sequential Recommendation with
MIM, which is called S3-Rec. Based on a self-attentive recommender architecture , we propose to first pre-train the sequential recommender with self-supervised signals and then fine-tune
the model parameters according to the recommendation task. The
major novelty lies in the pre-training stage. In particular, we carefully devise four self-supervised optimization objectives for capturing item-attribute, sequence-item, sequence-attribute and sequencesubsequence correlations, respectively. These optimization objectives are developed in a unified form of MIM. As such, S3-Rec is
able to characterize the correlation in varying levels of granularity
or between different forms in a general way. It is also flexible to
adapt to new data types or new correlation patterns. Via such a
pre-trained method, we can effectively fuse various kinds of context
data, and learn attribute-aware contextualized data representations.
Finally, the learned data representations are fed into the neural
recommender, which will be optimized according to the recommendation performance.
To validate the effectiveness of our proposed S3-Rec method, we
conduct extensive experiments on six real-world recommendation
datasets of different domains. Experimental results show that S3-
Rec achieves state-of-the-art performance compared to a number
of competitive methods, especially when training data is limited.
We also show that our S3-Rec is effective to adapt to other classes
of neural architectures, such as GRU and CNN.
Our main contributions are summarized as follows: (1) To the
best of our knowledge, it is the first time that self-supervised learning with MIM has been applied to improve the sequential recommendation task; (2) We propose four self-supervised optimization
objectives to maximize the mutual information of context information in different forms or granularities; (3) Extensive experiments
conducted on six real-world datasets demonstrate the effectiveness
of our proposed approach.
RELATED WORK
Sequential Recommendation
Early works on sequential recommendation are based on the Markov
Chain assumption. MC-based methods estimated an item-item
transition probability matrix and utilized it to predict the next item
given the last interaction of a user. A series of works follow this
line and extend it for high-order MCs . With the development of the neural networks, Hidasi et al. firstly introduced
Gated Recurrent Units (GRU) to the session-based recommendation
and a surge of following variants modified this model by introducing pair-wise loss functions , memory networks , hierarchical structures , copy mechanism and reinforcement
learning , etc. There are also studies that leverage other architectures for sequential recommendation. However, these
approaches neglect the rich attribute information about items. To
tackle this problem, TransFM utilized Factorization Machines
to incorporate arbitrary real-valued features to the sequential recommendation. FDSA employed a feature-level self-attention
block to leverage the attribute information about items in user
history. Despite the remarkable success of these sequential recommendation models, the correlations among attribute, item, and
sequence are still not utilized and modeled sufficiently.
Self-supervised Learning
Self-supervised learning aims at training a network on an
auxiliary objective where the ground-truth samples are obtained
from the raw data automatically. The general framework is to construct training signals directly from the correlation within the raw
data and utilize them to train the model. The correlation information learned through self-supervised learning can then be easily
utilized to benefit other tasks. Several self-supervised objectives
have been introduced to use non-visual but intrinsically correlated
features to guide the visual feature learning . As for language
modeling , it is a popular self-supervised objective for natural
language processing, where the model learns to predict the next
word or sentence given the previous sequences. The learned representations of words or sequences can improve the performance of
downstream tasks such as machine reading comprehension and
natural language understanding .
Mutual information maximization is a special branch
of the self-supervised learning. It is inspired by the InfoMax principle and has made important progress in several domains such
as computer vision , audio processing , and nature language
understanding . This method splits the input data into multiple
(possibly overlapping) views and maximizes the mutual information between representations of these views. The views derived
from other inputs are used as negative samples.
Different from the above approaches, our work is the first to
consider the correlations within the contextual information as the
self-supervised signals in sequential recommendation. We maximize the mutual information among the views of the attribute, item,
and sequence, which are in different levels of granularity of the
contextual information. The enhanced data representations can
improve recommendation performance.
PRELIMINARIES
In this section, we first formulate the sequential recommendation
problem and then introduce the technique of mutual information
maximization.
Problem Statement
Assume that we have a set of users and items, denoted by U and
I, respectively, where u ∈U denotes a user and i ∈I denotes
an item. The numbers of users and items are denoted as |U| and
|I|, respectively. Generally, a user u has a chronologically-ordered
interaction sequence with items: {i1, · · · ,in}, wheren is the number
of interactions and it is the t-th item that the user u has interacted
with. For convenience, we use ij:k to denote the subsequence, i.e.,
ij:k = {ij, · · · ,ik } where 1 ≤j < k ≤n. Besides, each item i is
associated with several attributes Ai = {a1, · · · ,am}. For example,
a song is typical with auxiliary information such as artist, album,
and popularity for music recommender. All attributes constitute an
attribute set A, and the number of attributes is donated as |A|.
Based on the above notations, we now define the task of sequential recommendation. Formally, given the historical behaviors of a
user {i1, · · · ,in} and the attributes Ai of each item i, the task of
sequential recommendation is to predict the next item that the user
is likely to interact with at the (n + 1)-th step.
Mutual Information Maximization
An important technique in our approach is the Mutual Information
Maximization (MIM). It is developed on the core concept of mutual
information, which measures dependencies between random variables. Given two random variables X and Y, it can be understood as
how much knowing X reduces the uncertainty in Y or vice versa.
Formally, the mutual information between X and Y is:
I(X,Y) = H(X) −H(X |Y) = H(Y) −H(Y |X).
Maximizing mutual information directly is usually intractable.
Thus we resort to a lower bound on I(X,Y). One particular lower
bound that has been shown to work well in practice is InfoNCE , which is based on Noise Contrastive Estimation (NCE) .
InfoNCE is defined as:
Ep(X,Y )[fθ (x,y) −Eq( ˜Y )[log
exp fθ (x, ˜y)]] + log | ˜Y |,
where x and y are different views of an input, and fθ is a function
parameterized by θ (e.g., a dot product between encoded representations of a word and its context or a dot product between
encoded representations of an image and the local regions of the
image ), and ˜Y is a set of samples drawn from a proposal distribution q( ˜Y), which contains a positive sample y and | ˜Y | −1 negative
Note that InfoNCE is related to the cross-entropy. If ˜Y always
includes all possible values of the random variableY (i.e., ˜Y = Y) and
they are uniformly distributed, maximizing InfoNCE is analogous
to maximize the standard cross-entropy loss:
Ep(X,Y )[fθ (x,y) −log
exp fθ (x, ˜y)].
This equation shows that InfoNCE is related to maximize pθ (y|x),
and it approximates the summation over elements in Y (i.e.,, the
partition function) by negative sampling. Based on this formula,
we can utilize specific X,Y to maximize the mutual information between different views of the raw data, e.g., an item and its attributes,
or a sequence and the items that it contains.
Existing studies mainly emphasize the effect of sequential characteristics using an item-level optimization objective alone.
Inspired by recent progress with MIM , we take a different
perspective to develop neural sequential recommenders by maximizing the mutual information among different views of the raw
The basic idea of our approach is to incorporate several elaborately designed self-supervised learning objectives for enhancing
the original model. To develop such objectives, we leverage effective correlation signals reflected in the intrinsic characteristics of
the input. For our task, we consider the information in different
levels of granularity, including attribute, item, segment (i.e., subsequence), and sequence, which are considered as different views of
the input. By capturing the multi-view correlation, we unify these
self-supervised learning objectives with the recently proposed pretraining framework in language modeling .
The overview of S3-Rec is presented in Fig. 1. In the following
sections, we first introduce the base model of our proposed approach
that is developed on the Transformer architecture . Then, we will
describe how we utilize the correlation signals among attributes,
items, segments, and sequences to enhance the data representations
based on the InfoNCE method. Finally, we present the
discussions on our approach.
Base Model
We develop the basic framework for sequential recommendation
model by stacking the embedding layer, self-attention blocks, and
the prediction layer.
Embedding Layer. In the embedding mapping stage, we maintains an item embedding matrix MI ∈R|I |×d and an attribute
embedding matrix MA ∈R|A |×d. The two matrices project the
high-dimensional one-hot representation of an item or attribute
to low-dimensional dense representations. Given a n-length item
sequence, we apply a look-up operation from MI to form the input
embedding matrix E ∈Rn×d. Besides, we incorporate a learnable
Attributes
Bidirectional
Self-Attenion
Sequence-Item
Sequence-Attribute
Item-Attribute
Sequence-Sequence
Bidirectional
Self-Attenion
(1) Associated Attribute Prediction
(2) Masked Item Prediction
(3) Masked Attribute Prediction
(4) Segment Prediction
Bidirectional
Self-Attenion
Bidirectional
Self-Attenion
Bidirectional
Self-Attenion
Masked Item
Masked Item
Attributes
ea, a ∈ 𝒜i2
Attributes
Embeddings
Attributes
Embeddings
ea, a ∈ 𝒜i1
ea, a ∈ 𝒜in
Figure 1: The overview of S3-Rec in the pre-training stage. We assume that the user sequence is {i1, · · · ,in} and each item i
is associated with several attributes Ai = {a1, · · · ,am}. We incorporate four self-supervised learning objectives: (1) Associated Attribute Prediction (AAP), (2) Masked Item Prediction (MIP), (3) Masked Attribute Prediction (MAP), and (4) Segment
Prediction (SP). The embedding layers and bidirectional self-attention blocks are shared by the four pre-training objectives.
position encoding matrix P ∈Rn×d to enhance the input representation of the item sequence. By this means, the sequence representation EI ∈Rn×d can be obtained by summing two embedding
matrices: EI = E + P. Since our task utilizes auxiliary context data,
we also form an embedding matrix EA ∈Rk×d for each item from
the entire attribute embedding matrix MA, where k is the number
of item attributes.
Self-Attention Block. Based on the embedding layer, we develop the item encoder by stacking multiple self-attention blocks.
A self-attention block generally consists of two sub-layers, i.e., a
multi-head self-attention layer and a point-wise feed-forward network. The multi-head self-attention mechanism has been adopted
for effectively extracting the information selectively from different
representation subspaces. Specifically, the multi-head self-attention
is defined as:
MultiHeadAttn(Fl) = [head1,head2, ...,headh]WO,
headi = Attention(FlWQ
where the Fl is the input for the l-th layer. When l = 0, we set
F0 = EI , and the projection matrix WQ
i ∈Rd×d/h, WK
i ∈Rd×d/h,
V ∈Rd×d/h and WO ∈Rd×d are the corresponding learnable
parameters for each attention head. The attention function is implemented by scaled dot-product operation:
Attention(Q, K, V) = softmax( QK⊤
where Q = FlWQ
i , K = FlWK
i , and V = FlWV
i are the linear
transformations of the input embedding matrix, and
d/h is the
scale factor to avoid large values of the inner product.
Since the multi-head attention function is mainly built on the
linear projections. We endow the non-linearity of the self-attention
block by applying a point-wise feed-forward network. The computation is defined as:
Fl = [FFN(Fl
1)⊤; · · · ; FFN(Fl
FFN(x) = (ReLU(xW1 + b1))W2 + b2,
where W1,b1,W2,b2 are trainable parameters.
In sequential recommendation, only the information before the
current time step can be utilized, thus we apply the mask operation
for the output of the multi-head self-attention function to remove
all connections between Qi and Ki. Inspired by BERT , at the
pre-training stage, we remove the mask mechanism to acquire the
bidirectional context-aware representation of each item in an item
sequence. It is beneficial to incorporate context from both directions
for sequence representation learning .
Prediction Layer. In the final layer of S3-Rec, we calculate
the user’s preference score for the item i in the step (t + 1) under
the context from user history as:
P(it+1 = i|i1:t ) = e⊤
where ei is the representation of item i from item embedding matrix
t is the output of the L-layer self-attention block at step t and
L is the number of self-attention blocks.
Self-supervised Learning with MIM
Based on the above self-attention model, we further incorporate
additional self-supervised signals with MIM to enhance the representations of input data. We adopt a pre-training way to construct
different loss functions based on the multi-view correlation.
Modeling Item-Attribute Correlation. We first maximize the
mutual information between items and attributes. For each item, the
attributes provide fine-grained information about it. Therefore, we
aim to fuse item- and attribute-level information through modeling
item-attribute correlation. In this way, it is expected to inject useful
attribute information into item representations.
Given an item i and the attribute set Ai = {a1, · · · ,ak }, we treat
the item itself and its associated attributes as two different views.
Formally, let ei denote the item embedding obtained by the embedding layer, and eaj denote the embedding for the j-th attribute
aj ∈Ai. We design a loss function by the contrastive learning
framework that maximizes the mutual information between the
two views. Following Eq. 3, we minimize the Associated Attribute
Prediction (AAP) loss by:
LAAP (i, Ai) = Eaj ∈Ai [f (i,aj) −log
exp(f (i, ˜a))],
where we sample negative attributes ˜a that enhance the association
between the item i and the ground-truth attributes, “\” defines set
subtraction operation. The function f (·, ·) is implemented with a
simple bilinear network:
f (i,aj) = σ  e⊤
i · WAAP · eaj
where WAAP ∈Rd×d is a parameter matrix to learn and σ(.) is the
sigmoid function. Note that for clarity, we give the loss definition
LAAP for a single item. It will be easy to define this loss over the
entire item set.
Modeling Sequence-Item Correlation. Conventional sequential recommendation models are usually trained to predict the item
at the next step. This approach only considers the sequential characteristics in an item sequence from left to right. While it is noted that
the entire interaction sequence is indeed observed by the model
in the training process. Inspired by the masked language model
like BERT , we propose to model the bidirectional information
in item sequence by a Cloze task. For our task, the Cloze setting
is described as below: at each training step, we randomly mask a
proportion of items in the input sequence (i.e., replace them with
special tokens “[mask]”). Then we predict the masked items from
the original sequence based on the surrounding context in both
directions.
Therefore, the second loss we consider is to recover the actual item with the bidirectional context from the input sequences.
For this purpose, we prepare a pre-trained version of the base
model in Section 4.2, which is a bidirectional Transformer architecture. As illustration, let us mask the t-th item it in a sequence
{i1, · · · ,it, · · · ,in}. We treat the rest sequence {i1, · · · ,mask, · · · ,in}
as the surrounding context for it , denoted by Cit . Given the surrounding context Cit and the masked item it , we treat them as two
different views to fuse for learning data representations. Following
Eq. 3, we minimize the Masked Item Prediction (MIP) loss by:
LMIP (Cit ,it ) = f (Cit ,it ) −log[
˜i ∈I\{it }
f (Cit ,it )],
where ˜i denotes an irrelevant item, and f (·, ·) is implemented according to the following formula:
f (Cit ,it ) = σ  F⊤
t · WMIP · eit
where WMIP ∈Rd×d is a parameter matrix to learn and Ft is the
learned representation for the t-th position using the bidirectional
Transformer architecture obtained in the same way as Eq. 7.
Modeling Sequence-Attribute Correlation. Having modeled
both item-attribute and sequence-item correlations, we further consider directly fusing attribute information with sequential contexts.
Specifically, we adopt a similar way as in Section 4.3.2 to recover
the attributes of a masked item based on surrounding contexts.
Given a masked item it , we treat its surrounding context Cit and
its attribute set Ait as two different views for MIM. As such, we
can develop the following Masked Attribute Prediction (MAP) loss
LMAP (Cit , Ait )
=Ea∈Ait [f (Cit ,a) −log
exp(f (Cit , ˜a))],
where f (·, ·) is implemented according to the following formula:
f (Cit ,a) = σ  F⊤
t · WMAP · ea
where WMAP ∈Rd×d is a parameter matrix to learn. Note that
existing methods seldom directly model the correlation
between the sequential context and attribute information. While,
we would like to explicitly model the correlation to derive more
meaningful supervision signals, which is useful to improve the data
representations for multi-granularity information.
Modeling Sequence-Segment Correlation. As shown above,
the Cloze learning strategy plays a key role in our pre-trained approach in fusing sequential contexts with target information. However, a major difference between item sequence with word sequence
is that a single target item may not be highly related to surrounding contexts. For example, a user has bought some products just
because they were on sale. Based on this concern, we extend the
Cloze strategy from a single item to item subsequence (i.e., called
segment). Apparently, an item segment reflects more clear, stable
user preference than a single item. Therefore, we follow a similar
strategy in Section 4.3.2 to recover an item subsequence from surrounding contexts. It is expected to enhance the self-supervised
learning signal and improve the pre-trained performance.
Let ij1:j2 denote the subsequence from item ij1 to ij2, and Cij1:j2
denote the context for ij1:j2 within the entire sequence. Similar
to Eq. 12, we can recover the missing item segment with a MIM
formulation, which is so called the Segment Prediction (SP) loss as:
LSP (Cij1:j2 ,ij1:j2)
=f (Cij1:j2 ,ij1:j2) −log
exp  f (Cij1:j2 , ˜ij1:j2),
where ˜ij1,j2 is the corrupted negative subsequence and f (·, ·) is
implemented according to the following formula:
f (Cij1:j2 ,ij1:j2) = σ  s⊤· WSP · ˜s,
where WSP ∈Rd×d is a parameter matrix to learn, and s and ˜s are
the learned representations for the contexts Cij1:j2 and subsequence
ij1:j2, respectively. In order to learn s and ˜s, we apply the bidirectional Transformer to obtain the state representations of the last
position in a sequence.
Learning and Discussion
In this part, we present the learning and related discussions of our
S3-Rec for sequential recommendation.
Learning. The entire procedure of S3-Rec consists of two
important stages, namely pre-training and fine-tuning stages. We
adopt bidirectional and unidirectional Transformer architectures for the two stages, respectively. At the pre-trained stage, we
optimize the self-supervised learning objectives by considering four
different kinds of correlations (Eq. 10, Eq. 12, Eq. 14 and Eq. 16); at
the fine-tuning stage, we utilize the learned parameters from the
pre-trained stage to initialize the parameters of the unidirectional
Transformer, and then utilize the left-to-right supervised signals to
train the network. We adopt the pairwise rank loss to optimize its
parameters as:
P(it+1|i1:t ) −P(i−
t+1|i1:t )
where we pair each ground-truth item it+1 with a negative item
t+1 that is randomly sampled.
Discussion. Our work provides a novel self-supervised approach to capturing the intrinsic data correlation from the input as
an additional signal through the pre-trained models. This approach
is quite general so that many existing methods can be included in
this framework. We make a brief discussion below.
Feature-based approaches such as Factorization Machine 
and AutoInt mainly learn data representations through the
interaction of context features. The final prediction is made according to the actual interaction results between the user and item
features. In S3-Rec, the associated attribute prediction loss LAAP
in Eq. 10 and the masked attribute prediction loss LMAP in Eq. 14
have the similar effect in feature interaction. However, we do not
explicitly model the interaction between attributes. Instead, we
focus on capturing the association between attribute information
and item/sequential contexts. A major difference in our work is
to utilize feature interaction as additional supervision signals to
enhance data representations instead of making predictions.
Sequential models such as GRU4Rec and SASRec mainly
focus on modeling the sequential dependencies between contextual
items and the target item in a left-to-right order. S3-Rec additionally
incorporates a pre-trained stage that leverages four different kinds
of self-supervised learning signals for enhancing data representations. In particular, the masked item prediction loss LMIP in Eq. 12
has a similar effect to capture sequential dependencies as in 
except that it can also utilize bidirectional sequential information.
Attribute-aware sequential models such as TransFM 
and FDSA leverage the contextual features to improve the sequential recommender models, in which these features are treated
as auxiliary information to enhance the representation of items
or sequences. In our S3-Rec, the LAAP loss and LMAP loss aim to
fuse attribute with items or sequential contexts, which is able to
achieve the same effect as previous methods . Besides, the
pre-trained data representations can be also applied to improve
existing methods.
Table 1: Statistics of the datasets after preprocessing.
Meituan Beauty
Yelp LastFM
# Avg. Actions / User
# Avg. Actions / Item
747,827 198,502 296,337 167,597 316,354
99.73% 99.93% 99.95% 99.93% 99.95% 98.68%
# Attributes
# Avg. Attribute / Item
EXPERIMENT
Experimental Setup
Dataset. We conduct experiments on six datasets collected
from four real-world platforms with varying domains and sparsity levels. The statistics of these datasets after preprocessing are
summarized in Table 1.
(1) Meituan1: this dataset consists of six-year transaction records in Beijing on the Meituan platform.
We select categories, locations, and the keywords extracted from
customer reviews as attributes.
(2) Amazon Beauty, Sports, and Toys: these three datasets are
obtained from Amazon review datasets in . In this work, we
select three subcategories: “Beauty”, “Sports and Outdoors”, and
“Toys and Games”, and utilize the fine-grained categories and the
brands of the goods as attributes.
(3) Yelp2: this is a popular dataset for business recommendation.
As it is very large, we only use the transaction records after January
1st, 2019. We treat the categories of businesses as attributes.
(4) LastFM3: this is a music artist recommendation dataset and
contains user tagging behaviors for artists. In this dataset, the tags
of the artists given by the users are used as attributes.
For all datasets, we group the interaction records by users and
sort them by the interaction timestamps ascendingly. Following , we only keep the 5-core datasets, and filter unpopular items
and inactive users with fewer than five interaction records.
Evaluation Metrics. We employ top-k Hit Ratio (HR@k), topk Normalized Discounted Cumulative Gain (NDCG@k), and Mean
Reciprocal Rank (MRR) to evaluate the performance, which are
widely used in related works . Since HR@1 is equal to
NDCG@1, we report results on HR@{1, 5, 10}, NGCG@{5, 10}, and
MRR. Following previous works , we apply the leave-oneout strategy for evaluation. Concretely, for each user interaction
sequence, the last item is used as the test data, the item before the
last one is used as the validation data, and the remaining data is
used for training. Since the item set is large, it is time-consuming
to use all items as candidates for testing. Following the common
strategy , we pair the ground-truth item with 99 randomly
sampled negative items that the user has not interacted with. We
calculate all metrics according to the ranking of the items and report
the average score over all test users.
1 
2 
3 
Baseline Models. We compare our proposed approach with
the following eleven baseline methods:
(1) PopRec is a non-personalized method that ranks items according to popularity measured by the number of interactions.
(2) FM characterizes the pairwise interactions between variables using factorized model.
(3) AutoInt utilizes the multi-head self-attentive neural
network to learn the feature interaction.
(4) GRU4Rec applies GRU to model user click sequence
for session-based recommendation. We represent the items using
embedding vectors rather than one-hot vectors.
(5) Caser is a CNN-based method capturing high-order
Markov Chains by applying horizontal and vertical convolutional
operations for sequential recommendation.
(6) SASRec is a self-attention based sequential recommendation model, which uses the multi-head attention mechanism to
recommend the next item.
(7) BERT4Rec uses a Cloze objective loss for sequential
recommendation by the bidirectional self-attention mechanism.
(8) HGN is recently proposed and adopts hierarchical gating
networks to capture long-term and short-term user interests.
(9) GRU4RecF is an improved version of GRU4Rec, which
leverages attributes to improve the performance.
(10) SASRecF is our extension of SASRec, which concatenates
the representations of item and attribute as the input to the model.
(11) FDSA constructs a feature sequence and uses a featurelevel self-attention block to model the feature transition patterns.
This is the state-of-the-art model in sequential recommendation.
Implementation Details. For Caser and HGN, we use the
source code provided by their authors. For other methods, we implement them by PyTorch. All hyper-parameters are set following
the suggestions from the original papers.
For our proposed S3-Rec, we set the number of the self-attention
blocks and the attention heads as 2. The dimension of the embedding
is 64, and the maximum sequence length is 50 (following ). Note
that our training phase contains two stages (i.e., pre-training and
fine-tuning stage), the learned parameters in the pre-training stage
are used to initialize the embedding layers and self-attention layers
of our model in the fine-tuning stage.
In the pre-training stage, the mask proportion of item is set as
0.2 and the weights for the four losses (i.e., AAP, MIP, MAP, and SP)
are set as 0.2, 1.0, 1.0, and 0.5, respectively, based on our empirical
experiments. We use the Adam optimizer with a learning rate of
0.001, where the batch size is set as 200 and 256 in the pre-training
and the fine-tuning stage, respectively. We pre-train our model for
100 epochs and fine-tune it on the recommendation task. The code
and data set are available at the link: 
CIKM2020-S3Rec 4.
Experimental Results
The results of different methods on all datasets are shown in Table 2.
Based on the results, we can find:
4To further verify the effectiveness of our method, we have performed the experiments
that rank the ground-truth item with all the items as candidates. The complete results
are shown on our project website at this link.
For three non-sequential recommendation baselines, the performance order is consistent across all datasets, i.e., PopRec > AutoInt
> FM. Due to the “rich-gets-richer” effect in product adoption,
PopRec is a robust baseline. AutoInt performs better than FM on
most datasets because the multi-head self-attention mechanism has
a stronger capacity to model attributes. However, the performance
of AutoInt is worse than that of FM on Meituan dataset. A potential reason is that the multi-head self-attention may incorporate
more noise from the attributes since they are keywords extracted
from the reviews on Meituan platform. In general, non-sequential
recommendation methods perform worse than sequential recommendation methods, since the sequential pattern is important to
consider in our task.
As for sequential recommendation baseline methods, SASRec
and BERT4Rec utilize the unidirectional and bidirectional selfattention mechanism respectively, and achieve better performance
than GRU4Rec and Caser. It indicates that self-attentive architecture is particularly suitable for modeling sequential data. However,
their improvements are not stable when training with the conventional next-item prediction loss. Besides, HGN achieves comparable
performance with SASRec and BERT4Rec. This indicates the hierarchical gating network can well model the relations between closely
relevant items. However, when directly injecting the attribute information into GRU4Rec and SASRec (i.e., GRU4RecF and SASRecF ),
the performance improvement is not consistent. This method yields
improvement on Beauty, Sports, Toys, and Yelp datasets, but has a
negative influence on other datasets. One possible reason is that
simply concatenating item representations and its attributes representations cannot effectively fuse the two kinds of information. In
most cases, FDSA achieves the best performance among all baselines. This suggests that the feature-level self-attention blocks can
capture useful sequential feature interaction patterns.
Finally, by comparing our approach with all the baselines, it is
clear to see that S3-Rec performs consistently better than them by
a large margin on six datasets. Different from these baselines, we
adopt the self-supervised learning to enhance the representations
of the attribute, item, and sequence for the recommendation task,
which incorporates four pre-training objectives to model multiple
data correlations by MIM. This result also shows that the selfsupervised approach is effective to improve the performance of the
self-attention architecture for sequential recommendation.
Further Analysis
Next, we continue to study whether S3-Rec works well in more
detailed analysis.
Ablation Study. Our proposed self-supervised approach S3-
Rec designs four pre-training objectives based on MIM. To verify
the effectiveness of each objective, we conduct the ablation study
on Meituan, Beauty, Sports, and Toys datasets to analyze the contribution of each objective. NDCG@10 is adopted for this evaluation.
The results from the best baseline FDSA are also provided for comparison.
From the results in Fig. 2, we can observe that removing any
self-supervised objective would lead to the performance decrease.
It indicates all the objectives are useful to improve the recommendation performance. Besides, the importance of these objectives
Table 2: Performance comparison of different methods on six datasets. The best performance and the second best performance
methods are denoted in bold and underlined fonts respectively. “∗” indicates the statistical significance for p < 0.01 compared
to the best baseline method.
Figure 2: Ablation study of our approach on four datasets (NDCG@10). “¬” indicates that the corresponding objective is removed in the pre-training stage, while the rest objectives are kept.
is varying on different datasets. Overall, the AAP (Associated Attribute Prediction) and the MAP (Masked Attribute Prediction) are
more important than the other objectives. Removing each of them
yields a larger drop of performance on all datasets. One possible
reason is that these two objectives enhance the representations of
item and sequence with the attributes information.
Figure 3: Performance (NDCG@10) comparison of different
models enhanced by our self-supervised learning approach
on Beauty and Toys datasets.
100% 80% 60% 40% 20%
(a) Sports
100% 80% 60% 40% 20%
Figure 4: Performance (NDCG@10) comparison w.r.t. different sparsity levels on Sport and Yelp datasets.
It is clearly seen that all model variants are better than the best
baseline FDSA, which is trained only with next-item predication
Applying Self-Supervised Learning to Other Models. Since
self-supervised learning itself is a learning paradigm, it can generally apply to various models. Thus, in this part, we conduct an
experiment to examine whether our method can bring improvements to other models. We use the self-supervised approach to
pre-training some baseline models on Beauty and Toys datasets.
For GRU4Rec, GRU4RecF , SASRec, and SASRecF , we directly apply
our pre-training objectives to improve them. It is worth noting that
GRU4Rec and SASRec are unidirectional models, so we maintain
the unidirectional encoder layer in the pre-training stage. For AutoInt and Caser, since their architectures do not support some of the
pre-training objectives5, we only utilize the pre-trained parameters
to initialize the parameters of the embedding layers.
The results of NDCG@10 on Beauty and Toys datasets are shown
in Fig. 3. First, after pre-training by our approach, all the baselines
achieve better performance. This shows that self-supervised learning can also be applied to improve their performance. Second, S3-
Rec outperforms all the baselines after pre-training. This is because
5Because their base models do not support the mask operations.
Pre-Training
w/o Pre-Training
(a) Beauty
Pre-Training
w/o Pre-Training
Figure 5: Performance (NDCG@10) comparison w.r.t. different numbers of pre-training epochs on Beauty and Toys
our model adopts the bidirectional Transformer encoder in the pretraining stage, which is more suitable for our approach. Third, we
can see the GRU-based models achieve less improvement than the
other models. One possible reason is that RNN-based architecture
limits the potential of self-supervised learning.
Performance Comparison w.r.t. the Amount of Training Data.
Conventional recommendation systems require a considerable amount of training data, thus they are likely to suffer from the cold
start problem in real-world applications. This problem can be alleviated by our method because the proposed self-supervised learning
approach can better utilize the data correlation from input. We
simulate the data sparsity scenarios by using different proportions
of the full dataset, i.e., 20%, 40%, 60%, 80%, and 100%.
Fig. 4 shows the evaluation results on Sports and Yelp datasets. As
we can see, the performance substantially drops when less training
data is used. While, S3-Rec is consistently better than baselines in all
cases, especially in an extreme sparsity level (20%). This observation
implies that S3-Rec is able to make better use of the data with
the self-supervised method, which alleviates the influence of data
sparsity problem for sequential recommendation to some extent.
Performance Comparison w.r.t. the Number of Pre-training
Epochs. Our approach consists of a pre-training stage and a finetuning stage. In the pre-training stage, our model can learn the
enhanced representations of the attribute, item, subsequence, and
sequence for the recommendation task. The number of pre-training
epochs affects the performance of the recommendation task. To
investigate this, we pre-train our model with a varying number of
epochs and fine-tune it on the recommendation task.
Fig. 5 presents the results on Beauty and Toys datasets. The horizontal dash lines represent the performance without pre-training.
We can see that our model benefits mostly from the first 20 pretraining epochs. And after that, the performance improves slightly.
Based on this observation, we can conclude that the correlations
among different views (i.e., the attribute, item, subsequence, and
sequence) can be well-captured by our self-supervised learning
approach through pre-training within a small number of epochs.
So that the enhanced data representations can improve the performance of sequential recommendation.
Convergence Speed Comparison. After obtaining the enhanced
representations of the attribute, item, and sequence, we fine-tune
(a) Beauty
Figure 6: Performance tuning (NDCG@10) of our approach
and other baselines with the increasing iterations in the finetuning stage.
our model on the recommendation task. To examine the convergence speed on the final recommendation task, we gradually increase the number of epochs for the fine-tuning stage and compare
the performance of our model and other baselines.
Fig. 6 shows the results on Beauty and Toys datasets. It can be
observed that our model converges quickly and achieves the best
performance after about 40 epochs. In contrast to our model, the
comparison models need more epochs to achieve stable performance. This result shows that our approach can utilize pre-trained
parameters to help the model converge faster and achieve better
performance.
CONCLUSION
In this paper, we proposed a self-supervised sequential recommendation model S3-Rec based on the mutual information maximization (MIM) principle. In our approach, we adopted the self-attentive
recommender architecture as the base model and devised four selfsupervised learning objectives to learn the correlations within the
raw data. Based on MIM, the four objectives can learn the correlations among attribute, item, segment, and sequence, which
enhances the data representations for sequential recommendation.
Experimental results have shown that our approach outperforms
several competitive baselines.
In the future, we will investigate how to design other forms of
self-supervised optimization objectives. We will also consider applying our approach to more complex recommendation tasks, such as
conversational recommendation and multimedia recommendation.
ACKNOWLEDGEMENT
This work was partially supported by the National Natural Science Foundation of China under Grant No. 61872369 and 61832017,
Beijing Academy of Artificial Intelligence (BAAI) under Grant No.
BAAI2020ZJ0301, and Beijing Outstanding Young Scientist Program under Grant No. BJJWZYJH012019100020098, the Fundamental Research Funds for the Central Universities, the Research Funds
of Renmin University of China under Grant No.18XNLG22 and
19XNQ047. Xin Zhao is the corresponding author.