Pattern Recognition 00 1–26
Recognition
Tree Ensembles for Predicting Structured Outputs
Dragi Koceva,∗, Celine Vensb, Jan Struyfb, Saˇso Dˇzeroskia,c,d
aDepartment of Knowledge Technologies, Joˇzef Stefan Institute, Jamova cesta 39, 1000 Ljubljana, Slovenia
bDepartment of Computer Science, Katholieke Universiteit Leuven, Celestijnenlaan 200A, 3001 Leuven, Belgium
cInternational Postgraduate School Joˇzef Stefan, Jamova cesta 39, 1000 Ljubljana, Slovenia
dCentre of Excellence for Integrated Approaches in Chemistry and Biology of Proteins (CIPKeBiP), Jamova cesta 39, 1000 Ljubljana, Slovenia
In this article, we address the task of learning models for predicting structured outputs. We consider both global
and local prediction of structured outputs, the former based on a single model that predicts the entire output structure
and the latter based on a collection of models, each predicting a component of the output structure. We use ensemble methods and apply them in the context of predicting structured outputs. We propose to build ensemble models
consisting of predictive clustering trees, which generalize classiﬁcation trees: these have been used for predicting
diﬀerent types of structured outputs, both locally and globally. More speciﬁcally, we develop methods for learning
two types of ensembles (bagging and random forests) of predictive clustering trees for global and local prediction of
diﬀerent types of structured outputs. The types of outputs considered correspond to diﬀerent predictive modelling
tasks: multi-target regression, multi-target classiﬁcation, and hierarchical multi-label classiﬁcation. Each of the combinations can be applied both in the context of global prediction (producing a single ensemble) or local prediction
(producing a collection of ensembles). We conduct an extensive experimental evaluation across a range of benchmark
datasets for each of the three types of structured outputs. We compare ensembles for global and local prediction, as
well as single trees for global prediction and tree collections for local prediction, both in terms of predictive performance and in terms of eﬃciency (running times and model complexity). The results show that both global and local
tree ensembles perform better than the single model counterparts in terms of predictive power. Global and local tree
ensembles perform equally well, with global ensembles being more eﬃcient and producing smaller models, as well
as needing fewer trees in the ensemble to achieve the maximal performance.
Keywords: ensemble methods, predictive clustering trees, structured outputs, multi-target regression, multi-target
classiﬁcation, hierarchical multi-label classiﬁcation
1. Introduction
Supervised learning is one of the most widely researched and investigated areas of machine learning. The goal in
supervised learning is to learn, from a set of examples with known class, a function that outputs a prediction for the
class of a previously unseen example. If the examples belong to two classes (e.g., the example has some property or
not) the task is called binary classiﬁcation. The task where the examples can belong to a single class from a given
∗Corresponding author (telephone: +386 1 477 3639)
Email addresses: (Dragi Kocev), (Celine Vens), 
(Jan Struyf), (Saˇso Dˇzeroski)
D. Kocev et al. / Pattern Recognition 00 1–26
set of m classes (m ≥3) is known as multi-class classiﬁcation. The case where the output is a real value is called
regression.
However, in many real life problems of predictive modelling the output (i.e., the target) is structured, meaning that
there can be dependencies between classes (e.g., classes are organized into a tree-shaped hierarchy or a directed acyclic
graph) or some internal relations between the classes (e.g., sequences). These types of problems occur in domains such
as life sciences (predicting gene function, ﬁnding the most important genes for a given disease, predicting toxicity of
molecules, etc.), ecology (analysis of remotely sensed data, habitat modelling), multimedia (annotation and retrieval
of images and videos) and the semantic web (categorization and analysis of text and web pages). Having in mind the
needs of these application domains and the increasing quantities of structured data, Yang and Wu and Kriegel et
al. listed the task of “mining complex knowledge from complex data” as one of the most challenging problems in
machine learning.
A variety of methods, specialized in predicting a given type of structured output (e.g., a hierarchy of classes ),
have been proposed . These methods can be categorized into two groups of methods for solving the problem of
predicting structured outputs : (1) local methods that predict component(s) of the output and then combine the
individual models to get the overall model and (2) global methods that predict the complete structure as a whole (also
known as ‘big-bang’ approaches). The global methods have several advantages over the local methods. First, they
exploit and use the dependencies that exist between the components of the structured output in the model learning
phase, which can result in better predictive performance. Next, they are typically more eﬃcient: it can easily happen
that the number of components in the output is very large (e.g., hierarchies in functional genomics can have several
thousands of components), in which case executing a basic method for each component is not feasible. Furthermore,
they produce models that are typically smaller than the sum of the sizes of the models built for each of the components.
The predictive models that we consider in this article are predictive clustering trees (PCTs). PCTs belong to the
group of global methods. PCTs oﬀer a unifying approach for dealing with diﬀerent types of structured outputs and
construct the predictive models very eﬃciently. They are able to make predictions for several types of structured
outputs: tuples of continuous/discrete variables, hierarchies of classes, and time series. More details about the PCT
framework can be found in .
PCTs can be considered as a generalization of standard decision trees towards predicting structured outputs. Although they oﬀer easily interpretable trees with good predictive performance, they inherit the stability issues of decision trees . Namely, change in just a few training examples can sometimes drastically change the structure of the
tree. Breiman states that un-stable predictive models (such as decision trees) could be combined into an ensemble
to improve their predictive performance. An ensemble is a set of (base) predictive models, whose output is combined.
For basic classiﬁcation and regression tasks, it is widely accepted that an ensemble lifts the predictive performance
of its base predictive models . However, for the task of predicting structured outputs, this issue has not been
thoroughly investigated. Moreover, in the case where the base predictive models are decision trees, Bauer and Kohavi
 conclude that the ensemble’s increase in performance is stronger if the trees are unpruned, i.e., allowed to overﬁt.
On the other hand, Blockeel et al. state that PCTs for structured outputs show less overﬁtting than the trees for
classiﬁcation of a single target variable. Having in mind these two conﬂicting inﬂuences, it is not obvious whether
an ensemble of predictive clustering trees can signiﬁcantly increase the predictive performance over that of a single
predictive clustering tree.
Furthermore, in an ensemble learning setting, it is not clear if the predictive performance of an ensemble of global
predictive models will be better or worse than the predictive performance of a combination of ensembles of local
predictive models. Generally, an ensemble is known to perform better than its base learner if the base learner is
accurate and diverse . While the superior predictive performance of global models has been shown before ,
less is known about their diversity or instability (i.e., whether they produce diﬀerent errors with small changes to
the training data). It is expected that a PCT for predicting structured outputs, especially in the case of hierarchical
classiﬁcation, is less unstable than a PCT for predicting components of the output. It is also not clear which approach
will be more eﬃcient, both in terms of running time and size of the predictive models.
In this article, we investigate the aforementioned questions. We use bagging and random forests as ensemble
learning methods, since they are the two most widely used ensemble learning methods in the context of decision trees
 . We consider two structured output machine learning tasks: predicting multiple target variables and hierarchical
multi-label classiﬁcation. We perform an extensive empirical evaluation of the proposed methods over a variety of
benchmark datasets.
D. Kocev et al. / Pattern Recognition 00 1–26
The paper is based on our previous work by Kocev et al. and Schietgat et al. . Kocev et al. conducted an
initial comparison of diﬀerent ensemble schemes using predictive clustering trees in the context of predicting multiple
targets. Schietgat et al. introduced bagging of PCTs for hierarchical multi-label classiﬁcation (HMC) in the
context of functional genomics. The present paper extends the previous work in the following directions:
• The tasks of predicting multiple target variables and hierarchical multi-label classiﬁcation are discussed jointly.
Formal deﬁnitions of the considered tasks are provided, as well as an extensive discussion of the related work.
• The experimental evaluation is performed on a much wider selection of datasets, from various domains. The
performance of ensembles with diﬀerent number of base predictive models is evaluated and saturation curves
are provided.
• A better methodology for performance comparisons is used and a more detailed discussion of the results is
presented. Friedman tests combined with a Nemenyi post-hoc test are used to compare diﬀerent ensemble
schemes, instead of performing pairwise comparisons.
• Global random forests of PCTs for HMC are introduced and evaluated. Local ensembles (both bagging and
random forests) of PCTs for HMC are introduced and evaluated.
• A computational complexity analysis of the proposed methods is performed, which is consistent with the empirical evidence. Global tree ensembles are most eﬃcient, especially random forests, and are scalable to large
The remainder of this paper is organized as follows. In Section 2, the considered machine learning tasks are
formally deﬁned. Section 3 ﬁrst explains the predictive clustering trees framework and the extensions for predicting
multiple targets and hierarchical multi-label classiﬁcation. It then describes the ensemble methods and their extension
for predicting structured outputs. The design of the experiments, the descriptions of the datasets, the evaluation
measures and the parameter settings for the algorithms are given in Section 4. Section 5 presents and discusses the
obtained results. The related work is presented in Section 6. Finally, the conclusions are stated in Section 7.
2. Machine learning tasks
The work presented in this article concerns the learning of ensembles for predicting structured outputs. First, we
formally describe the machine learning tasks that we consider here: predicting multiple targets and hierarchical multilabel classiﬁcation. We follow the suggestions by Dˇzeroski , where predictive modelling is deﬁned for arbitrary
types of input and output data. In particular, we describe the tasks of predicting multiple targets and hierarchical
multi-label classiﬁcation.
2.1. Predicting multiple targets
The task of predicting multiple targets was previously referred to as multi-objective prediction . However, the term ‘multi-objective’ is already established in the area of optimization. We will thus use the term ‘predicting
multiple targets’ or multi-target prediction (resp. multi-target classiﬁcation and regression). We deﬁne the task of predicting multiple targets as follows.
• A description space X that consists of tuples of values of primitive data types (discrete or continuous), i.e.,
∀Xi ∈X, Xi = (xi1, xi2, ..., xiD), where D is the size of the tuple (or number of descriptive variables),
• a target space Y which consists of a tuple of several variables that can be either continuous or discrete, i.e.,
∀Yi ∈Y, Yi = (yi1, yi2, ..., yiT ), where T is the size of the tuple (i.e., number of target variables),
• a set of examples E, where each example is a pair of tuples from the description and target space, respectively,
i.e., E = {(Xi, Yi)|Xi ∈X, Yi ∈Y, 1 ≤i ≤N} and N is the number of examples in E (N = |E|), and
D. Kocev et al. / Pattern Recognition 00 1–26
• a quality criterion q, which rewards models with high predictive accuracy and low complexity.
Find: a function f : X →Y such that f maximizes q.
Here, the function f is represented with decision trees, i.e., predictive clustering trees or ensembles thereof. If the
tuples from Y (the target space) consist of continuous/numeric variables then the task at hand is multi-target regression.
Likewise, if the tuples from Y consist of discrete/nominal variables then the task is called multi-target classiﬁcation.
The task of multi-target classiﬁcation can be seen as a generalization of the multi-label classiﬁcation task .
Namely, multi-label classiﬁcation is concerned with learning from examples, where each example is associated with
multiple labels. These multiple labels belong to a predeﬁned set of labels. The goal is then to construct a predictive
model that will provide a list of relevant labels for a given, previously unseen example. For addressing the task multilabel classiﬁcation using multi-target classiﬁcation, each of the labels can be considered as a binary (0/1) discrete
target variable: The label vector is then a target tuple of binary discrete variables. The labels for which 1’s are
predicted are thus considered as relevant labels.
2.2. Hierarchical classiﬁcation
Classiﬁcation is deﬁned as the task of learning a model using a set of previously classiﬁed instances and applying
the obtained model to a set of previously unseen examples . The unseen examples are classiﬁed into a single
class from a set of possible classes.
Hierarchical classiﬁcation diﬀers from traditional classiﬁcation that the classes are organized in a hierarchy. An
example that belongs to a given class automatically belongs to all its super-classes (this is known as the hierarchy
constraint). Furthermore, if an example can belong simultaneously to multiple classes that can follow multiple paths
from the root class, then the task is called hierarchical multi-label classiﬁcation (HMC) . This is the setting we
use in this article.
We formally deﬁne the task of hierarchical multi-label classiﬁcation as follows:
• A description space X that consists of tuples of values of primitive data types (discrete or continuous), i.e.,
∀Xi ∈X, Xi = (xi1, xi2, ..., xiD), where D is the size of the tuple (or number of descriptive variables),
• a target space S , deﬁned with a class hierarchy (C, ≤h), where C is a set of classes and ≤h is a partial order (e.g.,
structured as a rooted tree) representing the superclass relationship (∀c1, c2 ∈C : c1 ≤h c2 if and only if c1 is a
superclass of c2),
• a set E, where each example is a pair of a tuple and a set, from the descriptive and target space respectively, and
each set satisﬁes the hierarchy constraint, i.e., E = {(Xi, S i)|Xi ∈X, S i ⊆C, c ∈S i ⇒∀c′ ≤h c : c′ ∈S i, 1 ≤i ≤
N} and N is the number of examples in E (N = |E|), and
• a quality criterion q, which rewards models with high predictive accuracy and low complexity.
Find: a function f : X →2C (where 2C is the power set of C) such that f maximizes q and c ∈f(x) ⇒∀c′ ≤h c : c′ ∈
f(x), i.e., predictions made by the model satisfy the hierarchy constraint. In our case, the function f is represented
with decision trees, i.e., predictive clustering trees or ensembles thereof.
3. Tree ensembles for predicting structured outputs
In this section, we present our ensemble methods for predicting structured outputs. We begin by presenting
predictive clustering trees and their instantiations for predicting multiple continuous variables, predicting multiple
discrete variables, and hierarchical multi-label classiﬁcation. Next, we describe how ensemble learning methods
can be adapted to use predictive clustering trees as base predictive models. Finally, we describe an approach to the
prediction of structured outputs that uses local predictive models.
D. Kocev et al. / Pattern Recognition 00 1–26
3.1. PCTs for structured outputs
The Predictive Clustering Trees (PCTs) framework views a decision tree as a hierarchy of clusters: the top-node
corresponds to one cluster containing all data, which is recursively partitioned into smaller clusters while moving
down the tree. The PCT framework is implemented in the CLUS system , which is written in Java and is opensource software licensed under the GNU General Public Licence. The CLUS system is available for download at
 
CLUS takes as input a set of examples E = {(xi, yi)|i = 1, ...N}, where each xi is a vector of attribute values and
yi are values of a structured (output) datatype TY. In this article, we consider three diﬀerent classes of datatypes TY:
tuples of discrete values, tuples of real values, and hierarchies. For each type TY, CLUS needs two functions to be
deﬁned. The prototype function returns a representative structured value given a set of such structured values. For
example, given a set of tuples of discrete variables, the prototype function computes and returns a tuple of discrete
variables that is representative for the whole set. The variance function describes how homogeneous a set of structured
values is: It is typically based on a distance function on the space of structured values.
PCTs can be induced with a standard top-down induction of decision trees (TDIDT) algorithm . The algorithm
is presented in Table 1. It takes as input a set of examples (E) and outputs a tree. The heuristic (h) that is used for
selecting the tests (t) is the reduction in variance caused by partitioning (P) the instances (see line 4 of the BestTest
procedure in Table 1). By maximizing the variance reduction, the cluster homogeneity is maximized and the predictive
performance is improved.
Table 1: The top-down induction algorithm for PCTs.
procedure PCT
Input: A dataset E
Output: A predictive clustering tree
1: (t∗, h∗, P∗) = BestTest(E)
2: if t∗, none then
for each Ei ∈P∗do
treei = PCT(Ei)
return node(t∗, S
return leaf(Prototype(E))
procedure BestTest
Input: A dataset E
Output: the best test (t∗), its heuristic score (h∗) and the
partition (P∗) it induces on the dataset (E)
1: (t∗, h∗, P∗) = (none, 0, ∅)
2: for each possible test t do
P = partition induced by t on E
h = Var(E) −P
|E| Var(Ei)
if (h > h∗) ∧Acceptable(t, P) then
(t∗, h∗, P∗) = (t, h, P)
7: return (t∗, h∗, P∗)
The main diﬀerence between the algorithm for learning PCTs and a standard decision tree learner (i.e., the C4.5
algorithm ) is that the former considers the variance function and the prototype function, that computes a label for
each leaf, as parameters that can be instantiated for a given learning task. So far, PCTs have been instantiated for the
following tasks: multi-target prediction , hierarchical multi-label classiﬁcation and prediction of time-series
 . In this article, we focus on the ﬁrst two tasks.
3.1.1. PCTs for predicting multiple target variables
PCTs that are able to predict multiple targets simultaneously are called multi-target decision trees (MTDTs). The
MTDTs that predict a tuple of continuous variables (regression tasks) are called multi-target regression trees (MTRTs),
while the MTDTs that predict a tuple of discrete variables are called multi-target classiﬁcation trees (MTCTs). The
instantiation of the CLUS system that learns multi-target trees is called CLUS-MTDT.
The variance and prototype functions for MTRTs are instantiated as follows. The variance is calculated as the
sum of the variances of the target variables, i.e., Var(E) = PT
i=1 Var(Yi). The variances of the target variables are
normalized, so that each target variable contributes equally to the overall variance. This is due to the fact that the
target variables can have completely diﬀerent ranges. Namely, if one of the target variables is in the range (0, 1) and
another in the range (10, 100) and normalization is not used, then the values of the second variable will contribute
much more to the overall score than the values of the ﬁrst variable. In addition, CLUS-MTDT supports weighting of
the (normalized values of the) target variables so that the variance function gives more weight to some variables and
D. Kocev et al. / Pattern Recognition 00 1–26
less to others. The prototype function (calculated at each leaf) returns as a prediction the tuple with the mean values
of the target variables, calculated by using the training instances that belong to the given leaf.
The variance function for the MTCTs is computed as the sum of the Gini indices of the target variables, i.e.,
Var(E) = PT
i=1 Gini(E, Yi). Furthermore, one can also use the sum of the entropies of class variables as a variance
function, i.e., Var(E) = PT
i=1 Entropy(E, Yi) (this deﬁnition has also been used in the context of multi–label prediction
 ). The CLUS system also implements other variance functions, such as reduced error, gain ratio and the mestimate. The prototype function returns a vector of probabilities that an instance belongs to a given class for each
target variable. Using these probabilities, the most probable (majority) class for each target attribute can be calculated.
3.1.2. PCTs for hierarchical multi–label classiﬁcation
Hierarchical multi-label classiﬁcation is a variant of classiﬁcation where a single example may belong to multiple
classes at the same time and the classes are organized in a form of hierarchy. An example that belongs to some class c
automatically belongs to all super-classes of c: This is called the hierarchical constraint. Problems of this kind can be
found in many domains including text classiﬁcation, functional genomics, and object/scene classiﬁcation. Silla and
Freitas give a detailed overview of the possible application areas and the available approaches to HMC.
Silla and Freitas describe the algorithms for hierarchical classiﬁcation with a 4-tuple ⟨∆, Σ, Ω, Θ⟩. In this 4tuple, ∆indicates whether the algorithm makes predictions for a single or multiple paths in the hierarchy, Σ is the
depth of the predicted classes, Ωis the taxonomy structure of the classes that the algorithm can handle, and Θ is the
type of the algorithm (local or global). Using this categorization, the algorithm we present here can be described as
• ∆= multiple path prediction: the algorithm can assign single or multiple paths, i.e., predicted classes, to each
• Σ = non-mandatory leaf-node prediction: an instance can be labelled with a label at any level of the taxonomy.
• Ω= tree or directed acyclic graph: the algorithm can handle both tree-shaped and DAG hierarchies of classes.
• Θ = global classiﬁer: the algorithm constructs a single model valid for all classes.
CLUS-HMC is the instantiation (with the distances and prototypes as deﬁned below) of the PCT algorithm for
hierarchical classiﬁcation implemented in the CLUS system. The variance and prototype are deﬁned as follows .
First, the set of labels of each example is represented as a vector with binary components; the i’th component of the
vector is 1 if the example belongs to class ci and 0 otherwise. It is easily checked that the arithmetic mean of a set of
such vectors contains as i’th component the proportion of examples of the set belonging to class ci.
The variance of a set of examples E is deﬁned as the average squared distance between each example’s class vector
(Li) and the set’s mean class vector (L), i.e.,
Var(E) = 1
d(Li, L)2.
In the HMC context, the similarity at higher levels of the hierarchy is more important than the similarity at lower
levels. This is reﬂected in the distance measure used in the above formula, which is a weighted Euclidean distance:
d(L1, L2) =
w(cl) · (L1,l −L2,l)2,
where Li,l is the l’th component of the class vector Li of an instance Ei, |L| is the size of the class vector, and the class
weights w(c) decrease with the depth of the class in the hierarchy. More precisely, w(c) = w0 · {w(p(c))}, where p(c)
denotes the parent of class c and 0 < w0 < 1).
For example, consider the toy class hierarchy shown in Figure 1(a,b), and two data examples: (X1, S 1) and (X2, S 2)
that belong to the classes S 1 = {c1, c2, c2.2} (boldface in Figure 1(b)) and S 2 = {c2}, respectively. We use a vector
D. Kocev et al. / Pattern Recognition 00 1–26
representation with consecutive components representing membership of class c1, c2, c2.1, c2.2 and c3, in that order
(preorder traversal of the tree of class labels). The distance is then calculated as follows:
d(S 1, S 2) = d( , ) =
(1)(2)(3)(4)(5)
Lk = 
Figure 1: Toy examples of hierarchies structured as a tree and a DAG.(a) Class label names contain information about the position in the hierarchy,
e.g., c2.1 is a subclass of c2. (b) The set of classes S 1 = {c1, c2, c2.2}, shown in bold in the hierarchy, represented as a vector (Lk). (c) A class
hierarchy structured as a DAG. The class c6 has two parents: c1 and c4.
This example calculates the distance between two data instances that belong to classes organized into a tree-shaped
hierarchy. As discussed at the beginning of this section, the proposed methods also support hierarchies in the form of
a DAG. Figure 1(c) depicts an example of a DAG structured hierarchy. The main diﬀerence between tree-shaped and
DAG hierarchies is that the classes can have multiple parent classes at diﬀerent levels/depths in the hierarchy. Hence,
the depth of a class is not unique: classes do not have a single path from the top-node (for example see class c6 in
Figure 1(c)). This makes it impossible to calculate the value of the weight as w(c) = w0 · {w(p(c))}. To resolve this,
several approaches have been proposed to compute the weight of a class with multiple parents:
• Flattening the DAG into a tree by copying the subtrees that have multiple parents (w(c) = w0 · P
j{w(pj(c))},
where p(c) denotes the parent of class c). The more paths in DAG lead to a class, the larger weight is assigned
to this class by this method.
• Take the weight of the parent with largest depth (w(c) = w0 · minj {w(p j(c))}). The drawback of this approach
is that assigns a small weight to a class with multiple parents that appear both close to the top-level and deep in
the hierarchy.
• Take the weight of the parent with smallest depth (w(c) = w0 · maxj {w(pj(c))}). It guarantees a large weight
for classes that appear near the top-level of the hierarchy, however it does not satisfy the inequality w(c) <
w(parj(c)).
• Take the average weight of all parents (w(c) = w0 · avgj {w(p j(c))}). This approach is a compromise between
the previous two approaches.
Note that all these weighting schemes become equivalent for tree-shaped hierarchies. Vens et al. perform an
extensive experimental evaluation of the weighting schemes and recommend to use as a weight of a given class the
average over the weights of all its parents (i.e., w(c) = w0 · avgj{w(parj(c))}). We thus use the average weighting
scheme in this study.
D. Kocev et al. / Pattern Recognition 00 1–26
Recall that the instantiation of PCTs for a given task requires proper instantiation of the variance and prototype
functions. The variance function for the HMC task is instantiated by using the weighted Euclidean distance measure
(as given above), which is further used to select the best test for a given node by calculating the heuristic score (line 4
from the algorithm in Table 1). We now discuss the instantiation of the prototype function for the HMC task.
A classiﬁcation tree stores in a leaf the majority class for that leaf, which will be the tree’s prediction for all
examples that will arrive in the leaf. In the case of HMC, an example may have multiple classes, thus the notion of
majority class does not apply in a straightforward manner. Instead, the mean ¯L of the class vectors of the examples in
the leaf is stored as a prediction. Note that the value for the i-th component of ¯L can be interpreted as the probability
that an example arriving at the given leaf belongs to class ci.
The prediction for an example that arrives at the leaf can be obtained by applying a user deﬁned threshold τ to
the probability; if the i-th component of ¯L is above τ then the examples belong to class ci. When a PCT is making a
prediction, it preserves the hierarchy constraint (the predictions comply with the parent-child relationships from the
hierarchy) if the values for the thresholds τ are chosen as follows: τi ≤τj whenever ci ≤h cj (ci is ancestor of cj). The
threshold τ is selected depending on the context. The user may set the threshold such that the resulting classiﬁer has
high precision at the cost of lower recall or vice versa, to maximize the F-score, to maximize the interpretability or
plausibility of the resulting model etc. In this work, we use a threshold-independent measure (precision-recall curves)
to evaluate the performance of the HMC models.
3.2. Ensembles of PCTs for predicting structured outputs
An ensemble is a set of predictive models (called base predictive models). In homogeneous ensembles, such as
the ones we consider here, the base predictive models are constructed by using the same algorithm. The prediction
of an ensemble for a new instance is obtained by combining the predictions of all base predictive models from the
ensemble. In this article, we consider ensembles of PCTs for structured prediction. The PCTs in the ensembles are
constructed by using the bagging and random forests methods that are often used in the context of decision trees. We
have adapted these methods to use PCTs.
3.2.1. Constructing ensembles of PCTs
A necessary condition for an ensemble to have better predictive performance than any of its individual members
is that the base predictive models are accurate and diverse . However, there is an ongoing scientiﬁc debate
concerning the trade-oﬀbetween the accuracy and the diversity of the base predictive models in the context of the
ensembles’ predictive performance . An accurate predictive model does better than random guessing on new
examples. Two predictive models are diverse if they make diﬀerent errors on new examples. There are several ways to
introduce diversity in a set of base predictive models: by manipulating the training set (by changing the weight of the
examples , by changing the attribute values of the examples , by manipulating the feature space )
and by manipulating the learning algorithm itself .
We have implemented the bagging and random forests methods within the CLUS system. These two ensemble
learning techniques are most widely known and have primarily been used in the context of decision trees . The
algorithms of these ensemble learning methods are presented in Table 2. For the random forests method (Table 2,
right), the PCT algorithm for structured prediction was changed to PCT rnd: randomized version of the selection of
attributes was implemented, which replaced the standard selection of attributes.
3.2.2. Bagging
Bagging is an ensemble method that constructs the diﬀerent classiﬁers by making bootstrap replicates of the
training set and using each of these replicates to construct a predictive model (Table 2, left). Each bootstrap sample
is obtained by randomly sampling training instances, with replacement, from the original training set, until an equal
number of instances as in the training set is obtained. Breiman showed that bagging can give substantial gains in
predictive performance, when applied to an unstable learner (i.e., a learner for which small changes in the training set
result in large changes in the predictions), such as classiﬁcation and regression tree learners.
D. Kocev et al. / Pattern Recognition 00 1–26
Table 2: The ensemble learning algorithms: bagging and random forests. Here, E is the set of the training examples, k is the number of trees in the
forest, and f(D) is the size of the feature subset considered at each node during tree construction for random forests.
procedure Bagging(E, k)
returns Forest
2: for i = 1 to k do
Ei = bootstrap(E)
Ti = PCT(Ei)
F = F S{Ti}
6: return F
procedure RForest(E, k, f(D))
returns Forest
2: for i = 1 to k do
Ei = bootstrap(E)
Ti = PCT rnd(Ei, f(D))
F = F S{Ti}
6: return F
3.2.3. Random forests
A random forest is an ensemble of trees, where diversity among the predictors is obtained by using bootstrap
replicates as in bagging, and additionally by changing the set of descriptive attributes during learning (Table 2, right).
More precisely, at each node in the decision trees, a random subset of the descriptive attributes is taken, and the
best attribute is selected from this subset. The number of attributes that are retained is given by a function f of the
total number of descriptive attributes D (e.g., f(D) = 1, f(D) = ⌊
D + 1⌋, f(D) = ⌊log2(D) + 1⌋...). By setting
f(D) = D, we obtain the bagging procedure. The algorithm for learning a random forest using PCTs as base classiﬁers
is presented in Table 2.
3.2.4. Combining the predictions of individual PCTs
The prediction of an ensemble for a new instance is obtained by combining the predictions of all the base predictive
models from the ensemble. The predictions from the models can be combined by taking the average (for regression
tasks) and the majority or probability distribution vote (for classiﬁcation tasks), as described in , or by taking
more complex aggregation schemes .
We use PCTs as base predictive models for the ensembles for structured outputs. To obtain a prediction from an
ensemble for predicting structured outputs, we accordingly extend the voting schemes. For the datasets with multiple
continuous targets, as the prediction of the ensemble, we take the average per target of the predictions of the base
classiﬁers. For the datasets for hierarchical classiﬁcation we also use the average of the predictions and apply the
thresholding described in Section 3.1.2. We obtain the ensemble predictions for the datasets with multiple discrete
targets using probability distribution voting (as suggested by Bauer and Kohavi ) per target.
3.3. Local prediction of structured outputs with PCTs and ensembles
The presented structured output learning algorithms (CLUS-MTDT and CLUS-HMC) belong to the group of
methods known as ‘big-bang’ or global predictive models . Global predictive models make a single prediction
for the entire structured output, i.e., simultaneously predict all of its components. Local predictive models of structured
outputs, on the other hand, use a collection of predictive models, each predicting a component of the overall structure
that needs to be predicted.
The local predictive models for the task of predicting multiple targets are constructed by learning a predictive
model for each of the targets separately. In the task of hierarchical multi-label classiﬁcation, however, there are four
diﬀerent approaches that can be used: ﬂat classiﬁcation, local classiﬁers per level, local classiﬁers per node, and local
classiﬁers per parent node (see for details).
Vens et al. investigated the performance of the last two approaches with local classiﬁers over a large collection
of datasets from functional genomics. The conclusion of the study was that the last approach (called hierarchical
single-label classiﬁcation - HSC) performs better in terms of predictive performance, smaller total model size and
faster induction times.
In particular, the CLUS-HSC algorithm by Vens et al. , presented in Figure 2(a), constructs a decision tree
classiﬁer for each edge (connecting a class c with a parent class par(c)) in the hierarchy, thus creating an architecture
of classiﬁers. The corresponding tree predicts membership to class c, using the instances that belong to par(c). The
construction of this type of trees uses few instances, as only instances labeled with par(c) are used for training. The
D. Kocev et al. / Pattern Recognition 00 1–26
instances labeled with class c are positive instances, while the ones that are labeled with par(c), but not with c are
Figure 2: An illustration of the hierarchical single-label classiﬁcation approach used by Vens et al. . The local classiﬁers at each branch from
the hierarchy are: (a) decision trees and (b) ensembles of decision trees.
The resulting HSC tree predicts the conditional probability P(c|par(c)). A new instance is predicted by recursive
application of the product rule P(c) = min j P(c | parj(c)) · P(parj(c)) (with par j(c) denoting the j-th parent of c in
the case of a DAG), starting from the tree for the top-level class. Again, the probabilities are thresholded to obtain
the set of predicted classes. To satisfy the hierarchy constraint, the threshold τ should be chosen as in the case of
In this article, we extend the approach of Vens et al. by applying ensembles as local classiﬁers, instead of
single decision trees. The CLUS-HSC algorithm can be applied to ensemble learning in two ways: by constructing
an ensemble of architectures or an architecture of ensembles. The ﬁrst approach creates the ensemble by creating
multiple architectures (similar to the one shown in Figure 2(a)). These multiple architectures can be created on
diﬀerent bootstrap replicates, on diﬀerent feature spaces, by diﬀerent local classiﬁers etc. The second approach is
simpler and, instead of a single local classiﬁer (for example a decision tree), uses an ensemble as a classiﬁer at
each branch (depicted in Figure 2(b)). We prefer here the second approach since it is closer to the learning of local
classiﬁers for predicting multiple target variables.
3.4. Computational complexity
In this section, we analyze and discuss the computational complexity aspects of the proposed methods. We begin
by deriving the computational complexity of a single tree. We then analyze the computational complexity of a single
ensemble. Next, we discuss the computational complexity of the trees and the ensembles for local prediction of
structured outputs. After that, we discuss the computational complexity of the trees and the ensembles for global
prediction of structured outputs. Finally, we compare the computational complexities of the models for local and
global prediction of the structured outputs.
We assume that the training set contains N instances and D descriptive attributes (of which M are continuous).
Furthermore, S is the size of the output measured as number of target variables or the number of classes in the
hierarchy and the hierarchy contains G edges. The ensembles contain k base predictive models.
Single tree Three procedures contribute to the computational complexity of the tree learning algorithm described in
Table 1. The procedures are executed at each node of the tree and they include: sorting the values of the M numeric
descriptive attributes with a cost of O(MN log N), calculating the best split for a single target variable which costs
O(DN), and applying the split to the training instances with a cost of O(N). Furthermore, we assume, as in ,
that the tree is balanced and bushy. This means that the depth of the tree is in the order of log N, i.e., O(log N).
D. Kocev et al. / Pattern Recognition 00 1–26
Having this in mind and considering that M = O(D), the total computational cost of constructing a single tree is
O(DN log2 N) + O(DN log N) + O(N log N).
Single ensemble The computational complexity of constructing an ensemble depends on the computational complexity of constructing its base predictive models. In general, the computation complexity of learning an ensemble with
k base predictive models is k times higher than the computational complexity of learning a single base predictive
model. Thus, the overall computational complexity of an ensemble method can be calculated as k(O(DN log2 N) +
O(DN log N) + O(N log N)). However, ensemble methods perform sampling of the instances and/or the features, thus
potentially reducing the computational complexity of constructing a single base predictive model (by a constant factor) as compared to the construction of a base predictive model without instance/feature sampling. The creation of the
bootstrap replicates of the training set for bagging and random forests has a computational complexity of O(N), while
the number of instances used to train the base predictive models is not N, but N′ = 0.632· N . The random forests,
in addition to the bootstrap sampling of the instances, also perform random sampling of the features with a selection
function f(D), i.e., the number of descriptive attributes (considered at each step) is D′ = f(D). The random sampling
of the features at each node costs O(D′ log N′) Considering this, the overall computational complexity of constructing
a random forest is k(O(D′N′ log2 N′) + O(D′N′ log N′) + O(N′ log N′) + O(N) + O(D′ log N′)).
Local models We extend this analysis for local prediction of structured outputs. For the task of predicting multiple
targets, a model (a tree or an ensemble) is constructed for each target separately, thus the computational complexity
is S times higher than the computational complexity of a model for a single target attribute. For the HMC task, a
model is constructed for each edge in the hierarchy (see Figure 2). Hence, the computational complexity of this
architecture of models is at most G times higher than the computational complexity of a single model. However, the
computational complexity of an architecture of models also depends on the average number of classes per instance
and the average number of leaf classes per instance. Smaller average numbers of classes and leaf classes per instance
lead to smaller sets of training instances for the local models and, consequently, smaller computational complexity.
In typical applications, the computational complexity of this approach is smaller than G times the complexity of
constructing a single model.
Global models The derivation of the computational complexity of constructing PCTs for global prediction of structured outputs follows the same pattern as for a single PCT for local prediction. The diﬀerence here is in the procedure
for calculating the best split at a given node. This procedure, instead of a computational complexity of O(DN), for the
task of predicting structured outputs has a computational complexity of O(S DN). The computational complexity for
the construction of the complete tree is then as follows: O(DN log2 N)+O(S DN log N)+O(N log N). The construction
of an ensemble that consists of k PCTs for global prediction costs k times more than the construction of a single PCT.
As for the ensembles for local prediction, the ensemble methods reduce the computational cost with the selection of
instances and/or features.
Comparison of local and global models We further compare the computational complexity of local and global
models (both PCTs and ensembles) for prediction of structured outputs. The dominant terms in the computational
complexity of local models (a set of PCTs) for multiple targets is O(S DN log2 N) and local models (a set of PCTs)
for HSC is O(GDN log2 N). Let us assume that S < log N. This means that the dominant term in the computational
complexity of global models for both multiple targets and HSC is O(DN log2 N). Considering this, global models
have O(S ) and O(G) times lower computational complexity than local models for predicting multiple targets and
HMC, respectively. On the other hand, if we assume that S > log N, then the dominant term in the computational
complexity of global models is O(S DN log N). In this case, global models have O(log N) or O(G
S log N) times lower
computational complexity than local models for predicting multiple targets or HMC, respectively. Moreover, for the
task of HMC, if the target hierarchy is tree-shaped then G = S −1, i.e., G ≈S and the global models have O(log N)
times smaller computational complexity than local models. Furthermore, when the target hierarchy is a DAG, the
computational advantage of the global models depends of the average number of parents that classes have. If the
classes have more parents on average, the G
S ratio will be larger and the computational complexity of the global
models will be lower.
Global models have lower computational complexity than local models mainly due to the multiple repetitions of
the sorting of the numeric attributes in the latter. However, the diﬀerence in the computational complexity is further
ampliﬁed by the fact that global models are smaller, on average, than the local models. Also, implementation-wise,
global models are faster to construct also because of some properties of most CPU architectures. Namely, it is faster
and easier to multiply/add/subtract two arrays with size S , than to multiply/add/subtract S times two arrays with size
D. Kocev et al. / Pattern Recognition 00 1–26
Random forests are very eﬃcient ensemble methods and have very good predictive performance. Thus, we discuss
in more detail the computational complexity of random forests for global prediction of structured outputs. The upper
bound of the computational complexity of global random forests is k(O(D′N′ log2 N′) + O(S D′N′ log N′)). The complexity of global random forests depends linearly on the number of base predictive models (O(k)) and logarithmically
(or via some other user deﬁned function that determines the number of features sampled) on the number of numeric
descriptive attributes (O(D′) = O(log D)). Furthermore, if S > log N then the complexity will depend linearly on the
size of the structured output and O(N′ log2 N′) on the number of training instances. If S < log N, then the complexity
of the random forests depends O(N′ log N′2) on the number of training instances, and not on the size of the structured
The computational eﬃciency of the method for predicting structured outputs proposed in G¨artner and Vembu ,
one of the most recent and most eﬃcient methods for predicting structured outputs, depends polynomially on the size
of the structured outputs and the number of training instances. On the other hand, the complexity of global random
forests, depends linearly on the number of base predictive models (typically the ensembles have at most hundreds of
base predictive models), logarithmically on the number of continuous descriptive attributes and N log N on the number
of training instances. Furthermore, if the size of the output is larger than log N then the computational complexity
will depend linearly on the output size. To summarize, global random forests (and global ensembles, in general),
according to the above analyses of their computation complexity, are very eﬃcient methods for predicting structured
4. Experimental design
In this section, we describe the procedure for experimental evaluation of the proposed ensemble methods for
predicting structured outputs. First, we state the questions we consider. Next, we present the datasets we use to
evaluate the algorithms, and then the evaluation measures we applied. In the last subsection, we give the parameter
values used in the algorithms and the statistical tests that we used.
4.1. Experimental questions
Given the methodology from Section 3, we construct several types of trees and ensembles thereof. First, we
construct PCTs that predict components of the structured output: a separate tree for each variable from the target
tuple (predicting multiple targets) and a separate tree for each hierarchy edge (hierarchical classiﬁcation). Second, we
learn PCTs that predict the entire structured output simultaneously: a tree for the complete target tuple and a tree for
the complete hierarchy, respectively. Finally, we construct the ensemble models in the same manner by using both
bagging and random forests.
We consider three aspects of constructing tree ensembles for predicting structured outputs: predictive performance, convergence and eﬃciency. We ﬁrst assess the predictive performance of global and local tree ensembles and
investigate whether global and local ensembles have better predictive performance than the respective single model
counterparts. Moreover, we check whether the exploitation of the structure of the output can lift the predictive performance of an ensemble (i.e., global versus local ensembles). Next, we investigate the saturation/convergence of the
predictive performance of global and local ensembles with respect to the number of base predictive models they consist of. Namely, we inspect the predictive performance of the ensembles at diﬀerent ensemble sizes (i.e., we construct
saturation curves). The goal is to check which type of ensembles, global or local, saturates at a smaller number of
trees. Finally, we assess the eﬃciency of both global and local single predictive models and ensembles thereof by
comparing the running times for and the sizes of the models obtained by the diﬀerent approaches.
4.2. Descriptions of the datasets
In this section, we present the datasets that were used to evaluate the performance of the ensembles. The datasets
are divided into three groups based on the type of their output: multiple continuous targets datasets (regression),
multiple discrete targets datasets (classiﬁcation) and hierarchical multi-label classiﬁcation datasets (HMC). Statistics
about the used datasets are presented in Tables 3, 4, and 5, respectively.
D. Kocev et al. / Pattern Recognition 00 1–26
Table 3: Properties of the datasets with multiple continuous targets (regression datasets); N is the number of instances, D/C the number of
descriptive attributes (discrete/continuous), and T the number of target attributes.
Name of dataset
Collembola 
Forestry-Kras 
Forestry-Slivnica-LandSat 
Forestry-Slivnica-IRS 
Forestry-Slivnica-SPOT 
Sigmea real 
Soil quality 
Solar-ﬂare 1 
Solar-ﬂare 2 
Vegetation Clustering 
Vegetation Condition 
Water quality 
Table 4: Properties of the datasets with multiple discrete targets (classiﬁcation datasets); N is the number of instances, D/C the number of
descriptive attributes (discrete/continuous), and T the number of target attributes.
Name of dataset
Emotions 
Mediana 
Scene 
Sigmea real 
Solar-ﬂare 1 
Thyroid 
Water quality 
Yeast 
The datasets with multiple continuous targets (13 in total, see Table 3) are mainly from the domain of ecological
modelling. The datasets with multiple discrete targets (9 in total, see Table 4) are from various domains: ecological
modelling (Sigmea Real and Water Quality), biology (Yeast), multimedia (Scene and Emotions) and media space
analysis (Mediana). The datasets that have classes organized in a hierarchy come from various domains, such as:
biology (Expression-FunCat, SCOP-GO, Yeast-GO and Sequence-FunCat), text classiﬁcation (Enron, Reuters and
WIPO) and image annotation/classiﬁcation (ImCLEF07D, ImCLEF07A and Diatoms). Hence, we use 10 datasets
from 3 domains (see Table 5). Note that two datasets from the biological domain have a hierarchy organized as a
DAG (they have GO in the dataset name), while the remaining datasets have tree-shaped hierarchies. For more details
on the datasets, we refer the reader to the referenced literature.
4.3. Evaluation measures
Empirical evaluation is the most widely used approach for assessing the performance of machine learning algorithms. The performance of a machine learning algorithm is assessed using some evaluation measure. The diﬀerent
machine learning tasks, described in Section 2, use ‘task-speciﬁc’ evaluation measures. We ﬁrst describe the evaluation measures for multiple continuous targets (regression), then for multiple discrete targets (classiﬁcation) and at the
end for hierarchical classiﬁcation.
For the task of predicting multiple continuous targets (regression), we employed three well known measures: the
correlation coeﬃcient (CC), root mean squared error (RMS E) and relative root mean squared error (RRMS E). For
D. Kocev et al. / Pattern Recognition 00 1–26
Table 5: Properties of the datasets with hierarchical targets; Ntr/Nte is the number of instances in the training/testing dataset, D/C is the number of
descriptive attributes (discrete/continuous), |H| is the number of classes in the hierarchy, Hd is the maximal depth of the classes in the hierarchy, L
is the average number of labels per example, and LL is the average number of leaf labels per example. Note that the values for Hd are not always a
natural number because the hierarchy has a form of a DAG and the maximal depth of a node is calculated as the average of the depths of its parents.
ImCLEF07D 
10000/1006
ImCLEF07A 
10000/1006
Diatoms 
Enron 
Reuters 
Expression–FunCat 
SCOP-GO 
Sequence-FunCat 
Yeast-GO 
each of these measures, we performed tests for statistical signiﬁcance and constructed saturation curves. We present
here only the results in terms of RRMS E, but same conclusions hold for the other two measures.
What evaluation measure to use in the case of classiﬁcation algorithms is not as clear as in the case of regression.
Sokolova and Lapalme conducted a systematic analysis of twenty four performance measures that can be used in
a classiﬁcation context. They conclude that evaluation measures for classiﬁcation algorithms should be chosen based
on the application domain.
In our study, we used seven evaluation measures for classiﬁcation: accuracy, precision, recall, F-score, the
Matthews correlation coeﬃcient, balanced accuracy (also known as Area Under the Curve) and discriminant power.
We used two averaging approaches to adapt these measures for multi-class problems: micro and macro averaging
(note that averaging is not needed for accuracy). The formulas for calculating the evaluation measures are given in
Appendix A. Since the goal of this study is not to assess the evaluation measures themselves, we present here only
the results in terms of the micro average F-score (F = 2 · Precision·Recall
Precision+Recall). However, the conclusions drawn from the
evaluation of the performance of the algorithms using the other measures concur with the ones presented here.
In the case of hierarchical classiﬁcation, we evaluate the algorithms using the Area Under the Precision-Recall
Curve (AUPRC), and in particular, the Area Under the Average Precision-Recall Curve (AUPRC) as suggested by
Vens et al. . A Precision-Recall curve plots the precision of a classiﬁer as a function of its recall. The points in the
PR space are obtained by varying the value for the threshold τ from 0 to 1 with step 0.02. The precision and recall are
micro averaged for all classes from the hierarchy.
Finally, we compare the algorithms by measuring their eﬃciency in terms of time consumption and size of the
models. We measure the processor time needed to construct the models: in the case of predicting the components of
the structure, we sum the times needed to construct the separate models. In a similar way, we calculated the sizes of
the models as the total number of nodes (internal nodes and leafs). The experiments for predicting multiple targets
were performed on a server running Linux, with two Intel Quad-Core Processors running at 2.5GHz and 64GB of
RAM. The experiments for the hierarchical classiﬁcation were run on a cluster of AMD Opteron processors (1.8 –
2.4GHz, ≥2GB RAM).
4.4. Experimental setup
Here, we ﬁrst state the parameter values used in the algorithms for constructing the single trees and the ensembles
for all types of targets. We then describe how we assessed the statistical signiﬁcance of the diﬀerences in performance
of the studied algorithms.
The single trees for all types of outputs are obtained using F-test pruning. This pruning procedure uses the exact
Fisher test to check whether a given split/test in an internal node of the tree results in a reduction in variance that
is statistically signiﬁcant at a given signiﬁcance level. If there is no split/test that can satisfy this, then the node is
D. Kocev et al. / Pattern Recognition 00 1–26
converted to a leaf. An optimal signiﬁcance level was selected by using internal 3-fold cross validation, from the
following values: 0.125, 0.1, 0.05, 0.01, 0.005 and 0.001.
The construction of an ensemble takes, as an input parameter, the size of the ensemble, i.e., number of base
predictive models to be constructed. We constructed ensembles with 10, 25, 50, 75 and 100 base predictive models
for all types of outputs and all datasets. In addition, for the datasets with multiple continuous targets we constructed
ensembles with 150 and 250 base predictive models, and for the datasets with multiple discrete targets ensembles with
250, 500 and 1000 base predictive models. Following the ﬁndings from the study conducted by Bauer and Kohavi
 , the trees in the ensembles were not pruned.
The random forests algorithm takes as input the size of the feature subset that is randomly selected at each node.
For the multiple targets datasets, we apply the logarithmic function of the number of descriptive attributes ⌊log2 |D|⌋+1,
which is recommended by Breiman . For the hierarchical classiﬁcation datasets, we used ⌊0.1 · |D|⌋+ 1, since
the feature space of some of these datasets is large (several thousands of features, see Table 5) and the logarithmic
function is under-sampling the feature space (e.g., it will select 14 attributes from 10000 descriptive attributes).
On the datasets with multiple targets, the predictive performance of the algorithms is estimated by 10-fold crossvalidation. The hierarchical datasets were previously divided (by the data providers) into train and test sets. Thus, we
estimate the predictive performance of the algorithms on the test sets.
We adopt the recommendations by Demˇsar for the statistical evaluation of the results. We use the Friedman
test for statistical signiﬁcance with the correction from Iman and Davenport . Afterwards, to check where the
statistically signiﬁcant diﬀerences appear (between which algorithms), we use the Nemenyi post-hoc test . We
present the results from the statistical analysis with average ranks diagrams . The diagrams plot the average ranks
of the algorithms and connect the ones whose average ranks are smaller than a given value, called critical distance. The
critical distance depends on the level of the statistical signiﬁcance, in our case 0.05. The diﬀerence in the performance
of the algorithms connected with a line is not statistically signiﬁcant at the given signiﬁcance level.
5. Results and discussion
The results from the experiments can be analyzed along several dimensions. First, we present the saturation
curves of the ensemble methods (both for predicting the structured outputs and the components of the outputs). We
also compare single trees vs. ensembles of trees. Next, we compare models that predict the complete structured
output vs. models that predict components of the structured output. Finally, we evaluate the algorithms by their
eﬃciency in terms of running time and model size. We perform these comparisons for each task separately: multitarget regression, multi-target classiﬁcation and hierarchical multi–label classiﬁcation. We conclude the section with
a general discussion of the experimental results.
5.1. Multi-target regression
In Figure 3, we present the saturation curves for the ensemble methods for multi-target regression. Although these
curves are averaged across all target variables for a given dataset, they still provide useful insight into the performance
of the algorithms. Figure 3 (a) and (b) present the saturation curves for two speciﬁc datasets, while Figure 3 (c)
presents curves averaged across all datasets. We have checked at which ensemble size (saturation point) the RRMSE
no longer statistically signiﬁcantly changes, i.e., when adding trees to the ensemble does not increase the predictive
performance signiﬁcantly. For all algorithms, the diﬀerences are not statistically signiﬁcant after 50 trees are added.
Thus, in the remainder of the analysis, we use ensembles of 50 trees are added to the ensembles.
The statistical test in Figure 4 shows that the diﬀerences in predictive performance among the diﬀerent ensemble
methods are not statistically signiﬁcant at the level of 0.05. For most of the datasets, the best performing method is
random forests for predicting multiple targets. However, if we look at the saturation curves in Figure 3 (c), we note
that, on average, multiple target bagging is best. A closer look at the results shows that, especially for the larger
datasets (e.g., Forestry-Kras from Figure 3(a)), random forests tend to outperform bagging, both for the local and
global algorithms. The diﬀerence in performance between ensembles and single PCTs is statistically signiﬁcant. The
PCTs for predicting multiple targets simultaneously are better (though not signiﬁcantly) than the trees for predicting
multiple targets separately.
Finally, we compare the algorithms by their running time and the size of the models for ensembles of 50 trees (see
Figure 5). The statistical tests show that, in terms of the time eﬃciency, random forests for multi-target regression
D. Kocev et al. / Pattern Recognition 00 1–26
Size of ensemble
Size of ensemble
Size of ensemble
(a) Forestry-Kras
(b) Soil quality
(c) Overall
Figure 3: Saturation curves for the diﬀerent ensemble approaches to the prediction of multiple continuous targets. The curves (a) and (b) are
obtained by averaging the RRMS E values over all of the target variables in a dataset, while the curve (c) by averaging the RRMS E values over all
of the target variables in all datasets. Smaller RRMS E values mean better predictive performance. Note that the scale of the y-axis is adapted for
each curve. The algorithm names are abbreviated as follows: bagging - Bag, random forests - RF, multi-target prediction - MT and single-target
prediction - S T.
RF MT 
RF ST 
Bag MT 
Bag ST 
PCT 
PCT 
Critical Distance = 0.901
Figure 4: Average rank diagrams (with the critical distance at a signiﬁcance level of 0.05) for the prediction of multiple continuous targets. The
diﬀerences in performance of the algorithms connected with a red line are not statistically signiﬁcant. The number after the name of an algorithm
indicates its average rank. The abbreviations are the same as in Figure 3, with the addition of single predictive clustering tree - PCT.
signiﬁcantly outperform ensemble methods predicting the targets separately. Also, bagging for multiple targets is
signiﬁcantly faster than bagging for separate prediction of the targets. In terms of model size, both random forests and
bagging for predicting multiple targets simultaneously outperform signiﬁcantly the ensembles that predict multiple
targets separately.
RF MT 
Bag MT 
RF ST 
Bag ST 
Critical Distance = 1.254
RF MT 
Bag MT 
RF ST 
Bag ST 
Critical Distance = 1.254
(a) Time eﬃciency
(b) Size of the models
Figure 5: Eﬃciency (running time and model size) of the ensembles for prediction of multiple continuous targets. The size of the ensembles is 50
D. Kocev et al. / Pattern Recognition 00 1–26
5.2. Multi-target classiﬁcation
In Figure 6, we present three saturation curves for the ensemble methods for multi-target classiﬁcation. As for
multi-target regression, the values depicted in the curves are the averages over all target variables for a given dataset
(and in Figure 6(c) averaged across all datasets). As for multi-target regression, we determined the ensemble size
after which the changes in predictive performance are no longer signiﬁcant. The ensembles for predicting the multiple
targets simultaneously saturate with 50 trees added, while the ensembles for separate prediction of the targets require
more trees: 75 for random forests and 250 for bagging. After this, we select the ensembles size of 50 (Figure 7) to
compare the algorithms. This is in line with the results from the saturation curves which show that ensembles for
multi-target classiﬁcation perform better than the ensembles for single-target classiﬁcation at smaller ensemble sizes
(this can be also noticed in the overall saturation curve shown in Figure 6(c)).
100 200 300 400 500 600 700 800 900 1000
Size of ensemble
100 200 300 400 500 600 700 800 900 1000
Size of ensemble
100 200 300 400 500 600 700 800 900 1000
Size of ensemble
(a) Sigmea real
(b) Water quality
(c) Overall
Figure 6: Saturation curves for the prediction of multiple discrete targets. The curves (a) and (b) are obtained by averaging the µF −score values
over all of the target variables in a dataset, while the curve (c) by averaging the µF −score values over all of the target variables in all datasets.
Higher µF −score values mean better predictive performance. Note that the scale of the y-axis is adapted for each curve. The algorithm names are
abbreviated as follows: bagging - Bag, random forests - RF, multi-target prediction - MT and single-target prediction - S T.
The statistical tests reveal that there is no statistically signiﬁcant diﬀerence (at the level of 0.05) in the performance
of the ensemble methods (Figure 7). Bagging for predicting the multiple targets simultaneously is the best performing
method (average rank 2.59) and the remaining methods have larger average ranks very close to each other (ranging
from 3.0 to 3.11) with random forest for separate prediction of the targets having the largest average rank (worst
performance). Similar conclusions can be made if instead of ensembles with 50 trees, we select ensembles with 75 or
250 trees. The only diﬀerence is that in these cases random forests for multiple targets have larger average ranks (i.e.,
the diﬀerence in performance between random forests for multiple targets and the other methods is smaller).
Both types of ensembles (multi-target and single-target classiﬁcation) perform statistically signiﬁcantly better
than single PCTs. Furthermore, single PCTs for multi-target classiﬁcation perform better (although not statistically
signiﬁcantly) than the PCTs for single-target classiﬁcation.
Finally, we compare the ensembles by their eﬃciency: running times (Figure 8(a)) and size of models (Figure 8(b)). Concerning the running time, we can state that random forests for predicting multiple targets simultaneously
signiﬁcantly outperform bagging for predicting multiple targets separately. As for the size of the models, we can note
the following: (1) bagging for predicting multiple targets simultaneously signiﬁcantly outperforms both ensemble
methods for separate prediction of the targets and (2) random forests for predicting multiple targets simultaneously
signiﬁcantly outperform random forests for separate prediction of the targets.
5.3. Hierarchical multi-label classiﬁcation
In this subsection, we compare the performance of ensembles and PCTs for the tasks of HMC and HSC. We
begin by presenting the saturation curves of the ensemble methods in Figure 9. Figures 9(a) and 9(b) show the
saturation curves for the SCOP-GO and ImCLEF07D domains, respectively, while Figure 9(c) shows the saturation
curve averaged across all domains. We determine the ensemble size after which adding trees in the ensemble does
D. Kocev et al. / Pattern Recognition 00 1–26
Bag MT 
Bag ST 50@3
RF MT 
RF ST 
PCT MT @4.33
PCT ST @4.9
Critical Distance = 0.982
Figure 7: Average ranks diagrams (with the critical distance at signiﬁcance level of 0.05) for prediction of multiple discrete targets. The diﬀerences
in performance of the algorithms connected with a red line are not statistically signiﬁcant. The number after the name of an algorithm indicates its
average rank. The abbreviations are the same as in Figure 6, with the addition of single predictive clustering tree - PCT.
RF MT 50@1
RF ST 
Bag MT 
Bag ST 50@4
Critical Distance = 1.563
Bag MT 
RF MT 50@2
Bag ST 
RF ST 
Critical Distance = 1.563
(a) Time eﬃciency
(b) Size of the models
Figure 8: Eﬃciency of the ensembles for predicting multiple discrete targets. The size of the ensembles is 50 trees.
not statistically signiﬁcantly improve the ensemble’s performance. Both types of ensemble methods for HMC and
random forests for HSC saturate after 50 trees are added, while bagging for HSC saturates after only 25 trees. We
further compare the performance of the ensembles at 50 trees (Figure 10).
Size of ensemble
Size of ensemble
Size of ensemble
(a) SCOP-GO
(b) ImCLEF07D
(c) Overall
Figure 9: Saturation curves for hierarchical multi–label classiﬁcation. The curves (a) and (b) are obtained by giving the AUPRC value for a dataset,
while the curve (c) by averaging the AUPRC values over all of the datasets. Higher AUPRC values mean better predictive performance. Note that
the scale of the y-axis is adapted for each curve. The algorithm names are abbreviated as follows: bagging - Bag, random forests - RF, hierarchical
multi-label classiﬁcation - HMC and hierarchical single-label classiﬁcation - HSC.
The average ranks diagram for the ensembles with 50 trees (Figure 10) shows that the performance of the diﬀerent
ensembles is not statistically signiﬁcantly diﬀerent. Note that the best performing method is random forests for HSC
D. Kocev et al. / Pattern Recognition 00 1–26
RF HSC 
Bag HMC 
RF HMC 
Bag HSC 
PCT 
PCT 
Critical Distance = 2.384
Figure 10: Average ranks diagrams (with the critical distance at a signiﬁcance level of 0.05) for hierarchical multi–label classiﬁcation. The
diﬀerences in performance of algorithms connected with a red line are not statistically signiﬁcant. The number after the name of an algorithm
indicates its average rank. The abbreviations are the same as in Figure 9 with the addition of single predictive clustering tree - PCT.
(average rank 2.25) and the worst performing method is bagging for HSC (average rank 2.85). Ensembles for HMC
and HSC are statistically signiﬁcantly better than single PCT for HMC and HSC. However, the overall saturation
curve from Figure 9(c), indicates that random forests for HMC is the best performing method. Moreover, ensembles
for HMC perform better than ensembles for HSC on the datasets with larger hierarchies (i.e., datasets with (|H| > 300)
and in our case the datasets from functional genomics).
Finally, we compare the algorithms by their eﬃciency when they contain 50 trees (running times in Figure 11(a)
and model size in Figure 11(b)). The random forests for HMC are statistically signiﬁcantly faster than bagging for
both HMC and HSC, while random forests for HSC are signiﬁcantly faster than bagging for HSC. The models of
bagging of HMC are statistically signiﬁcantly smaller than the models from both types of ensembles for HSC, while
the random forests for HMC are statistically signiﬁcantly smaller than the random forests for HSC.
RF HMC 50@1
RF HSC 
Bag HMC 
Bag HSC 
Critical Distance = 1.483
Bag HMC 
RF HMC 
Bag HSC 50@3
RF HSC 50@4
Critical Distance = 1.483
(a) Time eﬃciency
(b) Size of the models
Figure 11: Eﬃciency of the ensembles for hierarchical multi–label classiﬁcations. The size of the ensembles is 50 trees.
5.4. Summary of the results
To summarize the results from the experiments, we discuss some general conclusions by formulating an answer
to each of the questions from Section 4.1.
5.4.1. Predictive performance
The ensembles of PCTs for predicting structured outputs lift the predictive performance of a single PCT for
predicting structured outputs. The diﬀerence in performance is statistically signiﬁcant at the signiﬁcance level of
0.05. This was previously shown only on applications where the target is a single continuous or discrete variable .
This ﬁnding is valid for all three machine learning tasks that we consider in this article.
The diﬀerences in predictive performance between ensembles of PCTs and ensembles of trees predicting components of the output are not statistically signiﬁcant (at the 0.05 level) in any task. Also, none of the four considered
ensemble algorithms is consistently performing best. However, the ensembles of PCTs often have better predictive
performance (i.e., smaller average ranks) than the ensembles of trees predicting components of the output.
D. Kocev et al. / Pattern Recognition 00 1–26
5.4.2. Convergence
We looked at the saturation point of the ensembles’ performances with respect to the number of base classiﬁers.
In the majority of the cases, the predictive performance of the ensembles saturates after the 50th tree is added in the
ensemble. Exceptions to this are bagging for HSC that saturates when 25 trees are added and random forests and bagging for predicting multiple discrete targets separately that saturate after 75 and 250 trees, respectively. Furthermore,
the saturation curves oﬀer some insight about the application of a given method on a group of datasets (summarized
by their number of examples and number of descriptive variables). Also, the curves show that on the majority of the
datasets, the ensembles of PCTs for predicting the structured outputs as a whole have better performance than the
ensembles that predict the components. This is especially the case when the ensembles contain fewer trees.
5.4.3. Eﬃciency
With respect to model size and induction times, the tree ensembles that exploit the structure do have a signiﬁcant
advantage. The advantage is more pronounced when the datasets have large numbers of instances and/or descriptive
attributes. Moreover, because of the feature sampling, the random forests for predicting structured outputs beneﬁt
even more, in terms of induction time, when the datasets have many descriptive attributes.
By averaging over all datasets considered in this study, we ﬁnd that random forests for predicting the complete
structured outputs are 4 times faster to construct and the models are 3.4 times smaller than a collection of random
forests for predicting single targets or labels. In addition, they are 5.5 times faster to construct and have models with
similar size as bagging for predicting the complete structured output. Furthermore, the latter is 3.9 times faster and
yields models that are 2.9 times smaller than a collection of bagged trees for predicting single targets or labels.
6. Related work
The task of predicting structured outputs is gaining more and more attention within the machine learning research
community . The community has proposed a number of diﬀerent methods for addressing this task. However,
they are typically “computationally demanding and ill-suited for dealing with large datasets” . In this article, we
have proposed a global method for predicting structured outputs that has good predictive performance and is very
eﬃcient: it scales linearly with the size of the output. We used the predictive clustering framework both for predicting
multiple targets and for hierarchical multi-label classiﬁcation. In the literature, individual methods typically solve only
one task: only predicting multiple discrete variables (multi-target classiﬁcation), only predicting multiple continuous
variables (multi-target regression), or hierarchical multi-label classiﬁcation only. In the remainder of this section, we
ﬁrst present the methods that predict multiple targets and then the methods for hierarchical multi-label classiﬁcation.
6.1. Methods for multi-target prediction
The task of predicting multiple targets is connected with multi-task learning and learning to learn 
paradigms. These paradigms include the task of predicting a variable (continuous or discrete) using multiple input
spaces (i.e., biological data for a disease obtained using diﬀerent technologies); predicting multiple variables from
multiple input spaces, and predicting multiple variables from a single input space. Here, we consider the last task.
There is extensive empirical work showing an increase in predictive performance when multiple tasks are learned
simultaneously as compared to learning each task separately 1–26
Similarly, Hern´andez-Lobato et al. use (eﬃciently parametrized) expectation propagation to approximate the
posterior distribution of the model to identify relevant features for predicting all the tasks.
We present and categorize the related work in four groups: statistics, statistical learning theory, Bayesian theory
and kernel learning. In statistics, Brown and Zidek extend the standard ridge regression to multivariate ridge
regression, while Breiman and Friedman propose the Curds&Whey method, where the relations between the
task are modelled in a post-processing phase. In statistical learning theory, for handling multiple tasks, an extension
of the VC-dimension and the basic generalization bounds for single task learning are proposed by Baxter and
Ben-David and Borbely .
Most of the work in multi-task learning is done using Bayesian theory . In this case, simultaneously
with the parameters of the models for each of the tasks, a probabilistic model that captures the relations between the
various tasks is being calculated. Most of these approaches use hierarchical Bayesian models.
Finally, there are many approaches for multi-task learning using kernel methods. For example, Evgeniou et al.
 extend the kernel methods to the case of multi-task learning by using a particular type of kernel (multi-task
kernel). The regularized multi-task learning then becomes equivalent to single-task learning when such a kernel is
used. They show experimentally that the support vector machines (SVMs) with multi-task kernels have signiﬁcantly
better performance than the ones with single-task kernels. Liu et al. proposed an approach to deﬁne the loss
functions on the output manifold by considering it as a Riemannian submanifold in order to include its geometric
structure in the learning (regression) process. The proposed approach can be used in the context of any regression
algorithm and the experimental evaluation using regression SVMs provided satisfactory results. For more details on
kernel methods and SVMs for multi-task learning, we refer the reader to and the references therein.
6.2. Methods for hierarchical multi-label classiﬁcation
A number of approaches have been proposed for the task of hierarchical multi-label classiﬁcation . Silla and
Freitas survey and categorize the HMC methods based on some of their characteristics and their application
domains. The characteristics of the methods they consider as most important are: prediction of single or multiple paths
from the hierarchy, the depth of the predicted class, the type of the taxonomy that can be handled (tree or directed
acyclic graph) and whether the method is local (constructs a model for each part of the taxonomy) or global (constructs
a model for the whole taxonomy). The most prominent application domains for these methods are functional genomics
(biology), image classiﬁcation and text categorization.
Here, we present and group some existing methods based on the learning technique they use. We group the
methods as follows: network based methods, kernel based methods and decision tree based methods.
Network based methods. The network based approaches predict functions of unannotated genes based on known
functions of genes that are nearby in a functional association network or protein-protein interaction network .
Since the network based approaches are based on label propagation, a number of approaches were proposed to combine predictions of functional networks with those of a predictive model. Tian et al. , for instance, use logistic
regression to combine predictions made by a functional association network with predictions from a random forest.
Kernel based methods. Obozinski et al. present a two-step approach in which SVMs are ﬁrst learned independently for each class separately (allowing violations of the hierarchy constraint) and are then reconciliated to enforce
the hierarchy constraint. Similarly, Barutcuoglu et al. use un-thresholded SVMs learned for each class separately
and then combine the SVMs by using a Bayesian network so that the predictions are consistent with the hierarchical
relationships. Guan et al. extend the method by Barutcuoglu et al. to an ensemble framework. Valentini
and Re also propose a hierarchical ensemble method that uses probabilistic SVMs as base learners. The method
combines the predictions by propagating the weighted true path rule both top-down and bottom-up through the hierarchy, which ensures consistency with the hierarchy constraint. Next, D´ıez et al. proposed a semi-dependent
decomposition approach in which the node classiﬁers (binary SVMs) are constructed considering the other classiﬁers,
their descendants and the loss function used to estimate the performance of the hierarchical classiﬁers. This approach
follows a bottom-up strategy with the aim to optimize the loss function at every subtree assuming that all classiﬁers
are known, except the one at the root of the hierarchy.
Rousu et al. present a more direct method that does not require a second step to make sure that the hierarchy
constraint is satisﬁed. Their approach is based on a large margin method for structured output prediction which deﬁnes
a joint feature map over the input and the output space. Next, it applies SVM based techniques to learn the weights of
D. Kocev et al. / Pattern Recognition 00 1–26
a discriminant function (deﬁned as the dot product of the weights and the joint feature map). Rousu et al. propose
a suitable joint feature map and an eﬃcient way for computing the argmax of the discriminant function (which is the
prediction for a new instance). Furthermore, G¨artner and Vembu propose to use counting of super-structures from
the output to eﬃciently calculate (in polynomial time) the argmax of the discriminant function.
Decision tree based methods. Clare adapts the well-known decision tree algorithm C4.5 to cope with
the issues introduced by the HMC task. This version of C4.5 (called C4.5H) uses the sum of entropies of the class
variables to select the best split. C4.5H predicts classes on several levels of the hierarchy, assigning a larger cost to
misclassiﬁcations higher up in the hierarchy. The resulting tree is then transformed into a set of rules, and the best
rules are selected, based on a signiﬁcance test on a validation set.
Geurts et al. present a decision tree based approach related to predictive clustering trees. They start from a
diﬀerent deﬁnition of variance and then kernelize this variance function. The result is a decision tree induction system
that can be applied to structured output prediction using a method similar to the large margin methods mentioned
above. Therefore, this system could also be used for HMC after deﬁning a suitable kernel. To this end, an approach
similar to that of Rousu et al. could be used.
The present work follows Blockeel et al. , who proposed the idea of using predictive clustering trees 
for HMC tasks (PCTs for HMC). Their work presents the ﬁrst thorough empirical comparison between an HMC
decision tree method in the context of tree shaped class hierarchies. Vens et al. extend the algorithm towards
hierarchies structured as directed acyclic graphs (DAGs) and show that learning one decision tree for predicting all
classes simultaneously outperforms learning one tree per class (even if those trees are built by taking into account the
hierarchy, via so-called hierarchical single-label classiﬁcation - HSC).
7. Conclusions
In this article, we address the task of learning predictive models for structured output learning, which takes as
input a tuple of attribute values and produces a structured object. In contrast to standard classiﬁcation and regression,
where the output is a single scalar value, in structured output learning the output is a data structure, such as a tuple
or a directed acyclic graph. We consider both global and local prediction of structured outputs, the former based on a
single model that predicts the entire output structure and the latter based on a collection of models, each predicting a
component of the output structure.
In particular, we take the notion of an ensemble, i.e., a collection of predictive models whose predictions are
combined, and apply it in the context of predicting structured outputs. Ensembles have proved to be highly eﬀective
methods for improving the predictive performance of their constituent models, especially for classiﬁcation tree models. We propose in this article to build ensemble models consisting of predictive clustering trees, which generalize
classiﬁcation trees. We use them for predicting diﬀerent types of structured outputs, both locally and globally.
More speciﬁcally, we develop methods for learning diﬀerent types of ensembles of predictive clustering trees for
global and local prediction of diﬀerent types of structured outputs. The types of outputs considered correspond to
diﬀerent predictive modelling tasks, i.e., multi-target regression, multi-target classiﬁcation, and hierarchical multilabel classiﬁcation. The diﬀerent types of ensembles include bagging and random forests. Each of the combinations
can be applied both in the context of global prediction (producing a single ensemble) or local prediction (producing a
collection of ensembles).
We conduct an extensive experimental evaluation of bagging and random forests across a range of benchmark
datasets for each of the three types of structured outputs. We compare tree ensembles for global and local prediction,
as well as single trees for global prediction and tree collections for local prediction, both in terms of predictive
performance and in terms of eﬃciency (running times and model complexity). Both global and local tree ensembles
perform better than the single model counterparts in terms of predictive power. Global and local tree ensembles
perform equally well, with global ensembles being more eﬃcient and producing smaller models, as well as needing
fewer trees in the ensemble to achieve the maximal performance.
We also analyse the computational complexity of the methods theoretically. The theoretical analyses are consistent
with the empirical evidence, showing that the global tree ensembles are most eﬃcient, especially random forests. The
analyses also indicate that the proposed approaches are scalable to large datasets, which can be large along any of the
following dimensions: number of attributes, number of examples, and size of the target.
D. Kocev et al. / Pattern Recognition 00 1–26
Several directions for further work deserve a mention. The presented methods for learning ensembles can be
extended to other types of structured outputs (e.g., time series or tuples of mixed primitive data types, both continuous
and discrete). Also, other distance measures for structured types can be implemented, thus making the algorithms
more ﬂexible and applicable to new domains.
Another line of further work is to use global random forests for obtaining feature ranking for structured outputs. To
produce a feature ranking for a structured output, a feature ranking is typically performed ﬁrst for each component of
the output separately: these rankings are then merged using some aggregation function. The feature ranking based on
global random forests can exploit the underlying dependencies and relations that may exist between the components
of the outputs. Thus, it will provide feature rankings that are more relevant for the complete output rather than a
component of it. Furthermore, it will be easily extended to a generic type of structured output.
Acknowledgment
We would like to thank Valentin Gjorgjioski for the discussions concerning the computational complexity of the
proposed methods. Celine Vens is a postdoctoral fellow of the Research Fund – Flanders (FWO–Vlaanderen). The
work of Dragi Kocev and Saˇso Dˇzeroski was supported by the Slovenian Research Agency (Grants P2-0103 and J2-
2285), the European Commission , and Operation no. OP13.1.1.2.02.0005
nanced by the European Regional Development Fund (85%) and the Ministry of Education, Science, Culture and Sport
of Slovenia (15%).