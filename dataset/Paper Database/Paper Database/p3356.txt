Graph Structure Learning for Robust Graph Neural Networks
Michigan State University
 
Michigan State University
 
Xiaorui Liu
Michigan State University
 
Xianfeng Tang
The Pennsylvania State University
 
Suhang Wang
The Pennsylvania State University
 
Jiliang Tang
Michigan State University
 
Graph Neural Networks (GNNs) are powerful tools in representation learning for graphs. However, recent studies show that GNNs
are vulnerable to carefully-crafted perturbations, called adversarial
attacks. Adversarial attacks can easily fool GNNs in making predictions for downstream tasks. The vulnerability to adversarial attacks
has raised increasing concerns for applying GNNs in safety-critical
applications. Therefore, developing robust algorithms to defend
adversarial attacks is of great significance. A natural idea to defend
adversarial attacks is to clean the perturbed graph. It is evident
that real-world graphs share some intrinsic properties. For example,
many real-world graphs are low-rank and sparse, and the features of
two adjacent nodes tend to be similar. In fact, we find that adversarial attacks are likely to violate these graph properties. Therefore, in
this paper, we explore these properties to defend adversarial attacks
on graphs. In particular, we propose a general framework Pro-GNN,
which can jointly learn a structural graph and a robust graph neural
network model from the perturbed graph guided by these properties. Extensive experiments on real-world graphs demonstrate that
the proposed framework achieves significantly better performance
compared with the state-of-the-art defense methods, even when
the graph is heavily perturbed. We release the implementation of
Pro-GNN to our DeepRobust repository for adversarial attacks and
defenses 1. The specific experimental settings to reproduce our results can be found in 
ACM Reference Format:
Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang
Tang. 2020. Graph Structure Learning for Robust Graph Neural Networks.
In Proceedings of the 26th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD â€™20), August 23â€“27, 2020, Virtual Event, CA, USA.
ACM, New York, NY, USA, 9 pages. 
INTRODUCTION
Graphs are ubiquitous data structures in numerous domains, such as
chemistry (molecules), finance (trading networks) and social media
1 
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from .
KDD â€™20, August 23â€“27, 2020, Virtual Event, CA, USA
Â© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-7998-4/20/08...$15.00
 
(the Facebook friend network). With their prevalence, it is particularly important to learn effective representations of graphs and then
apply them to solve downstream tasks. Recent years have witnessed
great success from Graph Neural Networks (GNNs) in
representation learning of graphs. GNNs follow a message-passing
scheme , where the node embedding is obtained by aggregating and transforming the embeddings of its neighbors. Due to the
good performance, GNNs have been applied to various analytical
tasks including node classification , link prediction , and
recommender systems .
Although promising results have been achieved, recent studies have shown that GNNs are vulnerable to adversarial attacks
 . In other words, the performance of GNNs can
greatly degrade under an unnoticeable perturbation in graphs. The
lack of robustness of these models can lead to severe consequences
for critical applications pertaining to the safety and privacy. For
example, in credit card fraud detection, fraudsters can create several
transactions with only a few high-credit users to disguise themselves, thus escaping from the detection based on GNNs. Hence,
developing robust GNN models to resist adversarial attacks is of
significant importance. Modifying graph data can perturb either
node features or graph structures. However, given the complexity
of structural information, the majority of existing adversarial attacks on graph data have focused on modifying graph structure
especially adding/deleting/rewiring edges . Thus, in this work,
we aim to defend against the most common setting of adversarial
attacks on graph data, i.e., poisoning adversarial attacks on graph
structure. Under this setting, the graph structure has already been
perturbed by modifying edges before training GNNs while node
features are not changed.
One perspective to design an effective defense algorithm is to
clean the perturbed graph such as removing the adversarial edges
and restoring the deleted edges . The key challenge from this
perspective is what criteria we should follow to clean the perturbed
graph. It is well known that real-world graphs often share certain
properties. First, many real-world clean graphs are low-rank and
sparse . For instance, in a social network, most individuals are
connected with only a small number of neighbors and there are
only a few factors influencing the connections among users .
Second, connected nodes in a clean graph are likely to share similar
features or attributes (or feature smoothness) . For example, in
a citation network, two connected publications often share similar
topics . Figure 1 demonstrates these properties of clean and
poisoned graphs. Specifically, we apply the state-of-the-art graph
poisoning attack, metattack , to perturb the graph data and
visualize the graph properties before and after mettack. As shown
 
Singular Value
(a) Singular Values
Perturbation Rate(%)
Growth Rate(%)
(b) Rank Growth
Number of Removed Edges
Decrease Rate(%)
Remove Normal Edge
Remove Adversary
(c) Rank Decrease Rate
Feature Difference Between Connected Nodes
Density Ditribution
Normal Edges
Adversarial Edges
(d) Feature Smoothness
Figure 1: An illustrative example on the property changes of the adjacency matrix by adversarial attacks
in Figure 1a, metattack enlarges the singular values of the adjacency
matrix and Figure 1b illustrates that metattack quickly increases
the rank of adjacency matrix. Moreover, when we remove the adversarial and normal edges from the perturbed graph respectively,
we observe that removing adversarial edges reduces the rank faster
than removing normal edges as demonstrated in Figure 1c. In addition, we depict the density distribution of feature difference of
connected nodes of the attacked graph in Figure 1d. It is observed
that metattack tends to connect nodes with large feature difference.
Observations from Figure 1 indicate that adversarial attacks could
violate these properties. Thus, these properties have the potential
to serve as the guidance to clean the perturbed graph. However,
work of exploring these properties to build robust graph neural
networks is rather limited.
In this paper, we target on exploring graph properties of sparsity,
low rank and feature smoothness to design robust graph neural
networks. Note that there could be more properties to be explored
and we would like to leave it as future work. In essence, we are
faced with two challenges: (i) how to learn clean graph structure
from poisoned graph data guided by these properties; and (ii) how
to jointly learn parameters for robust graph neural network and the
clean structure. To solve these two challenges, we propose a general
framework Property GNN (Pro-GNN) to simultaneously learn the
clean graph structure from perturbed graph and GNN parameters
to defend against adversarial attacks. Extensive experiments on a
variety of real-world graphs demonstrate that our proposed model
can effectively defend against different types of adversarial attacks
and outperforms the state-of-the-art defense methods.
The rest of the paper is organized as follows. In Section 2, we review some of the related work. In Section 3, we introduce notations
and formally define the problem. We explain our proposed framework in Section 4 and report our experimental results in Section 5.
Finally, we conclude the work with future directions in Section 6.
RELATED WORK
In line with the focus of our work, we briefly describe related work
on GNNs, and adversarial attacks and defense for graph data.
Graph Neural Networks
Over the past few years, graph neural networks have achieved great
success in solving machine learning problems on graph data. To
learn effective representation of graph data, two main families of
GNNs have been proposed, i.e., spectral methods and spatial methods. The first family learns node representation based on graph
spectral theory . Bruna et al. generalize the convolution
operation from Euclidean data to non-Euclidean data by using the
Fourier basis of a given graph. To simplify spectral GNNs, Defferrard et al. propose ChebNet and utilize Chebyshev polynomials
as the convolution filter. Kipf et al. propose GCN and simplify
ChebNet by using its first-order approximation. Further, Simple
Graph Convolution (SGC) reduces the graph convolution to a
linear model but still achieves competitive performance. The second
family of models define graph convolutions in the spatial domain
as aggregating and transforming local information . For
instance, DCNN treats graph convolutions as a diffusion process and assigns a certain transition probability for information
transferred from one node to the adjacent node. Hamilton et al. 
propose to learn aggregators by sampling and aggregating neighbor
information. VeliÄkoviÄ‡ et al. propose graph attention network
(GAT) to learn different attention scores for neighbors when aggregating information. To further improve the training efficiency,
FastGCN interprets graph convolutions as integral transforms
of embedding functions under probability measures and performs
importance sampling to sample a fixed number of nodes for each
layer. For a thorough review, we please refer the reader to recent
surveys .
Adversarial Attacks and Defense for GNNs
Extensive studies have demonstrated that deep learning models
are vulnerable to adversarial attacks. In other words, slight or unnoticeable perturbations to the input can fool a neural network to
output a wrong prediction. GNNs also suffer this problem . Different from image data, the graph structure is discrete and the nodes are dependent of each other, thus
making it far more challenging. The nettack generates unnoticeable perturbations by preserving degree distribution and imposing
constraints on feature co-occurrence. RL-S2V employs reinforcement learning to generate adversarial attacks. However, both
of the two methods are designed for targeted attack and can only
degrade the performance of GNN on target nodes. To perturb the
graph globally, metattack is proposed to generate poisoning
attacks based on meta-learning. Although increasing efforts have
been devoted to developing adversarial attacks on graph data, the
research about improving the robustness of GNNs has just started
recently . One way to solve the problem is to learn
a robust network by penalizing the attention scores of adversarial
edges. RGCN is to model Gaussian distributions as hidden
layers to absorb the effects of adversarial attacks in the variances.
PA-GNN leverages supervision knowledge from clean graphs
and applies a meta-optimization way to learn attention scores for robust graph neural networks. However, it requires additional graph
data from similar domain. The other way is to preprocess the perturbed graphs to get clean graphs and train GNNs on the clean
ones. Wu et. al have found that attackers tend to connect to
nodes with different features and they propose to remove the links
between dissimilar nodes. Entezari et al. have observed that
nettack results in changes in high-rank spectrum of the graph and
propose to preprocess the graph with its low-rank approximations.
However, due to the simplicity of two-stage preprocessing methods,
they may fail to counteract complex global attacks.
Different from the aforementioned defense methods, we aim
to explore important graph properties to recover the clean graph
while learning the GNN parameters simultaneously, which enables
the proposed model to extract intrinsic structure from perturbed
graph under different attacks.
PROBLEM STATEMENT
Before we present the problem statement, we first introduce some
notations and basic concepts. The Frobenius norm of a matrix S
is defined by ||S||2
ij. The â„“1 norm of a matrix S is given
by ||S||1 = Î£ij |Sij | and the nuclear norm of a matrix S is defined
as ||S||âˆ—= Î£rank(S)
Ïƒi , where Ïƒi is the i-th singular value of S.
(S)+ denotes the element-wise positive part of matrix S where
Sij = max{Sij, 0} and sÐ´n(S) indicates the sign matrix of S where
sÐ´n(S)ij = 1, 0, or âˆ’1 if Sij >0, =0, or <0, respectively. We use âŠ™
to denote Hadamard product of matrices. Finally, we use tr(S) to
indicate the trace of matrix S, i.e., tr(S) = Ã
Let G = (V, E) be a graph, where V is the set of N nodes
{v1,v2, ...,vN } and E is the set of edges. The edges describe the
relations between nodes and can also be represented by an adjacency matrix A âˆˆRN Ã—N where Aij denotes the relation between
nodesvi andvj. Furthermore, we use X = [x1, x2, . . . , xN ] âˆˆRN Ã—d
to denote the node feature matrix where xi is the feature vector
of the node vi. Thus a graph can also be denoted as G = (A, X).
Following the common node classification setting, only a part of
nodes VL = {v1,v2, ...,vl } are associated with corresponding labels YL = {y1,y2, ...,yl } where yi denotes the label of vi.
Given a graph G = (A, X) and the partial labels YL, the goal of
node classification for GNN is to learn a function fÎ¸ : VL â†’YL
that maps the nodes to the set of labels so that fÎ¸ can predict labels
of unlabeled nodes. The objective function can be formulated as
LGN N (Î¸, A, X, YL) =
â„“(fÎ¸ (X, A)i,yi) ,
where Î¸ is the parameters of fÎ¸ , fÎ¸ (X, A)i is the prediction of node
vi and â„“(Â·, Â·) is to measure the difference between prediction and
true label such as cross entropy. Though there exist a number of
different GNN methods, in this work, we focus on Graph Convolution Network (GCN) in . Note that it is straightforward to
extend the proposed framework to other GNN models. Specifically,
a two-layer GCN with Î¸ = (W1, W2) implements fÎ¸ as
fÎ¸ (X, A) = softmax
Update GNN Parameters
ðœƒ"#$ = ðœƒ" âˆ’ðœ‚ âˆ‡*+â„’-..
Low-rank and Sparsity
Feature Smoothness
Poisoned Graph
Update Structure
Clean Graph
Parameters ðœ½âˆ—
Figure 2: Overall framework of Pro-GNN. Dash lines indicate smaller weights.
where Ë†A = ËœDâˆ’1/2(A + I) ËœDâˆ’1/2 and ËœD is the diagonal matrix of A + I
with ËœDii = 1 + Ã
j Aij. Ïƒ is the activation function such as ReLU.
With aforementioned notations and definitions, the problem we
aim to study in this work can be formally stated as:
Given G = (A, X) and partial node label VL with A being poisoned
by adversarial edges and feature matrix X unperturbed, simultaneously learn a clean graph structure with the graph adjacency matrix
S âˆˆS = N Ã—N and the GNN parameters Î¸ to improve node
classification performance for unlabeled nodes.
THE PROPOSED FRAMEWORK
Adversarial attacks generate carefully-crafted perturbation on graph
data. We refer to the carefully-crafted perturbation as adversarial
structure. Adversarial structure can cause the performance of GNNs
to drop rapidly. Thus, to defend adversarial attacks, one natural
strategy is to eliminate the crafted adversarial structure, while maintaining the intrinsic graph structure. In this work, we aim to achieve
the goal by exploring graph structure properties of low rank, sparsity and feature smoothness. The illustration of the framework is
shown in Figure 2, where edges in black are normal edges and edges
in red are adversarial edges introduced by an attacker to reduce the
node classification performance. To defend against the attacks, Pro-
GNN iteratively reconstructs the clean graph by preserving the low
rank, sparsity, and feature smoothness properties of a graph so as
to reduce the negative effects of adversarial structure. Meanwhile,
to make sure that the reconstructed graph can help node classification, Pro-GNN simultaneously updates the GNN parameters on
the reconstructed graph by solving the optimization problem in an
alternating schema. In the following subsections, we will give the
details of the proposed framework.
Exploring Low rank and Sparsity Properties
Many real-world graphs are naturally low-rank and sparse as the
entities usually tend to form communities and would only be connected with a small number of neighbors . Adversarial attacks
on GCNs tend to add adversarial edges that link nodes of different
communities as this is more efficient to reduce node classification
performance of GCN. Introducing links connecting nodes of different communities in a sparse graph can significantly increase the
rank of the adjacency matrix and enlarge the singular values, thus
damaging the low rank and sparsity properties of graphs, which is
verified in Figure 1a and Figure 1b. Thus, to recover the clean graph
structure from the noisy and perturbed graph, one potential way
is to learn a clean adjacency matrix S close to the adjacency matrix of the poisoned graph by enforcing the new adjacency matrix
with the properties of low rank and sparsity. As demonstrated in
Figure1c, the rank decreases much faster by removing adversarial
edges than by removing normal edges. This implies that the low
rank and sparsity constraint can remove the adversarial edges instead of normal edges. Given the adjacency matrix A of a poisoned
graph, we can formulate the above process as a structure learning
problem :
L0 = âˆ¥A âˆ’Sâˆ¥2
F + R(S), s.t., S = SâŠ¤.
Since adversarial attacks target on performing unnoticeable perturbations to graphs, the first term âˆ¥A âˆ’Sâˆ¥2
F ensures that the new
adjacency matrix S should be close to A. As we assume that the
graph are undirected, the new adjacency matrix should be symmetric, i.e., S = SâŠ¤. R(S) denotes the constraints on S to enforce
the properties of low rank and sparsity. According to ,
minimizing the â„“1 norm and the nuclear norm of a matrix can force
the matrix to be sparse and low-rank, respectively. Hence, to ensure
a sparse and low-rank graph, we want to minimize the â„“1 norm and
the nuclear norm of S. Eq. (3) can be rewritten as:
L0 = âˆ¥A âˆ’Sâˆ¥2
F + Î±âˆ¥Sâˆ¥1 + Î²âˆ¥Sâˆ¥âˆ—, s.t., S = SâŠ¤,
where Î± and Î² are predefined parameters that control the contributions of the properties of sparsity and low rank, respectively.
One important benefit to minimize the nuclear norm âˆ¥Sâˆ¥âˆ—is that
we can reduce every singular value, thus alleviating the impact of
enlarging singular values from adversarial attacks.
Exploring Feature Smoothness
It is evident that connected nodes in a graph are likely to share
similar features. In fact, this observation has been made on graphs
from numerous domains. For example, two connected users in a
social graph are likely to share similar attributes , two linked
web pages in the webpage graph tend to have similar contents 
and two connected papers in the citation network usually have
similar topics . Meanwhile, recently it is demonstrated that
adversarial attacks on graphs tend to connect nodes with distinct
features . Thus, we aim to ensure the feature smoothness in
the learned graph. The feature smoothness can be captured by the
following term Ls:
Sij(xi âˆ’xj)2,
where S is the new adjacency matrix, Sij indicates the connection of
vi and vj in the learned graph and (xi âˆ’xj)2 measures the feature
difference between vi and vj. Ls can be rewritten as:
Ls = tr(XâŠ¤LX),
where L = D âˆ’S is the graph Laplacian matrix of S and D is the
diagonal matrix of S. In this work, we use normalized Laplacian
matrix Ë†L = Dâˆ’1/2LDâˆ’1/2 instead of L to make feature smoothness
independent on the degrees of the graph nodes , i.e.,
Ls = tr(XT Ë†LX) = 1
where di denotes the degree of vi in the learned graph. In the
learned graph, if vi and vj are connected (i.e., Sij , 0), we expect
that the feature difference (xi âˆ’xj)2 should be small. In other words,
if the features between two connected node are quite different, Ls
would be very large. Therefore, the smaller Ls is, the smoother
features X are on the graph S. Thus, to fulfill the feature smoothness
in the learned graph, we should minimize Ls. Therefore, we can
add the feature smoothness term to the objective function of Eq. (4)
to penalize rapid changes in features between adjacent nodes as:
L = L0 + Î» Â· Ls = L0 + Î» tr(XT Ë†LX), s.t., S = SâŠ¤,
where Î» is a predefined parameter to control the contribution from
feature smoothness.
Objective Function of Pro-GNN
Intuitively, we can follow the preprocessing strategy to
defend against adversarial attacks â€“ we first learn a graph from
the poisoned graph via Eq. (8) and then train a GNN model based
on the learned graph. However, with such a two-stage strategy,
the learned graph may be suboptimal for the GNN model on the
given task. Thus, we propose a better strategy to jointly learn the
graph structure and the GNN model for a specific downstream
task. We empirically show that jointly learning GNN model and
the adjacency matrix is better than two stage one in Sec 5.4.2. The
final objective function of Pro-GNN is given as
L = L0 + Î»Ls + Î³ LGN N
F + Î±âˆ¥Sâˆ¥1 + Î²âˆ¥Sâˆ¥âˆ—+ Î³ LGN N (Î¸, S, X, YL) + Î»tr(XT Ë†LX)
where LGN N is a loss function for the GNN model that is controlled
by a predefined parameter Î³. Another benefit of this formulation is
that the information from LGN N can also guide the graph learning
process to defend against adversarial attacks since the goal of graph
adversarial attacks is to maximize LGN N .
An Optimization Algorithm
Jointly optimizing Î¸ and S in Eq.(9) is challenging. The constraints
on S further exacerbate the difficulty. Thus, in this work, we use an
alternating optimization schema to iteratively update Î¸ and S.
UpdateÎ¸. To updateÎ¸, we fix S and remove terms that are irrelevant
to Î¸, then the objective function in Eq.(9) reduces to:
LGN N (Î¸, S, X, YL) =
â„“(fÎ¸ (X, S)u,yu) ,
which is a typical GNN optimization problem and we can learn Î¸
via stochastic gradient descent.
Update S. Similarly, to update S, we fix Î¸ and arrive at
S L(S, A) + Î±âˆ¥Sâˆ¥1 + Î²âˆ¥Sâˆ¥âˆ—
S = SâŠ¤, S âˆˆS,
where L(S, A) is defined as
L(S, A) = âˆ¥A âˆ’Sâˆ¥2
F + LGN N (Î¸, S, X,Y) + Î»tr(XT Ë†LX).
Note that both â„“1 norm and nuclear norm are non-differentiable.
For optimization problem with only one non-diffiential regularizer
Algorithm 1: Pro-GNN
Data: Adjacency matrix A, Attribute matrix X, Labels YL,
Hyper-parameters Î±, Î²,Î³, Î»,Ï„, Learning rate Î·,Î·â€²
Result: Learned adjacency S, GNN parameters Î¸
1 Initialize S â†A
2 Randomly initialize Î¸
3 while Stopping condition is not met do
S â†S âˆ’Î·âˆ‡S(âˆ¥S âˆ’Aâˆ¥2
F + Î³ LGN N + Î»Ls)
S â†proxÎ·Î² ||.||âˆ—(S)
S â†proxÎ·Î± ||.||1(S)
for i=1 to Ï„ do
Ð´ â†âˆ‚LGN N (Î¸,S,X, YL)
11 Return S,Î¸
R(S), we can use Forward-Backward splitting methods . The idea
is to alternate a gradient descent step and a proximal step as:
S(k)= proxÎ·R
S(kâˆ’1) âˆ’Î·âˆ‡S L(S,A)
where Î· is the learning rate, proxR is the proximal operator as:
proxR(Z) = arg min
2 âˆ¥S âˆ’Z||2
In particular, the proximal operator of â„“1 norm and nuclear norm
can be represented as ,
proxÎ± ||.||1(Z) = sÐ´n(Z) âŠ™(|Z| âˆ’Î±)+,
proxÎ² ||.||âˆ—(Z) = UdiaÐ´((Ïƒi âˆ’Î²)+)iVT ,
where Z = UdiaÐ´(Ïƒ1, ...,Ïƒn)VâŠ¤is the singular value decomposition
of Z. To optimize objective function with two non-differentiable
regularizers, Richard et al. introduce the Incremental Proximal
Descent method based on the introduced proximal operators. By
iterating the updating process in a cyclic manner, we can update S
as follows,
S(k) = S(kâˆ’1) âˆ’Î· Â· âˆ‡S (L(S, A)),
S(k) = proxÎ·Î² âˆ¥Â·âˆ¥âˆ—
S(k) = proxÎ·Î± âˆ¥Â·âˆ¥1
After we learn a relaxed S, we project S to satisfy the constraints.
For the symmetric constraint, we let S = S+SâŠ¤
. For the constraint
Sij âˆˆ , we project Sij < 0 to 0 and Sij > 1 to 1. We denote
these projection operations as PS(S).
Training Algorithm. With these updating and projection rules,
the optimization algorithm is shown in Algorithm 1. In line 1, we
first initialize the estimated graph S as the poisoned graph A. In line
2, we randomly initialize the GNN parameters. From lines 3 to 10,
we update S and the GNN parameters Î¸ alternatively and iteratively.
Specifically, we train the GNN parameters in each iteration while
training the graph reconstruction model every Ï„ iterations.
Table 1: Dataset Statistics. Following , we only consider the largest connected component (LCC).
EXPERIMENTS
In this section, we evaluate the effectiveness of Pro-GNN against
different graph adversarial attacks. In particular, we aim to answer
the following questions:
â€¢ RQ1 How does Pro-GNN perform compared to the state-of-theart defense methods under different adversarial attacks?
â€¢ RQ2 Does the learned graph work as expected?
â€¢ RQ3 How do different properties affect performance of Pro-GNN.
Before presenting our experimental results and observations, we
first introduce the experimental settings.
Experimental settings
Datasets. Following , we validate the proposed approach on four benchmark datasets, including three citation graphs,
i.e., Cora, Citeseer and Pubmed, and one blog graph, i.e., Polblogs.
The statistics of the datasets are shown in Table 1. Note that in the
Polblogs graph, node features are not available. In this case, we set
the attribute matrix to N Ã— N identity matrix.
Baselines. To evaluate the effectiveness of Pro-GNN, we compare it with the state-of-the-art GNN and defense models by using
the adversarial attack repository DeepRobust :
â€¢ GCN : while there exist a number of different Graph Convolutional Networks (GCN) models, we focus on the most representative one .
â€¢ GAT : Graph Attention Netowork (GAT) is composed of
attention layers which can learn different weights to different
nodes in the neighborhood. It is often used as a baseline to defend
against adversarial attacks.
â€¢ RGCN : RGCN models node representations as gaussian distributions to absorb effects of adversarial attacks. It also employs
attention mechanism to penalize nodes with high variance.
â€¢ GCN-Jaccard : Since attackers tend to connect nodes with
dissimilar features or different labels, GCN-Jaccard preprocesses
the network by eliminating edges that connect nodes with jaccard
similarity of features smaller than threshold Ï„. Note that this
method only works when node features are available.
â€¢ GCN-SVD : This is another preprocessing method to resist
adversarial attacks. It is noted that nettack is a high-rank attack,
thus GCN-SVD proposes to vaccinate GCN with the low-rank
approximation of the perturbed graph. Note that it originally targets at defending against nettack, however, it is straightforward
to extend it to non-targeted and random attacks.
In addition to representative baselines, we also include one variant
of the proposed framework, Pro-GNN-fs, which is the variant by
eliminating the feature smoothness term (or setting Î» = 0).
Table 2: Node classification performance (AccuracyÂ±Std) under non-targeted attack (metattack ).
Ptb Rate (%)
GCN-Jaccard2
Pro-GNN-fs
83.50Â±0.44
83.97Â±0.65
83.09Â±0.44
82.05Â±0.51
80.63Â±0.45
83.42Â±0.52
82.98Â±0.23
76.55Â±0.79
80.44Â±0.74
77.42Â±0.39
79.13Â±0.59
78.39Â±0.54
82.78Â±0.39
82.27Â±0.45
70.39Â±1.28
75.61Â±0.59
72.22Â±0.38
75.16Â±0.76
71.47Â±0.83
77.91Â±0.86
79.03Â±0.59
65.10Â±0.71
69.78Â±1.28
66.82Â±0.39
71.03Â±0.64
66.69Â±1.18
76.01Â±1.12
76.40Â±1.27
59.56Â±2.72
59.94Â±0.92
59.27Â±0.37
65.71Â±0.89
58.94Â±1.13
68.78Â±5.84
73.32Â±1.56
47.53Â±1.96
54.78Â±0.74
50.51Â±0.78
60.82Â±1.08
52.06Â±1.19
56.54Â±2.58
69.72Â±1.69
71.96Â±0.55
73.26Â±0.83
71.20Â±0.83
72.10Â±0.63
70.65Â±0.32
73.26Â±0.38
73.28Â±0.69
70.88Â±0.62
72.89Â±0.83
70.50Â±0.43
70.51Â±0.97
68.84Â±0.72
73.09Â±0.34
72.93Â±0.57
67.55Â±0.89
70.63Â±0.48
67.71Â±0.30
69.54Â±0.56
68.87Â±0.62
72.43Â±0.52
72.51Â±0.75
64.52Â±1.11
69.02Â±1.09
65.69Â±0.37
65.95Â±0.94
63.26Â±0.96
70.82Â±0.87
72.03Â±1.11
62.03Â±3.49
61.04Â±1.52
62.49Â±1.22
59.30Â±1.40
58.55Â±1.09
66.19Â±2.38
70.02Â±2.28
56.94Â±2.09
61.85Â±1.12
55.35Â±0.66
59.89Â±1.47
57.18Â±1.87
66.40Â±2.57
68.95Â±2.78
95.69Â±0.38
95.35Â±0.20
95.22Â±0.14
95.31Â±0.18
93.20Â±0.64
73.07Â±0.80
83.69Â±1.45
74.34Â±0.19
89.09Â±0.22
93.29Â±0.18
70.72Â±1.13
76.32Â±0.85
71.04Â±0.34
81.24Â±0.49
89.42Â±1.09
64.96Â±1.91
68.80Â±1.14
67.28Â±0.38
68.10Â±3.73
86.04Â±2.21
51.27Â±1.23
51.50Â±1.63
59.89Â±0.34
57.33Â±3.15
79.56Â±5.68
49.23Â±1.36
51.19Â±1.49
56.02Â±0.56
48.66Â±9.93
63.18Â±4.40
87.19Â±0.09
83.73Â±0.40
86.16Â±0.18
87.06Â±0.06
83.44Â±0.21
87.33Â±0.18
87.26Â±0.23
83.09Â±0.13
78.00Â±0.44
81.08Â±0.20
86.39Â±0.06
83.41Â±0.15
87.25Â±0.09
87.23Â±0.13
81.21Â±0.09
74.93Â±0.38
77.51Â±0.27
85.70Â±0.07
83.27Â±0.21
87.25Â±0.09
87.21Â±0.13
78.66Â±0.12
71.13Â±0.51
73.91Â±0.25
84.76Â±0.08
83.10Â±0.18
87.20Â±0.09
87.20Â±0.15
77.35Â±0.19
68.21Â±0.96
71.18Â±0.31
83.88Â±0.05
83.01Â±0.22
87.09Â±0.10
87.15Â±0.15
75.50Â±0.17
65.41Â±0.77
67.95Â±0.15
83.66Â±0.06
82.72Â±0.18
86.71Â±0.09
86.76Â±0.19
1 2 JaccardGCN and Pro-GNN cannot be directly applied to datasets where node features are not available.
Parameter Settings. For each graph, we randomly choose
10% of nodes for training, 10% of nodes for validation and the remaining 80% of nodes for testing. For each experiment, we report
the average performance of 10 runs. The hyper-parameters of all
the models are tuned based on the loss and accuracy on validation
set. For GCN and GAT, we adopt the default parameter setting
in the authorâ€™s implementation. For RGCN, the number of hidden units are tuned from {16, 32, 64, 128}. For GCN-Jaccard, the
threshold of similarity for removing dissimilar edges is chosen from
{0.01, 0.02, 0.03, 0.04, 0.05, 0.1}. For GCN-SVD , the reduced rank
of the perturbed graph is tuned from {5, 10, 15, 50, 100, 200}.
Defense Performance
To answer the first question, we evaluate the node classification
performance of Pro-GNN against three types of attacks, i.e., nontargeted attack, targeted attack and random attack:
â€¢ Targeted Attack: Targeted attack generates attacks on specific
nodes and aims to fool GNNs on these target nodes. We adopt
nettack for the targeted attack method, which is the stateof-the-art targeted attack on graph data.
â€¢ Non-targeted Attack: Different from targeted attack, the goal
of non-targeted attack is to degrade the overall performance of
GNNs on the whole graph. We adopt one representative nontargeted attack, metattack .
â€¢ Random Attack: It randomly injects fake edges into the graph.
It can also be viewed as adding random noise to the clean graph.
We first use the attack method to poison the graph. We then
train Pro-GNN and baselines on the poisoned graph and evaluate
the node classification performance achieved by these methods.
Against Non-targeted Adversarial Attacks. We first evaluate
the node classification accuracy of different methods against nontargeted adversarial attack. Specifically, we adopt metattack and
keep all the default parameter settings in the authorsâ€™ original implementation. The metattack has several variants. For Cora, Citeseer
and Polblogs datasets, we apply Meta-Self since it is the most destructive attack variant; while for Pubmed, the approximate version
of Meta-Self, A-Meta-Self is applied to save memory and time. We
vary the perturbation rate, i.e., the ratio of changed edges, from 0
to 25% with a step of 5%. As mentioned before, all the experiments
are conducted 10 times and we report the average accuracy with
standard deviation in Table 2. The best performance is highlighted
in bold. From the table, we make the following observations:
â€¢ Our method consistently outperforms other methods under different perturbation rates. For instance, on Polblogs dataset our
model improves GCN over 20% at 5% perturbation rate. Even under large perturbation, our method outperforms other baselines
by a larger margin. Specifically, under the 25% perturbation rate
on the three datasets, vanilla GCN performs very poorly and our
model improves GCN by 22%, 12% and 14%, respectively.
â€¢ Although GCN-SVD also employs SVD to get low-rank approximation of the graph, the performance of GCN-SVD drops rapidly.
This is because GCN-SVD is designed for targeted attack, it cannot adapt well to the non-targeted adversarial attack. Similarly,
Number of Perturbations Per Node
Test Accuracy
GCN-Jaccard
Pro-GNN-fs
Number of Perturbations Per Node
Test Accuracy
GCN-Jaccard
Pro-GNN-fs
(b) Citeseer
Number of Perturbations Per Node
Test Accuracy
Pro-GNN-fs
(c) Polblogs
Number of Perturbations Per Node
Test Accuracy
GCN-Jaccard
Pro-GNN-fs
(d) Pubmed
Figure 3: Results of different models under nettack
Perturbation Rate(%)
Test Accuracy
GCN-Jaccard
Pro-GNN-fs
Perturbation Rate(%)
Test Accuracy
GCN-Jaccard
Pro-GNN-fs
(b) Citeseer
Perturbation Rate(%)
Test Accuracy
Pro-GNN-fs
(c) Polblogs
Perturbation Rate(%)
Test Accuracy
GCN-Jaccard
Pro-GNN-fs
(d) Pubmed
Figure 4: Results of different models under random attack
GCN-Jaccard does not perform as well as Pro-GNN under different perturbation rates. This is because simply preprocessing the
perturbed graph once cannot recover the complex intrinsic graph
structure from the carefully-crafted adversarial noises. On the
contrary, simultaneously updating the graph structure and GNN
parameters with the low rank, sparsity and feature smoothness
constraints helps recover better graph structure and learn robust
GNN parameters.
â€¢ Pro-GNN achieves higher accuracy than Pro-GNN-fs especially
when the perturbation rate is large, which demonstrates the
effectiveness of feature smoothing in removing adversarial edges.
Against Targeted Adversarial Attack. In this experiment, nettack is adopted as the targeted-attack method and we use the default
parameter settings in the authorsâ€™ original implementation. Following , we vary the number of perturbations made on every
targeted node from 1 to 5 with a step size of 1. The nodes in test
set with degree larger than 10 are set as target nodes. For Pubmed
dataset, we only sample 10% of them to reduce the running time
of nettack while in other datasets we use all the target nodes. The
node classification accuracy on target nodes is shown in Figure 3.
From the figure, we can observe that when the number of perturbation increases, the performance of our method is better than other
methods on the attacked target nodes in most cases. For instance,
on Citeseer dataset at 5 perturbation per targeted node, our model
improves vanilla GCN by 23% and outperforms other defense methods by 11%. It demonstrates that our method can also resist the
targeted adversarial attack.
Against Random Attack. In this subsection, we evaluate how
Pro-GNN behaves under different ratios of random noises from 0%
to 100% with a step size of 20%. The results are reported in Figure 4.
The figure shows that Pro-GNN consistently outperforms all other
baselines and successfully resists random attack. Together with
observations from Sections 5.2.1 and 5.2.2, we can conclude that
Pro-GNN is able to defend various types of adversarial attacks. This
is a desired property in practice since attackers can adopt any kinds
of attacks to fool the system.
Importance of Graph Structure Learning
In the previous subsection, we have demonstrated the effectiveness
of the proposed framework. In this section, we aim to understand
the graph we learned and answer the second question.
Weights(values) in Adjacency Matrix
KDE Density
Normal Edges
Adversarial Edges
(a) Pubmed
Weights(values) in Adjacency Matrix
KDE Density
Normal Edges
Adversarial Edges
(b) Polblogs
Figure 5: Weight density distributions of normal and adversarial edges on the learned graph.
Normal Edges Against Adversarial Edges. Based on the fact
that adversary tends to add edges over delete edges , if the
model tends to learn a clean graph structure, the impact of the
adversarial edges should be mitigated from the poisoned graph.
Thus, we investigate the weights of normal and adversarial edges
in the learned adjacency matrix S. We visualize the weight density
distribution of normal and perturbed edges of S in Figure 5. Due to
the limit of space, we only show results on Pubmed and Polblogs
under metattack. As we can see in the figure, in both datasets,
the weights of adversarial edges are much smaller than those of
normal edges, which shows that Pro-GNN can alleviate the effect
of adversarial edges and thus learn robust GNN parameters.
Table 3: Node classification accuracy given the graph under
25% perturbation by metattack.
GCN-NoGraph
47.53Â±1.96
62.12Â±1.55
69.72Â±1.69
56.94Â±2.09
63.75Â±3.23
68.95Â±2.78
49.23Â±1.36
51.79Â±0.62
63.18Â±4.40
75.50Â±0.17
84.14Â±0.11
86.86Â±0.19
Performance on Heavily Poisoned Graph. In this subsection,
we study the performance when the graph is heavily poisoned. In
particular, we poison the graph with 25% perturbation by metattack .
If a graph is heavily poisoned, the performance of GCN will degrade
a lot. One straightforward solution is to remove the poisoned graph
structure. Specifically, when removing the graph structure, the
adjacency matrix will be all zeros and GCN normalizes the zero
matrix into identity matrix and then makes prediction totally by
node features. Under this circumstance, GCN actually becomes
a feed-forward neural network. We denote it as GCN-NoGraph.
We report the performance of GCN, GCN-NoGraph and Pro-GNN
when the graph is heavily poisoned in Table 3.
From the table, we first observe that when the graph structure is
heavily poisoned, by removing the graph structure, GCN-NoGraph
outperforms GCN. This observation suggests the necessity to defend poisoning attacks on graphs because the poisoned graph structure are useless or even hurt the prediction performance. We also
note that Pro-GNN obtains much better results than GCN-NoGraph.
This observation suggests that Pro-GNN can learn useful graph
structural information even when the graph is heavily poisoned.
Ablation Study
To get a better understanding of how different components help
our model defend against adversarial attacks, we conduct ablation
studies and answer the third question in this subsection.
Regularizers. There are four key predefined parameters, i.e.,
Î±, Î², Î³ and Î», which control the contributions for sparsity, low rank,
GNN loss and feature smoothness, respectively. To understand the
impact of each component, we vary the values of one parameter and
set other parameters to zero, and then check how the performance
changes. Correspondingly, four model variants are created: Pro-
GNN-Î±, Pro-GNN-Î², Pro-GNN-Î³ and Pro-GNN-Î». For example,
Pro-GNN-Î± denotes that we vary the values of Î± while setting Î², Î³
and Î» to zero. We only report results on Cora and Citeseer, since
similar patterns are observed in other cases, shown in Figure 6.
From the figure we can see Pro-GNN-Î± does not boost the
modelâ€™s performance too much with small perturbations. But when
the perturbation becomes large, Pro-GNN-Î± outperforms vanilla
GCN because it can learn a graph structure better than a heavily
poisoned adjacency graph as shown in Section 5.3.2. Also, Pro-
GNN-Î² and Pro-GNN-Î» perform much better than vanilla GCN. It
is worth noting that, Pro-GNN-Î² outperforms all other variants
except Pro-GNN, indicating that nuclear norm is of great significance in reducing the impact of adversarial attacks. It is in line
with our observation that adversarial attacks increase the rank of
the graph and enlarge the singular values. Another observation
from the figure is that, Pro-GNN-Î³ works better under small perturbation and when the perturbation rate increases, its performance
Perturbation Rate(%)
Test accuracy
GCN-NoGraph
Perturbation Rate(%)
Test accuracy
GCN-NoGraph
(b) Citeseer
Figure 6: Classification performance of Pro-GNN variants.
degrades. From the above observations, different components play
different roles in defending adversarial attacks. By incorporating
these components, Pro-GNN can explore the graph properties and
thus consistently outperform state-of-the-art baselines.
Two-Stage vs One-Stage. To study the contribution of jointly
learning structure and GNN parameters, we conduct experiments
with the variant Pro-GNN-two under metattack . Pro-GNN-two is
the two stage variant of Pro-GNN where we first obtain the clean
graph and then train a GNN model based on it. We only show
the results on Cora in Table 4 due to the page limitation. We can
observe from the results that although Pro-GNN-two can achieve
good performance under large perturbation, it fails to defend the
attacks when the perturbation rate is relatively low. The results
demonstrate that jointly learning structure and GNN parameters
can actually help defend attacks.
Parameter Analysis
In this subsection, we explore the sensitivity of hyper-parameters
Î±, Î²,Î³ and Î» for Pro-GNN. In the experiments, we alter the value of
Î±, Î²,Î³ and Î» to see how they affect the performance of our model.
More specifically, we vary Î± from 0.00025 to 0.064 in a log scale
of base 2, Î² from 0 to 5, Î³ from 0.0625 to 16 in a log scale of base
2 and Î» from 1.25 to 320 in a log scale of base 2. We only report
the results on Cora dataset with the perturbation rate of 10% by
metattack since similar observations are made in other settings.
The performance change of Pro-GNN is illustrated in Figure 7. As
we can see, the accuracy of Pro-GNN can be boosted when choosing
appropriate values for all the hyper-parameters. Different from Î³,
appropriate values of Î± and Î» can boost the performance but large
values will greatly hurt the performance. This is because focusing on
sparsity and feature smoothness will result in inaccurate estimation
on the graph structure. For example, if we set Î± and Î» to +âˆž, we
will get a trivial solution of the new adjacency matrix, i.e, S = 0. It
is worth noting that, appropriate value of Î² can greatly increase the
modelâ€™s performance (more than 10%) compared with the variant
without Î², while too large or too small value of Î² will hurt the
performance. This is also consistent with our observation in Section
5.4.1 that the low rank property plays an important role in defending
adversarial attacks.
CONCLUSION
Graph neural networks can be easily fooled by graph adversarial attacks. To defend against different types of graph adversarial
attacks, we introduced a novel defense approach Pro-GNN that
learns the graph structure and the GNN parameters simultaneously.
Table 4: Classification performance of Pro-GNN-two and Pro-GNN on Cora dataset
Ptb Rate (%)
Pro-GNN-two
73.31Â±0.71
73.70Â±1.02
73.69Â±0.81
75.38Â±1.10
73.22Â±1.08
70.57Â±0.61
82.98Â±0.23
82.27Â±0.45
79.03Â±0.59
76.40Â±1.27
73.32Â±1.56
69.72Â±1.69
Hyper-parameter
Test accuracy
Hyper-parameter
Test accuracy
0.0620.125 0.25
Hyper-parameter
Test accuracy
Hyper-parameter
Test accuracy
Figure 7: Results of parameter analysis on Cora dataset
Our experiments show that our model consistently outperforms
state-of-the-art baselines and improves the overall robustness under various adversarial attacks. In the future, we aim to explore
more properties to further improve the robustness of GNNs.
ACKNOWLEDGEMENTS
This research is supported by the National Science Foundation (NSF)
under grant numbers IIS1907704, IIS1928278, IIS1714741, IIS1715940,
IIS1845081, IIS1909702 and CNS1815636.