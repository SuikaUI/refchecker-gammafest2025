Auto-Balanced Filter Pruning for
Efﬁcient Convolutional Neural Networks∗
Xiaohan Ding,1 Guiguang Ding,1 Jungong Han,2 Sheng Tang3
1School of Software, Tsinghua University, Beijing 100084, China
2School of Computing and Communications, Lancaster University, Lancaster, LA1 4YW, UK
3Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China
 , , , 
In recent years considerable research efforts have been devoted to compression techniques of convolutional neural networks (CNNs). Many works so far have focused on CNN
connection pruning methods which produce sparse parameter tensors in convolutional or fully-connected layers. It has
been demonstrated in several studies that even simple methods can effectively eliminate connections of a CNN. However, since these methods make parameter tensors just sparser
but no smaller, the compression may not transfer directly to
acceleration without support from specially designed hardware. In this paper, we propose an iterative approach named
Auto-balanced Filter Pruning, where we pre-train the network
in an innovative auto-balanced way to transfer the representational capacity of its convolutional layers to a fraction of
the ﬁlters, prune the redundant ones, then re-train it to restore
the accuracy. In this way, a smaller version of the original network is learned and the ﬂoating-point operations (FLOPs) are
reduced. By applying this method on several common CNNs,
we show that a large portion of the ﬁlters can be discarded
without obvious accuracy drop, leading to signiﬁcant reduction of computational burdens. Concretely, we reduce the inference cost of LeNet-5 on MNIST, VGG-16 and ResNet-56
on CIFAR-10 by 95.1%, 79.7% and 60.9%, respectively.
Introduction
The past few years have witnessed rapid developments of
convolutional neural networks (CNNs) in the areas of computer vision, natural language processing, etc. However,
due to their nature of computational intensity, as CNNs
grow wider and deeper, their computational burdens have
increased dramatically, making them difﬁcult to deploy on
embedded systems. Therefore, this research community is
soliciting the solutions that are able to simplify the CNNs
but without losing too much accuracy.
Recent researches on CNN compression methods have attracted much attention. Some excellent works, such as and , have explored
∗This research was supported by the National Natural Science
Foundation of China (Grant No. 61571269) and the Royal Society Newton Mobility Grant (IE150997). Corresponding author:
Guiguang Ding.
Copyright c⃝2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
connection-wise pruning techniques. However, these methods make the parameter tensors of convolutional or fullyconnected layers just sparser, but no smaller, hence the compression may not transfer directly to acceleration without
support from specially designed hardware .
Some other works focus on ﬁlter-wise pruning approaches
and have made promising achievements, which can be divided into two categories. One category of works tends to
impose sparse constraints on the model and produce a compact network by training, where the representatives include
group sparsity regularizer ,
group Lasso regularization , tensor low rank constraints and group sparse constraints . Another category of works iteratively discards unimportant ﬁlters from a well-trained network and usually requires re-training to avoid severe accuracy drop. measures a ﬁlter’s importance by the classiﬁcation accuracy reduction of the network after pruning it, and eliminates one single ﬁlter at each
iteration based on this metric. and
 prune ﬁlters in a similar iterative way but
by different metrics. evaluates a ﬁlter’s importance by its output on a subset of the training data and
prunes a fraction of the ﬁlters in several layers simultaneously at one single iteration.
In this paper, we aim to combine the advantages of the
two paradigms. On the one hand, we are inspired by the idea
that the network can be trained under a certain constraint to
introduce structured sparsity in the parameters. On the other
hand, a progressive approach consisting of alternate pruning and re-training is more suitable for pursuing the highest
compression rate, since we can easily roll back to a previous state when the network is irreversibly damaged. Taking
both points into account, we seek for an approach where we
ﬁne-tune a well-trained model to make it robust to pruning,
then alternately prune and re-train it to produce a signiﬁcantly smaller network. The rationale behind our idea is that
we can certainly pre-train the network as a prevention of the
structural damage at the very beginning, instead of making
up for the performance reduction after pruning.
There is thereby an urgent need but it is still a signiﬁcant
challenge to ﬁnd a powerful pre-training method before the
The Thirty-Second AAAI Conference
on Artificial Intelligence (AAAI-18)
ﬁrst pruning which prepares the network for the upcoming
pruning process as well as maintains the accuracy. As a candidate approach, reducing the absolute value of parameters
is most natural: considering that the parameters are set to
zero during pruning, the closer to zero they used to be, the
less structural damage pruning does to the network . Though uses ℓ1 and ℓ2 regularizations to zero out parameters in the connection-wise iterative pruning pipeline and gets encouraging results, it must
be pointed out that ﬁlter-wise pruning is something different:
it’s hard to push all parameters in one ﬁlter close to zero simultaneously. If applying ℓ1 or ℓ2 regularization is expected
to zero out a whole ﬁlter, the regularization factor must be
much larger. Unfortunately, an overly strong regularization
puts another problem on the table: when the ℓ1 or ℓ2 regularization term added to the objective function becomes larger,
the accuracy loss is considered less important due to the nature of back-propagation algorithm, thus degrading the network performance. When we use usual ℓ1 or ℓ2 regularization in the proposed pipeline, we encounter a dilemma. If the
regularization factor is small, ﬁlters cannot be effectively zeroed out, thus the pruning operation causes severe accuracy
reduction. However, if the factor is large, the network’s performance gets badly harmed during the pre-training process.
As our ﬁrst contribution, we propose an approach called
Auto-balanced Regularization to address this problem,
where we apply ℓ2 regularization with positive factors on
unimportant ﬁlters and negative factors on important ones.
The intuition is that we dynamically adjust the negative
factors such that the sum of positive and negative regularization terms is always kept zero during training, meaning
that as weak ﬁlters become weaker, the strong ones grow
even stronger accordingly. Training with this regularization
is able to transfer the model’s representational capacity to a
fraction of its ﬁlters, thus minimizing performance drop in
the forthcoming pruning stage.
As our second contribution, we introduce a pre-training
stage to common iterative pruning methods, as demonstrated
in Figure 1. To be more speciﬁc, we propose a ﬁlter-wise
pruning pipeline called Auto-balanced Filter Pruning (AFP),
which removes redundant ﬁlters from a well-trained CNN
to produce a smaller one, signiﬁcantly reducing the computational cost. In this pipeline, the network is pre-trained
at the very beginning, then iteratively pruned and re-trained
to reach a satisfactory compression rate. In the pre-training
process, we ﬁne-tune the network with Auto-balanced Regularization to change the ﬁlter-wise distribution of parameters. In the pruning stage, the unimportant ﬁlters are discarded along with the corresponding feature maps. After
pruning, the model is re-trained to restore its accuracy. Note
that re-training is also done with our proposed regularization, hence each re-training stage can also be viewed as
the pre-training stage of the next iteration. Besides, instead
of pruning layer by layer, we prune ﬁlters from all convolutional layers simultaneously. To this end, we propose a
novel iteration strategy named Abreast Advancing to decide
which ﬁlters to prune at each iteration. By performing experiments on several common CNNs, we show that a large portion of the ﬁlters can be discarded without obvious accuracy
drop, leading to signiﬁcant reduction of computational burdens. Concretely, we reduce the inference cost of LeNet-5 on
MNIST, VGG-16 and ResNet-56 on CIFAR-10 by 95.1%,
79.7% and 60.9%, respectively.
Step Forward
No. Bad performance.
No. Make it smaller.
Figure 1: Auto-balanced Filter Pruning Pipeline
Related Work
Connection-wise CNN Pruning
A natural way to compress a CNN is removing some of
its unimportant parameters. Nevertheless, deﬁning what is
important is a non-trivial work. Optimal Brain Damage
 and Optimal Brain Surgeon use second-order Taylor expansion to calculate the metric for importance of the parameters. However, these two methods require computation of the second derivatives, which is costly for today’s
deep CNNs. proposes a simple but effective approach where the importance of a parameter is
measured by its absolute value. Based on that, the CNN
is iteratively pruned and ﬁne-tuned. In the pruning stage,
a layer’s unimportant parameters, i.e. the parameters with
absolute values below a certain threshold, are set to zero,
hence the network’s accuracy drops due to the structural
damage. Later in the re-training stage, the model recovers
from the damage and restores its accuracy. After every iteration, the threshold for each layer is raised, thus producing increasingly sparser weight parameters. In the experiments, LeNet-5 , AlexNet and VGG-16 are successfully compressed by an order of
magnitude. However, the compression effects mainly come
from fully-connected layers and the pruning of convolutional layers remains challenging.
Filter-wise CNN Pruning
In the past few years, several fully-convolutional neural networks have made satisfactory achievements in the
areas of semantic segmentation, object detection, etc. Besides, a substitute for fully-connected layers named global
average pooling has been applied in some main-stream
CNNs . In this
context, compression techniques for convolutional layers are
attracting much attention.
Recently the application of sparsity constraints on CNN
compression has been intensively investigated . These works
use group sparsity based constraints to penalize unimportant
parameters and learn a small compact network from a big
redundant one.
Another way of ﬁlter pruning is iteratively deleting unimportant ﬁlters from a well-trained network. , , , and have discussed different iteration
strategies as well as various metrics to measure the ﬁlter importance. Apparently, the number of ﬁlters pruned at each
iteration is a sort of trade-off to be solved: the fewer ﬁlters
are discarded once, the less damage is done to the network
structure, which means the less retraining time is required
for the network to restore the accuracy; but more iterations
are needed to reach a satisfactory compression rate.
Other Methods
Apart from pruning, there are some other CNN compression
methods such as parameter quantization , distillation 
and binarization , which are complementary to ours. It’s important to
point out that since our method simply learns a small and
compact full-precision network with no sparsity or custom
structure, it’s promising to combine it with other methods
and achieve even higher compression rate.
The Proposed Method
Filter-wise Pruning
In contrast to previous connection-wise pruning methods,
we prune a CNN at the ﬁlter level. Every time a CNN is
pruned, some 3-D ﬁlters along with the corresponding feature maps are deleted, resulting in a structural change in
the network. It must be mentioned that when several convolutional layers stacked together are pruned, the FLOPs of
the entire network are reduced quadratically. Imagine a simple CNN with n convolutional layers and m fully-connected
layers on top, let ui be the FLOPs of convolutional layer
i and vi be the FLOPs of fully-connected layer i, then the
original inference cost can be represented as
Let oi be the original number of ﬁlters in the convolutional
layer i and ri be the number of remaining ﬁlters after pruning. Then the FLOPs of the pruned network is
Metric for Filter Importance
It’s natural to measure a ﬁlter’s importance by observing the
performance drop of the network if it is deleted . However, calculating the ﬁlter’s importance in the way that remeasures the whole network’s performance after discarding ﬁlters one by one is computationally impractical. Therefore, how to approximate a ﬁlter’s importance without bringing in too much computational burdens becomes an inevitable topic. Recently many
researchers have focused on this topic and proposed some
enlightening metrics.
 uses ℓ1 norm to evaluate the importance of
a ﬁlter for the ease of the calculation. This metric is naive but
natural, for a parameter with a bigger absolute value is more
likely to have a strong inﬂuence on the network’s output.
Inspired by the early works of , some researchers choose
to approximate a ﬁlter’s importance in a Taylor expansion
based way . Instead of simply summing up ﬁlter parameters, an evaluation
process is performed on a subset of the training set to calculate the metrics. Though costly, these methods usually outperform the naive ways.
In our experiments, we choose ℓ1 norm as the ﬁlter importance metric to minimize the time cost as well as produce experimental results comparable to . Formally,
let Mi,j be the importance metric of the j-th ﬁlter in the i-th
convolutional layer, Fi,j be the corresponding 3-D parameter tensor, and vec be the function that ﬂattens a 3-D tensor
to obtain a vector, we have
Mi,j = ||vec(Fi,j)||1 .
Auto-balanced Training
 has demonstrated that ℓ2 norm gives better experimental results than ℓ1 norm as a means of regularization for iterative pruning. Considering that, we apply
a customized ℓ2 regularization to penalize weak ﬁlters and
stimulate strong ones to grow stronger. Speciﬁcally, we set
a hyper-parameter ri for each layer i to represent the target
number of remaining ﬁlters after pruning. For each layer i,
we pick ri ﬁlters with the biggest importance metric, i.e. ℓ1
norm, to form a remaining set denoted as Ri and the others to make up a to-be-pruned set denoted as Pi. We deﬁne
a threshold value θi as the ri-th largest importance metric
among all the ﬁlters in layer i. Then we have
∀Fi,j ∈Pi ,
∀Fi,j ∈Ri .
An ℓ2 regularization term is added on every ﬁlter with
different factors. The factors are positive for the ﬁlters in Pi
and negative for those in Ri. Furthermore, for the ﬁlters in
Pi, the weaker they are, the harder they are penalized. And
for those in Ri, the stronger they are, the more intensely
they are stimulated. Since the magnitude of the parameters
in different layers may vary considerably, a ﬁlter’s importance is only comparable to the other ones in the same layer.
In view of that, we deﬁne the factors based on θi as follows,
if Fi,j ∈Pi ,
−1 −log Mi,j
if Fi,j ∈Ri .
Note that these factors are calculated when a training
stage starts and remain unchanged until the end of the training. This design brings about another beneﬁt: when each retraining stage starts and Mi,j for each ﬁlter j in convolutional layer i is updated, if one ﬁlter Fi,j has been penalized
at the last iteration but not yet pruned, λi,j becomes larger,
since θi is not likely to have signiﬁcantly decreased. In this
way, the penalization of Fi,j becomes harder at the current
iteration so that the training process is accelerated. We use
logarithm here because the parameters may vary in magnitude but we don’t want their regularization factors to differ
that enormously. ϵ is set to 1 × 10−12 to avoid division by
zero. Then we have the positive regularization term for Pi
and the negative term for Ri denoted by S(Pi) and S(Ri),
λi,j||vec(Fi,j)||2
λi,j||vec(Fi,j)||2
We denote the union of all to-be-pruned sets by P and the
union of all remaining sets by R, thus we have,
The positive regularization terms of all convolutional layers are added to the network’s original objective function by
a factor α. Similarly, all negative terms are added by a factor
τ. Let Lo denote the original objective function including
common regularization terms, if any, then the new optimization objective of training is to minimize
L = Lo + αS(P ) + τS(R) .
Therefore, the gradients of ﬁlters are changed as
+ 2αλi,jFi,j
if Fi,j ∈Pi ,
+ 2τλi,jFi,j
if Fi,j ∈Ri .
Since λi,j > 0 ∀Fi,j ∈Pi, λi,j < 0 ∀Fi,j ∈Ri, the
weak ﬁlters are penalized and the strong ones are stimulated, respectively. Notably, τ is not a hyper-parameter but
calculated and reset before every batch of data is fed into
the network during training (Eq. 13). Therefore, our method
requires two hyper-parameters, namely the n-dimensional
vector r which represents the target number of remaining
ﬁlters in each layer, and the regularization factor α.
τ = −αS(P )
The word auto-balanced includes two meanings. On the
one hand, according to Eq. 13, the intensity of stimulation
on strong ﬁlters varies with the weak ones. When the weak
ﬁlters are zeroed out, the stimulation automatically stops and
the training converges. On the other hand, as the weak ﬁlters in a certain layer are weakened and the strong ones are
stimulated, the representational capacity of the weak part is
gradually transferred to the strong part, keeping the whole
layer’s representational capacity unharmed.
Abreast Advancing Iterative Pruning
Algorithm 1 Abreast Advancing Iterative Pruning
Require: Original network No; Target number of remaining ﬁlters vector r; Schedule vector s
1: Initialize: Index i = 1; Current progress p = 0
2: Build network N by imposing Auto-balanced Regularization to No
3: Pre-train N
Calculate vector z according to r and p, which denotes the number of ﬁlters to be pruned in each layer
at current iteration
For each layer i, remove zi ﬁlters with the smallest
Re-construct Auto-balanced Regularization of N
Re-train N
11: until p == 1
12: return N
For deep CNNs, such as VGG-16 and ResNet-56 , pre-training once
is not enough for transferring the representational capacity
of the weak ﬁlters to the strong ones entirely. Considering
that, an iterative process of pruning and re-training is applied
for these deep networks. In the pre-training stage, the network polarizes its ﬁlters and gets prepared for the pruning. In
every pruning iteration, the network is pruned and re-trained.
Concretely, in the pruning stage, some of the weak ﬁlters are
discarded and the remaining ones are re-organized to make
up a smaller network. In the following re-training stage, the
new network is re-trained to restore its accuracy and further polarize the remaining ﬁlters. So essentially, every retraining stage can also be seen as the pre-training stage of
the next iteration.
The iteration strategy that decides which ﬁlters to prune
at each iteration plays an important role in the pipeline.
In order to preserve the holistic structure of the original
model during training as well as reduce the time cost of the
pipeline, we do not prune the network layer by layer, which
is the most conservative and popular style. Instead, we propose a novel iteration strategy named Abreast Advancing,
which means that the pruning tasks for all convolutional layers are carried out synchronously. Concretely, we pre-set the
target number of pruned ﬁlters for each layer, and the ratios
of the already pruned ﬁlters and the target numbers are kept
the same among all convolutional layers at each iteration.
For simplicity, we use a vector s named schedule vector to
denote the pruning progress at each iteration. The elements
in s must be strictly increasing and the last one must be 1.
For example, s = (0.5, 0.75, 1) means that 50%, 75% and
100% of the target ﬁlters in every convolutional layer are
synchronously removed at each iteration, respectively, thus
the whole pruning process is ﬁnished within 3 iterations. We
specify the algorithm in Algorithm 1.
Experiment
We evaluate the effectiveness of our proposed method (AFP)
by pruning LeNet-5 on MNIST, VGG-
16 and ResNet-56 on CIFAR-10. We perform our experiments with
tensorﬂow 1.01 on one NVIDIA TITAN
X GPU. Every pruning experiment starts from a well-trained
model and produces a smaller network with dramatically reduced FLOPs and comparative accuracy.
LeNet-5 on MNIST
MNIST is a well-known database of handwritten digits containing 60,000 images for training and 10,000 for testing.
We perform experiments on a version of LeNet-5 deﬁned in
 , which consists of two convolutional and
two fully-connected layers. Since the two convolutional layers comprise 20 and 50 ﬁlters, respectively, we use a tuple
(20, 50) to denote the network structure for simplicity. We
train LeNet-5 from scratch on MNIST using the same training parameters as . The trained network is
tested on MNIST test set and achieves an error rate of 0.83%.
Since LeNet-5 is shallow, there is no need for pruning it
in a progressive way. We simply pre-train the network with
Auto-balanced Regularization and the ﬁlters are polarized
perfectly. According to our most satisfactory result labeled
as AFP-C, the error rate even further decreases to 0.76%. In
the end, the weak ﬁlters are discarded and the error rate remains unchanged. Some experimental results of our method
and another comparable work are demonstrated in Table 1.
To demonstrate the effectiveness of Auto-balanced Regularization, we visualize the ℓ1 norm of all the ﬁlters of the
result AFP-C before and after pre-training in Figure 2. Obviously, all the ﬁlters except for the 3 and 8 strongest ones
in two convolutional layers are zeroed out. It’s noteworthy
that, if pre-trained properly, the network even performs better with far fewer ﬁlters, which is consistent with the observation from . A possible explanation is that
eliminating parameters is an effective way of regularization
which reduces overﬁtting of the network .
(a) conv-1
(b) conv-2
Figure 2: The change of ℓ1 norm of LeNet-5 ﬁlters
Table 1: LeNet-5 Experimental Results
4.40 × 106
5.97 × 105
2.89 × 105
3.07 × 105
1.95 × 105
2.14 × 105
1.58 × 105
VGG-16 on CIFAR-10
CIFAR-10 is a widely used database comprising 50,000
RBG images of 32 × 32 pixels for training and 10,000 for
testing. In our experiments, we use random horizontal ﬂip
and random shift for data augmentation.
The experiments in this section are based on a VGG-16
version same as . Concretely, the overall
convolutional architecture remains the same as , while every convolutional layer is followed by a batch normalization 
layer. Besides, the fully-connected part is re-designed: only
one hidden layer with 512 neurons is applied without
dropout . The network is trained from
scratch for 160 epochs and eventually achieves 7.08% error
We set the hyper-parameter r based on the layer-wise histogram of ℓ1 norm of ﬁlters in the original network. Basically, the steeper the histogram looks, the more ﬁlters we
prune from the layer. Note that if we deﬁne this hyperparameter too conservatively and result in an unsatisfactory compression effect, we can adjust r and restart our
pipeline with the previous output as initialization, thus no
effort is wasted. Based on the histograms of the 13 layers,
we prune 50% of the ﬁlters from layer 4-8 and 90% of the
ﬁlters from layer 9-13. We only shift r1, r2 and r3, namely
= (r1, r2, r3, 64, 128, 128, 128, 256, 52, 52, 52, 52, 52).
Considering that the original ℓ2 regularization factor of the
network is 5 × 10−4, we set α to be an order larger i.e.
5 × 10−3. Since VGG-16 is deep, an Abreast Advancing
pipeline is applied with s = (0.5, 0.9, 1). We train the network with a learning rate of 3 × 10−4 and 3 × 10−5 for 50
epochs respectively in each training stage. We ﬁrst perform
experiments with no Auto-balanced Regularization but only
Abreast Advancing (AA) iterative pruning and then apply
the complete version of AFP. It can be seen from Table 2
that our method is able to reduce the FLOPs of VGG-16 on
CIFAR-10 by 79.69% with acceptable accuracy drop using
only Abreast Advancing. Better still, when AFP is applied,
no accuracy loss is observed (AFP-E).
Table 2: VGG-16 Experimental Results
r1, r2, r3
3.13 × 108
2.06 × 108
3.13 × 108
6.37 × 107
6.37 × 107
5.83 × 107
5.83 × 107
ResNet-56 on CIFAR-10
We use the same ResNet-56 architecture as described in , which contains 3 stages of convolutional layers
connected by projection mapping and followed by a global
average pooling layer and one fully-connected layer. After
trained on CIFAR-10 from scratch using the same training
parameters as , the network achieves an error
rate of 6.07%.
For simplicity and consistency, we number the convolutional layers of ResNet-56 as follows: the ﬁrst layer is numbered 1; from the shallower to the deeper in one stage, the
number increases; when it comes to the junction of two adjacent stages, we number the projection layer before the two
convolutional layers in the parallel residual block.
Due to the special structure of identity mapping, the
pruned indexes of the input and output feature maps of one
Table 3: ResNet-56
1.25 × 108
1.12 × 108
9.04 × 107
1.42 × 108
5.55 × 107
5.72 × 107
4.14 × 107
residual block must be identical , or the residual mechanism goes wrong. With Di denoting the set of discarded ﬁlter indexes of layer i, we have D1 = D3 = D5 =
. . . = D19, D20 = D22 = D24 = . . . = D38,D39 = D41 =
D43 = . . . = D57 as constraints. Since we cannot expect
that the important ﬁlters in the same stage are in the same
positions, pruning this network is a real challenge,
 performs experiments on ResNet-56 in
a conservative way. To avoid changing the input and output feature maps of each residual block, 
only prunes ﬁlters from the ﬁrst layers of each block.
Namely, only ﬁlters from layer 2,4,6,...,18 in stage 1, layer
21,23,25,...,37 in stage 2 and layer 40,42,44,... ,56 in stage
3 are pruned.
We perform our experiments far more aggressively by satisfying
∀1 < i ≤19 ,
∀20 < i ≤38 ,
∀39 < i ≤57 ,
which means all convolutional layers in the same stage (if
we group projection 1 into stage 2 and projection 2 into stage
3) are pruned following the same pattern. To this end, we
pick 3 layers from the 3 stages respectively as the pacesetters and force every other convolutional layer to transfer its
representational capacity exactly the same way as its corresponding pacesetter does. We use Mi := Mj as a simple
notation for assigning Mi,k := Mj,k
∀k ≤h where layer
i and j both have h ﬁlters. With this notation, we assign
Ma := Mb to force layer a to adjust its ﬁlters just like layer
b does. In our experiments, we pick the ﬁrst layer of stage
1, the second layer of stage 2 and the second layer of stage
3 as pacesetters. Note that since projection layers have 2×2
kernels instead of 3×3, we pick the ﬁrst convolutional layer
in the ﬁrst residual block of stage 2 but not the projection
layer as the pacesetter simply because it is more similar to
other convolutional layers, and the same is true for stage 3.
Formally, that is
∀1 ≤i ≤19 ,
∀20 ≤i ≤38 ,
∀39 ≤i ≤57 .
In practice, we simply calculate the ℓ1 norms of layer 1,
21 and 40 as the importance metrics and apply them to calculate the λ values of all ﬁlters. Since we don’t expect the
importance of all the ﬁlters in one stage to be of identical
distribution, some important ﬁlters may be penalized and
unimportant ones may be stimulated. Aggressive as it is, our
method performs well. We use a triple q to denote the number of remaining ﬁlters in each convolutional layer within
the 3 stages. Our results are demonstrated in Table 3 together
with a comparable work . We also visualize
the ℓ1 norm of ﬁlters in layer 4 and layer 14 after pre-training
but before pruning. We choose these two layers simply because of their distinctly different distribution of parameters.
It is observed that even if important ﬁlters are penalized and
zeroed out to follow the pattern of the pacesetter i.e. layer 1,
the network performs well.
Though we cannot learn a smaller network with an accuracy higher than our baseline, the experiment is still a
success considering that ResNet-56 is an originally compact
network with complex structure. Concretely, our method is
able to reduce the FLOPs of ResNet-56 by 60.86% with
an acceptable accuracy decrease of 0.99%. This experiment
demonstrates that our method is able to not only utilize and
amplify the inherent difference between ﬁlters, but also reshape the network drastically as we wish.
(a) conv-4
(b) conv-14
Figure 3: The change of ℓ1 norm of ResNet-56 ﬁlters
Conclusion
To prune CNNs at the ﬁlter level, we propose a method
named Auto-balanced Filter Pruning (AFP), which learns
a small compact network from a big redundant one, hence
signiﬁcantly reduces the FLOPs of the network. We propose
Auto-balanced Regularization, which penalize the weak ﬁlters as well as stimulate the strong ones, transferring the
representational capacity of a whole convolutional layer to
a fraction of its ﬁlters. Furthermore, we propose a pruning
pipeline, which contains one pre-training stage before the iterative pruning and re-training process. In the pre-training
stage, we train the network with Auto-balanced Regularization and polarize its ﬁlters. In the pruning stage, some of
the weak ﬁlters are discarded along with the corresponding
feature maps. In the re-training stage, the network is trained
to restore its accuracy and prepare for the next pruning. Besides, instead of pruning layer by layer, we propose a novel
iteration strategy named Abreast Advancing, which is aimed
to preserve the original holistic network structure as well as
reduce the time cost of the pipeline. By applying this method
on a few common CNNs, we reduce the inference cost of
LeNet-5 on MNIST, VGG-16 and ResNet-56 on CIFAR-10
by 95.1%, 79.7% and 60.9%, respectively.