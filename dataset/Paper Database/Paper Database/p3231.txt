Adversarial Examples for Semantic Segmentation and Object Detection
Cihang Xie1*, Jianyu Wang2*, Zhishuai Zhang1∗, Yuyin Zhou1, Lingxi Xie1, Alan Yuille1
1Department of Computer Science, The Johns Hopkins University, Baltimore, MD 21218 USA
2Baidu Research USA, Sunnyvale, CA 94089 USA
{cihangxie306, wjyouch, zhshuai.zhang, zhouyuyiner, 198808xc, alan.l.yuille}@gmail.com
It has been well demonstrated that adversarial examples,
i.e., natural images with visually imperceptible perturbations added, cause deep networks to fail on image classiﬁcation. In this paper, we extend adversarial examples
to semantic segmentation and object detection which are
much more difﬁcult.
Our observation is that both segmentation and detection are based on classifying multiple
targets on an image (e.g., the target is a pixel or a receptive
ﬁeld in segmentation, and an object proposal in detection).
This inspires us to optimize a loss function over a set of
pixels/proposals for generating adversarial perturbations.
Based on this idea, we propose a novel algorithm named
Dense Adversary Generation (DAG), which generates a
large family of adversarial examples, and applies to a wide
range of state-of-the-art deep networks for segmentation
and detection.
We also ﬁnd that the adversarial perturbations can be transferred across networks with different
training data, based on different architectures, and even
for different recognition tasks.
In particular, the transferability across networks with the same architecture is
more signiﬁcant than in other cases. Besides, summing up
heterogeneous perturbations often leads to better transfer
performance, which provides an effective method of blackbox adversarial attack.
1. Introduction
Convolutional Neural Networks (CNN) 
have become the state-of-the-art solution for a wide range of
visual recognition problems. Based on a large-scale labeled
dataset such as ImageNet and powerful computational
resources like modern GPUs, it is possible to train a hierarchical deep network to capture different levels of visual
A deep network is also capable of generating
transferrable features for different tasks such as image classiﬁcation and instance retrieval , or being ﬁne-tuned
∗The ﬁrst three authors contributed equally to this work. This work was
done when Jianyu Wang was a Ph.D. student at UCLA
dog : 1.000
dog : 0.986
dog : 0.640
cow : 0.538
person : 0.512
train : 0.954
Figure 1: An Adversarial example for semantic segmentation and object detection.
FCN is used for segmentation, and Faster-RCNN is used for detection.
Left column: the original image (top row) with the normal segmentation (the purple region is predicted as dog)
and detection results. Right column: after the adversarial
perturbation (top row, magniﬁed by 10) is added to the
original image, both segmentation (the light green region as
train and the pink region as person) and detection results are
completely wrong. Note that, though the added perturbation
can confuse both networks, it is visually imperceptible (the
maximal absolute intensity in each channel is less than 10).
to deal with a wide range of vision tasks, including object
detection , visual concept discovery , semantic
segmentation , boundary detection , etc.
 
Despite their success in visual recognition and feature
representation, deep networks are often sensitive to small
perturbations to the input image.
In , it was shown
that adding visually imperceptible perturbations can result
in failures for image classiﬁcation. These perturbed images,
often called adversarial examples, are considered to fall
on some areas in the large, high-dimensional feature space
which are not explored in the training process. Thus, investigating this not only helps understand the working mechanism of deep networks, but also provides opportunities to
improve the robustness of network training.
In this paper, we go one step further by generating adversarial examples for semantic segmentation and object
detection, and showing the transferability of them.
the best of our knowledge, this topic has not been systematically studied (e.g., on a large dataset) before. Note
that these tasks are much more difﬁcult, as we need to
consider orders of magnitude more targets (e.g., pixels or
proposals). Motivated by the fact that each target undergoes
a separate classiﬁcation process, we propose the Dense
Adversary Generation (DAG) algorithm, which considers
all the targets simultaneously and optimizes the overall
loss function. The implementation of DAG is simple, as
it only involves specifying an adversarial label for each
target and performing iterative gradient back-propagation.
In practice, the algorithm often comes to an end after a
reasonable number of, say, 150 to 200, iterations. Figure 1
shows an adversarial example which can confuse both deep
segmentation and detection networks.
We point out that generating an adversarial example is
more difﬁcult in detection than in segmentation, as the
number of targets is orders of magnitude larger in the former
case, e.g., for an image with K pixels, the number of possible proposals is O
while the number of pixels is only
O(K), where O(·) is the big-O notation. In addition, if only
a subset of proposals are considered, the perturbed image
may still be correctly recognized after a new set of proposals
are extracted (note that DAG aims at generating recognition
failures on the original proposals). To increase the robustness of adversarial attack, we change the intersection-overunion (IOU) rate to preserve an increased but still reasonable number of proposals in optimization. In experiments,
we verify that when the proposals are dense enough on the
original image, it is highly likely that incorrect recognition
results are also produced on the new proposals generated on
the perturbed image. We also study the effectiveness and
efﬁciency of the algorithm with respect to the denseness of
the considered proposals.
Following , we investigate the transferability of the
generated perturbations. To this end, we use the adversarial
perturbation computed on one network to attack another
Three situations are considered: (1) networks
with the same architecture but trained with different data;
(2) networks with different architectures but trained for the
same task; and (3) networks for different tasks. Although
the difﬁculty increases as the difference goes more signiﬁcant, the perturbations generated by DAG is able to transfer
to some extent. Interestingly, adding two or more heterogeneous perturbations signiﬁcantly increases the transferability, which provides an effective way of performing blackbox adversarial attack to some networks with unknown
structures and/or properties.
The remainder of this paper is organized as follows. Section 2 brieﬂy introduces prior work related to our research.
Section 3 describes our algorithm for generating adversarial
perturbations, and Section 4 investigates the transferability
of the perturbations. Conclusions are drawn in Section 5.
2. Related Work
2.1. Deep Learning for Detection and Segmentation
Deep learning approaches, especially deep convolutional neural networks, have been very successful in object
detection and semantic segmentation 
Currently, one of the most popular object detection pipeline involves ﬁrst generating a number
of proposals of different scales and positions, classifying
each of them, and performing post-processing such as nonmaximal suppression (NMS). On the other hand, the dominating segmentation pipeline works by ﬁrst predicting
a class-dependent score map at a reduced resolution, and
performing up-sampling to obtain high-resolution segmentation.
 incorporates the “atrous” algorithm and the
conditional random ﬁeld (CRF) to this pipeline to improve
the segmentation performance further.
2.2. Adversarial Attack and Defense
Generating adversarial examples for classiﬁcation has
been extensively studied in many different ways recently.
 ﬁrst showed that adversarial examples, computed by
adding visually imperceptible perturbations to the original
images, make CNNs predict a wrong label with high conﬁdence. proposed a simple and fast gradient sign method
to generate adversarial examples based on the linear nature
of CNNs. proposed a simple algorithm to compute
the minimal adversarial perturbation by assuming that the
loss function can be linearized around the current data point
at each iteration. showed the existence of universal
(image-agnostic) adversarial perturbations.
 trained a
network to generate adversarial examples for a particular
target model (without using gradients). showed the
adversarial examples for machine learning systems also
exist in the physical world. studied the transferability
of both non-targeted and targeted adversarial examples, and
proposed an ensemble-based approaches to generate adversarial examples with stronger transferability. generated
images using evolutionary algorithms that are unrecognizable to humans, but cause CNNs to output very conﬁdent
(incorrect) predictions. This can be thought of as in the
opposite direction of above works.
In contrast to generating adversarial examples, there
are some works trying to reduce the effect of adversarial
 proposed a forveation-based mechanism
to alleviate adversarial examples. showed networks
trained using defensive distillation can effectively against
adversarial examples, while developed stronger attacks
which are unable to defend by defensive distillation. 
trained the network on adversarial examples using the largescale ImageNet, and showed that this brings robustness to
adversarial attack. This is imporved by , which proposed an ensemble adversarial training method to increase
the network robustness to black-box attacks. trained
a detector on the inner layer of the classiﬁer to detect
adversarial examples.
There are two concurrent works and that studied adversarial examples in semantic segmentation on the
Cityscapes dataset , where showed the existence of
adversarial examples, and showed the existence of
universal perturbations. We refer interested readers to their
papers for details.
3. Generating Adversarial Examples
In this section, we introduce DAG algorithm. Given an
image and the recognition targets (proposals and/or pixels),
DAG generates an adversarial perturbation which is aimed
at confusing as many targets as possible.
3.1. Dense Adversary Generation
Let X be an image which contains N recognition targets
T = {t1, t2, . . . , tN}. Each target tn, n = 1, 2, . . . , N,
is assigned a ground-truth class label ln ∈{1, 2, . . . , C},
where C is the number of classes, e.g., C = 21 (including
the background class) in the PascalVOC dataset . Denote L = {l1, l2, . . . , ln}. The detailed form of T varies
among different tasks.
In image classiﬁcation, T only
contains one element, i.e., the entire image. Conversely,
T is composed of all pixels (or the corresponding receptive ﬁelds) in semantic segmentation, and all proposals in
object detection. We will discuss how to construct T in
Section 3.2.
Given a deep network for a speciﬁc task, we use
f(X, tn) ∈RC to denote the classiﬁcation score vector
(before softmax normalization) on the n-th recognition target of X.
To generate an adversarial example, the goal
is to make the predictions of all targets go wrong, i.e.,
∀n, arg maxc {fc(X + r, tn)} ̸= ln. Here r denotes an
adversarial perturbation added to X.
To this end, we
specify an adversarial label l′
n for each target, in which
n is randomly sampled from other incorrect classes, i.e.,
Algorithm 1: Dense Adversary Generation (DAG)
Input : input image X;
the classiﬁer f(·, ·) ∈RC;
the target set T = {t1, t2, . . . , tN};
the original label set L = {l1, l2, . . . , lN};
the adversarial label set L′ = {l′
2, . . . , l′
the maximal iterations M0;
Output: the adversarial perturbation r;
1 X0 ←X, r ←0, m ←0, T0 ←T ;
2 while m < M0 and Tm ̸= ∅do
Tm = {tn | arg maxc {fc(Xm, tn)} = ln};
∇Xmfl′n(Xm, tn) −∇Xmfln(Xm, tn)
Xm+1 ←Xm + r′
n ∈{1, 2, . . . , C} \ {ln}. Denote L′ = {l′
2, . . . , l′
In practice, we deﬁne a random permutation function π :
{1, 2, . . . , C} →{1, 2, . . . , C} for every image independently, in which π(c) ̸= c for c = 1, 2, . . . , C, and generate
L′ by setting l′
n = π(ln) for all n. Under this setting, the
loss function covering all targets can be written as:
L(X, T , L, L′) =
fln(X, tn) −fl′n(X, tn)
Minimizing L can be achieved via making every target to be
incorrectly predicted, i.e., suppressing the conﬁdence of the
original correct class fln(X + r, tn), while increasing that
of the desired (adversarial) incorrect class fl′n(X + r, tn).
We apply a gradient descent algorithm for optimization.
At the m-th iteration, denote the current image (possibly
after adding several perturbations) as Xm.
We ﬁnd the
set of correctly predicted targets, named the active target
set: Tm = {tn | arg maxc {fc(Xm, tn)} = ln}. Then we
compute the gradient with respect to the input data and then
accumulate all these perturbations:
∇Xmfl′n(Xm, tn) −∇Xmfln(Xm, tn)
Note that |Tm| ≪|T | when m gets large, thus this strategy
considerably reduces the computational overhead. To avoid
numerical instability, we normalize rm as
where γ = 0.5 is a ﬁxed hyper-parameter. We then add r′
to the current image Xm and proceed to the next iteration.
The algorithm terminates if either all the targets are predicted as desired, i.e., Tm = ∅, or it reaches the maximum
iteration number, which is set to be 200 in segmentation and
150 in detection.
The ﬁnal adversarial perturbation is computed as r =
m. Note that, in practice, we often obtain the input image X after subtracting the mean image bX. In this case, the
adversarial image is Trunc
X + r + bX
, where Trunc(·)
denotes the function that truncates every pixel value by
 . Although truncation may harm the adversarial perturbation, we observed little effect in experiments, mainly
because the magnitude of perturbation r is very small (see
Section 3.5.3). The overall pipeline of DAG algorithm is
illustrated in Algorithm 1.
3.2. Selecting Input Proposals for Detection
A critical issue in DAG is to select a proper set T of
targets. This is relatively easy in the semantic segmentation
task, because the goal is to produce incorrect classiﬁcation
on all pixels, and thus we can set each of them as a separate target, i.e., performing dense sampling on the image
lattice. This is tractable, i.e., the computational complexity
is proportional to the total number of pixels.
In the scenario of object detection, target selection becomes a lot more difﬁcult, as the total number of possible
targets (bounding box proposals) is orders of magnitudes
larger than that in semantic segmentation. A straightforward choice is to only consider the proposals generated
by a sideway network, e.g., the regional proposal network
(RPN) , but we ﬁnd that when the adversarial perturbation r is added to the original image X, a different set
of proposals may be generated according to the new input
X+r, and the network may still be able to correctly classify
these new proposals . To overcome this problem, we
make the proposals very dense by increasing the threshold
of NMS in RPN. In practice, when the intersection-overunion (IOU) goes up from 0.70 to 0.90, the average number
of proposals on each image increases from around 300 to
around 3000. Using this denser target set T , most probable
object bounding boxes are only pixels away from at least
one of the selected input proposals, and we can expect the
classiﬁcation error transfers among neighboring bounding
boxes. As shown in experiments, this heuristic idea works
very well, and the effect of adversarial perturbations is
positively correlated to the number of proposals considered
Technically, given the proposals generated by RPN, we
preserve all positive proposals and discard the remaining.
Here, a positive proposal satisﬁes the following two conditions: 1) the IOU with the closest ground-truth object
is greater than 0.1, and 2) the conﬁdence score for the
corresponding ground-truth class is greater than 0.1. If both
conditions hold on multiple ground-truth objects, we select
FR-ZF-0712
FR-VGG-0712
Table 1: Semantic segmentation (measured by mIOU, %)
and object detection (measured by mAP, %) results of
different networks.
Here, ORIG represents the accuracy
obtained on the original image set, ADVR is obtained on the
set after the adversarial perturbations are added, and PERM
is obtained after the randomly permuted perturbations are
added. Please see Section 3.3 for details.
the one with the maximal IOU. The label of the proposal is
deﬁned as the corresponding conﬁdent class. This strategy
aims at selecting high-quality targets for Algorithm 1.
3.3. Quantitative Evaluation
Following some previous work , we evaluate our
approach by measuring the drop in recognition accuracy,
i.e., mean intersection-over-union (mIOU) for semantic segmentation and mean average precision (mAP) for object
detection, using the original test images and the ones after
adding adversarial perturbations1.
• For semantic segmentation, we study two network architectures based on the FCN framework. One
of them is based on the AlexNet and the other
one is based on the 16-layer VGGNet .
networks have two variants. We use FCN-Alex and
FCN-VGG, which are publicly available, to denote
the networks that are trained on the original FCN 
training set which has 9610 images, and use FCN-
Alex* and FCN-VGG* to denote the networks that
are trained on the DeepLab training set which has
10582 images. We use the validation set in which
has 736 images as our semantic segmentation test set.
• For object detection, based on the Faster-RCNN 
framework, we study two network architectures, i.e.,
the ZFNet and the 16-layer VGGNet . Both
networks have two variants, which are either trained
on the PascalVOC-2007 trainval set, or the combined
PascalVOC-2007 and PascalVOC-2012 trainval sets.
These four models are publicly available, and are denoted as FR-ZF-07, FR-ZF-0712, FR-VGG-07 and
1For implementation simplicity, we keep targets with ground-truth class
label background unchanged when generating adversarial examples.
FR-VGG-0712, respectively. We use the PascalVOC-
2007 test set which has 4952 images as our object
detection test set.
Results are summarized in Table 1.
We can observe
that the accuracy (mIOU for segmentation and mAP for
detection) drops signiﬁcantly after the adversarial perturbations are added, demonstrating the effectiveness of DAG
algorithm. Moreover, for detection, the networks with more
training data are often more sensitive to the adversarial perturbation. This is veriﬁed by the fact that FR-ZF-07 (from
58.70% to 3.61%) has a smaller performance drop than FR-
ZF-0712 (from 61.07% to 1.95%), and that FR-VGG-07
(from 69.14% to 5.92%) has a smaller performance drop
than FR-VGG-0712 (from 72.04% to 3.36%).
To verify the importance of the spatial structure of adversarial perturbations, we evaluate the accuracy after randomly permuting the rows and/or columns of r. In Table 1,
we ﬁnd that permuted perturbations cause negligible accuracy drop, indicating that it is the spatial structure of r, instead of its magnitude, that indeed contributes in generating
adversarial examples. For permutation results, we randomly
permute r for three times and take the average.
3.4. Adversarial Examples
Figure 1 shows an adversarial example that fails in both
detection and segmentation networks. In addition, we show
that DAG is able to control the output of adversarial images
very well. In Figure 2, we apply DAG to generating one
adversarial image (which humans can recognize but deep
networks cannot) and one fooling image (which is
completely unrecognizable to humans but deep networks
produce false positives). This suggests that deep networks
only cover a limited area in the high-dimensional feature
space, and that we can easily ﬁnd adversarial and/or fooling
examples that fall in the unexplored parts.
3.5. Diagnostics
The Denseness of Proposals
We ﬁrst observe the impact on adversarial generation of the
denseness of the proposals. To this end, we use different
IOU rates in the NMS process after the RPN. This directly
affects the number of proposals preserved in Algorithm 1.
As we can see in Figure 3, the mAP value goes down (i.e.,
stronger adversarial perturbations are generated) as the IOU
rate increases, which means that fewer proposals are ﬁltered
out and thus the set of targets T becomes larger.
is in line of our expectation, since DAG only guarantees
misclassiﬁcation on the targets in T . The denser sampling
on proposals allows the recognition error to propagate to
other possible object positions better. Therefore, we choose
a large IOU value (0.90) which produces good results.
Figure 2: Fancy examples generated by DAG for semantic
segmentation.
The adversarial image is on the left and
the fooling image is on the right.
From top to bottom:
the original image, the perturbation (magniﬁed by 10),
the adversarial image after adding perturbation, and the
segmentation results. The red, blue and black regions are
predicted as airplane, bus and background, respectively.
Convergence
We then investigate the convergence of DAG, i.e., how
many iterations are needed to ﬁnd the desired adversarial
perturbation. Figure 4 shows the number of active targets,
i.e., |Tm|, with respect to the number of iterations m. In
general, the training process goes smoothly in the early
rounds, in which we ﬁnd that the number of active proposals
is signiﬁcantly reduced. After the algorithm reaches the
maximal number of iterations, i.e., 200 in segmentation
and 150 in detection, only few (less than 1%) image fail
to converge. Even on these cases, DAG is able to produce
reasonable adversarial perturbations.
Another interesting observation is the difﬁculty in generating adversarial examples.
In general, the detection
networks are more difﬁcult to attack than the segmentation
nms ratio of proposal candidates
FR-ZF-0712
Figure 3: The mAP of using adversarial
perturbations on FR-ZF-07 to attack FR-
ZF-07 and FR-ZF-0712, with respect to
the IOU rate. A larger IOU rate leads to
a denser set of proposals.
Number of iterations
Number of Active Pixels
Number of iterations
Number of Active Boxes
Figure 4: The convergence of DAG measured by the number of active targets,
i.e., |Tm|, with respect to the number of iterations. Over the entire dataset, the
average numbers of iterations are 31.78 and 54.02 for FCN-Alex and FCN-
VGG, and these numbers are 47.05 and 41.42 for FR-ZF-07 and FR-VGG-07,
respectively.
networks, which is arguably caused by the much larger
number of potential targets (recall that the total number of
possible bounding boxes is one or two orders of magnitudes
larger). Meanwhile, as the IOU rate increases, i.e., a larger
set T of proposals is considered, convergence also becomes
slower, implying that more iterations are required to generate stronger adversarial perturbations.
Perceptibility
Following , we compute the perceptibility of the adversarial perturbation r deﬁned by p =
where K is the number of pixels, and rk is the intensity
vector (3-dimensional in the RGB color space, k = 1, 2, 3)
normalized in . We average the perceptibility value
over the entire test set. In semantic segmentation, these values are 2.6×10−3, 2.5×10−3, 2.9×10−3 and 3.0×10−3 on
FCN-Alex, FCN-Alex*, FCN-VGG and FCN-VGG*, respectively. In object detection, these values are 2.4 × 10−3,
2.7 × 10−3, 1.5 × 10−3 and 1.7 × 10−3 on FR-ZF-07, FR-
ZF-0712, FR-VGG-07 and FR-VGG-0712, respectively.
One can see that these numbers are very small, which
guarantees the imperceptibility of the generated adversarial
perturbations. The visualized examples (Figures 1 and 2)
also verify this point.
4. Transferring Adversarial Perturbations
In this section, we investigate the transferability of the
generated adversarial perturbations. For this respect, we
add the adversarial perturbation computed on one model to
attack other models. The attacked model may be trained
based on a different (sometimes unknown) network architecture, or even targeted at a different vision task. Quantitative results are summarized in Tables 2 - 4, and typical
examples are illustrated in Figure 6. In the following parts,
we analyze these results by organizing them into three
categories, namely cross-training transfer, cross-network
transfer and cross-task transfer.
4.1. Cross-Training Transfer
By cross-training transfer, we mean to apply the perturbations learned from one network to another network
with the same architecture but trained on a different dataset.
We observe that the transferability largely exists within the
same network structure2. For example, using the adversarial
perturbations generated by FR-ZF-07 to attack FR-ZF-
0712 obtains a 22.15% mAP. This is a dramatic drop from
the performance (61.07%) reported on the original images,
although the drop is less than that observed in attacking FR-
ZF-07 itself (from 58.70% to 3.61%). Meanwhile, using
the adversarial perturbations generated by FR-ZF-0712 to
attack FR-ZF-07 causes the mAP drop from 58.70% to
13.14%, We observe similar phenomena when FR-VGG-
07 and FR-VGG-0712, or FCN-Alex and FCN-Alex*, or
FCN-VGG and FCN-VGG* are used to attack each other.
Detailed results are shown in Tables 2 and 3.
4.2. Cross-Network Transfer
We extend the previous case to consider the transferability through different network structures. We introduce
two models which are more powerful than what we used to
generate adversarial perturbations, namely DeepLab for
semantic segmentation and R-FCN for object detection.
For DeepLab , we use DL-VGG to denote the network based on 16-layer VGGNet , and use DL-RN101
to denote the network based on 101-layer ResNet .
Both networks are trained on original DeepLab training
2We also studied training on strictly non-overlapping datasets, e.g., the
model FR-ZF-07 trained on PascalVOC-2007 trainval set and the model
FR-ZF-12val trained on PascalVOC-2012 val set. The experiments deliver similar conclusions. For example, using FR-ZF-07 to attack FR-ZF-
12val results in a mAP drop from 56.03% to 25.40%, and using FR-ZF-
12val to attack FR-ZF-07 results in a mAP drop from 58.70% to 30.41%.
Adversarial
Perturbations from
FR-ZF-0712
FR-ZF-07 (r1)
FR-ZF-0712 (r2)
FR-VGG-07 (r3)
FR-VGG-0712 (r4)
r1 + r3 (permute)
r2 + r4 (permute)
Table 2: Transfer results for detection networks. FR-ZF-07, FR-ZF-0712, FR-VGG-07 and FR-VGG-0712 are used as four
basic models to generate adversarial perturbations, and R-FCN-RN50 and R-FCN-RN101 are used as black-box models.
All models are evaluated on the PascalVOC-2007 test set and its adversarial version, which both has 4952 images.
Adversarial
Perturbations from
FCN-Alex (r5)
FCN-Alex* (r6)
FCN-VGG (r7)
FCN-VGG* (r8)
r5 + r7 (permute)
r6 + r8 (permute)
Table 3: Transfer results for segmentation networks. FCN-Alex, FCN-Alex*, FCN-VGG and FCN-VGG* are used as four
basic models to generate adversarial perturbations, and DL-VGG and DL-RN101 are used as black-box models. All models
are evaluated on validation set in and its adversarial version, which both has 736 images.
Adversarial
Perturbations from
R-FCN-RN101
FR-ZF-07 (r1)
FR-VGG-07 (r3)
FCN-Alex (r5)
FCN-VGG (r7)
r1 + r3 + r5
r1 + r3 + r7
r1 + r5 + r7
r3 + r5 + r7
r1 + r3 + r5 + r7
Table 4: Transfer results between detection networks and segmentation networks. FR-ZF-07, FR-VGG-07, FCN-Alex and
FCN-VGG are used as four basic models to generate adversarial perturbations, and R-FCN-RN101 are used as black-box
model. When attacking the ﬁrst four basic networks, we use a subset of the PascalVOC-2012 segmentation validation set
which contains 687 images. In the black-box attack, we evaluate our method on the non-intersecting subset of 110 images.
set which has 10582 images.
For R-FCN , we use
R-FCN-RN50 to denote the network based on 50-layer
ResNet , and use R-FCN-RN101 to denote the network
based on 101-layer ResNet . Both networks are trained
on the combined trainval sets of PascalVOC-2007 and
PascalVOC-2012. The perturbations applied to these four
models are considered as black-box attacks , since DAG
does not know the structure of these networks beforehand.
Detailed results are shown in Tables 2 and 3. Experiments reveal that transferability between different network
structures becomes weaker. For example, applying the perturbations generated by FR-ZF-07 leads to slight accuracy
drop on FR-VGG-07 (from 69.14% to 66.01%), FR-VGG-
0712 (from 72.07% to 69.74%), R-FCN-RN50 (from
76.40% to 74.01%) and R-FCN-RN101 (from 78.06% to
75.87%), respectively. Similar phenomena are observed in
using different segmentation models to attack each other.
One exception is using FCN-VGG or FCN-VGG* to attack
DL-VGG (from 70.72% to 45.16% for FCN-VGG attack,
or from 70.72% to 46.33% by FCN-VGG* attack), which
results in a signiﬁcant accuracy drop of DL-VGG. Considering the cues obtained from previous experiments, we
conclude that adversarial perturbations are closely related
to the architecture of the network.
4.3. Cross-Task Transfer
Finally, we investigate cross-task transfer, i.e., using the
perturbations generated by a detection network to attack a
segmentation network or in the opposite direction. We use a
subset of PascalVOC-2012 segmentation validation set as
our test set 3. Results are summarized in Table 4. We note
that if the same network structure is used, e.g., using FCN-
VGG (segmentation) and FR-VGG-07 (detection) to attack
each other, the accuracy drop is signiﬁcant (the mIOU of
FCN-VGG drops from 54.87% to 43.06%, and the mAP
of FR-VGG-07 drops from 68.88% to 56.33%).
that this drop is even more signiﬁcant than cross-network
transfer on the same task, which veriﬁes our hypothesis
again that the adversarial perturbations are related to the
network architecture.
4.4. Combining Heterogeneous Perturbations
From the above experiments, we assume that different network structures generate roughly orthogonal
perturbations, which means that if rA is generated by
one structure A, then adding it to another structure B
merely changes the recognition results, i.e., f B(X, tn) ≈
f B(X + rA, tn).
This motivates us to combine heterogeneous perturbations towards better adversarial performance.
For example, if both rA and rB are added,
we have f A(X + rA + rB, tn)
f A(X + rA, tn) and
f B(X + rA + rB, tn) ≈f B(X + rB, tn). Thus, the combined perturbation rA + rB is able to confuse both network
structures.
In Tables 2– 4, we list some results by adding multiple
adversarial perturbations. Also, in order to verify that the
spatial structure of combined adversarial perturbations is
the key point that leads to statistically signiﬁcant accuracy drop, we randomly generate three permutations of the
combined adversarial perturbations and report the average
3There are training images of FR-ZF-07, FR-VGG-07, FCN-Alex and
FCN-VGG included in the PascalVOC-2012 segmentation validation set,
so we validate on the non-intersecting set of 687 images.
accuracy. From the results listed in Tables 2– 4, we can
observe that adding multiple adversarial perturbations often
works better than adding a single source of perturbations.
Indeed, the accuracy drop caused by the combined perturbation approximately equals to the sum of drops by each perturbation. For example, the adversarial perturbation r2 +r4
(combining FR-ZF-0712 and FR-VGG-0712) causes signiﬁcant mAP drop on all ZFNet-based and VGGNet-based
detection networks, and the adversarial perturbation r5 +r7
(combining FCN-Alex* and FCN-VGG*) causes signiﬁcant mIOU drop on all AlexNet-based and VGGNet-based
segmentation networks. However, permutation destroys the
spatial structure of the adversarial perturbations, leading to
negligible accuracy drops. The same conclusion holds when
the perturbations from different tasks are combined. Table 4
shows some quantitative results of such combination and
Figure 5 shows an example. Note that, the perceptibility
value deﬁned in Section 3.5.3 remains very small even
when multiple adversarial perturbations are combine (e.g.,
4.0 × 10−3 by r1 + r3 + r5 + r7).
boat : 0.809
boat : 0.857
pottedplant : 0.373
Figure 5: Adding one adversarial perturbation computed by
r1 + r3 + r5 + r7 (see Table 4) confuses four different
networks. The top row shows FR-VGG-07 and FR-ZF-07
detection results, and the bottom row shows FCN-Alex and
FCN-VGG segmentation results. The blue in segmentation
results corresponds to boat.
4.5. Black-Box Attack
Combining heterogeneous perturbations allows us to
perform better on the so-called black-box attack , in
which we do not need to know the detailed properties
(architecture, purpose, etc.) about the defender network.
According to the above experiments, a simple and effective
way is to compute the sum of perturbations from several
of known networks, such as FR-ZF-07, FR-VGG-07 and
FCN-Alex, and use it to attack an unknown network. This
strategy even works well when the structure of the defender
is not investigated before. As an example shown in Table 4,
dog : 0.990
sofa : 0.512
bird : 0.955
bird : 0.767
dog : 0.548
motorbike : 0.999
person : 0.407
person : 0.843
dog : 0.795
Original Image
Original Result
from Network 2
Adversarial Result
from Network 2
Adversarial Result
from Network 1
Network 1:
FR-ZF-0712
Network 2:
Network 1:
Network 2:
Network 1:
Network 2:
Network 1:
Network 2:
B-ground Aero plane
Dining-Table
Potted-Plant
TV/Monitor
Segmentation
Figure 6: Transferrable examples for semantic segmentation and object detection. These four rows, from top to bottom, shows
the adversarial attack examples within two detection networks, within two segmentation networks, using a segmentation
network to attack a detection network and in the opposite direction, respectively. The segmentation legend borrows from .
the perturbation r1 + r3 + r5 + r7 leads to signiﬁcant accuracy drop (from 80.20% to 64.52%) on R-FCN-RN101 ,
a powerful network based on the deep ResNet .
5. Conclusions
In this paper, we investigate the problem of generating
adversarial examples, and extend it from image classiﬁcation to semantic segmentation and object detection.
propose DAG algorithm for this purpose. The basic idea
is to deﬁne a dense set of targets as well as a different
set of desired labels, and optimize a loss function in order
to produce incorrect recognition results on all the targets
simultaneously. Extensive experimental results verify that
DAG is able to generate visually imperceptible perturbation, so that we can confuse the originally high-conﬁdence
recognition results in a well controllable manner.
An intriguing property of the perturbation generated by
DAG lies in the transferability. The perturbation can be
transferred across different training sets, different network
architectures and even different tasks. Combining heterogeneous perturbations often leads to more effective adversarial
perturbations in black-box attacks. The transferability also
suggests that deep networks, though started with different
initialization and trained in different ways, share some basic
principles such as local linearity, which make them sensitive
to a similar source of perturbations. This reveals an interesting topic for future research.
Acknowledgements
We thank Dr. Vittal Premachandran, Chenxu Luo, Weichao Qiu, Chenxi Liu, Zhuotun Zhu and Siyuan Qiao for
instructive discussions.