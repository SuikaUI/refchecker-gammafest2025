EEGNet: A Compact Convolutional Neural Network
for EEG-based Brain-Computer Interfaces
Vernon J. Lawhern1,*, Amelia J. Solon1,2, Nicholas R. Waytowich1,3, Stephen M. Gordon1,2,
Chou P. Hung1,4, and Brent J. Lance1
1Human Research and Engineering Directorate, U.S. Army Research Laboratory, Aberdeen
Proving Ground, MD
2DCS Corporation, Alexandria, VA
3Department of Biomedical Engineering, Columbia University, New York, NY
4Department of Neuroscience, Georgetown University, Washington, DC
*Corresponding Author
May 17, 2018
Objective: Brain computer interfaces (BCI) enable direct communication with a computer,
using neural activity as the control signal. This neural signal is generally chosen from a variety of well-studied electroencephalogram (EEG) signals. For a given BCI paradigm, feature
extractors and classiﬁers are tailored to the distinct characteristics of its expected EEG control
signal, limiting its application to that speciﬁc signal. Convolutional Neural Networks (CNNs),
which have been used in computer vision and speech recognition to perform automatic feature
extraction and classiﬁcation, have successfully been applied to EEG-based BCIs; however, they
have mainly been applied to single BCI paradigms and thus it remains unclear how these architectures generalize to other paradigms. Here, we ask if we can design a single CNN architecture
to accurately classify EEG signals from diﬀerent BCI paradigms, while simultaneously being as
compact as possible (deﬁned as the number of parameters in the model). Approach: In this work
we introduce EEGNet, a compact convolutional neural network for EEG-based BCIs. We introduce the use of depthwise and separable convolutions to construct an EEG-speciﬁc model which
encapsulates well-known EEG feature extraction concepts for BCI. We compare EEGNet, both
for within-subject and cross-subject classiﬁcation, to current state-of-the-art approaches across
four BCI paradigms: P300 visual-evoked potentials, error-related negativity responses (ERN),
movement-related cortical potentials (MRCP), and sensory motor rhythms (SMR). Results: We
show that EEGNet generalizes across paradigms better than, and achieves comparably high
performance to, the reference algorithms when only limited training data is available. We also
show that EEGNet eﬀectively generalizes to both ERP and oscillatory-based BCIs. In addition,
we demonstrate three diﬀerent approaches to visualize the contents of a trained EEGNet model
to enable interpretation of the learned features. Signiﬁcance: Our results suggest that EEGNet
is robust enough to learn a wide variety of interpretable features over a range of BCI tasks,
suggesting that the observed performances were not due to artifact or noise sources in the data.
Our models can be found at: 
 
Keywords: Brain-Computer Interface, EEG, Deep Learning, Convolutional Neural Network,
P300, Error-Related Negativity, Sensory Motor Rhythm
Introduction
A Brain-Computer Interface (BCI) enables direct communication with a machine via brain signals .
Traditionally, BCIs have been used for medical applications such as neural control of
prosthetic artiﬁcial limbs . However, recent research has opened up the possibility for novel BCIs
focused on enhancing performance of healthy users, often with noninvasive approaches based on
electroencephalography (EEG) . Generally speaking, a BCI consists of ﬁve main processing
stages : a data collection stage, where neural data is recorded; a signal processing stage, where
the recorded data is preprocessed and cleaned; a feature extraction stage, where meaningful information is extracted from the neural data; a classiﬁcation stage, where a decision is interpreted from
the data; and a feedback stage, where the result of that decision is provided to the user. While these
stages are largely the same across BCI paradigms, each paradigm relies on manual speciﬁcation
of signal processing , feature extraction and classiﬁcation methods , a process which often
requires signiﬁcant subject-matter expertise and/or a priori knowledge about the expected EEG
signal. It is also possible that, because the EEG signal preprocessing steps are often very speciﬁc
to the EEG feature of interest (for example, band-pass ﬁltering to a speciﬁc frequency range), that
other potentially relevant EEG features could be excluded from analysis (for example, features
outside of the band-pass frequency range). The need for robust feature extraction techniques will
only continue to increase as BCI technologies evolve into new application domains .
Deep Learning has largely alleviated the need for manual feature extraction, achieving state-ofthe-art performance in ﬁelds such as computer vision and speech recognition . Speciﬁcally,
the use of deep convolutional neural networks (CNNs) has grown due in part to their success in
many challenging image classiﬁcation problems , surpassing methods relying on hand-crafted
features (see and for recent reviews). Although the majority of BCI systems still rely on
the use of handcrafted features, many recent works have explored the application of Deep Learning
to EEG signals. For example, CNNs have been used for epilepsy prediction and monitoring ,
for auditory music retrieval , for detection of visual-evoked responses and for motor
imagery classiﬁcation , while Deep Belief Networks (DBNs) have been used for sleep stage
detection , anomaly detection and in motion-onset visual-evoked potential classiﬁcation .
CNNs using time-frequency transforms of EEG data were used for mental workload classiﬁcation
 and for motor imagery classiﬁcation ). Restricted Boltzman Machines (RBMs) have been
used for motor imagery . An adaptive method based on stacked denoising autoencoders has been
proposed for mental workload classiﬁcation ). These studies focused primarily on classiﬁcation
in a single BCI task, often times using task-speciﬁc knowledge in designing the network architecture.
In addition, the amount of data used to train these networks varied signiﬁcantly across studies, in
part due to the diﬃculty in collecting data under diﬀerent experimental designs. Thus, it remains
unclear how these previous deep learning approaches would generalize both to other BCI tasks as
well as to variable training data sizes.
In this work we introduce EEGNet, a compact CNN for classiﬁcation and interpretation of
EEG-based BCIs. We introduce the use of Depthwise and Separable convolutions, previously used
in computer vision , to construct an EEG-speciﬁc network that encapsulates several well-known
EEG feature extraction concepts, such as optimal spatial ﬁltering and ﬁlter-bank construction,
while simultaneously reducing the number of trainable parameters to ﬁt when compared to existing approaches. We evaluate the generalizability of EEGNet on EEG datasets collected from four
diﬀerent BCI paradigms: P300 visual-evoked potential (P300), error-related negativity (ERN),
movement-related cortical potential (MRCP) and the sensory motor rhythm (SMR), representing
a spectrum of paradigms based on classiﬁcation of Event-Related Potentials (P300, ERN, MRCP)
as well as classiﬁcation of oscillatory components (SMR). In addition, each of these data collections contained varying amounts of data, allowing us to explore the eﬃcacy of EEGNet on various
training data sizes. Our results are as follows: We show that EEGNet achieves improved classiﬁcation performance over an existing paradigm-agnostic EEG CNN model across nearly all tested
paradigms when limited training data is available. In addition, we show that EEGNet eﬀectively
generalizes across all tested paradigms. We also show that EEGNet performs just as well as a
more paradigm-speciﬁc EEG CNN model, but with two orders of magnitude fewer parameters to
ﬁt, representing a more eﬃcient use of model parameters (an aspect that has been explored in
previous deep learning literature, see ). Finally, through the use of feature visualization and
model ablation analysis, we show that neurophysiologically interpretable features can be extracted
from the EEGNet model. This is important as CNNs, despite their ability for robust and automatic feature extraction, often produce hard to interpret features. For neuroscience practitioners,
the ability to derive insights into CNN-derived neurophysiological phenomena may be just as important as achieving good classiﬁcation performance, depending on the intended application. We
validate our architecture’s ability to extract neurophysiologically interpretable signals on several
well-studied BCI paradigms to show that the network performance is not being driven by noise or
artifact signals in the data.
The remainder of this manuscript is structured as follows. Section 2.1 gives a brief description
of the four datasets used to validate our CNN model. Section 2.2 describes our EEGNet model as
well as other BCI models (both CNN and non-CNN based models) used in our model comparison.
Section 3 presents the results of both within-subject and cross-subject classiﬁcation performance,
as well as results of our feature explainability analysis. We discuss our ﬁndings in more detail in
the Discussion.
Materials and Methods
Data Description
BCIs are generally categorized into two types, depending on the EEG feature of interest :
event-related and oscillatory. Event-Related Potential (ERP) BCIs are designed to detect a high
amplitude and low frequency EEG response to a known, time-locked external stimulus. They are
generally robust across subjects and contain well-stereotyped waveforms, enabling the time course
of the ERP to be modeled through machine learning eﬃciently . In contrast to ERP-based BCIs,
which rely mainly on the detection of the ERP waveform from some external event or stimulus,
Feature Type
Bandpass Filter
# of Subjects
Trials per Subject
# of Classes
Class Imbalance?
Yes, ∼5.6:1
Yes, ∼3.4:1
ERP/Oscillatory
Oscillatory
Table 1: Summary of the data collections used in this study. Class imbalance, if present, is given as
odds; i.e.: an odds of 2:1 means the class imbalance is 2/3 of the data for class 1 to 1/3 of the data
for class 2. For the P300 and ERN datasets, the class imbalance is subject-dependent; therefore,
the odds is given as the average class imbalance over all subjects.
Oscillatory BCIs use the signal power of speciﬁc EEG frequency bands for external control and are
generally asynchronous . When oscillatory signals are time-locked to an external stimulus, they
can be represented through event-related spectral perturbation (ERSP) analyses . Oscillatory
BCIs are more diﬃcult to train, generally due to the lower signal-to-noise ratio (SNR) as well as
greater variation across subjects . A summary of the data used in this manuscript can be found
in Table 1
Dataset 1: P300 Event-Related Potential (P300)
The P300 event-related potential is a stereotyped neural response to novel visual stimuli . It is
commonly elicited with the visual oddball paradigm, where participants are shown repetitive “nontarget” visual stimuli that are interspersed with infrequent “target” stimuli at a ﬁxed presentation
rate (for example, 1 Hz). Observed over the parietal cortex, the P300 waveform is a large positive
deﬂection of electrical activity observed approximately 300 ms post stimulus onset, the strength
of the observed deﬂection being inversely proportional to the frequency of the target stimuli. The
P300 ERP is one of the strongest neural signatures observable by EEG, especially when targets
are presented infrequently . When the image presentation rate increases to 2 Hz or more, it is
commonly referred to as rapid serial visual presentation (RSVP), which has been used to develop
BCIs for large image database triage .
The EEG data used here have been previously described in ; a brief description is given
below. 18 participants volunteered for an RSVP BCI study. Participants were shown images of
natural scenery at 2 Hz rate, with images either containing a vehicle or person (target), or with no
vehicle or person present (non-target). Participants were instructed to press a button with their
dominant hand when a target image was shown. The target/non-target ratio was 20%/80%. Data
from 3 participants were excluded from the analysis due to excessive artifacts and/or noise within
the EEG data. Data from the remaining 15 participants (9 male and 14 right-handed) who ranged in
age from 18 to 57 years (mean age 39.5 years) were further analyzed. EEG recordings were digitally
sampled at 512 Hz from 64 scalp electrodes arranged in a 10-10 montage using a BioSemi Active
Two system (Amsterdam, The Netherlands). Continuous EEG data were referenced oﬄine to the
average of the left and right earlobes, digitally bandpass ﬁltered, using an FIR ﬁlter implemented
in EEGLAB , to 1-40 Hz and downsampled to 128 Hz. EEG trials of target and non-target
conditions were extracted at s post stimulus onset, and used for a two-class classiﬁcation.
Dataset 2: Feedback Error-Related Negativity (ERN)
Error-Related Negativity potentials are perturbations of the EEG following an erroneous or unusual
event in the subject’s environment or task. They can be observed in a variety of tasks, including time
interval production paradigms and in forced-choice paradigms . Here we focus on the
feedback error-related negativity (ERN), which is an amplitude perturbation of the EEG following
the perception of an erroneous feedback produced by a BCI. The feedback ERN is characterized as
a negative error component approximately 350ms, followed by a positive component approximately
500ms, after visual feedback (see Figure 7 of for an illustration). The detection of the feedback
ERN provides a mechanism to infer, and to possibly correct in real-time, the incorrect output of a
BCI. This two-stage system has been proposed as a hybrid BCI in and has been shown to
improve the performance of a P300 speller in online applications .
The EEG data used here comes from and was used in the “BCI Challenge” hosted by Kaggle
( a brief description is given below.
26 healthy
participants (16 for training, 10 for testing) participated in a P300 speller task, a system which uses a
random sequence of ﬂashing letters, arranged in a 6×6 grid, to elicit the P300 response . The goal
of the challenge was to determine whether the feedback of the P300 speller was correct or incorrect.
The EEG data were originally recorded at 600Hz using 56 passive Ag/AgCl EEG sensors (VSM-
CTF compatible system) following the extended 10-20 system for electrode placement. Prior to our
analysis, the EEG data were band-pass ﬁltered, using an FIR ﬁlter implemented in EEGLAB ,
to 1-40 Hz and down-sampled to 128Hz. EEG trials of correct and incorrect feedback were extracted
at [0, 1.25]s post feedback presentation and used as features for a two-class classiﬁcation.
Dataset 3: Movement-Related Cortical Potential (MRCP)
Some neural activities contain both ERP as well as an oscillatory components. One particular
example of this is the movement-related cortical potential (MRCP), which can be elicited by voluntary movements of the hands and feet and is observable through EEG along the central and
midline electrodes, contralateral to the hand or foot movement . The MRCP components
can be seen before movement onset (a slow 0-5Hz readiness potential and an early desynchronization in the 10-12Hz frequency band), at movement onset (a slow motor potential ),
and after movement onset (a late synchronization of 20-30Hz activity approximately 1 second after
movement execution). The MRCP has been used previously to develop motor control BCIs for
both healthy and physically disabled patients 
The EEG data used here have been previously described in ; a brief description is given
below. In this study, 13 subjects performed self-paced ﬁnger movements using the left index, left
middle, right index, or right middle ﬁngers. The data was recorded using a 256 channel BioSemi
Active II system at 1024 Hz. Due to extensive signal noise present in the data, the EEG data
were ﬁrst processed with the PREP pipeline . The data were referenced to linked mastoids,
bandpass ﬁltered, using an FIR ﬁlter implemented in EEGLAB , between 0.1 Hz and 40 Hz,
and then downsampled to 128 Hz. We further downsampled the channel space to the standard 64
channel BioSemi montage. The index and middle ﬁnger blocks for each hand were combined for
binary classiﬁcation of movements originating from the left or right hand. EEG trials of left and
right hand ﬁnger movements were extracted at [−.5, 1]s around ﬁnger movement onset and used
for a two-class classiﬁcation.
Dataset 4: Sensory Motor Rhythm (SMR)
A common control signal for oscillatory-based BCI is the sensorimotor rhythm (SMR), wherein
mu (8-12Hz) and beta (18-26Hz) bands desynchronize over the sensorimotor cortex contralateral
to an actual or imagined movement. The SMR is very similar to the oscillatory component of the
MRCP. Although SMR-based BCIs can facilitate nuanced, endogenous BCI control, they tend to
be weak and highly variable across and within subjects, conventionally demanding user-training
(neurofeedback) and long calibration times (20 minutes) in order to achieve reasonable performance
The EEG data used here comes from BCI Competition IV Dataset 2A (called the SMR
dataset for the remainder of the manuscript). The data consists of four classes of imagined movements of left and right hands, feet and tongue recorded from 9 subjects. The EEG data were
originally recorded using 22 Ag/AgCl electrodes, sampled at 250 Hz and bandpass ﬁltered between
0.5 and 100Hz. We resampled the timeseries to 128 Hz, and follow the same EEG pre-processing
procedure as described in , using software that was provided by the authors. For both the
training and test sets we epoched the data at [0.5, 2.5] seconds post cue onset (the same window
which was used in ). Note that we make predictions for only this time range on the test set.
We perform a four-class classiﬁcation using accuracy as the summary measure.
Classiﬁcation Methods
EEGNet: Compact CNN Architecture
Here we introduce EEGNet, a compact CNN architecture for EEG-based BCIs that (1) can be
applied across several diﬀerent BCI paradigms, (2) can be trained with very limited data and (3)
can produce neurophysiologically interpretable features. A visualization and full description of the
EEGNet model can be found in Figure 1 and Table 2, respectively, for EEG trials, collected at
128Hz sampling rate, having C channels and T time samples. We ﬁt the model using the Adam
optimizer, using default parameters as described in , minimizing the categorical cross-entropy
loss function. We run 500 training iterations (epochs) and perform validation stopping, saving the
model weights which produced the lowest validation set loss. All models were trained on an NVIDIA
Quadro M6000 GPU, with CUDA 9 and cuDNN v7, in Tensorﬂow , using the Keras API .
We omit the use of bias units in all convolutional layers. Note that, while all convolutions are onedimensional, we use two-dimensional convolution functions for ease of software implementation.
Our software implementation can be found at 
• In Block 1, we perform two convolutional steps in sequence. First, we ﬁt F1 2D convolutional
ﬁlters of size (1, 64), with the ﬁlter length chosen to be half the sampling rate of the data
Figure 1: Overall visualization of the EEGNet architecture. Lines denote the convolutional kernel
connectivity between inputs and outputs (called feature maps) . The network starts with a temporal
convolution (second column) to learn frequency ﬁlters, then uses a depthwise convolution (middle
column), connected to each feature map individually, to learn frequency-speciﬁc spatial ﬁlters. The
separable convolution (fourth column) is a combination of a depthwise convolution, which learns a
temporal summary for each feature map individually, followed by a pointwise convolution, which
learns how to optimally mix the feature maps together. Full details about the network architecture
can be found in Table 2.
(here, 128Hz), outputting F1 feature maps containing the EEG signal at diﬀerent band-pass
frequencies. Setting the length of the temporal kernel at half the sampling rate allows for
capturing frequency information at 2Hz and above. We then use a Depthwise Convolution 
of size (C, 1) to learn a spatial ﬁlter. In CNN applications for computer vision the main
beneﬁt of a depthwise convolution is reducing the number of trainable parameters to ﬁt, as
these convolutions are not fully-connected to all previous feature maps (see Figure 1 for an
illustration). Importantly, when used in EEG-speciﬁc applications, this operation provides a
direct way to learn spatial ﬁlters for each temporal ﬁlter, thus enabling the eﬃcient extraction
of frequency-speciﬁc spatial ﬁlters (see the middle column of Figure 1). A depth parameter
D controls the number of spatial ﬁlters to learn for each feature map (D = 1 is shown in
Figure 1 for illustration purposes). This two-step convolutional sequence is inspired in part
by the Filter-Bank Common Spatial Pattern (FBCSP) algorithm and is similar in nature
to another decomposition technique, Bilinear Discriminant Component Analysis .
keep both convolutions linear as we found no signiﬁcant gains in performance when using
nonlinear activations. We apply Batch Normalization along the feature map dimension
before applying the exponential linear unit (ELU) nonlinearity . To help regularize or
model, we use the Dropout technique . We set the dropout probability to 0.5 for withinsubject classiﬁcation to help prevent over-ﬁtting when training on small sample sizes, whereas
we set the dropout probability to 0.25 in cross-subject classiﬁcation, as the training set sizes
Activation
(F1, C, T)
mode = same
(F1, C, T)
DepthwiseConv2D
(D * F1, 1, T)
mode = valid, depth = D, max norm = 1
(D * F1, 1, T)
Activation
(D * F1, 1, T)
AveragePool2D
(D * F1, 1, T // 4)
(D * F1, 1, T // 4)
p = 0.25 or p = 0.5
SeparableConv2D
16 ∗D ∗F1 + F2 ∗(D ∗F1)
(F2, 1, T // 4)
mode = same
(F2, 1, T // 4)
Activation
(F2, 1, T // 4)
AveragePool2D
(F2, 1, T // 32)
(F2, 1, T // 32)
p = 0.25 or p = 0.5
(F2 * (T // 32))
N * (F2 * T // 32)
max norm = 0.25
Table 2: EEGNet architecture, where C = number of channels, T = number of time points, F1 =
number of temporal ﬁlters, D = depth multiplier (number of spatial ﬁlters), F2 = number of
pointwise ﬁlters, and N = number of classes, respectively. For the Dropout layer, we use p = 0.5
for within-subject classiﬁcation and p = 0.25 for cross-subject classiﬁcation (see Section 2.1.1 for
more details)
are much larger (see Section 2.3 for more details on our within- and cross-subject analyses).
We apply an average pooling layer of size (1, 4) to reduce the sampling rate of the signal to
32Hz. We also regularize each spatial ﬁlter by using a maximum norm constraint of 1 on its
weights; ∥w∥2 < 1.
• In Block 2, we use a Separable Convolution, which is a Depthwise Convolution (here, of
size (1, 16), representing 500ms of EEG activity at 32Hz) followed by F2 (1, 1) Pointwise
Convolutions . The main beneﬁts of separable convolutions are (1) reducing the number of
parameters to ﬁt and (2) explicitly decoupling the relationship within and across feature maps
by ﬁrst learning a kernel summarizing each feature map individually, then optimally merging
the outputs afterwards. When used for EEG-speciﬁc applications this operation separates
learning how to summarize individual feature maps in time (the depthwise convolution) with
how to optimally combine the feature maps (the pointwise convolution). This operation is also
particularly useful for EEG signals as diﬀerent feature maps may represent data at diﬀerent
time-scales of information. In our case we ﬁrst learn a 500 ms “summary” of each feature
map, then combine the outputs afterwards. An Average Pooling layer of size (1, 8) is used
for dimension reduction.
• In the classiﬁcation block, the features are passed directly to a softmax classiﬁcation with N
units, N being the number of classes in the data. We omit the use of a dense layer for feature
aggregation prior to the softmax classiﬁcation layer to reduce the number of free parameters
in the model, inspired by the work in .
We investigate several diﬀerent conﬁgurations of the EEGNet architecture by varying the number of ﬁlters, F1, and the number of spatial ﬁlters per temporal ﬁlter, D to learn. We set F2 = D∗F1
Trial Length (sec)
DeepConvNet
ShallowConvNet
EEGNet-4,2
EEGNet-8,2
Table 3: Number of trainable parameters per model and per dataset for all CNN-based models. We
see that the EEGNet models are up to two orders of magnitude smaller than both DeepConvNet
and ShallowConvNet across all datasets. Note that we use a temporal kernel length of 32 samples
for the SMR dataset as the data were high-passed at 4Hz.
(the number of temporal ﬁlters along with their associated spatial ﬁlters from Block 1) for the duration of the manuscript, although in principle F2 can take any value; F2 < D ∗F1 denotes a
compressed representation, learning fewer feature maps than inputs, whereas F2 > D ∗F1 denotes
an overcomplete representation, learning more feature maps than inputs.
We use the notation
EEGNet-F1,D to denote the number of temporal and spatial ﬁlters to learn; i.e.: EEGNet-4,2
denotes learning 4 temporal ﬁlters and 2 spatial ﬁlters per temporal ﬁlter.
Comparison with existing CNN Approaches
We compare the performance of EEGNet against the DeepConvNet and ShallowConvNet models
proposed by ; full table descriptions of both models can be found in the Appendix. We implemented these models in Tensorﬂow and Keras, following the descriptions found in the paper. As
their architectures were originally designed for 250Hz EEG signals (as opposed to 128Hz signals
used here) we divided the lengths of temporal kernels and pooling layers in their architectures by
2 to correspond approximately to the sampling rate used in our models. We train these models in
the same way we train the EEGNet model (see Section 2.2.1).
The DeepConvNet architecture consists of ﬁve convolutional layers with a softmax layer for
classiﬁcation (see Figure 1 of ). The ShallowConvNet architecture consists of two convolutional
layers (temporal, then spatial), a squaring nonlinearity (f(x) = x2), an average pooling layer and
a log nonlinearity (f(x) = log(x)). We would like to emphasize that the ShallowConvNet architecture was designed speciﬁcally for oscillatory signal classiﬁcation (by extracting features related
to log band-power); thus, it may not work well on ERP-based classiﬁcation tasks. However, the
DeepConvNet architecture was designed to be a general-purpose architecture that is not restricted
to speciﬁc feature types , and thus it serves as a more valid comparison to EEGNet. Table 3
shows the number of trainable parameters per model across all CNN models.
Comparison with Traditional Approaches
We also compare the performance of EEGNet to that of the best performing traditional approach
for each individual paradigm. For all ERP-based data analyses (P300, ERN, MRCP) the traditional approach is the approach which won the Kaggle BCI Competition , which uses a combination
of xDAWN Spatial Filtering , Riemannian Geometry , channel subset selection and L1
feature regularization (referred to as xDAWN + RG for the remainder of the manuscript). Here
we provide a summary of the approach, which is done in ﬁve steps:
1. Train two set of 5 xDAWN spatial ﬁlters, one set for each class of a binary classiﬁcation task,
using the ERP template concatenation method as described in .
2. Perform EEG electrode selection through backward elimination to keep only the most
relevant 35 channels.
3. Project the covariance matrices onto the tangent space using the log-euclidean metric .
4. Perform feature normalization using an L1 ratio of 0.5, signifying an equal weight for L1 and
L2 penalties. An L1 penalty encourages the sum of the absolute values of the parameters to
be small, whereas an L2 penalty encourages the sum of the squares of the parameters to be
small (a theoretical overview of these penalties can be found in ).
5. Perform classiﬁcation using an Elastic Net regression.
We use the same xDAWN+RG model parameters across all comparisons (P300, ERN, MRCP)
with the exception of the initial number of EEG channels to use, which was set to 56 for ERN
and 64 for P300 and MRCP. While the original solution used an ensemble of bagged classiﬁers,
for this analysis we only compared a single model with this approach to a single EEGNet model
on identical training and test sets, as we expect any gains from ensemble learning to beneﬁt both
approaches equally. The original solution also used a set of “meta features” that were speciﬁc to
that data collection. As the goal of this work is to investigate a general-purpose CNN model for
EEG-based BCIs, we omitted the use of these features as they are speciﬁc to that particular data
collection.
For oscillatory-based classiﬁcation of SMR, the traditional approach is our own implementation
of the One-Versus-Rest (OVR) ﬁlter-bank common spatial pattern (FBCSP) algorithm as described
in . Here we provide a brief summary of our approach:
1. Bandpass ﬁlter the EEG signal into 9 non-overlapping ﬁlter banks in 4Hz steps, starting at
4Hz: 4-8Hz, 8-12Hz, ..., 36-40Hz.
2. As the classiﬁcation problem is multi-class, we use OVR classiﬁcation, which requires that
we train a classiﬁer for all pairs of OVR combinations, which there are 4 here (class 1 vs all
others, class 2 vs all others, etc). We train 2 CSP ﬁlter pairs (4 ﬁlters total) for each ﬁlter
bank on the training data using the auto-covariance shrinkage method by . This will give
a total of 36 features (9 ﬁlter banks × 4 CSP ﬁlters) for each trial and each OVR combination.
3. Train an elastic-net logistic regression classiﬁer for each OVR combination. We set the
elastic net penalty α = 0.95.
4. Find the optimal λ value for the elastic-net logistic regression that maximizes the validation
set accuracy by evaluating the trained classiﬁers on a held-out validation set. The multi-class
label for each trial is the classiﬁer that produces the highest probability among the 4 OVR
classiﬁers.
5. Apply the trained classiﬁers to the test set, using the λ values obtained in Step 4.
Note that this approach diﬀers slightly from the original technique as proposed in , where they
use a Naive Bayes Parzen Window classiﬁer. We opted to use an elastic net logistic regression for
ease of implementation, and the fact that it has been used in existing software implementations of
FBCSP (for example, in BCILAB ).
Data Analysis
Classiﬁcation results are reported for two sets of analyses: within-subject and cross-subject. Withinsubject classiﬁcation uses a portion of the subjects data to train a model speciﬁcally for that subject,
although cross-subject classiﬁcation uses the data from other subjects to train a subject-agnostic
model. While within-subject models tend to perform better than cross-subject models on a variety
of tasks, there is ongoing research investigating techniques to minimize (or possibly eliminate) the
need for subject-speciﬁc information to train robust systems .
For within-subject, we use four-fold blockwise cross-validation, where two of the four blocks
are chosen to be the training set, one block as the validation set, and the ﬁnal block as testing.
We perform statistical testing using a repeated-measures Analysis of Variance (ANOVA), modeling
classiﬁcation results (AUC for P300/MRCP/ERN and Classiﬁcation Accuracy for SMR) as the
response variable with subject number and classiﬁer type as factors. For cross-subject analysis in
P300 and MRCP we choose, at random, 4 subjects for the validation set, one subject for the test
set, and all remaining subjects for the training set (see Table 1 for number of subjects per dataset).
This process was repeated 30 times, producing 30 diﬀerent folds. We follow the same procedure
for the ERN dataset, except we use the 10 test subjects from the original Kaggle Competition as
the test set for each fold. We perform statistical testing using a one-way Analysis of Variance,
using classiﬁer type as the factor. For the SMR dataset, we partitioned the data as follows: For
each subject, select the training data from 5 other subjects at random to be the training set and
the training data from the remaining 3 subjects to be the validation set. The test set remains
the same as the original test set for the competition. Note that this enforces a fully cross-subject
classiﬁcation analysis as we never use the test subjects’ training data. This process is repeated 10
times for each subject, creating 90 diﬀerent folds. The mean and standard error of classiﬁcation
performance were calculated over the 90 folds. We perform statistical testing for this analysis using
the same testing procedure as the within-subject analysis.
When training both the within-subject and cross-subject models, we apply a class-weight to
the loss function whenever the data is imbalanced (unequal number of trials for each class). The
class-weight we apply is the inverse of the proportion in the training data, with the majority class
set to 1. For example, in the P300 dataset, there is a 5.6:1 odds between non-targets and targets
(Table 1) . In this case the class-weight for non-targets was set to 1, while the class-weight for
targets was set to 6 (when the odds are a fraction, we take the next highest integer). This procedure
was applied to the P300 and ERN datasets only, as these were the only datasets where signiﬁcant
class imbalance was present.
Note that for the SMR analysis, we set the temporal kernel length to be 32 samples long (as
opposed to 64 samples long as given in Table 2) since the data were high-passed at 4Hz.
EEGNet Feature Explainability
The development of methods for enabling feature explainability from deep neural networks has
become an active research area over the past few years, and has been proposed as an essential
component of a robust model validation procedure, to ensure that the classiﬁcation performance
is being driven by relevant features as opposed to noise or artifacts in the data . We
present three diﬀerent approaches for understanding the features derived by EEGNet:
1. Summarizing averaged outputs of hidden unit activations: This approach focuses
on summarizing the activations of hidden units at layers speciﬁed by the user. In this work
we choose to summarize the hidden unit activations representing the data after the depthwise convolution (the spatial ﬁlter operation in EEGNet). Because the spatial ﬁlters are tied
directly to a particular temporal ﬁlter, they provide additional insights into the spatial localization of narrow-band frequency activity. Here we summarize the spatially-ﬁltered data by
calculating the diﬀerence in averaged time-frequency representations between classes, using
Morlet wavelets .
2. Visualizing the convolutional kernel weights: This approach focuses on directly visualizing and interpreting the convolutional kernel weights from the model. Generally speaking,
interpreting the convolutional kernel weights is very diﬃcult due to the cross-ﬁlter-map connectivity between any two layers. However, because EEGNet limits the connectivity of the
convolutional layers (using depthwise and separable convolutions), it is possible to interpret
the temporal convolution as narrow-band frequency ﬁlters and the depthwise convolution as
frequency-speciﬁc spatial ﬁlters.
3. Calculating single-trial feature relevance on the classiﬁcation decision: This approach focuses on calculating, on a single-trial basis, the relevance of individual features on
the resulting classiﬁcation decision. Positive values of relevance denote evidence supporting
the outcome, while negative values of relevance denote evidence against the outcome.
our analysis we used DeepLIFT with the Rescale rule , as implemented in , to calculate single-trial EEG feature relevance. DeepLIFT is a gradient-based relevance attribution
method that calculates relevance values per feature relative to a “reference” input (here, an
input of zeros, as is suggested in ), and is a technique similar to Layerwise Relevance
Propagation (LRP) which has been used previously for EEG analysis (a summary of
gradient-based relevance attribution methods can be found in ). This analysis can be used
to elucidate feature relevance from high-conﬁdence versus low-conﬁdence predictions, and can
4-fold Within-Subject Classification Performance
DeepConvNet
ShallowConvNet
EEGNet-4,2
EEGNet-8,2
Figure 2: 4-fold within-subject classiﬁcation performance for the P300, ERN and MRCP datasets
for each model, averaged over all folds and all subjects. Error bars denote 2 standard errors of
the mean. We see that, while there is minimal diﬀerence between all the CNN models for the
P300 dataset, there are signiﬁcant diﬀerences in the MRCP dataset, with both EEGNet models
outperforming all other models. For the ERN dataset we also see both EEGNet models performing
better than all others (p < 0.05).
be used to conﬁrm that the relevant features learned are interpretable, as opposed to noise
or artifact features.
Within-Subject Classiﬁcation
We compare the performance of both the CNN-based reference algorithms (DeepConvNet and
ShallowConvNet) and the traditional approach (xDAWN+RG for P300/MRCP/ERN and FBCSP
for SMR) with EEGNet-4,2 and EEGNet-8,2.
Within-subject four-fold cross-validation results
across all algorithms for P300, MRCP and ERN datasets are shown in Figure 2. We observed,
across all paradigms, that there was no statistically signiﬁcant diﬀerence between EEGNet-4,2
and EEGNet-8,2 (p > 0.05), indicating that the increase in model complexity did not statistically
improve classiﬁcation performance. For the P300 dataset, all CNN-based models signiﬁcantly outperform xDAWN+RG (p < 0.05) while not performing signiﬁcantly diﬀerent amongst themselves.
For the ERN dataset, EEGNet-8,2 outperforms DeepConvNet, ShallowConvNet and xDAWN+RG
(p < 0.05), while EEGNet-4,2 outperforms DeepConvNet and ShallowConvNet (p < 0.05). The
biggest diﬀerence observed among all the approaches is in the MRCP dataset, where both EEGNet
models statistically outperform all others by a signiﬁcant margin (DeepConvNet, ShallowConvNet
and xDAWN+RG, p < 0.05 for each comparison).
Four-fold cross-validation results for the SMR dataset are shown in Figure 3.
Here we see
the performances of ShallowConvNet and FBCSP are very similar, replicating previous results as
reported in , while DeepConvNet performance is signiﬁcantly lower. We also see that EEGNet-
4-fold Within-Subject Classification Performance: SMR
DeepConvNet
ShallowConvNet
EEGNet-4,2
EEGNet-8,2
Figure 3: 4-fold within-subject classiﬁcation performance for the SMR dataset for each model,
averaged over all folds and all subjects. Error bars denote 2 standard errors of the mean. Here we
see DeepConvNet statistically performed worse than all other models (p < 0.05). ShallowConvNet
and EEGNet-8,2 performed similarly to that of FBCSP.
8,2 performance is similar to FBCSP as well.
Cross-Subject Classiﬁcation
Cross-subject classiﬁcation results across all algorithms for P300, MRCP and ERN datasets are
shown in Figure 4. Similar to the within-subject analysis, we observed no statistical diﬀerence
between EEGNet-4,2 and EEGNet-8,2 across all datasets (p > 0.05). For the P300 dataset, all
CNN-based models signiﬁcantly outperform xDAWN+RG (p < 0.05) while not performing sig-
Cross-Subject Classification Performance
DeepConvNet
ShallowConvNet
EEGNet-4,2
EEGNet-8,2
Figure 4: Cross-Subject classiﬁcation performance for the P300, ERN and MRCP datasets for each
model, averaged for 30 folds. Error bars denote 2 standard errors of the mean. For the P300 and
MRCP datasets there is minimal diﬀerence between the DeepConvNet and the EEGNet models,
with both models outperforming ShallowConvNet. For the ERN dataset the reference algorithm
(xDAWN + RG) signiﬁcantly outperforms all other models.
Cross-Subject Classification Performance: SMR
DeepConvNet
ShallowConvNet
EEGNet-4,2
EEGNet-8,2
Figure 5: Cross-Subject classiﬁcation performance for the SMR for each model, averaged over all
folds and all subjects. Error bars denote 2 standard errors of the mean. We see that all CNN-based
models perform similarly, while slightly outperforming FBCSP.
niﬁcantly diﬀerent amongst themselves. For the MRCP dataset EEGNet-8,2 and DeepConvNet
signiﬁcantly outperform ShallowConvNet (p < 0.05). We also see that both DeepConvNet and
ShallowConvNet performance is better when compared to its within-subject performance for the
MRCP dataset. For the ERN dataset, xDAWN + RG outperforms all CNN models (p < 0.05).
Cross-subject classiﬁcation results for the SMR dataset are shown in Figure 5, where we found no
signiﬁcant diﬀerence in performance across all CNN-based models (p > 0.05).
EEGNet Feature Characterization
We illustrate three diﬀerent approaches to characterize the features learned by EEGNet: (1) Summarizing averaged outputs of hidden unit activations, (2) visualizing convolutional kernel weights,
and (3) calculating single-trial feature relevances on classiﬁcation decision. We illustrate Approach
1 on the P300 dataset for a cross-subject trained EEGNet-4,1 model. We chose to analyze the ﬁlters
from the P300 dataset due to the fact that multiple neurophysiological events occur simultaneously:
participants were told to press a button with their dominant hand whenever a target image appeared on the screen. Because of this, target trials contain both the P300 event-related potential
as well as the alpha/beta desynchronizations in contralateral motor cortex due to button presses.
Here we were interested in whether or not the EEGNet architecture was capable of separating out
these confounding events. We were also interested in quantifying the classiﬁcation performance of
the architecture whenever speciﬁc ﬁlters were removed from the model.
Figure 6 shows the spatial topographies of the four ﬁlters along with an average wavelet timefrequency diﬀerence, calculated using Morlet wavelets , between all target trials and all nontarget trials. Here we see four distinct ﬁlters appear. The time-frequency analysis of Filter 1 shows
an increase in low-frequency power approximately 500ms after image presentation, followed by
desynchronizations in alpha frequency. As nearly all subjects in the P300 dataset are right-handed,
we also see signiﬁcant activity along the left motor cortex. Time-frequency analysis of Filter 2
Figure 6: Visualization of the features derived from an EEGNet-4,1 model conﬁguration for one
particular cross-subject fold in the P300 dataset. (A) Spatial topoplots for each spatial ﬁlter. (B)
The mean wavelet time-frequency diﬀerence between target and non-target trials for each individual
appears to show a signiﬁcant theta-beta relationship; while increases in theta activity have been
previously noted in the P300 literature in response to targets , a relationship between theta
and beta has not previously been noted. The time-frequency diﬀerence for Filter 4 appears to
correspond with the P300, with an increase low-frequency power approximately 350ms after image
presentation.
Filters Removed
Test Set AUC
Table 4: Performance of a cross-subject trained EEGNet-4,1 model when removing certain ﬁlters
from the model, then using the model to predict the test set for one randomly chosen fold of the
P300 dataset. AUC values in bold denote the best performing model when removing 1, 2 or 3 ﬁlters
at a time. As the number of ﬁlters removed increases, we see decreases in classiﬁcation performance,
although the magnitude of the decrease depends on which ﬁlters are removed.
Spat. Filter 1
Spat. Filter 2
Figure 7: Visualization of the features derived from a within-subject trained EEGNet-8,2 model for
Subject 3 of the SMR dataset. Each of the 8 columns shows the learned temporal kernel for a 0.25
second window (top) with its two associated spatial ﬁlters (bottom two). We see that, while many
of the temporal ﬁlters are isolating slower-wave activity, the network identiﬁes a higher-frequency
ﬁlter at approximately 32Hz (Temp. Filter 3, which shows 8 cycles in a 0.25 s window).
We also conducted a feature ablation study, where we iteratively removed a set of ﬁlters (by
replacing the ﬁlters with zeros) and re-applied the model to predict trials in the test set. We do this
for all combinations of the four ﬁlters. Classiﬁcation results for this ablation study are shown in
Table 4. We see that test set performance is minimally impacted by the removal of any single ﬁlter,
with the largest decrease occurring when removing Filter 4. As expected, when removing pairs of
ﬁlters the decrease in performance is more pronounced, with the largest decrease observed when
removing Filters 3 and 4. Removing Filters 2 and 3 results in practically no change in classiﬁcation
performance when compared to the full model, suggesting that the most important features in
this task are being captured by Filters 1 and 4. This ﬁnding is further reinforced when looking
at classiﬁcation performance when three ﬁlters are removed; a model that contains only Filter 4
(0.8637 AUC) performs fairly well when compared to models that contain only Filter 2 (0.7108
AUC) or Filter 1 (0.7970 AUC).
Figure 7 shows the ﬁlters learned for the EEGNet-8,2 model for a within-subject classiﬁcation
of Subject 3 for the SMR dataset. Each column of this ﬁgure denotes the learned temporal kernel
(top row) with its two associated spatial ﬁlters (bottom two rows). Note that we are learning
temporal ﬁlters of length 32 samples, which correspond to 0.25 seconds in time; hence, we estimate
the frequency for each temporal ﬁlter as four times the number of observed cycles. Here we see that
EEGNet-8,2 learns both slow-frequency activity at approximately 12Hz (Filters 1, 2, 6 and 8, which
show three cycles in a 0.25 s window) and high-frequency activity at approximately 32Hz (Filter 3,
which show 8 cycles). Figure 8 compares the spatial ﬁlters associated with 8-12Hz frequency band
learned by EEGNet-8,2 with the spatial ﬁlters learned by FBCSP in the 8-12Hz ﬁlter-bank for each
of the four OVR combinations. For ease of description we will use the notation X-Y to denote the
row-column ﬁlter. Here we see many of the ﬁlters are strongly positively correlated across models
(i.e.: the 1-1 ﬁlter of EEGNet-8,2 with the 3-1 ﬁlter for FBCSP (ρ = 0.93) and the 2-1 ﬁlter of
EEGNet-8,2 with the 3-4 ﬁlter of FBCSP (ρ = 0.83)), while some are strongly negatively correlated
(the 3-1 ﬁlter of EEGNet-8,2 with the 1-1 ﬁlter of FBCSP (ρ = −0.93)), indicating a similar ﬁlter
up to a sign ambiguity. This suggests that EEGNet, through the use of depthwise convolutions, is
Spatial Filter 1 Spatial Filter 2 Spatial Filter 3 Spatial Filter 4
Spatial Filter 1 Spatial Filter 2
Left hand vs. all Temporal Filter 1
Right hand vs. all Temporal Filter 2
Both feet vs. all Temporal Filter 6
Tongue vs. all Temporal Filter 8
FBCSP 8-12Hz Spatial Filters EEGNet-8,2 12Hz Spatial Filters
Figure 8: Comparison of the 4 spatial ﬁlters learned by FBCSP in the 8-12Hz ﬁlter bank for each
OVR class combination (A) with the spatial ﬁlters learned by EEGNet-8,2 (B) for 4 temporal ﬁlters
that capture 12Hz frequency activity for Subject 3 of the SMR dataset (Temporal Filters 1, 2, 6
and 8, see Figure 7). We see that similar ﬁlters appear across both FBCSP and EEGNet-8,2.
capable of learning band-speciﬁc spatial ﬁlters in a similar manner as FBCSP.
Figure 9 shows the single-trial feature relevances for EEGNet-8,2, calculated using DeepLIFT,
for three three diﬀerent test trials for one cross-subject fold of the MRCP dataset. Here we see
that the high-conﬁdence predictions (Figure 9A and Figure 9B, for left and right ﬁnger movement,
respectively) both correctly show the contralateral motor cortex relevance as expected, whereas for
a low-conﬁdence prediction (Figure 9C), the feature relevance is more broadly distributed, both in
time and in space on the scalp.
Figure 10 shows an additional example of using DeepLIFT to analyze feature relevance for
a cross-subject trained EEGNet-4,2 model for one test subject of the ERN dataset.
et. al. previously noted that the average ERP for correct feedback trials has an earlier peak
positive potential, corresponding to approximately 325 ms, whereas the positive peak potential
for incorrect trials occurs slightly later, approximately 475 ms. Here we see the same temporal
diﬀerence in the timing of the peak positive potential for incorrect feedback trials (vertical line in
top row of Figure 10) and correct feedback trials (vertical line in bottom row of Figure 10). We also
see the DeepLIFT feature relevances align very closely to that of the peak positive potential for
both classes, suggesting that the network has focused on the peak positive potential as the relevant
feature for ERN classiﬁcation. This ﬁnding supports results previously reported in , where they
showed a strong positive correlation between the amplitude of the peak positive potential and the
accuracy of error detection.
Figure 9: (Top row) Single-trial EEG feature relevance for a cross-subject trained EEGNet-8,2
model, using DeepLIFT, for three diﬀerent test trials of the MRCP dataset: (A) a high-conﬁdence,
correct prediction of left ﬁnger movement, (B) a high-conﬁdence, correct prediction of right ﬁnger
movement and (C) a low-conﬁdence, incorrect prediction of left ﬁnger movement. Titles include the
true class label and the predicted probability of that label. (Bottom row) Spatial topoplots of the
relevances at two time points: approximately 50 ms and 150 ms after button press. As expected,
the high-conﬁdence trials show the correct relevances corresponding to contralateral motor cortex
for left (A) and right (B) button presses, respectively.
For the low-conﬁdence trial we see the
relevances are more mixed and broadly distributed, without a clear spatial localization to motor
Discussion
In this work we proposed EEGNet, a compact convolutional neural network for EEG-based BCIs
that can generalize across diﬀerent BCI paradigms in the presence of limited data and can produce
interpretable features. We evaluated EEGNet against the state-of-the-art approach for both ERP
and Oscillatory-based BCIs across four EEG datasets: P300 visual-evoked potentials, Error-Related
Negativity (ERN), Movement-Related Cortical Potentials (MRCP) and Sensory Motor Rhythms
(SMR). To the best of our knowledge, this represents the ﬁrst work that has validated the use of a
single network architecture across multiple BCI datasets, each with their own feature characteristics
and data set sizes. Our work introduced the use of Depthwise and Separable Convolutions 
for EEG signal classiﬁcation, and showed that they can be used to construct an EEG-speciﬁc
model which encapsulates well-known EEG feature extraction concepts. Finally, through the use of
feature visualization and ablation analysis, we show that neurophysiologically interpretable features
can be extracted from the EEGNet model, providing further validation and evidence that the
network performance is not being driven by noise or artifact signals in the data. This last ﬁnding is
particularly important, as it is a critical component to understanding the validity and robustness of
CNN model architectures not just for EEG , but for CNN architectures in general .
The learning capacity of CNNs comes in part from their ability to automatically extract intricate
feature representations from raw data.
However, since the features are not hand-designed by
Figure 10: Single-trial EEG feature relevance for a cross-subject trained EEGNet-4,2 model, using
DeepLIFT, for the one test subject of the ERN dataset. (Top Row) Feature relevances for three
correctly predicted trials of incorrect feedback, along with its predicted probability P. (Bottom
Row) Same as the top row but for three correctly predicted trials of correct feedback. The black
line denotes the average ERP, calculated at channel Cz, for incorrect feedback trials (top row) and
for correct feedback trials (bottom row). The thin vertical line denotes the positive peak of the
average ERP waveform. Here we see feature relevances coincide strongly with the positive peak of
the average ERP waveform for each trial. We also see the positive peak occurring slightly earlier
for correct feedback trials versus incorrect feedback trials, consistent with the results in .
human engineers, understanding the meaning of those features poses a signiﬁcant challenge in
producing interpretable models . This is especially true when CNNs are used for the analysis
of EEG data where features from neural signals are often non-stationary and corrupted by noise
artifacts .
In this study, we illustrated three diﬀerent approaches for visualizing the
features learned by EEGNet: (1) analyzing spatial ﬁlter outputs, averaged over trials, on the P300
dataset, (2) visualizing the convolutional kernel weights on the SMR dataset and comparing them
to the weights learned by FBCSP, and (3) performing single-trial relevance analysis on the MRCP
and SMR datasets. For the ERN dataset we compared single-trial feature relevances to averaged
ERPs and showed that relevant features coincided with the peak of the positive potential for
correct and incorrect feedback trials, which has been shown in previous literature to be positively
correlated to classiﬁer performance . In addition, we conducted a feature ablation study to
understand the impact of a classiﬁcation decision on the presence or absence of a particular feature
on the P300 dataset. In each of these analyses, we showed that EEGNet was capable of extracting
interpretable features that generally corresponded to known neurophysiological phenomena. These
results suggest that the classiﬁcation performances we observed were not due to artifact or noise
sources in the data.
Our results showed that the spatial ﬁlters learned by EEGNet for temporal kernels around
12Hz were signiﬁcantly correlated to the spatial ﬁlters learned by FBCSP in the 8-12Hz ﬁlter
bank for the SMR dataset.
This is interesting to note, as the optimization criterion for CSP
(optimal variance separation) is diﬀerent than the optimization criterion for EEGNet (minimum
cross-entropy loss). Because of this, it is not guaranteed that the learned ﬁlters from these methods
would be comparable. It was encouraging to see that many of the ﬁlters did in fact overlap (up to
a sign ambiguity), suggesting that EEGNet is learning a similar feature representation to that of
FBCSP. This analysis is directly enabled by EEGNet’s use of depthwise convolutions to tie spatial
ﬁlters directly to a temporal ﬁlter, an aspect that is unique to this model.
Generally speaking, the classiﬁcation performance of DeepConvNet and EEGNet were similar
across all cross-subject analyses, whereas DeepConvNet performance was lower across nearly all
within-subject analyses (with the exception of P300). One possible explanation for this discrepancy
is the amount of training data used to train the model; in cross-subject analyses the training set
sizes were about 10-15 times larger than that of within-subject analyses. This suggests that Deep-
ConvNet is more data-intensive compared to EEGNet, an unsurprising result given that the model
size of DeepConvNet is two orders of magnitude larger than EEGNet (see Table 3). We believe this
intuition is consistent with the ﬁndings originally reported by the developers of DeepConvNet ,
where they state that a training data augmentation strategy was needed to obtain good classiﬁcation performance on the SMR dataset. In contrast to their work, we show that EEGNet performed
well across all tested datasets without the need for data augmentation, making the model simpler
to use in practice.
In general we found that, both in within- and cross-subject analyses, that ShallowConvNet
tended to perform worse on the ERP BCI datasets than on the oscillatory BCI dataset (SMR),
while the opposite behavior was observed with DeepConvNet. We believe this is due to the fact
that the ShallowConvNet architecture was designed speciﬁcally to extract log bandpower features;
in situations where the dominant feature is signal amplitude (as is the case in many ERP BCIs),
ShallowConvNet performance tended to suﬀer. The opposite situation occurred with DeepConvNet;
as its architecture was not designed to extract frequency features, its performance was lower in
situations where frequency power is the dominant feature. In contrast, we found that EEGNet
performed just as well as ShallowConvNet in SMR classiﬁcation and just as well as DeepConvNet
in ERP classiﬁcation (and outperforming in the case of within-subject MRCP, ERN and SMR
classiﬁcations), suggesting that EEGNet is robust enough to learn a wide variety of features over a
range of BCI tasks.
The severe underperformance of ShallowConvNet on within-subject MRCP classiﬁcation was
unexpected, given the similarity in neural responses between the MRCP and SMR, and the fact
that ShallowConvNet performed well on SMR. This discrepancy in performance is not due to the
amount of training data used, as within-subject MRCP classiﬁcation has approximately 700 training
trials, evenly split among left and right ﬁnger movements, whereas the SMR dataset has only 192
training trials, evenly split among four classes. In addition, we did not observe large deviations
in ShallowConvNet performance on the other datasets (P300 and ERN). In fact, ShallowConvNet
performed fairly well on within-subject ERN classiﬁcation, even though this dataset is the smallest
among all datasets used in this study (only having 170 training trials total).
Determining the
underlying source of this phenomena will be explored in future research.
Deep Learning models for EEG generally employ one of three input styles, depending on their
targeted application: (1) the EEG signal of all available channels, (2) a transformed EEG signal
(generally a time-frequency decomposition) of all available channels or (3) a transformed EEG
signal of a subset of channels . Models that fall in (2) generally see a signiﬁcant increase in data
dimensionality, thus requiring either more data or more model regularization (or both) to learn
an eﬀective feature representation. This introduces more hyperparameters that must be learned,
increasing the potential variability in model performance due to hyperparameter misspeciﬁcation.
Models that fall in (3) generally require a priori knowledge about the channels to select.
example, the model proposed in uses the time-frequency decomposition of channels Cz, C3
and C4 as the inputs for a motor imagery classiﬁcation task. This channel selection is intentional,
given the fact that neural responses to motor actions (the sensory motor rhythm) are observed
strongest at those channels and are easily observed through a time-frequency analysis. Also, by
only working with three channels, the authors reduce the signiﬁcant increase in dimensionality
of the data. While this approach works well if the feature of interest is known beforehand, this
approach is not guaranteed to work well in other applications where the features are not observed
at those channels, limiting the overall utility of this approach. We believe models that fall in (1),
such as EEGNet and others , oﬀer the best tradeoﬀbetween input dimensionality and
the ﬂexibility to discover relevant features by providing all available channels. This is especially
important as BCI technologies evolve into novel application spaces, as the features needed for these
future BCIs may not be known beforehand .
Acknowledgments
This project was sponsored by the U.S. Army Research Laboratory under ARL-H70-HR52, ARL-
74A-HRCYB and through Cooperative Agreement Number W911NF-10-2-0022. The views and
conclusions contained in this document are those of the authors and should not be interpreted as
representing the oﬃcial policies, either expressed or implied, of the U.S. Government. The U.S.
Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.
Conﬂict of Interest Statement
The authors declare that the research was conducted in the absence of any commercial or ﬁnancial
relationships that could be construed as a potential conﬂict of interest.
DeepConvNet and ShallowConvNet architectures
The DeepConvNet and ShallowConvNet architectures are given in Tables 5 and 6, respectively. The
DeepConvNet was designed to be a general-purpose architecture that is not restricted to speciﬁc
feature types, whereas ShallowConvNet is designed speciﬁcally for oscillatory signal classiﬁcation.
Activation
mode = valid, max norm = 2
25 * 25 * C + 25
mode = valid, max norm = 2
epsilon = 1e-05, momentum = 0.1
Activation
25 * 50 * C + 50
mode = valid, max norm = 2
epsilon = 1e-05, momentum = 0.1
Activation
50 * 100 * C + 100
mode = valid, max norm = 2
epsilon = 1e-05, momentum = 0.1
Activation
100 * 200 * C + 200
mode = valid, max norm = 2
epsilon = 1e-05, momentum = 0.1
Activation
max norm = 0.5
Table 5: DeepConvNet architecture, where C = number of channels, T = number of time points
and N = number of classes, respectively.
Activation
mode = same, max norm = 2
40 * 40 * C
mode = valid, max norm = 2
epsilon = 1e-05, momentum = 0.1
Activation
AveragePool2D
(1, 35), stride (1, 7)
Activation
max norm = 0.5
Table 6: ShallowConvNet architecture, where C = number of channels, T = number of time points
and N = number of classes, respectively. Here, the ’square’ and ’log’ activation functions are given
as f(x) = x2 and f(x) = log(x), respectively. Note that we clip the log function such that the
minimum input value is a very small number (ϵ = 10e−7) for numerical stability.