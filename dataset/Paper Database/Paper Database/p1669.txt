Higher-Order Explanations of Graph Neural
Networks via Relevant Walks
Thomas Schnake, Oliver Eberle, Jonas Lederer , Shinichi Nakajima, Kristof T. Sch€utt
Klaus-Robert M€uller , Member, IEEE, and Gregoire Montavon
Abstract—Graph Neural Networks (GNNs) are a popular approach for predicting graph structured data. As GNNs tightly entangle the
input graph into the neural network structure, common explainable AI approaches are not applicable. To a large extent, GNNs have
remained black-boxes for the user so far. In this paper, we show that GNNs can in fact be naturally explained using higher-order
expansions, i.e., by identifying groups of edges that jointly contribute to the prediction. Practically, we ﬁnd that such explanations can be
extracted using a nested attribution scheme, where existing techniques such as layer-wise relevance propagation (LRP) can be applied
at each step. The output is a collection of walks into the input graph that are relevant for the prediction. Our novel explanation method,
which we denote by GNN-LRP, is applicable to a broad range of graph neural networks and lets us extract practically relevant insights
on sentiment analysis of text data, structure-property relationships in quantum chemistry, and image classiﬁcation.
Index Terms—Graph neural networks, higher-order explanations, layer-wise relevance propagation, explainable machine learning
INTRODUCTION
ANY interesting structures found in scientiﬁc and
industrial applications can be expressed as graphs.
Examples are lattices in ﬂuid modeling, molecular geometry,
biological interaction networks, or social/historical networks. Graph neural networks (GNNs) , have been
proposed as a method to learn from observations in general
graph structures and have found use in an ever growing
number of applications , , , , , . While GNNs
make useful predictions, they typically act as black-boxes,
and it has neither been directly possible (1) to extract novel
insight from the learned model nor (2) to verify that the
model has made the intended use of the graph structure,
e.g., that it has avoided Clever Hans phenomena .
Explainable AI (XAI) is an emerging research area that
aims to extract interpretable insights from trained ML models , . So far, research has focused, for example, on
full black-box models , , self-explainable models
 , , or deep neural networks , where in all cases,
the prediction can be attributed to the input features. For a
GNN, however, the graph being received as input is deeply
entangled with the model itself, hence requiring a more
sophisticated approach.
In this paper, we propose a theoretically founded XAI
method for explaining GNN predictions. The conceptual
starting point of our method is the observation that the function implemented by the GNN is locally polynomial with the
input graph. This function can therefore be analyzed using a
higher-order Taylor expansion to arrive at an attribution of
the GNN prediction on collections of edges, e.g., walks into
the input graph. —Such an attribution scheme goes beyond
existing XAI techniques for GNNs that are limited to identifying individual nodes or edges.
Furthermore, we ﬁnd that the higher-order expansion can
be expressed as a nesting of multiple ﬁrst-order expansions,
starting at the top layer of the GNN and moving towards the
input layer. This theoretical insight enables a principled
adaptation of the Layer-wise Relevance Propagation (LRP)
 explanation technique to GNN models, where the propagation procedure is guided along individual walks in the
input graph. The resulting procedure that we propose and
that we denote by GNN-LRP is shown in Fig. 1.
GNN-LRP applies directly to a broad range of GNN
architectures, without need to learn a surrogate function,
nor to run any optimization procedure. We demonstrate
Thomas Schnake, Oliver Eberle, Kristof T. Sch€utt, and Gregoire Montavon are with the BIFOLD – Berlin Institute for the Foundations of Learning and Data, Berlin Institute of Technology (TU Berlin), 10587 Berlin,
Germany. E-mail: {t.schnake, kristof.schuett, gregoire.montavon}@tu-berlin.de, .
Lederer is with the Berlin Institute of Technology (TU Berlin),
10587 Berlin, Germany. E-mail: .
Shinichi Nakajima is with the BIFOLD – Berlin Institute for the Foundations of Learning and Data, Berlin Institute of Technology (TU Berlin),
10587 Berlin, Germany, and also with the RIKEN AIP, Tokyo 103-0027,
Japan. E-mail: .
Klaus-Robert
M€uller is with the Google Research, Brain team, Berlin,
BIFOLD – Berlin Institute for the Foundations of Learning and Data, the
Berlin Institute of Technology (TU Berlin), 10587 Berlin, Germany, and
with the Department of Artiﬁcial Intelligence, Korea University, Seoul
136-713, Korea, and also with the Max Planck Institut f€ur Informatik,
66123 Saarbr€ucken, Germany. E-mail: .
Manuscript received 26 November 2020; revised 21 September 2021; accepted 22
September 2021. Date of publication 24 September 2021; date of current version 3
October 2022.
This work was funded by the German Ministry for Education and Research as
BIFOLD – Berlin Institute for the Foundations of Learning and Data (ref.
01IS18025A and ref. 01IS18037A), and the German Research Foundation
(DFG) as Math+: Berlin Mathematics Research Center . This work was partly supported by the Institute of Information & Communications Technology Planning & Evaluation (IITP) grants
funded by the Korea Government .
(Corresponding authors: Gregoire Montavon, Klaus-Robert M€uller.)
Recommended for acceptance by S. Ji.
Digital Object Identiﬁer no. 10.1109/TPAMI.2021.3115452
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 11, NOVEMBER 2022
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see 
GNN-LRP on a variety of GNN models from diverse application ﬁelds: (1) a sentiment prediction model receiving
sentence parse trees as input, (2) a state-of-the-art GNN for
quantum mechanically accurate prediction of electronic properties from molecular graphs, and (3) a widely adopted image
classiﬁer that we view as a GNN operating on pixel lattices.—
For each GNN model, our explanation method produces
detailed and reliable explanations of the decision strategy
from which novel application insights can be obtained.
The code for this paper can be found at 
tu-berlin.de/thomas_schnake/paper_gnn_lrp.
Related Work
We focus here on the related work that most directly connects to our novel GNN explanation approach, in particular,
(1) explanation techniques based on higher-order analysis,
and (2) explanation techniques that are specialized for
GNNs. For a more comprehensive set of related works, we
refer the reader to the review papers , for XAI and
 , for GNNs.
Higher-Order Explanations
Second-order methods (e.g., based on the model’s Hessian)
have been proposed to attribute predictions to pairs of input
features , , . Another work incorporates an
explicit sum-of-interactions structure into the model, in
order to obtain second-order or higher-order explanations.
Another approach detects higher-order feature interaction with an iterative algorithm which inspects neural network weights at the different layers.
Our work proposes instead to use the framework of Taylor expansions to arrive in a principled manner to the
higher-order explanations, and it identiﬁes GNNs as an
important use case for such explanations.
Explaining Graph Neural Networks
The work extends explanation techniques such as Grad-
CAM or Excitation Backprop to the GNN model, and arrives at
an attribution on nodes of the graph. In an NLP context, graph
convolutional networks (GCNs) have been explained in terms
of nodes and edges in the input graph using the LRP explanation method . GNNExplainer and PGExplainer 
explain the model by extracting the subgraph that maximizes
the mutual information to the prediction for the original
graph. XGNN is a model-level explanation method,
which produces typical graphs for a target class. PGMExplainer learns a probabilistic graphical model from the network, which is then able to measure the probability for
different higher order feature interactions in the model.
GraphMask learns binary masks for edges in each layer to
retain only the connections which are most important for the
prediction. Gem is a trained generative model that generates causal explanations of GNNs. The method SubgraphX
 proposes a Monte-Carlo tree search to ﬁnd relevant subgraphs in the input graph and uses Shapley values as an attribution function for subgraphs. Other recently proposed
methods that map the GNN prediction to graph substructures
include Trap2 and GraphLime . In the authors present a survey of different interpretation methods for GNNs.
Regarding the quality of the relevance features, most of the
proposed methods attribute the GNN prediction to nodes or
edges of the input graph , , , whereas GNN-LRP
gives scores for higher-order features, such as sequences of
edges. The quality of the attributions for GraphMask and
PGMExplainer are comparable with our approach. Yet, both
methods learn to understand the prediction strategy of a
GNN by a given optimization criterion, where the reliability
of such explanations strongly depends on the well-posedness
of the optimization criteria (e.g., convexity). Instead our
method is independent of any additional optimization criterion. The method SubgraphX attributes the prediction to
subgraphs of the input graph and is closely related to our subgraph selection technique presented in Section 4.1. SubgraphX uses a Monte-Carlo optimization algorithm to ﬁnd
the most relevant subgraph, whereas we use either a local best
guess or a random sampling approach . SubgraphX uses Shapley values for the subgraph scoring, whereas we use a backward
propagation pass.
TOWARDS EXPLAINING GNNS
In this section, we brieﬂy introduce graph neural networks
and then develop a Taylor-based explanation framework for
explaining these models. Throughout the paper, we make
use of graph-speciﬁc notation that we summarize in Table 1.
Graph Neural Networks
Graph neural networks (GNNs) , are special types of
neural networks that receive a graph as input. In practice,
graphs can take a variety of forms, e.g., directed, undirected,
labeled, unlabeled, spatial, time-evolving, etc. To handle the
high heterogeneity of graph structures, many variants of
GNNs have been developed (e.g., , , ). One commonality of most GNNs, however, is that the input graph is
not located at the ﬁrst layer, but occurs instead at multiple
layers, by deﬁning the connectivity of the network itself.
Graph neural networks are typically constructed by
stacking several interaction blocks. Each block t ¼ 1. . .T computes a graph representation HHt 2 Rndt where n is the
number of nodes in the input graph and dt is the number of
dimensions used to represent each node. Within a block, the
representation is produced by applying (i) an aggregate step
where each node receives information from the neighboring
Fig. 1. High-level illustration of GNN-LRP. The explanation procedure
starts at the GNN output, and proceeds backwards to progressively
uncover the walks that are relevant for the prediction.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 11, NOVEMBER 2022
nodes, and (ii) a combine step that extracts new features for
each node. These two steps (cf. ) connect the representations HHt1 and HHt of consecutive blocks as
aggregate:
ZZt ¼ LLHHt1
where LL is the input graph given as a matrix of size n  n,
e.g., the adjacency matrix to which we add self-connections.
We denote by ZZt;K the row of ZZt associated to node K, and
Ct is a ‘combine’ function, typically a one-layer or multilayer neural network, that produces the new representation
for each node in the graph.
The whole input-output relation implemented by the
GNN can then be expressed as a function
fðLL; HH0Þ ¼ g
LL; . . . HH1
which is a recursive application of Eqs. (1) and (2) starting
from some initial state HH0 2 Rnd0, followed by a readout
function g. The initial state typically incorporates information
that is intrinsic to the nodes, or it can be set to constant values
if no such information is present. The readout function is typically a classiﬁer (or regressor) of the whole graph, but it can
also be chosen to apply to subsets of nodes, for example, for
node classiﬁcation or link prediction tasks , .
First-Order Explanation
Consider ﬁrst a ‘classical’ approach to explanation where
we attribute the output of the neural network to variables in
the ﬁrst layer. In the case of the GNN, the ﬁrst layer is given
by the initial state HH0.
Let us now view the GNN as a function of the initial
state, i.e., fðHH0Þ. We will also denote by HH0;I the row of HH0
associated to node I. A Taylor expansion of the function f
at some reference point eHH0 gives
; ðHH0;I  eHH0;IÞ
where ‘. . .’ represents the zero-, second- and higher-order
terms that have not been expanded, and where the sum represents the ﬁrst-order terms. Because the sum runs over all
nodes I in the input graph, the addends in Eq. (4) readily
provide an attribution of the GNN output to each node.
It is arguable, however, whether this attribution can truly
be interpreted as identifying node contributions. Indeed,
the attribution may only reﬂect the importance of a node in
the ﬁrst layer, and not in the higher layers. Furthermore, an
attribution of the prediction on nodes may not be sufﬁcient
for the application needs. For example, it does not tell us
whether a node is important by itself, or if it is important
because of its connections to other nodes or some more complex structure in the graph.
These limitations can be attributed to the fact that we
have performed the decomposition w.r.t. the initial state HH0
instead of the ‘true’ input LL.
Higher-Order Explanation
Consider now the true input LL of the GNN. Since it occurs in
every interaction block and applies in a multiplicative manner (cf. Eq. (1)), a ﬁrst-order analysis of the GNN function
fðLLÞ would not be suitable to identify the multiplicative
interactions. These interactions can however be identiﬁed by
applying a higher-order Taylor expansion.
In the following, we will use the additional notation E to
denote the element of the matrix LL associated to a particular
edge E of the graph. Assuming that fðLLÞ is smooth on the
relevant input domain, we can compute at some reference
point eLL, a T-order Taylor expansion
@E1. . .@ET
 DE1  . . .  DET
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
with DE :¼ ðE  eEÞ and where we deﬁne aB! :¼ Q
with aB;E denoting the number of occurrences of edge E in the
bag B. The sum runs over all bags B of T edges. Hence, the
terms of the sum capture the joint effect of multiple edges on
the GNN output. The last line ‘þ. . .’ represents the nonexpanded terms of order lower or higher than T. Anecdotally,
for certain classes of functions, in particular, piecewise multilinear positively homogeneous functions (an example is
Notation Used Throughout the Paper
The table is separated between general mathematical notation (top), graph theoretic notation (middle) and notation that is used in the context of GNNs (bottom).
SCHNAKE ET AL.: HIGHER-ORDER EXPLANATIONS OF GRAPH NEURAL NETWORKS VIA RELEVANT WALKS
GNNs with ReLU nonlinearity and zero biases), choosing
the reference point eLL ¼ sLL makes the non-expanded terms
vanish in the limit of s ! 0, thereby leading to the conservation property P
B RB ¼ fðLLÞ (cf. Appendix A of the Supplement, available online).
Nested Computation and Relevant Walks
The higher-order Taylor expansion presented above is
conceptually simple and mathematically founded. However, systematically extracting higher-order derivatives of
a neural network is difﬁcult and does not scale to complex
To address this ﬁrst limitation, we introduce the concept
of a walk W, which we deﬁne to be an ordered sequence of
nodes that are connected in consecutive blocks of the GNN.
The relation between bag-of-edges and walks is illustrated
for a simple graph in Fig. 2.
Because each walk maps to a particular bag-of-edges, a
walk-based explanation inherits all information contained
in the bag-of-edges explanation. In particular, it is always
possible to recover the bag-of-edges explanation from a
walk-based explanation by applying the pooling operation
W2B RW. However, using walks brings two further
advantages:
A walk-based explanation gives more information,
how multiple blocks of the GNN have been used to
arrive at the prediction. For example, as illustrated
in Fig. 2, it can reveal whether message passing
between two nodes has occurred in the ﬁrst or in the
last blocks of the GNN.
The fact that the walks connect to the structure of the
GNN more directly, makes the explanation easier to
To show this, we introduce the new variable LL
ðLL; . . .; LLÞ which distinguishes between edges occurring in
different blocks of the GNN, and express the GNN output
as a function of this expanded input, i.e., fðLL
? Þ. We also
adopt a node-based notation, where walks are given by
the sequence of nodes they traverse from the ﬁrst interaction block to the last one, e.g., W ¼ ð. . .; J; K; L; . . .Þ. The
letters J; K; L denote nodes between the consecutive
blocks, and ‘. . .’ acts as a placeholder for the leading and
trailing nodes of the walk. We further denote by 
element of LL
? representing the connection between node J
and node K.
Proposition 1. For the considered function fðLL
? Þ the higherorder terms RB in Eq. (5) can be equivalently computed as a
sequential application of a ﬁrst order Taylor decomposition
along a walk W ¼ ð. . .; J; K; L; . . .Þ at the root point eLL
eLL? D
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
eLL? D
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
eLL? . . . ;
EÞ, and then applying the pooling operation
(A proof is given in Appendix B of the Supplement,
available online.) In other words, the attribution can be produced by analyzing each block iteratively from the top of
the network to the input features. Here again, when the
function is a nesting of piecewise linear positively homogeneous functions, choosing the reference point eLL
gives an explanation that is conservative in the limit of s !
0, and each step of the nested computation can be computed
as RJKL... ¼ ½rRKL...ðeLL
? ÞJ  JK i.e., ‘Gradient  Input (GI)’.
This simple explanation procedure which we call GNN-GI
will serve as a baseline in our experiments.
THE GNN-LRP METHOD
The approach of Section 2.4 gives us a practical way of
extracting higher-order explanations by analyzing interaction blocks individually. However, the analysis of each interaction block can itself be challenging. For example, the
interaction block of a GIN can be composed of multiple
layers. The high nonlinearity caused by these multiple layers
can make it difﬁcult if not impossible to choose a root point
? at which a Taylor expansion accurately models the quantity to explain.
In the following, we consider an extension of the Taylor
decomposition
(DTD) . It consists of replacing the Taylor expansion of
the multilayer model by several Taylor expansions performed at each layer. When applied to standard neural networks, DTD yields an explanation technique called Layerwise Relevance Propagation (LRP) , that was shown
to be more robust than simple gradient-based methods.
Application of LRP to the multiple interaction blocks of a
GNN yields a novel procedure for explaining GNNs that
we call ‘GNN-LRP’.
To simplify the exposition of our method, we will consider the special case of the graph convolutional network
(GCN) . In a GCN, each interaction block is composed
of a linear aggregate function with positive adjacencies, followed by a linear/ReLU combine function. We restate these
blocks by introducing the notation we use in this section
aggregate:
J denotes the activation of some neuron with index a
inside the node J. The notation P
a represents a sum over all
Fig. 2. Illustration of a bag-of-edges B and the corresponding walks W for a
simple input graph of three nodes. The two walks associated to the given
bag-of-edges are shown with a solid and a dashed line, respectively.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 11, NOVEMBER 2022
neurons a composing a node plus a hardcoded neuron ‘0’
with activation z0
K ¼ 1 and with w0b representing the bias. To
further simplify the notation, we also omit the star symbol
on the variable JK. The aggregate and combine steps of a
GCN are illustrated in Fig. 3 (top).
Deep Taylor Decomposition
Like in Section 2.4 we consider the problem of attributing
RKL... to the adjacencies JK. Unlike Eq. (6), deep Taylor
decomposition (DTD) adds further granularity to the attribution process, by considering relevance scores not at the
node level, but at the neuron level (i.e., Rb
KL...). Also, instead
of decomposing this quantity directly to the adjacencies K,
DTD considers as a ﬁrst step a redistribution on the intermediate representation zzzK.
For this, DTD deﬁnes a ‘relevance model’, which is chosen here to be bRb
KL...ðzzzKÞ ¼ hb
KðzzzKÞ cb
KL..., a product of the
neuron activation (which is a function of the intermediate
representation), and a term that is constant and set in a way
that the relevance model matches the true relevance Rb
locally. (A justiﬁcation for this relevance model is given in
Section 3.2.) Using this relevance model, we can now attribute the relevance score to neurons of the intermediate
representation by means of a ﬁrst-order Taylor expansion at
some root point ezzzK
KL... ¼ @ bRb
(Note that the root point ezzzK will typically be different for each
output neuron b.) An aggregate relevance score is then
obtained by summing contributions from neurons in the layer
above, i.e., Ra
KL.... The next step is to attribute the
newly computed relevance scores to the adjacencies JK in
the aggregate step. For this, we proceed similarly to above, by
ﬁrst deﬁning a relevance model bRa
KL... ¼ za
then computing the ﬁrst-order terms of a Taylor expansion
JKL... ¼ @ bRa
 ðJK  eJKÞ:
The overall layered attribution procedure is illustrated in
Fig. 3 (bottom).
Deriving GNN-LRP Propagation Rules
The equations above deﬁne a general framework for layerwise propagation of the relevance to the adjacencies JK. To
arrive at concrete propagation rules, it remains to set the
root points in Eqs. (7) and (8). For the ﬁrst Taylor expansion,
we choose ezzzK at the intersection of the line
fzzzK  szzzK ð11 þ g11wwb
00Þ j s 2 Rg;
and the ReLU hinge. The parameter g tilts the search for a root
point towards neurons with positive contributions. A high
value of g leads to root points ezzK that are closer to the activation zzK and that better contextualize the explanation. Injecting
this root point in Eq. (7) gives the relevance messages Ra b
K wab s ð1 þ g1wab
KL.... Resolving the parameter s and
pooling relevance messages coming from the multiple output
neurons, we obtain the propagation rule
Kðwab þ gwþ
Kðwab þ gwþ
For propagation in the aggregate layer, applying a similar
root search strategy as above yields the root point ek ¼ 00.
Injecting this root point in Eq. (8) gives the propagation rule
We are now in position to verify the validity of the relevance
model we have used in Section 3.1 to perform the Taylor
expansions. An inspection of Eqs. (9) and (10) shows that
the relevance score, resulting from applying these rules, can
always be written as a product of the corresponding activation and a term that depends on this activation only through
two nested sums. This provides a justiﬁcation for our relevance model which approximates the latter term as constant. (The same argument can be found in for standard
deep neural networks).
Interestingly, the rules in Eqs. (9) and (10) can be merged
into a single propagation rule that we show in Eq. (11) of
Table 2. The propagation rule can be seen as a generalization of the LRP-g rule to the GCN. It inherits some of its
theoretical properties such as the connection between LRP
and Gradient  Input (GI). In particular, in the limit of g !
0, the explanations produced by GNN-LRP become equivalent to those of the GNN-GI baseline. Our experiments will
however demonstrate that higher values of g produce better
explanations.
Fig. 3. Diagram of GNN-LRP annotated with variables that are used.
(1) Sketch of an interaction block featuring an aggregation layer followed
by a combine layer. The sketch shows nodes J and K, each of them represented by neurons. Two neurons of consecutive layers of the combine
function are denoted by a and b. (2) GNN-LRP propagation ﬂow, where
we observe that only relevance scores that propagate along a particular
walk in the input graph are retained.
SCHNAKE ET AL.: HIGHER-ORDER EXPLANATIONS OF GRAPH NEURAL NETWORKS VIA RELEVANT WALKS
Application of GNN-LRP Beyond the GCN Model
The derivation above has focused on the simple case of the
GCN model. However, the procedure can be extended to a
broad range of other models. For example, the GIN can
be seen as an extension of the GCN where the combine function consists of multiple layers. Here, we simply need to
apply LRP rules in each layer of the combine function followed by the aggregation layer (Eq. (12)). The procedure
can again be justiﬁed as a deep Taylor decomposition. Spectral ﬁltering methods such as the Spectral Network 
or ChebNet can be viewed as variants of the GCN with
multiple adjacency matrices, and GNN-LRP propagation
rules can also be derived for these models (Eq. (13)).
GNN-LRP can be applied to further GNN models such as
the original GNN model , GraphSAGE with mean aggregation , or Neural FP . GNN-LRP is also applicable to
other recent GNN architectures such as the SchNet used
for predicting molecular properties, and where the graph is a
representation of the distance between atoms. GNN-LRP is
also applicable to convolutional neural networks for computer vision such as VGG-16 , which can be seen as a particular GNN receiving as input a pixel lattice. GNN-LRP can
be extended to more advanced architectures such as joint
CNN-GNN models for spatio-temporal graphs . Furthermore, GNN-LRP allows in principle to use different edges
and nodes at different layers, which can be useful to handle
graph pooling structures, such as those described in .
More generally, GNN-LRP procedures can be designed
for any architecture that is expressible as an alternation of
aggregate and combine steps as given in Eqs. (1) and (2).
The combine step is then treated as a common neural network, built on some lower layer of activations. Here, existing LRP rules, that have been developed for a variety of
neural network models (e.g., , , ) can be applied.
In the aggregation step we either have a linear pooling over
nodes such as mean or sum aggregation, or a nonlinear
pooling, such as max-pooling in GraphSAGE . In the linear case, the same propagation rule as for the GCN in
Eq. (10) can be applied. In the nonlinear case, one needs to
build speciﬁc propagation rules, e.g., using the deep Taylor
decomposition approach of Section 3.1.
Implementing GNN-LRP
As an extension of LRP, GNN-LRP inherits several tricks
that strongly facilitate the implementation of the method
compared to a direct transcription of Eqs. (11), (12), and (13)
into code. One such trick is the use of forward/backward
hooks to alter the gradient computation in a way that it
matches the LRP signal.
For example, GNN-LRP for a GCN can be easily implemented by rewriting the combine function of each interaction block as
QQt PP t ½rðZZtW
MK þ ½QQtcst: ð11  M
where ½cst. detaches the quantity to which it applies from the
gradient and M
MK is a mask that retains node K. More details
are given in Appendix C in the Supplement, available online.
Along with automatic differentiation capabilities of neural
network software and the availability of predeﬁned layers
such as convolution or pooling, this implementation trick
allows to implement GNN-LRP for complex GNN architectures without much code overhead. This implementation trick
is also used in a GNN-LRP demo code that we provide at
 
Limitations
When implementing the masking approach from above, a
limitation of GNN-LRP is the need to compute in the general case as many forward-backward passes as there are
walks in the input graph. This typically limits the applicability of the method to GNNs of a limited depth (e.g., three
or four layers). Ability of GNN-LRP to scale to bigger
graphs and deeper models is tied to the application requirements, e.g., whether coarse-graining of certain nodes is permitted so that multiple relevant walks can be merged into a
single computation, or whether only the most relevant
walks are of interest, in which case pruning techniques can
be applied.
Some explanation techniques such as GNNExplainer
 , PGExplainer and SubgraphX are based solely
on evaluating the function or its gradient multiple times.
This enables an application of the method without further
knowledge of the model implementation. This is not the
case for GNN-LRP (and other explanation methods such as
GraphMask ), which require usage of the neural network internals. Our GNN-LRP method, in particular,
requires access to the representation at each layer in order
to implement appropriate propagation rules at each layer.
Practical GNN-LRP Propagation Rules for Different Types of GNNs
GNN-LRP Rule
ZZt ¼ LLHHt1
HHt ¼ rðZZtW
JKL... ¼ P
ZZt ¼ LLHHt1
HHt ¼ ðMLPðtÞðZZt;KÞÞK
JKL... ¼ P
Spectral , , and where the second class
has a slightly higher growth model and new nodes are
attached preferably to low-degree nodes (class 2). Examples
of graphs from the two classes are given in Fig. 4 (left).
Details on this synthetic dataset are given in Appendix E in
the Supplement, available online.
We consider a graph isomorphism network (GIN) that
we train on this task. Our GIN has two interaction blocks. In
each interaction block, the ‘combine’ function consists of a
two-layer network with 32 neurons per node at each layer.
The initial state HH0 is an all-ones matrix of size n  1, i.e.,
nodes do not have intrinsic information. The GIN receives
as input the connectivity matrix LL ¼ eAA=2 where eAA is the
adjacency matrix augmented with self-connections. The
GIN is trained on this task until convergence, where it
reaches an accuracy above 95 %. More details on the model
and its training are given in Appendix F.1 of the Supplement, available online. After training, we take an exemplary
input graph from class 1, predict it with our GIN, and apply
GNN-LRP on the prediction. We use the LRP parameter g ¼
2 and g ¼ 1 in each layer of the ﬁrst and second interaction
blocks respectively. The resulting explanation is shown in
Fig. 4 (right).
The explanation produced by GNN-LRP reveals that
walks that traverse or stay in the high-degree node are the
principal contributors to the GIN prediction. On the other
hand leaf nodes or sequences of low-degree nodes are
found to be either irrelevant or to be in slight contradiction
with the prediction.
In the following, we compare GNN-LRP to a selection of
other GNN explanation methods:
Pope et al. : The method views the GNN as a function of the initial state HH0, and performs an attribution of the GNN output on nodes as represented in
HH0. In principle, the proposed framework lets the
user choose the technique to perform attribution on
HH0. In our benchmark, we use the techniques Gradient  Input (GI) and LRP.
GNN-GI: This simple baseline replaces in the GNN-
LRP procedure the LRP steps by Gradient  Input
steps. It can also be seen as a special case of GNN-
LRP with parameter g ¼ 0.
GNNExplainer : The method runs an optimization
problem that ﬁnds a selection of edges that maximizes
the model output. The procedure can be viewed as
ﬁnding a mask M
M ¼ sðRRÞ where s denotes the logistic
sigmoid function, that maximizes the prediction
M LLÞ. The explanation is then given by RR.
Explanations produced by each method are shown in Fig. 5.
The method by Pope et al. highlights nodes that are relevant for the prediction. However, it is difﬁcult to determine
from the explanation whether the highlighted nodes are relevant by themselves or if they are relevant in relation to their
neighbors. The GNN-GI baseline we have contributed, and
our more advanced method GNN-LRP, provide a much
higher level of granularity, distinguishing between the contribution of the node and its interaction with other nodes. In comparison to GNN-LRP, however, the GNN-GI baseline tends to
be less selective, with spurious positive or negative relevance,
and generally more noisy (something we will also observe
later in Section 5.3). The GNNExplainer produces explanations that are in agreement with GNN-LRP but less detailed.
Overall, from our ﬁrst qualitative inspection, GNN-LRP
is the only method in our benchmark that produces explanations that have both the desired robustness and a high level
of detail.
From Attribution to Subgraph Selection
We would like to compare the methods above quantitatively.
A ﬁrst difﬁculty is that most quantitative benchmarks are
Fig. 4. Left: Examples from the two classes of the BA-growth dataset.
Right: GNN-LRP explanation for the GIN prediction of a graph of class 1.
Relevant (positively contributing) walks are shown in red and negatively
contributing walks are in blue. Circles represent walks (or part of the
walks) that are stationary.
Fig. 5. Comparison of different explanation techniques on the same graph
as in Fig. 4. GNN-LRP produces more detailed explanations compared to
 and , and brings more robustness compared to GNN-GI.
SCHNAKE ET AL.: HIGHER-ORDER EXPLANATIONS OF GRAPH NEURAL NETWORKS VIA RELEVANT WALKS
not making direct use of attribution scores, nor do they
operate at a speciﬁc granularity such as edges or walks.
Instead, two common evaluation methods that we will consider in Sections 4.2 and 4.3 require to output a maximally
relevant subgraph or sequence of subgraphs.
We introduce a technique to extract most relevant subgraphs from relevance scores obtained by an attribution
method. Our approach is applicable equally to node-based,
edge-based, and walks-based explanations and it makes
efﬁcient use of the higher-order information contained in
the latter explanations to extract more precise subgraphs.
Let X denote our unit of attribution. For a node-based attribution (e.g., ), an edge-based attribution (e.g., GNNExplainer ), or walks-based attribution, we replace X by a
node N , edge E or bag-of-edges B respectively. We deﬁne the
relevance of a subgraph S
G to be the relevance of subfeatures the subgraph is composed of. This means that a given
subgraph S is assigned the score
The best subgraph is then ideally chosen via the optimization problem
? ¼ arg max
where S is the set of admissible subgraphs, e.g., all graphs
composed of a given number of nodes. In the worst case, we
have exponential complexity to obtain S
? , which is especially costly for bag-of-edges attributions. Approximation
schemes that we use (random sampling, local best guess) for
these optimization problems are given in Appendix D in the
Supplement, available online.
Model Activation Task
Our ﬁrst evaluation experiment is called ‘model activation’
and is inspired by pixel-ﬂipping1 . We start with an
empty subgraph S and grow it by adding nodes one-byone. Nodes are selected at each step to incur a maximum
growth of the relevance score RS. While nodes are being
added to the graph, we keep track of the true GNN output
fðSÞ. The higher the GNN output, the more faithful the
explanation technique, as the latter was therefore able to
identify the correct substructure. Pseudo-code for our activation task is given in Algorithm 1.
The algorithm returns an area under the activation curve
(AUAC) score. The higher the AUAC score the better the
explanation technique. The procedure is repeated for a sufﬁciently large number of data points, leading to an averaged
AUAC score.
We now use the AUAC metric to evaluate the different
explanation methods on a broad set of architectures and
datasets. On the BA-growth dataset, we train a GCN, a
GIN, and a spectral network, each of them with two interaction blocks, and with respectively 128, 32 and 32 neurons
per node at each layer. For the spectral network, we give
in each layer the power expansion LL ¼ ½eAA0; 1
4 eAA2 as
input. All models perform almost perfectly on the classiﬁcation task. More details on the models and their training
can be found in Appendix F.1 of the Supplement, available
Algorithm 1. Computation of the AUAC Metric
Input: Ordered sequence of nodes N ð1Þ; . . .; N ðjGjÞ for
which PjGj
i¼1 RfN ð1Þ;...; N ðiÞg is maximized.
for i ¼ 1. . .jGj do
S S [ f N ðiÞg
" Add node to graph
AC.append(fðSÞ)
AUAC mean(AC)
return AUAC
In addition, we also consider networks trained on real
data: A ﬁrst one is designed and trained by ourselves on the
Stanford Sentiment Treebank (SST) dataset, where the
graph represents a syntactic tree with nodes carrying information about each word. Another one is the SchNet 
model used for molecular prediction where nodes and edges
represent atom types and distances respectively. Finally, we
use a pre-trained VGG-16 network for image recognition
that we interpret as a graph neural network operating on a
lattice of size 14  14 and starting at convolutional block 3. In
this last network, each node represents the collection of activations at a speciﬁc spatial location. Details for each network
are given in Appendix F in the Supplement, available online.
Results for the activation task for each network and explanation method are summarized in Table 3.
On the BA-growth dataset, we observe that GNN-LRP
outperforms other methods on average. Nearest competitors are Pope et al. (in combination with LRP), and the
GNNExplainer . This result corroborates our qualitative
analysis at the beginning of Section 4. We would also like to
AUAC Scores of Each Explanation Method
on Various Datasets and Models
All values are averages over 200 data points. The higher the score the better the
explanation. Best performers are shown in bold. The results on SchNet-E are
scaled by a factor of 103.
1. Pixel-ﬂipping starts with an original data point and ‘ﬂips’
(i.e., destroys) input features from most to least relevant according to
the explanation, and how quickly the output of the model drops
throughout the ﬂipping process. The faster the output drops, the more
faithful the explanation.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 11, NOVEMBER 2022
hyperparameters . The sensitivity of GNN-LRP towards
its hyperparameter g is shown in Fig. 6. We observe that
any choice of parameter g
1 delivers roughly the same
high benchmark performance. The lowest performance is at
g ¼ 0 which is equal to attributing with GNN-GI.
On the SST task, GNNExplainer and GNN-LRP come
ﬁrst and second, followed by GNN-GI and both node attribution methods. In natural language, the sentiment associated to
a sentence relies on word combinations, e.g., negation. This
can explain why methods which attribute on interactions of
words such as GNN-LRP and GNNExplainer perform better.
For the experiments on the SchNet model, our method performs again above competitors. Note that in this experiment,
we ﬁnd that using the LRP parameter g ¼ 0, which is equivalent to Gradient  Input (cf. , ), already gives good
explanations.2 Hence, GNN-GI and GNN-LRP have the
same performance in this case. The difference to other competitors (Pope et al. and GNNExplainer ) is small on
the prediction of the energy E but larger for the dipolemoment m, possibly because of a more complex structure of
the prediction task involving longer interactions.
Finally, on the VGG-16 image recognition model, GNN-
LRP again produces the highest AUAC score. Here, the
superiority of LRP over GI can be explained both by a better
handling of neuron biases and by a higher robustness to
shattered gradients, a key difﬁculty to account for when
explaining very deep models , .
In a similar experiment to the activation task we search
for the features that least effect the model output when
removing them from the input graph. We refer to this validation method as the ’pruning task’ in which we compute
the area under the pruning curve (AUPC). We found that
GNN-LRP outperforms most of the other baseline methods
in each of the experimental setups as well. For more details
on the pruning task and its results, we refer to Appendix G
in the Supplement, available online.
We also found that the activation and pruning tasks are
very similar to the ﬁdelity and sparsity tasks presented in 
and . We saw that a good performance for the AUAC and
AUPC aligns with a good performance in the ﬁdelity metric.
In addition we point out that the activation and pruning
tasks also reﬂect if the explanation method differentiate
properly between important and redundant graph features,
which is similar to what the sparsity measure does. We conclude that the activation and pruning tasks already reﬂect a
variety of existing validation methods. For more details on
the comparability of the evaluation metrics with additional
quantitative experiments, we refer to Appendix H in the
Supplement, available online.
Overall, we ﬁnd that GNN-LRP is systematically the best
method in our benchmark. With the ﬁne-grained yet robust
explanations it provides, GNN-LRP is capable of precisely
and contextually identifying elements of the graph that contribute the most or the least to the prediction.
BA-2motifs Benchmark
As a second quantitative evaluation, we consider the BA-
2motifs benchmark that comes with ‘ground-truth’ explanations. In this dataset, the class of each data point can be traced
to a particular motif in the input graph. We stress that, in contrast to the model activation task from Section 4.2, this benchmark, to detect the motifs in each data point, is only
meaningful if the model solves the problem well. Indeed, only
if the model exhibits a good performance without fraudulent
Clever Hans strategies, we can expect that the benchmark
measures the true quality of the attribution method.
For the benchmark evaluation we use the GIN architecture similar to the GIN introduced at the beginning of Section 4, which predicts the BA-2motifs dataset almost
perfectly, with an accuracy of 99.9%. For the exact model
architecture and its training we refer to Appendix F.1.2 of the
Supplement, available online. We also note that when using
GNN-LRP in this section we select the hyperparameter g ¼
3, g ¼ 1:5 and g ¼ 0 in the ﬁrst, second and third interaction
block respectively. The BA-2motifs benchmark assumes that
explanations are given as an ordered sequence of nodes (or
edges) from most to least relevant. It then computes the area
under the receiver operating characteristic (AUROC) of this
ordered sequence against the ground-truth subgraph. To
produce a good sequence of nodes/edges, we use the
approach described in Section 4.1, starting with an empty
graph and adding the nodes/edges in a way that that the
resulting sequence of subgraphs has its relevance scores
summing to the largest possible value. When the optimal
subgraph sequence is too expensive to compute, we use an
approximation scheme, namely the we use a random sampling
approach, which we discuss in more detail in Appendix D of
the Supplement, available online. In the random sampling
approach we choose the hyperparameter k, which represents
the number of feature orderings generated randomly, to be
100 and 150 when considering a sequence of ordered nodes
and ordered edges respectively. For the special case where
we need to produce a sequence of edges from a node-based
attribution (as in ), we generate the edge scores by summing the relevance scores of the containing nodes.
In Table 4 we see the AUROC for different attribution
methods divided into the cases where the explanation is
given as a sequence of nodes or edges (these scenarios are
referred to as ‘node classiﬁcation’ and ‘edge classiﬁcation’).
We see that for the node case, explanation methods Pope et al.
(LRP), GNN-LRP, PGExplainer and GNNExplainer, all have
an AUROC score above 0.9, which shows that all these explanation methods have been able to extract from the model the
class-speciﬁc motif in the input graphs. The GI methods
(namely P (GI) and GNN-GI) perform poorly for the
node and edge classiﬁcation, and are comparable to random
Fig. 6. The effect of the parameter g of GNN-LRP on the AUAC score on
the BA-growth dataset. Higher values of g give better performance.
2. This can be explained by the presence in SchNet of residual connections that reduce gradient noise, and also by the fact that some of
the nonlinearity of SchNet occurs in the mapping of distance between
atoms onto basis functions, which our explanation technique views as
constant and does not propagate through.
SCHNAKE ET AL.: HIGHER-ORDER EXPLANATIONS OF GRAPH NEURAL NETWORKS VIA RELEVANT WALKS
feature ordering. The method with the best performance for
both feature qualities, is GNN-LRP with a perfect performance in the case of nodes and an almost perfect score by
0.02 in the case of edges. This is an improvement of 0.05 to the
previous baseline performance in . We stress, that we
only used approximation schemes for ﬁnding the subgraph
ordering, hence we can potentially expect further performance gains if using more advanced algorithms for optimizing subgraph sequences.
Overall, we have shown that among all methods we have
tested, GNN-LRP performs best in ﬁnding motifs in the
input graph that give rise to the graph’s correct classiﬁcation.
Sanity Checks and Other Evaluations
As an additional evaluation, we perform the sanity checks
proposed in , which test if the explanation methods are
sensitive towards model randomization. We randomized
the parameter of a GCN model composed of two interaction
blocks, and trained on the BA-growth dataset, as described
in the beginning of Section 4. We found that the randomization of any layer has a profound effect on the heatmaps of
GNN-LRP and GNN-GI. In addition, for all interpretation
methods the relevance scores of the original and randomized models deviate drastically. We therefore conclude that
all interpretation methods pass the sanity test. For more
details on the randomization tests we refer to Appendix I of
the Supplement, available online.
Another work introduces further metrics for evaluating graph neural networks, called accuracy, consistency, faithfulness, and stability. Faithfulness and accuracy can be
related to our sanity checks and the analysis in Section 4.3
respectively. Consistency makes the assumption that each
high-performing model has a similar strategy, which does
not account for potential Clever Hans cases (we show
Clever-Hans-type strategies in the application sections).
Finally, attribution stability veriﬁes that the explanation
remains the same under a small perturbation of the input.
Here, we note that our proposed GNN-LRP method inherits
some properties of LRP, in particular, it is not subject to
potential discontinuities of the model’s gradient . It
therefore has better built-in stability properties compared to
gradient-based methods such as GI or GNN-GI.
As an additional remark, we would like to point out that
GNN-LRP—like for any explanation method in our benchmark—is not developed with built-in robustness to adversarial attacks . Hence, we recommend to restrict its
usage to non-adversarial scenarios.
NEW INSIGHTS WITH GNN-LRP
Having validated the proposed GNN-LRP method on a
diverse set of GNNs including state-of-the-art models, and
having shown the multiple advantages of our method compared to previous approaches, we will now inspect the explanations produced by GNN-LRP on some of these practically
relevant GNN models to demonstrate how useful insights can
be extracted about the GNN model and the task it predicts.
Sentiment Analysis
In natural language processing (NLP) text data can be processed either as a sequence, or with its corresponding grammatical structure represented by a parse tree , . The latter
serves as an additional structural input for the learning algorithm, to incorporate dependencies between words. NLP tasks
are therefore particularly amenable to GNNs since these models can naturally incorporate the graph structure.
In the following experiments, we will demonstrate how
GNN-LRP can be used to intuitively and systematically
assess the quality of a GNN model, including its overall prediction strength and also its few weaknesses. For this, we
will consider a GCN composed of two interaction layers, to
classify sentiments in natural language text . We train
our model on the Stanford Sentiment Treebank (SST) .3
(Note that this example serves as a mere demonstration for
the versatility of our explanation approach and is by no
means intended to reﬂect or compete with state-of-the-art
NLP systems.) For details on the experimental setup we
refer the reader to Appendix F.2 in the Supplement, available online.
In Fig. 7 (top) we show an example of a GNN-LRP explanation for some exemplary input sentence containing a mixture
of positive and negative sentiment. We observe that distinct
combinations of words give rise to the emphasis of these sentiments. The model correctly detects word combinations such
as “the best movies” to carry a positive sentiment, and “boring
pictures” to contribute negatively.
AUROC of Selected Explanation Methods on BA-2motifs
The AUROC computation is differentiated between node and edge classiﬁcation. All values are averages over 200 data points. The values for PGExplainer
are extracted from .
Fig. 7. Sentence predicted by the GNN and the BoW model, and
explained by GNN-LRP (applied on the difference between the positive
and negative sentiment logit, and with the LRP parameter g ¼ 3). Contributions to positive sentiment are in red, and contributions to negative
sentiment are in blue.
3. 
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 11, NOVEMBER 2022
GNN-LRP can also be used to assess the GNN prediction
strategy relative to other (simpler) models, such as Bag-of-
Words (BoW). The BoW model can be seen as a GNN model
with zero interaction layers, hence our explanation technique applies to that model as well. Fig. 7 (bottom) shows
the explanation of the BoW prediction for the same sentence
as for the GNN. Interestingly, several words are now attributed a sentiment different from the one obtained with the
GNN. For example, “like” becomes positive, which however
appears in contradiction with the preceding words “didn’t”.
Hence, GNN-LRP has highlighted from a single sentence
that the GNN model is able to properly capture and disambiguate the sentiment of consecutive words, whereas the
BoW model is not.
In the next experiment, we consider two additional sentences from the SST corpus, where GNN-LRP contributes to
uncovering or better understanding ﬂaws of a trained GNN
model. Fig. 8 A shows a data sample that contains sarcasm
and that is falsely classiﬁed by the GNN to be positive. Our
explanation method highlights that relevant walks are too
localized to capture the interaction between words that
jointly explain sarcasm. Instead, local positive interactions
such as “more entertained” dominate the prediction, which
leads to the incorrect prediction. In Fig. 8 B, we see a case of
entity bias where the GNN model is biased towards particular entities, namely “Hugh Grant” and “Sandra Bullock”. In
this example, GNN-LRP ﬁnds that the GNN model uses
with no objective reason “Hugh Grant” and “Sandra Bullock”
as evidence for positive and negative sentiment, respectively. In NLP model biases are well studied areas and
some approaches to tackle that problem have already been
developed , .
To identify words or combinations of words that systematically contribute to a positive or negative sentiment—and
potentially discover further cases of entity bias,—we apply
GNN-LRP on the whole dataset. This lets us ﬁnd the
combination of words (given by a walk W) that are on average the most positive/negative (according to their score
RW). Fig. 9 shows top-3 walks of both types (positive and
negative) and containing one to three unique words.
We see that the walks which are most relevant for the
task contain positive adjectives and adverbs, such as
“solidly”, “brilliant”, “witty” or “astonishing”. The walks with
a very negative relevance score contain negative words
such as “ridiculous”, “boring” or “horrible”, but also subsequences like “sorry, charlie” or “no more.” which clearly
transport a negative emotion. Here again, GNN-LRP detects
an entity bias by the word “lewis”, which the GNN model
considers to be positively contributing although this word
is objectively neutral. Note that this time, this entity bias
was discovered directly, without having to visualize a large
number of explanations.
Overall, applying the proposed GNN-LRP explanation
method to the GNN model for sentiment classiﬁcation has
highlighted that GNN predictions are based on detecting
meaningful sentence sub-structures, rather than single
words as in the BoW model. Furthermore, GNN-LRP was
able to ﬁnd the reasons for incorrect predictions, or to shed
light on potential model biases. The latter could be identi-
ﬁed manually by visual inspection of many explanations, or
systematically by averaging the GNN-LRP results on a
whole corpus.
Quantum Chemistry
In the ﬁeld of machine learning for quantum chemistry ,
 , , , , GNNs have been exhibiting state of the art
performance for predicting molecular properties , ,
 , . Such networks incorporate a graph structure of molecules either by the covalent bonds or the proximity of atoms.
In this section we will test the ability of GNN-LRP to
extract meaningful domain knowledge from these state-ofthe-art GNNs. We will consider for this the SchNet,4 a GNN
for the prediction of molecular properties , , , and
we set the number of interaction blocks in this GNN to
three. We train the model on the atomization energies and
the dipole moments for 110,000 randomly selected molecules in the QM9 dataset .5 For more details on the
model parameters and the network architecture, we refer to
Appendix F.3 in the Supplement, available online. On a test
set of 13,885 molecules, the atomization energy and dipole
moment are predicted well with a mean absolute error
(MAE) of 0.015 eV and 0.039 Debye, respectively.
Our ﬁrst objective is to get an insight into what structures in
the molecule contribute positively or negatively to the
Fig. 8. Two selected examples of dependency trees from the SST dataset, predicted by the GNN model to be positive, and for which GNN-LRP
highlights a ﬂaw in the prediction strategy.
Fig. 9. Walks that contribute the most to positive and negative sentiment
according to the GNN, split by the number of unique words they contain.
4. 
5. 
SCHNAKE ET AL.: HIGHER-ORDER EXPLANATIONS OF GRAPH NEURAL NETWORKS VIA RELEVANT WALKS
molecule’s energy. While it is common to look at the atomization energy (describing the energy difference to dissociated
atoms), we consider here for the purpose of explanation the
centered negative atomization energy, and we deﬁne this
quantity to be the actual ‘energy’. With this deﬁnition, molecules have high energy when they are hard to break and typically formed of strong bonds, and conversely, molecules have
low energy when they are easy to break and unstable.
We consider for illustration the case of the paracetamol
(acetaminophen) molecule, and feed this molecule to
SchNet. Once the SchNet model has predicted its energy,
we apply the GNN-LRP analysis in order to produce an
explanation of the prediction. The resulting explanation is
shown in Fig. 10 (left). We observe that the explanation is
dominated by self-walks (i.e., staying in a single atom) or
one-edge walks (traversing a single edge of the graph, in
most cases, a bond). Bonds associated to the aromatic ring
and bonds of higher order contribute strongly to the predicted energy, whereas regions involving single bonds contribute negatively. To verify whether this observation
generalizes to other molecules, we perform the GNN-LRP
analysis on a set of 1,000 molecules randomly drawn from
the QM9 dataset and show in Fig. 10 (right) the average
bond contribution for each bond type. We observe an
increasing energy contribution with ascending bond order.
This coincides with chemical intuition that bonds of higher
order are more stable and, thus, require more energy to
break.—Note that the SchNet does not take bond types as
an input, but as highlighted by GNN-LRP, it has clearly
inferred these chemical features from the data.
We now turn to another quantum chemical property, the
dipole moment, and its prediction by SchNet (cf. ). The quantity produced at the output of the model has the form kmmðLLÞk.
The nonlinearity of the norm introduces higher-order terms in
the GNN function and this prevents a direct application of
GNN-LRP. Instead, we consider for explanation the dot product hmmðLLÞ; ½mmðLLÞ=kmmðLLÞkcst.i, where the left hand side functionally depends on the GNN input LL, and where the right
hand side is the normalized direction of the predicted dipole
moment, detached from the gradient computation. With this
modiﬁcation, while the output of the model remains the same
locally, the top layer becomes linear, therefore GNN-LRP can
proceed as for the energy prediction case.
Fig. 11 (top) shows the GNN-LRP explanation of the predicted dipole moment for the same molecule as in the previous experiment. Here, we observe that the contributions to
the dipole moment found by GNN-LRP form a gradient from
one side to the other of the molecule. The orientation corresponds to the positive and negative pole of the molecule. To
verify whether this insight generalizes to other molecules, we
consider a set of 1,000 molecules and we normalize each molecule to a span of 1 along its dipole direction. Subsequently, we
project all walks onto their respective dipole to obtain a onedimensional distribution of absolute relevance values for all
molecules. Note that the atom density of molecules, in general, is not homogeneous, and thus, in some regions more
walks may occur while other regions do not exhibit as many
walks. Hence, for each molecule the distribution of absolute
relevance is normalized w.r.t. its atom density. The result of
this aggregated analysis is shown in Fig. 11 (bottom).
This quantitative result conﬁrms the alignment of GNN-
LRP contributions with the positive and negative poles of the
molecule, as it was found qualitatively on the paracetamol
molecule. This result is in accordance with chemical intuition
regarding the dependence of the dipole on the span of the
Fig. 10. Left: Paracetamol molecule, and the GNN-LRP explanation of its
predicted energy. Red and blue indicate positive and negative contributions. Opacity indicates the magnitude of these contributions. Right: Average energy contribution per bond, depicted for each bond type separately,
in arbitrary units.
Fig. 11. Top: Paracetamol molecule with the predicted dipole direction
shown as a dotted arrow, and the GNN-LRP explanation. Bottom: Distribution of contributions (in absolute terms) along the direction of the predicted dipole moment, averaged over the dataset.
Fig. 12. Expanded GNN-LRP explanation for the dipole moment prediction of the paracetamol molecule. To increase visibility of two-edge walks
and three-edge walks, we show the scaled relevance scores R0 ¼ R0:7.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 11, NOVEMBER 2022
Because the walk-based explanations produced by GNN-
LRP are very detailed, some of the more intricate details of
the explanation cannot always be visualized on a single
molecule. Hence, we perform a further experiment where
the GNN-LRP explanation is spread over multiple visuals,
each of them showing relevant walks covering a speciﬁc
number of edges. With this expanded visualization, we seek
in particular to better distinguish between local atom-wise
contribution and more global effects. Fig. 12 shows this
expanded analysis of the dipole moment prediction for the
paracetamol molecule.
From this visualization, we gain further insights into the
strategy used by the SchNet model when predicting the
dipole moment. In our expanded explanation, one-edge walks
clearly indicate the electrostatic poles of the molecule, while
giving a hint on local dipoles. Self-walks (i.e., 0-edge walks)
incorporate elements that are inherent to the atom types, in
particular, their electronegativity. Two-edge walks provide
rather complex and spatially less resolved contributions.
Finally, three-edge walks, that are also the most global
descriptors, again provide interesting spatial contributions,
and appear to dampen the one-edge walks contributions
based on the more complex structures they are able to capture.
Overall, GNN-LRP has provided insights into the structure-property relationship of molecules that reach beyond
the original prediction task. The resulting relevant walks
agree with chemical characteristics of the molecule, thus,
indicating that the neural network has indeed learned
chemically plausible regularities.
Revisiting Image Classiﬁcation
A convolutional neural network (CNN) can be seen as a particular graph neural network (GNN) operating on lattices of
pixels. CNN predictions have so far mainly been explained
using heatmaps highlighting pixels that are the most relevant for a given prediction , , . Heatmaps are a
useful representation summary of the decision structure,
but they do not reveal the more complex strategies of a
network that have been used to progressively build the prediction layer after layer. We will show by viewing CNNs as
graph neural networks and extracting relevant walks in the
resulting pixel lattice, that our GNN-LRP method is capable
of shedding light into these strategies.
For this, we consider the well-established VGG-16 network. It consists of a collection of blocks interleaved by pooling
layers, where each block is composed of a sequence of convolution and ReLU layers. We use the pretrained version of the
VGG-16 network without batch-normalization, which can
be retrieved using the TorchVision module of PyTorch.6
Because the VGG-16 neural network is deep and the
number of possible walks grows exponentially with neural
network depth, we marginalize explanations to only consider the position of the walk at the input and at the output
of a block. This is easily achieved by using a mask-based
implementation (cf. Appendix C of the Supplement, available online) and removing all masks except those at the
input and output of the block. We then compute one explanation for block 3, 4, and 5. Also, to cope with the large spatial lattices in each block, we make use of the multi-mask
strategy also outlined in Appendix C, available online. Speciﬁcally, observing that each block of the VGG-16 network
has receptive ﬁelds of size 7, we can process multiple walks
at the same time by choosing the mask to be a grid with
stride 7. This allows us to collect all relevant walks at the
given block in the order of 49 backward passes.
We consider two exemplary images7 that the VGG-16
network respectively predicts as ‘teapot’ and ‘dumbbell’.
We set the LRP parameter to g ¼ 0:5 in block 3, halving the
parameter value in each subsequent block, and choosing
g ¼ 0 in the top-level classiﬁer. Fig. 13 (left) shows the result
of the analysis for these two images at various blocks of the
VGG-16 network.
For the ﬁrst image, Block 3 detects local edges in the teapot, then, in Block 4, the walks converge to center points of
speciﬁc parts of the teapot (e.g., the handle, the spout and
the knob), and ﬁnally the walks converge in Block 5 to the
center of the teapot, which can be interpreted as composing
Fig. 13. Left: Relevant walks in the pixel lattice explaining the prediction by the VGG-16 network of two input images as ‘teapot’ and ‘dumbbell’
respectively. In each vector ﬁeld, arrows connect block input nodes to the relevance-weighted average position of the block output nodes. Right: Comparison of GNN-LRP with different explanation techniques on Block 4.
6. 
7. Images are from and 
ﬁffy, rescaled and cropped to the relevant region to produce images of
size 224  224 which are the standard input size for VGG-16.
SCHNAKE ET AL.: HIGHER-ORDER EXPLANATIONS OF GRAPH NEURAL NETWORKS VIA RELEVANT WALKS
the different parts of the teapot. For this exemplary image,
we further observe in Fig. 13 (right) the advantageous properties of GNN-LRP compared to more basic explanation
methods. The GNN-GI baseline also produces a vector ﬁeld,
however, the latter is signiﬁcantly more noisy than the one
produced by GNN-LRP. The method by Pope et al. 
robustly highlights relevant nodes at the input of the given
block, however, it does not reveal where these features are
being transported for use in the subsequent block.
For the second image, we investigate a known ‘Clever
Hans’ strategy where the network classiﬁes images as
‘dumbbell’ by detecting both the dumbbell and the arm that
holds it . Using GNN-LRP we observe that Blocks 3 and 4
detect the arm and the dumbbell separately, and then, Block 5
composes them into a single ‘dumbbell-arm’ concept, as
shown by the walks for both objects converging to some center
point near the wrist. Clearly these insights could not have been
obtained from a standard pixel-wise heatmap explanation.
Overall, our GNN-LRP method can be used to comprehensively inspect the prediction of an image classiﬁer
beyond what would be possible with a standard pixel-wise
heatmap explanation. This deeper explanation capability
allows us to better understand the detailed structure of
image classiﬁcations, and also to shed more light into anecdotal ‘Clever Hans’ effects observed in the context of stateof-the-art image classiﬁers.
CONCLUSION
Graph neural networks are a highly promising approach for
predicting graphs, with a strong demand from the practical
side. For these models to be broadly adopted, it is however
important that their predictions are made explainable to the
user.—Because the input of a GNN is tightly entangled with
the model itself, the explanation problem is particularly dif-
ﬁcult, and methods for explaining GNNs have so far been
In this paper we have proposed a novel theoretically
principled approach to produce these explanations, based
on higher-order Taylor expansions. From this conceptual
starting point, we have then contributed two practical algorithms: GNN-GI which we propose as a simple baseline,
and GNN-LRP which is more robust and scales to highly
non-linear models. GNN-LRP produces detailed explanations that subsume the complex nested interaction between
the GNN model and the input graph. It also signiﬁcantly
outperforms other explanation methods in our quantitative
benchmark.—In addition to the high quality of the explanations it produces, GNN-LRP is also broadly applicable (covering the GCN, the GIN, spectral ﬁltering approaches and
others), and it can handle virtually any type of input graph,
whether it is a parse tree, a spatial graph, a pixel lattice, etc.
This broad applicability is demonstrated in our extensive
application showcase, including sentiment analysis, quantum chemistry and image classiﬁcation. In each scenario,
GNN-LRP could highlight the diverse strategies employed
by the GNN models, and unmask some undesired ‘Clever
Hans’ strategies. In our quantum-chemical application
showcase, we could additionally extract interesting problem-relevant insights. Future work will apply the proposed
methodology to analyze properties of materials for practically highly relevant tasks, e.g., in catalysis.
ACKNOWLEDGMENTS
We would like to thank Jacob Kauffmann, Lukas Ruff and
Marina H€ohne for the highly helpful and illuminating discussions on the topic and also Niklas W. A. Gebauer for helping
with the molecule visualization. We would like to thank Jasmijn Bastings for very helpful comments on the manuscript.