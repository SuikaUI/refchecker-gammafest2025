Generalized Zero- and Few-Shot Learning via Aligned Variational Autoencoders
Edgar Sch¨onfeld1
Sayna Ebrahimi2
Samarth Sinha3
Trevor Darrell2
Zeynep Akata4
1Bosch Center for AI
2UC Berkeley
3 University of Toronto
4University of Amsterdam
Many approaches in generalized zero-shot learning rely
on cross-modal mapping between the image feature space
and the class embedding space. As labeled images are expensive, one direction is to augment the dataset by generating either images or image features. However, the former
misses ﬁne-grained details and the latter requires learning
a mapping associated with class embeddings. In this work,
we take feature generation one step further and propose a
model where a shared latent space of image features and
class embeddings is learned by modality-speciﬁc aligned
variational autoencoders. This leaves us with the required
discriminative information about the image and classes in
the latent features, on which we train a softmax classiﬁer.
The key to our approach is that we align the distributions
learned from images and from side-information to construct
latent features that contain the essential multi-modal information associated with unseen classes. We evaluate our
learned latent features on several benchmark datasets, i.e.
CUB, SUN, AWA1 and AWA2, and establish a new state
of the art on generalized zero-shot as well as on few-shot
learning. Moreover, our results on ImageNet with various
zero-shot splits show that our latent features generalize well
in large-scale settings.
1. Introduction
Generalized zero-shot learning (GZSL) is a challenging
task especially for unbalanced and large datasets such as
ImageNet . Although at training time no visual data of
some classes, i.e. unseen classes, are provided the classiﬁer
must learn to differentiate between all classes, i.e. seen and
unseen classes. As visual data of unseen classes is not available at training time, typically knowledge transfer from seen
to unseen classes is achieved via some form of side information that encode semantic relationship between classes,
i.e. class embeddings.
Most approaches to GZSL learn a mapping between images and their class embeddings. An orthogonal approach is to augment data by generating artiﬁlatent space
Classifier
pink belly
brown wings
feature space
Figure 1: Our CADA-VAE model learns a latent embedding
(z) of image features (x) and class embedding (c(y) of labels y) via aligned VAEs optimized with cross-alignment
(LCA) and distribution alignment (LDA) objectives, and
subsequently trains a classiﬁer on sampled latent features
of seen and unseen classes.
cial images . However, due to the level of detail missing
in the synthetic images, CNN features extracted from them
do not improve classiﬁcation accuracy. To alleviate this issue, proposed to generate image features via a conditional WGAN, which simpliﬁes the task of the generative
model and directly optimizes the loss on image features. Although the features generated by improved GZSL signiﬁcantly, GAN-based loss functions suffer from instability in training. Hence, recently conditional variational autoencoders (VAE) have been employed for this purpose. As GZSL is inherently a multi-modal learning task,
 proposed to transform both modalities to the latent
spaces of autoencoders and match the corresponding distributions by minimizing the Maximum Mean Discrepancy
(MMD). Learning such cross-modal embeddings is beneﬁcial for potential downstream tasks that require multimodal
fusion, e.g. visual question answering. In this domain, 
recently used a cross-modal autoencoder to extend visual
question answering to previously unseen objects.
 
In this work, we train VAEs to encode and decode features from different modalities, e.g. images and class attributes, and use the learned latent features to train a generalized zero-shot learning classiﬁer. Our latent representations are aligned by matching their parametrized distributions and by enforcing a cross-modal reconstruction criterion. Consequently, by explicitly enforcing alignment both
in the latent features and in the distributions of latent features learned using different modalities, the VAEs enable
knowledge transfer to unseen classes without forgetting the
previously seen classes.
Our contributions are as follows. (1) We propose the
CADA-VAE model that learns shared cross-modal latent
representations of multiple data modalities using VAEs
via distribution alignment and cross alignment objectives.
(2) We extensively evaluate our model using conventional
benchmark datasets, i.e. CUB, SUN, AWA1 and AWA2,
on zero-shot and few-shot learning settings. Our model establishes the new state-of-the-art performance on generalized zero-shot and few-shot learning settings on all these
datasets. Furthermore, we show that our model can be extended easily to more than two modalities that are trained
simultaneously. (3) Finally, we show that the latent features
learned by our model improve the state of the art in the truly
large-scale ImageNet dataset in all splits for the generalized
zero-shot learning task.
2. Related Work
In this section, we present related work on generalized
zero-shot learning, few-shot learning and cross-modal reconstruction.
Generalized Zero-and Few-Shot Learning. In zero-shot
learning, training and test classes are disjoint with shared
attributes annotated on class level, and the performance of
the method is solely judged on its classiﬁcation accuracy on
the novel, i.e. unseen classes. Generalized zero-shot learning is a more realistic variant of zero-shot learning, since
the same information is available at training time, but the
performance of the model is judged on the harmonic mean
of the classiﬁcation accuracy on seen and unseen classes.
In few-shot learning, there are k examples provided at
training time for the previously unseen classes . Using auxiliary information for few-shot learning was
introduced in , where attributes related to images were
used to improve the performance of the model. The use of
auxiliary information was also explored in ReViSE , in
which a common image-label semantic space for transductive few-shot learning is learned. Analogous to the relation
between ZSL and GZSL, we extend few-shot to the generalized few-shot learning (GFSL) setting, in which we evaluate
the model on both seen and unseen classes.
Data-Generating Models for GZSL. Generative models
are used for generating images or image feature as a dataaugmentation mechanism in GZSL. These approaches treat GZSL as a missing data problem and train
conditional GANs or conditional VAEs to generate image
features for unseen classes from semantic side-information.
In this work, we create latent space features instead.
Cross-Modal Embedding Models.
Recent cross-modal
embeddings for GZSL are based on autoencoders, such as
ReViSE and DMAE , which learn to jointly represent features from different modalities in their latent space.
By making use of autoencoders, it is possible to learn representations of visual and semantic information in a semisupervised fashion. Learning a joint representation for visual and semantic data is achieved by aligning the latent
distributions between different data types. ReViSE implements this distribution alignment by minimizing the maximum mean discrepancy between the two latent distributions . DMAE aligns distributions by means of minimizing the Squared-Loss Mutual Information . In this
work, we use Variational Autoencoders instead, and align
the latent distributions by minimizing their Wasserstein distance. In contrast to and , we also enforce a crossreconstruction loss, by decoding every encoded feature into
every other modality.
Cross-Reconstruction in Generative models.
Reconstructing data across domains,
referred to as crossalignment, is commonly used in the ﬁeld of domain adaptation. While models like CycleGAN learn to generate
data across domains directly, latent space models use crossreconstruction to capture the common information contained in both domains in their intermediate latent representations . In this regard, cross-aligned VAE’s have been
used previously for text-style transfer and image-toimage translation . In a cross-aligned VAE ensures
that the latent representations of texts from different input
domains are similar, while in a comparable approach
matches the latent representations of images from different
domains. Both methods have in common that they use a different variant of VAEs with an adversarial loss. Additionally, makes use of conditional encoders and decoders,
while enforces cycle consistency and weight sharing.
Similarly, better generalization can be achieved if the shared
representation space is amenable to class-interpolation, sentence interpolation and image interpolation . In this
paper, our building blocks are unconditional VAEs and we
achieve multi-modal alignment via cross-reconstruction and
latent distribution alignment in a highly reduced space.
3. CADA-VAE Model
Of the existing GZSL models, recent data generating approaches achieve superior performance over
other methods on disjoint datasets.
Classifying gener-
pink belly
brown wings
Figure 2: Our Cross- and Distribution Aligned VAE (CADA-VAE ). Latent distribution alignment is achieved by minimizing
the Wasserstein distance between the latent distributions (LDA). Similarly, the cross-alignment loss (LCA) encourages the
latent distributions to align through cross-modal reconstruction.
ated image features from GANS or conditional VAEs
 runs at risk of being compromised by the curse of
dimensionality. On the other hand, CADA-VAE has a control over the dimensionality and structure (via a prior) of
the features to be classiﬁed. The main insight of our proposed model is that instead of generating images or image features, we generate low-dimensional latent features
and achieve both stable training and state-of-the-art performance. Hence, the key to our approach is the choice of a
VAE latent-space, a reconstruction and cross-reconstruction
criterion to preserve class-discriminative information in
lower dimensions, as well as explicit distribution alignment
to encourage domain-agnostic representations.
3.1. Background
We ﬁrst provide the background as the task (GZSL) and
as the model building blocks (variational autoencoders).
Generalized Zero-shot Learning. The task deﬁnition of
GZSL is as follows. Let S = {(x, y, c(y))| x ∈X, y ∈
Y S, c(y) ∈C} be a set of training examples, consisting of
image-features x, e.g. extracted by a CNN, class labels y
available during training and class-embeddings c(y). Typical class-embeddings are vectors of hand-annotated continuous attributes or Word2Vec features . In addition, an
auxiliary training set U = {(u, c(u))| u ∈Y u, c(u) ∈C}
is used, where u denote unseen classes from a set Y u, which
is disjoint from Y S.
Here, C(U) = {c(u1), ..., c(uL)}
is the set of class-embeddings of unseen classes. In the
legacy challenge of ZSL, the task is to learn a classiﬁer
fZSL : X →Y U. However, in this work, we focus on
the more realistic and challenging setup of GZSL where the
aim is to learn a classiﬁer fGZSL : X →Y U ∪Y S.
Variational Autoencoder (VAE). The basic building block
of our model is the variational autoencoder (VAE) .
Variational inference aims at ﬁnding the true conditional
probability distribution over the latent variables, pφ(z|x).
Due to interactibility of this distribution, it can be approximated by ﬁnding its closest proxy posterior, qθ(z|x),
through minimizing their distance using a variational lower
bound limit. The objective function of a VAE is the variational lower bound on the marginal likelihood of a given
datapoint and can be formulated as:
L = Eqφ(z|x)[log pθ(x|z)] −DKL(qφ(z|x)||pθ(z))
where the ﬁrst term is the reconstruction error and the
second term is the unpacked Kullback-Leibler divergence
between the inference model q(z|x), and p(z).
A common choice for the prior is a multivariate standard Gaussian distribution. The encoder predicts µ and Σ such that
qφ(z|x) = N(µ, Σ), from which a latent vector z is generated via the reparametrization trick .
3.2. Cross and Distribution Aligned VAE
The goal of our model is to learn representations within
a common space for a combination of M data modalities. Hence, our model includes M encoders, one for every modality, to map into this representation space.
minimize information loss, the original data must be reconstructed via the decoder networks. In effect, the basic VAE
loss of our model is the sum of M VAE-losses:
Eqφ(z|x)[log pθ(x(i)|z)]
−βDKL(qφ(z|x(i))||pθ(z))
where β weights the KL-Divergence . In the case of
matching image features with class embeddings, M = 2,
x(1) ∈X and x(2) ∈C(Y S). Yet, making the modalityspeciﬁc autoencoders learn similar representations across
modalities requires additional regularization terms. Therefore, our model aligns the latent distributions explicitly and
enforces a cross-reconstruction criterion. In Figure 2 we
show an overview of our model, depicting these two forms
of latent distribution matching, which we refer to as Cross-
Alignment (CA) and Distribution-Alignment (DA).
Cross-Alignment (CA) Loss. Here, reconstructions are obtained by decoding the latent encoding of a sample from another modality, but the same class. Hence, every modalityspeciﬁc decoder is trained on the latent vectors derived from
the other modalities. This cross-reconstruction loss is:
|x(j) −Dj(Ei(x(i)))|.
where Ei is the encoder of a feature of ith modality and
Dj is the decoder of a feature of the same class but the jth
Distribution-Alignment (DA) Loss. Generated image and
class representations can also be matched by minimizing
their distance. Here, we minimize the Wasserstein distance
between the latent multivariate Gaussian distributions. In
the case of multivariate Gaussians, a closed-form solution
of the 2-Wasserstein distance between two distributions
i and j is given as:
||µi −µj||2
+ Tr(Σi) + Tr(Σj) −2(Σ
Since the encoder predicts diagonal covariance matrices,
which are commutative, this distance simpliﬁes to:
 ||µi −µj||2
and the Distribution Alignment (DA) loss for a group an
M-tuple is written as:
Cross- and Distribution Alignment (CADA-VAE) Loss.
The cross- and distribution aligned VAE combines the basic
VAE-loss with LCA (CA-VAE) and LDA (DA-VAE):
LCADA−V AE = LV AE + γLCA + δLDA
where γ and δ are the weighting factors of the cross alignment and the distribution alignment loss, respectively. We
show in section 4.1 that our model can learn shared multimodal embeddings of more than two modalities, without
examples of all modalities being available for all classes.
Implementation Details. All encoders and decoders are
multilayer perceptrons with one hidden layer. More hidden
layers degraded performance as CNN features and attributes
are already very high-level representations. We use 1560
hidden units for the image feature encoder and 1660 for the
decoder. The attribute encoder and decoder have 1450 and
660 hidden units, respectively.
The latent embedding size is 64.
For ImageNet, we
chose a size of 128, and use two hidden layers of identical
size for the encoder, with the number of hidden units speciﬁed above and the image feature decoder layers are of size
1160 and 1660, while the attribute decoder uses 460 and
660 units. The model is trained for 100 epochs by stochastic
gradient descent using the Adam optimizer and a batch
size of 128 for ImageNet and 50 for all other datasets. Each
training batch consists of pairs of CNN features and matching attributes, from different seen classes. A pair of data
always belongs to the same class. After individual VAEs
learn to encode features of only their speciﬁc datatype for
some epochs, we also start to compute cross- and distribution alignment losses. δ is increased from epoch 6 to epoch
22 by a rate of 0.54 per epoch, while γ is increased from
epoch 21 to 75 by 0.044 per epoch. For the KL-divergence
we use an annealing scheme , in which we increase the
weight β of the KL-divergence by a rate of 0.0026 per epoch
until epoch 90. A KL-annealing scheme serves the purpose
of ﬁrst letting the VAE learn “useful” representations before
they are “smoothed” out, since the KL-divergence would be
otherwise a very strong regularizer .
We empirically found that it is useful to use a variant of
the reparametrization trick , in which all dimensions of
the noise vector are sampled from a single unimodal Gaussian. Also, using the L1 distance as reconstruction error
appeared to yield slightly better results than L2. After training, the VAE encoders transform the training and test set of
the ﬁnal linear classiﬁer into the latent space 1.
4. Experiments
We evaluate our framework on four widely used benchmark datasets CUB-200-2011 (CUB), SUN attribute
(SUN) , Animals with Attributes 1 and 2 (AWA1 ,
AWA2 ) for the GZSL and GFSL settings.
All image features for VAE training originate from the 2048dimensional ﬁnal pooling layer of a ResNet-101. To avoid
violating the zero-shot assumption, i.e. test classes need to
be disjoint from the classes that ResNet-101 was trained
with, we use the proposed training splits in .
1code at 
Table 1: Ablation study. We compare GZSL accuracy on
CUB for different multi-modal alignment objective functions, i.e. DA-VAE (distribution aligned VAE) , CA-VAE
(cross-aligned VAE) and CADA-VAE (cross and distribution aligned VAE).
Attributes serve as class embeddings when available. For
CUB, we also use sentence embeddings extracted from 10
sentences annotated per image averaged per class and
for ImageNet we used Word2Vec embeddings provided
by . All hyperparameters were chosen on a validation set
provided by . We report the harmonic mean (H) between seen (S) and unseen (U) average per-class accuracy.
4.1. Analyzing CADA-VAE in Detail on CUB
In this section, we analyze several building blocks of our
proposed framework such as the model, the choice of class
embeddings as well as the size and the number of latent
embeddings generated by our model in the GZSL setting.
Analyzing Model Variants.
In this ablation study,
we present the results of different objective functions
and the corresponding VAE variants, CA-VAE (crossaligned VAE), DA-VAE (distribution-aligned VAE) and
CADA-VAE (cross- and distribution-aligned VAE) on the
CUB dataset in GZSL setting.
As shown in Table 1, the cross-alignment objective noticably improves performance compared to distribution alignment (50.2% vs. 45.8%). This is due to the fact that both
seen and unseen class accuracies increase, i.e. seen class
accuracy increases by 4.5% and unseen class accuracy increases by 4.3%, when we use cross alignment loss rather
than the distribution alignment loss.
Moreover, combining distribution alignment and the cross-alignment objectives, i.e. CADA-VAE, increases the accuracy to 52.4% that
comes from adding the distribution alignment to the CA-
VAE. Our ablation study shows that aligning both the latent representations and the latent spaces is complementary
since their combination leads to the highest result on both
seen, unseen classes and their harmonic mean.
Analyzing Side Information. In sparse data regimes especially in zero-shot learning semantic representation of
the classes, i.e.
class embeddings, are as important as
the image embeddings as they enable knowledge transfer from seen to unseen classes. We compare the results
obtained with per-class attributes, per-class sentences and
class-based Word2Vec representations.
Figure 3: Effect of different class embeddings. (Left) Seen,
unseen and harmonic mean accuracy for CUB using different class embeddings as side information. (Right) Using
both attributes and sentences as side information, i.e. XS:
the percentage of seen classes with sentences, XU: the percentage of unseen classes with sentences. Attributes are the
class embeddings for the (100 −X)% of the classes.
Our results in Figure 3 (left) show that per-class sentence embeddings result in the best performance among all
three, i.e. 53.4%, attributes follow closely, i.e. 52.4%. With
Word2Vec, the difference between the seen and the unseen
class accuracy is large, indicating that the latent representations learned by Word2Vec are weak in robustness. This
is expected, given that Word2Vec features do not explicitly or exclusively represent visual characteristics. In summary, these results demonstrate that our model is able to
learn from various sources of side information. The results
also show that latent features learned with more discriminative class embeddings lead to better overall accuracy.
To investigate one of the most prominent aspects of our
model, i.e.
the ability to handle missing side information, we train CADA-VAE such that XS% of seen class image features are paired with sentence embeddings while the
other (100−XS)% of seen classes are paired with attributes.
The setup is evaluated for XS = 0, 30, 50, 70, 100. We also
vary the fraction XU% of unseen classes that are learned
from sentence features (whereas (100 −XU)% denotes the
fraction of unseen classes for which image features are only
paired with attributes).
Figure 3 (right) shows the results using different fractions of sentence embeddings and attribute embeddings for
XS and XU. When XU is held stable at 50%, i.e. both
seen and unseen classes have access to sentences and attributes equally half the time, we reach the highest accuracy for an XS-XU ratio of 50%-50%.
Interestingly, at
(XS = 0, XU = 50), i.e. no seen-class sentences while unseen classes are represented by both attributes, the accuracy
is 40%. On the other hand, at (XS = 50, XU = 0), i.e. no
unseen-class sentences while seen classes are represented
by both attributes sentences, the accuracy increases to 45%.
At (XS = 50, XU = 100), i.e. 50% attributes and 50% sen-
100 200 250
latent dimensions (d)
Figure 4: The inﬂuence of the dimentionality of the latent
features that are generated by CADA-VAE and used to train
the GZSL classiﬁer. We measure the harmonic mean accuracy on the CUB dataset
tences for seen classes but unseen classes are represented
by sentences only, the accuracy further increases to 47%.
These results indicate that sentences have an edge over attributes. However, when either sentences or attributes are
not available, our model can recover the missing information from the other modality and still learn discriminative
representations.
Increasing Number of Latent Dimensions. In this analysis, we have explored the robustness of our method to the
dimensionality of the latent space. Higher dimensions allow more degrees of freedom but require more data, while
compact features capture the essential discriminative information. Without loss of generality, we report the harmonic
mean accuracy of CADA-VAE for different latent dimensions on CUB, i.e. 12, 25, 50, 64, 100, 200 and 250.
We observe in Figure 4 that the accuracy initially increases with increasing dimensionality until it achieves its
peak accuracy of 52.4% at d = 64 and ﬂattens until d = 100
after which it declines upon further increase of the latent
dimension. We conclude from these experiments that the
most discriminative properties of two modalities are captured when the latent space has around 64 −100 dimensions. For efﬁciency reasons, we use 64 dimensional latent
features for the rest of the paper.
Increasing Number of Latent Features. Our model can
be used to generate an arbitrarily large number of latent features. In this experiment, we vary the number of latent features per class from 1 to 300 on CUB in the GZSL setting
and reach the optimum performance with 50 or more latent
features per seen class (Figure 5, left). In principle, seen
and unseen classes do not need to have the same number
of samples. We also vary the number of features per seen
and unseen classes. Indeed, the best accuracy is achieved
when there are approximately twice as many features per
unseen than seen classes which improves the accuracy from
37% to 52%. While 100 latent features per every class, i.e.
features per seen class
features per unseen class
features per seen class
fixed: RUS=1
fixed: RUS=2
fixed: RUS=3
Figure 5: Analyzing the effect of the number of latent features per class on the harmonic mean accuracy in GZSL. An
unseen-seen ratio RUS of 2 means that twice as many samples are generated for unseen classes than for seen classes
. The dynamic dataset (light blue) does not rely on a ﬁxed
number of sampled latent features.
200 × 100 = 20K, gives 38% accuracy, having 50 latent
features per seen classes and 100 latent features per unseen
class, i.e. 100×50+50×150 = 12.5K, leads to 52% accuracy. Hence, generating more features of under-represented
classes is important for better accuracy.
As for our results in Figure 5 on the right, we build a
dynamic training set by generating latent features continuously at every iteration and do not use any sample more than
once. Hence, we eliminate one of the tunable parameters,
i.e. the number of latent features to generate. Because of
the non-deterministic mapping of the VAE encoder, every
latent feature of a different class is unique. Our results indicate that the best accuracy is achieved when unseen and
seen class samples are equally balanced. In CUB, using a
dynamic training set reaches the same performance as using a ﬁxed dataset with 100 unseen examples and 50 seen
examples. On the other hand, using a ﬁxed dataset leads to
a faster training procedure. Hence, we use a ﬁxed dataset
with 200 examples per seen class and 400 examples per unseen class in every benchmark reported in this paper.
4.2. Comparing CADA-VAE on Benchmark Datasets
In this section, we compare our CADA-VAE on four
benchmark datasets, i.e. CUB, SUN, AWA1 and AWA2,
in the GZSL and GFSL setting.
Generalized Zero-Shot Learning. We compare our model
with 11 state-of-the-art models. Among those, CVAE ,
SE , and f-CLSWGAN learn to generate artiﬁcial
visual data and thereby treat the zero-shot learning task as
a data-augmentation task. On the other hand, the classic
ZSL methods DeViSE , SJE , ALE , EZSL 
and LATEM use a linear compatibility function or
other similarity metrics to compare embedded visual and
semantic features; CMT and LATEM utilize multiple neural networks to learn a non-linear embedding; and
Feature Size
LATEM 
DeViSE 
f-CLSWGAN 
ReViSE 
ours (CADA-VAE)
Table 2: Comparing CADA-VAE with the state of the art. We report per class accuracy for seen (S) and unseen (S) classes
and their harmonic mean (H). All reported numbers for our method are averaged over ten runs.
SYNC aligns a class embedding space and a weighted
bipartite graph. ReViSE learns a shared latent manifold between the image features and class attributes using
autoencoders.
The results in Table 2 show that our CADA-VAE outperforms all other methods on all datasets. The accuracy
difference between our model and the closest baseline, Re-
ViSE , is as follows: 52.4% vs 32.3% on CUB, 40.6%
vs 22.0% on SUN, 64.1% vs 41.1% on AWA1 and 63.9% vs
42.8% on AWA2. Moreover, our model achieves signiﬁcant
improvements over feature generating models most notably
on CUB. In doing so, CADA-VAE is the ﬁrst cross-modal
embedding model to outperform methods based on featureaugmentation. Compared to the classic methods, our model
leads to at least 100% improvement in harmonic mean accuracies.
In the legacy challenge of zero-shot learning,
CADA-VAE provides competitive performance, i.e.
on CUB, 61.8 on SUN, 62.3 on AWA1, 64.0 on AWA2.
However, in this work, we focus on the more practical and
challenging GZSL setting.
Since our model does not use any CNN features, i.e.
we generate 64-dimensional latent features for all classes,
it achieves a balance between seen and unseen class accuracies better than CNN feature-generating approaches especially on CUB. In addition, CADA-VAE learns a shared representation in a weakly-supervised fashion, through a crossreconstruction objective. Since the latent features have to
be decoded into every involved modality, and since every
modality encodes complementary information, the model
is encouraged to learn an encoding that retains the information contained in all used modalities. In doing so, our
method is less biased towards learning the distribution of
the seen class image features, which is known as the projection domain shift problem . As we generate a certain
number of latent features per class using non-deterministic
encoders, our method is also akin to data-generating approaches.
However, the learned representations lie in a
lower dimensional space, i.e. only 64, and therefore, are
less prone to bias towards the training set of image features.
In effect, our training is more stable than the adversarial
training schemes used for data generation . In fact, we
did not conduct any dataset speciﬁc parameter tuning and
use the same parameters for all datasets.
Generalized Few-Shot Learning. We evaluate our models by using zero, two, ﬁve and ten shots for GFSL on four
datasets. We compare our results with the most similar published work in this domain, i.e. ReViSE . Figure 6
shows that our latent representations learned from the side
information improves over the GZSL setting signiﬁcantly
even by including only a few labeled samples. Speciﬁcally,
adding a single latent feature from unseen classes to the
training set improves the accuracy by 1-10%, depending
on the dataset. While on CUB the accuracy improvement
from 0 to 10 shots is 12%, on AWA1&2 this improvement
reaches 20%. Moreover, while the harmonic mean accuracy increases with the number of shots in both methods,
all variants of our method outperform the baseline by a large
margin across all the datasets indicating the generalization
capability of our method to the GFSL setting.
Furthermore, similar to the GZSL scenario, on the ﬁnegrained CUB and SUN datasets, CADA-VAE reaches the
highest performance where it is followed by CA-VAE and
DA-VAE, respectively.
However, on AWA1 and AWA2
Figure 6: Comparing CA-VAE, DA-VAE, CADA-VAE with ReViSE with increasing numbers of training samples from
unseen classes, i.e. in the generalized few-shot setting.
M500 M1K M5K L500 L1K
Figure 7: ImageNet results on GZSL. We report the top-
1 accuracy for unseen classes.
Both f-CLSWGAN and
CADA-VAE use a linear softmax classiﬁer.
the difference between different models is not signiﬁcant.
We associate this with the fact that as AWA1 and AWA2
datasets are coarse-grained datasets, the image features are
already discriminative.
Hence, aligning the latent space
with attributes does not lead to a signiﬁcant difference.
4.3. ImageNet Experiments
The ImageNet dataset serves as a challenging testbed for
GZSL. In several evaluation splits were proposed with
increasing granularity and size both in terms of the number of classes and the number of images. Note that since
all the images of 1K classes are used to train ResNet-101,
measuring seen class accuracies would be biased. However,
we can still evaluate the accuracy of unseen class images
in the GZSL search space that contains both seen and unseen classes. Hence, at test time the 1K seen classes act
as distractors. This way, we can measure the transferability
of our latent representations to completely unseen classes,
i.e. classes that are not seen either during ResNet training
nor CADA-VAE training. For ImageNet, since attributes
are not available, we use Word2Vec features as class embeddings provided by .
We compare our model with
f-CLSWGAN , i.e. an image feature generating framework which currently achieves the state of the art on ImageNet. We use the same evaluation protocol on all the splits.
Among the splits, 2H and 3H are the classes 2 or 3 hops
away from the 1K seen training classes of ImageNet according to the ImageNet hierarchy. M500, M1K and M5K
are the 500, 1000 and 5000 most populated classes, while
L500, L1K and L5K are the 500, 1000 and 5000 least populated classes that come from the rest of the 21K classes.
Finally, ‘All‘ denotes the remaining 20K classes.
As shown in Figure 7, our model signiﬁcantly improves
the state of the art in all splits. The accuracy improvement
is signiﬁcant especially on M500 and M1K splits, i.e. for
M500 the search space is 1.5K classes, for M1K, the search
space consists of 2K classes. For the L500, L1K and L5K
splits, there are on average only 1, 3 and 5 images per class
available . Since the test time search space in the ‘All‘
split is 22K dimensional, even a small improvement in accuracy is considered to be compelling. The achieved substantial increase in performance by CADA-VAE shows that
our 128-dim latent feature space constitutes a robust generalizable representation, surpassing the current state-of-theart image feature generating framework f-CLSWGAN.
5. Conclusion
In this work, we propose CADA-VAE, a cross-modal
embedding framework for generalized zero- and few-shot
learning. In CADA-VAE, we train a VAE for both visual
and semantic modalities. The VAE of each modality has to
jointly represent the information embodied by all modalities
in its latent space. The corresponding latent distributions
are aligned by minimizing their Wasserstein distance and
by enforcing cross-reconstruction. This procedure leaves
us with encoders that can encode features from different
modalities into one cross-modal embedding space, in which
a linear softmax classiﬁer can be trained.
We present
different variants of cross-aligned and distribution aligned
VAEs and establish new state-of-the-art results in generalized zero-shot learning for four medium-scale benchmark
datasets as well as the large-scale ImageNet. We further
show that a cross-modal embedding model for generalized
zero-shot learning achieves better performance than datagenerating methods, establishing the new state of the art.