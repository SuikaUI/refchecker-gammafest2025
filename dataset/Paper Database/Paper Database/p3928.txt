Adversarial Sampling for Active Learning
Christoph Mayer
Radu Timofte
{chmayer,timofter}@vision.ee.ethz.ch
Computer Vision Lab, ETH Z¨urich, Switzerland
This paper proposes ASAL, a new GAN based active
learning method that generates high entropy samples. Instead of directly annotating the synthetic samples, ASAL
searches similar samples from the pool and includes them
for training.
Hence, the quality of new samples is high
and annotations are reliable. To the best of our knowledge,
ASAL is the ﬁrst GAN based AL method applicable to multiclass problems that outperforms random sample selection.
Another beneﬁt of ASAL is its small run-time complexity
(sub-linear) compared to traditional uncertainty sampling
(linear). We present a comprehensive set of experiments on
multiple traditional data sets and show that ASAL outperforms similar methods and clearly exceeds the established
baseline (random sampling). In the discussion section we
analyze in which situations ASAL performs best and why it
is sometimes hard to outperform random sample selection.
1. Introduction
The goal of Active Learning (AL) algorithms is to train a
model most efﬁciently, i.e. achieving the best performance
with as few labelled samples as possible. Typical AL algorithms operate in an iterative fashion, where in each AL
cycle a query strategy selects samples that the oracle should
annotate. These samples are expected to improve the model
most effectively when added to the training set. This procedure continues until a predeﬁned stopping criteria is met.
In this paper we will mainly focus on pool based active learning, because a pool of unlabelled samples is often available beforehand or can easily be built. Furthermore, annotating all pool samples serves as an ideal evaluation environment for active learning algorithms. It enables
to train a fully-supervised model that establishes a performance upper bound on this data set. Similarly, randomly
selecting instead of actively choosing samples establishes a
lower bound. Then, the goal of an active learning algorithm
is to approximate the performance of the fully supervised
model with as few labelled samples as possible, while exceeding the performance of random sampling.
Uncertainty sampling is an effective query strategy that
identiﬁes samples that are more informative than random
ones. The heuristic is, that samples for which the model
is most uncertain contain new information and improve the
model. Uncertainty sampling is the most commonly used
AL strategy for Generative Adversarial Network (GAN)
based AL methods . However, these related methods are designed for small and very simple datasets, cover
only binary classiﬁcation tasks and use Support Vector Machines (SVMs) for classiﬁcation instead of CNNs. Generative Adversarial Active Learning (GAAL) even fails to
outperform random sample selection.
Our contributions are as follows:
• Adversarial Sampling for Active Learning (ASAL) is
to the best of our knowledge the ﬁrst pool based AL
method that uses GAN to tackle multi-class problems.
• ASAL achieves sub-linear run-time complexity even
though it searches the full pool in each AL cycle.
• We validate ASAL on full image data sets (MNIST,
CIFAR-10, CelebA, SVHN, LSUN) compared to related methods that use much simpler challenges such
as: two class subsets of MNIST, CIFAR-10 or SVHN.
2. Related Work
We review related work on AL especially on pool based
uncertainty sampling, GAN based AL strategies and methods attempting to improve the run-time complexity.
Pool-based active learning methods select new training
samples from a predeﬁned unlabelled data set . A common query strategy to identify new samples
is uncertainty sampling . A well known uncertainty
sampling strategy that is used to train SVMs, is minimum
distance sampling . Minimum distance sampling requires a linear classiﬁer in some feature space and assumes
that the classiﬁer is uncertain about samples in the vicinity of the separating hyper-plane. This strategy is mainly
used for two class but can be extended to multi-class problems . Joshi et al. use information entropy to measure the uncertainty of the classiﬁer for a particular sample.
 
Computing uncertainty with information entropy is suitable
for two or multiple classes.
Jain et al. propose two hashing based method to accelerate minimum distance sampling by selecting new samples in sub-linear time. These methods are designed to select the closest point (approximately) to a hyper-plane in
a k-dimensional feature space, where the positions of the
data points are ﬁxed but the hyper-plane is allowed to move.
Thus, these methods are limited to SVMs with ﬁxed feature
maps, because, if the feature map changes, the position of
the samples become obsolete and need to be recomputed.
Hence, the run time complexity is sub-linear for constant
feature maps and linear otherwise.
Unfortunately, CNN
based methods update their feature maps during training.
Thus, their methods are as efﬁcient as exhaustive uncertainty sampling if CNNs are involved.
Zhu and Bento propose GAAL, that uses a GAN
to generate uncertain synthetic samples in each AL cycle.
Generating instead of selecting uncertain samples leads to
a constant run-time complexity because producing a new
sample is independent of the pool size but requires training
a GAN beforehand. Zhu and Bento use the traditional
minimal distance optimization problem but replace the variable x (denoting a pool sample) with the trained generator. Then, they use gradient descent to minimize the objective. The latent variable minimizing the objective results in
a synthetic image close to the separating hyper-plane. They
annotate the synthetic sample and use it for training. Zhu
and Bento demonstrate GAAL on subsets of MNIST
and CIFAR-10 (two classes) using linear SVMs and DC-
GANs . However, GAAL performs worse than random sampling on both data sets, because it suffers from
sampling bias and annotating is arbitrarily hard caused by
sometimes poor quality of the synthetic uncertain samples.
Note, that GAAL requires visually distinct classes (horse &
automobile) to allow reliable annotations by humans.
Active Decision Boundary Annotation (ADBA) is
another GAN based AL strategy. The main contributions
of ADBA are, training the classiﬁer in the latent space and
a new annotation scheme. Hence, it requires computing
the latent state representation of each data sample using
the pretrained GAN. Then, ADBA searches the most uncertain sample in the latent space and generates a line that is
perpendicular to the current decision boundary and crosses
the most uncertain sample. Then, the GAN generates images along this line such that the annotator can specify for
which image in the sequence along this line the class label changes. ADBA shows that it outperforms uncertainty
sampling in the latent space but misses to compare to uncertainty sampling in image space. Computing the latent
space representation for each sample using a GAN is very
costly and requires high quality GANs. Sampling lines in
the latent space of multi-class problems might lead to many
crossings. Such lines might be arbitrarily hard to annotate
especially if many crossings are close and annotating a line
is more costly than annotating a single image.
Thus, we propose ASAL that reuses the sample generation idea of Zhu and Bento but we use information
entropy as uncertainty score and directly extend it to multiple classes. Our main contribution is avoiding to annotate
synthetic images by selecting the most similar samples from
the pool with a newly developed sample matching method.
We propose three different feature maps that we compute
for each pool sample to ﬁt a fast nearest neighbour model
beforehand. During active learning, we compute the feature
map of the synthetic sample and retrieve the most similar
one from the pool in sub-linear time. Additionally, ASAL
uses CNN based classiﬁers instead of linear SVMs. For the
generator we train Wasserstein GANs beforehand .
3. Proposed Adversarial Sampling for Active
ASAL allows using GANs for fast pool based active
learning by generating samples and retrieving similar real
samples from the pool, that will be labelled and added to the
training set. Fig. 1 shows the main components of the proposed ASAL. (Xk, Y k) denote the data set, used to train the
classiﬁer h with weights θk at active learning cycle k. Then,
the trained classiﬁer hθk and the generator G enable producing uncertain samples ˜x. The feature extractor F computes
features that the nearest neighbour model uses to retrieve
the most similar real samples from the pool. Finally, an oracle annotates the new real samples from the pool and adds
them to the training set. Then, the new AL cycle k+1 starts.
In the remainder of this section, we introduce the adversarial sample generation and the sample matching strategy.
3.1. Adversarial Sample Generation using GANs
Instead of selecting uncertain samples from the pool, we
follow Zhu et al. and generate such samples using a
trained GAN. Such a GAN enables to approximate the underlying data distribution of the pool. The discriminator D
ensures that the samples drawn from the generator G are
indistinguishable from real samples. At convergence, the
generator produces the function G : Rn →X that maps
the latent space variable z ∼N(0n, In) to the image domain X. The optimization problem that describes sample
generation reads as follow:
(H ◦hθk)(x)
subject to
where H(q) := −Pm
i=1 P(c = i|q) log[P(c = i|q)] and
m is the number of categories. Removing the constraint
x ∈P by including the generator simpliﬁes the problem
Lclassiﬁer
Annotation
Sample Matching
Training Classiﬁer
Adversarial Sample Generation
Figure 1: ASAL: (Xk, Y k) with (x, y) is the training set at cycle k, hθ is the classiﬁer, z the latent variable, G the generator,
˜x the synthetic samples, F the feature extractor, ˜f the features, P the pool and NN the nearest neighbour method.
but changes its solution. New samples are no longer selected from the pool but are artiﬁcially generated. We solve
the optimization problem in two steps: (i) we use the chain
rule and gradient descent to minimize the objective with respect to z and (ii) we use G to recover a synthetic sample x
from z. Thus, solving problem (1) has a constant run-time
complexity O(1) because it is independent of the pool size.
Note, that traditional uncertainty sampling achieves a linear
run-time complexity O(n) (where n = |P| is the pool size)
because it requires scanning each sample in the pool P.
3.2. Sample Matching
Sample matching compares real pool samples to generated synthetic samples in a feature space and retrieves the
closest matches. Therefore, we require a feature extractor
F : X →F, that maps data samples to a feature space, and
a distance function d : F × F →R+
0 , that computes the
distance between two data samples in this feature space.
Feature Extractors After the model generated meaningful features the task is selecting similar samples from the
pool. In order to ﬁnd suitable matches we need a feature
space where nearby samples achieve a similar entropy (if
entropy is the AL score used for sample generation). Naturally we would use the output of the (l −1) layers of a
CNN with l layers as features because entropy depends on
these features : FCLS(x) = hl−1
θk (x). Unfortunately, the
feature representation of each data sample becomes obsolete as soon as the weights θ of the CNN are updated. Thus,
using the classiﬁer to extract features for sample matching
requires recomputing the feature representation of each data
sample after each training iteration. This leads to a linear
run-time complexity O(n). Thus, we propose to use feature extractors that are independent of the current weights
θk such that we need to compute the features for each data
sample only once in a pre-processing step. A feature space
independent of the classiﬁer does not guarantee entropy
smoothness, i.e. samples with a nearby representation may
have different entropy. However, perfect matches will have
exactly the same entropy. Therefore, the closer the representations, the more likely they will score a similar entropy.
However, this requires representative features for both: the
real samples and the synthetic samples. Furthermore, we
require, that the data set is sufﬁciently dense because for a
sparse data set even the closest matches could be far away.
The image space is a simple feature space that uses the
raw values of each pixel as one feature (RGB or gray values) Fgray/rgb(x) = x. The drawback is its large number of
dimensions and that two visually close images with similar
entropy that for example differ because of background intensity, small noise component, different scaling or small
translations lead to far apart representations. Hence, we
require a feature extractor that is mostly invariant to such
perturbations. Thus, we propose to use the encoder of an
auto-encoder to extract data set speciﬁc features. Furthermore, we can train these methods to extract features that
are invariant to small perturbations in input images. We de-
ﬁne the encoder and the decoder as follows φ : X →F and
ψ : F →X and minimize P
x∈X ∥x−(φ◦ψ)(x)∥2
2 to train
the encoder and decoder. Thus, the feature extractor reads
as: Fauto(x) = φ(x). Another feature spaces is deﬁned by
the features extracted by the discriminator. Training a GAN
includes a discriminator that uses data-set speciﬁc features
to solve the task of differentiating synthetic from real samples by assigning each input sample a probability how likely
it is real or fake, D : X → . Thus, the features of the
discriminator are not only suitable for real but also for synthetic samples and are data set speciﬁc. We propose to use
the output of the (j −1) layer of a discriminator with j
layers as features: Fdisc(x) = Dj−1(x).
Efﬁcient Feature Matching In order to ﬁnd the best
match we extract the features of the synthetic sample.
Then, we retrieve the pool sample that has the most similar feature representation with respect to the distance
function d.
Sample matching reads as follows:
arg minx∈X d(F(x), F(˜x)), where ˜x is a synthetic sample.
We propose to use the euclidean distance function
d(f1, f2) = ∥f1 −f2∥2. This problem is equivalent to ﬁnding the nearest-neighbour of the synthetic sample in feature
Algorithm 1: ASAL
Input: Initialize the set X, Y by adding random pool
samples to X0 and their labels to Y 0. Train the
generator G and the feature extractor F.
Precompute the PCA, µ and the set
S = {FPCA(x) | x ∈X}.
Result: Trained Classiﬁer hθk
1. Train classiﬁer hθk to minimize empirical risk
(x,y)∈(Xk,Y k) l(hθk(x), y).
2. Generate synthetic samples ˜x with high
entropy by solving Eq. (1).
3. Compute the feature representations ˜f of the
generated samples: ˜f = FPCA(˜x)
4. Retrieve real samples x that match ˜x
x = {pi ∈P | i = arg minf∈S d(f, ˜f)}.
5. Annotate the samples x with labels y.
6. Update the sets Xk+1 = Xk ∪{x},
Y k+1 = Y k ∪{y}.
until Labelling budget is exhausted;
space. Therefore, we use multi-dimensional binary search
trees (k-d-trees) for efﬁcient nearest neighbour selection
because their run-time complexity to search a nearest neighbour is sub-linear O(log n) with respect to the pool size
n = |P|. However, the run time depends on the number
of dimension of the feature space. Therefore, we achieve
fast feature matching if dim(F) ≪dim(X). A property
of auto-encoders is that they allow to compress samples
from a high dimensional input spaces into a much lower
dimensional latent space such that they enable fast sample matching. However, all other discussed feature spaces
have typically a similar number of dimensions as the image space.
Therefore, we propose Principal Component
Analysis (PCA) to reduce the number of dimensions and
to produce fewer features that contain most of the variance:
FPCA(x) = PCA(F(x) −µ), where µ =
and F is one of the previously introduced feature extractors.
Then, the full sample matching reads as
i = arg min
d(f, FPCA(˜x)
where S = {FPCA(x) | x ∈X} can be precomputed before
starting AL. We compress the features independent of the
extractor to the same number of dimensions using PCA to
ensure very similar absolute run-times of the nearest neighn.a.
Autoencoder
Discriminator
Figure 2: The rows show either generated or matched samples using different feature sets for MNIST - ten classes.
The brackets denote (label id / sample id).
bour method. Alg. 1 shows a detailed description of the
different steps of ASAL and Fig. 2 shows examples of synthetic samples with high entropy and their closest matches
using the proposed matching with different features. We
show more examples in the supplementary.
4. Experiments
4.1. Datasets
For the experiments we use ﬁve datasets: MNIST ,
CIFAR-10 , CelebA , SVHN and LSUN
Scenes . The MNIST data set contains ten digits unevenly distributed. Each image has a resolution of 28 × 28
gray-scale pixels. The data set consists of 50k training, 10k
validation and 10k testing samples. CIFAR-10 consists of
50k training and 10k validation 32 × 32 color images with
uniformly distributed label categories. We use the validation set for testing. CelebA consists of roughly 160k training, 20k validation and 20k testing 64 × 64 color images
and a list specifying the presence or absence of 40 face attributes for each image. SVHN consists of 73k training, 26k
testing and 531k extra 32 × 32 color images with unevenly
distributed label categories. We use the training and extra
images to build the pool for AL. LSUN Scenes consists of
roughly 10M training images with unevenly distributed labels. We split it into training and testing sets and centrally
crop all the images to 64 × 64 color images.
For a fair comparison to GAAL, we follow Zhu and
Bento and construct the same binary data sets, consisting of the MNIST digits 5 & 7 and the CIFAR-10 classes
automobile & horse. In addition we validate ADBA on the
same MNIST data set. Furthermore, we validate ASAL on
the full MNIST, CIFAR-10, SVHN and LSUN data sets and
use four face attributes to build four different CelebA classiﬁcation benchmarks. Each benchmark contains all 200k
images, labelled according to the presence or absence of
the attribute: Blond Hair, Wearing Hat, Bangs, Eyeglasses.
Table 1: Summary of all experiments. We run the experiment on CelebA four times but consider a different face attribute
each time. Budget denotes the maximum amount of samples in the AL data set, New denotes the number of newly labelled
samples in each AL cycle and Initial denotes the number of samples in the data set when AL begins. These three number
allow computing the number of AL cycles.
Train Size
Feature matching
Gray/Disc/Auto
RGB/Disc/Auto
Gray/Disc/Auto
RGB/Disc/Auto
Auto-Encoder
Auto-Encoder
Auto-Encoder
4.2. Experimental Settings
First, we produce different references to assess the performance of ASAL. (i) Maximum-entropy sampling (upper bound) because ASAL tries to approximate this strategy in sub-linear run-time complexity. (ii) Random sampling (lower bound, baseline) and (iii) the fully supervised
model (upper bound). In addition we report for a subset of
the experiments the results of Core-set based AL (MNIST
& SVHN). We examine three different versions of ASAL
using the previously introduced set of features:
Gray/RGB, ASAL-Autoencoder, and ASAL-Discriminator.
For some settings we compare to ASAL-CLS-Features that
uses the classiﬁer features for matching. We reduce the dimension of the feature space to 50 using PCA. We experimentally veriﬁed that more dimensions only increase the
run-time but lead to similar accuracy. Fig. 3d shows the test
accuracy of ASAL-Autoencoder with three different number of PCA dimensions. To synthesize new samples we use
Adam and apply 100 gradient steps to maximize the entropy with respect to the latent space variable, see Eq. (1).
We directly optimize for multiple latent space variables at
the same time by embedding them in one batch. We always
draw samples from the pool without replacement. We do
not use data augmentation for any experiment except LSUN
and train all models from scratch in each AL cycle. We run
all experiments for ﬁve different runs with different random
seeds and report the mean (solid line) except the computationally demanding experiments on CIFAR-10 - ten classes
SVHN and LSUN, that we run for three random seeds. The
shaded areas correspond to the maximum and minimum
value for each operating point considering all random seeds.
Please refer to the supplementary for the model architectures (classiﬁers, auto-encoders, and GANs), the training
strategies and parameters. Tab. 1 summarizes all experimental setups. For the linear models (h(x) = Wx + b)
we use directly the raw pixel values as input features for
the classiﬁer. We use Wasserstein GANs with gradient penalty and add a consistency term for CIFAR-
10 - ten classes because it produces synthetic samples with
higher visual quality . Note, that we use the same setup
for CelebA for four different experiments but only change
the target classiﬁcation labels.
5. Results
5.1. Linear Models
ASAL outperforms random sampling and approaches
maximum entropy sampling quickly on MNIST - two
classes. We observe, that all three proposed feature spaces
for sample matching perform equally well.
strategies reach a classiﬁcation accuracy of 98.5% with only
200 labelled samples, whereas random sampling requires
500 labelled samples, see Fig. 3a.
On CIFAR-10 - two classes only ASAL-Autoencoder exceeds the performance of random sampling. However, using auto-encoder features for sample matching reaches the
classiﬁcation accuracy of maximum entropy sampling already with 500 labelled samples, whereas random sampling requires approximately twice the amount of samples
to reach a comparable performance, see Fig. 4a.
5.2. Convolutional Neural Networks
ASAL clearly outperforms random sampling on MNIST
- ten classes. In contrast to the binary setting we used a
CNN where the weights and thererfore the extracted features change in each AL cycle. Nonetheless, all three feature spaces, used for sample matching, exceed the performance of random sampling. However, the discriminator features lead to the highest classiﬁcation accuracy, see
Fig. 3c. Furthermore, we observe that ASAL-discriminator
achieves almost similar test accuracy as core-set based AL
but at a smaller run-time complexity. Unfortunately, ASAL
performs similar to random sampling on CIFAR-10 - ten
classes independent of feature space used for sample matching but classical uncertainty sampling exceeds the performance of random sampling, see Fig 4b. The four experiments using different target labels on CelebA emphasize,
that ASAL outperforms random sampling and approaches
number of samples
Fully Supervised
Maximal Entropy
(a) Accuracy (MNIST - two classes)
number of samples
mean entropy
Maximal Entropy
ASAL-Autoencoder
ASAL-Discriminator
(b) Entropy of new samples.
number of samples
Fully Supervised
Maximal Entropy
ASAL-CLS-Feats
(c) Accuracy (MNIST - ten classes)
number of samples
Fully Supervised
Maximal Entropy
ASAL: PCA-5
ASAL: PCA-50
ASAL: PCA-500
(d) Small ablation study for PCA.
Figure 3: Test accuracy and entropy for different methods, data sets and benchmarks.
number of samples
Fully Supervised
Maximal Entropy
ASAL-Autoencoder
ASAL-Discriminator
GAAL: WGAN-GP
(a) CIFAR-10 - two classes
number of samples
Fully Supervised
Maximal Entropy
ASAL-Autoencoder
ASAL-Discriminator
ASAL-CLS-Features
(b) CIFAR-10 - ten classes.
10k 15k 20k 25k 30k 35k 40k 45k 50k
number of samples
Fully Supervised
Maximal Entropy
(c) SVHN - ten classes
number of samples
Fully Supervised: 89.52 ± 0.11
Maximal Entropy
(d) LSUN - ten classes
Figure 4: Comparison of classiﬁcation accuracy between different methods on four different benchmarks.
uncertainty sampling with a better run-time complexity.
However, for CelebA - Eyeglasses ASAL performs only
marginally better than random sampling. ASAL exceeds
random sampling during the ﬁrst cycles but equals its performance when using more than 750 labelled samples, see
Fig. 5. The results on SVHN in Fig 4c show that ASAL
outperforms random sampling but achieves lower test accuracy than the more costly core-set based AL and uncertainty
sampling. Similarly, Fig. 4d shows that ASAL outperforms
random sampling on LSUN. We omit the comparison with
core-set based AL on LSUN because it is demanding with
respect to memory and leads to an even higher run time
than uncertainty sampling. To summarize, ASAL outperforms random sampling on eight out of ten benchmarks.
On CelebA - Blond Hair and CIFAR-10 - two classes ASAL
achieves almost the same performance as maximum entropy
sampling. We will analyze the successful and the failure
cases in Sec. 5.4 and give intuition when ASAL works.
5.3. Comparison between ASAL, GAAL and ADBA
The most similar method to ASAL is GAAL . Even
though Zhu & Bento report that GAAL clearly performs worse than random sampling, we reproduced their results. For fairer comparison we replace their DCGAN with
our Wasserstein GAN that we also use for ASAL and generate images with higher quality. Fig. 3a shows that GAAL
achieves a higher accuracy than random sampling and an
accuracy almost as high as ASAL at the beginning of AL.
However, after adding more than 350 samples, the classi-
ﬁcation accuracy does not even saturate but drops and approaches the quality of random sampling. The reason for
this decrease are generated samples where identifying the
correct labels is very difﬁcult such that the annotations get
unreliable. Nonetheless, Fig. 2 shows that labelling synthetic samples is possible and therefore GAAL can work.
Furthermore, we implement ADBA and use again the same
GAN for sample and line generation. ADBA requires labeling transition points in generated lines of samples such that
directly comparing the labeling effort is difﬁcult. We count
annotating one line as one annotation. Annotating one line
leads to eleven labeled synthetic images. Thus, 500 samples
in Fig. 3a correspond to 5500 labeled synthetic samples.
The classiﬁer is trained in the latent instead of the image
space and requires much more annotations to achieve competitive results. Thus, we conclude that ADBA achieves
worse performance than all other methods and is limited
to binary classiﬁcation with linear models in latent space.
Hence, we omit further comparison with ADBA.
We reproduce GAAL on CIFAR-10 - two classes and observe that reliably labelling uncertain synthetic images is
number of samples
Fully Supervised
Maximal Entropy
ASAL-Autoencoder
(a) Wearing Hat
number of samples
Fully Supervised
Maximal Entropy
ASAL-Autoencoder
(b) Blond Hair
number of samples
Fully Supervised
Maximal Entropy
ASAL-Autoencoder
number of samples
Fully Supervised
Maximal Entropy
ASAL-Autoencoder
ASAL-CLS-Features
(d) Eyeglasses
Figure 5: Test accuracy on four CelebA benchmarks. The target classes correspond to absence or presence of face attributes.
very difﬁcult even with a state-of-the-art GAN. Fig. 4a reports the performance of GAAL on CIFAR-10 - two classes.
We observe that GAAL performs clearly worse than random
sampling and ASAL. We run GAAL only up to 400 labelled
samples because the trend is clear and because manually labelling synthetic images is very costly and tedious. Thus,
we conclude, that ASAL outperforms GAAL and ADBA.
5.4. Discussion
When designing ASAL we make several assumptions
and approximations to enable a sub-linear run-time complexity. In this section we analyze the experiments of ASAL
and investigate for the failure cases which of these assumptions hold and which do not. We assume that: (i) the GAN
can generate synthetic images with high entropy that match
the true data distribution, (ii) the data set is sufﬁciently
large, such that there exists always a real sample that is
close to each synthetic image, (iii) there exists a ﬁxed feature space (independent of the classiﬁer), where nearby representations have a similar entropy.
Fig. 3b shows that all three ASAL strategies retrieve
on average samples with 63% higher entropy than random
sampling on MNIST - two classes. We conclude that for this
data set all assumptions hold. Especially the relatively large
and dense data set with 10k samples that cover many variations of the digits enables reliable sample matching and
leads to a well trained GAN. In Sec. 3.2 we described the
feature extractor FCLS that uses directly the CNN features.
This feature space guarantees entropy smoothness such that
nearby representations share a similar entropy. Using the
best feature extractor FCLS increases the run-time but avoids
assumption (iii). Thus, if ASAL works only when using
FCLS we require a different feature extractor than the proposed. Furthermore, if ASAL fails with FCLS features it
indicates that the data set is too small such that training
the GAN to generate uncertain samples with realistic features and matching is unfeasible. Indeed, Fig. 3c shows
that ASAL on MNIST - ten classes using FCLS approaches
quickly the quality of maximum entropy sampling and veriﬁes that FCLS performs better than ﬁxed features. It shows
that using a non-optimal feature extractor reduces the performance but veriﬁes that sample matching with the data set
and synthetic samples works. We redo the same experiment
on CIFAR-10 - ten classes and observe that using FCLS only
marginally exceeds the quality of random sampling. Therefore, our proposed sample matching and feature spaces are
not the reason why ASAL fails but rather the small data
set size. Hence, we expect that either the GAN generates
synthetic samples with unrealistic characteristics or that the
generated samples would be useful but close matches are
missing in the data set.
We redo the same experiments
for the benchmark CelebA - Eyeglasses where ASAL fails.
However, we already veriﬁed that ASAL works on three
benchmarks on this data set and know that the quality of the
synthetic uncertain images is sufﬁcient. Fig. 5d shows that
FCLS achieves the same performance as maximum entropy
sampling. Hence, the performance drop is caused by using
the ﬁxed instead of the varying features. Furthermore, the
amount of images in the data set, that contain eyeglasses
is very small such that the synthetic image might contain a
face with an eyeglass and the matching retrieves a very similar face without eyeglasses. The issue is that the proposed
feature extractor concentrates on many face attributes for
matching but uncertainty depends only on a small subset.
SVHN is a less diverse data set than CelebA and CIFAR-
10 but contains many more samples. Thus, the quality of
generated samples is high and similar matching retrieves
meaningful samples.
Hence, all three assumptions hold.
LSUN is the biggest tested data set in this paper but training
a GAN to generate high quality samples is still challenging.
Nonetheless, ASAL outperforms random sampling. Thus,
the GAN is able to generate samples with features that help
training and that are present in the data set too. Furthermore, matching is able to select similar samples because
LSUN contains on average 1M samples per class. Thus, we
conclude that ASAL can work with lower quality synthetic
samples as long as they contain meaningful characteristics
because we label matched real instead of generated samples.
Data set size
Selection time per AL cyle
Maximal Entropy
(a) Timings for sample selection
Data set size
Number of AL cycles
more eﬃcient than
Maximal Entropy
Maximal Entropy
more eﬃcient than
(b) Transition point
Figure 6: Run-time of uncertainty sampling and ASAL to
select 10 samples with respect to the data set size. The transition point denotes the number of AL cycles when ASAL
gets more efﬁcient than maximum entropy sampling
5.5. Timings
In this section we report timings on CelebA and LSUN.
For CelebA we ﬁrst concentrate on the run time of one AL
cycle with respect to different data set sizes. Next, we report the transition point after how many AL cycles ASAL
gets more efﬁcient than uncertainty sampling in case preprocessing time is taken into account. Finally, we report
the run time of ASAL and other AL methods on LSUN including I/O-time. All measurements omit classiﬁer training
time because it is equivalent for all AL methods. We use a
Nvidia TITAN X GPU and an Intel Xeon CPU E5-2680 v4.
Fig. 6a reports the time required to select ten new samples in each AL cycle with respect to data set size. We
randomly augmented the original data set (160k) to create
larger data sets containing up to 16M samples. Whereas it
is possible to keep all images in memory for 160k (1.98GB)
this is hardly possible for 16M images (198GB). For experiments on CelebA we omit including I/O-time. The sample
matching proposed in ASAL stores only 50 ﬂoat features
per image (32MB for 160k and 3.2GB for 16M images).
This saving enables to keep the features in memory and to
build the nearest-neighbor model even for huge data sets.
ASAL has a sub-linear run-time complexity to select
new samples. However, it requires several pre-processing
steps such as training the GAN (∼25h) and auto-encoder
(∼1.6h), extracting the features (∼32s per 160k samples) and ﬁtting the nearest-neighbor model (∼5min for
16M samples).
The sample selection time is ∼44s for
16M. Conversely, maximum entropy sampling avoids any
pre-processing cost but has a much higher sample selection time: ∼53min for 16M. Fig. 6a shows that ASAL
is much faster than uncertainty sampling but requires preprocessing. However, the time savings for ASAL in each
AL cycle is large and allows to compensate for the initial pre-computation time when running ASAL for sufﬁciently many AL cycles. Fig. 6b shows the transition point,
Table 2: Run-time complexity and runtime for one AL cycle
on LSUN including I/O-time (n = |P| refers to the pool
size and k = |Xk| to the number of labeled samples).
Feature Extraction
Sample Selection
Maximal-Entropy
Learning-Loss 
Core-Set 
ASAL-Auto (ours)
the point where ASAL achieves a higher efﬁciency than
maximum entropy sampling depending on the data set size
and number of AL cycles. Note, that the sample selection
time for uncertainty sampling is independent of the number of selected samples but the run time of ASAL increases
when selecting more samples. However, the sample selection time for ASAL is still much smaller than for uncertainty sampling even when querying much more samples.
The reason is that we can generate many artiﬁcial samples
within one batch at once. Note that selecting fewer samples
reduces the risk of correlated uncertain samples.
Tab. 2 reports timings for AL methods including I/O
time. We measure the time for ASAL, random and uncertainty sampling. For the other methods we predict the run
time based on previous measurements: Learning-Loss 
requires propagating each sample through the network each
AL cycle and computing the learned loss. Hence, the runtime complexity is linear and the run time is similar to maximal entropy sampling. Similarly, Core-Set based AL 
requires extracting the features for each sample every AL
cycle to select new core samples. Note that in each AL
cycle sample selection takes longer than training a classiﬁer
with less than 15k samples for methods with linear run time.
6. Conclusion
We proposed a new pool-based AL sampling method that
uses sample synthesis and matching to achieve a sub-linear
run-time complexity. We demonstrated, that ASAL outperforms random sampling on eight out of ten benchmarks. We
analyzed the failure cases and conclude, that the success of
ASAL depends on the structure and size of the data set and
the consequential quality of generated images and matches.
ASAL works exactly on the large data sets where it is most
needed. There the sub-linear run-time compensates quickly
for any pre-processing. ASAL is suitable for interactive
AL where pre-processing is acceptable but small sampling
times are required. In future research we propose to test
ASAL for other AL scores such as the Learned-Loss 
to accelerate their run time. Although auto encoder features
work well for natural images we suggest to study VGG 
or AlexNet feature in future research.