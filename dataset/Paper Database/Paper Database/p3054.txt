Cross-stitch Networks for Multi-task Learning
Ishan Misra‚àó
Abhinav Shrivastava‚àó
Abhinav Gupta
Martial Hebert
The Robotics Institute, Carnegie Mellon University
Multi-task learning in Convolutional Networks has displayed remarkable success in the Ô¨Åeld of recognition. This
success can be largely attributed to learning shared representations from multiple supervisory tasks. However, existing multi-task approaches rely on enumerating multiple network architectures speciÔ¨Åc to the tasks at hand, that do not
generalize. In this paper, we propose a principled approach
to learn shared representations in ConvNets using multitask learning. SpeciÔ¨Åcally, we propose a new sharing unit:
‚Äúcross-stitch‚Äù unit.
These units combine the activations
from multiple networks and can be trained end-to-end. A
network with cross-stitch units can learn an optimal combination of shared and task-speciÔ¨Åc representations. Our proposed method generalizes across multiple tasks and shows
dramatically improved performance over baseline methods
for categories with few training examples.
1. Introduction
Over the last few years, ConvNets have given huge performance boosts in recognition tasks ranging from classiÔ¨Åcation and detection to segmentation and even surface
normal estimation. One of the reasons for this success is
attributed to the inbuilt sharing mechanism, which allows
ConvNets to learn representations shared across different
categories.
This insight naturally extends to sharing between tasks (see Figure 1) and leads to further performance
improvements, e.g., the gains in segmentation and detection . A key takeaway from these works is that
multiple tasks, and thus multiple types of supervision, helps
achieve better performance with the same input. But unfortunately, the network architectures used by them for multitask learning notably differ. There are no insights or principles for how one should choose ConvNet architectures for
multi-task learning.
1.1. Multi-task sharing: an empirical study
How should one pick the right architecture for multi-task
learning? Does it depend on the Ô¨Ånal tasks? Should we
‚àóBoth authors contributed equally
Attributes
Has saddle
Object location
Surface Normals
Sem. Segmentation
Surface orientation
Pixel labels
Figure 1: Given an input image, one can leverage multiple related properties to improve performance by using a
multi-task learning framework. In this paper, we propose
cross-stitch units, a principled way to use such a multi-task
framework for ConvNets.
have a completely shared representation between tasks? Or
should we have a combination of shared and task-speciÔ¨Åc
representations?
Is there a principled way of answering
these questions?
To investigate these questions, we Ô¨Årst perform extensive experimental analysis to understand the performance
trade-offs amongst different combinations of shared and
task-speciÔ¨Åc representations. Consider a simple experiment
where we train a ConvNet on two related tasks (e.g., semantic segmentation and surface normal estimation). Depending on the amount of sharing one wants to enforce, there
is a spectrum of possible network architectures. Figure 2(a)
shows different ways of creating such network architectures
based on AlexNet . On one end of the spectrum is a
fully shared representation where all layers, from the Ô¨Årst
convolution (conv2) to the last fully-connected (fc7), are
shared and only the last layers (two fc8s) are task speciÔ¨Åc. An example of such sharing is where separate
fc8 layers are used for classiÔ¨Åcation and bounding box regression. On the other end of the sharing spectrum, we can
train two networks separately for each task and there is no
cross-talk between them. In practice, different amount of
sharing tends to work best for different tasks.
 
Generic Network
All Parameters Shared
Specific Network
No Parameters Shared
Reducing sharing between tasks
Shared Layers
Task A layers
Task B layers
Attributes Classification (mAP)
Object Detection (mAP)
Surface Normal (Median Error)
Semantic Segmentation (mean IU)
Difference
and Specific
(Ssùëùùëôùëñùë°‚àíSspecific)
Split conv5
Split conv4
Split conv3
Split conv2
Figure 2: We train a variety of multi-task (two-task) architectures by splitting at different layers in a ConvNet for two
pairs of tasks. For each of these networks, we plot their performance on each task relative to the task-speciÔ¨Åc network. We
notice that the best performing multi-task architecture depends on the individual tasks and does not transfer across different
pairs of tasks.
So given a pair of tasks, how should one pick a network
architecture? To empirically study this question, we pick
two varied pairs of tasks:
‚Ä¢ We Ô¨Årst pair semantic segmentation (SemSeg) and surface normal prediction (SN). We believe the two tasks are
closely related to each other since segmentation boundaries also correspond to surface normal boundaries. For
this pair of tasks, we use NYU-v2 dataset.
‚Ä¢ For our second pair of tasks we use detection (Det) and
Attribute prediction (Attr). Again we believe that two
tasks are related: for example, a box labeled as ‚Äúcar‚Äù
would also be a positive example of ‚Äúhas wheel‚Äù attribute. For this experiment, we use the attribute PAS-
CAL dataset .
We exhaustively enumerate all the possible Split architectures as shown in Figure 2(a) for these two pairs of tasks
and show their respective performance in Figure 2(b). The
best performance for both the SemSeg and SN tasks is using
the ‚ÄúSplit conv4‚Äù architecture (splitting at conv4), while
for the Det task it is using the Split conv2, and for Attr with
Split fc6. These results indicate two things ‚Äì 1) Networks
learned in a multi-task fashion have an edge over networks
trained with one task; and 2) The best Split architecture for
multi-task learning depends on the tasks at hand.
While the gain from multi-task learning is encouraging,
getting the most out of it is still cumbersome in practice.
This is largely due to the task dependent nature of picking
architectures and the lack of a principled way of exploring
them. Additionally, enumerating all possible architectures
for each set of tasks is impractical. This paper proposes
cross-stitch units, using which a single network can capture
all these Split-architectures (and more). It automatically
learns an optimal combination of shared and task-speciÔ¨Åc
representations. We demonstrate that such a cross-stitched
network can achieve better performance than the networks
found by brute-force enumeration and search.
2. Related Work
Generic Multi-task learning has a rich history in
machine learning. The term multi-task learning (MTL) itself has been broadly used as an
umbrella term to include representation learning and selection , transfer learning etc.
and their widespread applications in other Ô¨Åelds, such as
genomics , natural language processing and
computer vision . In fact, many
times multi-task learning is implicitly used without reference; a good example being Ô¨Åne-tuning or transfer learning , now a mainstay in computer vision, can be viewed
as sequential multi-task learning . Given the broad scope,
in this section we focus only on multi-task learning in the
context of ConvNets used in computer vision.
Multi-task learning is generally used with ConvNets in
computer vision to model related tasks jointly, e.g. pose estimation and action recognition , surface normals and
edge labels , face landmark detection and face detection , auxiliary tasks in detection , related
Cross-stitch unit
Activation Maps
Activation Maps
Figure 3: We model shared representations by learning a
linear combination of input activation maps. At each layer
of the network, we learn such a linear combination of the
activation maps from both the tasks. The next layers‚Äô Ô¨Ålters
operate on this shared representation.
classes for image classiÔ¨Åcation etc.
Usually these
methods share some features (layers in ConvNets) amongst
tasks and have some task-speciÔ¨Åc features. This sharing or
split-architecture (as explained in Section 1.1) is decided
after experimenting with splits at multiple layers and picking the best one. Of course, depending on the task at hand,
a different Split architecture tends to work best, and thus
given new tasks, new split architectures need to be explored.
In this paper, we propose cross-stitch units as a principled
approach to explore and embody such Split architectures,
without having to train all of them.
In order to demonstrate the robustness and effectiveness
of cross-stitch units in multi-task learning, we choose varied tasks on multiple datasets. In particular, we select four
well established and diverse tasks on different types of image datasets: 1) We pair semantic segmentation 
and surface normal estimation , both of which
require predictions over all pixels, on the NYU-v2 indoor
dataset . These two tasks capture both semantic and
geometric information about the scene.
2) We choose
the task of object detection and attribute
prediction on web-images from the PASCAL
dataset . These tasks make predictions about localized regions of an image.
3. Cross-stitch Networks
In this paper, we present a novel approach to multitask learning for ConvNets by proposing cross-stitch units.
Cross-stitch units try to Ô¨Ånd the best shared representations
for multi-task learning. They model these shared representations using linear combinations, and learn the optimal linear combinations for a given set of tasks. We integrate these
cross-stitch units into a ConvNet and provide an end-to-end
learning framework. We use detailed ablative studies to better understand these units and their training procedure. Further, we demonstrate the effectiveness of these units for two
different pairs of tasks. To limit the scope of this paper, we
only consider tasks which take the same single input, e.g.,
an image as opposed to say an image and a depth-map .
3.1. Split Architectures
Given a single input image with multiple labels, one can
design ‚ÄúSplit architectures‚Äù as shown in Figure 2. These
architectures have both a shared representation and a task
speciÔ¨Åc representation.
‚ÄòSplitting‚Äô a network at a lower
layer allows for more task-speciÔ¨Åc and fewer shared layers. One extreme of Split architectures is splitting at the
lowest convolution layer which results in two separate networks altogether, and thus only task-speciÔ¨Åc representations. The other extreme is using ‚Äúsibling‚Äù prediction layers (as in ), which allows for a more shared representation. Thus, Split architectures allow for a varying amount
of shared and task-speciÔ¨Åc representations.
3.2. Unifying Split Architectures
Given that Split architectures hold promise for multi-task
learning, an obvious question is ‚Äì At which layer of the
network should one split? This decision is highly dependent
on the input data and tasks at hand. Rather than enumerating
the possibilities of Split architectures for every new input
task, we propose a simple architecture that can learn how
much shared and task speciÔ¨Åc representation to use.
3.3. Cross-stitch units
Consider a case of multi task learning with two tasks A
and B on the same input image. For the sake of explanation,
consider two networks that have been trained separately for
these tasks. We propose a new unit, cross-stitch unit, that
combines these two networks into a multi-task network in
a way such that the tasks supervise how much sharing is
needed, as illustrated in Figure 3. At each layer of the network, we model sharing of representations by learning a linear combination of the activation maps using a crossstitch unit. Given two activation maps xA, xB from layer
l for both the tasks, we learn linear combinations ÀúxA, ÀúxB
(Eq 1) of both the input activations and feed these combinations as input to the next layers‚Äô Ô¨Ålters. This linear combination is parameterized using Œ±. SpeciÔ¨Åcally, at location
(i, j) in the activation map,
We refer to this the cross-stitch operation, and the unit that
models it for each layer l as the cross-stitch unit. The network can decide to make certain layers task speciÔ¨Åc by setting Œ±AB or Œ±BA to zero, or choose a more shared representation by assigning a higher value to them.
Backpropagating
cross-stitch
cross-stitch units are modeled as linear combination, their
partial derivatives for loss L with tasks A, B are computed
We denote Œ±AB, Œ±BA by Œ±D and call them the differenttask values because they weigh the activations of another
task. Likewise, Œ±AA, Œ±BB are denoted by Œ±S, the same-task
values, since they weigh the activations of the same task.
By varying Œ±D and Œ±S values, the unit can freely move between shared and task-speciÔ¨Åc representations, and choose
a middle ground if needed.
4. Design decisions for cross-stitching
We use the cross-stitch unit for multi-task learning in
ConvNets. For the sake of simplicity, we assume multi-task
learning with two tasks. Figure 4 shows this architecture
for two tasks A and B. The sub-network in Figure 4(top)
gets direct supervision from task A and indirect supervision
(through cross-stitch units) from task B. We call the subnetwork that gets direct supervision from task A as network
A, and correspondingly the other as B. Cross-stitch units
help regularize both tasks by learning and enforcing shared
representations by combining activation (feature) maps. As
we show in our experiments, in the case where one task
has less labels than the other, such regularization helps the
‚Äúdata-starved‚Äù tasks.
Next, we enumerate the design decisions when using
cross-stitch units with networks, and in later sections perform ablative studies on each of them.
Cross-stitch units initialization and learning rates: The
Œ± values of a cross-stitch unit model linear combinations
of feature maps. Their initialization in the range is
important for stable learning, as it ensures that values in
the output activation map (after cross-stitch unit) are of the
same order of magnitude as the input values before linear
combination. We study the impact of different initializations and learning rates for cross-stitch units in Section 5.
Network initialization:
Cross-stitch units combine together two networks as shown in Figure 4. However, an
obvious question is ‚Äì how should one initialize the networks
A and B? We can initialize networks A and B by networks
that were trained on these tasks separately, or have the same
initialization and train them jointly.
conv1, pool1
conv2, pool2
Cross-stitch
conv5, pool5
Figure 4: Using cross-stitch units to stitch two AlexNet 
networks. In this case, we apply cross-stitch units only after pooling layers and fully connected layers. Cross-stitch
units can model shared representations as a linear combination of input activation maps. This network tries to learn
representations that can help with both tasks A and B. We
call the sub-network that gets direct supervision from task
A as network A (top) and the other as network B (bottom).
5. Ablative analysis
We now describe the experimental setup in detail, which
is common throughout the ablation studies.
Datasets and Tasks: For ablative analysis we consider the
tasks of semantic segmentation (SemSeg) and Surface Normal Prediction (SN) on the NYU-v2 dataset. We use
the standard train/test splits from . For semantic segmentation, we follow the setup from and evaluate on
the 40 classes using the standard metrics from their work
Setup for Surface Normal Prediction: Following , we
cast the problem of surface normal prediction as classiÔ¨Åcation into one of 20 categories. For evaluation, we convert the model predictions to 3D surface normals and apply
the Manhattan-World post-processing following the method
We evaluate all our methods using the metrics
from . These metrics measure the error in the ground
truth normals and the predicted normals in terms of their
angular distance (measured in degrees). SpeciÔ¨Åcally, they
measure the mean and median error in angular distance,
in which case lower error is better (denoted by ‚ÄòMean‚Äô
and ‚ÄòMedian‚Äô error). They also report percentage of pixels which have their angular distance under a threshold (denoted by ‚ÄòWithin t‚ó¶‚Äô at a threshold of 11.25‚ó¶, 22.5‚ó¶, 30‚ó¶), in
which case a higher number indicates better performance.
For semantic segmentation (SemSeg) and
surface normal (SN) prediction,
we use the Fully-
Convolutional Network (FCN 32-s) architecture from 
based on CaffeNet (essentially AlexNet ). For both
the tasks of SemSeg and SN, we use RGB images at full
resolution, and use mirroring and color data augmentation.
We then Ô¨Ånetune the network (referred to as one-task network) from ImageNet for each task using hyperparame-
Table 1: Initializing cross-stitch units with different Œ± values, each corresponding to a convex combination. Higher
values for Œ±S indicate that we bias the cross-stitch unit to
prefer task speciÔ¨Åc representations. The cross-stitched network is robust across different initializations of the units.
Surface Normal
Segmentation
Angle Distance
(Lower Better)
(Higher Better)
(Higher Better)
(Œ±S, Œ±D) Mean Med.
(0.1, 0.9)
(0.5, 0.5)
(0.7, 0.3)
(0.9, 0.1)
ters reported in . We Ô¨Åne-tune the network for semantic segmentation for 25k iterations using SGD (mini-batch
size 20) and for surface normal prediction for 15k iterations
(mini-batch size 20) as they gave the best performance, and
further training (up to 40k iterations) showed no improvement. These one-task networks serve as our baselines and
initializations for cross-stitching, when applicable.
Cross-stitching: We combine two AlexNet architectures
using the cross-stitch units as shown in Figure 4. We experimented with applying cross-stitch units after every convolution activation map and after every pooling activation
map, and found the latter performed better. Thus, the crossstitch units for AlexNet are applied on the activation maps
for pool1, pool2, pool5, fc6 and fc7. We maintain
one cross-stitch unit per ‚Äòchannel‚Äô of the activation map,
e.g., for pool1 we have 96 cross-stitch units.
5.1. Initializing parameters of cross-stitch units
Cross-stitch units capture the intuition that shared representations can be modeled by linear combinations .
To ensure that values after the cross-stitch operation are of
the same order of magnitude as the input values, an obvious
initialization of the unit is that the Œ± values form a convex linear combination, i.e., the different-task Œ±D and the
same-task Œ±S to sum to one. Note that this convexity is
not enforced on the Œ± values in either Equation 1 or 2, but
serves as a reasonable initialization. For this experiment,
we initialize the networks A and B with one-task networks
that were Ô¨Åne-tuned on the respective tasks. Table 1 shows
the results of evaluating cross-stitch networks for different
initializations of Œ± values.
5.2. Learning rates for cross-stitch units
We initialize the Œ± values of the cross-stitch units in the
range [0.1, 0.9], which is about one to two orders of magnitude larger than the typical range of layer parameters in
AlexNet . While training, we found that the gradient
updates at various layers had magnitudes which were rea-
Table 2: Scaling the learning rate of cross-stitch units wrt.
the base network. Since the cross-stitch units are initialized
in a different range from the layer parameters, we scale their
learning rate for better training.
Surface Normal
Segmentation
Angle Distance
(Lower Better)
(Higher Better)
(Higher Better)
sonable for updating the layer parameters, but too small for
the cross-stitch units. Thus, we use higher learning rates
for the cross-stitch units than the base network. In practice,
this leads to faster convergence and better performance. To
study the impact of different learning rates, we again use
a cross-stitched network initialized with two one-task networks. We scale the learning rates (wrt. the network‚Äôs learning rate) of cross-stitch units in powers of 10 (by setting the
lr mult layer parameter in Caffe ). Table 2 shows the
results of using different learning rates for the cross-stitch
units after training for 10k iterations. Setting a higher scale
for the learning rate improves performance, with the best
range for the scale being 102 ‚àí103. We observed that setting the scale to an even higher value made the loss diverge.
5.3. Initialization of networks A and B
When cross-stitching two networks, how should one initialize the networks A and B? Should one start with task
speciÔ¨Åc one-task networks (Ô¨Åne-tuned for one task only)
and add cross-stitch units? Or should one start with networks that have not been Ô¨Åne-tuned for the tasks?
explore the effect of both choices by initializing using
two one-task networks and two networks trained on ImageNet .
We train the one-task initialized crossstitched network for 10k iterations and the ImageNet initialized cross-stitched network for 30k iterations (to account
for the 20k Ô¨Åne-tuning iterations of the one-task networks),
and report the results in Table 3. Task-speciÔ¨Åc initialization performs better than ImageNet initialization for both
the tasks, which suggests that cross-stitching should be used
after training task-speciÔ¨Åc networks.
5.4. Visualization of learned combinations
We visualize the weights Œ±S and Œ±D of the cross-stitch
units for different initializations in Figure 4. For this experiment, we initialize sub-networks A and B using one-task
networks and trained the cross-stitched network till convergence. Each plot shows (in sorted order) the Œ± values
for all the cross-stitch units in a layer (one per channel).
Table 3: We initialize the networks A, B (from Figure 4)
from ImageNet, as well as task-speciÔ¨Åc networks.
observe that task-based initialization performs better than
task-agnostic ImageNet initialization.
Surface Normal
Segmentation
Angle Distance
(Lower Better)
(Higher Better)
(Higher Better)
We show plots for three layers: pool1, pool5 and fc7.
The initialization of cross-stitch units biases the network to
start its training preferring a certain type of shared representation, e.g., (Œ±S, Œ±D) = (0.9, 0.1) biases the network
to learn more task-speciÔ¨Åc features, while (0.5, 0.5) biases
it to share representations. Figure 4 (second row) shows
that both the tasks, across all initializations, prefer a more
task-speciÔ¨Åc representation for pool5, as shown by higher
values of Œ±S. This is inline with the observation from Section 1.1 that Split conv4 performs best for these two tasks.
We also notice that the surface normal task prefers shared
representations as can be seen by Figure 4(b), where Œ±S and
Œ±D values are in similar range.
6. Experiments
We now present experiments with cross-stitch networks
for two pairs of tasks: semantic segmentation and surface
normal prediction on NYU-v2 , and object detection
and attribute prediction on PASCAL VOC 2008 .
We use the experimental setup from Section 5 for semantic
segmentation and surface normal prediction, and describe
the setup for detection and attribute prediction below.
Dataset, Metrics and Network: We consider the PAS-
CAL VOC 20 classes for object detection, and the 64 attribute categories data from . We use the PASCAL VOC
2008 dataset for our experiments and report results
using the standard Average Precision (AP) metric. We start
with the recent Fast-RCNN method for object detection
using the AlexNet architecture.
Training: For object detection, Fast-RCNN is trained using 21-way 1-vs-all classiÔ¨Åcation with 20 foreground and 1
background class. However, there is a severe data imbalance in the foreground and background data points (boxes).
To circumvent this, Fast-RCNN carefully constructs minibatches with 1 : 3 foreground-to-background ratio, i.e.,
at most 25% of foreground samples in a mini-batch. Attribute prediction, on the other hand, is a multi-label classi-
Ô¨Åcation problem with 64 attributes, which only train using
foreground bounding boxes. To implement both tasks in
the Fast R-CNN framework, we use the same mini-batch
sampling strategy; and in every mini-batch only the foreground samples contribute to the attribute loss (and background samples are ignored).
Scaling losses: Both SemSeg and SN used same classiÔ¨Åcation loss for training, and hence we were set their loss
weights to be equal (= 1). However, since object detection
is formulated as 1-vs-all classiÔ¨Åcation and attribute classi-
Ô¨Åcation as multi-label classiÔ¨Åcation, we balance the losses
by scaling the attribute loss by 1/64.
Cross-stitching: We combine two AlexNet architectures
using the cross-stitch units after every pooling layer as
shown in Figure 4. In the case of object detection and attribute prediction, we use one cross-stitch unit per layer activation map. We found that maintaining a unit per channel,
like in the case of semantic segmentation, led to unstable
learning for these tasks.
6.1. Baselines
We compare against four strong baselines for the two
pairs of tasks and report the results in Table 5 and 6.
Single-task Baselines: These serve as baselines without
beneÔ¨Åts of multi-task learning. First we evaluate a single
network trained on only one task (denoted by ‚ÄòOne-task‚Äô)
as described in Section 5. Since our approach cross-stitches
two networks and therefore uses 2√ó parameters, we also
consider an ensemble of two one-task networks (denoted
by ‚ÄòEnsemble‚Äô). However, note that the ensemble has 2√ó
network parameters for only one task, while the cross-stitch
network has roughly 2√ó parameters for two tasks. So for a
pair of tasks, the ensemble baseline uses ‚àº2√ó the crossstitch parameters.
Multi-task Baselines: The cross-stitch units enable the network to pick an optimal combination of shared and taskspeciÔ¨Åc representation. We demonstrate that these units remove the need for Ô¨Ånding such a combination by exhaustive
brute-force search (from Section 1.1). So as a baseline, we
train all possible ‚ÄúSplit architectures‚Äù for each pair of tasks
and report numbers for the best Split for each pair of tasks.
There has been extensive work in Multi-task learning
outside of the computer vision and deep learning community.
However, most of such work, with publicly available code, formulates multi-task learning in an optimization framework that requires all data points in memory . Such requirement is not practical for
the vision tasks we consider.
So as our Ô¨Ånal baseline, we compare to a variant of by adapting their method to our setting and report this
as ‚ÄòMTL-shared‚Äô.
The original method treats each category as a separate ‚Äòtask‚Äô, a separate network is required
for each category and all these networks are trained jointly.
Directly applied to our setting, this would require training
100s of ConvNets jointly, which is impractical. Thus, instead of treating each category as an independent task, we
Table 4: We show the sorted Œ± values (increasing left to right) for three layers. A higher value of Œ±S indicates a strong preference towards task speciÔ¨Åc features, and a higher Œ±D implies preference for shared representations. More detailed analysis
in Section 5.4. Note that both Œ±S and Œ±D are sorted independently, so the channel-index across them do not correspond.
(a) Œ±S = 0.9, Œ±D = 0.1
(b) Œ±S = 0.5, Œ±D = 0.5
(c) Œ±S = 0.1, Œ±D = 0.9
Segmentation
Surface Normal
Segmentation
Surface Normal
Segmentation
Surface Normal
1024 2048 3072 4096
1024 2048 3072 4096
1024 2048 3072 4096
1024 2048 3072 4096
1024 2048 3072 4096
1024 2048 3072 4096
Jet engine
Stem/Trunk
Vegetation
Handlebars
Side mirror
Furn. Seat
Furn. Back
Change in performance
Per class change in performance for attribute prediction and
Per class number of instances
No. of instances
Figure 5: Change in performance for attribute categories over the baseline is indicated by blue bars. We sort the categories
in increasing order (from left to right) by the number of instance labels in the train set, and indicate the number of instance
labels by the solid black line. The performance gain for attributes with lesser data (towards the left) is considerably higher
compared to the baseline. We also notice that the gain for categories with lots of data is smaller.
adapt their method to our two-task setting. We train these
two networks jointly, using end-to-end learning, as opposed
to their dual optimization to reduce hyperparameter search.
6.2. Semantic Segmentation and Surface Normal
Prediction
Table 5 shows the results for semantic segmentation and
surface normal prediction on the NYUv2 dataset . We
compare against two one-task networks, an ensemble of two
networks, and the best Split architecture (found using brute
force enumeration).
The sub-networks A, B (Figure 4)
in our cross-stitched network are initialized from the onetask networks. We use cross-stitch units after every pooling layer and fully connected layer (one per channel). Our
proposed cross-stitched network improves results over the
baseline one-task networks and the ensemble. Note that
even though the ensemble has 2√ó parameters compared to
cross-stitched network, the latter performs better. Finally,
our performance is better than the best Split architecture
network found using brute force search. This shows that the
cross-stitch units can effectively search for optimal amount
of sharing in multi-task networks.
6.3. Data-starved categories for segmentation
Multiple tasks are particularly helpful in regularizing the
learning of shared representations . This regularization manifests itself empirically in the improvement of
‚Äúdata-starved‚Äù (few examples) categories and tasks.
For semantic segmentation, there is a high mismatch in
the number of labels per category (see the black line in Fig-
Table 5: Surface normal prediction and semantic segmentation results on the NYU-v2 dataset. Our method outperforms the baselines for both the tasks.
Surface Normal
Segmentation
Angle Distance
(Lower Better)
(Higher Better)
(Higher Better)
Split conv4
MTL-shared
Cross-stitch [ours]
whiteboard
night-stand
shower-curtain
refridgerator
television
otherfurniture
otherstructure
Change in performance
Per class change in performance for semantic segmentation and
Per class number of pixel labels
No. of pixel labels (millions)
Figure 6: Change in performance (meanIU metric) for semantic segmentation categories over the baseline is indicated by blue bars. We sort the categories (in increasing
order from left to right) by the number of pixel labels in
the train set, and indicate the number of pixel labels by a
solid black line. The performance gain for categories with
lesser data (towards the left) is more when compared to the
baseline one-task network.
ure 6). Some classes like wall, Ô¨Çoor have many more instances than other classes like bag, whiteboard etc. Figure 6 also shows the per-class gain in performance using
our method over the baseline one-task network. We see that
cross-stitch units considerably improve the performance of
‚Äúdata-starved‚Äù categories (e.g., bag, whiteboard).
6.4. Object detection and attribute prediction
We train a cross-stitch network for the tasks of object detection and attribute prediction. We compare against baseline one-task networks and the best split architectures per
task (found after enumeration and search, Section 1.1). Table 6 shows the results for object detection and attribute prediction on PASCAL VOC 2008 . Our method shows
improvements over the baseline for attribute prediction. It
is worth noting that because we use a background class for
detection, and not attributes (described in ‚ÄòScaling losses‚Äô
in Section 6), detection has many more data points than attribute classiÔ¨Åcation (only 25% of a mini-batch has attribute
labels). Thus, we see an improvement for the data-starved
Table 6: Object detection and attribute prediction results on
the attribute PASCAL 2008 dataset
Detection (mAP)
Attributes (mAP)
Split conv2
MTL-shared
Cross-stitch [ours]
task of attribute prediction. It is also interesting to note that
the detection task prefers a shared representation (best performance by Split fc7), whereas the attribute task prefers a
task-speciÔ¨Åc network (best performance by Split conv2).
6.5. Data-starved categories for attribute prediction
Following a similar analysis to Section 6.3, we plot the
relative performance of our cross-stitch approach over the
baseline one-task attribute prediction network in Figure 5.
The performance gain for attributes with smaller number
of training examples is considerably large compared to the
baseline (4.6% and 4.3% mAP for the top 10 and 20 attributes with the least data respectively). This shows that
our proposed cross-stitch method provides signiÔ¨Åcant gains
for data-starved tasks by learning shared representations.
7. Conclusion
We present cross-stitch units which are a generalized
way of learning shared representations for multi-task learning in ConvNets. Cross-stitch units model shared representations as linear combinations, and can be learned end-toend in a ConvNet. These units generalize across different
types of tasks and eliminate the need to search through several multi-task network architectures on a per task basis. We
show detailed ablative experiments to see effects of hyperparameters, initialization etc. when using these units. We
also show considerable gains over the baseline methods for
data-starved categories. Studying other properties of crossstitch units, such as where in the network should they be
used and how should their weights be constrained, is an interesting future direction.
Acknowledgments:
We would like to thank Alyosha Efros
and Carl Doersch for helpful discussions. This work was supported in part by ONR MURI N000141612007 and the US Army
Research Laboratory (ARL) under the CTA program (Agreement
W911NF-10-2-0016). AS was supported by the MSR fellowship.
We thank NVIDIA for donating GPUs.