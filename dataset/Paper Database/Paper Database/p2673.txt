HAL Id: hal-01934928
 
Submitted on 26 Nov 2018
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Distributed under a Creative Commons Attribution 4.0 International License
Explainable AI: the new 42?
Randy Goebel, Ajay Chander, Katharina Holzinger, Freddy Lecue, Zeynep
Akata, Simone Stumpf, Peter Kieseberg, Andreas Holzinger
To cite this version:
Randy Goebel, Ajay Chander, Katharina Holzinger, Freddy Lecue, Zeynep Akata, et al.. Explainable
AI: the new 42?.
2nd International Cross-Domain Conference for Machine Learning and Knowledge Extraction (CD-MAKE), Aug 2018, Hamburg, Germany. pp.295-303, ￿10.1007/978-3-319-99740-
7_21￿. ￿hal-01934928￿
Explainable AI: the new 42?
Randy Goebel1, Ajay Chander2, Katharina Holzinger3, Freddy Lecue4,5
Zeynep Akata6,7, Simone Stumpf8, Peter Kieseberg3,9, Andreas Holzinger10,11
1 Alberta Machine Intelligence Institute
University of Alberta, Edmonton, Canada
 
2 Fujitsu Labs of America, US
 
3 SBA-Research, Vienna, Austria
 
4 INRIA, Sophia Antipolis, France
5 Accenture Labs, Dublin, Ireland
 
6 Amsterdam Machine Learning Lab, University of Amsterdam,The Netherlands
7 Max Planck Institute for Informatics, Saarbruecken, Germany
 
8 City, University of London, UK
 
9 University of Applied Sciences St. P¨olten, Austria
 
10 Holzinger Group HCI-KDD, Institute for Medical Informatics, Statistics &
Documentation, Medical University Graz, Austria
11 Institute of Interactive Systems and Data Science & Computer Media
Graz University of Technology, Austria
 
Abstract. Explainable AI is not a new ﬁeld. Since at least the early
exploitation of C.S. Pierce’s abductive reasoning in expert systems of
the 1980s, there were reasoning architectures to support an explanation
function for complex AI systems, including applications in medical diagnosis, complex multi-component design, and reasoning about the real
world. So explainability is at least as old as early AI, and a natural consequence of the design of AI systems. While early expert systems consisted
of handcrafted knowledge bases that enabled reasoning over narrowly
well-deﬁned domains (e.g., INTERNIST, MYCIN), such systems had no
learning capabilities and had only primitive uncertainty handling. But
the evolution of formal reasoning architectures to incorporate principled
probabilistic reasoning helped address the capture and use of uncertain
knowledge.
There has been recent and relatively rapid success of AI/machine learning solutions arises from neural network architectures. A new generation
of neural methods now scale to exploit the practical applicability of statistical and algebraic learning approaches in arbitrarily high dimensional
spaces. But despite their huge successes, largely in problems which can
be cast as classiﬁcation problems, their eﬀectiveness is still limited by
their un-debuggability, and their inability to “explain” their decisions in
a human understandable and reconstructable way. So while AlphaGo or
DeepStack can crush the best humans at Go or Poker, neither program
has any internal model of its task; its representations defy interpretation
by humans, there is no mechanism to explain their actions and behaviour,
and furthermore, there is no obvious instructional value . . . the high performance systems can not help humans improve.
Even when we understand the underlying mathematical scaﬀolding of
current machine learning architectures, it is often impossible to get insight into the internal working of the models; we need explicit modeling
and reasoning tools to explain how and why a result was achieved. We
also know that a signiﬁcant challenge for future AI is contextual adaptation, i.e., systems that incrementally help to construct explanatory
models for solving real-world problems. Here it would be beneﬁcial not
to exclude human expertise, but to augment human intelligence with
artiﬁcial intelligence.
Keywords: artiﬁcial intelligence, machine learning, Explainability, Explainable AI
Introduction
Artiﬁcial intelligence (AI) and machine learning (ML) have recently been highly
successful in many practical applications (e.g., speech recognition, face recognition, autonomous driving, recommender systems, image classiﬁcation, natural
language processing, automated diagnosis, . . . ), particularly when components
of those practical problems can be articulated as data classiﬁcation problems.
Deep learning approaches, including the more sophisticated reinforcement learning architectures, exceed human performance in many areas , , , .
However, an enormous problem is that deep learning methods turn out to be
uninterpretable ”black boxes,” which create serious challenges, including that
of interpreting a predictive result when it may be conﬁrmed as incorrect. For
example, consider Figure 1, which presents an example from the Nature review
by LeCun, Bengio, and Hinton . The ﬁgure incorrectly labels an image of a
dog lying on a ﬂoor and half hidden under a bed as “A dog sitting on a hardwood
ﬂoor.” To be sure, the coverage of their image classiﬁcation/prediction model is
impressive, as is the learned coupling of language labels. But the reality is that
the dog is not sitting.
The ﬁrst problem is the naive but popular remedy about how to debug the
predictive classiﬁer to correct the error: augment the original labeled training set
with more carefully crafted inputs to distinguish, say, a sitting from a laying dog
might improve the incorrect output. This may or may not correct the problem,
and doesn’t address the resource challenge of recreating the original learned
The transparency challenge gets much more complex when the output predictions are not obviously wrong. Consider medical or legal reasoning, where
one typically seeks not just an answer or output (e.g., a diagnostic prediction of
Fig. 1. Segment of an example from LeCun, Bengio,Hinton, Science 
Description: This is a large bird with a white neck and a black back in the water.
Definition: The Western Grebe is has a yellow pointy beak, white neck and belly, and black back.
Visual Explanation: This is a Western Grebe because this bird has a long white neck, pointy yellow
beak and red eye.
Image Relevance
Class Relevance
Description
Explanation
Definition
Laysan Albatross
Description: This is a large flying bird with black wings and a white belly.
Definition: The Laysan Albatross is a seabird with a hooked yellow beak, black back and white belly.
Visual Explanation: This is a Laysan Albatross because this bird has a large wingspan, hooked yellow
beak, and white belly.
Description: This is a large bird with a white neck and a black back in the water.
Definition: The Laysan Albatross is a seabird with a hooked yellow beak, black back and white belly.
Visual Explanation: This is a Laysan Albatross because this bird has a hooked yellow beak white neck
and black back.
Laysan Albatross
Western Grebe
Fig. 2. The goal is to generate explanations that are both image relevant and class
relevant. In contrast, descriptions are image relevant, but not necessarily class relevant,
and deﬁnitions are class relevant but not necessarily image relevant.
prostate cancer would require some kind of explanation or structuring of evidence
used to support such a prediction). In short, false positives can be disastrous.
Brieﬂy, the representational and computational challenge is about how to
construct more explicit models of what is learned, in order to support explicit
computation that produces a model-based explanation of a predicted output.
However, this is one of the historical challenges of AI: what are appropriate
representations of knowledge that demonstrate some veracity with the domain
being captured? What reasoning mechanisms oﬀer the basis for conveying a
computed inference in terms of that model?
The reality of practical applications of AI and ML in sensitive areas (such as
the medical domain) reveals an inability of deep learned systems to communicate
eﬀectively with their users. So emerges the urgent need to make results and
machine decisions transparent, understandable and explainable , , . The
big advantage of such systems would include not only explainability, but deeper
understanding and replicability . Most of all, this would increase acceptance
and trust, which is mandatory in safety-critical systems , and desirable in
many applications (e.g., in medical robotics , Ambient Assisted Living ,
Enterprise decision making , etc.). First steps have been taken towards making
these systems understandable to their users, by providing textual and visual
explanations , (see Figures 2 and 3).
Deep Finegrained Classifier
Compact Bilinear
Recurrent explanation generator model
This is a cardinal because ...
Fig. 3. A joint classiﬁcation and explanation model . Visual features are extracted
using a ﬁne-grained classiﬁer before sentence generation; unlike other sentence generation models, condition sentence generation on the predicted class label. A discriminative
loss function encourages generated sentences to include class speciﬁc attributes.
Current State-of-the-Art
Explaining decisions is an integral part of human communication, understanding,
and learning, and humans naturally provide both deictic (pointing) and textual
modalities in a typical explanation. The challenge is to build deep learning models that are also able to explain their decisions with similar ﬂuency in both visual
and textual modalities (see Figure 2). Previous machine learning methods for
explanation were able to provide a text-only explanation conditioned on an image in context of a task, or were able to visualize active intermediate units in
a deep network performing a task, but were unable to provide explanatory text
grounded in an image.
Existing approaches for deep visual recognition are generally opaque and
do not output any justiﬁcation text; contemporary vision-language models can
describe image content but fail to take into account class-discriminative image
aspects which justify visual predictions.
Hendriks et al. propose a new model (see Figure 3) that focuses on the
discriminating properties of the visible object, jointly predicts a class label, and
explains why the predicted label is appropriate for the image. The idea relies on a
loss function based on sampling and reinforcement learning, which learns to generate sentences that realize a global sentence property, such as class speciﬁcity.
This produces a ﬁne-grained bird species classiﬁcation dataset, and shows that
an ability to generate explanations which are not only consistent with an image
but also more discriminative than descriptions produced by existing captioning
Although, deep models that are both eﬀective and explainable are desirable
in many settings, prior explainable models have been unimodal, oﬀering either
image-based visualization of attention weights or text-based generation of posthoc justiﬁcations. Park et al. propose a multimodal approach to explanation,
and argue that the two modalities provide complementary explanatory strengths.
Two new datasets are created to deﬁne and evaluate this task, and use a
model which can provide joint textual rationale generation and attention visu-
The activity is
A: Mowing Lawn
… because he is kneeling
in the grass next to a lawn
… because he is pushing a
lawn mower over a grassy
The activity is
A: Mountain Biking
… because he is riding a
bicycle down a mountain
path in a mountainous area.
… because he is wearing a
cycling uniform and riding
a bicycle down the road.
A: Mowing Lawn
A: Road Biking
Q: Is this a zoo?
… because the zebras are
standing in a green field.
… because there are
animals in an enclosure.
Q: Is the water calm?
… because there are waves
… because there are no
waves and you can see the
reflection of the sun.
Fig. 4. Left: ACT-X qualitative results: For each image the PJ-X model provides an
answer and a justiﬁcation, and points to the evidence for that justiﬁcation. Right:
VQA-X qualitative results: For each image the PJ-X model provides an answer and a
justiﬁcation, and points to the evidence for that justiﬁcation.
alization (see Figure 4). These datasets deﬁne visual and textual justiﬁcations
of a classiﬁcation decision for activity recognition tasks (ACT-X) and for visual
question answering tasks (VQA-X). They quantitatively show that training with
the textual explanations not only yields better textual justiﬁcation models, but
also better localizes the evidence that supports the decision.
Qualitative cases also show both where visual explanation is more insightful
than textual explanation, and vice versa, supporting the hypothesis that multimodal explanation models oﬀer signiﬁcant beneﬁts over unimodal approaches.
This model identiﬁes visual evidence important for understanding each human
activity. For example to classify “mowing lawn” in the top row of Figure 4 the
model focuses both on the person, who is on the grass, as well as the lawn mower.
This model can also diﬀerentiate between similar activities based on the context,
e.g.“mountain biking” or “road biking.”
Similarly, when asked “Is this a zoo?” the explanation model is able to discuss
what the concept of “zoo” represents, i.e., “animals in an enclosure.” When
determining whether the water is calm, which requires attention to speciﬁc image
regions, the textual justiﬁcation discusses foam on the waves.
Visually, this attention model is able to point to important visual evidence.
For example in the top row of Figure 2, for the question “Is this a zoo?” the
visual explanation focuses on the ﬁeld in one case, and on the fence in another.
There are also other approaches to explanation that formulate heuristics for
creating what have been called “Deep Visual Explanation” . For example, in
the application to debugging image classiﬁcation learned models, we can create a
heat map ﬁlter to explain where in an image a classiﬁcation decision was made.
There are an arbitrary number of methods to identify diﬀerences in learned
variable distributions to create such maps; one such is to compute a Kullback-
Leibler (KL) divergence gradient, experiments with which are described in ,
and illustrated in (see Figure 5). In that ﬁgure, the divergence for each input
image and the standard VGG image classiﬁcation predictor is rendered as a heat
map, to provide a visual explanation of which portion of an image was used in
the classiﬁcation.
Fig. 5. Explaining the decisions made by the VGG-16 (park bench, street sign, racket,
cockatoo, traﬃc light and chihuahua), our approach highlights the most discriminative
region in the image.
Conclusion and Future Outlook
We may think of an explanation in general as a ﬁlter on facts in a context . An
eﬀective explanation helps the explainer cross a cognitive valley, allowing them to
update their understanding and beliefs . AI is becoming an increasingly ubiquitous co-pilot for human decision making. So AI learning systems will require
explicit attention to the construction of problem domain models and companion
reasoning mechanisms which support general explainability.
Figure 6 provides one example of how we might bridge the gaps between
digital inference and human understanding. Deep Tensor is a deep neural
Fig. 6. Explainable AI with Deep Tensor and a knowledge graph
network that is especially suited to datasets with meaningful graph-like properties. The domains of biology, chemistry, medicine, and drug design oﬀer many
such datasets where the interactions between various entities (mutations, genes,
drugs, disease) can be encoded using graphs. Let’s consider a Deep Tensor network that learns to identify biological interaction paths that lead to disease. As
part of this process, the network identiﬁes inference factors that signiﬁcantly
inﬂuenced the ﬁnal classiﬁcation result. These inﬂuence factors are then used
to ﬁlter a knowledge graph constructed from publicly available medical research
corpora. In addition, the resulting interaction paths are further constrained by
known logical constraints of the domain, biology in this case. As a result, the
classiﬁcation result is presented (explained) to the human user as an annotated
interaction path, with annotations on each edge linking to speciﬁc medical texts
that provide supporting evidence.
Explanation in AI systems is considered to be critical across all areas where
machine learning is used. There are examples which combine multiple architectures, e.g., combining logic-based system with classic stochastic systems to
derive human-understandable semantic explanations . Another example is in
the case of transfer learning , where learning complex behaviours from small
volumes of data is also in strong needs of explanation of eﬃcient, robust and
scalable transferability .
Acknowledgements
The authors thanks their colleagues from local and international institutions for
their valuable feedback, remarks and critics on this introduction to the MAKE-
Explainable-AI workshop.