Cost-Sensitive Boosting
Hamed Masnadi-Shirazi and Nuno Vasconcelos
SVCL-TR 2007/06
Cost-Sensitive Boosting
Hamed Masnadi-Shirazi and Nuno Vasconcelos
Statistical Visual Computing Lab
Department of Electrical and Computer Engineering
University of California, San Diego
A novel framework, based on the statistical interpretation of boosting, is proposed
for the design of cost sensitive boosting algorithms. It is argued that, although predictors produced with boosting converge to the ratio of posterior class probabilities that
also appears in Bayes decision rule, this convergence only occurs in a small neighborhood of the optimal cost-insensitive classiﬁcation boundary. This is due to a combination of the cost-insensitive nature of current boosting losses, and boosting’s sample
reweighing mechanism. It is then shown that convergence in the neighborhood of a
target cost-sensitive boundary can be achieved through boosting-style minimization of
extended, cost-sensitive, losses. The framework is applied to the design of speciﬁc algorithms, by introduction of cost-sensitive extensions of the exponential and binomial
losses. Minimization of these losses leads to cost sensitive extensions of the popular
AdaBoost, RealBoost, and LogitBoost algorithms. Experimental validation, on various
UCI datasets and the computer vision problem of face detection, shows that the new
algorithms substantially improve performance over what was achievable with previous
cost-sensitive boosting approaches.
Author email: 
c⃝University of California San Diego, 2007
This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part without payment of fee is granted for nonproﬁt educational and
research purposes provided that all such whole or partial copies include the following: a notice that
such copying is by permission of the Statistical Visual Computing Laboratory of the University of
California, San Diego; an acknowledgment of the authors and individual contributors to the work;
and all applicable portions of the copyright notice. Copying, reproducing, or republishing for any
other purpose shall require a license with payment of fee to the University of California, San Diego.
All rights reserved.
SVCL Technical reports are available on the SVCL’s web page at
 
University of California, San Diego
Statistical Visual Computing Laboratory
9500 Gilman Drive, Mail code 0407
EBU 1, Room 5512
La Jolla, CA 92093-0407
Introduction
Classiﬁcation problems such as fraud detection , medical diagnosis , or object detection in computer vision , are naturally cost sensitive .
In these problems the cost of missing a target is much higher than that of a falsepositive, and classiﬁers that are optimal under symmetric costs (such as the popular
zero-one loss) tend to under perform. The design of optimal classiﬁers with respect to
losses that weigh certain types of errors more heavily than others is denoted as costsensitive learning . Current research in this area falls into two main categories. The
ﬁrst aims for generic procedures that can make arbitrary classiﬁers cost sensitive, by
resorting to Bayes risk theory or some other cost minimization strategy .
The second attempts to extend particular algorithms, so as to produce cost-sensitive
generalizations.
Of interest to this work are classiﬁers obtained by thresholding a continuous function, here denoted as a predictor, and therefore similar to the Bayes decision rule
(BDR) , which is well known to be optimal for both cost-insensitive and costsensitive classiﬁcation. In particular, we consider learning algorithms in the boosting
family , such as the popular AdaBoost , which is not cost-sensitive
but has achieved tremendous practical success in important areas of application, such
as computer vision . Like all other boosting algorithms, AdaBoost learns a predictor by composing an ensemble of weak classiﬁcation rules (weak learners), and relies
on a sample re-weighting mechanism to place greater emphasis on a neighborhood
of the classiﬁcation boundary. This guarantees a large classiﬁcation margin and good
(cost-insensitive) generalization with small amounts of training data. There are multiple interpretations for Adaboost, including those of a large margin method ,
a gradient descent procedure in the functional space of convex combinations of weak
learners , and a method for step-wise logistic regression , among others .
This work builds on a combination of these interpretations to derive a cost-sensitive
boosting extension. We start with the observation, by Friedman et al. , that the
predictor which minimizes the exponential loss used by AdaBoost (and many other
boosting algorithms) is the ratio of posterior distributions that also appears in the BDR.
Given the optimality of the latter, this offers an explanation for the excellent performance of boosted detectors in cost-insensitive classiﬁcation problems. It is, however,
at odds with various empirical observations of boosting’s 1) poor cost-sensitive performance , and 2) inability to produce well calibrated estimates of class
posterior probabilities . We argue that this is an intrinsic limitation of
the large-margin nature of boosting: due to the emphasis (sample reweighing) on the
classiﬁcation border, the predictor produced by boosting only converges to the BDR in
a small neighborhood of that border. Outside this neighborhood, it has identical sign to
the BDR (a sufﬁcient condition for cost-insensitive classiﬁcation) but does not necessarily approximate it well (a necessary condition for good cost-sensitive performance).
Two conditions are identiﬁed as necessary for optimal cost-sensitive boosting: 1)
that the predictor does converge to the BDR in the neighborhood of a classiﬁcation
boundary, but 2) that the latter is the target cost-sensitive boundary, rather than the
one optimal in the cost-insensitive sense. We propose that this is best accomplished
COST-SENSITIVE CLASSIFICATION
by modifying the loss function minimized by boosting, so that boosting-style gradient
descent can satisfy the two conditions. This leads to a general framework for the costsensitive extension of boosting algorithms. We introduce cost-sensitive versions of the
exponential and binomial losses, which underly some of the most popular boosting
algorithms, including AdaBoost , RealBoost , and LogitBoost . Costsensitive extensions of these algorithms are then derived, and shown to satisfy the two
necessary conditions for cost-sensitive optimality.
Various cost-sensitive extensions of boosting have been previously proposed in the
literature, including AdaCost , CSB0, CSB1, CSB2 asymmetric-AdaBoost 
and AdaC1, AdaC2, AdaC3 . All of these algorithms are heuristic in nature, attempting to achieve cost-sensitivity by direct manipulation of the weights and conﬁdence parameters of Adaboost. In most cases, it is not clear if, or how, these manipulations modify the loss minimized by boosting, or even how they relate to any of
the different interpretations of boosting. This is unlike the framework now proposed,
which relies on the statistical interpretation of boosting to derive cost-sensitive extensions of the boosting loss. Due to this, the algorithms now proposed inherit all the
properties of classical, cost-insensitive, boosting. They simply shift boosting’s emphasis from the neighborhood of the cost-insensitive boundary to the neighborhood of the
target cost-sensitive boundary.
The performance of the proposed cost-sensitive boosting algorithms is evaluated
empirically, through experiments on both synthetic classiﬁcation problems (which provide insight) and standard datasets from the UCI repository and computer vision (face
detection). These experiments show that the algorithms do indeed possess cost sensitive optimality, and can meet target detection rates without (sub-optimal) weight or
threshold manipulation. They are also shown to outperform the previously available
cost-sensitive boosting methods, consistently achieving the best results in all experiments.
The paper is organized as follows. In Section 2 we review the main principles
of cost-sensitive classiﬁcation. Section 3 then presents a brief review of the standard boosting algorithms and previous attempts at cost-sensitive extensions, discussing
their limitations for optimal cost-sensitive classiﬁcation. The new framework for costsensitive boosting is introduced in Section 4, where the extensions of AdaBoost, Real-
Boost, and LogitBoost, are also derived. Finally, the empirical evaluation is discussed
in Section 5, and some conclusions are drawn in Section 6.
Cost-sensitive classiﬁcation
We start by reviewing the fundamental concepts of cost-sensitive classiﬁcation. Although most of these apply to multi-way classiﬁcation problems, in this work we only
consider the binary case, usually referred to as the detection problem.
A detector, or binary classiﬁer, is a function h : X →{−1, 1} that maps a feature
vector x = (x1, . . . , xN)T ∈X ⊂RN into a class label y ∈{−1, 1}. This mapping
Optimal detection
is implemented as
h(x) = sgn[f(x)]
where f : X →R is a predictor, and sgn[x] = 1 if x ≥0, and sgn[x] = −1 otherwise.
Feature vectors are samples from a random process X that induces a probability distribution PX(x) on X, and labels are samples from a random variable Y that induces a
probability distribution PY (y) in {−1, 1}.
The detector is optimal if it minimizes the risk
R = EX,Y [L(x, y)],
where L(x, y) is a loss function. We consider losses of the form
if h(x) = y
if y = −1 and h(x) = 1
if y = 1 and h(x) = −1
with Ci > 0. When C1 = C2 the detector is said to be cost-insensitive, otherwise
it is cost-sensitive. The three scenarios accounted by L(x, y) are denoted as correct
decisions (h(x) = y), false positives (y = −1 and h(x) = 1), and false-negatives or
misses (y = 1 and h(x) = −1).
For many cost-sensitive problems, the costs C1 and C2 are naturally speciﬁed from
domain knowledge. For example, in a fraud detection application, prior experience
dictates that there is an average cost of C2 dollars per false positive, while a false
negative (miss) will cost C1 > C2 dollars, on average. In this case, the costs are simply
the values C2 and C1. There are, nevertheless, other problems in which it is more
natural to specify target detection or false-positive rates than to specify costs. The two
types of problems can be addressed within a common optimal detection framework.
Optimal detection
We start by considering the case where the costs C1 and C2 are speciﬁed. In this case,
it is well known that the optimal predictor is given by the BDR , i.e.
f ∗= arg min
EX,Y [L(x, y)]
if and only if
f ∗(x) = PY |X(1|x)C1
PY |X(−1|x)C2
When the speciﬁcation is in terms of error rates, this result still holds, but the cost
structure (C1, C2) that meets the speciﬁed rates must be determined. This can be done
with resort to the Neyman-Pearson Lemma . For example, given the speciﬁcation
of detection rate ξ, the optimal cost structure is the one such that
P(x|y = 1)dx
COST-SENSITIVE CLASSIFICATION
P(y = 1|x)
P(y = −1|x) > C2
Note that the optimal decision rule is still the BDR, i.e. to decide for class 1 if x ∈H
(and −1 otherwise). The only difference is that, rather than specifying the costs, one
has to search for the costs that achieve the detection rate of (4). This can be done by
cross-validation. Note that, because all that matters is the ratio C1/C2, C2 can be set
to one and the search is one-dimensional.
In any case, the optimal detector can be written as
T (x) = sgn [log (f ∗
0 (x)) −T]
0 (x) = PY |X(1|x)
PY |X(−1|x),
is the optimal cost-insensitive predictor and
T = log C1
Hence, for any cost structure (C1, C2), cost-sensitive optimality differs from costinsensitive optimality only through the threshold T: given f ∗
0 (x) all optimal costsensitive rules can be obtained by simple threshold manipulation. Furthermore, from (4),
different thresholds correspond to different detection rates, and threshold manipulation can produce the optimal decision functions at any desired detection (or falsepositive) rate. This is the motivation for the widespread use of receiver operating curves
(ROCs) , and the tuning of error rates by threshold manipulation.
Practical detection
In practice, the posterior probabilities of (6) are unknown, and a learning algorithm is
used to estimate the predictor
ˆf(x) ≈f ∗
enabling the implementation of approximately optimal cost-sensitive rules
ˆhT (x) = sgn[ ˆf(x) −T].
While this is a commonly used strategy to obtain cost-sensitive rules, it does not necessarily guarantee good cost-sensitive performance. In fact, there are no guarantees of
the latter even when the cost-insensitive detector is optimal, i.e. when
ˆh0(x) = sgn[f ∗
While the necessary and sufﬁcient conditions for (10) are that
0 (x) = 0, ∀x ∈C
sgn[ ˆf(x)]
0 (x)], ∀x ̸∈C,
PY |X(1|x)
PY |X(−1|x) = 1
is the optimal cost-insensitive classiﬁcation boundary, the optimality of (9) requires
0 (x) = T, ∀x ∈CT
sgn[ ˆf(x) −T]
0 (x) −T], ∀x ̸∈CT
PY |X(1|x)
PY |X(−1|x) = T
Hence, for any x ∈CT , the necessary condition for cost-sensitive optimality
ˆf(x) = f ∗
is much tighter than the sufﬁcient condition for cost-insensitive optimality
sgn[ ˆf(x)] = sgn[f ∗
It follows that threshold manipulation can only produce optimal cost-sensitive detectors for all values of T if ˆf(x) = f ∗
0 (x), ∀x ∈X. Since this is a much more
restrictive constraint than the necessary and sufﬁcient conditions, (11) and (12), for
cost-insensitive optimality there is, in general, no reason for a cost-insensitive learning
algorithm to enforce it. This is, in fact, Vapnik’s argument against generative solutions
to the classiﬁcation problem: that there is no point in attempting to learn the optimal
predictor everywhere, when it is sufﬁcient to do so on the classiﬁcation boundary .
In summary, manipulating the threshold of an optimal cost-insensitive detector provides no guarantees of optimal cost-sensitive performance.
We consider cost-sensitive extensions of boosting algorithms. Such algorithms learn a
predictor f(x) by linear combination of simple decision rules Gm(x), known as weak
learners ,
Predictor optimality is deﬁned with respect to some loss function l[y, f(x)], such as
the exponential loss
le[y, f(x)] = EX,Y [exp(−yf(x))],
or the expected negative binomial log-likelihood
lb[y′, f(x)]
−EX,Y [y′ log(p(x)) + (1 −y′) log(1 −p(x))]
where y′ = (y + 1)/2 ∈{0, 1} is a re-parametrization of y and
ef(x) + e−f(x) .
Learning is based on a training sample of feature vectors {xi}n
i=1 and labels {yi}n
empirical estimates of these losses, and the iterative selection of weak learners. At
iteration m, a weight w(m)
is assigned to example (xi, yi) and the sample is reweighed
so as to amplify the importance of points that are poorly classiﬁed with the current
ensemble predictor of (17). We next review some popular examples of algorithms in
this family, whose cost-sensitive extensions will be introduced in later sections. In all
these cases, boosting can be interpreted as gradient descent on a functional space of
linear combinations of weak learners, with respect to one of the losses above [13, 25,
AdaBoost produces combinations of scaled binary classiﬁers
m (x) = αmgm(x),
where {αm}M
m=1 is a weight sequence and {gm(x)}M
m=1 a sequence of binary rules,
gm(x) : X →{−1, 1}, usually implemented with a decision stump
gm(x) = sgn[φm(x) −tm]
where φm(x) is a feature response (usually the projection of x along the direction of
a basis function φm) and tm a threshold. The ensemble predictor of (17) is learned by
gradient descent with respect to the exponential loss, for which the gradient at the mth
iteration is 
gm(x) = arg min
[1 −I(yi = g(xi))],
where I(·) is the indicator function
I(y = x) =
αm is the optimal step size in the direction of the gradient, found by a line search with
closed-form solution
1 −err(m)
[1 −I(yi = gm(xi))],
is the total error of gm(x). The weights are updated according to
RealBoost is an extension of AdaBoost that produces better estimates of the
optimal predictor f ∗
0 (x) by using real-valued weak learners in (17). In this case, the
gradient of the exponential loss is a (re-weighted) log-odds ratio
Y |X(1|φm(x))
Y |X(−1|φm(x))
where, as before, φm(x) is a feature response to x, and the superscript w indicates
that the probability distribution is that of the re-weighted sample. Weights are updated
according to
LogitBoost
Logitboost is motivated by the following observation, initially made by Friedman et
Lemma 1. (Statistical interpretation of boosting.)
The loss E[exp(−yf(x))] is minimized by the symmetric logistic transform of
PY |X(1|x),
2 log PY |X(1|x)
PY |X(−1|x).
Proof. See .
This implies that both Ada and RealBoost can be interpreted as step-wise procedures for ﬁtting an additive logistic regression model. Friedman et al. argued that this
is more naturally accomplished by step-wise minimization of the classical logistic regression losses, namely the expected negative binomial log-likelihood of (19). At the
mth boosting iteration, the optimal step with respect to the binomial loss can be found
by solving a weighted least squares regression for the weak learner Glogit
(x) that best
ﬁts a set of working responses
i −p(m)(xi)
p(m)(xi)(1 −p(m)(xi)),
where p(m)(x) is the probability of (20) based on the ensemble predictor of (17) after
m −1 iterations. The weights are
= p(m)(xi)(1 −p(m)(xi)).
Limitations for cost-sensitive learning
While both the minimization of the exponential and binomial losses are sufﬁcient to obtain the optimal cost-insensitive predictor of (29), we have already seen that everywhere
convergence to this predictor is not necessary to produce the optimal cost-insensitive
detector. For this, it sufﬁces that the ensemble predictor of (17) converges to any function ˆf(x) that satisﬁes (11) and (12). From a purely cost-insensitive perspective it is,
thus, sensible to require a greater accuracy of the approximation inside a neighborhood of the optimal cost-insensitive boundary C than outside of it. This is exactly what
boosting does, through the example re-weighting step of (26), (28), or (30). For both
Ada and RealBoost, a simple recursion shows that, after M iterations,
m=1 Gm(xi) = e−yif(xi),
where we have also used (17). Assuming that f(x) satisﬁes the necessary condition for
cost-insensitive optimality of (11), this ratio is one along C, exponentially increasing
(with the distance to this boundary) for incorrectly classiﬁed points, and exponentially
decreasing for correctly classiﬁed points. Hence, with the exception of a (hopefully)
small number of misclassiﬁed points, the weight is concentrated on a neighborhood
N(C) of the cost-insensitive boundary C. For LogitBoost, the weight w(M)
is a symmetric function of p(M)(xi), with maximum at p(M)(xi) = 1/2 or, from (20), at
f(xi) = 0. In fact,
ef(xi) + e−f(xi)−2
≈e−2sgn[f(xi)]f(xi) = e−2|f(xi)|
and the weight decays exponentially with the distance from the boundary, independently of whether the points are correctly classiﬁed or not.
In summary, boosting assigns exponentially decaying weight to points that have
been well classiﬁed during previous iterations, in the cost-insensitive sense. These
points, which are far from the cost-insensitive boundary, are exponentially discounted
as the optimization progresses. The resulting emphasis on N(C) is a deﬁnite advantage
for the design of the cost-insensitive detector, by guaranteing a large margin and an
ensemble predictor f(x) whose zero-level set very closely approximates C. This is
illustrated in Figure 1, where we depict the level sets of a hypothetic optimal costinsensitive predictor f ∗
0 (x) and a hypothetic ensemble predictor f(x). Because f ∗
is monotonically increasing to the left of C (and monotonically decreasing to its right),
any ensemble predictor which 1) has C as a zero-level set, and 2) exhibits the same
monotonicity, will both 1) satisfy (11)-(12), and 2) have great generalization ability for
cost-insensitive classiﬁcation.
However, this effort to maximize the margin does not guarantee that, outside N(C),
the level sets of f(x) are identical to those of f ∗(x).
In particular, the level set
f(x) = T is signiﬁcantly different from the level set f ∗
0 (x) = T, the optimal costsensitive boundary CT under the cost-structure correspondent to a threshold of T in (5).
It follows that threshold manipulation on the ensemble predictor f(x) does not lead to
the optimal cost-sensitive decision rule of (5). The inability of boosting to produce
Limitations for cost-sensitive learning
f0*(x)=f(x)=0
Figure 1: Example of a detection problem where boosting produces the optimal costinsensitive detector but threshold manipulation does not lead to optimal cost-sensitive
detectors. The ﬁgure presents level-sets of both the optimal predictor f ∗
0 (x) (solid line)
and the boosted predictor f(x) (dashed line). As iterations progress boosting emphasizes the minimization inside N(C). In result, while the zero level-set is optimal, the
same does not hold for other level-sets. This implies that the decision boundaries produced by threshold manipulation will be sub-optimal. Optimal cost-sensitive rules can,
however, be obtained by emphasizing the optimization in other regions, e.g. N(CT ).
accurate estimates of the posterior probabilities PY |X(y|x), sometimes referred to as
calibrated probabilities, has been noted by various authors . In and
 , this is attributed to the fact that the empirical estimate of either the exponential or
binomial losses is minimized by letting yif(xi) grow to inﬁnity for all training points.
When the span of the space of weak learners is rich enough to separate the training set
into the two classes, this is always possible and, if run for enough iterations, all boosting
algorithms produce a distribution of posterior probabilities PY |X(y|x) which is highly
concentrated in the neighborhoods of 0 and 1, independently of the true distribution.
Note that this does not compromise cost-insensitive optimality, but rather reinforces it,
since f(xi) grows to ∞for positive, and to −∞for negative examples. In summary,
boosting does not produce calibrated probabilities and will, in fact, converge to a binary posterior distribution (of values 0 and 1) if run sufﬁciently long. Independent of
the number of iterations, the probability estimates are usually not accurate enough to
guarantee acceptable cost-sensitive performance by threshold manipulation.
Prior work on cost-sensitive boosting
This limitation is well known in the boosting literature, where a number of costsensitive boosting extensions have been proposed . Since, for cost-sensitive
learning, the main problem is that boosting’s reweighing mechanism emphasizes N(C),
instead of the optimal cost-sensitive boundary N(CT ), it has long been noted that good
cost-sensitive performance requires a modiﬁcation of this mechanism. This is also supported by the intuition that, in cost-sensitive detection, examples from different classes
should be weighted differently.
A naive implementation of this intuition would be to modify the initial boosting
weights, so as to represent the asymmetry of the costs. However, because boosting
re-updates all weights at each iteration, it quickly destroys the initial asymmetry, and
the predictor obtained after convergence is usually not different from that produced
with symmetric initial conditions. A second natural strategy is to somehow change the
weight update equation. For example, one could make the updated weight equal to
a mixture of the result of (26), (28), or (30), and the initial cost-sensitive weights.
We refer to heuristics of this type as “weight manipulation”. Previously proposed
cost-sensitive boosting algorithms, such as AdaCost , CSB0, CSB1, CSB2 ,
Asymmetric-AdaBoost , AdaC1, AdaC2, or AdaC3 , fall in this class. For
example, CSB2 modiﬁes the weight update rule of AdaBoost to
= Ci · w(m)
relying on (24) for the computation of αm.
While various justiﬁcations are available for the different proposals for direct manipulation of boosting equations, these manipulations are essentially heuristic, and provide no guarantees of convergence to a good cost-sensitive decision rule. Furthermore,
none of the cost-sensitive extensions can be easily applied to algorithms other than Adaboost. We next introduce a framework for cost-sensitive boosting that addresses these
two limitations.
Cost-sensitive boosting
The new framework is inspired by two observations. First, the unifying principle behind the different boosting algorithms is that they perform gradient descent 
with respect to losses whose minimum is the optimal cost-insensitive predictor of (29).
Second, their main limitation for cost-sensitive learning is the emphasis on the neighborhood of the cost-insensitive boundary N(C), as shown in Figure 1. We have already
noted that these two properties are interconnected. While the limitation is due to the
weight-update mechanism, simply modifying this mechanism (as discussed in the previous section) is usually not sufﬁcient to achieve acceptable cost-sensitive performance.
Instead, boosting involves a balance between weight updates and gradient steps which
must be components of the minimization of the common loss. For cost-sensitive optimality, this balance requires that the loss function satisﬁes two conditions, which we
denote as the necessary conditions for cost-sensitive optimality.
1. It is minimized by the optimal cost-sensitive predictor.
2. It leads to a weight-updating mechanism that emphasizes a neighborhood of the
cost-sensitive boundary N(CT ).
This suggests an alternative strategy to design cost-sensitive boosting algorithms: to
modify the loss functions so that these two conditions are met. In what follows, we
show how this can be accomplished for Ada, Real and LogitBoost. The framework
could be used to derive cost-sensitive extensions of any algorithm that performs gradient descent on the space of combination of weak learners, e.g. GentleBoost or
AnyBoost . We limit our attention to the algorithms above for reasons of brevity,
and their popularity.
Cost-sensitive losses
We start by noting that the optimal cost-sensitive detector of (5) can be re-written as
T = sgn[log f ∗
T (x)] with
T (x) = PY |X(1|x)C1
PY |X(−1|x)C2
Noting that the zero level-set of this predictor is the cost-sensitive boundary CT , suggests that boosting-style of gradient descent on any loss function minimized, up to a
scaling factor, by f ∗
T (x) should satisfy the two necessary conditions for cost-sensitive
optimality. The following extensions of the expected exponential and binomial losses
guarantee that the ﬁrst is indeed met.
Lemma 2. The losses
I(y = 1)e−y.C1f(x) + I(y = −1)e−y.C2f(x)i
where I(·) is the indicator function of (23), and
−EX,Y [y′ log(pc(x)) + (1 −y′) log(1 −pc(x))]
COST-SENSITIVE BOOSTING
Algorithm 1 Cost-sensitive AdaBoost
Input: Training set D = {(x1, y1), . . . , (xn, yn)}, where y ∈{1, −1} is the class
label of example x, costs C1, C2, set of weak learners {gk(x)}K
k=1, and number M
of weak learners in the ﬁnal decision rule.
Initialization: Select uniformly distributed weights
2|I+|, ∀i ∈I+,
2|I−|, ∀i ∈I−.
for m = {1, . . . , M} do
for k = {1, . . . , K} do
train a weak learner/step-size pair (gk(x); αk), by considering various thresholds for gk(x). For each threshold compute α with (41) and the the resulting
loss with (40).
select (gm(x), αm) as the weak learner/step-size pair of smallest loss.
update weights wi according to (39).
Output: decision rule h(x) = sgn[PM
m=1 αmgm(x)].
eγf(x)+η + e−γf(x)−η .
γ = C1 + C2
are minimized by the asymmetric logistic transform of PY |X(1|x),
log P(y = 1|x)C1
P(y = y′′|x)C2
where y′′ = −1 for (33) and y′′ = 0 for (34).
Proof. See appendix A
We next derive cost-sensitive extensions of the boosting algorithms, by performing
gradient descent on these losses, and will later show that these extensions shift the
emphasis of the boosting weights from N(C) to N(CT ).
Cost-sensitive AdaBoost
We start by extending Adaboost.
Theorem 3. (Cost-sensitive AdaBoost) Consider the minimization of the empirical
estimate of the asymmetric loss of (33), based on a training sample {(xi, yi)}n
Cost-sensitive AdaBoost
gradient descent on the space, S, of functions of the form of (17) and (21), and deﬁne
I+ = {i|yi = 1}
I−= {i|yi = −1}.
The gradient direction and optimal step, at iteration m, are the solution of
exp(−C1αg(xi))
exp(C2αg(xi))
e−C1αmgm(xi),
eC2αmgm(xi),
Given the step size α, the gradient direction is
(eC1α −e−C1α) · b + e−C1αT+
+(eC2α −e−C2α) · d + e−C2αT−
and the optimal step size is the solution of
2C1 · b · cosh(C1α) + 2C2 · d · cosh(C2α) =
C1 · T+ · e−C1α + C2 · T−· e−C2α
[1 −I(yi = g(xi))]
[1 −I(yi = g(xi))]
Proof. See appendix B
COST-SENSITIVE BOOSTING
The gradient descent iteration cycles through the weak learners, for each, solving (41). This can be done efﬁciently with standard scalar search procedures. In the
experiments reported in this paper, the optimal α was found in an average of 6 iterations of bisection search. Given α, the loss associated with the weak learner can be
computed, and the optimal learner selected with (40). A summary of the cost-sensitive
boosting algorithm is presented in Algorithm 1. It is worth mentioning that the algorithm is fully compatible with Adaboost, in the sense that it reduces to the latter when
C1 = C2 = 1.
Cost-sensitive RealBoost
We next consider the cost-sensitive extension of RealBoost.
Theorem 4. (Cost-sensitive RealBoost) Consider the minimization of the asymmetric
loss of (33), based on a training sample {(xi, yi)}n
i=1, by gradient descent on the space,
Sr, of predictors of the form of (17) where the weak learners Gm(x) are real functions.
Given a dictionary of features {φ1(x), . . . , φK(x)}, the gradient at iteration m has the
(x) = Gφk∗(x)
where the optimal feature is determined by
k∗= arg min
exp(−C1Gφk(xi)) +
exp(C2Gφk(xi))]
with weights given by
Y |X(1|φ(x))C1
Y |X(−1|φ(x))C2
Y |X(y|φ(x)), y ∈{1, −1} are estimates of the posterior probabilities for the two
classes, after the application of the feature transformation φ(x) to a sample re-weighted
according to the weights w(m)
Proof. See appendix C
The posterior probabilities P (w)
Y |X(y|φm(x)), y ∈{1, −1} of (49) can be estimated
with standard techniques . For example, if the φk(x) are scalar features, they can be
obtained with weighted histograms of feature responses. Standard histogram regularization procedures should be used to avoid empty histogram bins. A summary of the
cost-sensitive RealBoost algorithm is presented in Algorithm 2. The algorithm is fully
compatible with RealBoost, in the sense that it reduces to the latter when C1 = C2 = 1.
Cost-sensitive LogitBoost
Algorithm 2 Cost-sensitive RealBoost
Input: Training set D = {(x1, y1), . . . , (xn, yn)}, where y ∈{1, −1} is the class
label of example x, costs C1, C2, and number M of weak learners in the ﬁnal decision rule.
Initialization: Select uniformly distributed weights
2|I+|, ∀i ∈I+,
2|I−|, ∀i ∈I−.
for m = {1, . . . , M} do
for k = {1, . . . , K} do
compute the gradient step Gφk(x) with (49).
select the optimal direction according to (47) and set the weak learner Greal
according to (46).
update weights wi according to (48).
Output: decision rule h(x) = sgn[PM
Cost-sensitive LogitBoost
Finally, we consider LogitBoost.
Theorem 5. (Cost-sensitive LogitBoost) Consider the minimization of the expected
binomial loss of (34), based on a training sample {(xi, yi)}n
i=1, on the space Sr of
predictors of the form of (17) where the weak learners Gm(x) are real functions. Given
a dictionary of features {φ1(x), . . . , φK(x)}, and a predictor f (m)(x), the Newton
step at iteration m has the form
2γ Gφk∗(x)
where Gφ(x) = aφφ(x) + bφ is the result of the weighted regression
(aφ, bφ) = arg min
(zi −aφφ(xi) −bφ)2
(xi)(1 −p(m)
p(m)(xi)(1 −p(m)(xi)),
where p(m)
(x) is the link function of (35), and p(m)(x) that of (20), with f(x) =
f (m)(x). The optimal feature is determined by
k∗= arg min
(zi −aφkφk(xi) −bφk)2.
COST-SENSITIVE BOOSTING
Algorithm 3 Cost-sensitive LogitBoost
Input: Training set D = {(x1, y′
1), . . . , (xn, y′
n)}, where y′ ∈{0, 1} is the class
label of example x, costs C1, C2, γ = C1+C2
C1 , I+ the set of examples
with label 1, I−the set of examples with label 0, and number M of weak learners
in the ﬁnal decision rule.
Initialization: Set uniformly distributed probabilities p(1)
c (xi) = p(1)(xi) = 1
and f (1)(x) = 0.
for m = {1, . . . , M} do
compute the working responses z(m)
as in (54) and weights w(m)
as in (55).
for k = {1, . . . , K} do
compute the solution to the least squares problem of (53),
⟨1⟩w · ⟨φk(xi)zi⟩w −⟨φk(xi)⟩w · ⟨zi⟩w
⟨1⟩w · ⟨φ2
k(xi)⟩w −⟨φk(xi)⟩2
w · ⟨zi⟩w −⟨φk(xi)⟩w · ⟨φk(xi)zi⟩w
⟨1⟩w · ⟨φ2
k(xi)⟩w −⟨φk(xi)⟩2
where we have deﬁned
select the optimal direction according to (56) and set the weak learner Glogit
according to (52).
set f (m+1)(x) = f (m)(x) + Glogit
Output: decision rule h(x) = sgn[PM
m=1 Glogit
Proof. See appendix D
A summary of the cost-sensitive LogitBoost algorithm is presented in Algorithm 3.
It is instructive to compare this to a procedure commonly used to calibrate the probabilities produced by large-margin classiﬁers, known as Platt calibration .
This procedure attempts to map the prediction f(x) ∈[−∞, +∞] to a posterior probability p(x) ∈ , using the link function of (35). The γ and η parameters are
determined by gradient descent with respect to the binomial loss of (34), also used in
cost-sensitive LogitBoost. The difference is that, in Platt’s method, cost-insensitive
boosting is ﬁrst used to learn the predictor f(x) and maximum likelihood is then used
to determine the parameters γ and η that best ﬁt a cross-validation data set. On the
other hand, cost-sensitive LogitBoost uses the calibrated link function throughout all
boosting iterations. Note that, besides requiring an additional validation set, Platt’s
method does not solve the problem of Figure 1, since the emphasis of the boosting
Cost-sensitive margins
component remains on N(C), not on N(CT ). We next show that the cost-sensitive
boosting algorithms introduced above do provide a solution to this problem.
Cost-sensitive margins
We have seen, in Section 4.1, that cost-sensitive boosting algorithms should satisfy two
conditions:
• convergence to the optimal predictor of (32),
• emphasis on a neighborhood of the cost-sensitive boundary N(CT ).
The ﬁrst condition is guaranteed by the use of the losses of (33) and (34). To investigate
the second we consider the weighting mechanisms of the three algorithms.
For both cost-sensitive Ada and RealBoost, a simple recursion shows that, after M
iterations,
= e−yiQif(xi),
where Qi = C1 if i ∈I+ and Qi = C2 otherwise. Assuming that f(x) converges
to the optimum of (36), this ratio is one along CT , exponentially increasing (with the
distance to this boundary) for xi such that f(xi)yi < 0, and exponentially decreasing
for xi such that f(xi)yi > 0. Hence, with respect to the cost-insensitive AdaBoost
algorithm, the only difference is whether the points are on the correct side of the cost
sensitive boundary CT . With the exception of the points which lie on the incorrect side,
all weight is concentrated on the neighborhood N(CT ) of the cost-sensitive boundary.
For LogitBoost, the weight w(M)
is a symmetric function of p(M)(xi), with maximum
at p(M)(xi) = 1/2 or, from (20), at f(xi) = 0. As in the cost-insensitive case,
ef(xi) + e−f(xi)−2
≈e−2|f(xi)|
and the weight decays exponentially with the distance from the zero-level set of f(x),
independently of whether the points are correctly classiﬁed or not. The only difference
is that, as f(x) converges to (36), this zero-level set is the cost-sensitive boundary CT .
This shows that all cost-sensitive boosting algorithms shift the margin emphasis from
N(C) to N(CT ).
Experimental evaluation
Two sets of experiments were designed to evaluate the cost-sensitive boosting algorithms. The ﬁrst was based on a simple synthetic problem, for which the BDR is
known, allowing explicit comparison to the optimal cost-sensitive detector. These experiments aimed for insight on various properties of the proposed algorithms. The
second set was based on standard datasets, and targeted a comparison between the new
algorithms and previously proposed methods.
EXPERIMENTAL EVALUATION
Synthetic datasets
We start with a synthetic binary scalar classiﬁcation problem, involving Gaussian classes
of equal variance σ2 = 1 and means µ−= −1 (y = −1) and µ+ = 1 (y = 1). 10K
examples were sampled per class, simulating the scenario where the class probabilities
are uniform.
To test the accuracy of the classiﬁers produced by cost-sensitive boosting we relied on
the following observations. First, given a cost structure (C1, C2), the boosted detector
is optimal if and only if the asymmetric logistic transform of (36) holds along the costsensitive boundary, i.e. if and only if x∗= f −1(0) where f(x) is the optimal predictor
of (36) and x∗the zero-crossing of the boosted predictor. Second, from (36), this is
equivalent to
PY |X(1|x∗) =
and it follows that, given cost structure and location x∗, it is possible to infer the true
class posterior probabilities at the latter. This is equally valid for multivariate problems,
in which case the location x∗becomes a level set. Hence, if the boosting algorithm
produces truly optimal cost-sensitive detectors, the plot of
C1+C2 as a function of x∗
should be identical to the plot of the class posterior probability PY |X(1|x∗). For the
Gaussian problem considered, it is straightforward to show that
PY |X(1|x) =
1 + e−2x ,
and (57) implies that x∗= −T/2, with T given by (7). It is therefore possible to evaluate the accuracy of the boosted cost-sensitive detectors, for the entire range of (C1, C2)
by either measuring the similarity between the plots (x∗,
C1+C2 ) and (x∗,
1+e−2x∗) or
the plots (x∗, −T
2 ) and (x∗, x∗).
These comparisons are shown on Figure 2 (a) and (b) for the detectors produced by
cost-sensitive Ada, Real, and LogitBoost. In all cases C2 = 1 and C1 was varied over
a wide range of values. For each value of C1, boosting was run for ﬁve iterations. It
is clear that both Real and LogitBoost produce accurate cost-sensitive detectors. The
difﬁculties of AdaBoost are due to the restriction of the predictor to a combination of
binary functions.
Comparison to previous algorithms
We next considered two cases in greater detail, namely the problems with cost structures C2 = 1 and C1 ∈{5, 20}, and compared the performance of the novel costsensitive boosting algorithms to those of the algorithms discussed in Section 3.5. For
these cost structures, the perfect detector has x∗= −.8047 (when C1 = 5) and
x∗= −1.4979 (when C1 = 20). The goal was to determine if the different algorithms could generate predictors with these zero-crossings, by manipulating their cost
parameters (e.g. the parameter C1 of cost-sensitive AdaBoost, which we denote by ˆC1
Synthetic datasets
P(y=1 | x)
Cost Sensitive RealBoost
Cost Sensitive AdaBoost
Cost Sensitive LogitBoost
Cost Sensitive RealBoost
Cost Sensitive AdaBoost
Cost Sensitive LogitBoost
Figure 2: a) Posterior probability PY |X(y = 1|x) used in the BDR, and estimates
produced by cost-sensitive Ada, Logit and RealBoost. b) Comparison of the plots
2 ) and (x∗, x∗).
to avoid confusions with the true value of the cost). Figure 3 compares how x∗evolved
(as a function of boosting iteration) for cost-sensitive AdaBoost and the cost sensitive
boosting algorithms previously available in the literature. For brevity, we limit the presentation to cost-sensitive AdaBoost since, as discussed above, this is the weakest of
the new cost-sensitive boosting algorithms on this problem. In all cases, a (rather extensive) search over values of the cost parameters of each algorithm was performed, so
as to guarantee the best possible performance after 50 iterations.
Despite the simplicity of the problem, this search did not produce a good solution
for most of the algorithms. As illustrated by Figure 3, four classes of behavior were
observed. Algorithms in the ﬁrst class (AdaC1, AdaCost) never produced any solutions other than the cost-insensitive optimal x∗= 0. The second class consisted of
algorithms (CSB0, AdaC2, AdaC3) that never converged to any meaningful solution.
Algorithms in the third class (CSB1, CSB2) showed some tendency to converge to
the right solution, but were really not able to. While in some cases this was due to
a slow convergence rate, in others the algorithms seemed to have converged only to
start oscillating, or even diverging. Only cost-sensitive AdaBoost was able to consistently converge to a good solution in the allotted number of iterations. In particular, the
latter produced x∗= −1.4993 when C1 = 20 in two iterations, and x∗= −0.7352
when C1 = 5 in four iterations. The value of the ˆC1 estimate that led to the best
solution was, however, not always the true C1. While when C1 = 5 cost-sensitive
AdaBoost was nearly optimal with ˆC1 = 4.5, near optimal performance in the case
where C1 = 20 required a cost estimate of ˆC1 = 4.7. This mismatch is compliant with
Figure 2, which shows some inability of cost-sensitive AdaBoost to replicate the posterior class probabilities required for optimal performance with highly unbalanced cost
structures (x∗of very large magnitude). This was not observed for cost-sensitive Logit
or RealBoost. These results show that 1) the new algorithms are far superior than those
previously available, and 2) the optimal solution can be found for most cost-sensitive
EXPERIMENTAL EVALUATION
Boosting Iteration
Cost Sensitive AdaBoost
Bayes Rule
Boosting Iteration
Cost Sensitive AdaBoost
Bayes Rule
Figure 3: Decision boundaries produced by the different boosting algorithms for various cost factors. Left: C1 = 5, right: C1 = 20.
problems, but may require (in particular for cost-sensitive AdaBoost) cross-validation
of cost-parameters.
The most plausible explanation for the poor performance of all other algorithms
appears to be the inappropriate choice of the α parameter: while the weight update
rules seemed to produce cost-sensitive weak learners, the incorrect choice of α frequently gave disproportionate weight to weak learners with poor decision boundaries.
For example, in the case of AdaC1, the ﬁrst two weak learners had threshold of 0.0152
and −0.9186 but the corresponding values of α were 0.9056 and 0.2404. Although the
second threshold is close to optimal (x∗= −0.8047), the poor choice of α gave it little
weight, much smaller than that of the ﬁrst. This made the overall decision boundary
close to zero. Of all algorithms tested, only CSB1 and CSB2 achieved performance
comparable to that of cost-sensitive AdaBoost, even though their slow convergence in
this simple problem appears problematic.
Real datasets
Two sets of experiments were performed with real data. The ﬁrst involved a number
of datasets from the UCI repository, while the second addressed the computer vision
problem of face detection. To simplify the comparison of results, the quality of costsensitive classiﬁcation is frequently measured by a scalar metric that weighs errors of
one type more than others. A common metric , which we have adopted, is
ǫ = pfalse + fcost × mmiss
where pfalse is the number of false-positives of the detector, mmiss the number of
misses and fcost > 1 a cost factor that weighs misses more heavily than false positives.
A number of cost factors were considered, and ǫ computed for each combination of
1) cost sensitive boosting method, 2) training cost structure estimates, and 3) true cost
factor fcost used to evaluate performance. By training cost structure estimates we refer
Real datasets
Table 1: Minimum ǫ and corresponding ˆC1 for cost-sensitive RealBoost on Cleveland
fcost = 10
to the parameters used during training, e.g. the parameters ˆC1 and ˆC2 of cost-sensitive
UCI datasets
Ten binary UCI data sets were used: Pima-diabetes, breast cancer diagnostic,
breast cancer prognostic, original Wisconsin breast cancer, liver disorder, sonar, echocardiogram, Cleveland heart disease, tic-tac-toe and Haberman’s survival. In all cases,
ﬁve fold validation was used to ﬁnd the best cost estimate by searching over ˆC1 ∈
 ( ˆC2 = 1). Three cost factors fcost ∈{2, 5, 10} were considered, and the minimum ǫ was found for each. Table 1 gives an example (cost-sensitive RealBoost and
the Cleveland heart disease dataset) of the relationship between the minimum ǫ, fcost,
and the best value of the training cost parameter ˆC1. Performance across the various
values of f was further summarized by computing the average value of the minimum ǫ
achieved by each algorithm.
This average is shown in Table 2 for each of the algorithms considered. The table
also shows the median value of the average across all datasets, and the number of
times that each of the proposed cost-sensitive boosting algorithms outperformed all of
the previously available methods (# of wins). All new algorithms have a median loss
smaller than those in the literature, and outperform all of them in 60 to 90% of the
datasets. Overall, cost-sensitive RealBoost has the lowest median loss, and is the top
performer in 7/10 datasets. Cost-sensitive LogitBoost achieves the best performance in
the remaining three. This is strong evidence for the superiority of the new cost-sensitive
boosting algorithms over those previously available.
Face detection
An important area of application of cost-sensitive learning is the problem of object
detection in computer vision, where boosting has recently emerged as the main tool
for the design of detector cascades . Since a substantial amount of effort has also
been devoted to the design of evaluation protocols in areas like face detection, this is
a good domain in which to test cost-sensitive classiﬁers. We have adopted the protocol of to compare the new cost-sensitive boosting to those previously available.
Given the computational complexity of these experiments we, once again, restricted
the comparison to (the worst-case performer) cost-sensitive AdaBoost. All experiments used a face database of 9832 positive and 9832 negative examples, and weak
learners based on a combination of decision stumps and Haar wavelet features, as described in . 6000 examples were used, per class, for training, the remaining 3832
EXPERIMENTAL EVALUATION
Table 2: Average minimum ǫ for the UCI datasets considered.
being left for testing, and all boosting algorithms were ran for 100 iterations. Four
cost factors (fcost ∈{10, 20, 50, 100}) and a number of training cost structures were
considered. This is illustrated in Figure 4 a) for cost-sensitive AdaBoost. The ﬁgure
presents plots of ǫ as a function of fcost for various training cost structures with ˆC2 = 1
and ˆC1 ∈[1.2, 1000]. Note that detectors trained with larger ˆC1 perform better when
fcost is larger, while smaller ˆC1 lead to best performance when ǫ weighs the two errors
more equally.
Figure 4 b) presents a comparison of the best performances achieved with costsensitive AdaBoost and each of the previously available cost-sensitive boosting methods. The plots were produced by considering four values of fcost and searching for
the cost structure and threshold that achieved the minimum ǫ for each of these values.
The search across cost-structures produces the cost-sensitive detector with classiﬁcation boundary x∗= f −1(0) closest to the optimal boundary for the particular value
of fcost under consideration, and threshold manipulation then enables slight adjustments of this boundary. The inclusion of threshold manipulation also permits a fair
comparison to the combination of a cost-insensitive detector (learned with the standard
AdaBoost algorithm) and threshold manipulation. In fact, because standard AdaBoost
is identical to cost-sensitive AdaBoost with ˆC1 = 1, this is equivalent to disabling the
search over training cost structures.
It is clear that cost-sensitive AdaBoost consistently outperforms all other methods,
cost factor
ˆC 1 = 1.2
ˆC 1 = 400
cost factor
Cost Sensitive
Figure 4: (a) Misclassiﬁcation cost for cost-sensitive boosting under different training
cost structures. (b) Minimum misclassiﬁcation cost of various cost-sensitive boosting
methods on a face detection problem.
for all values of fcost. These results illustrate the importance of choosing the conﬁdence α optimally, at each iteration. Methods that do not use α in the weight update
rule (CSB0 and CSB1) have extremely poor performance. Methods that update α but
are not provably optimal (AdaC2, AdaC3, and AdaCost) perform worse than standard
AdaBoost (or CSB2, which relies on the same α updates). Finally, the combination
of standard AdaBoost and threshold manipulation is not sufﬁcient to match the performance of the optimal cost-sensitive version of AdaBoost, except when the costs of the
two types of errors are approximately equal (small fcost).
Conclusion
We have presented a novel framework for the design of cost-sensitive boosting algorithms. The framework is based on the statistical interpretation of boosting, and derived
with recourse to an asymmetric extension of the logistic transform, which is well motivated from a decision theoretic point of view. The statistical interpretation enables the
derivation of cost-sensitive boosting losses which, similarly to the original AdaBoost
algorithm, can then be minimized by gradient descent in the functional space of convex
combinations of weak learners. The general requirements for optimal cost-sensitive
classiﬁcation were identiﬁed, laying the groundwork for the cost sensitive extension
of many large margin classiﬁcation algorithms. Speciﬁcally, the cost-sensitive extensions of AdaBoost, RealBoost and LogitBoost were derived and shown to satisfy these
requirements.
Experimental evidence, derived from a synthetic problem, standard data sets and
the (timely) problem of face detection, was presented in support of the cost-sensitive
optimality of the new algorithms. The performance of the latter was also compared
to those of various previous cost-sensitive boosting proposals (CSB0, CSB1, CSB2,
PROOF OF LEMMA 2
AdaC1, AdaC2, AdaC3 and AdaCost). Cost-sensitive boosting was shown to consistently outperform all other methods, achieving the smallest misclassiﬁcation cost at all
cost factors considered.
Proof of Lemma 2
To ﬁnd the minimum of the cost-sensitive extension of the exponential loss of (33) it
sufﬁces to search for the the function f(x) of minimum expected loss conditioned on
I(y = 1)e−y.C1f(x) + I(y = −1)e−y.C2f(x)|x
PY |X(1|x)e−C1f(x) + PY |X(−1|x)eC2f(x).
Setting derivatives to zero
−C1PY |X(1|x)e−C1f(x) + C2PY |X(−1|x)eC2f(x) = 0
it follows that
C1PY |X(1|x)
C2PY |X(−1|x) = e(C1+C2)f(x)
log PY |X(1|x)C1
PY |X(−1|x)C2
It is straightforward to show that the second derivative is non-negative, from which the
loss is minimized by f(x).
To ﬁnd the minimum of the cost sensitive extension of the binomial loss of (34) it
sufﬁces to search for the the function f(x) of minimum expected loss conditioned on
−EY |X[y′ log(pc(x)) + (1 −y′) log(1 −pc(x))|x]
−PY |X(1|x) log(pc(x)) −PY |X(0|x) log(1 −pc(x))
with pc(x) given by (35). For this, we ﬁrst compute the minimum with respect to
pc(x), which is given by
−PY |X(1|x)
pc(x) + PY |X(0|x)
1 −pc(x) = 0
1 −pc(x) = log PY |X(1|x)
PY |X(0|x).
Using (35), this is equivalent to
2(γf(x) + η) = log PY |X(1|x)
PY |X(0|x),
log PY |X(1|x)C1
PY |X(0|x)C2
Since ∂2lb(x)
∂pc(x)2 ≥0 and pc(x) is monotonically increasing on f(x) this is a minimum.
Proof of Theorem 3
From (33) the cost function can be written as
EX,Y [I(y = 1) exp(−C1f(x)) + I(y = −1) exp(C2f(x))]
and the addition of the weak learner G(x) = αg(x) to the predictor f(x) results in
EX,Y [I(y = 1)w(x, 1) exp(−C1αg(x))
+I(y = −1)w(x, −1) exp(C2αg(x))]
w(x, 1) = exp(−C1f(x))
w(x, −1) = exp(C2f(x)).
Since J[f + αg] is minimized if and only if the argument of the expectation is minimized for all x, the gradient direction and optimal step size are the solution of
(αm, gm(x)) =
I(y = 1)w(x, 1)e−C1αg(x)
+I(y = −1)w(x, −1)eC2αg(x)|x
PROOF OF THEOREM 3
Noting that
I(y = 1)w(x, 1)e−C1αg(x) + I(y = −1)w(x, −1)eC2αg(x)|x
I(y = 1)I(g(x) = 1)w(x, 1)e−C1α+
I(y = 1)I(g(x) = −1)w(x, 1)eC1α +
I(y = −1)I(g(x) = 1)w(x, −1)eC2α +
I(y = −1)I(g(x) = −1)w(x, −1)e−C2α|x
I(y = 1)I(g(x) = −1)w(x, 1)(eC1α −e−C1α)+
I(y = 1)w(x, 1)e−C1α +
I(y = −1)I(g(x) = 1)w(x, −1)(eC2α −e−C2α) +
I(y = −1)w(x, −1)e−C2α|x
PY |X(1|x)w(x, 1)I(g(x) = −1)(eC1α −e−C1α) +
PY |X(1|x)w(x, 1)e−C1α +
PY |X(−1|x)w(x, −1)I(g(x) = 1)(eC2α −e−C2α) +
PY |X(−1|x)w(x, −1)e−C2α
it follows that
(αm, gm(x))
Y |X(1|x)I(g(x) = −1)(eC1α −e−C1α)+
Y |X(1|x)e−C1α +
Y |X(−1|x)I(g(x) = 1)(eC2α −e−C2α) +
Y |X(−1|x)e−C2αo
Y |X(y|x) =
PY |X(y|x)w(x, y)
y∈{1,−1} PY |X(y|x)w(x, y)
are the posterior estimates associated with a sample reweighed according to w(x, y).
Hence, the weak learner of minimum cost is
Y |X(1|x)I(g(x) = −1)(eC1α −e−C1α) +
Y |X(1|x)e−C1α +
Y |X(−1|x)I(g(x) = 1)(eC2α −e−C2α) +
Y |X(−1|x)e−C2αo
and, replacing expectations by sample averages,
(eC1α −e−C1α) · b + e−C1α · T+ +
(eC2α −e−C2α) · d + e−C2α · T−
with the empirical estimates T+, T−, b and d of (42) - (45). Given g(x), and setting
the derivative with respect to α to zero
C1(eC1α + e−C1α) · b −C1e−C1α · T+ +
C2(eC2α + e−C2α) · d −C2e−C2α · T−= 0
the optimal step size α is the solution of
2C1 · b · cosh(C1α) + 2C2 · d · cosh(C2α) = C1 · T+ · e−C1α + C2 · T−· e−C2α.
Proof of Theorem 4
From (33) the cost function can be written as
EX,Y [I(y = 1) exp(−C1f(x)) + I(y = −1) exp(C2f(x))]
and the addition of the weak learner G(x) to the predictor f(x) results in
EX,Y [I(y = 1)w(x, 1) exp(−C1G(x)) +
I(y = −1)w(x, −1) exp(C2G(x))]
w(x, 1) = exp(−C1f(x))
PROOF OF THEOREM 4
w(x, −1) = exp(C2f(x)).
Since J[f +G] is minimized if and only if the argument of the expectation is minimized
for all x, and assuming that the weak learners depend on x only through some feature
φ(x), the optimal weak learner is the solution of
= arg minG
EY |X[I(y = 1)w(x, 1) exp(−C1G(x))
+I(y = −1)w(x, −1) exp(C2G(x))|x]
= arg minG
PY |X(1|φ(x))w(x, 1) exp(−C1G(x))
+PY |X(−1|φ(x))w(x, −1) exp(C2G(x))
= arg minG
Y |X(1|φ(x)) exp(−C1G(x))
Y |X(−1|φ(x)) exp(C2G(x))
Y |X(y|φ(x)) =
PY |X(y|φ(x))w(x, y)
y∈{1,−1} PY |X(y|φ(x))w(x, y)
are the posterior estimates associated with a sample reweighed according to w(x, y).
Setting the derivatives of the cost to zero it follows that
Y |X(1|φ(x))C1 exp(−C1G(x)) = P (w)
Y |X(−1|φ(x))C2 exp(C2G(x))
Y |X(1|φ(x))C1
Y |X(−1|φ(x))C2
The optimal feature φ∗is the one of smallest minimum cost
φ J[f + Gφ]
φ EX,Y [I(y = 1)w(x, 1) exp(−C1Gφ(x)) +
I(y = −1)w(x, −1) exp(C2Gφ(x))]
w(xi, 1) exp(−C1Gφ(xi)) +
w(xi, −1) exp(C2Gφ(xi))
Once Greal
(x) is found, the weights are updated so as to comply with (64) and (65),
w(x, 1) ←w(x, 1) exp(−C1Gφ∗(x))
w(x, −1) ←w(x, −1) exp(C2Gφ∗(x)).
Proof of Theorem 5
Rewriting the negative loglikelihood as
lb[y′, f(x)] = −EX,Y
1 −pc(x) + log(1 −pc(x))
and using (35), it follows that
lb[y′, f(x)] = −EX,Y
2y′(γf(x) + η) −log
1 + e2(γf(x)+η)ii
This loss is minimized by maximizing the conditional expectation
−lb[y′, f(x)|x]
2y′(γf(x) + η) −log
1 + e2(γf(x)+η)ii
2EY |X[y′|x](γf(x) + η) −log
1 + e2(γf(x)+η)i
for all x, i.e. by searching for the weak learner G(x) that maximizes the cost
J[f(x) + G(x)] = −lb[y′, f(x) + G(x)|x].
The maximization is done by Newton’s method, which requires the computation of the
∂J[f(x) + G(x)]
= 2γ(EY |X[y′|x] −pc(x))
and Hessian
∂2J[f(x) + G(x)]
= −4γ2pc(x)(1 −pc(x))
leading to a Newton update
pc(x)(1 −pc(x))
This is equivalent to solving the least squares problem
pc(x)(1 −pc(x)) −G(x)
REFERENCES
and the optimal weak learner can, therefore, be computed with
PY |X(y′|x)
pc(x)(1 −pc(x)) −G(x)
PY |X(y′|x)w(x)
j=0 PY |X(j|x)w(x)
pc(x)(1 −pc(x)) −G(x)
Y |X(y′|x)
pc(x)(1 −pc(x)) −G(x)
pc(x)(1 −pc(x)) −G(x)
which is the weighted least squares regression of zi to xi using weights wi, as given
by (54) and (55). The optimal feature is the one of smallest regression error.
References
 AGARWAL, S., GRAEPEL, T., HERBRICH, R., HAR-PELED, S., AND ROTH, D.
 . Generalization bounds for the area under the roc curve. The Journal of
Machine Learning Research 6, 393–425.
 BREIMAN, L. . Arcing classiﬁers. The Annals of Statistics 26, 3, 801–849.
 COLLINS, M., SCHAPIRE, R. E., AND SINGER, Y. . Logistic regression,
adaboost and bregman distances. In Journal of Machine Leaning. Vol. 48. 253–285.
 DOMINGOS, P. . Metacost: a general method for making classiﬁers costsensitive. In Knowledge Discovery and Data Mining. 155–164.
 DUDA, R. O., HART, P. E., AND STORK, D. G. . Pattern Classiﬁcation.
John Wiley Sons Inc, New York.
 ELKAN, C. . The foundations of cost-sensitive learning. In Seventeenth
Intrnl. Joint Conference on Artiﬁcial Intelligence. 973–978.
 FAN, W., STOLFO, S., ZHANG, J., AND CHAN, P. . Adacost: Misclassiﬁcation cost-sensitive boosting. In Proc. of 6th International Conf. on Machine
Learning. 97–105.
REFERENCES
 FREUND, Y. AND SCHAPIRE, R. . A decision-theoretic generalization of
on-line learning and an application to boosting. Journal of Computer and System
Sciences 55, 1, part 2, 119–139.
 FREUND, Y. AND SCHAPIRE, R. . A discussion of “Process consistency for
AdaBoost” by Wenxin Jiang, “On the Bayes-risk consistency of regularized boosting methods” by Gbor Lugosi and Nicolas Vayatis, “Statistical behavior and consistency of classiﬁcation methods based on convex risk minimization” by Tong Zhang.
The Annals of Statistics 32, 1.
 FREUND, Y. AND SCHAPIRE, R. E. . Experiments with a new boosting
algorithm. In International Conference on Machine Learning. 148–156.
 FREUND, Y. AND SCHAPIRE, R. E. . Game theory, on-line prediction
and boosting. In Computational Learing Theory. 325–332.
 FRIEDMAN, J., HASTIE, T., AND TIBSHIRANI, R. . Additive logistic
regression: A statistical view of boosting. The Annals of Statistics 38, 337–374.
 FRIEDMAN, J. H. . Greedy function approximation: A gradient boosting
machine. The Annals of Statistics 29, 5, 1189–1232.
 GREEN, D. AND SWETS, J. . Signal detection theory and psychophysics.
John Wiley and Sons Inc., New York.
 GREINER, R., GROVE, A. J., AND ROTH, D. . Learning cost-sensitive
active classiﬁers. Artiﬁcial Intelligence 139, 2, 137–174.
 HANLEY, J. A. AND MCNEIL, B. J. . The meaning and use of the area
under a receiver operating characteristic (roc) curve. Radiology 143, 1, 29–36.
 HASTIE, TIBSHIRANI, AND FRIEDMAN. .
The Elements of Statistical
Learning. Springer-Verlag Inc, New York.
 HSIEH, F. AND TURNBULL, B. W. . Nonparametric and semiparametric estimation of the receiver operating characteristic curve. The Annals of Statistics 24, 1, 25–40.
 JIANG, W. . Process consistency for adaboost. The Annals of Statistics 32,
 KONONENKO, I. . Machine learning for medical diagnosis: history, state
of the art and perspective. Artiﬁcial Intelligence in Medicine 23, 1, 89–109.
 LANE, T. AND BRODLEY, C. E. . An empirical study of two approaches
to sequence learning for anomaly detection. Machine Learning 51, 1, 73–107.
 LIN, H.-T., LIN, C.-J., AND WENG, R. C. . A note on platt’s probabilistic
outputs for support vector machines. Machine Learning 68, 3, 267–276.
REFERENCES
 MA, J., NGUYEN, N., AND RAJAPAKSE, J. . Gene classiﬁcation using
codon usage analysis and support vector machines. IEEE/ACM Transactions on
Computational Biology and Bioinformatics. to be published.
 MARGINEANTU, D. D. AND DIETTERICH, T. G. . Bootstrap methods
for the cost-sensitive evaluation of classiﬁers. In Proc. 17th International Conf. on
Machine Learning. 583–590.
 MASON, L., BAXTER, J., BARTLETT, P., AND FREAN, M. . Boosting
Algorithms as Gradient Descent. In Advances in Neural Information Processing
Systems. 512–518.
 MEASE, D. AND WYNER, A. J. . Evidence contrary to the statistical view
of boosting. Journal of Machine Learning Research. to be published.
 MEASE, D., WYNER, A. J., AND BUJA, A. . Boosted classiﬁcation trees
and class probability/quantile estimation. The Journal of Machine Learning Research 8, 409–439.
 NEWMAN,
repository
databases.
 
 NEYMAN, J. AND PEARSON, E. S. . On the problem of the most efﬁcient
tests of statistical hypotheses. Philosophical Transactions of the Royal Society of
London 231, 289–337.
 NICULESCU-MIZIL, A. AND CARUANA, R. . Obtaining calibrated probabilities from boosting. In Proc. 21st Conference on Uncertainty in Artiﬁcial Intelligence (UAI ’05). AUAI Press, 413–420.
 PARK, S.-B., HWANG, S., AND ZHANG, B.-T. . Mining the risk types of
human papillomavirus (hpv) by adacost. In International Conference on Database
and expert Systems Applications. 403–412.
 PLATT, J. . Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. In Adv. in Large Margin Classiﬁers. 61–74.
 REYZIN, L. AND SCHAPIRE, R. E. . How boosting the margin can also
boost classiﬁer complexity. In International Conference on Machine Learning. 753–
 SCHAPIRE, R. E. . The strength of weak learnability. Machine Learning 5,
 SCHAPIRE, R. E., FREUND, Y., BARTLETT, P., AND LEE, W. S. . Boosting the margin: A new explanation for the effectiveness of voting methods. The
Annals of Statistics 26, 5, 1651–1686.
 SCHAPIRE, R. E. AND SINGER, Y. . Improved boosting using conﬁdencerated predictions. Machine Learning 37, 3, 297–336.
REFERENCES
 SUN, Y., WONG, A. K. C., AND WANG, Y. . Parameter inference of costsensitive boosting algorithms. In Machine Learning and Data Mining in Pattern
Recognition,4th International Conference. 21–30.
 TING, K. M. . A comparative study of cost-sensitive boosting algorithms.
In Proc. 17th International Conf. on Machine Learning. 983–990.
 TREE, H. L. V. . Detection, Estimation and Modulation Theory. John
Wiley and Sons Inc, New York.
 VAPNIK, V. N. . Statistical Learning Theory. John Wiley Sons Inc, New
 VIAENE, S., DERRIG, R. A., AND DEDENE, G. . Cost-sensitive learning
and decision making for massachusetts pip claim fraud data. International Journal
of Intelligent Systems 19, 1197–1215.
 VIOLA, P. AND JONES, M. . Fast and robust classiﬁcation using asymmetric adaboost and a detector cascade. In Advances in Neural Information Processing
System. Vol. 2. 1311–1318.
 VIOLA, P. A. AND JONES, M. J. . Robust real-time face detection. International Journal of Computer Vision 57, 2, 137–154.
 VLAHOU, A., SCHORGE, J. O., GREGORY, B. W., AND COLEMAN, R. L.
 . Diagnosis of ovarian cancer using decision tree classiﬁcation of mass spectral data. Journal of Biomedicine and Biotechnology 2003, 5, 308314.
 WALD, A. . Contributions to the theory of statistical estimation and testing
hypotheses. The Annals of Mathematical Statistics 10, 299–326.
 WANG, L., CHU, F., AND XIE, W. . Accurate cancer classiﬁcation using
expressions of very few genes. IEEE/ACM Transactions on Computational Biology
and Bioinformatics 4, 1, 40–53.
 WU, T.-F., LIN, C.-J., AND WENG, R. C. . Probability estimates for
multi-class classiﬁcation by pairwise coupling. The Journal of Machine Learning
Research 5, 975–1005.
 ZADROZNY, B. AND ELKAN, C. . Learning and making decisions when
costs and probabilities are both unknown.
In 7th International Conference on
Knowledge Discovery and Data Mining. 203–213.
 ZADROZNY, B., LANGFORD, J., AND ABE, N. . Cost-sensitive learning
by cost-proportionate example weighting. In Third IEEE International Conference
on Data Mining. 435–442.
 ZEMEL, R. S. AND PITASSI, T. . A gradient-based boosting algorithm
for regression problems. In Advances in Neural Information Processing Systems.