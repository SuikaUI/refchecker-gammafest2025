HAL Id: hal-01436134
 
 
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
TorchCraft: a Library for Machine Learning Research
on Real-Time Strategy Games
Gabriel Synnaeve, Nantas Nardelli, Alex Auvolat, Soumith Chintala,
Timothée Lacroix, Zeming Lin, Florian Richoux, Nicolas Usunier
To cite this version:
Gabriel Synnaeve, Nantas Nardelli, Alex Auvolat, Soumith Chintala, Timothée Lacroix, et al..
TorchCraft: a Library for Machine Learning Research on Real-Time Strategy Games. 2018. ￿hal-
TorchCraft: a Library for Machine Learning Research
on Real-Time Strategy Games
Gabriel Synnaeve, Nantas Nardelli, Alex Auvolat, Soumith Chintala,
Timothée Lacroix, Zeming Lin, Florian Richoux, Nicolas Usunier
 , 
November 7, 2016
We present TorchCraft, a library that enables deep learning research on Real-Time
Strategy (RTS) games such as StarCraft: Brood War, by making it easier to control these
games from a machine learning framework, here Torch . This white paper argues for
using RTS games as a benchmark for AI research, and describes the design and components
of TorchCraft.
Introduction
Deep Learning techniques have recently enabled researchers to successfully tackle low-level
perception problems in a supervised learning fashion. In the ﬁeld of Reinforcement Learning this
has transferred into the ability to develop agents able to learn to act in high-dimensional input
spaces. In particular, deep neural networks have been used to help reinforcement learning scale
to environments with visual inputs, allowing them to learn policies in testbeds that previously
were completely intractable. For instance, algorithms such as Deep Q-Network (DQN) 
have been shown to reach human-level performances on most of the classic ATARI 2600 games
by learning a controller directly from raw pixels, and without any additional supervision beside
the score. Most of the work spawned in this new area has however tackled environments where
the state is fully observable, the reward function has no or low delay, and the action set is
relatively small. To solve the great majority of real life problems agents must instead be able to
handle partial observability, structured and complex dynamics, and noisy and high-dimensional
control interfaces.
To provide the community with useful research environments, work was done towards
building platforms based on videogames such as Torcs , Mario AI , Unreal’s BotPrize
 , the Atari Learning Environment , VizDoom , and Minecraft , all of which have
allowed researchers to train deep learning models with imitation learning, reinforcement learning
and various decision making algorithms on increasingly diﬃcult problems. Recently there have
also been eﬀorts to unite those and many other such environments in one platform to provide
a standard interface for interacting with them . We propose a bridge between StarCraft:
Brood War, an RTS game with an active AI research community and annual AI competitions
 , and Lua, with examples in Torch (a machine learning library).
 
Real-Time Strategy for Games AI
Real-time strategy (RTS) games have historically been a domain of interest of the planning and
decision making research communities . This type of games aims to simulate
the control of multiple units in a military setting at diﬀerent scales and level of complexity,
usually in a ﬁxed-size 2D map, in duel or in small teams. The goal of the player is to collect
resources which can be used to expand their control on the map, create buildings and units
to ﬁght oﬀenemy deployments, and ultimately destroy the opponents. These games exhibit
durative moves (with complex game dynamics) with simultaneous actions (all players can give
commands to any of their units at any time), and very often partial observability (a “fog of
war”: opponent units not in the vicinity of a player’s units are not shown).
RTS gameplay:
Components RTS game play are economy and battles (“macro” and
“micro” respectively): players need to gather resources to build military units and defeat their
opponents. To that end, they often have worker units (or extraction structures) that can gather
resources needed to build workers, buildings, military units and research upgrades. Workers
are often also builders (as in StarCraft), and are weak in ﬁghts compared to military units.
Resources may be of varying degrees of abundance and importance. For instance, in StarCraft
minerals are used for everything, whereas gas is only required for advanced buildings or military
units, and technology upgrades. Buildings and research deﬁne technology trees (directed acyclic
graphs) and each state of a “tech tree” allow for the production of diﬀerent unit types and the
training of new unit abilities. Each unit and building has a range of sight that provides the
player with a view of the map. Parts of the map not in the sight range of the player’s units are
under fog of war and the player cannot observe what happens there. A considerable part of the
strategy and the tactics lies in which armies to deploy and where.
Military units in RTS games have multiple properties which diﬀer between unit types, such
as: attack range (including melee), damage types, armor, speed, area of eﬀects, invisibility,
ﬂight, and special abilities. Units can have attacks and defenses that counter each others in a
rock-paper-scissors fashion, making planning armies a extremely challenging and strategically
rich process. An “opening” denotes the same thing as in Chess: an early game plan for which
the player has to make choices. That is the case in Chess because one can move only one
piece at a time (each turn), and in RTS games because, during the development phase, one is
economically limited and has to choose which tech paths to pursue. Available resources constrain
the technology advancement and the number of units one can produce. As producing buildings
and units also take time, the arbitrage between investing in the economy, in technological
advancement, and in units production is the crux of the strategy during the whole game.
Related work:
Classical AI approaches normally involving planning and search are extremely challenged by the combinatorial action space and the complex dynamics
of RTS games, making simulation (and thus Monte Carlo tree search) diﬃcult . Other
characteristics such as partial observability, the non-obvious quantiﬁcation of the value of the
state, and the problem of featurizing a dynamic and structured state contribute to making them
an interesting problem, which altogether ultimately also make them an excellent benchmark for
AI. As the scope of this paper is not to give a review of RTS AI research, we refer the reader to
these surveys about existing research on RTS and StarCraft AI .
It is currently tedious to do machine learning research in this domain. Most previous
reinforcement learning research involve simple models or limited experimental settings .
Other models are trained on oﬄine datasets of highly skilled players . Contrary
to most Atari games , RTS games have much higher action spaces and much more structured
states. Thus, we advocate here to have not only the pixels as input and keyboard/mouse
for commands, as in , but also a structured representation of the game state, as in
-- main game engine loop:
while true do
game.receive_player_actions()
game.compute_dynamics()
-- our injected code:
torchcraft.send_state()
torchcraft.receive_actions()
featurize, model = init()
tc = require ’torchcraft’
tc:connect(port)
while not tc.state.game_ended do
tc:receive()
features = featurize(tc.state)
actions = model:forward(features)
tc:send(tc:tocommand(actions))
Figure 1: Simpliﬁed client/server code that runs in the game engine (server, on the left) and
the library for the machine learning library or framework (client, on the right).
 . This makes it easier to try a broad variety of models, and may be useful in shaping loss
functions for pixel-based models.
Finally, StarCraft: Brood War is a highly popular game (more than 9.5 million copies sold)
with professional players, which provides interesting datasets, human feedback, and a good
benchmark of what is possible to achieve within the game. There also exists an active academic
community that organizes AI competitions.
The simplistic design of TorchCraft is applicable to any video game and any machine learning
library or framework. Our current implementation connects Torch to a low level interface 
to StarCraft: Brood War. TorchCraft’s approach is to dynamically inject a piece of code in
the game engine that will be a server. This server sends the state of the game to a client (our
machine learning code), and receives commands to send to the game. This is illustrated in
Figure 1. The two modules are entirely synchronous, but the we provide two modalities of
execution based on how we interact with the game:
Game-controlled - we inject a DLL that provides the game interface to the bots, and one that
includes all the instructions to communicate with the machine learning client, interpreted
by the game as a player (or bot AI). In this mode, the server starts at the beginning of the
match and shuts down when that ends. In-between matches it is therefore necessary to
re-establish the connection with the client, however this allows for the setting of multiple
learning instances extremely easily.
Game-attached - we inject a DLL that provides the game interface to the bots, and we
interact with it by attaching to the game process and communicating via pipes. In this
mode there is no need to re-establish the connection with the game every time, and the
control of the game is completely automatized out of the box, however it’s currently
impossible to create multiple learning instances on the same guest OS.
Whatever mode one chooses to use, TorchCraft is seen by the AI programmer as a library
that provides: connect(), receive() (to get the state), send(commands), and some helper
functions about speciﬁcs of StarCraft’s rules and state representation. TorchCraft also provides
an eﬃcient way to store game frames data from past (played or observed) games so that existing
state (“replays”, “traces”) can be re-examined.
Conclusion
We presented several work that established RTS games as a source of interesting and relevant
problems for the AI research community to work on. We believe that an eﬃcient bridge between
low level existing APIs and machine learning frameworks/libraries would enable and foster
research on such games. We presented TorchCraft: a library that enables state-of-the-art
machine learning research on real game data by interfacing Torch with StarCraft: BroodWar.
TorchCraft has already been used in reinforcement learning experiments on StarCraft, which
led to the results in (soon to be open-sourced too and included within TorchCraft).
Acknowledgements
We would like to thank Yann LeCun, Léon Bottou, Pushmeet Kohli, Subramanian Ramamoorthy,
and Phil Torr for the continuous feedback and help with various aspects of this work. Many
thanks to David Churchill for proofreading early versions of this paper.