Transitive Invariance for Self-supervised Visual Representation Learning
Xiaolong Wang1
Kaiming He2
Abhinav Gupta1
1Carnegie Mellon University
2Facebook AI Research
Learning visual representations with self-supervised
learning has become popular in computer vision. The idea
is to design auxiliary tasks where labels are free to obtain.
Most of these tasks end up providing data to learn speciﬁc
kinds of invariance useful for recognition. In this paper,
we propose to exploit different self-supervised approaches
to learn representations invariant to (i) inter-instance variations (two objects in the same class should have similar
features) and (ii) intra-instance variations (viewpoint, pose,
deformations, illumination, etc.). Instead of combining two
approaches with multi-task learning, we argue to organize
and reason the data with multiple variations. Speciﬁcally,
we propose to generate a graph with millions of objects
mined from hundreds of thousands of videos. The objects
are connected by two types of edges which correspond to
two types of invariance: “different instances but a similar viewpoint and category” and “different viewpoints of
the same instance”. By applying simple transitivity on the
graph with these edges, we can obtain pairs of images exhibiting richer visual invariance. We use this data to train
a Triplet-Siamese network with VGG16 as the base architecture and apply the learned representations to different
recognition tasks. For object detection, we achieve 63.2%
mAP on PASCAL VOC 2007 using Fast R-CNN (compare
to 67.3% with ImageNet pre-training). For the challenging
COCO dataset, our method is surprisingly close (23.5%)
to the ImageNet-supervised counterpart (24.4%) using the
Faster R-CNN framework. We also show that our network
can perform signiﬁcantly better than the ImageNet network
in the surface normal estimation task.
1. Introduction
Visual invariance is a core issue in learning visual representations. Traditional features like SIFT and HOG
 are histograms of edges that are to an extent invariant to
illumination, orientations, scales, and translations. Modern
deep representations are capable of learning high-level invariance from large-scale data , e.g., viewpoint, pose,
deformation, and semantics. These can also be transferred
Intra-instance
Invariance
Inter-instance
Invariance
Intra-instance
Invariance
Transitive
Invariance
More Examples:
Figure 1: We propose to obtain rich invariance by applying simple transitive relations. In this example, two different cars A and B are linked by the features that are good
for inter-instance invariance (e.g., using ); and each car
is linked to another view (A′ and B′) by visual tracking
 . Then we can obtain new invariance from object pairs
⟨A, B′⟩, ⟨A′, B⟩, and ⟨A′, B′⟩via transitivity. We show
more examples in the bottom.
to complicated visual recognition tasks .
In the scheme of supervised learning, human annotations
that map a variety of examples into a single label provide
supervision for learning invariant representations. For example, two horses with different illumination, poses, and
breeds are invariantly annotated as a category of “horse”.
Such human knowledge on invariance is expected to be
learned by capable deep neural networks through
 
carefully annotated data. However, large-scale, high-quality
annotations come at a cost of expensive human effort.
Unsupervised or “self-supervised” learning (e.g., ) recently has attracted increasing interests because the “labels” are free to obtain.
Unlike supervised learning that learns invariance from the
semantic labels, the self-supervised learning scheme mines
it from the nature of the data. We observe that most selfsupervised approaches learn representations that are invariant to: (i) inter-instance variations, which reﬂects the commonality among different instances. For example, relative
positions of patches (see also Figure 3) or channels of
colors can be predicted through the commonality
shared by many object instances; (ii) intra-instance variations. Intra-instance invariance is learned from the pose,
viewpoint, and illumination changes by tracking a single
moving instance in videos . However, either source
of invariance can be as rich as that provided by human annotations on large-scale datasets like ImageNet.
Even after signiﬁcant advances in the ﬁeld of selfsupervised learning, there is still a long way to go compared
to supervised learning. What should be the next steps? It
seems that an obvious way is to obtain multiple sources
of invariance by combining multiple self-supervised tasks,
e.g., via multiple losses. Unfortunately, this na¨ıve solution
turns out to give little improvement (as we will show by
experiments).
We argue that the trick lies not in the tasks but in the way
of exploiting data. To leverage both intra-instance and interinstance invariance, in this paper we construct a huge afﬁnity graph consisting of two types of edges (see Figure 1):
the ﬁrst type of edges relates “different instances of similar
viewpoints/poses and potentially the same category”, and
the second type of edges relates “different viewpoints/poses
of an identical instance”. We instantiate the ﬁrst type of
edges by learning commonalities across instances via the
approach of , and the second type by unsupervised tracking of objects in videos . We set up simple transitive relations on this graph to infer more complex invariance from
the data, which are then used to train a Triplet-Siamese network for learning visual representations.
Experiments show that our representations learned without any annotations can be well transferred to the object
detection task. Speciﬁcally, we achieve 63.2% mAP with
VGG16 when ﬁne-tuning Fast R-CNN on VOC2007,
against the ImageNet pre-training baseline of 67.3%. More
importantly, we also report the ﬁrst-ever result of un-/selfsupervised pre-training models ﬁne-tuned on the challenging COCO object detection dataset , achieving 23.5%
AP comparing against 24.4% AP that is ﬁne-tuned from an
ImageNet pre-trained counterpart (both using VGG16). To
our knowledge, this is the closest accuracy to the ImageNet
pre-training counterpart obtained on object detection tasks.
2. Related Work
Unsupervised learning of visual representations is a research area of particular interest. Approaches to unsupervised learning can be roughly categorized into two main
(i) generative models, and (ii) self-supervised
Earlier methods for generative models include
Anto-Encoders and Restricted Boltzmann
Machines (RBMs) .
For example, Le et
al. trained a multi-layer auto-encoder on a large-scale
dataset of YouTube videos: although no label is provided,
some neurons in high-level layers can recognize cats and
human faces. Recent generative models such as Generative
Adversarial Networks and Variational Auto-Encoders
 are capable of generating more realistic images. The
generated examples or the neural networks that learn to generate examples can be exploited to learn representations of
data .
Self-supervised learning is another popular stream for
learning invariant features. Visual invariance can be captured by the same instance/scene taken in a sequence of
video frames . For
example, Wang and Gupta leverage tracking of objects
in videos to learn visual invariance within individual objects; Jayaraman and Grauman train a Siamese network to model the ego-motion between two frames in a
scene; Mathieu et al. propose to learn representations
by predicting future frames; Pathak et al. train a network to segment the foreground objects where are acquired
via motion cues. On the other hand, common characteristics of different object instances can also be mined from
data . For example, relative positions of
image patches may reﬂect feasible spatial layouts of objects; possible colors can be inferred if the networks
can relate colors to object appearances. Rather than rely on
temporal changes in video, these methods are able to exploit
still images.
Our work is also closely related to mid-level patch clustering and unsupervised discovery of semantic
classes as we attempt to ﬁnd reliable clusters in
our afﬁnity graph. In addition, the ranking function used in
this paper is related to deep metric learning with Siamese
architectures .
Analysis of the two types of invariance.
Our generic
framework can be instantiated by any two self-supervised
methods that can respectively learn inter-/intra-instance invariance. In this paper we adopt Doersch et al.’s context prediction method to build inter-instance invariance,
and Wang and Gupta’s tracking method to build intrainstance invariance. We analyze their behaviors as follows.
The context prediction task in randomly samples a
patch (blue in Figure 3) and one of its eight neighbors (red),
and trains the network to predict their relative position, deparent
intra-instance edge
inter-instance edge
Figure 2: Illustrations for our graph construction.
ﬁrst cluster the object nodes into coarser clusters (namely
“parent” clusters) and then inside each cluster we perform
nearest-neighbor search to obtain “child” clusters consisting of 4 samples. Samples in each child cluster are linked
to each other with the “inter-instance” edges. We add new
samples via visual tracking and link them to the original
objects by “intra-instance” edges.
ﬁned as an 8-way classiﬁcation problem. In the ﬁrst two
examples in Figure 3, the context prediction model is able
to predict that the “leg” patch is below the “face” patch of
the cat, indicating that the model has learned some commonality of spatial layout from the training data. However,
the model would fail if the pose, viewpoint, or deformation of the object is changed drastically, e.g., in the third
example of Figure 3 — unless the dataset is diversiﬁed and
large enough to include gradually changing poses, it is hard
for the models to learn that the changed pose can be of the
same object type.
On the other hand, these changes can be more successfully captured by the visual tracking method presented in
 , e.g., see ⟨A, A′⟩and ⟨B, B′⟩in Figure 1.
tracking an identical instance we cannot associate different
instances of the same semantics. Thus we expect the representations learned in are weak in handling the variations between different objects in the same category.
3. Overview
Our goal is to learn visual representations which capture: (i) inter-instance invariance (e.g., two instances of cats
should have similar features), and (ii) intra-instance invariance (pose, viewpoint, deformation, illumination, and other
variance of the same object instance). We have tried to formulate this as a multi-task (multi-loss) learning problem in
our initial experiments (detailed in Table 2 and 3) and observed unsatisfactory performance. Instead of doing so, we
propose to obtain a richer set of invariance by performing
transitive reasoning on the data.
Our ﬁrst step is to construct a graph that describes the
afﬁnity among image patches. A node in the graph denotes
X=( ),Y=7 X=( ),Y=7 X=( ),Y=2
Figure 3: The context prediction task deﬁned in . Given
two patches in an image, it learns to predict the relative position between them.
an image patch. We deﬁne two types of edges in the graph
that relate image patches to each other. The ﬁrst type of
edges, called inter-instance edges, link two nodes which
correspond to different object instances of similar visual appearance; the second type of edges, called intra-instance
edges, link two nodes which correspond to an identical object captured at different time steps of a track. The solid
arrows in Figure 1 illustrate these two types of edges.
Given the built graph, we want to transit the relations via
the known edges and associate unconnected nodes that may
provide under-explored invariance (Figure 1, dash arrows).
Speciﬁcally, as shown in Figure 1, if patches ⟨A, B⟩are
linked via an inter-instance edge and ⟨A, A′⟩and ⟨B, B′⟩
respectively are linked via “intra-instance” edges, we hope
to enrich the invariance by simple transitivity and relate
three new pairs of: ⟨A′, B′⟩, ⟨A, B′⟩, and ⟨A′, B⟩(Figure 1,
dash arrows).
We train a Triplet-Siamese network that encourages similar visual representations between the invariant samples
(e.g., any pair consisting of A, A′, B, B′) and at the same
time discourages similar visual representations to a third
distractor sample (e.g., a random sample C unconnected
to A, A′, B, B′).
In all of our experiments, we apply
VGG16 as the backbone architecture for each branch
of this Triplet-Siamese network. The visual representations
learned by this backbone architecture are evaluated on other
recognition tasks.
4. Graph Construction
We construct a graph with inter-instance and intrainstance edges. Firstly, we apply the method of on
a large set of 100K unlabeled videos (introduced in )
and mine millions of moving objects using motion cues
(Sec. 4.1). We use the image patches of them to construct
the nodes of the graph.
instantiate
inter-instance
selfsupervised method of that learns context predictions on
a large set of still images, which provide features to cluster
the nodes and set up inter-instance edges (Sec. 4.2). On the
other hand, we connect the image patches in the same visual
track by intra-instance edges (Sec. 4.3).
Figure 4: Some example clustering results. Each row shows
the 4 examples in a child cluster (Sec. 4.2).
4.1. Mining Moving Objects
We follow the approach in to ﬁnd the moving objects in videos. As a brief introduction, this method ﬁrst applies Improved Dense Trajectories (IDT) on videos to
extract SURF feature points and their motion. The video
frames are then pruned if there is too much motion (indicating camera motion) or too little motion (e.g., noisy signals).
For the remaining frames, it crop a 227×227 bounding box
(from ∼600×400 images) which includes the most number
of moving points as the foreground object. However, for
computational efﬁciency, in this paper we rescale the image
patches to 96×96 after cropping and we use them as inputs
for clustering and training.
4.2. Inter-instance Edges via Clustering
Given the extracted image patches which act as nodes,
we want to link them with extra inter-instance edges. We
rely on the visual representations learned from to do this.
We connect the nodes representing image patches which are
close in the feature space. In addition, motivated by the
mid-level clustering approaches , we want to obtain
millions of object clusters with a small number of objects in
each to maintain high “purity” of the clusters. We describe
the implementation details of this step as follows.
We extract the pool5 features of the VGG16 network
trained as in . Following , we use ImageNet without
labels to train this network. Note that because we use a
patch size of 96×96, the dimension of our pool5 feature
is 3×3×512=4608. The distance between samples is calculated by the cosine distance of these features. We want
the object patches in each cluster to be close to each other
in the feature space, and we care less about the differences
between clusters. However, directly clustering millions of
image patches into millions of small clusters (e.g., by Kmeans) is time consuming. So we apply a hierarchical clustering approach (2-stage in this paper) where we ﬁrst group
the images into a relatively small number of clusters, and
then ﬁnd groups of small number of examples inside each
cluster via nearest-neighbor search.
Speciﬁcally, in the ﬁrst stage of clustering, we apply Kmeans clustering with K = 5000 on the image patches. We
then remove the clusters with number of examples less than
100 (this reduces K to 546 in our experiments on the image patches mined from the video dataset). We view these
clusters as the “parent” clusters (blue circles in Figure 2).
Then in the second stage of clustering, inside each parent
cluster, we perform nearest-neighbor search for each sample and obtain its top 10 nearest neighbors in the feature
space. We then ﬁnd any group of samples with a group size
of 4, inside which all the samples are each other’s top-10
nearest neighbors. We call these small clusters with 4 samples “child” clusters (green circles in Figure 2). We then
link these image patches with each other inside a child cluster via “inter-instance” edges. Note that different child clusters may overlap, i.e., we allow the same sample to appear
in different groups. However, in our experiments we ﬁnd
that most samples appear only in one group. We show some
results of clustering in Figure 4.
4.3. Intra-instance Edges via Tracking
To obtain rich variations of viewpoint and deformation
changes of the same object instance, we apply visual tracking on the mined moving objects in the videos as in .
More speciﬁcally, given a moving object in the video, it applies KCF to track the object for N = 30 frames and
obtain another sample of the object in the end of the track.
Note that the KCF tracker does not require any human supervision. We add these new objects as nodes to the graph
and link the two samples in the same track with an intrainstance edge (purple in Figure 2).
5. Learning with Transitions in the Graph
With the graph constructed, we want to link more image
patches (see dotted links in Figure 1) which may be related
via the transitivity of invariance. Objects subject to different levels of invariance can thus be related to each other.
Speciﬁcally, if we have a set of nodes {A, B, A′, B′} where
⟨A, B⟩are connected by an inter-instance edge and ⟨A, A′⟩
and ⟨B, B′⟩are connected by an intra-instance edge, by assuming transitivity of invariance we expect the new pairs of
⟨A, B′⟩, ⟨A′, B⟩, and ⟨A′, B′⟩to share similar high-level
visual representations.
Some examples are illustrated in
Figure 1 and 5.
We train a deep neural network (VGG16) to generates similar visual representations if the image patches are
linked by inter-instance/intra-instance edges or their transitivity (which we call a positive pair of samples). To avoid a
Figure 5: Examples used for training the network. Each
column shows a set of image patches {A, B, A′, B′}. Here,
A and B is linked by an inter-instance edge, and A′/B′ is
linked to A/B via intra-instance edges.
(𝐴, 𝐴′, 𝐶)
3 conv 3 conv
(𝐴, 𝐵′, 𝐶)
Figure 6: Our Triplet-Siamese network. We can feed in the
network with different combinations of examples.
trivial solution of identical representations, we also encourage the network to generate dissimilar representations if a
node is expected to be unrelated. Speciﬁcally, we constrain
the image patches from different “parent” clusters (which
are more likely to have different categories) to have different representations (which we call a negative pair of samples). We design a Triplet-Siamese network with a ranking
loss function such that the distance between related
samples should be smaller than the distance of unrelated
Our Triplet-Siamese network includes three towers of
a ConvNet with shared weights (Figure 6).
tower, we adopt the standard VGG16 architecture to
the convolutional layers, after which we add two fullyconnected layers with 4096-d and 1024-d outputs.
Triplet-Siamese network accepts a triplet sample as its input: the ﬁrst two image patches in the triplet are a positive
pair, and the last two are a negative pair. We extract their
1024-d features and calculate the ranking loss as follows.
Given an arbitrary pair of image patches A and B, we de-
ﬁne their distance as: D(A, B) = 1 −
F (A)·F (B)
∥F (A)∥∥F (B)∥where
F(·) is the representation mapping of the network. With a
triplet of (X, X+, X−) where (X, X+) is a positive pair
and (X, X−) is a negative pair as deﬁned above, we minimize the ranking loss:
L(X, X+, X−) = max{0, D(X, X+) −D(X, X−) + m},
where m is a margin set as 0.5 in our experiments. Although we have only one objective function, we have different types of training examples. As illustrated in Figure
6, given the set of related samples {A, B, A′, B′} (see Figure 5) and a random distractor sample C from another parent cluster, we can train the network to handle, e.g., viewpoint invariance for the same instance via L(A, A′, C) and
invariance to different objects sharing the same semantics
via L(A, B′, C).
Besides exploring these relations, we have also tried to
enforce the distance between different objects to be larger
than the distance between two different viewpoints of the
same object, e.g., D(A, A′) < D(A, B′). But we have not
found this extra relation brings any improvement. Interestingly, we found that the representations learned by our
method can in general satisfy D(A, A′) < D(A, B′) after
6. Experiments
We perform extensive analysis on our self-supervised
representations. We ﬁrst evaluate our ConvNet as a feature extractor on different tasks without ﬁne-tuning . We
then show the results of transferring the representations to
vision tasks including object detection and surface normal
estimation with ﬁne-tuning.
Implementation Details.
To prepare the data for training, we download the 100K videos from YouTube using the
URLs provided by . By mining the moving objects
and tracking in the videos, we obtain ∼10 million image
patches of objects. By applying the transitivity on the graph
constructed, we obtain 7 million positive pairs of objects
where each pair of objects are two different instances with
different viewpoints. We also randomly sample 2 million
object pairs connected by the intra-instance edges.
We train our network with these 9 million pairs of images
using a learning rate of 0.001 and a mini-batch size of 100.
For each pair we sample the third distractor patch from a
different “parent cluster” in the same mini-batch. We use
the network pre-trained in to initialize our convolutional
layers and randomly initialized the fully connected layers.
We train the network for 200K iterations with our method.
6.1. Qualitative Results without Fine-tuning
We ﬁrst perform nearest-neighbor search to show qualitative results. We adopt the pool5 feature of the VGG16
(a) Context Prediction Network
(b) Our Network
(c) ImageNet Pre-trained Network
Figure 7: Nearest-neighbor search on the PASCAL VOC dataset. We extract three types of features: (a) context prediction
network from , (b) network trained with our self-supervised method, and (c) the network pre-trained in the annotated
ImageNet dataset. We show that our network can represent a greater variety (e.g., viewpoints) of objects of the same category.
Figure 8: Top 6 responses for neurons in 4 different convolutional units of our network, visualized using .
network for all methods without any ﬁne-tuning (Figure 7).
We do this experiment on the object instances cropped from
the PASCAL VOC 2007 dataset (trainval). As Figure 7 shows, given an query image on the left, the network
pre-trained with the context prediction task can retrieve
objects of very similar viewpoints. On the other hand, our
network shows more variations of objects and can often retrieve objects with the same class as the query. We also
show the nearest-neighbor results using fully-supervised
ImageNet pre-trained features as a comparison.
We also visualize the features using the visualization
technique of . For each convolutional unit in conv5 3,
we retrieve the objects which give highest activation responses and highlight the receptive ﬁelds on the images. We
visualize the top 6 images for 4 different convolutional units
in Figure 8. We can see these convolutional units are corresponding to different semantic object parts (e.g., fronts of
cars or buses wheels, animal legs, eyes or faces).
6.2. Analysis on Object Detection
We evaluate how well our representations can be transferred to object detection by ﬁne-tuning Fast R-CNN
 on PASCAL VOC 2007 . We use the standard
trainval set for training and test set for testing with
VGG16 as the base architecture. For the detection network,
we initialize the weights of convolutional layers from our
self-supervised network and randomly initialize the fullyconnected layers using Gaussian noise with zero mean and
0.001 standard deviation.
During ﬁne-tuning Fast R-CNN, we use 0.00025 as the
starting learning rate. We reduce the learning rate by 1/10
in every 50K iterations. We ﬁne-tune the network for 150K
iterations. Unlike standard Fast R-CNN where the ﬁrst few
convolutional layers of the ImageNet pre-trained network
are ﬁxed, we ﬁne-tuned all layers on the PASCAL data as
our model is pre-trained in a very different domain (e.g.,
video patches).
We report the results in Table 1.
If we train Fast R-
CNN from scratch without any pre-training, we can only
obtain 39.7% mAP. With our self-supervised trained network as initialization, the detection mAP is increased to
63.2% (with a 23.5 points improvement). Our result compares competitively (4.1 points lower) to the counterpart using ImageNet pre-training (67.3% with VGG16).
As we incorporate the invariance captured from and
 , we also evaluate the results using these two approaches
individually (Table 1). By ﬁne-tuning the context prediction network of , we can obtain 61.5% mAP. To train
the network of , we use exactly the same loss function
and initialization as our approach except that there are only
training examples of the same instance in the same visual
track (i.e., only the samples linked by intra-instance edges
in our graph). Its results is 60.2% mAP. Our result (63.2%)
mAP aero bike
boat bottle
horse mbike persn plant sheep
from scratch
51.7 55.8 21.7 24.0
58.7 59.2 41.1
Vid-Edge 
54.4 58.2 39.6 30.8
58.7 61.9 51.0
Context 
70.8 72.1 54.7 49.7
72.3 76.9 70.8
Tracking 
65.7 73.2 55.4 46.4
74.0 76.9 67.8
68.4 74.6 57.1 49.6
73.5 76.9 73.2
74.4 78.0 65.9 54.4
76.4 78.6 82.5
Table 1: Object detection Average Precision (%) on the VOC 2007 test set using Fast R-CNN (with selective search proposals ):
comparisons among different self-supervised learning approaches.
mAP aero bike
boat bottle
horse mbike persn plant sheep
68.4 74.6 57.1 49.6
73.5 76.9 73.2
70.3 75.9 59.3
Multi-Task
70.0 74.2 57.2 48.4
73.6 77.6 70.7
64.1 74.1 57.5
Ours (15-frame)
70.3 74.1 53.3 47.1
74.6 77.1 67.7
65.6 74.6 57.2
Ours (HOG)
65.8 73.4 54.7 47.7
75.6 77.1 67.6
62.1 72.9 53.9
Table 2: More ablative studies on object detection on the VOC 2007 test set using Fast R-CNN (with selective search proposals ).
is better than both methods. This comparison indicates the
effectiveness of exploiting a greater variety of invariance in
representation learning.
Is multi-task learning sufﬁcient? An alternative way of
obtaining both intra- and inter-instance invariance is to apply multi-task learning with the two losses of and .
Next we compare with this method.
For the task in , we use the same network architecture as our approach; for the task in , we follow their
design of a Siamese network. We apply different fully connected layers for different tasks, but share the convolutional
layers between these two tasks. Given a mini-batch of training samples, we perform ranking among these images as
well as context prediction in each image simultaneously via
two losses. The representations learned in this way, when
ﬁne-tuned with Fast R-CNN, obtain 62.1% mAP (“Multitask” in Table 2). Comparing to only using context prediction (61.5%), the multi-task learning only gives a
marginal improvement (0.6%).
This result suggests that
multi-task learning in this way is not sufﬁcient; organizing and exploiting the relationships of data, as done by our
method, is more effective for representation learning.
How important is tracking? To further understand how
much visual tracking helps, we perform ablative analysis
by making the visual tracks shorter: we track the moving objects for 15 frames instead of by default 30 frames.
This is expected to reduce the viewpoint/pose/deformation
variance contributed by tracking. Our model pre-trained
in this way shows 61.5% mAP (“15-frame” in Table 2)
when ﬁne-tuned for detection. This number is similar to
that of using context prediction only (Table 1). This result is not surprising, because it does not add much new
information for training. It suggests adding stronger viewpoint/pose/deformation invariance is important for learning
Context 
Tracking 
Multi-Task 62.4 63.2
Table 3: Object detection Average Precision (%) on the VOC
2007 test set using joint training Faster R-CNN .
better features for object detection.
How important is clustering? Furthermore, we want to
understand how important it is to cluster images with features learned from still images . We perform another ablative analysis by replacing the features of with HOG
 during clustering. The rest of the pipeline remains exactly the same. The ﬁnal result is 60.4% mAP (“HOG” in
Table 2). This shows that if the features for clustering are
not invariant enough to handle different object instances, the
transitivity in the graph becomes less reliable.
6.3. Object Detection with Faster R-CNN
Although Fast R-CNN has been a popular testbed
of un-/self-supervised features, it relies on Selective Search
proposals and thus is not fully end-to-end. We further evaluate the representations on object detection with
the end-to-end Faster R-CNN where the Region Proposal Network (RPN) may suffer from the features if they
are low-quality.
PASCAL VOC 2007 Results. We ﬁne-tune Faster R-
CNN in 8 GPUs for 35K iterations with an initial learning rate of 0.00025 which is reduced by 1/10 after every
15K iterations. Table 3 shows the results of ﬁne-tuning all
from scratch
Context 
Tracking 
Multi-Task 22.0 42.3
ImageNet (shorter) 23.7 44.5
Table 4: Object detection Average Precision (%, COCO definitions) on COCO minival using joint training Faster R-CNN
 . “(shorter)” indicates a shorter training time (fewer iterations,
61.25K) used by the codebase of .
layers (“All”) and also ablative results on freezing different
levels of convolutional layers (e.g., the column >c3 represents freezing all the layers below and including conv3 x in
VGG16 during ﬁne-tuning). Our method gets even better
results of 65.0% by using Faster R-CNN, showing a larger
gap compared to the counterparts of (62.6%) and 
(62.2%). Noteworthily, when freezing all the convolutional
layers and only ﬁne-tuning the fully-connected layers, our
method (43.1%) is much better than other competitors. And
we again ﬁnd that the multi-task alternative does not work
well for Faster R-CNN.
COCO Results. We further report results on the challenging COCO detection dataset . To the best of our
knowledge this is the ﬁrst work of this kind presented on
COCO detection. We ﬁne-tune Faster R-CNN in 8 GPUs
for 120K iterations with an initial learning rate of 0.001
which is reduced by 1/10 after 80k iterations. This is trained
on the COCO trainval35k split and evaluated on the
minival5k split, introduced by .
We report the COCO results on Table 4. Faster R-CNN
ﬁne-tuned with our self-supervised network obtains 23.5%
AP using the COCO metric, which is very close (<1%) to
ﬁne-tuning Faster R-CNN with the ImageNet pre-trained
counterpart (24.4%). Actually, if the ﬁne-tuning of the ImageNet counterpart follows the “shorter” schedule in the public code (61.25K iterations in 8 GPUs, converted from 490K
in 1 GPU)1, the ImageNet supervised pre-training version
has 23.7% AP and is comparable with ours. This comparison also strengthens the signiﬁcance of our result.
To the best of our knowledge, our model achieves the
best performance reported to date on VOC 2007 and COCO
using un-/self-supervised pre-training.
6.4. Adapting to Surface Normal Estimation
To show the generalization ability of our self-supervised
representations, we adopt the learned network to the surface normal estimation task. In this task, given a single
1 
Median 11.25◦22.5◦30◦
(lower is better)
(higher is better)
from scratch
Context 
Tracking 
Table 5: Results on NYU v2 for per-pixel surface normal
estimation, evaluated over valid pixels.
RGB image as input, we train the network to predict the
normal/orientation of the pixels. We evaluate our method
on the NYUv2 RGBD dataset dataset. We use the of-
ﬁcial split of 795 images for training and 654 images for
testing. We follow the same protocols for generating surface normal ground truth and evaluations as .
To train the network for surface normal estimation, we
apply the Fully Convolutional Network (FCN 32-s) proposed in with the VGG16 network as base architecture.
For the loss function, we follow the design in . Specifically, instead of direct regression to obtain the normal, we
use a codebook of 40 codewords to encode the 3-dimension
normals. Each codeword represents one class thus we turn
the problem into a 40-class classiﬁcation for each pixel. We
use the same hyperparameters as in for training and the
network is ﬁne-tuned for same number of iterations (100K)
for different initializations.
To initialize the FCN model with self-supervised nets,
we copy the weights of the convolutional layers to the corresponding layers in FCN. For ImageNet pre-trained network, we follow by converting the fully connected layers to convolutional layers and copy all the weights. For the
model trained from scratch, we randomly initialize all the
layers with “Xavier” initialization .
Table 5 shows the results. We report mean and median
error for all visible pixels (in degrees) and also the percentage of pixels with error less than 11.25, 22.5 and 30 degrees. Surprisingly, we obtain much better results with our
self-supervised trained network than ImageNet pre-training
in this task (3 to 4% better in most metrics). As a comparison, the network trained in are slightly worse
than the ImageNet pre-trained network. These results suggest that our learned representations are competitive to ImageNet pre-training for high-level semantic tasks, but outperforms it on tasks such as surface normal estimation. This
experiment suggests that different visual tasks may prefer
different levels of visual invariance.
Acknowledgement: This work was supported by ONR
MURI N000141612007 and Sloan Fellowship to AG.