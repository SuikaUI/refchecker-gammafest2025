Relative Entropy Policy Search
Jan Peters, Katharina M¨ulling, Yasemin Alt¨un
Max Planck Institute for Biological Cybernetics, Spemannstr. 38, 72076 T¨ubingen, Germany
{jrpeters,muelling,altun}@tuebingen.mpg.de
Policy search is a successful approach to reinforcement learning. However, policy improvements often result in the loss of
information. Hence, it has been marred by premature convergence and implausible solutions. As ﬁrst suggested in the
context of covariant policy gradients , many of these problems may be addressed by constraining the information loss. In this paper, we continue this
path of reasoning and suggest the Relative Entropy Policy
Search (REPS) method. The resulting method differs significantly from previous policy gradient approaches and yields
an exact update step. It works well on typical reinforcement
learning benchmark problems.
Introduction
Policy search is a reinforcement learning approach that attempts to learn improved policies based on information observed in past trials or from observations of another agent’s
actions .
However, policy
search, as most reinforcement learning approaches, is usually phrased in an optimal control framework where it directly optimizes the expected return. As there is no notion
of the sampled data or a sampling policy in this problem
statement, there is a disconnect between ﬁnding an optimal
policy and staying close to the observed data. In an online
setting, many methods can deal with this problem by staying
close to the previous policy (e.g., policy gradient methods
allow only small incremental policy updates). Hence, approaches that allow stepping further away from the data are
problematic, particularly, off-policy approaches Directly optimizing a policy will automatically result in a loss of data as
an improved policy needs to forget experience to avoid the
mistakes of the past and to aim on the observed successes.
However, choosing an improved policy purely based on its
return favors biased solutions that eliminate states in which
only bad actions have been tried out. This problem is known
as optimization bias . Optimization biases may appear in most on- and off-policy reinforcement
learning methods due to undersampling (e.g., if we cannot
sample all state-actions pairs prescribed by a policy, we will
overﬁt the taken actions), model errors or even the policy
update step itself.
Copyright c⃝2010, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
Policy updates may often result in a loss of essential information due to the policy improvement step. For example, a policy update that eliminates most exploration by taking the best observed action often yields fast but premature
convergence to a suboptimal policy. This problem was observed by Kakade in the context of policy gradients.
There, it can be attributed to the fact that the policy parameter update δθ was maximizing it collinearity δθT ∇θJ to
the policy gradient while only regularized by ﬁxing the Euclidian length of the parameter update δθT δθ = ε to a stepsize ε. Kakade concluded that the identity metric
of the distance measure was the problem, and that the usage of the Fisher information metric F(θ) in a constraint
δθT F(θ)δθ = ε leads to a better, more natural gradient.
Bagnell and Schneider clariﬁed that the constraint
introduced in can be seen as a Taylor expansion of the loss of information or relative entropy between
the path distributions generated by the original and the updated policy. Bagnell and Schneider’s clariﬁcation
serves as a key insight to this paper.
In this paper, we propose a new method based on this insight, that allows us to estimate new policies given a data
distribution both for off-policy or on-policy reinforcement
learning. We start from the optimal control problem statement subject to the constraint that the loss in information
is bounded by a maximal step size. Note that the methods proposed in used a small ﬁxed step size instead.
As we do not work in a parametrized policy gradient framework, we can directly compute a policy update based on all
information observed from previous policies or exploratory
sampling distributions. All sufﬁcient statistics can be determined by optimizing the dual function that yields the equivalent of a value function of a policy for a data set. We show
that the method outperforms the previous policy gradient algorithms as well as SARSA .
Background & Notation
We consider the regular reinforcememt learning setting of a stationary
Markov decision process (MDP) with n states s and m actions a. When an agent is in state s, he draws an action
a ∼π(a|s) from a stochastic policy π. Subsequently, the
Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence (AAAI-10)
agent transfers from state s to s′ with transition probability
p(s′|s, a) = Pa
ss′, and receives a reward r(s, a) = Ra
As a result from these state transfers, the agent may converge
to a stationary state distribution µπ(s) for which
s,a µπ(s)π(a|s)p(s′|s, a) = µπ(s′)
holds under mild conditions, see . The
goal of the agent is to ﬁnd a policy π that maximizes the
expected return
s,a µπ(s)π(a|s)r(s, a),
subject to the constraints of Eq.(1) and that both µπ and π
are probability distributions. This problem is called the optimal control problem; however, it does not include any notion of data as discussed in the previous section. In some
cases, only some features of the full state s are relevant for
the agent. In this case, we only require stationary feature
s,a,s′ µπ(s)π(a|s)p(s′|s, a)φs′ =
s′ µπ(s′)φs′. (3)
Note that when using Cartesian unit vectors us′ of length
n as features φs′ = us′, Eq.(3) will become Eq.(1). Using
features instead of states relaxes the stationarity condition
considerably and often allows a signiﬁcant speed-up while
only resulting in approximate solutions and being highly dependable on the choice of the features. Good features may
be RBF features and tile codes, see .
Relative Entropy Policy Search
We will ﬁrst motivate our approach and, subsequently, give
several practical implementations that will be applied in the
evaluations.
Motivation
Relative entropy policy search (REPS) aims at ﬁnding the
optimal policy that maximizes the expected return based
on all observed series of states, actions and rewards. At
the same time, we intend to bound the loss of information measured using relative entropy between the observed
data distribution q(s, a) and the data distribution pπ(s, a) =
µπ(s)π(a|s) generated by the new policy π. Ideally, we
want to make use of every sample (s, a, s′, r) independently,
hence, we express the information loss bound as
D(pπ||q) =
µπ(s)π(a|s) log µπ(s)π(a|s)
where D(pπ||q) denotes the Kullback-Leibler divergence,
q(s, a) denotes the observed state-action distribution, and ε
is our maximal information loss.
Problem Statement. The goal of relative entropy policy
search is to obtain policies that maximize the expected reward J(π) while the information loss is bounded, i.e.,
π,µπ J(π) =
µπ(s)π(a|s)Ra
µπ(s)π(a|s) log µπ(s)π(a|s)
µπ(s′)φs′ =
µπ(s)π(a|s)Pa
µπ(s)π(a|s).
Both µπ and π are probability distributions and the features
φs′ of the MDP are stationary under policy π.
Without the information loss bound constraint in Eq.(6),
there is no notion of sampled data and we obtain the stochastic control problem where differentiation of the Langrangian
also yields the classical Bellman equation φT
s′θ. In this equation, φT
s θ = Vθ(s) is known today as value function while the Langrangian multipliers θ
become parameters and λ the average return. While such
MDPs may be solved by linear programming , approaches that employ sampled experience cannot
be derived properly from these equations. The key difference to past optimal control approaches lies in the addition
of the constraint in Eq. (6).
As discussed in the introduction, natural policy gradient
may be derived from a similar problem statement. However,
the natural policy gradient requires that ε is small, it can
only be properly derived for the path space formulation and
it can only be derived from a local, second order Taylor approximation of the problem. Stepping away further from the
sampling distribution q will violate these assumptions and,
hence, natural policy gradients are inevitably on-policy1.
The ε can be chosen freely where larger values lead to
bigger steps while excessively large values can destroy the
policy. Its size depends on the problem as well as on the
amount of available samples.
Relative Entropy Policy Search Method
As shown in the appendix, we can obtain a reinforcement
learning algorithm straightforwardly.
Proposed Solution. The optimal policy for Problem
q(s, a) exp
b q(s, b) exp
where δθ(s, a) = Ra
ss′Vθ(s′) −Vθ(s) denotes the
Bellman error. Here, the value function Vs(θ) = θT φs is
determined by minimizing
g(θ, η) = η log
s,aq(s, a)exp
η δθ(s, a)
with respect to θ and η.
The value function Vθ(s) = φT
s θ appears naturally in the
derivation of this formulation (see Appendix). The new error
1Note that there exist sample re-use strategies for larger step
away from q using importance sampling, see , or off-policy approaches such as Q-Learning
(which is known to have problems in approximate, feature-based
learning).
Relative Entropy Policy Search
input: features φ(s), maximal information loss ϵ.
for each policy update
Sampling: Obtain samples (si, ai, s′
i, ri), e.g.,
by observing another policy or being on-policy.
Counting: Count samples to obtain the
sampling distribution q(s, a) = 1
Critic: Evaluate policy for η and θ.
Deﬁne Bellman Error Function:
δθ(s, a) = Ra
Compute Dual Function:
g(θ, η) = η log
s,a q(s, a)eε+ 1
η δθ(s,a)
Compute the Dual Function’s Derivative :
s,a q(s,a)e
η δθ(s,a)(
ss′φs′−φs)
s,a q(s,a)e
s,a q(s, a)eε+ 1
η δθ(s,a)
s,a q(s,a)e
η δθ(s,a) 1
η2 δθ(s,a)
s,a q(s,a)e
Optimize: (θ∗, η∗) = fmin BFGS(g, ∂g, [θ0, η0])
Determine Value Function: Vθ∗(s) = φT
Actor: Compute new policy π(a|s).
q(s,a) exp( 1
η∗δθ∗(s,a))
b q(s,b) exp( 1
η∗δθ∗(s,b)),
Output: Policy π(a|s).
Table 1: Algorithmic description of Relative Entropy Policy Search. This algorithm reﬂects the proposed solution
Note that Ii
sa is an indicator function such that
sa = 1 if s = si and a = ai while Ii
sa = 0 otherwise.
In Table 2, we show a possible application of this method in
policy iteration.
function for the critic in Eq.(10) differs substantially from
traditional temporal difference errors, residual gradient errors and monte-carlo rollout ﬁttings . The presented solution is derived for arbitrary stationary features and is therefore sound with function approximation.
The derived policy is similar to the
Gibbs policy used in policy gradient approaches and in SARSA .
In order to turn proposed solution into algorithms, we
need to efﬁciently determine the solution (θ∗, η∗) of the dual
function g. Eq. (10) can be rewritten as
θ,˜η g(θ, ˜η) = ˜η−1 log
exp (log q(s, a) + ε + ˜ηδθ(s, a)) ,
which is known to be convex as δθ(s, a) is linear in θ. Given that g is convex and
smoothly differentiable, we can determine the optimal solution g(θ∗, η∗) efﬁciently with any standard optimizer such as
Broyden–Fletcher–Goldfarb–Shannon (BFGS) method (denoted in this paper by fmin BFGS(g,∂g,[θ0, η0]) with
∂g = [∂θg, ∂ηg]). The resulting method is given in Table
Sample-based Policy Iteration with REPS
If the REPS algorithm is used in a policy iteration scenario,
one can re-use parts of the sampling distribution q(s, a). As
we know that q(s, a) = µπl(s)πl(a|s) where πl denotes the
last policy in a policy iteration scenario, we can also write
our new policy as
πl+1(a|s) =
πl(a|s) exp
b πl(a|s) exp
As a result, we can also evaluate our policy at states where
no actions have been taken. Setting πl to good locations
allows encoding prior knowledge on the policy. This update has the intuitive interpretation that an increase in logprobability of an action is determined by the Bellman error
minus a baseline similar to its mean, i.e., log πl+1(a|s) =
log πl(a|s) + 1
ηδθ(s, a) −b(s).
Obviously, the algorithm as presented in the previous section would be handicapped by maintaining a high accuracy
model of the Markov decision problem (Ra
ss′). Model
estimation would require covering prohibitively many states
and actions, and it is hard to obtain an error-free model from
data . Furtheremore, in most interesting control problems, we do not intend
to visit all states and take all actions — hence, the number
of samples N may often be smaller than the number of all
state-action pairs mn. Thus, in order to become model-free,
we need to rephrase the algorithm in terms of sample averages instead of the system model.
The next step is hence to replace the summations over
states s, s′, and actions a by summations over samples
(si, ai, s′
It turns out that this step can be accomplished straightforwardly as all components of REPS can
be expressed using sample-based replacements such as
s,a q(s, a)f(s, a) =
i=1 f(si, ai). As the Bellman
error δθ(si, ai) only needs to be maintained for the executed
actions, we can also approximate it using sample averages.
Using these two insights, we can design a generalized policy iteration algorithm that is based on samples while using
the main insights of Relative Entropy Policy Search. The resulting method is shown in Table 2. Note that Table 2 does
not include sample re-use in REPS policy iteration. However, this step may be included straightfowardly as we can
mix data from previous iterations with the current one by using all data in the critic and the sum of all previous policies
in the actor update. While such remixing will require more
policy update steps, it may improve robustness and allow
updates after fewer sampled actions.
Experiments
In the following section, we test our Sample-based Policy Iteration with Relative Entropy Policy Search approach using
(a) Two State Problem
(b) Single Chain Problem
(c) Double Chain Problem
 
 
 
Figure 1: Three different methods are compared on three toy examples. The vanilla policy gradients are signiﬁcantly outperformed due to their slow convergence as already discussed by Bagnell and Schneider for the Two State Problem. Policy
iteration based on Relative Entropy Policy Search (REPS) exhibited the best performance.
ﬁrst several example problems from the literature and, subsequently, on the Mountain Car standard evaluation. Subsequently, we show ﬁrst steps towards a robot application
currently under development.
Example Problems
We compare our approach both to ‘vanilla’ policy gradient
methods and natural policy gradients using several toy problems. As such, we have chosen (i) the Two-State Problem
 , (ii) the Single Chain Problem , and (iii) the Double Chain
Problem . In all of these problems, the optimal policy can be observed straightforwardly
by a human observer but they pose a major challenge for
‘vanilla’ policy gradient approaches.
Two State Problem.
The two state problem has two states
and two actions. If it takes the action that has the same number as its current state, it will remain in this state. If it takes
the action that has the others state’s number, it will transfer to that one. State transfers are punished while staying
in ones’ state will give an immidiate reward that equals the
number of the state. This problem is a derivate of the one
in . The optimal policy can be
observed straightforwardly: always take action 2. See Fig. 1
(a) for more information.
Single Chain Problem.
The Single Chain Problem can be
seen as an extension of the Two State Problem. Here, the
actor may return to state 1 at any point in time by taking
action 2 and receiving a reward of 2. However, if he keeps
using action 1 all the time, he will not receive any rewards
until he reaches state 5 where he obtains the reward of 10
and may remain in state 5. The version presented here was
inspired by Furmston & Barbar . See Fig. 1 (b) for
more information.
Double Chain Problem.
The Double Chain Problem concatinates two single chain problems into one big one were
state 1 is shared. As before, returning to state 1 will yield
a reward 2 and requires taking action 2. If in state 1, action
2 will lead to state 6 and also yield a reward of 2. An action 1 yields a reward 5 in state 9 and a reward 10 in state
5. In all other states, action 1 will yield 0 reward. Note
that this problem differs from 
signiﬁcantly. We have made it purposefully harder for any
incremental method in order to highligh the advantage of the
presented approach. See Fig. 1 (c) for more information.
We used unit features for all methods. For the two policy
gradient approaches a Gibbs policy was employed . On all three problems, we let our policy run until the state distribution has
converged to the stationary distribution. For small problems
like the presented ones, this usually takes less than 200 steps.
Subsequently, we update the policy and resample. We take
highly optimized vanilla policy gradients with minimumvariance baselines and the Natural Actor-Critic with unit basis functions as additional function approximation .
Instead of
a small ﬁxed learning rate, we use an additional momentum term in order to improve the performance. We tuned
all meta-parameters of the gradient methods to maximum
performance. We start with the same random initial policies for all algorithms and average over 150 learning runs.
Nevertheless, similar as in , we directly observe that natural gradient outperforms the vanilla policy gradient. Fur-
Policy Iteration with REPS
input: features φ(s), maximal information loss ϵ,
initial policy π0(a|s).
for each policy update k
Sampling: Obtain N samples (si, ai, s′
i, ri) using
current policy πk(a|s) in an on-policy setting.
Critic: Evaluate policy for η and θ.
for every sample i = 0 to N do:
nδ(si, ai) = nδ(si, ai) + (ri + φT
nΛ(si, ai) = nΛ(si, ai) + (φs′
d(si, ai) = d(si, ai) + 1
Bellman Error Function: δθ(s, a) = nδ(s,a)
Feature Difference: Λ(s, a) = nΛ(s,a)
Compute Dual Function:
g(θ, η) = η log
η δθ(si,ai)
Compute the Dual Function’s Derivative :
η δθ(si,ai)Λ(si,ai)
η δθ(si,ai)
η δθ(si,ai)
η δθ(si,ai) 1
η2 δθ(si,ai)
η δθ(si,ai)
Optimize: (θ∗, η∗) = fmin BFGS(g, ∂g, [θ0, η0])
Determine Value Function: Vθ∗(s) = φT
Actor: Compute new policy πk+1(a|s).
πk+1(a|s) =
πk(a|s) exp( 1
η∗δθ∗(s,a))
b πk(a|s) exp( 1
η∗δθ∗(s,b)),
Output: Optimal policy π∗(a|s).
Table 2: Algorithmic description of Policy Iteration based
on Relative Entropy Policy Search. This version of the algorithm extends the one in Table 1 for practical application.
Note that N is not a ﬁxed number but may change after every iteration.
thermore, we also observe that our REPS policy iteration
yields a signiﬁcantly higher performance.
A comparison
with PoWER was not necessary
as the episodic form of REPS appears to be equivalent to the
applicable version of PoWER. The performance of all three
methods for all three problems is shown in Fig. 1 (a-c).
Mountain-Car Problem
The mountain car problem is a wellknown problem in reinforcement learning.
We adapt the code from and employ the
same tile-coding features for both SARSA and REPS. We
implement our algorithm in the same settings and are able to
show that REPS policy iteration also outperforms SARSA.
While SARSA is superﬁcially quite similar to the presented
Figure 2: Performance on the mountain-car problem.
Figure 3: Simulated setup for learning robot table tennis.
method, it differs signiﬁcantly in two parts, i.e., the critic of
SARSA converges slower, and the additional multiplication
by the previous policy results in a faster pruning of taken
bad actions in the REPS approach. As a result, REPS is
signiﬁcantly faster than SARSA as can be observed in Fig. 2.
Primitive Selection in Robot Table Tennis
Table tennis is a hard benchmark problem for robot learning that includes most difﬁculties of complex skill.
setup is shown in Fig. 3. A key problem in a skill learning system with multiple motor primitives (e.g., many different forehands, backhands, smashes, etc.) is the selection
of task-appropriate primitives triggered by an external stimulus. Here, we have generated a large set of motor primitives
that are triggered by a gating network that selects and generalizes among them similar to a mixture of experts. REPS improves the gating network by reinforcement learning where
any successful hit results as a reward of +1 and for failures
no reward is given. REPS appears to be sensitive to good
initial sampling policies. The results vary considerably with
initial policy performance. When the system starts with an
initial policy that has a success rate of ∼24%, it may quickly
converge prematurely yielding a success rate of ∼39%. If
provided a better initialization, it can reach success rates of
up to ∼59%.
Discussion & Conclusion
In this paper, we have introduced a new reinforcement learning method called Relative Entropy Policy Search. It is derived from a principle as previous covariant policy gradient
methods , i.e., attaining maximal expected reward while bounding the amount of information loss. Unlike parametric gradient method, it allows an
exact policy update and may use data generated while following an unknown policy to generate a new, better policy.
It resembles the well-known reinforcement learning method
SARSA to an extent; however, it can be shown to outperform it as the critic operates on a different, more sound cost
function than traditional temporal difference learning, and
as its weighted “soft-max” policy update will promote successful actions faster than the standard soft-max. We have
shown that the method performs efﬁciently when used in a
policy iteration setup. REPS is sound with function approximation and can be kernelized straightforwardly which offers interesting possibilities for new algorithms. The relation
to PoWER and Reward-Weighted
Regression is not yet fully understood as these methods
minimize D(pπ(τ)||r(τ)q(τ)) which is superﬁcially similar
to maximizing Ep{r(τ)} subject to D(pπ(τ)||q(τ)). Both
methods end up with very similar update equations for the
episodic case. Application of REPS for reinforcement learning of motor primitive selection for robot table tennis has
been successful in simulation.