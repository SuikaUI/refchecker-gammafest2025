Regularizing and Optimizing LSTM Language Models
Stephen Merity 1 Nitish Shirish Keskar 1 Richard Socher 1
Recurrent neural networks (RNNs), such as long
short-term memory networks (LSTMs), serve as
a fundamental building block for many sequence
learning tasks, including machine translation,
language modeling, and question answering. In
this paper, we consider the speciﬁc problem of
word-level language modeling and investigate
strategies for regularizing and optimizing LSTMbased models. We propose the weight-dropped
LSTM which uses DropConnect on hidden-tohidden weights as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method,
wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art
word level perplexities on two data sets: 57.3 on
Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an
even lower state-of-the-art perplexity of 52.8 on
Penn Treebank and 52.0 on WikiText-2.
1. Introduction
Effective regularization techniques for deep learning have
been the subject of much research in recent years. Given
the over-parameterization of neural networks, generalization performance crucially relies on the ability to
regularize the models sufﬁciently.
Strategies such as
dropout and batch normalization
 have found great success and are
now ubiquitous in feed-forward and convolutional neural
networks. Naïvely applying these approaches to the case
of recurrent neural networks (RNNs) has not been highly
successful however. Many recent works have hence been
focused on the extension of these regularization strategies
to RNNs; we brieﬂy discuss some of them below.
1Salesforce Research, Palo Alto, USA. Correspondence to:
Stephen Merity < >.
A naïve application of dropout 
to an RNN’s hidden state is ineffective as it disrupts
the RNN’s ability to retain long term dependencies
 . Gal & Ghahramani propose
overcoming this problem by retaining the same dropout
mask across multiple time steps as opposed to sampling
a new binary mask at each timestep. Another approach
is to regularize the network through limiting updates to
the RNN’s hidden state. One such approach is taken by
Semeniuta et al. wherein the authors drop updates
to network units, speciﬁcally the input gates of the LSTM,
in lieu of the units themselves. This is reminiscent of zoneout where updates to the hidden state
may fail to occur for randomly selected neurons.
Instead of operating on the RNN’s hidden states, one can
regularize the network through restrictions on the recurrent matrices as well.
This can be done either through
restricting the capacity of the matrix or through
element-wise
interactions
 .
Other forms of regularization explicitly act upon activations such as batch normalization ,
recurrent batch normalization , and
layer normalization . These all introduce
additional training parameters and can complicate the training process while increasing the sensitivity of the model.
In this work, we investigate a set of regularization strategies
that are not only highly effective but which can also be used
with no modiﬁcation to existing LSTM implementations.
The weight-dropped LSTM applies recurrent regularization through a DropConnect mask on the hidden-to-hidden
recurrent weights.
Other strategies include the use of
randomized-length backpropagation through time (BPTT),
embedding dropout, activation regularization (AR), and
temporal activation regularization (TAR).
As no modiﬁcations are required of the LSTM implementation these regularization strategies are compatible with
black box libraries, such as NVIDIA cuDNN, which can
be many times faster than naïve LSTM implementations.
Effective methods for training deep recurrent networks
have also been a topic of renewed interest. Once a model
Regularizing and Optimizing LSTM Language Models
has been deﬁned, the training algorithm used is required
to not only ﬁnd a good minimizer of the loss function but
also converge to such a minimizer rapidly. The choice of
the optimizer is even more important in the context of regularized models since such strategies, especially the use
of dropout, can impede the training process. Stochastic
gradient descent (SGD), and its variants such as Adam
 and RMSprop are amongst the most popular training methods.
These methods iteratively reduce the training loss through
scaled (stochastic) gradient steps. In particular, Adam has
been found to be widely applicable despite requiring less
tuning of its hyperparameters. In the context of word-level
language modeling, past work has empirically found that
SGD outperforms other methods in not only the ﬁnal loss
but also in the rate of convergence. This is in agreement
with recent evidence pointing to the insufﬁciency of adaptive gradient methods .
Given the success of SGD, especially within the language
modeling domain, we investigate the use of averaged SGD
(ASGD) which is known to have
superior theoretical guarantees. ASGD carries out iterations similar to SGD, but instead of returning the last iterate
as the solution, returns an average of the iterates past a certain, tuned, threshold T . This threshold T is typically tuned
and has a direct impact on the performance of the method.
We propose a variant of ASGD where T is determined on
the ﬂy through a non-monotonic criterion and show that it
achieves better training outcomes compared to SGD.
2. Weight-dropped LSTM
We refer to the mathematical formulation of the LSTM,
it = σ(W ixt + U iht−1)
ft = σ(W fxt + U fht−1)
ot = σ(W oxt + U oht−1)
˜ct = tanh(W cxt + U cht−1)
ct = it ⊙˜ct + ft ⊙+˜ct−1
ht = ot ⊙tanh(ct)
where [W i, W f, W o, U i, U f, U o] are weight matrices, xt
is the vector input to the timestep t, ht is the current exposed hidden state, ct is the memory cell state, and ⊙is
element-wise multiplication.
Preventing overﬁtting within the recurrent connections of
an RNN has been an area of extensive research in language
modeling. The majority of previous recurrent regularization techniques have acted on the hidden state vector ht−1,
most frequently introducing a dropout operation between
timesteps, or performing dropout on the update to the memory state ct. These modiﬁcations to a standard LSTM prevent the use of black box RNN implementations that may
be many times faster due to low-level hardware-speciﬁc optimizations.
We propose the use of DropConnect 
on the recurrent hidden to hidden weight matrices which
does not require any modiﬁcations to an RNN’s formulation.
As the dropout operation is applied once to the
weight matrices, before the forward and backward pass,
the impact on training speed is minimal and any standard
RNN implementation can be used, including inﬂexible but
highly optimized black box LSTM implementations such
as NVIDIA’s cuDNN LSTM.
By performing DropConnect on the hidden-to-hidden
weight matrices [U i, U f, U o, U c] within the LSTM, we can
prevent overﬁtting from occurring on the recurrent connections of the LSTM. This regularization technique would
also be applicable to preventing overﬁtting on the recurrent
weight matrices of other RNN cells.
As the same weights are reused over multiple timesteps,
the same individual dropped weights remain dropped for
the entirety of the forward and backward pass. The result
is similar to variational dropout, which applies the same
dropout mask to recurrent connections within the LSTM
by performing dropout on ht−1, except that the dropout
is applied to the recurrent weights. DropConnect could
also be used on the non-recurrent weights of the LSTM
[W i, W f, W o] though our focus was on preventing over-
ﬁtting on the recurrent connection.
3. Optimization
SGD is among the most popular methods for training deep
learning models across various modalities including computer vision, natural language processing, and reinforcement learning. The training of deep networks can be posed
as a non-convex optimization problem
where fi is the loss function for the ith data point, w are
the weights of the network, and the expectation is taken
over the data. Given a sequence of learning rates, γk, SGD
iteratively takes steps of the form
wk+1 = wk −γk ˆ∇f(wk),
where the subscript denotes the iteration number and the
ˆ∇denotes a stochastic gradient that may be computed on a
minibatch of data points. SGD demonstrably performs well
in practice and also possesses several attractive theoretical
properties such as linear convergence ,
saddle point avoidance and
Regularizing and Optimizing LSTM Language Models
better generalization performance . For
the speciﬁc task of neural language modeling, traditionally SGD without momentum has been found to outperform
other algorithms such as momentum SGD , Adam , Adagrad and RMSProp by a statistically signiﬁcant margin.
Motivated by this observation, we investigate averaged
SGD (ASGD) to further improve the training process.
ASGD has been analyzed in depth theoretically and many
surprising results have been shown including its asymptotic second-order convergence . ASGD takes steps identical to equation (1) but instead of returning the last iterate as the solution, returns
i=T wi, where K is the total number of iterations and T < K is a user-speciﬁed averaging
Algorithm 1 Non-monotonically Triggered ASGD (NT-
Inputs: Initial point w0, learning rate γ, logging interval L,
non-monotone interval n.
1: Initialize k ←0, t ←0, T ←0, logs ←[]
2: while stopping criterion not met do
Compute stochastic gradient ˆ∇f(wk) and take SGD
if mod(k, L) = 0 and T = 0 then
Compute validation perplexity v.
if t > n and v >
l∈{t−n,··· ,t} logs[l] then
Append v to logs
12: end while
Despite its theoretical appeal, ASGD has found limited
practical use in training of deep networks. This may be in
part due to unclear tuning guidelines for the learning-rate
schedule γk and averaging trigger T . If the averaging is
triggered too soon, the efﬁcacy of the method is impacted,
and if it is triggered too late, many additional iterations may
be needed to converge to the solution. In this section, we
describe a non-monotonically triggered variant of ASGD
(NT-ASGD), which obviates the need for tuning T . Further, the algorithm uses a constant learning rate throughout
the experiment and hence no further tuning is necessary for
the decay scheduling.
Ideally, averaging needs to be triggered when the SGD iterates converge to a steady-state distribution . This is roughly equivalent to the convergence of
SGD to a neighborhood around a solution. In the case of
SGD, certain learning-rate reduction strategies such as the
step-wise strategy analogously reduce the learning rate by
a ﬁxed quantity at such a point. A common strategy employed in language modeling is to reduce the learning rates
by a ﬁxed proportion when the performance of the model’s
primary metric (such as perplexity) worsens or stagnates.
Along the same lines, one could make a triggering decision
based on the performance of the model on the validation
set. However, instead of averaging immediately after the
validation metric worsens, we propose a non-monotonic
criterion that conservatively triggers the averaging when
the validation metric fails to improve for multiple cycles;
see Algorithm 1. Given that the choice of triggering is irreversible, this conservatism ensures that the randomness of
training does not play a major role in the decision. Analogous strategies have also been proposed for learning-rate
reduction in SGD .
While the algorithm introduces two additional hyperparameters, the logging interval L and non-monotone interval n,
we found that setting L to be the number of iterations in
an epoch and n = 5 worked well across various models
and data sets. As such, we use this setting in all of our NT-
ASGD experiments in the following section and demonstrate that it achieves better training outcomes as compared
4. Extended regularization techniques
In addition to the regularization and optimization techniques above, we explored additional regularization techniques that aimed to improve data efﬁciency during training
and to prevent overﬁtting of the RNN model.
4.1. Variable length backpropagation sequences
Given a ﬁxed sequence length that is used to break a data
set into ﬁxed length batches, the data set is not efﬁciently
used. To illustrate this, imagine being given 100 elements
to perform backpropagation through with a ﬁxed backpropagation through time (BPTT) window of 10. Any element
divisible by 10 will never have any elements to backprop
into, no matter how many times you may traverse the data
set. Indeed, the backpropagation window that each element
receives is equal to i mod 10 where i is the element’s index. This is data inefﬁcient, preventing
10 of the data set
from ever being able to improve itself in a recurrent fashion, and resulting in 8
10 of the remaining elements receiving
only a partial backpropagationwindow compared to the full
possible backpropagation window of length 10.
To prevent such inefﬁcient data usage, we randomly select
the sequence length for the forward and backward pass in
two steps. First, we select the base sequence length to be
Regularizing and Optimizing LSTM Language Models
seq with probability p and seq
2 with probability 1−p, where
p is a high value approaching 1. This spreads the starting point for the BPTT window beyond the base sequence
length. We then select the sequence length according to
N(seq, s), where seq is the base sequence length and s is
the standard deviation. This jitters the starting point such
that it doesn’t always fall on a speciﬁc word divisible by
seq or seq
2 . From these, the sequence length more efﬁciently
uses the data set, ensuring that when given enough epochs
all the elements in the data set experience a full BPTT window, while ensuring the average sequence length remains
around the base sequence length for computational efﬁciency.
During training, we rescale the learning rate depending
on the length of the resulting sequence compared to the
original speciﬁed sequence length. The rescaling step is
necessary as sampling arbitrary sequence lengths with a
ﬁxed learning rate favors short sequences over longer ones.
This linear scaling rule has been noted as important for
training large scale minibatch SGD without loss of accuracy and is a component of unbiased
truncated backpropagation through time , samples a
binary dropout mask only once upon the ﬁrst call and then
to repeatedly use that locked dropout mask for all repeated
connections within the forward and backward pass.
While we propose using DropConnect rather than variational dropout to regularize the hidden-to-hidden transition
within an RNN, we use variational dropout for all other
dropout operations, speciﬁcally using the same dropout
mask for all inputs and outputs of the LSTM within a given
forward and backward pass. Each example within the minibatch uses a unique dropout mask, rather than a single
dropout mask being used over all examples, ensuring diversity in the elements dropped out.
4.3. Embedding dropout
Following Gal & Ghahramani , we employ embedding dropout. This is equivalent to performing dropout on
the embedding matrix at a word level, where the dropout is
broadcast across all the word vector’s embedding. The remaining non-dropped-out word embeddings are scaled by
1−pe where pe is the probability of embedding dropout. As
the dropout occurs on the embedding matrix that is used
for a full forward and backward pass, this means that all
occurrences of a speciﬁc word will disappear within that
pass, equivalent to performing variational dropout on the
connection between the one-hot embedding and the embedding lookup.
4.4. Weight tying
Weight tying shares
the weights between the embedding and softmax layer, substantially reducing the total parameter count in the model.
The technique has theoretical motivation 
and prevents the model from having to learn a one-to-one
correspondence between the input and output, resulting in
substantial improvements to the standard LSTM language
4.5. Independent embedding size and hidden size
In most natural language processing tasks, both pretrained and trained word vectors are of relatively low
dimensionality—frequently between 100 and 400 dimensions in size. Most previous LSTM language models tie
the dimensionality of the word vectors to the dimensionality of the LSTM’s hidden state. Even if reducing the word
embedding size was not beneﬁcial in preventing overﬁtting, the easiest reduction in total parameters for a language
model is reducing the word vector size. To achieve this, the
ﬁrst and last LSTM layers are modiﬁed such that their input and output dimensionality respectively are equal to the
reduced embedding size.
4.6. Activation Regularization (AR) and Temporal
Activation Regularization (TAR)
L2-regularization is often used on the weights of the network to control the norm of the resulting model and reduce
overﬁtting. In addition, L2 decay can be used on the individual unit activations and on the difference in outputs
of an RNN at different time steps; these strategies labeled
as activation regularization (AR) and temporal activation
regularization (TAR) respectively . AR
penalizes activations that are signiﬁcantly larger than 0 as
a means of regularizing the network. Concretely, AR is
α L2(m ⊙ht)
where m is the dropout mask, L2(·) = ∥·∥2, ht is the output of the RNN at timestep t, and α is a scaling coefﬁcient.
TAR falls under the broad category of slowness regularizers which penalize the
model from producing large changes in the hidden state.
Regularizing and Optimizing LSTM Language Models
Using the notation from AR, TAR is deﬁned as
β L2(ht −ht+1)
where β is a scaling coefﬁcient. As in Merity et al. ,
the AR and TAR loss are only applied to the output of the
ﬁnal RNN layer as opposed to being applied to all layers.
5. Experiment Details
For evaluating the impact of these approaches, we perform
language modeling over a preprocessed version of the Penn
Treebank (PTB) and the WikiText-2
(WT2) data set .
PTB: The Penn Treebank data set has long been a central
data set for experimenting with language modeling. The
data set is heavily preprocessed and does not contain capital
letters, numbers, or punctuation. The vocabulary is also
capped at 10,000 unique words, quite small in comparison
to most modern datasets, which results in a large number
of out of vocabulary (OoV) tokens.
WT2: WikiText-2 is sourced from curated Wikipedia articles and is approximately twice the size of the PTB data
set. The text is tokenized and processed using the Moses
tokenizer , frequently used for machine
translation, and features a vocabulary of over 30,000 words.
Capitalization, punctuation, and numbers are retained in
this data set.
All experiments use a three-layer LSTM model with 1150
units in the hidden layer and an embedding of size 400. The
loss was averaged over all examples and timesteps. All embedding weights were uniformly initialized in the interval
[−0.1, 0.1] and all other weights were initialized between
H ], where H is the hidden size.
For training the models, we use the NT-ASGD algorithm
discussed in the previous section for 750 epochs with L
equivalent to one epoch and n = 5. We use a batch size
of 80 for WT2 and 40 for PTB. Empirically, we found relatively large batch sizes (e.g., 40-80) performed better than
smaller sizes (e.g., 10-20) for NT-ASGD. After completion, we run ASGD with T = 0 and hot-started w0 as a
ﬁne-tuning step to further improve the solution. For this
ﬁne-tuning step, we terminate the run using the same nonmonotonic criterion detailed in Algorithm 1.
We carry out gradient clipping with maximum norm 0.25
and use an initial learning rate of 30 for all experiments. We
use a random BPTT length which is N(70, 5) with probability 0.95 and N(35, 5) with probability 0.05. The values
used for dropout on the word vectors, the output between
LSTM layers, the output of the ﬁnal LSTM layer, and embedding dropout where (0.4, 0.3, 0.4, 0.1) respectively. For
the weight-dropped LSTM, a dropout of 0.5 was applied to
the recurrent weight matrices. For WT2, we increase the
input dropout to 0.65 to account for the increased vocabulary size. For all experiments, we use AR and TAR values
of 2 and 1 respectively, and tie the embedding and softmax weights. These hyperparameters were chosen through
trial and error and we expect further improvements may be
possible if a ﬁne-grained hyperparameter search were to be
conducted. In the results, we abbreviate our approach as
AWD-LSTM for ASGD Weight-Dropped LSTM.
6. Experimental Analysis
We present the single-model perplexity results for both our
models (AWD-LSTM) and other competitive models in Table 1 and 2 for PTB and WT2 respectively. On both data
sets we improve the state-of-the-art, with our vanilla LSTM
model beating the state of the art by approximately 1 unit
on PTB and 0.1 units on WT2.
In comparison to other recent state-of-the-art models, our
model uses a vanilla LSTM. Zilly et al. propose the
recurrent highway network, which extends the LSTM to allow multiple hidden state updates per timestep. Zoph & Le
 use a reinforcement learning agent to generate an
RNN cell tailored to the speciﬁc task of language modeling, with the cell far more complex than the LSTM.
Independently of our work, Melis et al. apply extensive hyperparameter search to an LSTM based language modeling implementation, analyzing the sensitivity
of RNN based language models to hyperparameters. Unlike our work, they use a modiﬁed LSTM, which caps the
input gate it to be min(1 −ft, it), use Adam with β1 = 0
rather than SGD or ASGD, use skip connections between
LSTM layers, and use a black box hyperparametertuner for
exploring models and settings. Of particular interest is that
their hyperparameterswere tuned individually for each data
set compared to our work which shared almost all hyperparameters between PTB and WT2, including the embedding
and hidden size for both data sets. Due to this, they used
less model parameters than our model and found shallow
LSTMs of one or two layers worked best for WT2.
Like our work, Melis et al. ﬁnd that the underlying LSTM architecture can be highly effective compared
to complex custom architectures when well tuned hyperparameters are used. The approaches used in our work and
Melis et al. may be complementary and would be
worth exploration.
7. Pointer models
In past work, pointer based attention models have been
shown to be highly effective in improving language modeling . Given such
Regularizing and Optimizing LSTM Language Models
Parameters
Validation
Mikolov & Zweig - KN-5
Mikolov & Zweig - KN5 + cache
Mikolov & Zweig - RNN
Mikolov & Zweig - RNN-LDA
Mikolov & Zweig - RNN-LDA + KN-5 + cache
Zaremba et al. - LSTM (medium)
Zaremba et al. - LSTM (large)
Gal & Ghahramani - Variational LSTM (medium)
81.9 ± 0.2
79.7 ± 0.1
Gal & Ghahramani - Variational LSTM (medium, MC)
78.6 ± 0.1
Gal & Ghahramani - Variational LSTM (large)
77.9 ± 0.3
75.2 ± 0.2
Gal & Ghahramani - Variational LSTM (large, MC)
73.4 ± 0.0
Kim et al. - CharCNN
Merity et al. - Pointer Sentinel-LSTM
Grave et al. - LSTM
Grave et al. - LSTM + continuous cache pointer
Inan et al. - Variational LSTM (tied) + augmented loss
Inan et al. - Variational LSTM (tied) + augmented loss
Zilly et al. - Variational RHN (tied)
Zoph & Le - NAS Cell (tied)
Zoph & Le - NAS Cell (tied)
Melis et al. - 4-layer skip connection LSTM (tied)
AWD-LSTM - 3-layer LSTM (tied)
AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer
Table 1. Single model perplexity on validation and test sets for the Penn Treebank language modeling task. Parameter numbers with ‡
are estimates based upon our understanding of the model and with reference to Merity et al. . Models noting tied use weight tying
on the embedding and softmax weights. Our model, AWD-LSTM, stands for ASGD Weight-Dropped LSTM.
Parameters
Validation
Inan et al. - Variational LSTM (tied) (h = 650)
Inan et al. - Variational LSTM (tied) (h = 650) + augmented loss
Grave et al. - LSTM
Grave et al. - LSTM + continuous cache pointer
Melis et al. - 1-layer LSTM (tied)
Melis et al. - 2-layer skip connection LSTM (tied)
AWD-LSTM - 3-layer LSTM (tied)
AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer
Table 2. Single model perplexity over WikiText-2. Models noting tied use weight tying on the embedding and softmax weights. Our
model, AWD-LSTM, stands for ASGD Weight-Dropped LSTM.
Regularizing and Optimizing LSTM Language Models
substantial improvements to the underlying neural language model, it remained an open question as to how effective pointer augmentation may be, especially when improvements such as weight tying may act in mutually exclusive ways.
The neural cache model can be added
on top of a pre-trained language model at negligible cost.
The neural cache stores the previous hidden states in memory cells and then uses a simple convex combination of
the probability distributions suggested by the cache and the
language model for prediction. The cache model has three
hyperparameters: the memory size (window) for the cache,
the coefﬁcient of the combination (which determines how
the two distributions are mixed), and the ﬂatness of the
cache distribution. All of these are tuned on the validation
set once a trained language model has been obtained and
require no training by themselves, making it quite inexpensive to use. The tuned values for these hyperparameters
were for PTB and (3785, 0.1279, 0.662)
for WT2 respectively.
In Tables 1 and 2, we show that the model further improves
the perplexity of the language model by as much as 6 perplexity points for PTB and 11 points for WT2. While this
is smaller than the gains reported in Grave et al. ,
which used an LSTM without weight tying, this is still a
substantial drop. Given the simplicity of the neural cache
model, and the lack of any trained components, these results suggest that existing neural language models remain
fundamentally lacking, failing to capture long term dependencies or remember recently seen words effectively.
To understand the impact the pointer had on the model,
speciﬁcally the validation set perplexity, we detail the contribution that each word has on the cache model’s overall
perplexity in Table 3. We compute the sum of the total difference in the loss function value (i.e., log perplexity) between the LSTM-only and LSTM-with-cache models for
the target words in the validation portion of the WikiText-2
data set. We present results for the sum of the difference as
opposed to the mean since the latter undesirably overemphasizes infrequently occurring words for which the cache
helps signiﬁcantly and ignores frequently occurring words
for which the cache provides modest improvements that cumulatively make a strong contribution.
The largest cumulative gain is in improving the handling
of <unk> tokens, though this is over 11540 instances. The
second best improvement, approximately one ﬁfth the gain
given by the <unk> tokens, is for Meridian, yet this word
only occurs 161 times. This indicates the cache still helps
signiﬁcantly even for relatively rare words, further demonstrated by Churchill, Blythe, or Sonic. The cache is not
beneﬁcial when handling frequent word categories, such as
punctuation or stop words, for which the language model is
Australian
Mississippi
Table 3. The sum total difference in loss (log perplexity) that a
given word results in over all instances in the validation data set
of WikiText-2 when the continuous cache pointer is introduced.
The right column contains the words with the twenty best improvements (i.e., where the cache was advantageous), and the left
column the twenty most deteriorated (i.e., where the cache was
disadvantageous).
likely well suited. These observations motivate the design
of a cache framework that is more aware of the relative
strengths of the two models.
8. Model Ablation Analysis
In Table 4, we present the values of validation and testing perplexity for different variants of our best-performing
LSTM model. Each variant removes a form of optimization
or regularization.
The ﬁrst two variants deal with the optimization of the language models while the rest deal with the regularization.
For the model using SGD with learning rate reduced by 2
using the same nonmonotonic fashion, there is a signiﬁcant degradation in performance. This stands as empirical
evidence regarding the beneﬁt of averaging of the iterates.
Using a monotonic criterion instead also hampered performance. Similarly, the removal of the ﬁne-tuning step expectedly also degrades the performance. This step helps
improve the estimate of the minimizer by resetting the
memory of the previous experiment. While this process of
ﬁne-tuning can be repeated multiple times, we found little
beneﬁt in repeating it more than once.
The removal of regularization strategies paints a similar
picture; the inclusion of all of the proposed strategies
Regularizing and Optimizing LSTM Language Models
Validation
Validation
AWD-LSTM (tied)
– ﬁne-tuning
– variable sequence lengths
– embedding dropout
– weight decay
– full sized embedding
– weight-dropping
Table 4. Model ablations for our best LSTM models reporting results over the validation and test set on Penn Treebank and WikiText-2.
Ablations are split into optimization and regularization variants, sorted according to the achieved validation perplexity on WikiText-2.
was pivotal in ensuring state-of-the-art performance. The
most extreme perplexity jump was in removing the hiddento-hidden LSTM regularization provided by the weightdropped LSTM. Without such hidden-to-hidden regularization, perplexity rises substantially, up to 11 points.
This is in line with previous work showing the necessity of recurrent regularization in state-of-the-art models
 .
We also experiment with static sequence lengths which we
had hypothesized would lead to inefﬁcient data usage. This
also worsens the performance by approximately one perplexity unit. Next, we experiment with reverting to matching the sizes of the embedding vectors and the hidden
states. This signiﬁcantly increases the number of parameters in the network (to 43M in the case of PTB and 70M
for WT2) and leads to degradation by almost 8 perplexity
points, which we attribute to overﬁtting in the word embeddings. While this could potentially be improved with
more aggressive regularization, the computational overhead involved with substantially larger embeddings likely
outweighs any advantages. Finally, we experiment with the
removal of embedding dropout, AR/TAR and weight decay.
In all of the cases, the model suffers a perplexity increase
of 2–6 points which we hypothesize is due to insufﬁcient
regularization in the network.
9. Conclusion
In this work, we discuss regularization and optimization
strategies for neural language models.
We propose the
weight-dropped LSTM, a strategy that uses a DropConnect
mask on the hidden-to-hidden weight matrices, as a means
to prevent overﬁtting across the recurrent connections. Further, we investigate the use of averaged SGD with a nonmonontonic trigger for training language models and show
that it outperforms SGD by a signiﬁcant margin. We investigate other regularization strategies including the use
of variable BPTT length and achieve a new state-of-the-art
perplexity on the PTB and WikiText-2 data sets. Our models outperform custom-built RNN cells and complex regularization strategies that preclude the possibility of using
optimized libraries such as the NVIDIA cuDNN LSTM.
Finally, we explore the use of a neural cache in conjunction with our proposed model and show that this further
improves the performance, thus attaining an even lower
state-of-the-art perplexity. While the regularization and optimization strategies proposed are demonstrated on the task
of language modeling, we anticipate that they would be
generally applicable across other sequence learning tasks.