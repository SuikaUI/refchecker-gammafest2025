DEEP LEARNING FOR TIME SERIES FORECASTING: TUTORIAL
AND LITERATURE SURVEY
Konstantinos Benidis∗
Amazon Research
Berlin, Germany
 
Syama Sundar Rangapuram
Amazon Research
Berlin, Germany
 
Valentin Flunkert
Amazon Research
Berlin, Germany
 
Yuyang Wang
Amazon Research
East Palo Alto, CA, USA
 
Danielle Maddix
Amazon Research
East Palo Alto, CA, USA
 
Caner Turkmen
Amazon Research
Berlin, Germany
 
Jan Gasthaus
Amazon Research
Berlin, Germany
 
Michael Bohlke-Schneider
Amazon Research
Berlin, Germany
 
David Salinas
Amazon Research
Berlin, Germany
 
Lorenzo Stella
Amazon Research
Berlin, Germany
 
Franc¸ois-Xavier Aubet
Amazon Research
Berlin, Germany
 
Laurent Callot
Amazon Research
Berlin, Germany
 
Tim Januschowski∗†
Zalando SE
Berlin, Germany
 
Deep learning based forecasting methods have become the methods of choice in many applications
of time series prediction or forecasting often outperforming other approaches. Consequently, over
the last years, these methods are now ubiquitous in large-scale industrial forecasting applications
and have consistently ranked among the best entries in forecasting competitions (e.g., M4 and M5).
This practical success has further increased the academic interest to understand and improve deep
forecasting methods. In this article we provide an introduction and overview of the ﬁeld: We present
important building blocks for deep forecasting in some depth; using these building blocks, we then
survey the breadth of the recent deep forecasting literature.
Keywords Time series · Forecasting · Deep learning
Introduction
Forecasting is the task of extrapolating time series into the future. It has many important applications such as
forecasting the demand for items sold by retailers , the ﬂow of trafﬁc ,
the demand and supply of energy , or the covariance matrix, volatility and long-tail distributions
in ﬁnance . As such, it is a well-studied area (e.g., see for an introduction) with its own
dedicated research community. The machine learning, data science, systems, and operations research communities
as well as application-speciﬁc research communities have also studied the problem intensively (e.g., see a series of
recent tutorials ). In contrast to traditional forecasting applications, modern incarnations often exhibit
∗Equal contribution.
†Work done while at AWS.
 
Benidis et al.
large panels of related time series, all of which need to be forecasted simultaneously . Although these problem
characteristics make them amenable to deep learning or neural networks (NNs), as in many other domains over the
course of history, NNs were not always a standard tool to tackle such problems. Indeed, their effectiveness has
historically been regarded as mixed (e.g., ).
The history of NNs starts in 1957 and in 1964 for NNs in forecasting . Since then, interest in NNs has
oscillated, with upsurges in attention attributable to breakthroughs. The application of NNs in time series forecasting
has followed the general popularity, typically with a lag of a few years. Examples of such breakthroughs include
Rumelhart et al. that popularized the training of multilayer perceptrons (MLPs) using back-propagation.
Signiﬁcant advances were made subsequently such as the use of convolutional NNs (CNNs) , and Long Short
Term Memory (LSTM) cells that address the issue of recurrent NNs’ (RNNs) training, just to name a few. Despite
these advances, NNs remained hard to train and difﬁcult to work with. Methods such as Support Vector Machines
(SVMs) and Random Forests that were developed in the 1990s proved to be highly effective (LeCun et al.
 found that SVMs were as good as the best designed NNs available at the time) and were supported by attractive
theory. This shifted the interest of researchers away from NNs. Forecasting was no exception and results obtained with
NNs were mostly mixed as reﬂected in a highly cited review . The breakthrough that marked the dawn of the
deep learning era came in 2006 when Hinton et al. showed that it was possible to train NNs with a large number
of layers (deep) if the weights are initialized appropriately. Accordingly, deep learning has had a sizable impact on
forecasting and NNs have long entered the canon of standard techniques for forecasting . New models
speciﬁcally designed for forecasting tasks have been proposed, taking advantage of deep learning to supercharge
classical forecasting models or to develop entirely novel approaches. This recent burst of attention on deep forecasting
models is the latest twist in a long and rich history.
Driven by the availability of (closed-source) large time series panels, the potential of deep forecasting models, i.e.,
forecasting models based on NNs, has been exploited primarily in applied industrial research divisions over the last
years .3 With the overwhelming success of deep forecasting methods in the M4 competition ,
this has convinced also formerly skeptical academics . In the most recent M5 competition, deep forecasting
methods were the second and third placed solutions although the competition was otherwise dominated by treebased forecasting methods such as LightGBM and XGBoost , see e.g., . Modern software frameworks have sped up the development of NN models and dedicated forecasting packages available .
While the history of NNs for forecasting is rich, the focus of this article is on more recent developments in NN for
forecasting, roughly since the time that the term “deep learning” was coined. As such, we do not attempt to give
a complete historical overview and sacriﬁce comprehensiveness for recency. The main objectives of this article are
to educate on, review and popularize the recent developments in forecasting driven by NNs for a general audience.
Therefore, we place emphasis on an educational aspect via a tutorial of deep forecasting in the ﬁrst part (Section 2).
In the second part, Section 3, we provide an overview of the state-of-the-art of modern deep forecasting models. Our
exposition is driven by an attempt to identify the main building blocks of modern deep forecasting models which
hopefully enables the reader to digest the rapidly increasing literature more easily. We do not attempt a taxonomy of
all existing methods and our selection of the building blocks is opinionated, motivated by our experience of innovating
in this area with a strong focus on practical applicability. Compared with other surveys , we provide a
more comprehensive overview with a particular focus on recent, advanced topics. Finally, in Section 4, we conclude
and speculate on potentially fruitful areas for future research.
Deep Forecasting: A Tutorial
In the following, we formalize the forecasting problem, summarize those advances in deep learning that we deem as
the most relevant for forecasting, expose important building blocks for NNs and discuss archetypal models in detail.
For general improvements that fueled the deep learning renaissance, like weight initialization, optimization algorithms
or general-purpose components such as activation functions, we refer to standard textbooks like . We are aware to
be opinionated in both the selection of topics as well as the style of exposition. We attempt to take a perspective akin
to a deep forecasting model builder who would compose a forecasting model out of several building blocks such as
NN architectures, input transformations and output representations. Although not all models will ﬁt perfectly into this
exposition, it is our hope that this downside is outweighed by the beneﬁt of allowing the inclined reader to invent new
models more easily.
3Forecasting is an example of a sub-discipline in the machine learning community where the comparatively modest attention it
receives in published research is in stark contrast to a tremendous business impact.
Benidis et al.
Notation and Formalization of the Forecasting Problem
Matrices, vectors and scalars are denoted by uppercase bold, lowercase bold and lowercase normal letters, i.e., X, x
and x, respectively. Let Z = {zi,1:Ti}N
i=1 be a set of N univariate time series, where zi,1:Ti = (zi,1, . . . , zi,Ti), zi,t
is the value of the i-th time series at time t and Zt1:t2 the values of all N time series at the time slice [t1, t2]. Typical
examples for the domain of the time series values include R, N, Z, . The set of time series is associated with a
set of covariate vectors denoted by X = {Xi,1:Ti}N
i=1, with xi,t ∈Rdx. Note that each vector xi,t can include both
time-varying or static features. We denote by α a general input in a model (that can be any combination of covariates
and lagged values of the target) and by β a general output. Since α and β refer to a general case, we always represent
them with lowercase normal letters. We denote by θ the parameters of a model (e.g., parameters of a distribution) and
by Φ the learnable free parameters of the underlying NN (e.g., the weights and biases).
In the most general form, the object of interest in forecasting is the conditional distribution
p(Zt+1:t+h|Z1:t, X1:t+h; θ),
where θ are the parameters of a (probabilistic) model. Eq. (1) is general in the sense that each zi ∈Z is multidimensional (the length of the time series), Z is multivariate (the number of time series |Z| = N > 1) and the forecast
is multi-step (h steps). Varying degrees of simpliﬁcation of Eq. (1) are considered in the literature, for example by
assuming factorizations of p and different ways of estimating θ. In the following, we present the three archetypical
models for addressing Eq. (1).
Local univariate model: A separate (local) model is trained independently for each of the N time series, modelling
the predictive distribution
p(zi,t+1:t+h|zi,1:t, Xi,1:t+h; θi),
θi = Ψ(zi,1:t, Xi,1:t+h),
where Ψ is a generic function mapping input features to the parameters θi of the probabilistic model that are local
to the i-th time series. Note that one may use multidimensional covariates xi,t for each of the N models, but they
are still solving a univariate problem, i.e., forecasting only one time series. The use of covariates common to all N
models is possible but any pattern that is learned in one model is not used in another (unless provided explicitly which
prohibits parallel training). Many classical approaches fall into this category and traditionally NNs were employed in
this local fashion (e.g., ). Note that this approach is not suitable for cold start problems: i.e., forecasting a time
series without historical values.
Global univariate model: A single, global model is trained using available data from all N time series.
However, the model is still used to predict a univariate target. It does not produce joint forecasts of all time series but
forecasts of any single time series at a time. This is also sometimes referred to as a cross-learning approach, e.g., .
In a more general form, global univariate models specialize Eq. (1) to
p(zi,t+1:t+h|Z1:t, X1:t+h; θi),
θi = Ψ(zi,1:t, Xi,1:t+h, Φ),
where Φ are shared parameters among all N time series.
In this article, Ψ in global models is usually a NN and Xi include item-speciﬁc features to allow the model to distinguish between the time series. Although the parameters θi of the probabilistic model for each time series are different,
they are still predicted using shared parameters (or weights) Φ in Ψ. This allows for efﬁcient learning since the model
pools information from all time series and in particular improves inference for shorter time series compared to local
univariate models. Such a model is expected to learn some advanced features (“embeddings”) exploiting information
across time series. Once these advanced features are learned via Ψ, the global model is then used to forecast each
time series independently. That is, although during training the model sees all the related time series together, the
prediction is done by looking at each time series individually. Note that the embeddings learned in the global model
are useful beyond the N time series used in the training. This addresses the cold start problem in the sense that the
global model can be used to provide forecasts for time series without historical values. Global models are also referred
to as cross-learning or panel models in econometrics and statistics and have been the subject of considerable study,
e.g., via dynamic factor models .
Multivariate model: Here, a single model is learned for all N time series using all available data, directly predicting
the multivariate target:
p(Zt+1:t+h|Z1:t, X1:t+h; θ),
θ = Ψ(Z1:t, X1:t+h, Φ).
Note that the model also learns the dependency structure among the time series. Technically speaking, Eq. (4) is a
global multivariate model and a further distinction from local multivariate models, such as VARMA , is possible.
Benidis et al.
Table 1: Summary of deep forecasting models based on forecast and model type. For one-step and multi-step forecasting models h = 1 and h > 1, respectively.
Forecast type
Model type
Formulation
Local univariate
ˆzi,t+1:t+h = Ψ(zi,1:t, Xi,1:t+h)
Global univariate
ˆzi,t+1:t+h = Ψ(zi,1:t, Xi,1:t+h, Φ)
Multivariate
ˆZt+1:t+h = Ψ(Z1:t, X1:t+h, Φ)
Probabilistic
Local univariate
P(zi,t+1:t+h|zi,1:t, Xi,1:t+h; θi),
θi = Ψ(zi,1:t, Xi,1:t+h)
Global univariate
P(zi,t+1:t+h|Z1:t, X1:t+h; θi),
θi = Ψ(zi,1:t, Xi,1:t+h, Φ)
Multivariate
P(Zt+1:t+h|Z1:t, X1:t+h; θ),
θ = Ψ(Z1:t, X1:t+h, Φ)
Note that in Eq. (1) and in the following model-speciﬁc cases we have chosen the multi-step ahead predictive distribution. We can always obtain a multi-step predictive distribution via a rolling one-step predictive distribution. In our discussion so far, we presented probabilistic forecast models that learn the entire distribution of
the future values. However, it may be desirable to model speciﬁc values such as the mean, median or some other
quantile, instead of the whole probability distribution. These are called point-forecast models and the optimal choice
of the summary statistics to turn a probabilistic forecast into a point forecast depends on the metric used to judge
the quality of the point forecast . More concretely, a point-forecast global univariate model learns a quantity
ˆzi,t+1:t+h = Ψ(zi,1:t, Xi,1:t+h, Φ), where ˆzi,t+1:t+h is some point estimate of the future values of the time series.
Table 1 summarizes the various modelling option based on the forecast and model types.
Neural Network Architectures
NNs are compositions of differentiable functions formed from simple building blocks to learn an approximation of
some unknown function from data. An NN is commonly represented as a directed acyclic graph consisting of nodes
and edges. The edges between the nodes contain weights (also called parameters) that are learned from the data. The
basic unit of every NN is a neuron (illustrated in Fig. 1a), consisting of an input, an afﬁne transformation with learnable
weights and (optionally) a nonlinear activation function. Different types of NNs arrange these components in different
ways. We refer to other reviews for more details on the main architectures. Here, we only offer a high-level
summary for completeness, focusing instead on forecasting speciﬁc ingredients for NNs such as input processing and
loss functions.
Multilayer perceptron
In multilayer perceptrons (MLPs) or synonymously feedforward NNs, layers of neurons are stacked on top of each
other to learn more complex nonlinear representations of the data. An MLP consists of an input and an output layer,
while the intermediate layers are called hidden. The nodes in each layer of the network are fully connected to all the
nodes in the previous layer. The output of the last hidden layer can be seen as some nonlinear feature representation
(also called an embedding) obtained from the inputs of the network. The output layer then learns a mapping from
these nonlinear features to the actual target. Learning with MLPs, and more generally with NNs, can be thought of
as the process of learning a nonlinear feature map of the inputs and the relationship between this feature map and the
actual target. Figure 1b illustrates the structure of an MLP with two hidden layers. Modern incarnations of the MLP
have added important details to alleviate problems like vanishing gradients . For example, ResNet , contains
direct connections between hidden layers ℓ−1 and ℓ+ 1, skipping over the hidden layer ℓ.
One of the main limitations of MLPs is that they do not exploit the structure often present in the data in applications
such as computer vision, natural language processing and forecasting. Moreover, the number of inputs and outputs is
ﬁxed making them inapplicable to problems with varying input and output sizes as in forecasting. Next, we discuss
more complex architectures that overcome these limitations, for which MLPs are often used as the basic building
Convolutional neural networks
Convolutional neural networks (CNNs) are a special class of NNs that are designed for applications where
inputs have a known ordinal structure such as images and time series . CNNs are locally connected NNs that use
convolutional layers to exploit the structure present in the input data by applying a convolution function to smaller
neighborhoods of the input data. Convolution here refers to the process of computing moving weighted sums by
Benidis et al.
Activation
(a) Single node
Figure 1: (a) Structure of a single node or neuron. An afﬁne transformation is applied to the input followed by an
activation function, i.e., β = f (P αiwi + b). The weights and bias parameters are learned during training. (b)
Illustration of the MLP structure. Each circle in the hidden and output layers is a node, i.e., it applies an afﬁne
transformation followed by a nonlinear activation to the set of its inputs.
sliding the so-called ﬁlter or kernel over different parts of the input data. The size of the ﬁlter as well as how the ﬁlter
is slid across the input are part of the hyperparameters of the model. A nonlinear activation, typically ReLU , is
then applied to the output of the convolution operation.
In addition to convolutional layers, CNNs also typically use a pooling layer to reduce the size of the feature representation as well as to make the features extracted from the convolutional layer more robust. For example, a commonly
used max-pooling layer, which is applied to the output of convolutional layers, extracts the maximum value of the
features in a given neighborhood. Similarly to the convolution operation, the pooling operation is applied to smaller
neighborhoods by sliding the corresponding ﬁlter over the input. A pooling layer, however, does not have any learnable
weights and hence both the convolution and the pooling layer are counted as one layer in CNNs.
Of particular importance for forecasting are the so-called causal convolutions, deﬁned as
where hj is the output of a hidden node, α denotes the input, D = {1, . . . , n} for some n, |D| is the width of the causal
convolution (or also called the receptive ﬁeld) and w are the learnable parameters. In other words, causal convolutions
are weighted moving averages which only take inputs into account which are before j hence the reference to causality
in its name. A variation are dilated causal convolutions where we vary the index set D, e.g., such that it does not
necessarily contain consecutive values, but only every k-th value. Typically, these dilated causal convolutions are
stacked on top of each other where the output of one layer of dilated causal convolutions is the input of another layer
of causal convolution and the dilation grows by the depth of the NN. Figure 2a illustrates the general structure of a
CNN with dilated causal convolutions.
Recurrent neural networks
Recurrent neural networks (RNNs) are NNs speciﬁcally designed to handle sequential data that arise in applications
such as time series, natural language processing and speech recognition. The core idea consists of connecting recurrently the NNs’ hidden units back to themselves with a time delay . Since hidden units learn some kind of
feature representations of the raw input, feeding them back to themselves can be interpreted as providing the network
with a dynamic memory. One crucial detail here is that the same network is used for all timesteps, i.e., the weights
of the network are shared across timesteps. This weight-sharing idea is similar to that in CNNs where the same ﬁlter
is used across different parts of the input. This allows the RNNs to handle sequences of varying length during training and, more importantly, generalize to sequence lengths not seen during training. Figure 2b illustrates the general
structure of an (unrolled) RNN.
Although RNNs have been widely used in practice, training them is difﬁcult given that they are typically applied
to long sequences of data. A common issue while training very deep NNs by gradient-based methods using backpropagation is that of vanishing or exploding gradients which renders learning challenging.
Hochreiter and
Benidis et al.
Figure 2: (a) Structure of a CNN consisting of a stack of three causal convolution layer. The input layer (green) is
non-dilated and the other two are dilated. (b) Structure of an unrolled RNN. At each timestep t the network receives
an external input αt and the output of the hidden units from the previous time step ht−1. The hidden units all share
the same weights. The internal state of the network is updated to ht that is going to play the role of the previous state
in the next timestep t + 1. Finally, the network outputs βt which is a function of αt and ht.
Schmidhuber proposed Long short-term memory networks (LSTM) to address this problem. Similar to Resnet,
via the skip-connections, LSTMs (and a simpliﬁed version Gated recurrent units (GRU) ) always offer a path
where the gradient does not vanish or explode.
Transformer
A more recent architecture is based on the attention mechanism which has received increased interest in other sequence
learning tasks for its ability to improve on long sequence prediction tasks over RNNs. One natural
way to address this issue is to learn more than one feature representation (contrary to RNNs), e.g., one for each time
step of the input sequence and decide which of these representations are useful to predict the current element of the
target sequence. Bahdanau et al. suggest using a weighted sum of the representations where the weights are jointly
learned along with the feature representation learning and the prediction. Note that at each time step in the prediction,
one needs to learn a separate set of weights for the representations. This is essentially training the predictor to learn
to which parts of the input sequence it should pay attention to produce a prediction. This attention mechanism has
been shown to be instrumental for the state of the art in speech recognition and machine translations tasks .
Inspired by the success of attention models, Vaswani et al. developed the so-called Transformer model and
showed that attention alone is sufﬁcient, thus making the training amenable for parallelization and large number of
parameters . In the literature, the term Transformer can refer to both the speciﬁc model and to the overall
architecture as well.
Input Transformations
The careful handling of the input (parameters αt in Fig. 1 and 2) is a practically important ingredient for deep learning
models in general and deep forecasting models in particular. Deep forecasting models are most commonly deployed
as so-called global models (see Section 2.1), which means that the weights of the NN are trained across the panel
of time series. Hence, it is important that the scale of the input is comparable. Standard techniques such as meanvariance scaling carry over to the forecasting setting. In practice, it is important to avoid leakage of future values in
normalization schemes, so that mean and variance are taken over past windows (similar to causal convolutions).
Traditionally, the forecasting literature has used transformations such as the Box-Cox, i.e.,
where z is the input of the transformation, h is the output and λ is a free parameter. Box-Cox is a popular heuristic
to have the input data more closely resemble the Gaussian distribution. A Box-Cox transformation can be readily
integrated into an NN, with the free parameter λ optimized as part of the training process jointly with the other
parameters of the network. More sophisticated approaches based on probability integral transformation (PIT) or
Copulas are similarly possible, see e.g., (and references therein) for a recent example.
A further standard technique is the discretization of input into categorical values or bins, for example by choosing the
number and borders of bins such that each bins contains equal mass, see e.g., for an example in forecasting.
Benidis et al.
Figure 3: For a Gaussian distribution, its density function f is on the left-hand panel, the corresponding cumulative
density function F (the primitive integral of f) in the central panel and the quantile function F −1 on the right-hand
We note that any input transformation must be reversed also to obtain values in the actual domain of interest. It is
a choice for the modeller where/when to apply this reversal. Two extreme choices are to have transformation of the
input and output fully outside the NN or have the input transformations as part of the NN and hence be subjected to
Output Models and Loss Functions
Similar to the input, the output (βt in Fig. 1 and 2) deserve a special discussion. Closely related is the question on
the choice of loss function which we use to train a NN. The simplest form of an output is a single value, also referred
to as a point forecast. For this case, the output ˆzi,t is the best (w.r.t. the chosen loss function) estimate for the true
value zi,t. Standard regression loss functions (like ℓp losses with their regularized modiﬁcations) can be used or more
sophistication accuracy metrics speciﬁcally geared towards forecasting such as the MASE, sMAPE or others .
As remarked in Section 2.1, a point estimate ˆzi,t can be seen as a particular realization from a probabilistic estimate
of p(zi,t). Depending on the accuracy metric used in forecasting, a different realization may be appropriate . So,
even for obtaining point forecasts, probabilistic forecasts are important. More importantly, forecasts are often used
in downstream optimization problem where some form of expected cost is to be minimized and for this, an estimate
of the entire probability distribution is required. The probability distribution can be represented equivalently by its
probability density function (PDF), the cumulative density function (CDF) or its inverse, the quantile function. Fig. 3
contains a visualization of the different representations for the Gaussian distribution. Across the deep forecasting
landscape, most approaches (e.g., ), have chosen the PDF and quantile function to represent
p(zi,t) and we will discuss general recipes next. Since the CDF has typically not been chosen to represent p(zi,t), we
do not discuss it further.
Arguably the most common way to represent a probability distribution in forecasting is via its PDF. The literature contains examples of using the standard parametric distribution families to represent probabilistic forecasts. For example,
the output layer of an NN may produce the mean and variance parameter of a Gaussian distribution. So, the parameter
βt in Fig. 1 and 2 is a two-dimensional vector corresponding to µt and σt of a Gaussian distribution. We typically
achieve σt ≥0 by mapping the corresponding parameter through a softplus function. For the loss function, a natural
choice is the negative log-likelihood (NLL) since a PDF allows to readily compute the likelihood of a point under it.
Beyond Gaussian likelihood, a number of differentiable parametric distributions have been used in the literature depending on the nature of the forecasting problem, e.g., the student-t distribution or the Tweedie distribution for continuous data, the negative binomial distribution for count data and more ﬂexible approaches via mixtures of Gaussian.
Although forecasting is most commonly done for domains of numerical values (i.e., we assume zi,t to be in R or N),
other distributions such as the multinomial have also been employed successfully in forecasting even though they have
no notion of the order on the domain . The deployment of a multinomial distribution requires a binning of the
input values (see Section 2.3). An alternative approach is to cut the output space in bins and treat each of them as
a uniform distribution, while modelling the tails with a parametric distribution , this results in a piecewise linear
Benidis et al.
Figure 4: An illustration how a quantile function parametrized by linear splines (left panel) corresponds to a piecewise linear CDF (middle) which in turn corresponds to a piece-wise constant PDF as assumed in an adaptive binning
strategy (right panel).
Quantile function
Another representation of p(zi,t) is via the quantile function which has a particular importance for forecasting. Often,
a particular quantile is of practical interest. For example, in a simpliﬁed supply chain scenario for inventory control,
there is a direct correspondence between the chosen quantile and a safety stock level in the newsvendor problem .
So naturally, estimating the quantiles directly via quantile regression approaches is a common choice in
forecasting either via choosing a single quantile (in a point-forecasting approach) or multiple quantiles simultaneously . Essentially, this discretizes the quantile function and estimates speciﬁc points only. A common
choice for the loss function is the quantile loss or pinball loss. For the q-th quantile and F −1 the quantile function, the
quantile loss is deﬁned as
i,t (q), zi,t
i,t (q)} −q
i,t (q) −zi,t
where 1{cond} is the indicator function that is equal to 1 if cond is true and 0 otherwise. The output of the NN is ˆF −1
i.e., the estimated value of the q-th quantile. For q = 0.5 this reduces to the median of the forecast distribution and is
a common choice of point forecasts.
As an alternative to a quantile regression approach, we can make a parametric assumption on the quantile function
and estimate it directly. The main requirements for modelling a quantile function are that its domain should be
constrained to and the function should be monotonically increasing. This can be achieved easily via linear
splines for example, so the output of the NN’s last layers are the corresponding free parameters. For the loss function,
a rich theory around the continuous ranked probability score (CRPS) exists and CRPS can be used as a loss
function directly. CRPS can be deﬁned to summarize all possible quantile losses as
CRPS( ˆFi,t, zi,t) :=
i,t (q), zi,t
Multivariate extensions such as the energy score exist.
Interestingly, a popular discretization strategy, adaptive binning, used with multinomial distributions corresponds to
quantile functions parametrized by piece-wise linear splines, see Fig. 4.
Further approaches
The recent deep learning literature contains more advanced examples for density estimation, most prominently via
Generalized Adversarial Networks (GANs). We discuss them in Section 3.8 and discuss normalizing ﬂows here which
have arguably resonated more strongly in forecasting. Normalizing ﬂows are invertible NNs that transform a simple
distribution to a more complex output distribution. Invertibility guarantees the conservation of probability mass and
allows the evaluation of the associated density function everywhere. The key observation is that the probability density
of an observation zi,t can be computed using the change of variables formula:
p(zi,t) = pyi,t(f −1(zi,t))|det[Jacf −1
i,t zi,t]|,
Benidis et al.
Canonical (One-to-One)
Seq2Seq (Many-to-Many)
Figure 5: Canonical versus sequence-to-sequence models.
where the ﬁrst term pyi,t(f −1(zi,t)) is the (in general simple) density of a variable yi,t, and the second is the absolute
value of the determinant of the Jacobian of f −1
i,t , evaluated at zi,t.
The invertible function f is typically parametrized by an NN. A particular instantiation is the Box-Cox transformation,
Eq. (5). The ﬁeld of normalizing ﬂows (e.g., ) studies invertible NNs that typically transform isotropic
Gaussians to more complex data distributions. The choice of a particular instantiation of f can facilitate the computation of the likelihood of a given point when the NLL is amenable as a loss function. Alternatively, generating samples
may be computationally more viable for other instantiations (this is typically the cases with generative adversarial
networks as well). In this case, the NLL can be replaced by other loss functions such as CRPS.
A number of extensions are possible. For example, more complex models for p(zi,t) are possible such as Hidden
Markov Models or Linear Dynamical Systems. NNs can output the free parameters of these models but then need to be
combined with the learning and inference schemes associated with these models, such as Kalman Filtering/Smoothing
in the case of Linear Dynamical System or the Forward/Backward Algorithm in the case of Hidden Markov
Models . Another avenue is to relax constraints on the representation of p(zi,t) to obtain closely related objects with
more favorable computational properties. For example, energy based models (EBMs) approximate the unnormalized
log-probability . EBMs perform well in learning high dimensional distributions at the cost of being difﬁcult
to train and have been employed in forecasting .
Archetypical Architectures
With all key components in place, in this section we present in more details popular forecasting architectures.
In particular we focus on the widely-used RNN-based architecture that takes as input its previous hidden state, the currently available information and produces an one-step ahead estimate of the target time series.
There are subtle details on how to handle a multi-step unrolled model during training (e.g., ), which we will skip over.
We further examine the sequence-to-sequence (seq2seq) modelling approach where the model takes an encoding sequence as input and maps it to a decoding sequence (of predetermined length) on which the loss is computed against the actual values z during training.
(ˆµt, ˆσt) (ˆµt+1, ˆσt+1)(ˆµt+2, ˆσt+2)
Figure 6: DeepAR: The model outputs parameters of a previously chosen family of distributions. Samples from this
distribution can be fed back into the model during prediction (dotted lines) or in case of αt being missing.
A typical instance in the training set in this approach
consists of the target and covariate values up to a certain
point in time t as the encoding sequence and the outputs
of the NN are a predetermined number of target values
after time t. Figure 5 contrasts both approaches. In the
following we present two popular deep forecasting models, DeepAR and MQRNN/MQCNN, in some details to
illustrate the core concepts. They represent the one-stepahead RNN-based and seq2seq approach, respectively.
Among the ﬁrst of the modern deep forecasting models
is DeepAR , a global univariate model (see Table 1)
Benidis et al.
that consists of an RNN backbone (typically an LSTM).4 The input of the model is a combination of lagged target
values and relevant covariates. The output is either a point forecast with a standard loss function or, in the basic variant,
a probabilistic forecast via the parameters of a PDF (e.g., µ and σ of a Gaussian distribution), where the loss function
is then the NLL. The output modelling of DeepAR has been the subject of follow-up work, e.g., Jeon and Seong
 propose a Tweedie loss, Mukherjee et al. propose a mixture of Gaussians as the distribution and domain
speciﬁc feature processing blocks. Figure 6 summarizes the architecture. The dotted arrows in the picture correspond
to drawing a sample that can be used as alternative input (as a lagged target) during training (even though αt may be
available or in the case where an αt is missing) and during prediction to obtain multi-step ahead forecasts.
It is also possible to change the output of DeepAR to model the quantile function and use CRPS, Eq. (7), as the loss
function . While this in general computationally challenging, special cases are amendable for practical
computation. For example, we can assume a parametrization of the quantile function by linear isotonic regression
s(q; γ, b, d) = γ +
bℓ(q −dℓ)+
where q ∈ is the quantile level, γ ∈R is the intercept term, b ∈RL+1 are weights describing the slopes of the
function pieces, d ∈RL+1 is a vector of knot positions, L the number of pieces of the spline and (x)+ = max(x, 0)
is the ReLU function. In order for s(·) to represent a quantile function we need to guarantee its monotonicity and
restrict its domain to . Both of these constraints can readily be achieved using standard NN tooling using a
reparametrization of Eq. (9), while CRPS can be solved in closed form for linear splines (see ).
Bag of tricks.
While the general setup of DeepAR is straightforward, a number of algorithmic optimizations turn
it into a robust, general-purpose forecasting model. The handling of missing values via sample replacement from the
probability distribution is one such example. Another one is oversampling of “important” training examples during
training, where importance typically corresponds to time series with larger absolute values. Adding lagged values
further help improve predictive accuracy. Lags can be chosen heuristically based on the frequency of the time series.
For example, in a time series with daily frequency, a lag of 7 days often helps. Similarly, covariates corresponding to
calendar events (e.g., indicator variables for weekends or holidays) can help further.
MQRNN/MQCNN
As an example for another type of deep forecasting model, we discuss the multi-horizon quantile recurrent forecaster (MQRNN) next which was conceived concurrently to DeepAR. Contrary to DeepAR, it is most naturally
deployed as a discriminative, seq2seq model in a quantile regression setting. For each time point t in the forecast
horizon, MQRNN outputs a chosen number of estimates for corresponding quantiles and the loss function in MQRNN
is Eq. (6), i.e., the pinball loss summed over all quantiles and time points.
While MQRNN can use multiple conﬁgurations, often a CNN-based architecture is chosen in practice in the encoder
(MQCNN) for computational efﬁciency reasons over RNN-based methods and two MLPs in the decoder. The ﬁrst
MLP captures all inputs during the forecast horizon and the context provided by the encoder. A second, local MLP
applies only to speciﬁc horizons for which it uses the corresponding available input and the output of the MLP. A
further innovation provided by MQCNN is the training scheme via the so-called forking sequences where the model
forecasts by placing a series of decoders with shared parameters at each timestep in the encoder. Thus, the model can
structurally forecast at each timestep, while the optimization process is stabilized by updating the gradients from the
sequences together. An additional component of MQRNN is a local MLP component that aims to model spikes and
events speciﬁcally.
Literature review
In the prior section, we provided an in-depth introduction to selected, basic topics. Building on these topics, we survey
the literature on modern deep forecasting models more broadly in this section. Given the breadth of the literature
available, our selection is necessarily subjective.
We proceed as follows. In Section 3.1 we present probabilistic forecasting models, both one-step and multi-step. Similarly, in Section 3.2 we summarize point forecast models. We remark that, after Section 2, we have recipes at hand
to turn an one-step ahead forecasting model into a multi-step forecasting model and a point forecasting model into a
4Hewamalage et al. provide an overview speciﬁcally targeted at RNNs for forecasting.
Benidis et al.
probabilistic model. We discuss hybrids of deep learning with state space models in Section 3.3, multivariate forecasting in Section 3.4, physics-based model in Section 3.5, global-local models in Section 3.6, models for intermittent
time series in Section 3.7 and generative adversarial networks for forecasting in Section 3.8. We close this section
with an overview of the large number of available models in Section 3.9 where we also provide guidelines on where
to start the journey with deep forecasting models.
Probabilistic Forecast Models
One-step forecast
The DeepAR model presented in Sec. 2.5.1, is an example of one-step canonical forecasting model. In its base
variant, DeepAR is a global univariate model which learns a univariate distribution; we discuss multivariate extensions
in Sec. 3.4. DeepAR can be equipped with outputs representing a parametrized PDF including Gaussian Mixture
Distributions Mukherjee et al. or quantile functions Gasthaus et al. .
Rasul et al. propose TimeGrad which, like DeepAR, is an RNN model using LSTM or GRU cells for which
samples are drawn from the data distribution at each time step, with the difference that in TimeGrad the RNN conditions a diffusion probabilistic model which allows the model to easily scale to multivariate time series and
accurately use the dependencies between dimensions. Replacing the RNN-backbone of DeepAR with dilated causal
convolutions has been proposed as both point and probabilistic forecasting models .
Multi-step forecast
Contrary to the some of the models in Section 3.1.1 which produce one-step ahead forecasts, multi-step forecasts can be
obtained directly with a seq2seq architecture. In Section 2.5.2, we reviewed the MQRNN/MQCNN architecture 
as a seq2seq architecture for probabilistic forecasting. The main advantage of seq2seq over one-step ahead forecast
models is that the decoder architecture can be chosen to output all future target values at once. This removes the need
to unroll over the forecast horizon which can lead to error accumulation since early forecast errors propagate through
the forecast horizon. Thus, the decoder of seq2seq forecasting models is typically an MLP while other architectures
are also used for the encoder .
Wen and Torkkola extended the MQCNN model with a generative quantile copula. This model learns the
conditional quantile function that maps the quantile index, which is a uniform random variable conditioned on the
covariates, to the target. During training, the model draws the quantile index from a uniform distribution. This turns
MQCNN into a generative, marginal quantile model. The authors combine this approach with a Gaussian copula to
draw correlated marginal quantile index random values. They show that the Gaussian copula component improves
the forecast at the distribution tails. Chen et al. proposed DeepTCN, another seq2seq model where the encoder
is the dilated causal convolution with residual blocks, and the decoder is simply an MLP with residual connections.
Structure-wise, DeepTCN is almost the same as the basic structure of MQCNN , i.e., without the local MLP
component that aims to model spikes and events.
Park et al. propose the incremental quantile functions (IQF), a ﬂexible and efﬁcient distribution-free quantile
estimation framework that resolves quantile crossing with a simple NN layer. A seq2seq encoder-decoder structure
is used although the method can be readily applied to recurrent models with one-step ahead forecasts . IQF is
trained using the CRPS loss (Eq. (7)) similar to .
A combination of recurrent and encoder-decoder structures has also been explored. In , the authors use an LSTM
with Monte Carlo dropout as both the encoder and decoder. However, unlike other models that directly use RNNs to
generate forecasts, the learned embedding at the end of the decoding step is fed into an MLP prediction network and
is combined with other external features to generate the forecast. Along a similar line, Laptev et al. employ an
LSTM as a feature extractor (LSTM autoencoder), and use the extracted features, combined with external inputs to
generate the forecasts with another LSTM.
Van Den Oord et al. introduced the WaveNet architecture, a generative model for speech synthesis, which uses
dilated causal convolutions to learn the long range dependencies important for audio signals. Since this architecture
is based on convolutions, training is very efﬁcient on GPUs – prediction is still sequential and further changes are
necessary for fast inference. Adaptations of WaveNet for forecasting are available .
Point Forecast Models
Point forecast models do not model the probability distribution of the future values of a time series but rather output
directly a point forecast that typically corresponds to a summary statistic of the predictive distribution. We have
Benidis et al.
discussed generic recipes on how to turn a point forecasting model into a probabilistic forecasting model in Section 2.4
and the literature contains further examples (see e.g., for recent complementary approaches).
One-step forecast
A considerable amount of attention of the community is dedicated to one-step forecasting. LSTNet is a model
using a combination of a CNN and an RNN. Targeting multivariate time series, LSTNet uses a convolution network
(without pooling) to extract short-term temporal patterns as well as correlations among variables. The output of the
convolution network is fed into a recurrent layer and a temporal attention layer which, combined with the autoregressive component, generates the ﬁnal point forecast. While LSTNet uses a standard point forecast loss function, it can
readily be turned into a probabilistic forecast model using the components described in Sec. 2, e.g., by modifying LST-
Net to output the parameters of a probability distribution and using NLL as a loss function. Qiu et al. proposed
an ensemble of deep belief networks for forecasting. The outputs of all the networks is concatenated and fed into a
support vector regression model (SVR) that gives the ﬁnal prediction. The NNs and the SVR are not trained jointly
though. Hsu proposed an augmented LSTM model which combines autoencoders with LSTM cells. The input
observations are ﬁrst encoded to latent variables, which is equivalent to feature extraction, and are fed into the LSTM
cells. The decoder is an MLP which maps the LSTM output into the predicted values. For point forecast multivariate
forecasting, Yoo and Kang proposed time-invariant attention to learn the dependencies between the dimensions
of the time series and use them with a convolution architecture to model the time series.
Building upon the success of CNNs in other application domains, Borovykh et al. proposed an adjustment to
WaveNet that makes it applicable to conditional forecasting. They evaluated their model on various datasets
with mixed results, concluding that it can serve as a strong baseline and that various improvements could be made. In
a similar vein, inspired by the Transformer architecture Song et al. proposed an adjustment that makes the
architecture applicable to time series. Their method is applied to both regression and classiﬁcation tasks.
Multi-step forecast
N-BEATS is an NN architecture purpose-built for the forecasting task that relies on a deep, residual stack of MLP
layers to obtain point forecasts. The basic building block in this architecture is a forked MLP stack that takes the block
input and feeds the intermediate representation into separate MLPs to learn the parameters of the context (the authors
call it backcast) and forecast time series models. The residual architecture removes the part of the context signal it can
explain well before passing to the next block and adds up the forecasts. The learned time series model can have free
parameters or be constrained to follow a particular, functional form. Constraining the model to trend and seasonality
functional forms does not have a big impact on the error and generates models whose stacks are interpretable since the
trend and seasonality components of the model can be separated and analyzed. N-BEATS has also been interpreted as
a meta-learning model , where the repeated application of residual blocks can be seen as an inner optimization
loop. N-BEATS generalizes better than other architectures when trained on a source dataset (e.g., M4-monthly) and
applied to a different target datasets (e.g., M3-monthly).
Lv et al. propose a stacked autoencoder (SAE) architecture to learn features from spatio-temporal trafﬁc ﬂow
data. On top of the autoencoder, a logistic regression layer is used to output predictions of the trafﬁc ﬂow at all locations
in a future time window. The resulting architecture is trained layer-wise in a greedy manner. The experimental results
show that the method signiﬁcantly improves over other shallow architectures, suggesting that the SAE is capable of
extracting latent features regarding the spatio-temporal correlations of the data. In the same context of spatio-temporal
forecasting and under the seq2seq framework, Li et al. proposed the Diffusion Convolutional Recurrent NN
(DCRNN). Diffusion convolution is employed to capture the dependencies on the spatial domain, while an RNN is
utilized to model the temporal dependencies. Finally, Asadi and Regan proposed a framework where the time
series are decomposed in an initial preprocessing step to separately feed short-term, long-term, and spatial patterns
into different components of a NN. Neighbouring time series are clustered based on their similarity of the residuals
as there can be meaningful short-term patterns for spatial time series. Then, in a CNN based architecture, each kernel
of a multi-kernel convolution layer is applied to a cluster of time series to extract short-term features in neighbouring
areas. The output of the convolution layer is concatenated by trends and is followed by a convolution-LSTM layer to
capture long-term patterns in larger regional areas.
Bandara et al. addressed the problem of predicting a set of disparate time series, which may not be well captured
by a single global model. For this reason, the authors propose to cluster the time series according to a vector of features
extracted using the technique from and the Snob clustering algorithm . Only then, an RNN is trained per
cluster, after having decomposed the series into trend, seasonality and residual components. The RNN is followed
by an afﬁne neural layer to project the cell outputs to the dimension of the intended forecast horizon. This approach
is applied to publicly available datasets from time series competitions, and appears to consistently improve against
Benidis et al.
learning a single global model. In subsequent work, Bandara et al. continued to mix heuristics, in this instance
seasonality decomposition techniques, known from classical forecasting methods with standard NN techniques. Their
aim is to improve on scenarios with multiple seasonalities such as inter and intra daily. The ﬁndings are that for panels
of somewhat unrelated time series, such decomposition techniques help global models whereas for panels of related
or homogeneous time series this may be harmful. The authors do not attempt to integrate these steps into the NN
architecture itself, which would allow for end-to-end learning.
Cinar et al. proposed a content attention mechanism that seats on top of any seq2seq RNN. The idea is to select
a combination of the hidden states from the history and combine them using a pseudo-period vector of weights to the
predicted output step.
Li et al. introduce two modiﬁcations to the Transformer architecture to improve its performance for forecasting.
First, they include causal convolutions in the attention to make the key and query context dependent, which makes the
model more sensitive to local contexts. Second, they introduce a sparse attention, meaning the model cannot attend to
all points in the history, but only to selected points. Through exponentially increasing distances between these points,
the memory complexity can be reduced from quadratic to O(T(log T)2), where T is the sequence length, which is
important for long sequences that occur frequently in forecasting. Other architectural improvements to the Transformer model have also been used more recently to improve accuracy and computational complexity in forecasting
applications. For example, Lim et al. introduce the Temporal Fusion Transformer (TFT), which incorporates
novel model components for embedding static covariates, performing “variable selection”, and gating components
that skip over irrelevant parts of the context. The TFT is trained to predict forecast quantiles, and promotes forecast interpretability by modifying self-attention and learning input variable importance. Eisenach et al. propose
MQ-Transformer, a Transformer architecture that employs novel attention mechanisms in the encoder and decoder
separately, and consider learning positional embeddings from event indicators. The authors discuss the improvements
not only on forecast accuracy, but also on excess forecast volatility where their model improves over the state of the
art. Finally, Zhou et al. recently proposed the Informer, a computationally efﬁcient Transformer architecture,
that speciﬁcally targets applications with long forecast horizons. The Informer introduces a ProbSparse attention layer
and a distilling mechanism to reduce both the time complexity and memory usage of learning to O(T log T), while
improving forecast performance over deep forecasting benchmark.
Deep State Space Models
In contrast to pure deep learning methods for time series forecasting introduced in Section 2, Rangapuram et al. 
propose to combine classical state space models (SSM) with deep learning. The main motivation is to bridge
the gap between SSMs that provide a principled framework for incorporating structural assumptions but fail to learn
patterns across a collection of time series, and NNs that are capable of extracting higher order features but results in
models that are hard to interpret. Their method parametrizes a linear Gaussian SSM using an RNN. The parameters
of the RNN are learned jointly from a dataset of raw time series and associated covariates. Instead of learning the
SSM parameters θi,1:Ti for the i-th time series individually or locally in the terminology of Section 2.1), the model
is global and learns a shared mapping from the covariates associated with each target time series to the parameters
of a linear SSM. This mapping θi,t = f(xi,1:t; Φ), for i = 1, . . . , N and t = 1, . . . , Ti + h, is implemented by an
RNN with weights Φ which are shared across different time series as well as different time steps. Note that f depends
on the entire covariate time series up to time t as well as the set of shared parameters Φ. Since each individual time
series i is modelled using an SSM with parameters Θi, assumptions such as temporal smoothness in the forecasts
are easily enforced. The shared model parameters Φ are learned by maximizing the likelihood given the observations
Z = {zi,1:Ti}N
i=1. The likelihood terms for each time series reduce to the standard likelihood computation under
the linear-Gaussian SSM, which can be carried out efﬁciently via Kalman ﬁltering . Once the parameters Φ are
learned, it is straightforward to obtain the forecast distribution via the SSM parameters θi,Ti+1:Ti+h.
There are two major limitations of the method proposed in Rangapuram et al. : ﬁrst, the observations are assumed
to follow a Gaussian distribution and second, the underlying latent process that generates observations is assumed to
evolve linearly. de B´ezenac et al. address the ﬁrst limitation via Normalizing Kalman Filters (NKF) by augmenting
SSMs with normalizing ﬂows thereby giving them the ﬂexibility to model non-Gaussian, multimodal
data. Their main idea is to map the non-Gaussian observations {zi,1:Ti} to more Gaussian-like observations via a
sequence of learnable, nonlinear transformations (e.g., a normalizing ﬂow) so that the method in can then be
applied on the transformed observations. While being more ﬂexible, their method still retains attractive properties
of linear Gaussian SSMs, namely, tractability of exact inference and likelihood computation, efﬁcient sampling, and
robustness to noise.
In a concurrent work to , Kurle et al. improve the method in by addressing both limitations. In particular, to model nonlinear latent dynamics, they propose a recurrent switching Gaussian SSM, which uses additional latent
Benidis et al.
variables to switch between different linear dynamics. Moreover, to handle non-Gaussian observations, they propose
a nonlinear emission model via a decoder-type NN . Although the exact inference is no longer tractable with these
improvements, they show that the approximate inference and likelihood estimation can be Rao-Blackwellised; i.e., the
inference for the Gaussian latent states can be done exactly while the inference for the switch variables needs to be
approximated.
Finally, Ansari et al. propose to extend via incorporating switching dynamics. The recurrent explicit duration switching dynamical system (RED-SDS) is a ﬂexible model that is capable of identifying both state- and timedependent switching dynamics of a time series. State-dependent switching is enabled by a recurrent state-to-switch
connection and an explicit duration count variable is used to improve the time-dependent switching behavior. A hybrid algorithm that approximates the posterior of the continuous states via an inference network and performs exact
inference for the discrete switches and counts provides efﬁcient inference. The method is able to infer meaningful
switching patterns from the data and extrapolate the learned patterns into the forecast horizon.
Multivariate Forecasting
The models presented up to this point are mainly global univariate models, i.e., they are trained on all time series but
they are still used to predict a univariate target. When dealing with multivariate time series, one should be able to
exploit the dependency structure between the different time series in the panel in a generalization of Eq. (3) to Eq. (4).
Toubeau et al. and Salinas et al. combined RNN-based models with copulas to model multivariate distributions. The model in uses a nonparametric copula to capture the multivariate dependence structure. In contrast,
the work in uses a Gaussian copula process approach. Salinas et al. use a low-rank covariance matrix
approximation to scale to thousands of dimensions. Additionally, the model implements a non-parametric transformation of the marginals to deal with varying scales in the dimensions and non-Gaussian data. More recently, Rasul et al.
 proposed to represent the data distribution with a type of normalizing ﬂows called Masked Autoregressive Flows
 while using either an RNN or a Transformer to model the multivariate temporal dynamics of time series.
Normalizing ﬂows were also used to bring deep SSMs to a ﬂexible, multivariate scenario . Rasul et al. 
propose TimeGrad which, like DeepAR, is an RNN model for which samples are drawn from the data distribution at
each time step, with the difference that in TimeGrad the RNN conditions a diffusion probabilistic model which
allows the model to easily scale to multivariate time series and accurately use the dependencies between dimensions.
A recent application of global multivariate models is for hierarchical forecasting problems . Typically,
in such problems an aggregation structure is deﬁned (e.g., via a product hierarchy) and a trade-off between forecast
accuracy and forecast coherency with respect to the aggregation structure must be managed. Here, forecast coherency
or consistency means that the forecasts conforms to the aggregation structure, so that aggregated forecasts are the same
as forecasts of aggregated time series. This aggregation structure is typically encoded via linear constraints where the
aggregation structure is captured in a matrix S. Rangapuram et al. propose to use a multivariate model such
as and enforce consistency of forecast samples via incorporation of a projection of the samples with S into the
learning problem. Dedicated work exists for aggregation along the time dimension .
In some multivariate forecasting settings the different dimensions are tied together by some interpretable connections
other than a hierarchy and this can be modelled as part of the input layer rather than the output as discussed so far.
One can for example think of forecasting the trafﬁc network of a city where the trafﬁc at each of the location in the
city is mostly inﬂuenced by the trafﬁc at the neighboring locations, like in PEMS-BAY and METR-LA . Graph
Neural Networks (GNN) have been used in this forecasting setting where, in addition to
the forecasting task, the challenge is to best use the graph information that is provided or even learn the graph if none
is available. The methods that propose to learn the graph do so by looking for the graph that allows to produce the
most accurate forecasts. An embedding is learned for each dimension, and similarity scores are computed between
every two dimension using these embeddings from which the adjacency matrix is obtained, either by taking the Ktop edges or sampling from them . As of now, two main strategies have been proposed to learn
the node embeddings, either simply by gradient descent or by taking representation from the time series
 , with the latter approach to seemingly yielding better results. While these methods were all presented as
point forecasting method, one could obtain probabilistic forecasts by training these models to parametrize a predictive
distribution as explained in Section 2.4.
Physics-based Models
In physics-based models, deep forecasting methods have been proposed that model the underlying dynamics in sophisticated ways. Chen et al. proposed the Neural ODE (NODE) model, where an ordinary differential equation
(ODE) is solved forward in time, and the adjoint equation is solved backwards in time using backpropagation. One
Benidis et al.
limitation of the Neural ODE model is that the unknown parameters θ are assumed to be constant in time. Other
limitations such as computational complexity have been addressed in follow-up work, e.g., . Vialard et al. 
extends the NODE model to allow the parameters θ(t) to be time-varying by introducing a shooting formulation. In
the shooting formulation, the optimal θ is determined by minimizing a regularized loss function. Vialard et al. 
also shows that a residual network (ResNet) can be expressed as the Forward Euler discretization of an ODE with
time step ∆t = 1. Wang et al. compares successful time series deep sequence models, such as to
NODE and other hybrid deep learning models to model COVID-19 dynamics, as well as the population dynamics using the Lotka-Volterra equations. Through their benchmarking study, the authors show that distribution shifts can pose
problems for deep sequence models on these tasks, and propose a hybrid model AutoODE to model the underlying
Global-local
With local models, the free parameters of the model are learned individually for each series in a collection, see
Section 2.1. Classical local time series models such as SSMs, ARIMA, and exponential smoothing (ETS) excel
at modelling the complex dynamics of individual time series given a sufﬁciently long history. Other local models
include Gaussian SSMs, which are computationally efﬁcient, e.g., via a Kalman ﬁlter, and Gaussian Processes (GPs)
 . These methods provide uncertainty estimates, which are critical for optimal downstream decision
making. Since these methods are local, they learn one model per time series and cannot effectively extract information
across multiple time series. These methods are unable to address cold-start problems where there is a need to generate
predictions for a time series with little or no observed history.
Conversely, recall that in global models, their free parameters are learned jointly on every series in a collection of
time series. NNs have proven particularly well suited as global models . Global methods can
extract patterns from collections of irregular time series even when these patterns would not be distinguishable using
a single series.
Global-local models have been proposed to combine the advantages of both global and local models. Examples
include mixed effect models , which consist of two kinds of effects: ﬁxed (global) effects that describe the whole
population, and random (local) effects that capture the idiosyncratic of individuals or subgroups. A similar mixed
approach is used in hierarchical Bayesian methods, which combine global and local models to jointly model a
population of related statistical problems. In an early example of hierarchical Bayesian models, combined global
and local features for intermittent demand forecasting in retail planning. In , other combined global and local
models are detailed.
A recent global-local family of models, Deep Factors provide an alternative way to combine the expressive
power of NNs with the data efﬁciency and uncertainty estimation abilities of classical probabilistic local models. Each
time series, or its latent function for non-Gaussian data, is represented as the weighted sum of a global time series and
a local model. The global part is given by a linear combination of a set of deep dynamic factors, where the loading
is temporally determined by attentions. The local model is stochastic. Typical choices include white noise processes,
linear dynamical systems, GPs or RNNs. The stochastic local component allows for the uncertainty to propagate
forward in time, while the global NN model is capable of extracting complex nonlinear patterns across multiple time
series. The global-local structure extracts complex nonlinear patterns globally while capturing individual random
effects for each time series locally.
The Deep Global Local Forecaster (DeepGLO) is a method that “thinks globally and acts locally” to forecast
collections of up to millions of time series. It crucially relies on a type of temporal convolution (a so-called leveled
network), that can be trained across a large amount time series with different scales without the need for normalization
or rescaling. DeepGLO is a hybrid model that uses a global matrix factorization model regularized by a temporal
deep leveled network and a local temporal deep level network to capture patterns speciﬁc to each time series. Each
time series is represented by a linear combination of k basis time series, where k ≪N, with N the total number of
time series. The global and local models are combined through data-driven attention for each time series.
A further example in the global-local model class is the ES-RNN model proposed by Smyl that has recently
attracted attention by winning the M4 competition by a large margin on both evaluation settings. In the ES-RNN
model, locally estimated level and trend components are multiplicatively combined with an RNN model. Apart from
its global-local nature, it also integrates aspects of different model classes into a a single model similar to Deep State
Space models (Section 3.3). In particular, the h-step ahead prediction ˆzi,t+1:t+h = li,t · si,t+1:t+h · exp(RNN(xi,t))
consists of a level li,t and a seasonal component si,t obtained through local exponential smoothing, and the output
of a global RNN model RNN(xi,t), where xi,t is a vector of preprocessed data extracted from deseasonalized and
normalized time series xi,t = log(zi,t−K:t/(si,t−K:tli,t)) cut in a window of length K + 1. The RNN models are
Benidis et al.
composed of dilated LSTM layers with additional residual connections. The M4-winning entry used slightly different
architectures for the different type of time series in the competition.
Intermittent Time Series
We noted in the introduction that deep forecasting models had a major impact on operational forecasting problems. In
these large-scale problem, intermittent time series occur regularly . Accordingly, research on NNs for intermittent
time series forecasting has been an active area. Salinas et al. propose a standard RNN architecture with a negative
binomial likelihood to handle intermittent demand similar to in classical methods. To the best of our knowledge,
other likelihoods that have been proposed for intermittent time series in classical models, e.g., by , have not yet
been carried over to NNs. However, some initial work is available via more standard likelihoods .
In the seminal paper on intermittent demand forecasting , Croston separates the data in a sequence of observed
non-zero demands and a sequence of time intervals between positive demand observations, and runs exponential
smoothing separately on both series. A comparison of NNs to classical models for intermittent demand ﬁrst appeared
in Gutierrez et al. , where the authors compare the performance of a shallow and narrow MLP with Croston’s
method. They ﬁnd NNs to outperform classical methods by a signiﬁcant margin.
Kourentzes proposes two MLP architectures for intermittent demand, taking demand sizes and intervals as
inputs. As in Gutierrez et al. , the networks are shallow and narrow by modern standards, with only a single
hidden layer and three hidden units. The difference between the two architectures is in the output. In one case
interval times and non-zero occurrences are output separately, while in the other a ratio of the two is computed. The
approach proposed by Kourentzes outperforms other approaches primarily with respect to inventory metrics, but
not forecasting accuracy metrics, challenging previous results in . It is unclear whether the models are used as
global or local. However, given the concern around overﬁtting and regularization, we assume that these models were
primarily used as local models in the experiments.
Both approaches of only offer point forecasts. This shortcoming is addressed by , where the
authors propose renewal processes as natural models for intermittent demand forecasting. Speciﬁcally, they use RNNs
to modulate both discrete time and continuous time renewal processes, using the simple analogy that RNNs can replace
exponential smoothing in .
Finally, a recent trend in sequence modelling employs NNs in modelling discrete event sequences observed in continuous time and for an overview. Notably, Xiao et al. use two RNNs to
parametrize a probabilistic “point process” model. These networks consume data from asynchronous event sequences
and uniformly sampled time series observations respectively. Their model can be used in forecasting tasks where time
series data can be enriched with discrete event observations in continuous time.
Generalized Adversarial Networks
Additionally to the approaches mentioned in Sections 2.4 and 2.4.3, the recent literature contains further examples for
density estimation, most prominently via Generalized Adversarial Networks (GANs) . While GANs have received
much attention in the overall deep learning literature , this has not been reﬂected in forecasting.
We speculate that this is because a discriminator network can be replaced by metrics such as CRPS which measure the
quality of generated samples. We therefore only provide a brief overview here and mention that, while they rely on
the buildings blocks discussed in Section 2, they typically require architectures that are more complex than then ones
discussed here and lead to involved optimization problems.
Despite the comparably less attention that GANs have received in forecasting, they have been recently applied to the
time series domain to synthesize data or to employ an adversarial loss in forecasting tasks .
Many time series GAN architectures use recurrent networks to model temporal dynamics . Modelling
long-range dependencies and scaling recurrent networks to higher lengths is inherently difﬁcult and limits the application of time series GANs to short sequence lengths . One way to achieve longer realistic synthetic time series
is by employing convolutional and self-attention architectures .
Convolutional architectures are able to learn relevant features from the raw time series data , but are ultimately limited to local receptive ﬁelds and can only capture long-range dependencies via many stacks of convolutional
layers. Self-attention can bridge this gap and allow for modelling long-range dependencies from convolutional feature
maps, which has been a successful approach in the image and time series forecasting domain . Another
technique to achieve long sample sizes is progressive growing, which successively increases the resolution by adding
layers to a GAN generator and discriminator during training . A recent proposal synthesizes progressive
growing with convolutions and self-attention into a novel architecture particularly geared towards time series.
Benidis et al.
Summary and Practical Guidelines
In Section 2 and this section, we introduced a large number of deep forecasting models. We summarize the main
approaches in Table 2. The list below provide keys to reading the table.
• Forecast distinguishes between probabilistic (Prob) and Point forecasts.
• Horizon indicates whether the model does one-step predictions (noted 1) in which case multi-step forecasts
are obtained recursively, or if it directly predicts a whole sequence (≥1).
• Loss and Metrics speciﬁes the loss used for training and metrics used for evaluation. Here, we only provide
an explanation of the acronyms and not the deﬁnition of each metric which can be easily found in the corresponding papers: negative log-likelihood (NLL), quantile loss (QL), continuous ranked probability score
(CRPS), (normalized) (root) mean squared error (NRMSE, RMSE, MSE), root relative squared error (RRSE),
relative geometric RMSE (RGRMSE), weighted absolute percentage error (WAPE), normalized deviation
(ND), mean absolute deviation (MAD), mean absolute error (MAE), mean relative error (MRE), (weighted)
mean absolute percentage error (wMAPE, MAPE), mean absolute scaled error (MASE), overall weighted
average (OWA), mean scaled interval score (MSIS), Kullback-Leibler divergence (KL), Value-at-Risk (VaR),
expected shortfall (ES), empirical correlation coefﬁcient (CORR), area under the receiver operating characteristic (AUROC), percentage best (PB).
While Table 2 serves to illustrate the wealth of deep forecasting methods now available, their sheer number may be
slightly overwhelming. Furthermore, empirical evidence on the effectiveness of the different architectures has so far
not revealed a clearly superior approach . In this, forecasting differs from other domains, e.g., natural language
processing where Transformer-based models dominate overall. Also, deep forecasting methods seem to differ
from other model families, such as tree-based methods where LightGBM or XGBoost dominate (as in the
recent M5 forecasting competition ). We speculate that this diffuse picture is in part due to the practical reasons,
the relative immaturity of the ﬁeld and the corresponding software implementations and in part due to fundamental
reason as a natural consequence of the breadth and diversity of forecasting problems.
So, choosing the appropriate architecture for a problem at hand can be a daunting task. In the following, we therefore
attempt to provide guidelines for a more informed deep forecasting model selection. These are largely based on our
own experience in working with practical forecasting problem and they should primarily be taken as a non-exhaustive
guidance on where to start model exploration.
Baseline methods and standard mode of deployment
At the start of any in-depth model exploration, considering a baseline model is commonly accepted best practice. To
the best of our knowledge, the most mature deep forecasting models are DeepAR and MQCNN which
exist in a number of open-source and commercial implementations.5 As a practical guideline, we recommend to start
model exploration using at least these methods as baselines. Other candidates we would consider are N-BEATS ,
WaveNet and a Transformer-based model. The relative performance of these methods compared with other
methods should give reasonable, directional evidence whether the problem at hand is amenable to deep forecasting
methods. We note that AutoML approaches for forecasting are available6 but while promising are in their infancy.
At least in the M5 competition, they are still outperformed by the aforementioned more specialized deep forecasting
Our typical suggestion is to employ NNs as global models since, given enough data, global methods outperform
classical local methods when dealing with groups of similar time series.7 Interestingly, recent empirical evidence have
shown that global models can achieve a state-of-the-art performance even in heterogeneous groups of time series. This
is supported by the M4 and M5 competitions where the top performing models had some form of globality. This
suggests a more general applicability of global methods with a high impact on practical application where a general
automated forecasting mechanism is required.
5 
6 
7This is a more generally applicable fact beyond NN. Montero-Manso and Hyndman show favorable theoretical and
empirical properties for global over local models.
Benidis et al.
Data Types
DeepAR 
Coverage, QL,
demand, trafﬁc,
electricity
Learns parametric distributions
Toubeau et al. 
RMSE, price
electricity
Nonparametric copula to
capture multivariate
dependence
Salinas et al. 
electricity, trafﬁc,
exchange rate, solar,
taxi, wiki
Learns multivariate model via
low-rank Gaussian copula
ARMDN 
Like , but using mixture
of Gaussian’s and domain
speciﬁc feature processing
QARNN 
Conditional quantile function
estimation
SQF-RNN 
NRMSE, OWA
demand, trafﬁc, count
data, ﬁnance, M4
Models non-parametric
distributions with splines
LSTNet 
CNN + RNN + MLP
RRSE, CORR
trafﬁc, solar,
electricity, exchange
Extracts short and long
temporal patterns with a CNN
and RNN, respectively
Zhu and Laptev
calibration
daily trips
Fits an encoder (RNN) that
constructs an embedding state,
which is fed to a prediction
network (MLP)
Laptev et al. 
trafﬁc, M3
LSTM as feature extractor
Qiu et al. 
ℓ2 for MLP, SVR
RMSE, MAPE
energy, housing
Ensemble of DBNs where their
output is fed to an SVR
A-LSTM 
ℓ2, ℓ2 regularizer
electricity
consumption
Combination of LSTM with
autoencoders
Borovykh et al. 
ℓ1, ℓ2 regularizer
RMSE, MASE,
index forecasting,
exchange rate
WaveNet based model
adjusted for time series
forecasting
SAnD 
MLP + Attention
ℓ2, cross-entropy,
multi-label
classiﬁcation loss
AUROC, MASE,
Transformer based
model adjusted for time series
forecasting
Benidis et al.
Zhang 
sunspot, lynx,
exchange rate
Hybrid local model that uses
ARIMA to capture the linear
component and a NN for the
nonlinear residuals
Khashei and Bijari
sunspot, lynx,
exchange rate
Hybrid local model that uses
ARIMA and a NN for trend
correction
Deep State Space
RNN + State Space
P50, P90 quantile
trafﬁc, electricity,
tourism, M4
RNN parametrized linear
Gaussian SSM
RNN + State Space +
Normalizing Flow (NF)
trafﬁc, electricity,
exchange rate, solar,
RNN parametrized linear
Gaussian SSM combined with
normalizing ﬂow, which acts as
an emission model to handle
non-Gaussian data
ARSGLS 
Recurrent Switching
State Space + NN
trafﬁc, electricity,
exchange rate, solar,
Recurrent Switching State
Space combined with
decoder-type NN, which acts
as an emission model to handle
non-Gaussian data
MQ-RNN/CNN
RNN/CNN + MLP
QL, calibration,
Learns pre-speciﬁed grid of
Wen and Torkkola
QL, inverse
reconstruction
QL, quantile
crossing, QL over
sum of future
Combines model in with
Gaussian copula
DeepTCN 
retail demand
Learns pre-speciﬁed grid of
N-BEATS 
sMAPE, MASE,
sMAPE, MASE,
Deep, residual MLP that learns
interpretable trend and
seasonality function
Lv et al. 
Stacked autoencoder
MSE, KL sparsity
constraint
Stacked autoencoders with
logistic regression output layer
DCRNN 
MAE, MAPE,
Diffusion convolution for
spatial and RNN for temporal
dependencies
Asadi and Regan
Decomposition-based model
for spatio-temporal forecasting
Benidis et al.
Bandara et al. 
RNN + Classical
Decomposition
CIF2016, NN5
Clusters time series based on
set of features and train one
model per cluster
LSTM-MSNet 
RNN + Classical
Decomposition
sMAPE, MASE
M4, energy
Decomposition based model
with multiple seasonal patterns
Cinar et al. 
RNN + Attention
ℓ2, ℓ2 regularizer
MSE, sMAPE
energy, max
temperature, CPU
usage, air quality
Attention mechanism on top of
Deep Factors 
electricity, trafﬁc,
taxi, uber
Global RNN and a local GP
DeepGLO 
WAPE, MAPE,
electricity, trafﬁc,
Global matrix factorization
regularized by a deep leveled
ES-RNN 
MASE, sMAPE,
Locally estimated seasonality
and trend and global RNN
Kourentzes 
ME, MAE, service
intermittent demand
MLP-based intermittent
demand model
Attentional Twin RNN
point process data
Event sequence prediction
Gutierrez et al. 
MAPE, RGRMSE,
intermittent demand
MLP-based intermittent
demand model
Deep Renewal Process
P50, P90 quantile
intermittent demand
RNN-based intermittent
demand model inspired by
point processes
WaveNet 
mean opinion
trafﬁc, electricity, M4
Diluted causal convolutions
Transformer 
electricity, trafﬁc,
wind, M4, solar
Transformer with causal
convolutions and sparse
AttnAR 
electricity, trafﬁc,
solar, exchange rate
Multivariate forecasting
LSTM MAF 
electricity, trafﬁc,
solar, exchange rate
Multivariate forecasting using
normalizing ﬂows
Benidis et al.
TimeGrad 
electricity, trafﬁc,
solar, exchange rate,
taxi, wikipedia
Multivariate forecasting using
diffusion models.
P50, P90 quantile
electricity, trafﬁc,
retail, volatility
Modiﬁed transformer
architecture for improved
interpretability
MQ-Transformer
P50, P90, LT-SP
electricity, trafﬁc,
retail, volatility, retail
demand (proprietary)
Architectural improvements on
MQ-RNN/CNN for multi-step
forecasting
Informer 
electricity, weather,
sensor data
Sparse and computationally
efﬁcient transformer
architecture
Table 2: Summary of modern deep forecasting models.
Benidis et al.
Data characteristics
The amount of data available is among the easiest dimensions in choosing a deep forecasting model. First, NNs require
a minimum amount of data to be effective in comparison to other, more parsimoniously parametrized models. This
is perhaps the most important factor in successful applications of NNs in forecasting. How much data does one need
for a given application? Several important points should be discussed on this question. First, the amount of data is
often misunderstood as the number of time series but in reality the amount of data typically relates to the number of
observations. For instance, one may have only one time series but many thousands of observations, as in the case of a
time series from a real-time sensor where measurements happen every second for a year, allowing to ﬁt a complex NN
 . Second, it is probably better to see the amount of data in terms of information quantity. For instance, in ﬁnance
the amount of information of many millions of hourly transactions is limited given the very low signal-to-noise ratio
in contrast to a retailer whose products follow clear seasonality and patterns, making it easier to apply deep learning
methods. The more structured the data is (e.g., via strong seasonality or knowledge about the underlying process)
the better deep forecasting models that incorporate these structures will fare. On the contrary, if the time series are
more irregular or short, a more data-driven approach (e.g., via Transformer-based models) will often be preferable.
The importance of covariate information for the forecasting problem at hand can further help determine the correct
method. Some NN architectures need extensions to include such information while others readily accept them.
From a practical perspective, NNs have been reported to outperform demand forecasting baselines starting from 50000
observations in and from a few hundred observations in load-forecasting . Understanding better these
limitation, both theoretically and empirically, is an area of current research and is not yet well understood. See 
for some current theoretical work on sample complexity of global-local approaches for instance and for empiricial
Problem characteristics
The characteristics of the forecasting problem to be solved are natural important decision points. We list a few dimensions to consider here.
One important aspect of a model is its forecast nature, i.e., if it produces probabilistic or point forecasts. The choice
of this is highly dependent on the underlying application. To illustrate this we can examine two different forecasting
use cases: product demand and CPU utilization. In the former use case one wishes to forecast the future demand
of a product in order to take a more informed decision about the stock that is required to have in a warehouse or
to optimize the labour planning based on the trafﬁc that is expected. In the latter, the forecast of CPU utilization
could be used to identify in a timely manner if a process will fail in order to proactively resolve associated issues,
or to detect possible anomalous behaviours that could trigger some root cause analysis and system improvements.
Although in both applications a forecast is required, the end goal is different, which changes the requirements of the
chosen forecasting model. For example, for product demand the whole distribution of the future demand might be
important: one cannot rely on a single forecast value since the variance in the forecast plays an important role to avoid
out of stock issues or under/over planning the expected required labour. Therefore, in this application it is important
to use a model that focuses on predicting accurately the whole distribution. On the other hand, for CPU utilization one
might be interested in the 99-th percentile, since everything below that threshold might not be of particular interest or
does not produce any actionable alarm. In this case, a model that focuses on a particular quantile of importance is of
higher interest than a model that predicts the whole distribution with possibly worse accuracy on the selected quantile.
It is observed empirically that autoregressive models are superior in performance (in terms
of forecast accuracy) compared to state space models, especially when the data is less noisy and the forecast horizon
is not too long. This is not surprising given that the autoregressive models directly use past observations as input
features and treat own predictions as lag inputs in the multi-step forecast setting. A general rule of thumb is that if
one knows details such as the forecast horizon, the quantile to query or the exact goals of the forecasting problem in
advance and these are unlikely to change, then a discriminative model is often a good default choice. Conversely, state
space models proved to be robust when there are missing and/or noisy observations . Moreover, if the applicationspeciﬁc constraints can be incorporated in the latent state, then state space models usually perform better even in the
low-data regimes .
The length of the forecast horizon relative to the history or, more generally speaking, the importance of the historic
values for future values must further be taken into account. For example, very long forecast horizons may require
to control (e.g., via differential equations) the exponential growth in the target. A canonical example for this is
forecasting of a pandemic. This example further clariﬁes the importance of being able to produce counterfactuals for
what-if analysis (e.g., the incorporation of intervention). Not all deep forecasting models allow for this.
Benidis et al.
Other Aspects
A number of other aspects can further help to narrow the model exploration space. For example, computational
constraints (how much time/money for training is available, are there constraints on the latency during inference) can
favor “simpler” NNs, see e.g., for a discussion on multi-objective forecasting model selection. Another aspect
to consider could be CNN over RNN-based architectures. The skill set of the research team available is an important
factor. For example, probabilistic models often are more sensitive towards parametrization and identifying reasonable
parameter ranges requires in-depth knowledge. On the other extreme, troubleshooting Transformer-based models
requires deep learning experience that not every research team may possess. The time budget available for model
development and the willingness to extend existing models are further factors.
Conclusions and Avenues for Future Work
This article has attempted to provide an introduction to and an overview of NNs for forecasting or deep forecasting.
We began by providing a panorama of some of the core concepts in the modern literature on NNs chosen by their
degree of relevance for forecasting. We then reviewed the literature on recent advances in deep forecasting models.
Deep forecasting methods have received considerable attention in the literature because they excel at addressing forecasting problems with many related time series and at extracting weak signals and complex patterns from large amounts
of data. From a practical perspective, the availability of efﬁcient programming frameworks helps to alleviate many of
the pain points that practitioners experience with other forecasting methods such as manual feature engineering or the
need to derive gradients. However, NNs are not a silver bullet. For many important classes of forecasting problems
such as long-range macro-economic forecasts or other problems requiring external domain knowledge not learnable
from the data, deep forecasting methods are not the most appropriate choice and will likely never be. Still, it is our
ﬁrm belief that NNs belong to the toolbox of every forecaster, in industry and academia.
Building onto the existing promising work in NNs for forecasting, many challenges remain to be solved. We expect that
the current trends of hybridizing existing time series techniques with NNs and bringing innovations
from other related areas or general purpose techniques to forecasting will continue organically. Typical
general challenges for NNs, such as data effectiveness, are important in forecasting and likely need a special treatment
(see for an approach in time series classiﬁcation with transfer learning). Other topics of general ML interest
such as interpretability, explainability and causality (e.g., ) are of particular practical importance in the
forecasting setting. It is our hope that original methods such as new NN architectures will be pioneered in the time
series prediction sector (e.g., ) and that those will then feed back into the general NN literature to help solve
problems in other disciplines.
Beyond such organic improvements, we speculate that another area in which NNs have had tremendous impact may become important for forecasting, namely deep reinforcement learning. In contrast to current
practice, where forecasting merely serves as input to downstream decision problems (often mixed-integer nonlinear stochastic optimization problems), for example to address problems such as restocking decisions, reinforcement
learning allows to directly learn optimal decisions in business context . It will be interesting to see whether reinforcement based approaches can improve decision making – and how good forecasting models could help improve
reinforcement approaches.
As methodology advances, so will the applicability. Many potential applications of forecasting methods are underexplored. To pick areas that are close to the authors’ interests, in database management, cloud computing, and system
operations a host of applications would greatly beneﬁt from the use of principled forecasting methods (see e.g., ). Forecasting can also be used to improve core ML tasks such as hyperparameter optimization (e.g., ) and we
expect more applications to open up in this area.