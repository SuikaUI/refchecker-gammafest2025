Optuna: A Next-generation Hyperparameter Optimization Framework
 
Takuya Akiba1∗, Shotaro Sano1, Toshihiko Yanase1, Takeru Ohta1, and Masanori Koyama1
1Preferred Networks, Inc.
The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software.
The criteria we propose include (1) deﬁne-by-run API that allows users to construct the parameter search space
dynamically, (2) eﬃcient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile
architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight
experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization
software which is a culmination of our eﬀort in the development of a next generation optimization software. As an
optimization software designed with deﬁne-by-run principle, Optuna is particularly the ﬁrst of its kind. We will present
the design-techniques that became necessary in the development of the software that meets the above criteria, and
demonstrate the power of our new design through experimental results and real world applications. Our software is
available under the MIT license ( 
Introduction
Hyperparameter search is one of the most cumbersome tasks
in machine learning projects. The complexity of deep learning
method is growing with its popularity, and the framework of eﬃcient automatic hyperparameter tuning is in higher demand than
ever. Hyperparameter optimization softwares such as Hyperopt , Spearmint , SMAC , Autotune , and Vizier 
were all developed in order to meet this need.
The choice of the parameter-sampling algorithms varies across
frameworks. Spearmint and GPyOpt use Gaussian Processes, and Hyperopt employs tree-structured Parzen estimator (TPE) . Hutter et al. proposed SMAC that uses random
forests. Recent frameworks such as Google Vizier , Katib
and Tune also support pruning algorithms, which monitor
the intermediate result of each trial and kills the unpromising
trials prematurely in order to speed up the exploration. There is
an active research ﬁeld for the pruning algorithm in hyperparameter optimization. Domhan et al. proposed a method that uses
parametric models to predict the learning curve . Klein et al.
constructed Bayesian neural networks to predict the expected
learning curve . Li et al. employed a bandit-based algorithm
and proposed Hyperband .
A still another way to accelerate the optimization process is to
use distributed computing, which enables parallel processing of
multiple trials. Katib is built on Kubeﬂow, which is a computing
platform for machine learning services that is based on Kubernetes. Tune also supports parallel optimization, and uses the Ray
distributed computing platform .
However, there are several serious problems that are being
overlooked in many of these existing optimization frameworks.
Firstly, all previous hyperparameter optimization frameworks to
date require the user to statically construct the parameter-searchspace for each model, and the search space can be extremely
hard to describe in these frameworks for large-scale experiments
that involve massive number of candidate models of diﬀerent
types with large parameter spaces and many conditional variables. When the parameter space is not appropriately described
by the user, application of advanced optimization method can
be in vain. Secondly, many existing frameworks do not feature
eﬃcient pruning strategy, when in fact both parameter searching
strategy and performance estimation strategy are important for
high-performance optimization under limited resource availability . Finally, in order to accommodate with a variety
of models in a variety of situations, the architecture shall be able
to handle both small and large scale experiments with minimum
setup requirements. If possible, architecture shall be installable
with a single command as well, and it shall be designed as
an open source software so that it can continuously incorporate
newest species of optimization methods by interacting with open
source community.
In order to address these concerns, we propose to introduce the
following new design criteria for next-generation optimization
framework:
• Deﬁne-by-run programming that allows the user to dynamically construct the search space,
• Eﬃcient sampling algorithm and pruning algorithm that
allows some user-customization,
• Easy-to-setup, versatile architecture that can be deployed
for tasks of various types, ranging from light-weight experiments conducted via interactive interfaces to heavy-weight
distributed computations.
In this study, we will demonstrate the signiﬁcance of these
criteria through Optuna, an open-source optimization software
which is a culmination of our eﬀort in making our deﬁnition of
next-generation optimization framework come to reality.
We will also present new design techniques and new optimization algorithms that we had to develop in order to meet our
proposed criteria. Thanks to these new design techniques, our
implementation outperforms many major black-box optimization frameworks while being easy to use and easy to setup in
various environments. In what follows, we will elaborate each
of our proposed criteria together with our technical solutions,
and present experimental results in both real world applications
and benchmark datasets.
 
import optuna
import ...
def objective(trial):
n layers = trial.suggest int(’n layers’, 1, 4)
layers = []
for i in range(n layers):
layers.append(trial.suggest int(’n units l{}’.
format(i), 1, 128))
clf = MLPClassifier(tuple(layers))
mnist = fetch mldata(’MNIST original’)
x train , x test , y train , y test = train test split(
mnist.data, mnist.target)
clf.fit(x train , y train)
return 1.0 −clf.score(x test , y test)
study = optuna.create study()
study.optimize(objective , n trials=100)
Figure 1: An example code of Optuna’s deﬁne-by-run style API.
This code builds a space of hyperparameters for a classiﬁer of
the MNIST dataset and optimizes the number of layers and the
number of hidden units at each layer.
import hyperopt
import ...
’n units l1’: hp.randint(’n units l1’, 128),
’l2’: hp.choice(’l2’, [{
’has l2’: True,
’n units l2’: hp.randint(’n units l2’, 128),
’l3’: hp.choice(’l3’, [{
’has l3’: True,
’n units l3’: hp.randint(’n units l3’, 128),
’l4’: hp.choice(’l4’, [{
’has l4’: True,
’n units l4’: hp.randint(’n units l4’,
}, {’has l4’: False}]),
}, {’has l3’: False}]),
}, {’has l2’: False}]),
def objective(space):
layers = [space[’n units l1’] + 1]
for i in range(2, 5):
space = space[’l{}’.format(i)]
if not space[’has l{}’.format(i)]:
layers.append(space[’n units l{}’.format(i)] +
clf = MLPClassifier(tuple(layers))
mnist = fetch mldata(’MNIST original’)
x train , x test , y train , y test = train test split(
mnist.data, mnist.target)
clf.fit(x train , y train)
return 1.0 −clf.score(x test , y test)
hyperopt.fmin(fn=objective , space=space, max evals=100,
algo=hyperopt.tpe.suggest)
Figure 2: An example code of Hyperopt that has the exactly
same functionality as the code in 1. Hyperopt is an example of
deﬁne-and-run style API.
Optuna is released under the MIT license ( 
com/pfnet/optuna/), and is in production use at Preferred
Networks for more than one year.
Define-by-run API
In this section we describe the signiﬁcance of the deﬁne-by-run
principle. As we will elaborate later, we are borrowing the
term deﬁne-by-run from a trending philosophy in deep learning
frameworks that allows the user to dynamically program deep
networks. Following the original deﬁnition, we use the term
deﬁne-by-run in the context of optimization framework to refer
to a design that allows the user to dynamically construct the
search space. In deﬁne-by-run API, the user does not have to
bear the full burden of explicitly deﬁning everything in advance
about the optimization strategy.
The power of deﬁne-by-run API is more easily understood with
actual code. Optuna formulates the hyperparameter optimization
as a process of minimizing/maximizing an objective function
that takes a set of hyperparameters as an input and returns its
(validation) score. Figure 1 is an example of an objective function written in Optuna. This function dynamically constructs
the search space of neural network architecture (the number
of layers and the number of hidden units) without relying on
externally deﬁned static variables. Optuna refers to each process
of optimization as a study, and to each evaluation of objective
function as a trial. In the code of Figure 1, Optuna deﬁnes an objective function (Lines 4–18), and invokes the ‘optimize API’
that takes the objective function as an input (Line 21). Instead of
hyperparameter values, an objective function in Optuna receives
a living trial object, which is associated with a single trial.
Optuna gradually builds the objective function through the interaction with the trial object. The search spaces are constructed
dynamically by the methods of the trial object during the runtime of the objective function. The user is asked to invoke
‘suggest API’ inside the objective function in order to dynamically generate the hyperparameters for each trial (Lines 5 and
9). Upon the invocation of ‘suggest API’, a hyperparameter
is statistically sampled based on the history of previously evaluated trials. At Line 5, ‘suggest int’ method suggests a value
for ‘n layers’, the integer hyperparameter that determines the
number of layers in the Multilayer Perceptron. Using loops
and conditional statements written in usual Python syntax, the
user can easily represent a wide variety of parameter spaces.
With this functionality, the user of Optuna can even express heterogeneous parameter space with an intuitive and simple code
(Figure 3).
Meanwhile, Figure 2 is an example code of Hyperopt that has
the exactly same functionality as the Optuna code in Figure 1.
Note that the same function written in Hyperopt (Figure 2) is
signiﬁcantly longer, more convoluted, and harder to interpret. It
is not even obvious at ﬁrst glance that the code in Figure 2 is
in fact equivalent to the code in Figure 1! In order to write the
same for-loop in Figure 1 using Hyperopt, the user must prepare
the list of all the parameters in the parameter-space prior to the
exploration (see line 4-18 in Figure 2). This requirement will
lead the user to even darker nightmares when the optimization
problem is more complicated.
Modular Programming
A keen reader might have noticed in Figure 3 that the optimization code written in Optuna is highly modular, thanks to its
deﬁne-by-run design. Compatibility with modular programming
 
Table 1: Software frameworks for deep learning and hyperparameter optimization, sorted by their API styles: deﬁne-and-run and
deﬁne-by-run.
Deep Learning Frameworks
Hyperparameter Optimization Frameworks
Deﬁne-and-Run Style
(symbolic, static)
Torch , Theano , Caﬀe ,
TensorFlow , MXNet , Keras 
SMAC , Spearmint , Hyperopt , GPyOpt ,
Vizier , Katib , Tune , Autotune 
Deﬁne-by-Run Style
(imperative, dynamic)
Chainer , DyNet , PyTorch ,
TensorFlow Eager , Gluon 
Optuna 
Table 2: Comparison of previous hyperparameter optimization frameworks and Optuna. There is a checkmark for lightweight if
the setup for the framework is easy and it can be easily used for lightweight purposes.
Lightweight
Distributed
deﬁne-and-run
deﬁne-and-run
Spearmint 
deﬁne-and-run
Hyperopt 
deﬁne-and-run
Autotune 
deﬁne-and-run
Vizier 
deﬁne-and-run
deﬁne-and-run
deﬁne-and-run
Optuna (this work)
deﬁne-by-run
import sklearn
import ...
def create rf(trial):
rf max depth = trial.suggest int(’rf max depth’, 2,
return RandomForestClassifier(max depth=
rf max depth)
def create mlp(trial):
n layers = trial.suggest int(’n layers’, 1, 4)
layers = []
for i in range(n layers):
layers.append(trial.suggest int(’n units l{}’.
format(i), 1, 128))
return MLPClassifier(tuple(layers))
def objective(trial):
classifier name = trial.suggest categorical(’
classifier’, [’rf’ , ’mlp’])
if classifier name == ’rf’:
classifier obj = create rf(trial)
classifier obj = create mlp(trial)
Figure 3: An example code of Optuna for the construction of
a heterogeneous parameter-space. This code simultaneously
explores the parameter spaces of both random forest and MLP.
is another important strength of the deﬁne-by-run design. Figure 4 is another example code written in Optuna for a more
complex scenario. This code is capable of simultaneously optimizing both the topology of a multilayer perceptron (method
‘create model’) and the hyperparameters of stochastic gradient descent (method ‘create optimizer’).
The method
‘create model’ generates ‘n layers’ in Line 5 and uses a for
loop to construct a neural network of depth equal to ‘n layers’.
The method also generates ‘n units i’ at each i-th loop, a hyperparameter that determines the number of the units in the i-th
layer. The method ‘create optimizer’, on the other hand,
makes suggestions for both learning rate and weight-decay parameter. Again, a complex space of hyperparameters is simply
import chainer
import ...
def create model(trial):
n layers = trial.suggest int(’n layers’, 1, 3)
layers = []
for i in range(n layers):
n units = trial.suggest int(’n units l{}’.format
(i), 4, 128)
layers.append(L.Linear(None, n units))
layers.append(F.relu)
layers.append(L.Linear(None, 10))
return chainer.Sequential(*layers)
def create optimizer(trial, model):
lr = trial.suggest loguniform(’lr’, 1e−5, 1e−1)
optimizer = chainer.optimizers.MomentumSGD(lr=lr)
weight decay = trial.suggest loguniform(’
weight decay’, 1e−10, 1e−3)
optimizer.setup(model)
optimizer.add hook(chainer.optimizer.WeightDecay(
weight decay))
return optimizer
def objective(trial):
model = create model(trial)
optimizer = create optimizer(trial, model)
study = optuna.create study()
study.optimize(objective , n trials=100)
Figure 4: Another example of Optuna’s objective function. This
code simultaneously optimizes neural network architecture (the
create model method) and the hyperparameters for stochastic
gradient descent (the create optimizer method).
expressed in Optuna. Most notably, in this example, the methods
‘create model’ and ‘create optimizer’ are independent of
one another, so that we can make changes to each one of them
separately. Thus, the user can easily augment this code with
other conditional variables and methods for other set of parameters, and make a choice from more diverse pool of models.
Deployment
Indeed, the beneﬁt of our deﬁne-by-run API means nothing if
we cannot easily deploy the model with the best set of hyperpa-
 
rameters found by the algorithm. The above example (Figure 4)
might make it seem as if the user has to write a diﬀerent version
of the objective function that does not invoke ‘trial.suggest’
in order to deploy the best conﬁguration. Luckily, this is not
a concern. For deployment purpose, Optuna features a separate class called ‘FixedTrial’ that can be passed to objective
functions. The ‘FixedTrial’ object has practically the same
set of functionalities as the trial class, except that it will only
suggest the user deﬁned set of the hyperparameters when passed
to the objective functions. Once a parameter-set of interest is
found (e.g., the best ones), the user simply has to construct a
‘FixedTrial’ object with the parameter set.
Historical Remarks
Historically, the term deﬁne-by-run was coined by the developers of deep learning frameworks. In the beginning, most deep
learning frameworks like Theano and Torch used to be declarative, and constructed the networks in their domain speciﬁc
languages (DSL). These frameworks are called deﬁne-and-run
frameworks because they do not allow the user to alter the manipulation of intermediate variables once the network is deﬁned.
In deﬁne-and-run frameworks, computation is conducted in two
phases: (1) construction phase and (2) evaluation phase. In a
way, contemporary optimization methods like Hyperopt are built
on the philosophy similar to deﬁne-and-run, because there are
two phases in their optimization: (1) construction of the search
space and (3) exploration in the search space.
Because of their diﬃculty of programming, the deﬁne-and-run
style deep learning frameworks are quickly being replaced by
deﬁne-by-run style deep learning frameworks like Chainer ,
DyNet , PyTorch , eager-mode TensorFlow , and
Gluon. In the deﬁne-by-run style DL framework, there are no
two separate phases for the construction of the network and the
computation on the network. Instead, the user is allowed to
directly program how each variables are to be manipulated in
the network. What we propose in this article is an analogue of
the deﬁne-by-run DL framework for hyperparameter optimization, in which the framework asks the user to directly program
the parameter search-space (See Table 1) . Armed with the
architecture built on the deﬁne-by-run principle, our Optuna can
express highly sophisticated search space at ease.
Efficient Sampling and Pruning Mechanism
In general, the cost-eﬀectiveness of hyperparameter optimization framework is determined by the eﬃciency of (1) searching
strategy that determines the set of parameters that shall be investigated, and (2) performance estimation strategy that estimates
the value of currently investigated parameters from learning
curves and determines the set of parameters that shall be discarded. As we will experimentally show later, the eﬃciency of
both searching strategy and performance estimation strategy is
necessary for cost-eﬀective optimization method.
The strategy for the termination of unpromising trials is often
referred to as pruning in many literatures, and it is also well
known as automated early stopping . We, however, refer
to this functionality as pruning in order to distinguish it from the
early stopping regularization in machine learning that exists as
a countermeasure against overﬁtting. As shown in table 2, many
existing frameworks do not provide eﬃcient pruning strategies.
In this section we will provide our design for both sampling and
Sampling Methods on Dynamically Constructed
Parameter Space
There are generally two types of sampling method: relational
sampling that exploits the correlations among the parameters
and independent sampling that samples each parameter independently. The independent sampling is not necessarily a naive
option, because some sampling algorithms like TPE are
known to perform well even without using the parameter correlations, and the cost eﬀectiveness for both relational and independent sampling depends on environment and task. Our Optuna
features both, and it can handle various independent sampling
methods including TPE as well as relational sampling methods
like CMA-ES. However, some words of caution are in order
for the implementation of relational sampling in deﬁne-by-run
framework.
Relational sampling in deﬁne-by-run frameworks
One valid claim about the advantage of the old deﬁne-and-run
optimization design is that the program is given the knowledge
of the concurrence relations among the hyperparamters from the
beginning of the optimization process. Implementing of optimization methods that takes the concurrence relations among
the parameters into account is a nontrivial challenge when the
search spaces are dynamically constructed. To overcome this
challenge, Optuna features an ability to identify trial results that
are informative about the concurrence relations. This way, the
framework can identify the underlying concurrence relations
after some number of independent samplings, and use the inferred concurrence relation to conduct user-selected relational
sampling algorithms like CMA-ES and GP-BO . Being
an open source software, Optuna also allows the user to use
his/her own customized sampling procedure.
Eﬃcient Pruning Algorithm
Pruning algorithm is essential in ensuring the ”cost” part of the
cost-eﬀectiveness. Pruning mechanism in general works in two
phases. It (1) periodically monitors the intermediate objective
values, and (2) terminates the trial that does not meet the prede-
ﬁned condition. In Optuna, ‘report API’ is responsible for the
monitoring functionality, and ‘should prune API’ is responsible for the premature termination of the unpromising trials
(see Figure 5). The background algorithm of ‘should prune’
method is implemented by the family of pruner classes. Optuna features a variant of Asynchronous Successive Halving
algorithm , a recently developed state of the art method
that scales linearly with the number of workers in distributed
environment.
Asynchronous Successive Halving(ASHA) is an extension of
Successive Halving in which each worker is allowed to
asynchronously execute aggressive early stopping based on provisional ranking of trials. The most prominent advantage of
asynchronous pruning is that it is particularly well suited for
applications in distributional environment; because each worker
does not have to wait for the results from other workers at each
 
import ...
def objective(trial):
lr = trial.suggest loguniform(’lr’, 1e−5, 1e−1)
clf = sklearn.linear model.SGDClassifier(
learning rate=lr)
for step in range(100):
clf.partial fit(x train , y train , classes)
# Report intermediate objective value.
intermediate value = clf.score(x val , y val)
trial.report(intermediate value , step=step)
# Handle pruning based on the intermediate value
if trial.should prune(step):
raise TrialPruned()
return 1.0 −clf.score(x val , y val)
study = optuna.create study()
study.optimize(objective)
Figure 5: An example of implementation of a pruning algorithm
with Optuna. An intermediate value is reported at each step of
iterative training. The Pruner class stops unpromising trials
based on the history of reported values.
Algorithm 1: Pruning algorithm based on Successive Halving
Input: target trial trial, current step step, minimum
resource r, reduction factor η, minimum
early-stopping rate s.
Output: true if the trial should be pruned, false otherwise.
1 rung ←max(0, log η(⌊step/r⌋) −s)
2 if step , rηs+rung then
return false
5 value ←get trial intermediate value(trial, step)
6 values ←get all trials intermediate values(step)
7 top k values ←top k(values, ⌊|values|/η⌋)
8 if top k values = ∅then
top k values ←top k(values, 1)
11 return value < top k values
round of the pruning, the parallel computation can process multiple trials simultaneously without delay.
Algorithm 1 is the actual pruning algorithm implemented in
Optuna. Inputs to the algorithm include the trial that is subject
to pruning, number of steps, reducing factor, minimum resource
to be used before the pruning, and minimum early stopping rate.
Algorithm begins by computing the current rung for the trial,
which is the number of times the trial has survived the pruning.
The trial is allowed to enter the next round of the competition
if its provisional ranking is within top 1/η. If the number of
trials with the same rung is less than η, the best trial among the
trials with the same rung becomes promoted. In order to avoid
having to record massive number of checkpointed conﬁgurations(snapshots), our implementation does not allow repechage.
As experimentally verify in the next section, our modiﬁed implementation of Successive Halving scales linearly with the number
of workers without any problem. We will present the details of
our optimization performance in Section 5.2.
Figure 6: Overview of Optuna’s system design. Each worker
executes one instance of an objective function in each study.
The Objective function runs its trial using Optuna APIs. When
the API is invoked, the objective function accesses the shared
storage and obtains the information of the past studies from the
storage when necessary. Each worker runs the objective function
independently and shares the progress of the current study via
the storage.
Scalable and versatile System that is Easy to
Our last criterion for the next generation optimization software
is a scalable system that can handle a wide variety of tasks,
ranging from a heavy experiment that requires a massive number
of workers to a trial-level, light-weight computation conducted
through interactive interfaces like Jupyter Notebook. The ﬁgure
6 illustrates how the database(storage) is incorporated into the
system of Optuna; the trial objects shares the evaluations history
of objective functions via storage. Optuna features a mechanism
that allows the user to change the storage backend to meet his/her
For example, when the user wants to run experiment with Jupyter
Notebook in a local machine, the user may want to avoid spending eﬀort in accessing a multi-tenant system deployed by some
organization or in deploying a database on his/her own. When
there is no speciﬁcation given, Optuna automatically uses its
built-in in-memory data-structure as the storage back-end. From
general user’s perspective, that the framework can be easily used
for lightweight purposes is one of the most essential strengths
of Optuna, and it is a particularly important part of our criteria
for next-generation optimization frameworks. This lightweight
purpose compatibility is also featured by select few frameworks
like Hyperopt and GPyOt as well. The user of Optuna can also
conduct more involved analysis by exporting the results in the
pandas dataframe, which is highly compatible with interactive analysis frameworks like Jupyter Notebooks . Optuna
also provides web-dashboard for visualization and analysis of
studies in real time (see Figure 8).
Meanwhile, when the user wants to conduct distributed computation, the user of Optuna can deploy relational database as
the backend. The user of Optuna can also use SQLite database
as well. The ﬁgure 7b is an example code that deploys SQLite
database. This code conducts distributed computation by simply
executing run.py multiple times with the same study identiﬁer
and the same storage URL.
 
import ...
def objective(trial):
return objective value
study name = sys.argv 
storage = sys.argv 
study = optuna.Study(study name , storage)
study.optimize(objective)
(a) Python code: run.py
# Setup: the shared storage URL and study identifier.
STORAGE URL =' sqlite:///example.db '
STUDY ID=$(optuna create−study −−storage $STORAGE URL)
# Run the script from multiple processes and/or nodes.
# Their execution can be asynchronous.
python run.py $STUDY ID $STORAGE URL &
python run.py $STUDY ID $STORAGE URL &
python run.py $STUDY ID $STORAGE URL &
Figure 7: Distributed optimization in Optuna. Figure (a) is the
optimization script executed by one worker. Figure (b) is an
example shell for the optimization with multiple workers in a
distributed environment.
Figure 8: Optuna dashboard. This example shows the online
transition of objective values, the parallel coordinates plot of
sampled parameters, the learning curves, and the tabular descriptions of investigated trials.
Optuna’s new design thus signiﬁcantly reduces the eﬀort required for storage deployment. This new design can be easily incorporated into a container-orchestration system like Kubernetes
as well. As we verify in the experiment section, the distributed
computations conducted with our ﬂexible system-design scales
linearly with the number of workers. Optuna is also an open
source software that can be installed to user’s system with one
Experimental Evaluation
We demonstrate the eﬃciency of the new design-framework
through three sets of experiments.
Performance Evaluation Using a Collection of Tests
As described in the previous section, Optuna not only allows the
user to use his/her own customized sampling procedure that suits
Figure 9: Result of comparing TPE+CMA-ES against other
existing methods in terms of best attained objective value. Each
algorithm was applied to each study 30 times, and Paired Mann-
Whitney U test with α = 0.0005 was used to determine whether
TPE+CMA-ES outperforms each rival.
the purpose, but also comes with multiple built-in optimization
algorithms including the mixture of independent and relational
sampling, which is not featured in currently existing frameworks.
For example, Optuna can use the mixture of TPE and CMA-ES.
We compared the optimization performance of the TPE+CMA-
ES against those of other sampling algorithms on a collection
of tests for black-box optimization , which contains 56
test cases. We implemented four adversaries to compare against
TPE+CMA-ES: random search as a baseline method, Hyperopt as a TPE-based method, SMAC3 as a random-forest
based method, and GPyOpt as a Gaussian Process based method.
For TPE+CMA-ES, we used TPE for the ﬁrst 40 steps and used
CMA-ES for the rest. For the evaluation metric, we used the
best-attained objective value found in 80 trials. Following the
work of Dewancker et al. , we repeated each study 30 times
for each algorithm and applied Paired Mann-Whitney U test
with α = 0.0005 to the results in order to statistically compare
TPE+CMA-ES’s performance against the rival algorithms.
The results are shown in Figure 9. TPE+CMA-ES ﬁnds statistically worse solution than random search in only 1/56 test
cases, performs worse than Hyperopt in 1/56 cases, and performs worse than SMAC3 in 3/56 cases. Meanwhile, GPyOpt
performed better than TPE+CMA-ES in 34/56 cases in terms of
the best-attained loss value. At the same time, TPE+CMA-ES
takes an order-of-magnitude less times per trial than GPyOpt.
Figure 10 shows the average time spent for each test case.
TPE+CMA-ES, Hyperopt, SMAC3, and random search ﬁnished
one study within few seconds even for the test case with more
than ten design variables. On the other hand, GPyOpt required
twenty times longer duration to complete a study. We see that the
mixture of TPE and CMA-ES is a cost-eﬀective choice among
current lines of advanced optimization algorithms. If the time of
evaluation is a bottleneck, the user may use Gaussian Process
based method as a sampling algorithm. We plan in near future
to also develop an interface on which the user of Optuna can
easily deploy external optimization software as well.
 
Figure 10: Computational time spent by diﬀerent frameworks
for each test case.
Performance Evaluation of Pruning
We evaluated the performance gain from the pruning procedure
in the Optuna-implemented optimization of Alex Krizhevsky’s
neural network (AlexNet) on the Street View House Numbers (SVHN) dataset . We tested our pruning system together with random search and TPE. Following the experiment
in , we used a subnetwork of AlexNet (hereinafter called
simpliﬁed AlexNet), which consists of three convolutional layers
and a fully-connected layer and involves 8 hyperparameters.
For each experiment, we executed a study with one NVIDIA
Tesla P100 card, and terminated each study 4 hours into the
experiment. We repeated each study 40 times. With pruning,
both TPE and random search was able to conduct a greater
number of trials within the same time limit. On average, TPE
and random search without pruning completed 35.8 and 36.0
trials per study, respectively. On the other hand, TPE with
pruning explored 1278.6 trials on average per study, of which
1271.5 were pruned during the process. Random search with
pruning explored 1119.3 trials with 1111.3 pruned trials.
Figure 11a shows the transition of the average test errors. The
result clearly suggests that pruning can signiﬁcantly accelerate
the optimization for both TPE and random search. Our implementation of ASHA signiﬁcantly outperforms Median pruning,
a pruning method featured in Vizier. This result also suggests
that sampling algorithm alone is not suﬃcient for cost-eﬀective
optimization. The bottleneck of sampling algorithm is the computational cost required for each trial, and pruning algorithm is
necessary for fast optimization.
Performance Evaluation of Distributed Optimization
We also evaluated the scalability of Optuna’s distributed optimization. Based on the same experimental setup used in Section 5.2, we recorded the transition of the best scores obtained
by TPE with 1, 2, 4, and 8 workers in a distributed environment.
Figure 11b shows the relationship between optimization score
and execution time. We can see that the convergence speed
increases with the number of workers.
In the interpretation of this experimental results, however, we
have to give a consideration to the fact that the relationship between the number of workers and the eﬃciency of optimization
is not as intuitive as the relationship between the number of
workers and the number of trials. This is especially the case
for a SMBO such as TPE, where the algorithm is designed
to sequentially evaluate each trial. The result illustrated in Figure 11c resolves this concern. Note that the optimization scores
per the number of trials (i.e., parallelization eﬃciency) barely
changes with the number of workers. This shows that the performance is linearly scaling with the number of trials, and hence
with the number of workers. Figure 12 illustrates the result of
optimization that uses both parallel computation and pruning.
The result suggests that our optimization scales linearly with
the number of workers even when implemented with a pruning
algorithm.
Real World Applications
Optuna is already in production use, and it has been successfully applied to a number of real world applications. Optuna is
also being actively used by third parties for various purposes,
including projects based on TensorFlow and PyTorch. Some
projects use Optuna as a part of pipeline for machine-learning
framework (e.g., redshells2, pyannote-pipeline3). In this section,
we present the examples of Optuna’s applications in the projects
at Preferred Networks.
Open Images Object Detection Track 2018. Optuna was a
key player in the development of Preferred Networks’ Faster-
RCNN models for Google AI Open Images Object Detection
Track 2018 on Kaggle 4, whose dataset is at present the largest
in the ﬁeld of object detection . Our ﬁnal model, PFDet ,
won the 2nd place in the competition.
As a versatile next generation optimization software, Optuna
can be used in applications outside the ﬁeld of machine learning
as well. Followings are applications of Optuna for non-machine
learning tasks.
High Performance Linpack for TOP500. The Linpack benchmark is a task whose purpose is to measure the ﬂoating point
computation power of a system in which the system is asked
to solve a dense matrix LU factorization. The performance on
this task is used as a measure of sheer computing power of a
system and is used to rank the supercomputers in the TOP500
list5. High Performance Linpack (HPL) is one of the implementations for Linpack. HPL involves many hyperparameters, and
the performance result of any system heavily relies on them.
We used Optuna to optimize these hyperparameters in the evaluation of the maximum performance of MN-1b, an in-house
supercomputer owned by Preferred Networks.
RocksDB. RocksDB is a persistent key-value store for fast
storage that has over hundred user-customizable parameters. As
described by the developers in the oﬃcial website, ”conﬁguring
RocksDB optimally is not trivial”, and even the ”RocksDB developers don’t fully understand the eﬀect of each conﬁguration
change”6. For this experiment, we prepared a set of 500,000 ﬁles
2 
3 
4 
5 
6 
final-thoughts
 
Figure 11: The transition of average test errors of simpliﬁed AlexNet for SVHN dataset. Figure (a) illustrates the eﬀect of pruning
mechanisms on TPE and random search. Figure (b) illustrates the eﬀect of the number of workers on the performance. Figure (c)
plots the test errors against the number of trials for diﬀerent number of workers. Note that the number of workers has no eﬀect on
the relation between the number of executed trials and the test error. The result also shows the superiority of ASHA pruning over
median pruning.
Figure 12: Distributed hyperparameter optimization process for
the minimization of average test errors of simpliﬁed AlexNet for
SVHN dataset. The optimization was done with ASHA pruning.
of size 10KB each, and used Optuna to look for parameter-set
that minimizes the computation time required for applying a
certain set of operations(store, search, delete) to this ﬁle set. Out
of over hundred customizable parameters, we used Optuna to
explore the space of 34 parameters. With the default parameter
setting, RocksDB takes 372seconds on HDD to apply the set of
operation to the ﬁle set. With pruning, Optuna was able to ﬁnd
a parameter-set that reduces the computation time to 30 seconds.
Within the same 4 hours, the algorithm with pruning explores
937 sets of parameters while the algorithm without pruning only
explores 39. When we disable the time-out option for the evaluation process, the algorithm without pruning explores only 2
trials. This experiment again veriﬁes the crucial role of pruning.
Encoder Parameters for FFmpeg. FFmpeg7 is a multimedia
framework that is widely used in the world for decoding, encoding and streaming of movies and audio dataset. FFmpeg
has numerous customizable parameters for encoding. However,
ﬁnding of good encoding parameter-set for FFmpeg is a nontrivial task, as it requires expert knowledge of codec. We used
Optuna to seek the encoding parameter-set that minimizes the
reconstruction error for the Blender Open Movie Project’s ”Big
7 
Buck Bunny”8. Optuna was able to ﬁnd a parameter-set whose
performance is on par with the second best parameter-set among
the presets provided by the developers.
Conclusions
The eﬃcacy of Optuna strongly supports our claim that our new
design criteria for next generation optimization frameworks are
worth adopting in the development of future frameworks. The
deﬁne-by-run principle enables the user to dynamically construct
the search space in the way that has never been possible with
previous hyperparameter tuning frameworks. Combination of
eﬃcient searching and pruning algorithm greatly improves the
cost eﬀectiveness of optimization. Finally, scalable and versatile
design allows users of various types to deploy the frameworks
for a wide variety of purposes. As an open source software,
Optuna itself can also evolve even further as a next generation
software by interacting with open source community. It is our
strong hope that the set of design techniques we developed for
Optuna will serve as a basis of other next generation optimization frameworks to be developed in the future.
Acknowledgement.
The authors thank R. Calland, S. Tokui,
H. Maruyama, K. Fukuda, K. Nakago, M. Yoshikawa, M. Abe, H. Imamura, and Y. Kitamura for valuable feedback and suggestion.