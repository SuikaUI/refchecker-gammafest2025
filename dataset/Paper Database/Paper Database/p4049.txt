Black-box Generation of Adversarial Text Sequences to Evade Deep Learning
Classiﬁers
Ji Gao, Jack Lanchantin, Mary Lou Soffa, Yanjun Qi
Department of Computer Science, University of Virginia; {jg6yd,jjl5sw, soffa,yanjun}@virginia.edu
Abstract—Although various techniques have been proposed
to generate adversarial samples for white-box attacks on text,
little attention has been paid to a black-box attack, which is
a more realistic scenario. In this paper, we present a novel
algorithm, DeepWordBug, to effectively generate small text
perturbations in a black-box setting that forces a deep-learning
classiﬁer to misclassify a text input. We develop novel scoring
strategies to ﬁnd the most important words to modify such
that the deep classiﬁer makes a wrong prediction. Simple
character-level transformations are applied to the highestranked words in order to minimize the edit distance of the
perturbation. We evaluated DeepWordBug on two real-world
text datasets: Enron spam emails and IMDB movie reviews.
Our experimental results indicate that DeepWordBug can
reduce the classiﬁcation accuracy from 99% to
Enron and from 87% to 26% on IMDB. Our results strongly
demonstrate that the generated adversarial sequences from a
deep-learning model can similarly evade other deep models.
I. INTRODUCTION
Although deep learning has achieved remarkable results
in the ﬁeld of natural language processing (NLP), including
sentiment analysis, relation extraction, and machine translation – , a few recent studies pointed out that adding
small modiﬁcations to text inputs can fool deep classiﬁers
to incorrect classiﬁcation , . Similar phenomenon exist
in image classiﬁcation where adding tiny and often imperceptible perturbations on images could fool deep classiﬁers.
It naturally raises concerns about the robustness of deep
learning systems considering that deep learning has become
core components of many security-sensitive applications,
like text-based spam detection.
Formally, for a given classiﬁer F and test sample x, recent
literature deﬁned such perturbations as Δx and the resulting
sample x′ as an adversarial sample :
x′ = x + Δx, ∥Δx∥p < ϵ, x′ ∈X
F(x) ̸= F(x′) or F(x′) = t
Here we denote a machine learning classiﬁer as F : X →Y,
where X is the sample space, x ∈X denotes a single sample,
and Y describes the set of output classes. The strength of the
adversary, ϵ, measures the permissible transformations. The
choice of condition in Eq. (1) indicates two methods for
ﬁnding adversarial examples: whether they are untargeted
(F(x) ̸= F(x′)) or targeted (F(x′) = t).
The choice of Δ is typically an Lp-norm distance metric.
Recent studies – used three norms L∞, L2 and L0.
Formally for Δx = x′ −x ∈Rp, the Lp norm is
The L∞norm measures the maximum change in any dimension. This means an L∞adversary is limited by the
maximum change it can make to each feature, but can alter
all the features by up to that maximum . The L2 norm
corresponds to the Euclidean distance between x and x′ .
This distance can still remain small when small changes
are applied to many different features. An L0 adversary is
limited by the number of feature variables it can alter .
In addition to targeted/untargeted and Δ choices, a third
parameter for categorizing recent methods is whether their
assumption of an adversary is black-box or white-box. An
adversary may have various degrees of knowledge about the
model it tries to fool, ranging from no information to complete information. In the black box setting, an adversary is
only allowed to query the target classiﬁer and does not know
the details of learned models or the feature representations
of inputs. Since the adversary does not know the feature
set, it can only manipulate input samples by testing and
observing outputs. In the white box setting, an adversary has
access to the model, model parameters, and the feature set of
inputs. Similar to the black-box setting, the adversary is not
allowed to modify the model itself, or change the training
data used to train the model. Most studies of adversarial
examples in the literature use the white-box assumption ,
 – . One study proposed by showed that it is possible
to create adversarial samples that successfully reduce the
classiﬁcation accuracy without knowing the model structure
or parameters.
Recent studies have focused on image classiﬁcation and
typically created imperceptible modiﬁcations to pixel values through an optimization procedure – . Szegedy et
al. ﬁrst observed that DNN models are vulnerable to
adversarial perturbation (by limiting the modiﬁcation using
L2 norm) and used the Limited-memory Broyden-Fletcher-
Goldfarb-Shanno (L-BFGS) algorithm to ﬁnd adversarial
examples. Their study also found that adversarial perturbations generated from one Convolutional Neural Network
(CNN) model can also force other CNN models to produce
incorrect outputs. Subsequent papers have explored other
strategies to generate adversarial manipulations, including
2018 IEEE Symposium on Security and Privacy Workshops
© 2018, Ji Gao. Under license to IEEE.
DOI 10.1109/SPW.2018.00016
Figure 1: An example of WordBug generated adversarial sequence.
Part (1) shows an original text sample and part (2) shows an
adversarial sequence generated from the original sample in Part
(1). From part (1) to part (2), only a few characters are modiﬁed;
however this fools the deep classiﬁer to a wrong classiﬁcation.
using the linear assumption behind a model (by limits
on L∞norm), saliency maps (by limits on L0 norm),
and evolutionary algorithms . Recently, Carlini et al.
proposed a group of attacking methods with optimization
techniques to generate adversarial images with even smaller
perturbations .
Images can be naturally represented as points in a continuous Rd space (d denotes the total number of pixels in an
image). Using an Lp-norm based distance metric to limit
the modiﬁcation of images appears natural and intuitive.
However, for text sequence inputs it is hard to search for
small text modiﬁcations because of the following reasons:
1) Text tokens are categorical features. Imperceptible perturbations using Lp-norms makes sense on continuous
pixel values, but not on letters since they are discrete.
2) Each text sample includes a linearly-ordered sequence
of words, and the length of sequences varies.
Due to above reasons, the original deﬁnition of adversarial
modiﬁcations: Δx = x′ −x (from Equation
(1)) cannot
apply directly to text inputs. One feasible deﬁnition of
adversarial modiﬁcations on text is the edit distance between
text x and text x′ that is deﬁned as the minimal edit
operations that are required to change x to x′.
A few recent studies , deﬁned adversarial perturbations on RNN-based text classiﬁers. ﬁrst chose
the word at a random position in a text input, then used a
projected Fast Gradient Sign Method to perturb the word’s
embedding vector. The perturbed vector is projected to the
nearest word vector in the word embedding space, resulting
in an adversarial sequence (adversarial examples in the text
case). This procedure may, however, replace words in an
input sequence with totally irrelevant words since there is
no hard guarantee that words close in the embedding space
are semantically similar. used the “saliency map” of
input words and complicated linguistic strategies to generate
adversarial sequences that are semantically meaningful to
a human. However, this strategy is difﬁcult to perform
automatically.
We instead design scoring functions to adversarial sequences by making small edit operations to a text sequence
such that a human would consider it similar to the original
sequence. I.e., the small changes should produce adversarial
words which are imperceptibly different from the original
words. We do this by ﬁrst targeting the important tokens
in the sequence and then executing a modiﬁcation on those
tokens (deﬁned in Section II) that can effectively force a
deep classiﬁer to make a wrong decision. An example of
the adversarial sequence we deﬁne is shown in Figure 1.
The original text input is correctly classiﬁed as positive
sentiment by a deep RNN model. However, by changing
only a few characters, the generated adversarial sequence can
mislead the deep classiﬁer to a wrong classiﬁcation (negative
sentiment in this case).
Contributions: This paper presents an effective algorithm, DeepWordBug (or WordBug in short), that can generate adversarial sequences for natural language inputs to
evade deep-learning classiﬁers. Our novel algorithm has the
following properties:
• Black-box: Previous methods require knowledge of the
model structure and parameters of the word embedding
layer, while our method can work in a black-box setting.
• Effective: Using several novel scoring functions, with two
real-world text classiﬁcation tasks our WordBug can fool
two different deep RNN models more successfully than
the state-of-the-art baseline.
• Simple: WordBug uses simple character-level transformations to generate adversarial sequences, in contrast to
previous works that use projected gradient or multiple
linguistic-driven steps.
• Small perturbations to human observers: WordBug can
generate adversarial sequences that look quite similar to
seed sequences.
II. DEEPWORDBUG
For the rest of the paper, we denote samples in the form of
pair (x, y), where x = x1x2x3...xn is an input text sequence
and y ∈{1, ..., K} is a label of K classes. A machine
learning model is represented as F : X →Y, a function
mapping from the input set to the label set.
A. Recurrent Neural Networks
Recurrent neural networks (RNN) are a group of
neural networks that include a recurrent structure to capture
the sequential dependency among items of a sequence.
RNNs have been widely used and have been proven to be
effective on various NLP tasks including sentiment analysis
 , parsing and translation . Due to their recursive
nature, RNNs can model inputs of variable length and can
capture the complete set of dependencies among all items
being modeled, such as all spatial positions in a text sample.
To handle the “vanishing gradient” issue of training basic
RNNs, Hochreiter et al. proposed an RNN variant called
the Long Short-term Memory (LSTM) network that achieves
better performance comparing to vanilla RNNs on tasks with
long-term dependencies.
B. Word based modiﬁcation for adversarial sequences
In typical adversarial generation scenarios, gradients are
used to guide the change from an original sample to an
adversarial sample. However, in the black-box setting, calculating gradients is not available since the model parameters
are not observable.
Therefore we need to change the words of an input
directly without the guidance of gradients. Consider the
vast search space of possible changes (among all words
and all possible character changes), we propose to ﬁrst
determine the important words to change, and then modify
them slightly by controlling the edit distance to the original
sample. More speciﬁcally, we need a scoring function to
evaluate which words are important and should be changed
to create an adversarial sample and a method that can be
used to change those words with a control of the edit
To ﬁnd critical words for the model’s prediction in a
black-box setting, we introduce a temporal score (TS) and a
temporal tail score (TTS). These two scoring functions are
used to determine the importance of any word to the ﬁnal
prediction.
We assume the perturbation happens directly on the input
words (i.e., not on embedding, or at the “semantic” level).
We assume the perturbation approximately minimizes the
edit distance to the seed sample. We ﬁnd an efﬁcient strategy
to change a word slightly and is sufﬁcient for creating
adversarial text sequences.
In summary, the process of generating word-based adversarial samples on NLP data in the black-box setting is
a 2-step approach: (1) use a scoring function to determine
the importance of every word to the classiﬁcation result,
and rank the words based on their scores, and (2) use a
transformation algorithm to change the selected words.
C. Step 1: Token Scoring Function and Ranking
First, we construct scoring functions to determine which
words are important for the ﬁnal prediction. The proposed
scoring functions have the following properties:
• 1. Our scoring functions are able to correctly reﬂect the
importance of words for the prediction.
• 2. Our scoring functions calculate word scores without
the knowledge of the parameters and structure of the
classiﬁcation model.
• 3. Our scoring functions are efﬁcient to calculate.
In the following, we explain three scoring functions we
propose: temporal score, temporal tail score, and a combination of the two.
1) Temporal Score (TS)
Suppose the input sequence x = x1x2...xn, where xi
represents the word at the ith position. To rank words by
importance for prediction, we need to measure the effect of
the ith word on the output classiﬁcation.
In the continuous case (e.g., image), suppose a small
perturbation changes xi to x′
i. The resulting change of
prediction output ΔiF(x) can be approximated using the
partial derivative of this ith feature:
ΔiF(x) = (x′
i −xi)∇xiF(x)
However, in a black-box setting, ∇xiF(x) is not available.
Also in the text case it is difﬁcult to measure x′
i −xi since
words are discrete.
Therefore, we directly measure ΔiF(x) by removing the
ith word. Comparing the prediction before and after a word
is removed reﬂects how the word inﬂuences the classiﬁcation
result. RNNs models words of an input in a sequential
(temporal) manner. Therefore we deﬁne a so-called temporal
score (TS) of the ith word in an input x as
TS(xi) = F(x1, x2, ..., xi−1, xi) −F(x1, x2, ..., xi−1)
The temporal score of every word in an input x can be
calculated by one forward pass of the RNN, which is
inexpensive.
2) Temporal Tail Score (TTS)
The problem with the temporal score is that it scores
a word based on its preceding words. However, words
following a word are often important for the purpose of
classiﬁcation. Therefore we deﬁne the Temporal Tail Score
as the complement of the temporal score. It compares the
difference between two trailing parts of a sentence, the
one containing a certain word versus the one that does
not. The difference reﬂects whether the word inﬂuences the
ﬁnal prediction when coupled with words after itself. The
Temporal Tail Score (TTS) of word i is calculated by:
TTS(xi) = F(xi, xi+1, xi+2, ..., xn)−F(xi+1, xi+2, ..., xn)
3) Combined Score
Since the temporal score and temporal tail scores model
the importance of a word from two opposition directions of
a text sequence, we can combine the two. We calculate the
combined scoring function as:
Combined Score = TS + λ(TTS),
where λ is a hyperparameter.
Once we calculate the importance score of each word in
an input, we select the top m words to perturb in order to
create an adversarial sequence.
D. Step 2: Token Transformer
Previous approaches (summarized in Table V) change
words following the gradient direction (gradient of the target
adversarial class w.r.t the word), or following some perturbation guided by the gradient. However, in our case there
is no gradient direction available. Therefore, we propose
an efﬁcient method to modify a word, and we do this by
deliberately creating misspelled words.
The key observation is that words are symbolic and
learning-based classiﬁcation programs handle NLP words
through a dictionary to represent a ﬁnite set of possible
words. The size of the typical NLP dictionary is much
smaller than the possible combinations of characters at
a similar length (e.g., about 26n for the English case).
(a) Prediction process on an input sentence. (b) The curve of prediction score and temporal score.
Figure 2: Illustration of RNN model prediction process and Temporal Score
Table I: Different transformer functions and their results
Substitute
This means if we deliberately create misspelled words on
important words, we can easily convert those important
words to “unknown” (i.e., words not in the dictionary). The
unknown words are mapped to the “unknown” embedding
vector in deep-learning modeling. Our results (Section III)
strongly indicate that this simple strategy can effectively
force RNN models to make a wrong classiﬁcation.
To create such a misspelling, many strategies can be used.
However, we prefer small changes to the original word as
we want the generated adversarial sequences and its seed
input appear (visually or morphological) similar to human
observers. Therefore, we prefer methods with a small edit
distance and use the Levenshtein distance , which is
a metric measuring the similarity between sequences. We
propose four similar methods: (1) substitute a letter in the
word with a random letter, (2) delete a random letter from
the word, (3) insert a random letter in the word, and (4)
swap two adjacent letters in the word. The edit distance for
the substitution, deletion and insertion operations is 1 and 2
for the swap operation.
These methods do not guarantee the original word is
changed to a misspelled word. It is possible for a word
to “collide” with another word after the transformation.
However, the probability of collision is very small as there
are 267 ≈8 × 109 combinations for 7 letter words without
hyphens and apostrophes, but a dictionary often includes no
more than 30000 words, making the space very sparse.
The adversarial sample generation of DeepWordBug is
summarized in Algorithm 1.
III. EXPERIMENTS ON EFFECTIVENESS OF
ADVERSARIAL SEQUENCES
We evaluate the effectiveness of our algorithm by conducting experiments on different RNN models across two
real-world NLP datasets. In particular, we want to answer
the following research questions: (1). Does the accuracy of
Algorithm 1 DeepWordBug algorithm with the combined
Input: Input sequence x = x1x2 . . . xn, RNN classiﬁer F(·), maximum
allowed number of words changed m, hyperparameter λ.
1: for i = 1..n do
Stemporal(i) = F(x1x2...xi) −F(x1x2...xi−1)
3: end for
4: for i = n..1 do
Stail(i) = F(xi+1xi+2...xn) −F(xixi+1...xn)
6: end for
7: Scombined = Stemporal + λStail
8: Sort Scombined into an ordered index list: L1 .. Ln
10: for i = 1..m do
Li = Transform(x′
12: end for
13: Return x′
deep learning models decrease when feeding the adversarial
samples? (2). Does the adversarial samples generated by our
method transfers between models?
A. Experimental Setup
Datasets: In our experiments, we use the Large Movie
Review Dataset (IMDB Dataset) and the Enron Spam
Dataset .
The IMDB Movie Review Dataset contains 50000 highly
polarized movie reviews, 25000 for training and 25000 for
testing. We train an RNN model to classify the movie
reviews into 2 classes: positive and negative.
The Enron Spam Dataset is a subset of the original Enron
Email Dataset. The goal is to train a spam ﬁlter that can
determine whether a certain message is spam or not. We
use a subset containing 3,672 ham (i.e. not spam) emails,
and 1,500 spam emails.
Details of the datasets are listed in Table II.
Target deep models: To show that our method is effective, we performed our experiments on both uni- and bidirectional LSTMs.
The ﬁrst model contains a random embedding layer, a
uni-directional LSTM with 100 hidden nodes and a fully
connected layer for the classiﬁcation. Without adversarial
examples, this model achieves 84% accuracy on the IMDB
Dataset and 99% accuracy on the Enron Spam Dataset.
The second model is the same as the ﬁrst, except with a
Table II: Dataset details
Enron Spam Dataset
Sample type
Movie reviews
Sentiment analysis
Spam Detection
Avg. length
215.63 words
148.96 words
Table III: Comparison of the accuracy on different methods, ﬁrst
row shows the original model accuracy in non-adversarial setting.
Each row after show the model accuracy on generated adversarial
samples of one algorithm. (Number of words changed m = 20,
The lower score represents a better performance.)
No adversary
Replace-1 score
WordBug - Tail
bi-directional LSTM (also with 100 hidden nodes) instead
of uni-directional. Without adversarial examples, it achieves
86% accuracy on the IMDB Dataset and 98% accuracy on
the Enron Spam Dataset.
Baselines: We implemented the following attacking algorithms to generate adversarial samples:
• Projected FGSM: L∞attack from . In our implementation, we use the Fast Gradient Sign Method code
from Cleverhans , a library developed by the original
authors. As we discussed, this method is not black-box.
• Random + DeepWordBug Transformer: This technique
randomly selects words to change and use our transformer
to change the words.
Our method: We use our socring functions to better mutate
words. In our implementations, we use different score functions: replace-1 score, temporal score, temporal tail score
and the combined score. After that, we use our tranformer
to change the words.
Platform: We train the target deep-learning models and
implement attacking methods using Keras with Tensorﬂow
as back-end. We use Nvidia GTX Titan cards.
Performance: Performance of the attacking methods is
measured by the accuracy of the deep-learning models on
the generated adversarial sequences. The lower the accuracy
the more effective the attacking method is. Essentialy it
indicates the adversarial samples can successfully fool the
deep-learning classiﬁer model. The number of words that is
allowed for modiﬁcation is a hyperparameter.
B. Experimental Results on Classiﬁcation
We analyze the effectiveness of the attacks on two deep
models (uni- and bi-directional LSTMs). The results of
model accuracy are summarized in Table III. Detailed experimental results at different numbers of allowed word
Table IV: Result of the transferability of WordBug: The values
are the accuracy of the target model tested on the adversarial
samples. Different from LSTM1 and Bi-LSTM1 which are trained
with randomly-initialized embedding, LSTM2 and Bi-LSTM2 are
models trained with a pretrained word embedding.
From \Target at
modiﬁcations are presented in Figure 3. The results of unidirectional LSTM are in Figure 3 (a)(b), and the results of
bi-directional LSTM are in Figure 3 (c)(d).
From Figure 3, we ﬁrst see that the model has a signiﬁcantly lower accuracy when classifying the adversarial
samples generated by our method on both datasets when
compared to the accuracy results from the original test
samples. On the IMDB Dataset, changing 20 words per
review using WordBug-Combined reduced the model accuracy from 86% to around 41%. As the movie reviews
have an average length of 215 words, we consider the 20word modiﬁcation as effective. On the Enron Spam Dataset,
changing 20 words following WordBug-Combined reduced
the model accuracy from 99% to around 44%. For the bidirectional model, changing 20 words on every sequence
reduce model accuracy from 86% to around 26% on the
IMDB Dataset and from 99% to around 40% on the Enron
Spam Dataset. We can see that randomly choosing words to
change (i.e., Random in Table III) has little inﬂuence on the
ﬁnal result.
Surprisingly our method achieves better results when
compared with the projected FGSM which is a white-box
attack. The improvement is most likely because the selection
of words is more important than how to change the words.
Since the projected FGSM selects words randomly, it does
not achieve as sound performance as ours.
It is also interesting to compare different score functions
that we proposed. On both the IMDB and Enron datasets, the
combined score performs notably better than the temporal
score and the tail temporal score. It utilizes more information
compared to other score functions. The Replace-1 score does
not perform well in these datasets, presumably because it
does not consider the temporal relationship among words.
C. Transferability of the adversarial sequences
Next, we evaluate the transferability of adversarial sequences generated from our methods. Previous studies have
found that transferability is an important property of adversarial image samples: adversarial images generated for
a certain DNN model can successfully fool another DNN
model for the same task, i.e., transferred to another model.
We use the combined score and the substitution transformer to generate adversarial samples. The number of
words we change is 20. The results in Table IV are acquired
by feeding adversarial sequences generated by one RNN
model to another RNN model on the same task.
Figure 3: Experiment results. The
X axis represents the number of
modiﬁed words, and the Y axis
corresponds to the test accuracy on
adversarial samples generated using the respective attacking methods.
Uni-directional
on the IMDB Dataset (b) Unidirectional LSTM on the Enron
Spam Dataset (c) Bi-directional
LSTM on IMDB Dataset (d) Bidirectional LSTM on the Enron
Spam Dataset
Table V: Prior works
Modiﬁcations
Swapping two characters
Gradient + Projection
modiﬁed (L0)
Complicated
Linguistic-driven
From the table, we see that most adversarial samples can
successfully transfer to other models, even to those models
with different word embeddings. This experiment demonstrates that our method can successfully ﬁnd those words
that are important for classiﬁcation and the transformation
is effective across multiple models.
IV. CONNECTING TO PREVIOUS STUDIES
Compared to studies of adversarial examples on images,
little attention has been paid on generating adversarial sequences on text. We compare the most relevant two and
ours in Table V. (1) Papernot et.al., applied gradient-based
adversarial modiﬁcations directly to NLP inputs targeting
RNN-based classiﬁers in . The resulting samples are
called “adversarial sequence,” and we also adopt the name in
this paper. The study proposed a white-box adversarial attack
called projected Fast Gradient Sign Method and applied
it repetitively to modify an input text until the generated
sequence is misclassiﬁed. It ﬁrst randomly picks a word,
and then uses the gradient to generate a perturbation on the
corresponding word vector. Then it maps the perturbed word
vector into the nearest word based on Euclidean distance
in the word embedding space. If the sequence is not yet
misclassiﬁed, the algorithm will then randomly pick another
position in the input. (2) Recently, used the embedding gradient to determine important words. The technique
then uses heuristic driven rules together with hand-crafted
synonyms and typos. Differently, from ours, this study is
a white-box attack because it accesses the gradient of the
model. (3) Another paper measures the importance of
each word to a speciﬁc class using the word frequency
from that class’s training data. Then the study uses heuristic
driven techniques to generate adversarial samples by adding,
modifying or removing important words. Differently, this
method needs to access a large set of labeled samples.
In summary, previous approaches do not apply to blackbox settings. Besides previous approaches mostly used
heuristic-driven and complicated modiﬁcations. We summarize the differences between our method and the previous
studies on generating adversarial text samples in Table V.
Our method is black-box while previous approaches all used
the stronger white-box assumption. Our method uses the
edit distance at the sequence input space to search for the
adversarial perturbations. Also, our modiﬁcation algorithm
is simpler compared to previous approaches.
V. CONCLUSION
In this paper we introduce a vulnerability with deep
learning models for text classiﬁcation. We present a novel
framework, DeepWordBug to generate adversarial text sequences that can mislead deep learning models by exploiting
this vulnerability. Our method has the following advantages:
• Black-box: DeepWordBug generates adversarial samples
in a black-box manner.
• Performance: While minimizing edit distance (approximately minimized), DeepWordBug achieves better performance comparing to baseline methods on two NLP
datasets across multiple deep learning architectures.
Our experimental results indicate that DeepWordBug results in about 70% decrease from the original classiﬁcation
accuracy for two state-of-the-art word-level LSTM models
across two different datasets. We also demonstrate that
the adversarial samples generated on one model can be
successfully transferred to other models, reducing the target
model accuracy from around 90% to 30-60%.