Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 452–457,
Baltimore, Maryland, USA, June 23-25 2014. c⃝2014 Association for Computational Linguistics
Comparing Automatic Evaluation Measures for Image Description
Desmond Elliott and Frank Keller
Institute for Language, Cognition, and Computation
School of Informatics, University of Edinburgh
 , 
Image description is a new natural language generation task, where the aim is to
generate a human-like description of an image. The evaluation of computer-generated
text is a notoriously difﬁcult problem, however, the quality of image descriptions has
typically been measured using unigram
BLEU and human judgements. The focus
of this paper is to determine the correlation
of automatic measures with human judgements for this task. We estimate the correlation of unigram and Smoothed BLEU, TER,
ROUGE-SU4, and Meteor against human
judgements on two data sets. The main
ﬁnding is that unigram BLEU has a weak
correlation, and Meteor has the strongest
correlation with human judgements.
Introduction
Recent advances in computer vision and natural
language processing have led to an upsurge of research on tasks involving both vision and language.
State of the art visual detectors have made it possible to hypothesise what is in an image , paving
the way for automatic image description systems.
The aim of such systems is to extract and reason
about visual aspects of images to generate a humanlike description. An example of the type of image
and gold-standard descriptions available can be
seen in Figure 1. Recent approaches to this task
have been based on slot-ﬁlling , combining web-scale ngrams , syntactic tree substitution
 , and description-by-retrieval
 . Image description has been compared
to translating an image into text or summarising an image
1. An older woman with a small dog in the snow.
2. A woman and a cat are outside in the snow.
3. A woman in a brown vest is walking on the
snow with an animal.
4. A woman with a red scarf covering her head
walks with her cat on snow-covered ground.
5. Heavy set woman in snow with a cat.
Figure 1: An image from the Flickr8K data set and
ﬁve human-written descriptions. These descriptions vary in the adjectives or prepositional phrases
that describe the woman (1, 3, 4, 5), incorrect or uncertain identiﬁcation of the cat (1, 3), and include
a sentence without a verb (5).
 , resulting in the adoption of the
evaluation measures from those communities.
In this paper we estimate the correlation of human judgements with ﬁve automatic evaluation
measures on two image description data sets. Our
work extends previous studies of evaluation measures for image description ,
which focused on unigram-based measures and reported agreement scores such as Cohen’s κ rather
than correlations. The main ﬁnding of our analysis
is that TER and unigram BLEU are weakly corre-
lated against human judgements, ROUGE-SU4 and
Smoothed BLEU are moderately correlated, and the
strongest correlation is found with Meteor.
Methodology
We estimate Spearman’s ρ for ﬁve different automatic evaluation measures against human judgements for the automatic image description task.
Spearman’s ρ is a non-parametric correlation coefﬁcient that restricts the ability of outlier data
points to skew the co-efﬁcient value. The automatic
measures are calculated on the sentence level and
correlated against human judgements of semantic
correctness.
We perform the correlation analysis on the Flickr8K
data set of Hodosh et al. , and the data set of
Elliott and Keller .
The test data of the Flickr8K data set contains
1,000 images paired with ﬁve reference descriptions. The images were retrieved from Flickr, the
reference descriptions were collected from Mechanical Turk, and the human judgements were
collected from expert annotators as follows: each
image in the test data was paired with the highest
scoring sentence(s) retrieved from all possible test
sentences by the TRI5SEM model in Hodosh et al.
 . Each image–description pairing in the test
data was judged for semantic correctness by three
expert human judges on a scale of 1–4. We calculate automatic measures for each image–retrieved
sentence pair against the ﬁve reference descriptions
for the original image.
The test data of Elliott and Keller contains 101 images paired with three reference descriptions. The images were taken from the PAS-
CAL VOC Action Recognition Task, the reference
descriptions were collected from Mechanical Turk,
and the judgements were also collected from Mechanical Turk. Elliott and Keller generated two-sentence descriptions for each of the test
images using four variants of a slot-ﬁlling model,
and collected ﬁve human judgements of the semantic correctness and grammatical correctness of
the description on a scale of 1–5 for each image–
description pair, resulting in a total of 2,042 human
judgement–description pairings. In this analysis,
we use only the ﬁrst sentence of the description,
which describes the event depicted in the image.
Automatic Evaluation Measures
BLEU measures the effective overlap between a
reference sentence X and a candidate sentence Y.
It is deﬁned as the geometric mean of the effective
n-gram precision scores, multiplied by the brevity
penalty factor BP to penalise short translations. pn
measures the effective overlap by calculating the
proportion of the maximum number of n-grams
co-occurring between a candidate and a reference
and the total number of n-grams in the candidate
text. More formally,
BLEU = BP·exp
ngram∈ccountclip(ngram)
ngram∈ccount(ngram)
Unigram BLEU without a brevity penalty has been
reported by Kulkarni et al. , Li et al. ,
Ordonez et al. , and Kuznetsova et al. ;
to the best of our knowledge, the only image description work to use higher-order n-grams with
BLEU is Elliott and Keller . In this paper we
use the smoothed BLEU implementation of Clark et
al. to perform a sentence-level analysis, setting n = 1 and no brevity penalty to get the unigram
BLEU measure, or n = 4 with the brevity penalty
to get the Smoothed BLEU measure. We note that a
higher BLEU score is better.
ROUGE measures the longest common subsequence of tokens between a candidate Y and reference X. There is also a variant that measures the cooccurrence of pairs of tokens in both the candidate
and reference (a skip-bigram): ROUGE-SU*. The
skip-bigram calculation is parameterised with dskip,
the maximum number of tokens between the words
in the skip-bigram. Setting dskip to 0 is equivalent to
bigram overlap and setting dskip to ∞means tokens
can be any distance apart. If α = |SKIP2(X,Y)|
is the number of matching skip-bigrams between
the reference and the candidate, then skip-bigram
ROUGE is formally deﬁned as:
RSKIP2 = α /
ROUGE has been used by only Yang et al. 
to measure the quality of generated descriptions,
using a variant they describe as ROUGE-1. We set
dskip = 4 and award partial credit for unigram only
matches, otherwise known as ROUGE-SU4. We use
ROUGE v.1.5.5 for the analysis, and conﬁgure the
evaluation script to return the result for the average
score for matching between the candidate and the
references. A higher ROUGE score is better.
TER measures the number of modiﬁcations a human would need to make to transform a candidate
Y into a reference X. The modiﬁcations available
are insertion, deletion, substitute a single word, and
shift a word an arbitrary distance. TER is expressed
as the percentage of the sentence that needs to be
changed, and can be greater than 100 if the candidate is longer than the reference. More formally,
|reference tokens|
TER has not yet been used to evaluate image description models. We use v.0.8.0 of the TER evaluation tool, and a lower TER is better.
Meteor is the harmonic mean of unigram precision and recall that allows for exact, synonym, and
paraphrase matchings between candidates and references. It is calculated by generating an alignment
between the tokens in the candidate and reference
sentences, with the aim of a 1:1 alignment between
tokens and minimising the number of chunks ch
of contiguous and identically ordered tokens in the
sentence pair. The alignment is based on exact token matching, followed by Wordnet synonyms, and
then stemmed tokens. We can calculate precision,
recall, and F-measure, where m is the number of
aligned unigrams between candidate and reference.
Meteor is deﬁned as:
M = (1−Pen)·Fmean
|unigrams in candidate|
|unigrams in reference|
We calculated the Meteor scores using release 1.4.0
with the package-provided free parameter settings
of 0.85, 0.2, 0.6, and 0.75 for the matching components. Meteor has not yet been reported to evaluate
co-efﬁcient
n = 17,466
E&K 
co-efﬁcient
ROUGE SU-4
Smoothed BLEU
Unigram BLEU
Table 1: Spearman’s correlation co-efﬁcient of automatic evaluation measures against human judgements. All correlations are signiﬁcant at p < 0.001.
the performance of different models on the image
description task; a higher Meteor score is better.
We performed the correlation analysis as follows.
The sentence-level evaluation measures were calculated for each image–description–reference tuple. We collected the BLEU, TER, and Meteor
scores using MultEval , and the
ROUGE-SU4 scores using the RELEASE-1.5.5.pl
script. The evaluation measure scores were then
compared with the human judgements using Spearman’s correlation estimated at the sentence-level.
Table 1 shows the correlation co-efﬁcients between
automatic measures and human judgements and
Figures 2(a) and (b) show the distribution of scores
for each measure against human judgements. To
classify the strength of the correlations, we followed the guidance of Dancey and Reidy ,
who posit that a co-efﬁcient of 0.0–0.1 is uncorrelated, 0.11–0.4 is weak, 0.41–0.7 is moderate,
0.71–0.90 is strong, and 0.91–1.0 is perfect.
On the Flickr8k data set, all evaluation measures
can be classiﬁed as either weakly correlated or moderately correlated with human judgements and all
results are signiﬁcant.
TER is only weakly correlated with human judgements but could prove
useful in comparing the types of differences between models. An analysis of the distribution of
TER scores in Figure 2(a) shows that differences in
candidate and reference length are prevalent in the
image description task. Unigram BLEU is also only
weakly correlated against human judgements, even
though it has been reported extensively for this task.
METEOR ρ= 0.524
ROUGE-SU4 ρ= 0.435
Smoothed BLEU ρ= 0.429
Unigram BLEU ρ= 0.345
TER ρ= -0.279
Sentence-level automated measure score
Human Judgement
(a) Flick8K data set, n=17,466.
METEOR ρ= 0.233
ROUGE-SU4 ρ= 0.188
Smoothed BLEU ρ= 0.177
Unigram BLEU ρ= 0.0965
TER ρ= -0.0443
Sentence-level automated measure score
Human Judgement
(b) E&K data set, n=2,042.
Figure 2: Distribution of automatic evaluation measures against human judgements. ρ is the correlation
between human judgements and the automatic measure. The intensity of each point indicates the number
of occurrences that fall into that range.
Figure 2(a) shows an almost uniform distribution
of unigram BLEU scores, regardless of the human
judgement. Smoothed BLEU and ROUGE-SU4 are
moderately correlated with human judgements, and
the correlation is stronger than with unigram BLEU.
Finally, Meteor is most strongly correlated measure against human judgements. A similar pattern
is observed in the Elliott and Keller data set,
though the correlations are lower across all measures. This could be caused by the smaller sample
size or because the descriptions were generated
by a computer, and not retrieved from a collection
of human-written descriptions containing the goldstandard text, as in the Flickr8K data set.
Qualitative Analysis
Figure 3 shows two images from the test collection of the Flickr8K data set with a low Meteor
score and a maximum human judgement of semantic correctness. The main difference between the
candidates and references are in deciding what to
describe (content selection), and how to describe it
(realisation). We can hypothesise that in both translation and summarisation, the source text acts as a
lexical and semantic framework within which the
translation or summarisation process takes place.
In Figure 3(a), the authors of the descriptions made
different decisions on what to describe. A decision
has been made to describe the role of the ofﬁcials in
the candidate text, and not in the reference text. The
underlying cause of this is an active area of research
in the human vision literature and can be attributed
to bottom-up effects, such as saliency , top-down contextual effects , or rapidly-obtained scene properties . In (b), we can see the problem
of deciding how to describe the selected content.
The reference uses a more speciﬁc noun to describe
the person on the bicycle than the candidate.
Discussion
There are several differences between our analysis
and that of Hodosh et al. . First, we report
Spearman’s ρ correlation coefﬁcient of automatic
measures against human judgements, whereas they
report agreement between judgements and automatic measures in terms of Cohen’s κ. The use of
κ requires the transformation of real-valued scores
into categorical values, and thus loses information; we use the judgement and evaluation measure
scores in their original forms. Second, our use of
Spearman’s ρ means we can readily use all of the
available data for the correlation analysis, whereas
Hodosh et al. report agreement on thresholded subsets of the data. Third, we report the correlation coefﬁcients against ﬁve evaluation measures,
Candidate: Football players gathering to contest something to collaborating ofﬁcials.
Reference: A football player in red and white
is holding both hands up.
Candidate: A man is attempting a stunt with a
Reference: Bmx biker Jumps off of ramp.
Figure 3: Examples in the test data with low Meteor scores and the maximum expert human judgement.
(a) the candidate and reference are from the same image, and show differences in what to describe, in
(b) the descriptions are retrieved from different images and show differences in how to describe an image.
some of which go beyond unigram matchings between references and candidates, whereas they only
report unigram BLEU and unigram ROUGE. It is
therefore difﬁcult to directly compare the results
of our correlation analysis against Hodosh et al.’s
agreement analysis, but they also reach the conclusion that unigram BLEU is not an appropriate measure of image description performance. However,
we do ﬁnd stronger correlations with Smoothed
BLEU, skip-bigram ROUGE, and Meteor.
In contrast to the results presented here, Reiter
and Belz found no signiﬁcant correlations
of automatic evaluation measures against human
judgements of the accuracy of machine-generated
weather forecasts. They did, however, ﬁnd significant correlations of automatic measures against
ﬂuency judgements. There are no ﬂuency judgements available for Flickr8K, but Elliott and Keller
 report grammaticality judgements for their
data, which are comparable to ﬂuency ratings. We
failed to ﬁnd signiﬁcant correlations between grammatlicality judgements and any of the automatic
measures on the Elliott and Keller data. This
discrepancy could be explained in terms of the differences between the weather forecast generation
and image description tasks, or because the image
description data sets contain thousands of texts and
a few human judgements per text, whereas the data
sets of Reiter and Belz included hundreds
of texts with 30 human judges.
Conclusions
In this paper we performed a sentence-level correlation analysis of automatic evaluation measures
against expert human judgements for the automatic
image description task. We found that sentencelevel unigram BLEU is only weakly correlated with
human judgements, even though it has extensively
reported in the literature for this task. Meteor was
found to have the highest correlation with human
judgements, but it requires Wordnet and paraphrase
resources that are not available for all languages.
Our ﬁndings held when judgements were made on
human-written or computer-generated descriptions.
The variability in what and how people describe
images will cause problems for all of the measures
compared in this paper. Nevertheless, we propose
that unigram BLEU should no longer be used as
an objective function for automatic image description because it has a weak correlation with human
accuracy judgements. We recommend adopting
either Meteor, Smoothed BLEU, or ROUGE-SU4 because they show stronger correlations with human
judgements. We believe these suggestions are also
applicable to the ranking tasks proposed in Hodosh
et al. , where automatic evaluation scores
could act as features to a ranking function.
Acknowledgments
Alexandra Birch and R. Calen Walshe, and the
anonymous reviewers provided valuable feedback
on this paper. The research is funded by ERC
Starting Grant SYNPROC No. 203427.