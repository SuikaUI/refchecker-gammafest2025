Proceedings of the 3rd Workshop on Narrative Understanding, pages 48–55
June 11, 2021. ©2021 Association for Computational Linguistics
Gender and Representation Bias in GPT-3 Generated Stories
University of California, Berkeley
 
David Bamman
University of California, Berkeley
 
Using topic modeling and lexicon-based word
similarity, we ﬁnd that stories generated by
GPT-3 exhibit many known gender stereotypes. Generated stories depict different topics and descriptions depending on GPT-3’s perceived gender of the character in a prompt,
with feminine characters1 more likely to be associated with family and appearance, and described as less powerful than masculine characters, even when associated with high power
verbs in a prompt. Our study raises questions
on how one can avoid unintended social biases
when using large language models for storytelling.
Introduction
Advances in large language models have allowed
new possibilities for their use in storytelling, such
as machine-in-the-loop creative writing and narrative generation for games . However, ﬁctional stories can reinforce real stereotypes, and artiﬁcially generated
stories are no exception. Language models mimic
patterns in their training data, parroting or even
amplifying social biases .
An ongoing line of research examines the nature
and effects of these biases in natural language generation . Language models generate different occupations and levels of respect for
different genders, races, and sexual orientations
 . Abid et al.
 showed that GPT-3’s association of Muslims and violence can be difﬁcult to diminish, even
when prompts include anti-stereotype content.
Our work focuses on representational harms in
generated narratives, especially the reproduction
1We use “feminine character" to refer to characters with
feminine pronouns, honoriﬁcs, or names, and ditto for “masculine character". See §3.1 for details.
Douloti understood some and didn’t understand some. But
he didn’t care to understand. It was enough for him to know
the facts of the situation and why his mother had left ...
Douloti understood some and didn’t understand some. But
more, she could tell that Nenn had sympathy for one who
had given up life. Sister Nenn went on with her mending ...
Figure 1: GPT-3 can assign different gender pronouns
to a character across different generations, as shown
in this example using a prompt, in bold, pulled from
Mahasweta Devi’s Imaginary Maps.
of gender stereotypes found in ﬁlm, television, and
books. We use GPT-3, a large language model
that has been released as a commercial product and
thus has potential for wide use in narrative generation tasks . Our experiments compare GPT-3’s
stories with literature as a form of domain control, using generated stories and book excerpts that
begin with the same sentence.
We examine the topic distributions of books
and GPT-3 stories, as well as the amount of attention given to characters’ appearances, intellect,
and power. We ﬁnd that GPT-3’s stories tend to
include more masculine characters than feminine
ones (mirroring a similar tendency in books), and
identical prompts can lead to topics and descriptions that follow social stereotypes, depending on
the prompt character’s gender. Stereotype-related
topics in prompts tend to persist further in a story
if the character’s gender aligns with the stereotype.
Finally, using prompts containing different verbs,
we are able to steer GPT-3 towards more intellectual, but not more powerful, characters. Code
and materials to support this work can be found at
 
Our prompts are single sentences containing main
characters sampled from 402 English contemporary ﬁction books, which includes texts from the
Black Book Interactive Project, global Anglophone
ﬁction, Pulitzer Prize winners, and bestsellers reported by Publisher’s Weekly and the New York
Times. We use BookNLP to ﬁnd main characters
and sentences containing them . We deﬁne a main character as someone who
is within their book’s top 2% most frequent characters and mentioned at least 50 times. Every prompt
is longer than 3 tokens, does not contain feminine
or masculine pronouns, is from the main narrative
and not dialogue, and contains only one singletoken character name. This results in 2154 characters, with 10 randomly selected prompts each.
We use the GPT-3 API to obtain 5 text completions per prompt, with the davinci model, a
temperature of 0.9, and a limit of 1800 tokens. A
high temperature is often recommended to yield
more “creative" responses . We also pull excerpts that begin with
each prompt from the original books, where each
excerpt length is the average length of stories generated by that prompt. This human-authored text
provides a control that contains the same main character names and initial content as GPT-3 data. The
collection of generated stories contains over 161
million tokens, and the set of book excerpts contains over 32 million tokens.
Text processing methods
We use BookNLP’s tokenizer and dependency
parser on our data , followed by coreference resolution on named entities using the model annotated and trained on literature by Bamman et al.
 . Pronoun chains containing the same character name within the same story are combined.
Gender inference
Depending on the context, gender may refer to a
person’s self-determined identity, how they express
their identity, how they are perceived, and others’
social expectations of them . Gender inference raises
many ethical considerations and carries a risk of
harmful misgendering, so it is best to have individuals self-report their gender . However, ﬁctional characters typically do not state their
genders in machine-generated text, and GPT-3 may
gender a character differently from the original
book. Our study focuses on how GPT-3 may perceive a character’s gender based on textual features.
Thus, we infer conceptual gender, or gender used
by a perceiver, which may differ from the gender
experienced internally by an individual being perceived .
First, we use a character’s pronouns (he/him/his,
she/her/hers, their/theirs) as a rough heuristic for
gender. For book character gender, we aggregate
pronouns for characters across all excerpts, while
for generated text, we assign gender on a per-story
basis. Since coreference resolution can be noisy,
we label a character as feminine if at least 75%
of their pronouns are she/her, and a character as
masculine if at least 75% of their pronouns are
he/his. The use of pronouns as the primary gendering step labels the majority of main characters
(Figure 2). This approach has several limitations.
Gender and pronoun use can be ﬂuid, but we do
not determine which cases of mixed-gender pronouns are gender ﬂuidity rather than coreference
error. Coreference models are also susceptible to
gender biases , and they are
not inclusive of nonbinary genders and pronouns
 .
Out of 734,560 characters, 48.3% have no pronouns. For these characters, we perform a second
step of estimating expected conceptual gender by
name, ﬁrst using a list of gendered honoriﬁcs if
they appear.2 Then, if a name has no pronouns or
honoriﬁcs, we use U.S. birth names from 1990 to
2019 , labeling a name as a gender if at least 90% of birth
names have that gender. This step also has limitations. The gender categories of names are not exact, and the association between a name and gender
can change over time .
Some cultures do not commonly gender names,
and U.S. name lists do not always generalize to
names from other countries. Still, humans and
NLP models associate many names with gender
and consequently, with gender stereotypes . We assume that GPT-3
also draws on social connotations when generating
and processing names. We hope that future work
can further improve the respectful measurement of
gender in ﬁction.
All book excerpts and generated stories are more
likely to have masculine characters, and in ones
with feminine main characters in the prompt, there
is a slightly smaller gap between feminine and mas-
2The full of list of honoriﬁcs is in our Github repo.
Figure 2: Frequency of masculine (M), feminine (F),
and other (O) main prompt characters in our datasets.
Bars are colored by gendering method.
culine characters (Figure 3). This pattern persists
even when only looking at pronoun-gendered characters, who are referred to multiple times and are
likely to play larger roles. Our results echo previous
work that show that English literature pays more
attention to men in text .
Matched stories
Prompts containing main characters of different
genders may also contain different content, which
can introduce confounding factors when isolating
the effect of perceived gender on generated stories. We also run all our experiments on a subset
of 7334 paired GPT-3 stories. Every prompt does
not contain gendered pronouns and is used to generate multiple stories. GPT-3 may assign different
gender pronouns to the main character in the same
prompt across different stories (Table 1). We ﬁnd
cases where this occurs, randomly pairing stories
with the same prompt, where one has the main
character associated with feminine pronouns and
another has them associated with masculine pronouns. In this setup, we exclude stories where the
main character in the prompt is gendered by name.
Topic differences
Given this dataset of book excerpts and stories generated by GPT-3, we carry out several analyses
to understand the representation of gender within
them. We focus on overall content differences between stories containing prompt characters of different genders in this current section, and lexiconbased stereotypes in §5.
Topic modeling is a common unsupervised method
for uncovering coherent collections of words across
Figure 3: On average, there are more masculine characters in each GPT-3 story or book excerpt. Each column
is the gender of the prompt character, and the bars are
colored by gendering method. Error bars are 95% con-
ﬁdence intervals.
narratives . We train latent Dirichlet allocation (LDA) on unigrams and bigrams
from book excerpts and generated stories using
MALLET, with 50 topics and default parameters. We remove character names from the text
during training. For each topic t, we calculate
∆T(t) = P(t|F) −P(t|M), where P(t|M) is the
average probability of a topic occurring in stories
with masculine main characters, and P(t|F) is the
analogous value for feminine main characters.
Table 1 shows that generated stories place masculine and feminine characters in different topics, and
in the subset of matched GPT-3 stories, these differences still persist (Pearson r = 0.91, p < 0.001).
Feminine characters are more likely to be discussed
in topics related to family, emotions, and body
parts, while masculine ones are more aligned to
politics, war, sports, and crime. The differences in
generated stories follow those seen in books (Pearson r = 0.84, p < 0.001). Prompts with the same
content can still lead to different narratives that
are tied to character gender, suggesting that GPT-3
has internally linked stereotypical contexts to gender. In previous work, GPT-3’s predecessor GPT-2
also places women in caregiving roles , and character tropes for women emphasize
maternalism and appearance .
We also use our trained LDA model to infer topic
probabilities for each prompt, and examine prompts
high probability words
really, time, want, going, sure, lot,
feel, little, life, things
baby, little, sister, child, girl, want,
children, father, mom, mama
appearance
woman, girl, black, hair, white,
women, looked, look, face, eyes
people, country, government, president, war, american, world, chinese,
political, united states
men, war, soldiers, soldier, general,
enemy, camp, ﬁght, battle, ﬁghting
plane, time, air, ship, machine, pilot,
space, computer, screen, control
Table 1: Feminine and masculine main characters are
associated with different topics, even in the matched
prompt setup. These topics have the biggest ∆T in all
GPT-3 stories, and these differences are statistically signiﬁcant (t-test with Bonferroni correction, p < 0.05).
Figure 4: Prompt character gender is related the probability of a generated story continuing the family and
politics topics. Each dot is a GPT-3 story, and the larger
dots are means with 95% conﬁdence intervals.
with a high (> 0.15) probability of a topic with gender bias, such as politics or family. We chose this
threshold using manual inspection, and prompts
that meet this threshold tended to have at least one
topic-related word in them. When prompts contain the family topic, the resulting story tends to
continue or amplify that topic more so if the main
character is feminine (Figure 4). The reverse occurs
when prompts have a high probability of politics:
the resulting story is more likely to continue the
topic if the main character is masculine. So, even
if characters are in a prompt with anti-stereotypical
content, it is still challenging to generate stories
with topic probabilities at similar levels as a character with the stereotype-aligned gender.
Lexicon-based stereotypes
Now, we measure how much descriptions of characters correspond to a few established gender stereotypes. Men are often portrayed as strong, intelligent, and natural leaders .
Popular culture has increased its attention towards
women in science, politics, academia, and law
 .
Even so, depictions of women still foreground their
physical appearances , and portray them as weak and less powerful . Thus, our present study
measures three dimensions of character descriptions: appearance, intellect, and power.
Words linked to people via linguistic dependencies
can be used to analyze descriptions of people in
text .
These words can be aligned with lexicons curated
by human annotators, such as Fast et al. ’s
categories of adjectives and verbs, which were used
to measure gender stereotypes in online ﬁction.
We train 100-dimensional word2vec embeddings
 on lowercased, punctuationless generated stories and books, using default parameters in the gensim Python package. We extract adjectives and verbs using the dependency
relations nsubj and amod attached to main character names and their pronouns in non-prompt text.
For masculine and feminine characters, we only
use their gender-conforming pronouns.
To gather words describing appearance, we combine Fast et al. ’s lexicons for beautiful and
sexual (201 words). For words related to intellect,
we use Fast et al. ’s Empath categories containing the word intellectual (98 words). For measuring power, we take Fast et al. ’s lexicons
for strong and dominant (113 words), and contrast
them with a union of their lexicons for weak, dependent, submissive, and afraid (141 words).
Counting lexicon word frequency can overemphasize popular words (e.g. want) and exclude
related words.
Therefore, we calculate semantic similarity instead. For appearance and intellect, we compute the average cosine similarity of
a verb or adjective to every word in each lexicon.
For power, we take a different approach, because
antonyms tend be close in semantic space . Previous work has used differences
between antonyms to create semantic axes and compare words to these axes . Let a
Appearance, intellect, and power scores
across genders in books and GPT-3-generated stories.
Error bars are 95% conﬁdence intervals.
All differences between feminine and masculine characters are
signiﬁcant (Welch’s t-test, p < 0.001), except for intellect in matched GPT-3 stories.
be a word in the lexicon related to strength and b
be a word embedding from the lexicon related to
weakness. We use An et al. ’s SEMAXIS to
calculate word x’s score:
S(x) = cos
where a positive value means x is stronger, and a
negative value means x is weaker. We z-score all
three of our metrics, and average the scores for all
words associated with characters of each gender.
Book characters have higher power and intellect
than generated characters, but relative gender differences are similar between the two datasets (Figure 5). As hypothesized, feminine characters are
most likely to be described by their appearance,
and masculine characters are most powerful. The
gender differences between masculine and feminine characters for appearance and power persist
in matched GPT-3 stories, suggesting that GPT-3
has internally linked gender to these attributes. The
patterns for intellect show that feminine characters
are usually highest, though the insigniﬁcant difference in matched GPT-3 stories (p > 0.05) suggests
that this attribute may be more affected by other
content than gender.
We also test the ability of prompts to steer GPT-3
towards stronger and more intellectual characters.
We examine character descriptions in stories gener-
Figure 6: A comparison of stories generated by all
prompts with stories generated by prompts where characters are linked to cognitive or high power verbs. Error
bars are 95% conﬁdence intervals.
ated by prompts in which characters are the subject
of high power verbs from Sap et al. ’s connotation frame lexicon, which was created for the
study of characters in ﬁlm. We also examine GPT-3
stories with prompts where characters use cognitive verbs from Bloom’s Taxonomy, which is used
to measure student learning, such as summarize,
interpret, or critique . We
match verbs based on their lemmatized forms.
We ﬁnd that prompts containing cognitive verbs
result in descriptions with higher intellect scores
(Figure 6). Prompts containing high power verbs,
however, do not lead to similar change, and nonmasculine characters with high power verbs still
have lower power on average than all masculine
characters. Traditional power differentials in gender may be challenging to override and require
more targeted prompts.
Conclusion
The use of GPT-3 for storytelling requires a balance between creativity and controllability to avoid
unintended generations. We show that multiple
gender stereotypes occur in generated narratives,
and can emerge even when prompts do not contain
explicit gender cues or stereotype-related content.
Our study uses prompt design as a possible mechanism for mitigating bias, but we do not intend to
shift the responsibility of preventing social harm
from the creators of these systems to their users.
Future studies can use causal inference and more
carefully designed prompts to untangle the factors
that inﬂuence GPT-3 and other text generation models’ narrative outputs.
Acknowledgments
We thank Nicholas Tomlin, Julia Mendelsohn, and
Emma Lurie for their helpful feedback on earlier
versions of this paper. This work was supported
by funding from the National Science Foundation
(Graduate Research Fellowship DGE-1752814 and
grant IIS-1942591).