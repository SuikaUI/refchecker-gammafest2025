Training of photonic neural networks through in situ backpropagation
Tyler W. Hughes, Momchil Minkov, Yu Shi, Shanhui Fan
Ginzton Laboratory, Stanford University, Stanford, CA, 94305.
 
Recently, integrated optics has gained interest as a hardware platform for implementing machine
learning algorithms. Of particular interest are artiﬁcial neural networks, since matrix-vector multiplications, which are used heavily in artiﬁcial neural networks, can be done eﬃciently in photonic
circuits. The training of an artiﬁcial neural network is a crucial step in its application. However,
currently on the integrated photonics platform there is no eﬃcient protocol for the training of these
networks. In this work, we introduce a method that enables highly eﬃcient, in situ training of a
photonic neural network. We use adjoint variable methods to derive the photonic analogue of the
backpropagation algorithm, which is the standard method for computing gradients of conventional
neural networks.
We further show how these gradients may be obtained exactly by performing
intensity measurements within the device.
As an application, we demonstrate the training of a
numerically simulated photonic artiﬁcial neural network. Beyond the training of photonic machine
learning implementations, our method may also be of broad interest to experimental sensitivity
analysis of photonic systems and the optimization of reconﬁgurable optics platforms.
INTRODUCTION
Artiﬁcial neural networks (ANNs), and machine learning in general, are becoming ubiquitous for an impressively large number of applications . This has brought
ANNs into the focus of research in not only computer
science, but also electrical engineering, with hardware
speciﬁcally suited to perform neural network operations
actively being developed. There are signiﬁcant eﬀorts in
constructing artiﬁcial neural network architectures using
various electronic solid-state platforms , but ever
since the conception of ANNs, a hardware implementation using optical signals has also been considered .
In this domain, some of the recent work has been devoted
to photonic spike processing and photonic reservoir
computing , as well as to devising universal, chipintegrated photonic platforms that can implement any
arbitrary ANN . Photonic implementations beneﬁt from the fact that, due to the non-interacting nature
of photons, linear operations – like the repeated matrix
multiplications found in every neural network algorithm
– can be performed in parallel, and at a lower energy
cost, when using light as opposed to electrons.
A key requirement for the utility of any ANN platform
is the ability to train the network using algorithms such
as error backpropagation . Such training typically demands signiﬁcant computational time and resources and
it is generally desirable for error backpropagation to implemented on the same platform. This is indeed possible for the technologies of Refs. and has also
been demonstrated e.g. in memristive devices . In
optics, as early as three decades ago, an adaptive platform that could approximately implement the backpropagation algorithm experimentally was proposed .
However, this algorithm requires a number of complex
optical operations that are diﬃcult to implement, particularly in integrated optics platforms. Thus, the current
implementation of a photonic neural network using integrated optics has been trained using a model of the
system simulated on a regular computer .
ineﬃcient for two reasons. First, this strategy depends
entirely on the accuracy of the model representation of
the physical system. Second, unless one is interested in
deploying a large number of identical, ﬁxed copies of the
ANN, any advantage in speed or energy associated with
using the photonic circuit is lost if the training must be
done on a regular computer. Alternatively, training using
a brute force, in situ computation of the gradient of the
objective function has been proposed . However, this
strategy involves sequentially perturbing each individual
parameter of the circuit, which is highly ineﬃcient for
large systems.
In this work, we propose a procedure, which we label
the time-reversal interference method (TRIM), to compute the gradient of the cost function of a photonic ANN
by use of only in situ intensity measurements. Our procedure works by physically implementing the adjoint variable method (AVM), a technique that has typically been
implemented computationally in the optimization and
inverse design of photonic structures . Furthermore, the method scales in constant time with respect
to the number of parameters, which allows for backpropagation to be eﬃciently implemented in a hybrid optoelectronic network.
Although we focus our discussion
on a particular hardware implementation of a photonic
ANN, our conclusions are derived starting from Maxwells
equations, and may therefore be extended to other photonic platforms.
The paper is organized as follows: In Section II, we introduce the working principles of a photonic ANN based
on the hardware platform introduced in Ref. . We
also derive the mathematics of the forward and backward
propagation steps and show that the gradient computation needed for training can be expressed as a modal
overlap. Then, in Section III we discuss how the adjoint
method may be used to describe the gradient of the ANN
cost function in terms of physical parameters. In Section
IV, we describe our procedure for determining this graarXiv:1805.09943v1 [physics.optics] 25 May 2018
dient information experimentally using in situ intensity
measurements. We give a numerical validation of these
ﬁndings in Section V and demonstrate our method by
training a model of a photonic ANN in Section VI. We
provide ﬁnal comments and conclude in Section VII.
THE PHOTONIC NEURAL NETWORK
In this Section, we introduce the operation and gradient computation of a feed-forward photonic ANN. In its
most general case, a feed-forward ANN maps an input
vector to an output vector via an alternating sequence of
linear operations and element-wise nonlinear functions
of the vectors, also called ‘activations’. A cost function,
L, is deﬁned over the outputs of the ANN and the matrix elements involved in the linear operations are tuned
to minimize L over a number of training examples via
gradient-based optimization. The ‘backpropagation algorithm’ is typically used to compute these gradients analytically by sequentially utilizing the chain rule from the
output layer backwards to the input layer.
Here, we will outline these steps mathematically for a
single training example, with the procedure diagrammed
We focus our discussion on the photonic
hardware platform presented in , which performs the
linear operations using optical interference units (OIUs).
The OIU is a mesh of controllable Mach-Zehnder interferometers (MZIs) integrated in a silicon photonic circuit.
By tuning the phase shifters integrated in the MZIs, any
unitary N × N operation on the input can be implemented , which ﬁnds applications both in classical
and quantum photonics . In the photonic ANN
implementation from Ref. , an OIU is used for each
linear matrix-vector multiplication, whereas the nonlinear activations are performed using an electronic circuit,
which involves measuring the optical state before activation, performing the nonlinear activation function on an
electronic circuit such as a digital computer, and preparing the resulting optical state to be injected to the next
stage of the ANN.
We ﬁrst introduce the notation used to describe the
OIU, which consists of a number, N, of single-mode
waveguide input ports coupled to the same number of
single-mode output ports through a linear and lossless
In principle, the device may also be extended
to operate on a diﬀerent number of inputs and outputs.
We further assume directional propagation such that all
power ﬂows exclusively from the input ports to the output ports, which is a typical assumption for the devices
of Refs. . In its most general form, the device
implements the linear operation
ˆWXin = Zout,
where Xin and Zout are the modal amplitudes at the
input and output ports, respectively, and ˆW, which we
will refer to as the transfer matrix, is the oﬀ-diagonal
block of the system’s full scattering matrix,
Here, the diagonal blocks are zero because we assume
forward-only propagation, while the oﬀ-diagonal blocks
are the transpose of each other because we assume a reciprocal system. Zin and Xout correspond to the input
and output modal amplitudes, respectively, if we were to
run this device in reverse, i.e. sending a signal in from
the output ports.
Now we may use this notation to describe the forward
and backward propagation steps in a photonic ANN. In
the forward propagation step, we start with an initial
input to the system, X0, and perform a linear operation
on this input using an OIU represented by the matrix
This is followed by the application of a elementwise nonlinear activation, f1(·), on the outputs, giving
the input to the next layer. This process repeats for the
each layer l until the output layer, L. Written compactly,
for l = 1 ... L
Xl = fl( ˆWlXl−1) ≡fl(Zl).
Finally, our cost function L is an explicit function of the
outputs from the last layer, L = L(XL). This process is
shown in Fig. 1(a).
To train the network, we must minimize this cost function with respect to the linear operators, ˆWl, which may
be adjusted by tuning the integrated phase shifters within
the OIUs. While a number of recent papers have clari-
ﬁed how an individual OIU can be tuned by sequential,
in situ methods to perform an arbitrary, pre-deﬁned operation , these strategies do not straightforwardly
apply to the training of ANNs, where nonlinearities and
several layers of computation are present. In particular,
the training of ANN requires gradient information which
is not provided directly in the methods of Ref. .
 , the training of the ANN was done ex
situ on a computer model of the system, which was used
to ﬁnd the optimal weight matrices ˆWl for a given cost
function. Then, the ﬁnal weights were recreated in the
physical device, using an idealized model that relates the
matrix elements to the phase shifters.
discusses a possible in situ method for computing the
gradient of the ANN cost function through a serial perturbation of every individual phase shifter (‘brute force’
gradient computation). However, this gradient computation has an unavoidable linear scaling with the number
of parameters of the system. The training method that
we propose here operates without resorting to an external
model of the system, while allowing for the tuning of each
parameter to be done in parallel, therefore scaling significantly better with respect to the number of parameters
when compared to the brute force gradient computation.
To introduce our training method we ﬁrst use the backpropagation algorithm to derive an expression for the gradient of the cost function with respect to the permittivities of the phase shifters in the OIUs. In the following,
(a) A schematic of the ANN architecture demonstrated in Ref . The boxed regions correspond to OIUs that
perform a linear operation represented by the matrix ˆ
Wl. Integrated phase shifters (blue) are used to control the OIU and train
the network. The red regions correspond to nonlinear activations fl(·). (b) Illustration of operation and gradient computation
in an ANN. The top and bottom rows correspond to the forward and backward propagation steps, respectively. Propagation
through a square cell corresponds to matrix multiplication. Propagation through a rounded region corresponds to activation.
⊙is element-wise vector multiplication.
we denote ϵl as the permittivity of a single, arbitrarily
chosen phase shifter in layer l, as the same derivation
holds for each of the phase shifters present in that layer.
Note that ˆWl has an explicit dependence on ϵl, but all
ﬁeld components in the subsequent layers also depend
implicitly on ϵl.
As a demonstration, we take a mean squared cost function
where T is a complex-valued target vector corresponding
to the desired output of our system given input X0.
Starting from the last layer in the circuit, the derivative
of the cost function with respect to the permittivity ϵL
of one of the phase shifters in the last layer is given by
where ⊙is element-wise vector multiplication, deﬁned
such that, for vectors a and b, the i-th element of the
vector a ⊙b is given by aibi. R{·} gives the real part,
′(·) is the derivative of the lth layer activation function
with respect to its (complex) argument. We deﬁne the
vector δL ≡ΓL ⊙fL
′ in terms of the error vector ΓL ≡
For any layer l < L, we may use the chain rule to
perform a recursive calculation of the gradients
δl = Γl ⊙fl
Figure 1(b) diagrams this process, which computes the
δl vectors sequentially from the output layer to the input
A treatment for non-holomorphic activations is
derived Appendix A.
We note that the computation of δl requires performing the operation Γl =
l+1δl+1, which corresponds
physically to sending δl+1 into the output end of the
OIU in layer l + 1.
In this way, our procedure ‘backpropagates’ the vectors δl and Γl physically through the
entire circuit.
GRADIENT COMPUTATION USING THE
ADJOINT VARIABLE METHOD
In the previous Section, we showed that the crucial
step in training the ANN is computing gradient terms
of the form R
, which contain derivatives
with respect to the permittivity of the phase shifters in
the OIUs. In this Section, we show how this gradient
may be expressed as the solution to an electromagnetic
adjoint problem.
The OIU used to implement the matrix ˆWl, relating
the complex mode amplitudes of input and output ports,
can be described using ﬁrst-principles electrodynamics.
This will allow us to compute its gradient with respect to
each ϵl, as these are the physically adjustable parameters
in the system.
Assuming a source at frequency ω, at
steady state Maxwell’s equations take the form
ˆ∇× ˆ∇× −k2
e = −iωµ0j,
which can be written more succinctly as
ˆA(ϵr)e = b.
Here, ˆϵr describes the spatial distribution of the relative
permittivity (ϵr), k0 = ω2/c2 is the free-space wavenumber, e is the electric ﬁeld distribution, j is the electric current density, and ˆA = ˆAT due to Lorentz reciprocity.
(12) is the starting point of the ﬁnitediﬀerence frequency-domain (FDFD) simulation technique , where it is discretized on a spatial grid, and
the electric ﬁeld e is solved given a particular permittivity distribution, ϵr, and source, b.
To relate this formulation to the transfer matrix ˆW,
we now deﬁne source terms bi, i ∈1 . . . 2N, that correspond to a source placed in one of the input or output
ports. Here we assume a total of N input and N output
waveguides. The spatial distribution of the source term,
bi, matches the mode of the i-th single-mode waveguide.
Thus, the electric ﬁeld amplitude in port i is given by
i e, and we may establish a relationship between e and
Xin,i = b T
for i = 1 ... N over the input port indices, where Xin,i is
the i-th component of Xin. Or more compactly,
Xin ≡ˆPine,
Similarly, we can deﬁne
Zout,i = b T
for i + N = (N + 1) ... 2N over the output port indices,
Zout ≡ˆPoute,
and, with this notation, Eq. (1) becomes
ˆW ˆPine = ˆPoute
We now use the above deﬁnitions to evaluate the cost
function gradient in Eq. (10). In particular, with Eqs.
(10) and (17), we arrive at
l ˆPout ˆA−1 d ˆA
ˆA−1bx,l−1
Here bx,l−1 is the modal source proﬁle that creates the
input ﬁeld amplitudes Xl−1 at the input ports.
The key insight of the adjoint variable method is that
we may interpret this expression as an operation involving the ﬁeld solutions of two electromagnetic simulations,
which we refer to as the ‘original’ (og) and the ‘adjoint’
ˆAeog = bx,l−1
ˆAeaj = ˆP T
where we have made use of the symmetric property of ˆA.
Eq. (18) can now be expressed in a compact form as
If we assume that this phase shifter spans a set of
points, rφ in our system, then, from Eq. (11), we obtain
where ˆδr,r ′ is the Kronecker delta.
Inserting this into Eq.
(21), we thus ﬁnd that the
gradient is given by the overlap of the two ﬁelds over the
phase-shifter positions
eaj(r)eog(r)
This result now allows for the computation in parallel of
the gradient of the loss function with respect to all phase
shifters in the system, given knowledge of the original and
adjoint ﬁelds.
EXPERIMENTAL MEASUREMENT OF
time-reversal
interference
method (TRIM) for computing the gradient from the
previous section through in situ intensity measurements.
This represents the most signiﬁcant result of this paper.
Speciﬁcally, we wish to generate an intensity pattern with
the form R
, matching that of Eq. (23). We note
that interfering eog and e ∗
aj directly in the system results
in the intensity pattern:
I = |eog|2 + |eaj|2 + 2R
the last term of which matches Eq.
gradient can be computed purely through intensity measurements if the ﬁeld e ∗
aj can be generated in the OIU.
The adjoint ﬁeld for our problem, eaj, as deﬁned in
Eq. (20), is sourced by ˆP T
outδl, meaning that it physically corresponds to a mode sent into the system from the
output ports. As complex conjugation in the frequency
steps 4 & 5
steps 2 & 3
Schematic illustration of our proposed method for
experimental measurement of gradient information. The box
region represents the OIU. The colored ovals represent tunable phase shifters, and we illustrate computing the gradient
with respect to the red and the yellow phase shifters, labeled
1 and 2, respectively. (a): We send the original set of amplitudes Xl−1 and measure the constant intensity terms at each
phase shifter.
(b): We send the adjoint mode amplitudes,
given by δl, through the output side of our device, recording
T R from the opposite side, as well as |eaj|2 in each phaseshifter. (c): We send in Xl−1 + XT R, interfering eog and e∗
inside the device and recovering the gradient information for
all phase shifters simultaneously.
domain corresponds to time-reversal of the ﬁelds, we expect e ∗
aj to be sent in from the input ports. Formally, to
generate e ∗
aj, we wish to ﬁnd a set of input source amplitudes, XT R, such that the output port source amplitudes,
ZT R = ˆWXT R, are equal to the complex conjugate of the
adjoint amplitudes, or δ∗
l . Using the unitarity property
of transfer matrix ˆWl for a lossless system, along with
the fact that ˆPout ˆP T
out = ˆI for output modes, the input
mode amplitudes for the time-reversed adjoint can be
computed as
T R = ˆW T
As discussed earlier,
is the transfer matrix from
output ports to input ports. Thus, we can experimentally determine XT R by sending δl into the device output ports, measuring the output at the input ports, and
taking the complex conjugate of the result.
We now summarize the procedure for experimentally
measuring the gradient of an OIU layer in the ANN with
respect to the permittivities of this layer’s integrated
phase shifters:
1. Send in the original ﬁeld amplitudes Xl−1 and measure and store the intensities at each phase shifter.
2. Send δl into the output ports and measure and
store the intensities at each phase shifter.
3. Compute the time-reversed adjoint input ﬁeld amplitudes as in Eq. (25).
4. Interfere the original and the time-reversed adjoint
ﬁelds in the device, measuring again the resulting
intensities at each phase shifter.
5. Subtract the constant intensity terms from steps 1
and 2 and multiply by k2
0 to recover the gradient as
in Eq. (23).
This procedure is also illustrated in Fig. 2.
NUMERICAL GRADIENT
DEMONSTRATION
We numerically demonstrate this procedure in Fig. 3
with a series of FDFD simulations of an OIU implementing a 3 × 3 unitary matrix . These simulations are
intended to represent the gradient computation corresponding to one OIU in a single layer, l, of a neural network with input Xl−1 and delta vector δl. In these simulations, we use absorbing boundary conditions on the
outer edges of the system to eliminate back-reﬂections.
The relative permittivity distribution is shown in Fig.
3(a) with the positions of the variable phase shifters in
For demonstration, we simulate a speciﬁc case
where Xl−1 = [0 0 1]T , with unit amplitude in the bottom port and we choose δl = [0 1 0]T . In Fig. 3(b), we
display the real part of eog, corresponding to the original,
forward ﬁeld.
The real part of the adjoint ﬁeld, eaj, corresponding
to the cost function L = R
is shown in
3(d) we show the real part of the
time-reversed copy of eaj as computed by the method
described in the previous section, in which X∗
T R is sent in
through the input ports. There is excellent agreement, up
to a constant, between the complex conjugate of the ﬁeld
pattern of (c) and the ﬁeld pattern of (d), as expected.
In Fig. 3(e), we display the gradient of the objective
function with respect to the permittivity of each point
of space in the system, as computed with the adjoint
method, described in Eq. (23). In Fig. 3(f), we show the
same gradient information, but instead computed with
the method described in the previous section. Namely,
we interfere the ﬁeld pattern from panel (b) with the
ﬁeld pattern from panel (d), subtract constant intensity
terms, and multiply by the appropriate constants. Again,
(b) and (d) agree with good precision.
We note that in a realistic system, the gradient must be
constant for any stretch of waveguide between waveguide
Numerical demonstration of the TRIM method of Section IV. (a): Relative permittivity distribution for three MZIs
arranged to perform a 3x3 linear operation. Blue boxes represent where phase shifters would be placed in this system. As an
example, we compute the gradient information for a layer with Xl−1 = [0 0 1]T and δl = [0 1 0]T , corresponding to the bottom
left and middle right port, respectively. (b): Real part of the simulated electric ﬁeld Ez corresponding to injection from the
bottom left port. (c): Real part of the adjoint Ez, corresponding to injection from the middle right port. (d): Time-reversed
adjoint ﬁeld as constructed by our method, fed in through all three ports on the left. (e): The gradient information
dϵl (x, y)
as obtained directly by the adjoint method, normalized by its maximum absolute value. (f): The gradient information as
obtained by the method introduced in this work, normalized by its maximum absolute value. Namely, the ﬁeld pattern from
(b) is interfered with the time-reversed adjoint ﬁeld of (d) and the constant intensity terms are subtracted from the resulting
intensity pattern. Panels (e) and (f) match with high precision.
couplers because the interfering ﬁelds are at the same
frequency and are traveling in the same direction. Thus,
there should be no distance dependence in the corresponding intensity distribution. This is largely observed
in our simulation, although small ﬂuctuations are visible because of the proximity of the waveguides and the
sharp bends, which were needed to make the structure
compact enough for simulation within a reasonable time.
In practice, the importance of this constant intensity is
that it can be detected after each phase shifter, instead
of inside of it.
Finally, we note that this numerically generated system
experiences a total power loss of 41% due to scattering
caused by very sharp bends and stair-casing of the structure in the simulation. We also observe approximately
5-10% mode-dependent loss, as determined by measuring
the diﬀerence in total transmitted power corresponding
to injection at diﬀerent input ports. Minimal amounts of
reﬂection are also visible in the ﬁeld plots. Nevertheless,
TRIM still reconstructs the adjoint sensitivity with very
good ﬁdelity.
EXAMPLE OF ANN TRAINING
Finally, we use the techniques from the previous Sections to numerically demonstrate the training of a photonic ANN to implement a logical XOR gate, deﬁned by
the following input to target (X0 →T) pairs
[0 0]T →0,
[0 1]T →1,
[1 0]T →1,
[1 1]T →0. (26)
This problem was chosen as demonstration of learning
a nonlinear mapping from input to output and is
simple enough to be solved with a small network with
only four training examples.
iteration number
 
output (| |)
 
FIG. 4. Numerical demonstration of a photonic ANN implementing an XOR gate using the backpropagation algorithm
and adjoint method described in this work. (a) The architecture of the ANN. Two layers of 3×3 OIUs with z2 activations.
(b) The mean-squared error (MSE) between the predictions
and targets as a function of training iterations. (c) The absolute value of the network predictions (blue circles) and targets
(black crosses) before training. (d) The absolute value of the
network predictions after training, showing that the network
has successfully learned the XOR function.
As diagrammed in Fig. 6a, we choose a network architecture consisting of two 3×3 unitary OIUs. On the forward propagation step, the binary representation of the
inputs, X0, is sent into the ﬁrst two input elements of
the ANN and a constant value of 1 is sent into the third
input element, which serves to introduce artiﬁcial bias
terms into the network. These inputs are sent through
a 3 × 3 unitary OIU and then the element-wise activation f(z) = z2 is applied. The output of this step is sent
to another 3 × 3 OIU and sent through another activation of the same form. Finally, the ﬁrst output element
is taken to be our prediction, XL, ignoring the last two
output elements. Our network is repeatedly trained on
the four training examples deﬁned in Eq. (26) and using
the mean-squared cost function presented in Eq. (4).
For this demonstration, we utilized a matrix model of
the system, as described in , with mathematical
details described in Appendix B. This model allows us to
compute an output of the system given an input mode
and the settings of each phase shifter.
Although this
is not a ﬁrst-principle electromagnetic simulation of the
system, it provides information about the complex ﬁelds
at speciﬁc reference points within the circuit, which enables us to implement training using the backpropagation
method as described in Section II, combined with the adjoint gradient calculation from Section III. Using these
methods, at each iteration of training we compute the
gradient of our cost function with respect to the phases
of each of the integrated phase shifters, and sum them
over the four training examples.
Then, we perform a
simple steepest-descent update to the phase shifters, in
accordance with the gradient information. This is consistent with the standard training protocol for an ANN
implemented on a conventional computer. Our network
successfully learned the XOR gate in around 400 iterations. The results of the training are shown in Fig. 6b-d.
We note that this is meant to serve as a simple demonstration of using the in-situ backpropagation technique
for computing the gradients needed to train photonic
ANNs. However, our method may equally be performed
on more complicated tasks, which we show in the Appendix C.
DISCUSSION AND CONCLUSION
Here, we justify some of the assumptions made in this
work. Our strategy for training a photonic ANN relies on
the ability to create arbitrary complex inputs. We note
that a device for accomplishing this has been proposed
and discussed in .
Our recovery technique further
requires an integrated intensity detection scheme to occur in parallel and with virtually no loss. This may be
implemented by integrated, transparent photo-detectors,
which have already been demonstrated in similar systems
 . Furthermore, as discussed, this measurement may
occur in the waveguide regions directly after the phase
shifters, which eliminates the need for phase shifter and
photodetector components at the same location. Finally,
in our procedure for experimentally measuring the gradient information, we suggested running isolated forward
and adjoint steps, storing the intensities at each phase
shifter for each step, and then subtracting this information from the ﬁnal interference intensity. Alternatively,
one may bypass the need to store these constant intensities by introducing a low-frequency modulation on top
of one of the two interfering ﬁelds in Fig. 2(c), such that
the product term of Eq. (24) can be directly measured
from the low-frequency signal. A similar technique was
used in .
We now discuss some of the limitations of our method.
In the derivation, we had assumed the ˆW operator to be
unitary, which corresponds to a lossless OIU. In fact, we
note that our procedure is exact in the limit of a lossless,
feed-forward, and reciprocal system. However, with the
addition of any amount of uniform loss, ˆW is still unitary up to a constant, and our procedure may still be
performed with the added step of scaling the measured
gradients depending on this loss (see a related discussion in Ref.
Uniform loss conditions are satis-
ﬁed in the OIUs experimentally demonstrated in Refs.
Mode-dependent loss, such as asymmetry in
the MZI mesh layout or fabrication errors, should be
avoided as its presence limits the ability to accurately
reconstruct the time-reversed adjoint ﬁeld. Nevertheless,
our simulation in Fig. 3 indicates that an accurate gradient can be obtained even in the presence of signiﬁcant
mode-dependent loss. In the experimental structures of
Refs. , the mode-dependent loss is made much
lower due to the choice of the MZI mesh. Thus we expect
our protocol to work in practical systems. Our method,
in principle, computes gradients in parallel and scales in
constant time. In practice, to get this scaling would require careful design of the circuits controlling the OIUs.
Conveniently, since our method does not directly assume any speciﬁc model for the linear operations, it may
gracefully handle imperfections in the OIUs, such as deviations from perfect 50-50 splits in the MZIs. Lastly,
while we chose to make an explicit distinction between
the input ports and the output ports, i.e.
no backscattering in the system, this requirement is not
strictly necessary. Our formalism can be extended to the
full scattering matrix. However, this would require special treatment for subtracting the backscattering.
The problem of overﬁtting is one that must be addressed by ‘regularization’ in any practical realization of
a neural network. Photonic ANNs of this class provide a
convenient approach to regularization based on ‘dropout’
 . In the dropout procedure, certain nodes are probabilistically and temporarily ‘deleted from the network
during train time, which has the eﬀect of forcing the network to ﬁnd alternative paths to solve the problem at
hand. This has a strong regularization eﬀect and has become popular in conventional ANNs. Dropout may be
implemented simply in the photonic ANN by ‘shutting
oﬀchannels in the activation functions during training.
Speciﬁcally, at each time step and for each layer l and
element i, one may set fl(Zi) = 0 with some ﬁxed probability.
In conclusion, we have demonstrated a method for performing backpropagation in an ANN based on a photonic
circuit. This method works by physically propagating the
adjoint ﬁeld and interfering its time-reversed copy with
the original ﬁeld. The gradient information can then be
directly measured out as an in-situ intensity measurement.
While we chose to demonstrate this procedure
in the context of ANNs, it is broadly applicable to any
reconﬁgurable photonic system. One could imagine this
setup being used to tune phased arrays , optical delivery systems for dielectric laser accelerators , or other
systems that rely on large meshes of integrated optical
phase shifters. Furthermore, it may be applied to sensitivity analysis of photonic devices, enabling spatial sensitivity information to be measured as an intensity in the
Our work should enhance the appeal of photonic circuits in deep learning applications, allowing for training
to happen directly inside the device in an eﬃcient and
scalable manner. Furthermore, this method is broadly
applicable to integrated and adaptive optical systems,
enabling the possibility for automatic self-conﬁguration
and optimization without resorting to brute force gradient computation or model-based methods, which often
do not perfectly represent the physical system.
FUNDING INFORMATION
Gordon and Betty Moore Foundation (GBMF4744);
Schweizerischer Nationalfonds zur Frderung der Wissenschaftlichen Forschung (P300P2 177721); Air Force
Oﬃce of Scientiﬁc Research (FA9550-17-1-0002).
 Y. LeCun, Y. Bengio, and G. Hinton, Nature 521, 436
Alvarez-Icaza,
A. S. Cassidy, J. Sawada, F. Akopyan, B. L. Jackson,
Y. Nakamura,
B. Brezzo,
Appuswamy,
A. Amir, M. D. Flickner, W. P. Risk, R. Manohar,
 
 M. Prezioso, F. Merrikh-Bayat, B. D. Hoskins, G. C.
Adam, K. K. Likharev, and D. B. Strukov, Nature 521,
61 , arXiv:1412.0611.
 Y. S. Abu-Mostafa and D. Pslatis, Scientiﬁc American
256, 88 .
 S. Jutamulia, Science 28 .
 D. Rosenbluth, K. Kravtsov, M. P. Fok, and P. R. Prucnal, Optics Express 17, 22767 .
 A. N. Tait, M. A. Nahmias, B. J. Shastri, and P. R. Prucnal, Journal of Lightwave Technology 32, 3427 .
 D. Brunner, M. C. Soriano, C. R. Mirasso, and I. Fischer,
Nature Communications 4, 1364 .
 K. Vandoorne, P. Mechet, T. Van Vaerenbergh, M. Fiers,
G. Morthier, D. Verstraeten, B. Schrauwen, J. Dambre,
and P. Bienstman, Nature Communications 5, 1 .
 J. M. Shainline, S. M. Buckley, R. P. Mirin,
S. W. Nam, Physical Review Applied 7, 1 ,
 
 Y. Shen, N. C. Harris, S. Skirlo, M. Prabhu, T. Baehr-
Jones, M. Hochberg, X. Sun, S. Zhao, H. Larochelle,
D. Englund, and M. Soljacic, Nature Photonics 11, 441
 , arXiv:1610.02365.
 D. E. Rumelhart, G. E. Hinton, and R. J. Williams, Parallel Distributed Processing, edited by D. E. Rumelhart
and R. J. McClelland, Vol. 1 Chap. 8.
I. Danihelka, A. Grabska-Barwi´nska, S. G. Colmenarejo,
Grefenstette,
Ostrovski,
Summerﬁeld,
K. Kavukcuoglu,
and D. Hassabis, Nature 538, 471
 , arXiv:arXiv:1410.5401v2.
 M. Hermans, M. Burm, T. Van Vaerenbergh, J. Dambre,
and P. Bienstman, Nature Communications 6, 6729
 F. Alibart, E. Zamanidoost, and D. B. Strukov, Nature
Communications 4, 2072 .
 K. Wagner and D. Psaltis, Applied Optics 26, 5061
 D. Psaltis, D. Brady,
and K. Wagner, Applied Optics
27, 1752 .
 N. Georgieva, S. Glavic, M. Bakr, and J. Bandler, IEEE
Transactions on Microwave Theory and Techniques 50,
2751 .
 G. Veronis, R. W. Dutton,
and S. Fan, Optics Letters
29, 2288 .
 T. Hughes, G. Veronis, K. P. Wootton, R. J. England,
and S. Fan, Optics Express 25, 15414 .
 M. Reck, A. Zeilinger, H. J. Bernstein, and P. Bertani,
Physical Review Letters 73, 58 , arXiv:9612010
[quant-ph].
 W. R. Clements, P. C. Humphreys, B. J. Metcalf, W. S.
Kolthammer, and I. A. Walsmley, Optica 3, 1460 .
 J. Carolan, C. Harrold, C. Sparrow, E. Mart´ın-L´opez,
N. J. Russell, J. W. Silverstone, P. J. Shadbolt, N. Matsuda, M. Oguma, M. Itoh, G. D. Marshall, M. G. Thompson, J. C. Matthews, T. Hashimoto, J. L. O’Brien, and
A. Laing, Science 349, 711 , arXiv:1505.01182.
 N. C. Harris, G. R. Steinbrecher, M. Prabhu, Y. Lahini,
J. Mower, D. Bunandar, C. Chen, F. N. C. Wong,
T. Baehr-Jones, M. Hochberg, S. Lloyd, and D. Englund,
Nature Photonics 11, 447 .
 D. A. B. Miller, Optics Express 21, 6360 ,
 
 D. A. B. Miller, Photonics Research 1, 1 ,
 
 D. A. B. Miller, Optica 2, 747 .
Guglielmi,
Carminati,
Ferrari, M. Sampietro, D. A. Miller, A. Melloni,
F. Morichetti, Light: Science & Applications 6, e17110
 W. Shin and S. Fan, Journal of Computational Physics
231, 3406 .
 K. Vandoorne, P. Mechet, T. Van Vaerenbergh, M. Fiers,
G. Morthier, D. Verstraeten, B. Schrauwen, J. Dambre,
and P. Bienstman, Nature Communications 5, 3541
 D. A. B. Miller, Opt. Express 25, 29233 .
 N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever,
and R. Salakhutdinov, Journal of Machine Learning Research 15, 1929 .
 J. Sun, E. Timurdogan, A. Yaacobi, E. S. Hosseini, and
M. R. Watts, Nature 493, 195 .
 T. W. Hughes, S. Tan, Z. Zhao, N. V. Sapra, Y. J. Lee,
K. J. Leedle, H. Deng, Y. Miao, D. S. Black, M. Qi,
O. Solgaard, J. S. Harris, J. Vuckovic, R. L. Byer, and
S. Fan, Physical Review Applied .
Appendix A: Non-holomorphic Backpropagation
In the previous derivation, we have assumed that the
functions fl(·) are holomorphic. For each element of input
Zl, labeled z, this means that the derivative of fl(z) with
respect to its complex argument is well deﬁned, or the
derivative
fl(z + ∆z) −fl(z −∆z)
does not depend on the direction that ∆z approaches 0
in the complex plane.
Here we show how to extend the backpropagation
derivation to non-holomorphic activation functions. We
ﬁrst examine the starting point of the backpropagation
algorithm, considering the change in the mean-squared
loss function with respect to the permittivity of a phase
shifter in the last layer OIU, as written in Eq. (7) of the
main text as
Where we had deﬁned the error vector ΓL ≡
for simplicity and XL = fL( ˆWLXL−1) is the output of
the ﬁnal layer.
To evaluate this expression for non-holomorphic activation functions, we split fL(Z) and its argument into
their real and imaginary parts
fL(Z) = u(α, β) + iv(α, β),
where i is the imaginary unit and α and β are the real
and imaginary parts of ZL, respectively.
We now wish to evaluate dXL
dϵL = dfL(Z)
dϵL , which gives
the following via the chain rule
where we have dropped the layer index for simplicity.
Here, terms of the form dx
dy correspond to element-wise
diﬀerentiation of the vector x with respect to the vector
y. For example, the i-th element of the vector dx
dy is given
Now, inserting into Eq. (A2), we have
We now deﬁne real and imaginary parts of ΓL as ΓR
and ΓI, respectively. Inserting the deﬁnitions of α and
β in terms of ˆWL and XL−1 and doing some algebra, we
Finally, the expression simpliﬁes to
As a check, if we insert the conditions for fL(Z) to be
holomorphic, namely
Eq. (A10) simpliﬁes to
as before.
This derivation may be similarly extended to any layer
l in the network. For holomorphic activation functions,
whereas we originally deﬁned the δ vectors as
δl = Γl ⊙fl
for non-holomorphic activation functions, the respective
deﬁnition is
FIG. 5. (a): An OIU implementing a universal 3 × 3 unitary
operation, parameterized as in Ref. . (b): Illustration of
Eq. (B4) for the computation of the gradient with respect to
a given phase shifter.
where ΓR and ΓI are the respective real and imaginary
parts of Γl, u and v are the real and imaginary parts of
fl(·), and α and β are the real and imaginary parts of
Zl, respectively.
We can write this more simply as
In polar coordinates where Z = r exp (iφ) and f =
f(r, φ), this equation becomes
δl = exp (−iφ)
where all operations are element-wise.
Appendix B: Photonic neural network simulation
In Sections 4 and 5 of the main text, we have shown,
starting from Maxwell’s equations, how the gradient
information deﬁned for an arbitrary problem can be
obtained through electric ﬁeld intensity measurements.
However, since the full electromagnetic problem is too
large to solve repeatedly, for the purposes of demonstration of a functioning neural network, in Section 6 we use
the analytic, matrix representation of a mesh of MZIs as
described in Ref. . Namely, for an even N, the matrix
ˆW of the OIU is parametrized as the product of N + 1
unitary matrices:
ˆW = ˆRN ˆRN−1 . . . ˆR2 ˆR1 ˆD,
where each ˆRi implements a number of two-by-two unitary operations corresponding to a given MZI, and ˆD is a
diagonal matrix corresponding to an arbitrary phase delay added to each port. This is schematically illustrated
in Fig. 5(a) for N = 3. For the ANN training, we need
to compute terms of the form
for an arbitrary phase φ and vectors X and Y deﬁned
following the steps in the main text. Because of the feedforward nature of the OIU-s, the matrix ˆW can also be
ˆW = ˆW2 ˆ
Fφ is a diagonal matrix which applies a phase shift
eiφ in port i (the other elements are independent of φ),
while ˆW1 and ˆW2 are the parts that precede and follow
the phase shifter, respectively (Fig.
(B2) becomes
2 Y)ieiφ( ˆW1X)i
where (V)i is the i-th element of the vector V, and I
denotes the imaginary part. This result can be written
more intuitively in a notation similar to the main text.
Namely, if Aφ is the ﬁeld amplitude generated by input
X from the right, measured right after the phase shifter
corresponding to φ, while Aadj
is the ﬁeld amplitude generated by input Y from the right, measured at the same
point, then
By recording the amplitudes in all ports during the forward and the backward propagation, we can thus compute in parallel the gradient with respect to every phase
shifter. Notice that, within this computational model,
we do not need to go through the full procedure outlined
in Section 4 of the main text. However, this procedure is
crucial for the in situ measurement of the gradients, and
works even in cases which cannot be correctly captured
by the simpliﬁed matrix model used here.
Appendix C: Training demonstration
In the main text we show how the in-situ backpropagation method may be used to train a simple XOR network.
Here we demonstrate training on a more complex problem. Speciﬁcally, we generate a set of one thousand training examples represented by input and target (X0 →T)
pairs. Here, X0 = [x1, x2, P, 0]T where x1 and x2 are the
independent inputs, which we constrain to be real for
simplicity, and P(x1, x2) =
2 represents a
mode added to the third port to make the norm of X0 the
same for each training example. In this case, we choose
P0 = 10. Each training example has a corresponding label, y ∈{0, 1} which is encoded in the desired output,
T, as .T and .T for y = 0 and y = 1
respectively.
For a given x1 and x2, we deﬁne r and φ as the magnitude and phase of the vector (x1, x2) in the 2D-plane,
respectively. To generate the corresponding class label,
we ﬁrst generate a uniform random variable between 0
and 1, labeled U, and then set y = 1 if
−(r −r0 −∆sin(2φ))2
+ 0.1 U > 0.5.
Otherwise, we set y = 0. For the demonstration, r0 =
0.6, ∆= 0.15, and σ = 0.2. The underlying distribution
thus resembles an oblong ring centered around x1 = x2 =
0, with added noise.
As diagrammed in Fig. 6(a), we use a network architecture consisting of six 4×4 layers of unitary OIUs, with
an element-wise activation f(z) = |z| after each unitary
transformation except for the last in the series, which
has an activation of f(z) = |z|2. After the ﬁnal activation, we apply an additional ‘softmax’ activation, which
gives a normalized probability distribution corresponding to the predicted class of X0. Speciﬁcally, these are
given by s(zi) = exp (zi)/
j exp (zj)
, where zi=1,2
is the ﬁrst/second element of the output vector of the
last activation (the other two elements are ignored). The
ANN prediction for the input X0 is set as the larger one
of these two outputs, while the total cost function is de-
ﬁned in the cross-entropy form
−log(s(zm,t)),
where L(m) is the cost function of the m-th example, the
summation is over all training examples, and zm,t is the
output from the target port, t, as deﬁned by the target
output T(m) of the m-th example. We randomly split our
generated examples into a training set containing 75%
of the originally generated training examples, while the
remaining 25% are used as a test set to evaluate the performance of our network on unseen examples.
As in the XOR demonstration, we utilized our matrix
model of the system described in Section B. As in the
main text, at each iteration of training we compute the
gradient of the cost function with respect to the phases
of each of the integrated phase shifters, and sum this over
each of the training examples. For the backpropagation
through the activation functions, since |z| and |z|2 are
non-holomorphic, we use eq.
A22 from Section A, to
δl = exp(−iφl) ⊙R{Γl},
f(z) = |z|
f(z) = |z|
f(z) = |z|2
FIG. 6. Numerical demonstration of a photonic ANN learning
to classify an oblong ring. (a) The architecture of the ANN.
Six layers of 4 × 4 OIUs with |z| activations. A ﬁnal softmax
activation is applied at the very end. (b) The loss function of
Eq. (C2) over training iterations. (c) The training examples,
blue and red dots correspond to y = 0 and y = 1 labels on a
given x1 and x2 input. The background shows the prediction
of the network on a continuum of x1 and x2 pairs, with colors
representing the corresponding predictions. One can see that
the ring was learned successfully without overﬁtting.
where φl is a vector containing the phases of Zl and ΓL is
given by the derivative of the cross-entropy loss function
for a single training example
ΓL = ∂L(m)
= s(zm,i) −δi,t,
where δi,t is the Kronecker delta.
With this, we can now compute the gradient of the loss
function of eq. C2 with respect to all trainable parameters, and perform a parallel, steepest-descent update to
the phase shifters, in accordance with the gradient information. Our network successfully learned the this task
in around 4000 iterations. The results of the training are
shown in Fig. 6(b). We achieved a training and test accuracy of 91% on both the training and test sets, indicating
that the network was not overﬁtting to the dataset. This
can also be conﬁrmed visually from Fig. 6(c). The lack of
perfect predictions is likely due to the inclusion of noise.