Transformers in Vision: A Survey
Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir,
Fahad Shahbaz Khan, and Mubarak Shah
Abstract—Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their
application to computer vision problems. Among their salient beneﬁts, Transformers enable modeling long dependencies between input
sequence elements and support parallel processing of sequence as compared to recurrent networks e.g., Long short-term memory
(LSTM). Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited
as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos,
text and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge
datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to
provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to
fundamental concepts behind the success of Transformers i.e., self-attention, large-scale pre-training, and bidirectional feature
encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classiﬁcation,
object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual
reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image
super-resolution, image enhancement, and colorization) and 3D analysis (e.g., point cloud classiﬁcation and segmentation). We
compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental
value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further
interest in the community to solve current challenges towards the application of transformer models in computer vision.
Index Terms—Self-attention, transformers, bidirectional encoders, deep neural networks, convolutional networks, self-supervision.
INTRODUCTION
RANSFORMER models have recently demonstrated
exemplary performance on a broad range of language
tasks e.g., text classiﬁcation, machine translation and
question answering. Among these models, the most popular
ones include BERT (Bidirectional Encoder Representations
from Transformers) , GPT (Generative Pre-trained Transformer) v1-3 – , RoBERTa (Robustly Optimized BERT
Pre-training) and T5 (Text-to-Text Transfer Transformer)
 . The profound impact of Transformer models has become
more clear with their scalability to very large capacity models , . For example, the BERT-large model with
340 million parameters was signiﬁcantly outperformed by
the GPT-3 model with 175 billion parameters while the
latest mixture-of-experts Switch transformer scales up
to a whopping 1.6 trillion parameters!
The breakthroughs from Transformer networks in Natural Language Processing (NLP) domain has sparked great
interest in the computer vision community to adapt these
models for vision and multi-modal learning tasks (Fig. 1).
S. Khan, M. Naseer and F. S. Khan are with the MBZ University of
Artiﬁcial Intelligence, Abu Dhabi, UAE.
E-mail: 
M. Hayat is with the Faculty of IT, Monash University, Clayton VIC
3800, Australia.
S. W. Zamir is with the Inception Institute of Artiﬁcial Intelligence, Abu
Dhabi, UAE.
S. Khan and M. Naseer are also with the CECS, Australian National
University, Canberra ACT 0200, Australia.
F. S. Khan is also with the Computer Vision Laboratory, Link¨oping
University, Sweden.
M. Shah is with the Center for Research in Computer Vision, University
of Central Florida, Orlando, FL 32816, United States.
Manuscript received March, 2021.
However, visual data follows a typical structure (e.g., spatial
and temporal coherence), thus demanding novel network
designs and training schemes. As a result, Transformer models and their variants have been successfully used for image
recognition , , object detection , , segmentation , image super-resolution , video understanding
 , , image generation , text-image synthesis 
and visual question answering , , among several
other use cases – . This survey aims to cover such
recent and exciting efforts in the computer vision domain,
providing a comprehensive reference to interested readers.
Transformer architectures are based on a self-attention
mechanism that learns the relationships between elements
of a sequence. As opposed to recurrent networks that process sequence elements recursively and can only attend to
short-term context, Transformers can attend to complete
sequences thereby learning long-range relationships. Although attention models have been extensively used in
both feed-forward and recurrent networks , , Transformers are based solely on the attention mechanism and
have a unique implementation (i.e., multi-head attention)
optimized for parallelization. An important feature of these
models is their scalability to high-complexity models and
large-scale datasets e.g., in comparison to some of the other
alternatives such as hard attention which is stochastic in
nature and requires Monte Carlo sampling for sampling attention locations. Since Transformers assume minimal prior
knowledge about the structure of the problem as compared
to their convolutional and recurrent counterparts – ,
they are typically pre-trained using pretext tasks on largescale (unlabelled) datasets , . Such a pre-training avoids
costly manual annotations, thereby encoding highly expresarXiv:2101.01169v5 [cs.CV] 19 Jan 2022
Fig. 1: Statistics on the number of times keywords such as BERT, Self-Attention, and Transformers appear in the titles of Peerreviewed and arXiv papers over the past few years (in Computer Vision and Machine Learning). The plots show consistent growth
in recent literature. This survey covers recent progress on Transformers in the computer vision domain.
sive and generalizable representations that model rich relationships between the entities present in a given dataset. The
learned representations are then ﬁne-tuned on the downstream tasks in a supervised manner to obtain favorable
This paper provides a holistic overview of the transformer models developed for computer vision applications.
We develop a taxonomy of the network design space and
highlight the major strengths and shortcomings of the existing methods. Other literature reviews mainly focus on
the NLP domain , or cover generic attention-based
approaches , . By focusing on the newly emerging
area of visual transformers, we comprehensively organize
the recent approaches according to the intrinsic features of
self-attention and the investigated task. We ﬁrst provide an
introduction to the salient concepts underlying Transformer
networks and then elaborate on the speciﬁcs of recent vision
transformers. Where ever possible, we draw parallels between the Transformers used in the NLP domain and the
ones developed for vision problems to ﬂash major novelties
and interesting domain-speciﬁc insights. Recent approaches
show that convolution operations can be fully replaced
with attention-based transformer modules and have also
been used jointly in a single design to encourage symbiosis
between the two complementary set of operations. This survey ﬁnally details open research questions with an outlook
towards the possible future work.
FOUNDATIONS
There exist two key ideas that have contributed towards
the development of conventional transformer models. (a)
The ﬁrst one is self-attention, which allows capturing ‘longterm’ dependencies between sequence elements as compared to conventional recurrent models that ﬁnd it challenging to encode such relationships. (b) The second key
idea is that of pre-training1 on a large (un)labelled corpus in
a (self)supervised manner, and subsequently ﬁne-tuning to
the target task with a small labeled dataset , , . Below, we provide a brief tutorial on these two ideas (Sec. 2.2
and 2.1), along with a summary of seminal Transformer
1. Several recent Vision Transformers demonstrate that the model
can be learned end-to-end on ImageNet-1K without any dedicated pretraining phase – . However, the performance generally remains
lower than the pre-trained counter-parts.
Fig. 2: An example self-attention block used in the vision
domain . Given the input sequence of image features, the
triplet of (key, query, value) is calculated followed by attention
calculation and applying it to reweight the values. A single
head is shown here and an output projection (W) is ﬁnally
applied to obtain output features with the same dimension as
the input. Figure adapted from .
networks (Sec. 2.3 and 2.4) where these ideas have been
applied. This background will help us better understand
the forthcoming Transformer based models used in the
computer vision domain (Sec. 3).
Self-Attention in Transformers
Given a sequence of items, self-attention estimates the relevance of one item to other items (e.g., which words are
likely to come together in a sentence). The self-attention
mechanism is an integral component of Transformers, which
explicitly models the interactions between all entities of a
sequence for structured prediction tasks. Basically, a selfattention layer updates each component of a sequence by
aggregating global information from the complete input
sequence. Lets denote a sequence of n entities (x1, x2, · · · xn)
by X ∈Rn×d, where d is the embedding dimension to represent each entity. The goal of self-attention is to capture the
interaction amongst all n entities by encoding each entity
in terms of the global contextual information. This is done
by deﬁning three learnable weight matrices to transform
Queries (WQ ∈Rd×dq), Keys (WK ∈Rd×dk) and Values
(WV ∈Rd×dv), where dq = dk. The input sequence X is
ﬁrst projected onto these weight matrices to get Q = XWQ,
K = XWK and V = XWV . The output Z ∈Rn×dv of the
self attention layer is,
Z = softmax
Fig. 3: Architecture of the Transformer Model . The model was ﬁrst developed for the language translation task where an input
sequence in one language is required to be converted to the output sequence in another language. The Transformer encoder
(middle row) operates on the input language sequence and converts it to an embedding before passing it on to the encoder blocks.
The Transformer decoder (bottom row) operates on the previously generated outputs in the translated language and the encoded
input sequence from the middle branch to output the next word in the output sequence. The sequence of previous outputs (used
as input to the decoder) is obtained by shifting the output sentence to the right by one position and appending start-of-sentence
token at the beginning. This shifting avoids the model to learn to simply copy the decoder input to the output. The ground-truth
to train the model is simply the output language sequence (without any right shift) appended with an end-of-sentence token. The
blocks consisting of multi-head attention (top row) and feed-forward layers are repeated N times in both the encoder and decoder.
For a given entity in the sequence, the self-attention basically computes the dot-product of the query with all keys,
which is then normalized using softmax operator to get the
attention scores. Each entity then becomes the weighted sum
of all entities in the sequence, where weights are given by
the attention scores (Fig. 2 and Fig. 3, top row-left block).
Self-Attention: The standard self-attention
layer attends to all entities. For the Transformer model 
which is trained to predict the next entity of the sequence,
the self-attention blocks used in the decoder are masked to
prevent attending to the subsequent future entities. This is
simply done by an element-wise multiplication operation
with a mask M ∈Rn×n, where M is an upper-triangular
matrix. The masked self-attention is deﬁned by,
where ◦denotes Hadamard product. Basically, while predicting an entity in the sequence, the attention scores of the
future entities are set to zero in masked self-attention.
Multi-Head Attention: In order to encapsulate multiple
complex relationships amongst different elements in the
sequence, the multi-head attention comprises multiple selfattention blocks (h = 8 in the original Transformer model
 ). Each block has its own set of learnable weight matrices {WQi, WKi, WVi}, where i = 0 · · · (h−1). For an
input X, the output of the h self-attention blocks in multihead attention is then concatenated into a single matrix
[Z0, Z1, · · · Zh−1] ∈Rn×h·dv and projected onto a weight
matrix W ∈Rh·dv×d (Fig. 3, top row).
The main difference of self-attention with convolution
operation is that the ﬁlters are dynamically calculated instead of static ﬁlters (that stay the same for any input) as in
the case of convolution. Further, self-attention is invariant
to permutations and changes in the number of input points.
As a result, it can easily operate on irregular inputs as opposed to standard convolution that requires grid structure.
Furthermore, it has been shown in the literature how selfattention (with positional encodings) is theoretically a more
ﬂexible operation which can model the behaviour of convolutional models towards encoding local features . Cordonnier et al. further studied the relationships between
self-attention and convolution operations. Their empirical
results conﬁrm that multi-head self-attention (with sufﬁcient
parameters) is a more generic operation which can model
the expressiveness of convolution as a special case. In fact,
self-attention provides the capability to learn the global as
well as local features, and provide expressivity to adaptively
learn kernel weights as well as the receptive ﬁeld (similar to
deformable convolutions ).
(Self) Supervised Pre-training
Self-attention based Transformer models generally operate
in a two-stage training mechanism. First, pre-training is
performed on a large-scale dataset (and sometimes a combination of several available datasets , ) in either a
supervised or a self-supervised manner , , .
Later, the pre-trained weights are adapted to the downstream tasks using small-mid scale datasets. Examples of
downstream tasks include image classiﬁcation , object detection , zero-shot classiﬁcation , questionanswering and action recognition . The effectiveness of pre-training for large-scale Transformers has been
advocated in both the language and vision domains. For
example, Vision Transformer model (ViT-L) experiences
an absolute 13% drop in accuracy on ImageNet test set
when trained only on ImageNet train set as compared to the
case when pretrained on JFT dataset with 300 million
Since acquiring manual labels at a massive scale is cumbersome, self-supervised learning has been very effectively
used in the pre-training stage. The self-supervision based
pre-training stage training has played a crucial role in unleashing the scalability and generalization of Transformer
networks, enabling training even above a trillion parameter networks (e.g., the latest Switch Transformer from
Google). An extensive survey on SSL can be found in ,
 . As nicely summarized by Y. LeCun , the basic
idea of SSL is to ﬁll in the blanks, i.e., try to predict the
occluded data in images, future or past frames in temporal
video sequences or predict a pretext task e.g., the amount
of rotation applied to inputs, the permutation applied to
image patches or the color of a gray-scale image. Another
effective way to impose self-supervised constraints is via
contrastive learning. In this case, nuisance transformations
are used to create two types of modiﬁed versions of the same
image i.e., without changing the underlying class semantics
(e.g., image stylizing, cropping) and with semantic changes
(e.g., replacing an object with another in the same scene, or
changing the class with minor adversarial changes to the
image). Subsequently, the model is trained to be invariant to
the nuisance transformations and emphasize on modeling
minor changes that can alter semantic labels.
Self-supervised learning provides a promising learning
paradigm since it enables learning from a vast amount of
readily available non-annotated data. In the SSL based pretraining stage, a model is trained to learn a meaningful
representation of the underlying data by solving a pretext
task. The pseudo-labels for the pretext task are automatically generated (without requiring any expensive manual
annotations) based on data attributes and task deﬁnition.
Therefore, the pretext task deﬁnition is a critical choice in
SSL. We can broadly categorize existing SSL methods based
upon their pretext tasks into (a) generative approaches which
synthesize images or videos (given conditional inputs), (b)
context-based methods which exploit the relationships between image patches or video frames, and (c) cross-modal
methods which leverage from multiple data modalities.
Examples of generative approaches include conditional generation tasks such as masked image modeling and
image colorization , image super-resolution , image
in-painting , and GANs based methods , . The
context-based pretext methods solve problems such as a
jigsaw puzzle on image patches – , masked object
classiﬁcation , predict geometric transformation such as
rotation , , or verify temporal sequence of video
frames – . Cross-modal pretext methods verify the
correspondence of two input modalities e.g., text & image
 , audio & video , or RGB & ﬂow .
Transformer Model
The architecture of the Transformer model proposed in 
is shown in Fig. 3. It has an encoder-decoder structure. The
encoder (middle row) consists of six identical blocks (i.e.,
N=6 in Fig. 3), with each block having two sub-layers: a
multi-head self-attention network, and a simple positionwise fully connected feed-forward network. Residual connections alongside layer normalization are employed after each block as in Fig. 3. Note that, different from
regular convolutional networks where feature aggregation
and feature transformation are simultaneously performed
(e.g., with a convolution layer followed by a non-linearity),
these two steps are decoupled in the Transformer model
i.e., self-attention layer only performs aggregation while the
feed-forward layer performs transformation. Similar to the
encoder, the decoder (bottom row) in the Transformer model
comprises six identical blocks. Each decoder block has three
sub-layers, ﬁrst two (multi-head self-attention, and feedforward) are similar to the encoder, while the third sublayer performs multi-head attention on the outputs of the
corresponding encoder block, as shown in Fig. 3.
The original Transformer model in was trained for
the Machine Translation task. The input to the encoder is
a sequence of words (sentence) in one language. Positional
encodings are added to the input sequence to capture the
relative position of each word in the sequence. Positional
encodings have the same dimensions as the input d = 512,
and can be learned or pre-deﬁned e.g., by sine or cosine
functions. Being an auto-regressive model, the decoder of
the Transformer uses previous predictions to output the
next word in the sequence. The decoder, therefore, takes
inputs from the encoder as well as the previous outputs
to predict the next word of the sentence in the translated
language. To facilitate residual connections the output dimensions of all layers are kept the same i.e., d = 512.
The dimensions of query, key and value weight matrices
in multi-head attention are set to dq = 64, dk = 64, dv = 64.
Bidirectional Representations
The training strategy of the original Transformer model 
could only attend to the context on the left of a given word
in the sentence. This is limiting, since for most language
tasks, contextual information from both left and right sides
is important. Bidirectional Encoder Representations from
Transformers (BERT) proposed to jointly encode the right
and left context of a word in a sentence, thus improving
the learned feature representations for textual data in an
self-supervised manner. To this end, BERT introduced
two pretext tasks to pre-train the Transformer model in
a self-supervised manner: Masked Language Model and Next
Sentence Prediction. For adapting the pre-trained model for
downstream tasks, a task-speciﬁc additional output module
is appended to the pre-trained model, and the full model
is ﬁne-tuned end-to-end. Here, we brieﬂy touch upon the
pretext tasks. (1) Masked Language Model (MLM) - A
ﬁxed percentage (15%) of words in a sentence are randomly
masked and the model is trained to predict these masked
words using cross-entropy loss. In predicting the masked
words, the model learns to incorporate the bidirectional
context. (2) Next Sentence Prediction (NSP) - Given a pair
of sentences, the model predicts a binary label i.e., whether
the pair is valid from the original document or not. The
training data for this can easily be generated from any
monolingual text corpus. A pair of sentences A and B is
Fig. 4: A taxonomy of self-attention design space. Existing approaches based on self-attention explore single-head or multi-head
(transformer) designs for vision tasks. We note that interesting efforts have been made to utilize knowledge from convolution
based architectures to improve ViTs (e.g., multi-scale and hybrid designs). We categorize the upcoming sections of this survey
according to the types of self-attention block (left tree diagram) as well as the prominent tasks in computer vision (right).
formed, such that B is the actual sentence (next to A) 50% of
the time, and B is a random sentence for other 50% of the
time. NSP enables the model to capture sentence-to-sentence
relationships which are crucial in many language modeling
tasks such as Question Answering and Natural Language
Inference.
SELF-ATTENTION & TRANSFORMERS IN VISION
We broadly categorize vision models with self-attention
into two categories: the models which use single-head selfattention (Sec. 3.1), and the models which employ multihead self-attention based Transformer modules into their
architectures (Sec. 3.2). Below, we ﬁrst discuss the ﬁrst
category of single-head self-attention based frameworks,
which generally apply global or local self-attention within
CNN architectures, or utilize matrix factorization to enhance
design efﬁciency and use vectorized attention models. We
then discuss the Transformer-based vision architectures in
Single-head Self-Attention
Self-Attention in CNNs
Inspired by non-local means operation which was
mainly designed for image denoising, Wang et al. proposed a differentiable non-local operation for deep neural
networks to capture long-range dependencies both in space
and time in a feed-forward fashion. Given a feature map,
their proposed operator computes the response at a
position as a weighted sum of the features at all positions
in the feature map. This way, the non-local operation is
able to capture interactions between any two positions in
the feature map regardless of the distance between them.
Videos classiﬁcation is an example of a task where longrange interactions between pixels exist both in space and
time. Equipped with the capability to model long-range
interactions, demonstrated the superiority of non-local
deep neural networks for more accurate video classiﬁcation
on Kinetics dataset .
(a) Non-local block 
(b) Criss-cross attention 
Fig. 5: Comparison of two different self-attention approaches:
Non-local self-attention block and Criss-cross self-attention
module . Figure is from .
Although the self-attention allows us to model fullimage contextual information, it is both memory and compute intensive. As shown in Fig. 5(a), in order to encode
global context for a given pixel location, non-local block 
computes a dense attention map (in green). The non-local
block has a high complexity of O(N 2), where N denotes the number of input feature maps. To reduce this
computational burden, Huang et al. propose the crisscross attention module that for each pixel position generates
a sparse attention map only on the criss-cross path, as illustrated in Fig. 5(b). Further, by applying criss-cross attention
recurrently, each pixel position can capture context from all
other pixels. Compared to non-local block, the criss-cross
uses 11× lesser GPU memory, and has a complexity of
N). State-of-the-art results are reported for the
semantic and instance segmentation tasks on several benchmark datasets including Cityscapes , ADE20K ,
COCO , LIP and CamVid .
Another shortcoming of the convolutional operator
comes from the fact that after training, it applies ﬁxed
weights regardless of any changes to the visual input. Hu
et al. proposed local relation networks to adaptively
compose pixels in a local window. They introduced a new
differentiable layer that adapts its weight aggregation based
on the compositional relations (similarity) between pixels/features within a local window. Such adaptive weight
aggregation introduces geometric priors into the network
which are important for the recognition tasks . Convolution is considered to be a top-down operator as it remains
ﬁxed across positions while a non-local operation such as
introduced in is a bottom-up method as it aggregates
input features over the full image. The local relation layer
belongs to the category of bottom-up methods but it is
restricted to a ﬁxed window size e.g., 7x7 neighborhood.
Bello et al. explore the possibility of employing
self-attention as an alternative to convolutional operators.
They employ the relative position encoding in two
dimensions to develop a new self-attention mechanism that
maintains translation equivariance, a desirable property for
handling images. Although this self-attention provides competitive results as a stand-alone computational primitive,
the best performance is obtained in combination with the
convolutional operations. Authors show that attention augmentation leads to systematic performance gains in image
classiﬁcation and object detection for different architectures.
Self-Attention as Stand-alone Primitive
As discussed above, convolutional layers possess translation equivariance but can not scale with a large receptive
ﬁeld, therefore can not capture long-range interactions .
On the other hand, global attention which attend to
all spatial locations of the input can be computationally
intensive and is preferred on down-sampled small images,
image patches or augmenting the convolutional features
space . Ramachandran et al. proposed to replace
convolutional layers in deep neural networks with a local
self-attention layer which can be applied to small or large
inputs without increasing the computational cost. At a basic
level, the proposed self-attention layer considers all
pixel positions in a speciﬁc window size around a given
pixel, compute queries, keys and value vectors for these
pixels, and then aggregates the spatial information within
this window. The value vectors are aggregated after projecting the softmax score of queries and keys. This process
is repeated for all given pixels and the response is concatenated to produce the output pixel. ResNet models with local
self-attention layer can solve ImageNet and COCO object
detection with fewer parameters as compared to ResNet
models based on convolutional layers .
Zhao et al. note that a traditional convolution
operator performs feature aggregation and transformation
jointly (by applying a ﬁlter and then passing it through
a non-linearity). In contrast, they propose to perform feature aggregation separately with self-attention followed by
transformation using an element-wise perceptron layer. For
feature aggregation, they propose two alternate strategies:
(a) pairwise self-attention and (b) patch-wise self-attention.
The pairwise self-attention is permutation and cardinality
invariant operation, while the patch-wise self-attention does
not have such invariance properties (similar to convolution). Both pairwise and patch-wise self-attentions are implemented as a vector attention that learns weights for
both the spatial and channel dimensions. This provides an
alternate approach for attention that is conventionally performed using scalar weights (by taking a dot-product). The
pairwise self-attention is a set operator that computes a vector attention keeping in view the relationships of a particular
feature with its neighbors in a given local neighborhood.
In contrast, patch-wise self-attention is a generalization of
the convolution operator (not a set operator) and looks at
all the feature vectors in the local neighbourhood when
deriving the attention vectors. Authors show that with considerably fewer parameters, self-attention networks (SAN)
can beat ResNet baselines on the ImageNet dataset. They
further show robustness against adversarial perturbations
 , and generalization to unseen transformations .
This behaviour is due to the dynamic nature of attention
that makes it difﬁcult for the adversary to calculate useful
fooling directions.
Multi-head Self-Attention (Transformers)
Unlike the approaches discussed in Sec. 3.1 which insert
self-attention as a component in CNN inspired architectures,
Vision Transformer (ViTs) adapts the architecture of 
(see Fig. 3), which cascades multiple Transformer layers.
ViTs have gained signiﬁcant research attention, and a number of recent approaches have been proposed which build
upon ViTs. Below, we discuss these methods by categorizing
them into: uniform scale ViTs having single-scale features
through all layers (Sec. 3.2.1), multi-scale ViTs that learn
hierarchical features which are more suitable for dense
prediction tasks (Sec. 3.2.2), and hybrid designs having
convolution operations within ViTs (Sec. 3.2.3).
Uniform-scale Vision Transformers
The original Vision Transformer model belongs to this
family, where the multi-head self-attention is applied to a
consistent scale in the input image where the spatial scale is
maintained through the network hierarchy. We name such
models as the uniform-scale ViTs, as described below.
Vision Transformer (ViT) (Fig. 6) is the ﬁrst work
to showcase how Transformers can ‘altogether’ replace
standard convolutions in deep neural networks on largescale image datasets. They applied the original Transformer
model (with minimal changes) on a sequence of image
’patches’ ﬂattend as vectors. The model was pre-trained
on a large propriety dataset (JFT dataset with 300
million images) and then ﬁne-tuned to downstream recognition benchmarks e.g., ImageNet classiﬁcation. This is an
important step since pre-training ViT on a medium-range
dataset would not give competitive results, because the
CNNs encode prior knowledge about the images (inductive
biases e.g., translation equivariance) that reduces the need of
data as compared to Transformers which must discover such
information from very large-scale data. Notably, compared
to the iGPT model that also applied Transformers to
full-sized images but performs training as a generative task,
ViT pre-trains the model with a supervised classiﬁcation
task (although a self-supervision variant is also explored
which results in a less performance).
The DeiT is the ﬁrst work to demonstrate that
Transformers can be learned on mid-sized datasets (i.e., 1.2
million ImageNet examples compared to 300 million images
of JFT used in ViT ) in relatively shorter training
episodes. Besides using augmentation and regularization
procedures common in CNNs, the main contribution of
DeiT is a novel native distillation approach for Transformers which uses a CNN as a teacher model (RegNetY-
16GF ) to train the Transformer model. The outputs
from the CNN aid the Transformer in efﬁciently ﬁguring
Fig. 6: An overview of Vision Transformer (on the left) and the
details of Transformer encoder (on the right). The architecture
resembles Transformers used in the NLP domain and the image
patches are simply fed to the model after ﬂattening. After
training, the feature obtained from the ﬁrst token position is
used for classiﬁcation. Image obtained from .
out useful representations for input images. A distillation
token is appended with the input patch embeddings and
the class token. The self-attention layers operate on these
tokens to learn their inter-dependencies and outputs the
learned class, patch, and distillation tokens. The network is
trained with a cross-entropy loss deﬁned on the output class
token and a distillation loss to match the distillation token
with the teacher output. Both soft and hard label choices
were explored for distillation, where the hard distillation
was found to perform better. Interestingly, the learned class
and distillation tokens do not exhibit a high correlation indicating their complementary nature. The learned representations compare favorably well against top-performing CNN
architectures such as EfﬁcientNet and also generalize
well for a number of downstream recognition tasks.
Token to Token (T2T) ViT recursively combines
neighboring tokens into a single token to reduce tokens
length and aggregate spatial context. Transformer in Transformer computes attention at two levels: patch-level
(as done is standard ViTs ) and local sub-patch-level
(e.g.by subdividing a 16 × 16 patch into four 4 × 4 blocks,
and computing attention amongst these blocks). In token
labelling ViT , all patch tokens contribute towards loss
calculation, different from regular ViTs that only use classiﬁcation token in the loss. This process includes auxiliary
supervision where each image-patch (token) is labeled using
a pre-trained CNN model. Similar to CutMix augmentation
 , tokens from different images are mixed as an augmentation strategy, and the model is trained using the standard
classiﬁcation loss and auxiliary token-label loss. Their model
demonstrates excellent performance specially for smaller
sized models.
The quadratic complexity of self-attention hinders its
applicability to longer sequences (high-resolution images).
Cross-Covariance Image Transformers (XCiT) incorporate attention across feature-channels instead of tokens, i.e., their cross-covariance attention is given by
. The proposed cross-covariance attention has linear complexity (since it depends upon feature
dimension instead of the number of tokens). XCiT can
therefore handle large resolution images and demonstrate
excellent performance across different vision tasks i.e., selfsupervised and fully supervised image classiﬁcation and
dense prediction (detection, segmentation). DeepViT 
observes that the similarity between attention maps of
deeper layer is high and hinders scaling models depth.
They propose to re-attend the attention maps in a multihead block instead of simple aggregation of these attention
maps, and show consistent gains over standard multi-head
self attention based ViTs.
Multi-scale Vision Transformers
In standard ViTs, the number of the tokens and token feature
dimension are kept ﬁxed throughout different blocks of
the network. This is limiting, since the model is unable
to capture ﬁne spatial details at different scales. Initial
Transformer based dense prediction methods (e.g., DETR
 ) therefore have a convolutional backend. Multi-stage
hierarchical design for ViTs, where number of tokens is
gradually reduced while the token feature dimension is
progressively increased, has been shown to produce effective features for dense prediction tasks , – .
These models generally also perform well for recognition
tasks. These architectures mostly sparsify tokens by merging neighboring tokens and projecting them to a higher
dimensional feature space. Examples of multi-stage ViTs
include Pyramid ViT , , Twins , CoaT , Swin
Transformer , Convolutional vision Transformer (CvT)
 , Shufﬂe Transformer , CrossFormer , RegionViT
 and Focal Transformer models . Some of them are
hybrid designs (with both convolution and self-attention
operations, see Sec. 3.2.3), while others only employ pure
self-attention based design (discussed next).
Pyramid ViT (PVT) is the ﬁrst hierarchical design
for ViT, and proposes a progressive shrinking pyramid
and spatial-reduction attention. PVTv2 and SegFormer
 improve original PVT by introducing overlapping
patch embedding, depth-wise convolution, and efﬁcient
attention. Swin Transformer has a multi-stage hierarchical architecture which computes attention within a local
window, by partitioning the window into multiple subpatches. To capture interactions between different windows
(image locations), window partitioning is gradually shifted,
along the hierarchy of the network, to capture overlapping
regions. Focal Transformer models is another hierarchical design, where focal self-attention is introduced to
simultaneously capture global and local relationships. Similarly, CrossFormer has a hierarchical pyramid structure,
and introduces cross-scale embedding module, along-with
long short distance attention and dynamic position bias
to faithfully capture both local and global visual cues.
RegionViT proposes a regional-to-local attention to
encode hierarchical features. Multi-Scale Vision Longformer
 also considers a local context in self-attention, but
employs the efﬁcient Longformer design for selfattention. CrossViT encodes multi-scale features with
two branches (each with multiple transformer blocks), by
separately processesing smaller and larger image patches.
The information from these two multi-scale bracnches is
then fused together using a cross-attention module.
Hybrid ViTs with Convolutions
Convolutions do an excellent job at capturing low-level local
features in images, and have been explored in multiple hybrid ViT designs, specially at the beginning to “patchify and
tokenize” an input image. For example, Convolutional vision Transformer (CvT) incorporate convolution based
projection to capture the spatial structure and low-level
details, for tokenization of image patches. CvT has a hierarchical design, where number of tokens is progressively reduced while the token-width is increased, thus imitating the
impact of spatial downsampling as in CNNs. Convolution
enhanced image Transformers employ convolutions
based image-to-token module to extract low-level features.
Compact Convolutional Transformer (CCT) introduces
a new sequence pooling scheme, and incorporates convolutional blocks (conv-pool-reshape) for tokenization. CCT can
be trained from scratch on smaller datasets, e.g., CIFAR10
with ∼95% accuracy, which is a remarkable property not
possible with the traditional ViTs.
LocalViT introduces depthwise convolutions to enhance local features modeling capability of ViTs. LeViT 
(name inspired from LeNet ) applies a four-layered
CNN block (with 3 × 3 convolutions) at the beginning with
progressively increasing channels (3,32,64,128,256). For a
3×224×224 input image, the resulting 256×14×14 output
from the CNN block becomes input to a hierarchical ViT.
By virtue of its design, LeViT is 5× faster than EfﬁcientNet
 on CPU, at inference. ResT is another hierarchical
architecture which applies a CNN block at the beginning for
patch-embedding. It incorporates depth-wise convolutions
and adaptive position encoding to tackle varying image
sizes. A recent approach NesT proposes a simple
technique to introduce hierarchy in ViTs. NesT divides an
image into non-overlapping blocks (each block is further
split into patches). It ﬁrst separately applies local selfattention on patches within each block, and then enables
global interaction between blocks by aggregating them into
an image space and applying convolution operation, followed by downsampling. The number of blocks is gradually
reduced along the hierarchy of the model, while number
of local-patches is kept ﬁxed. This simple scheme performs
favorably compared with more sophisticated designs ,
 , and enables training NesT on smaller datasets (e.g.,
CIFAR-10) from scratch.
Depthwise Convolution and self-Attention Networks
(CoAtNets)
module (which combines depthwise convolutions and selfattention), and vertically stack convolution and attention
layers. CoAtNets demonstrate an impressive 86% ImageNet top-1 accuracy without extra data (i.e. trained only
on ImageNet-1k). Shufﬂe Transformer performs selfattention within a window and has depth-wise convolutions
between the window-based multi-head self-attention and
MLP. It introduces a shufﬂe operation to build stronger
cross-patch connections. Co-scale conv-attentional image
Transformers (CoaT) , is a hybrid hierarchical pyramid
design, with serial and parallel blocks, where the serial
block is similar to standard transformer block except for
the attention layer replaced with depthwise convolution.
The parallel blocks is applied on the output of serial blocks
and encodes relationships between tokens at multiple scales
using cross-attention. Twins builds upon PVT (an
attention only pyramid design), by replacing the absolute
position embedding in PVT with relative conditional position embedding , and incorporating the separable
depth-wise convolutions instead of the standard spatial
attention, to capture local and global context of the image. In
this sense, the hybrid designs tend to combine the strengths
of both convolution and transformer models. TransCNN
 propose a hierarchical multi-head self attention block,
which ﬁrst learns interactions within small grids (tokens)
using self-attention, and then gradually merges the smaller
grids into larger grids. The proposed block can then be
plugged into existing CNN architectures.
Self-Supervised Vision Transformers
Contrastive learning based self-supervised approaches,
which have gained signiﬁcant success for CNN based vision
tasks, have also been investigated for ViTs. Chen et al. 
evaluate different self-supervised frameworks and propose
practical strategies including MoCo v3 (extended from
v1/v2 , ) for stabilized training of self-supervised
ViTs. Xie et al. combine MoCo v2 and BYOL 
to train DeiT and SwinTransformer . They demonstrate generalization of self-supervised SwinTransformer
for dense prediction tasks of detection and segmentation.
Self distillation with no labels (DINO) demonstrate
that self-supervised ViTs can automatically segment the
background pixels of an image, even though they were
never trained using pixel-level supervision, a phenomena
otherwise not observed in CNNs or fully supervised ViTs.
Efﬁcient self-supervised vision transformer (EsViT) 
propose a multi-stage design, where neighboring tokens are
gradually merged along the hierarchy of the network, and
use DINO for self-supervision. Apart from standard imagelevel self-supervision as in DINO, they incorporate additional patch-level self-supervision in which correspondence
is promoted between similar patches within augmented
versions of an image. EsViT demonstrates excellent performance under self-supervision settings, and its off-the-shelf
features transfer better than supervised SwinTransformer on
17 out of 18 evaluated datasets.
Transformers for Object Detection
Transformers based modules have been used for object
detection in the following manner: (a) Transformer backbones for feature extraction, with a R-CNN based head
for detection (see Sec. 3.2.2), (b) CNN backbone for visual
features and a Transformer based decoder for object detection , , , (see Sec. 3.3.1, and (c) a purely
transformer based design for end-to-end object detection
 (see Sec. 3.3.2).
Detection Transformers with CNN Backbone
Detection Transformer (DETR) treats object detection
as a set prediction task i.e., given a set of image features,
the objective is to predict the set of object bounding boxes.
The Transformer model enables the prediction of a set of
objects (in a single shot) and also allows modeling their
relationships. DETR adapts a set loss function which allows
Fig. 7: Detection Transformer (DETR) treats the object
detection task as a set prediction problem and uses the Transformer network to encode relationships between set elements.
A bipartite set loss is used to uniquely match the box predictions with the ground-truth boxes (shown on the right two
columns). In case of no match, a ’no object’ class prediction
is selected. Its simple design with minimal problem-speciﬁc
modiﬁcations can beat a carefully built and popular Faster R-
CNN model. Figure from .
bipartite matching between predictions and ground-truth
boxes. The main advantage of DETR is that it removes
the dependence on hand-crafted modules and operations,
such as the RPN (region proposal network) and NMS (nonmaximal suppression) commonly used in object detection
 – . In this manner, the dependence on prior knowledge and careful engineering design is relaxed for complex
structured tasks like object detection.
Given spatial feature maps from the CNN backbone, the
encoder ﬁrst ﬂattens the spatial dimensions (see Fig. 7). This
gives a sequence of features d × n, where d is the feature
dimension and n = h × w with h, w being the height
and width of the spatial feature maps. These features are
then encoded and decoded using multi-head self-attention
modules as in . The main difference in the decoding
stage is that all boxes are predicted in parallel while 
uses an RNN to predict sequence elements one by one.
Since the encoder and decoder are permutation invariant,
learned positional encodings are used as the object queries
by the decoder to generate different boxes. Note that the
spatial structure in a CNN detector (e.g., Faster R-CNN)
automatically encodes the positional information. DETR
obtains performance comparable to the popular Faster R-
CNN model which is an impressive feat given its
simple design. The DETR has also been extended to interesting applications in other domains, e.g., Cell-DETR 
extends it for instance segmentation of biological cells. A
dedicated attention branch is added to obtain instance-wise
segmentations in addition box predictions that are enhanced
with a CNN decoder to generate accurate instance masks.
The DETR model successfully combines convolutional networks with Transformers to remove handcrafted design requirements and achieves an end-to-end
trainable object detection pipeline. However, it struggles
to detect small objects and suffers from slow convergence
and a relatively high computational cost . DETR maps
images to features space before using the Transformer for
the relation modeling. Thus, the computational cost of selfattention grows quadratically with the spatial size of the
feature map i.e., O(H2W 2C), where H and W represent
the height and width of the feature map. This inherently
puts a limitation on the use of multi-scale hierarchical
features in DETR training framework which is ultimately important to detect small objects. Furthermore, at the
beginning of training, the attention module simply projects
uniform attention to all the locations of the feature map and
Fig. 8: Axial attention module that sequentially applies
multi-head axial attention operations along height and width
axes. Image from .
requires a large number of training epochs to tune attention
weights to converge to meaningfully sparse locations. This
approach contributes to a slow convergence rate of DETR.
To mitigate the above-mentioned issues, proposed a
deformable attention module to process the feature maps.
Inspired from deformable convolutions , deformable
attention module only attends to sparse set of elements
from the whole feature map regardless of its spatial size.
This further allows cross-scale aggregation of feature maps
with the help of multi-scale attention modules without
increasing the computational cost signiﬁcantly. Deformable
DETR not only performs better but its training time also
remains 10× lower than the original DETR model .
Anchor DETR replaces the learnable query tokens in
 with anchor-point based queries, such that each query
focuses on predicting the object near the anchor point. The
anchor points can be ﬁxed on 2D grid, or learned from
uniformly distributed points. Anchor DETR requires
10 × fewer training epochs with comparable performance.
Pix2Seq is a generic Transformer-based framework,
without any specialized task-speciﬁc modules, and learns
to directly produce a sequence of tokens with object descriptions (bounding-boxes and class-labels). A quantization
and serialization scheme ﬁrst converts bounding boxes and
class-labels into a sequence of discrete tokens. A generic
Transformer based encoder-decoder network is then used
to generate these tokens in an auto-regressive manner conditioned on previous predictions and image features.
Detection with Pure Transformers
You Only Look at One Sequence (YOLOS) is a simple, attention-only architecture directly built upon the ViT
 , . It replaces the class-token in ViT with multiple
learnable object query tokens, and the bipartite matching
loss is used for object detection similar to . YOLOS
demonstrates the ﬂexibility of ViTs to object detection, in a
pure sequence-to-sequence learning manner, with minimal
image related 2D inductive biases. In similar spirit, PVT 
is combined with DETR to perform object detection
with an end-to-end transformer pipeline. We note that it is
feasible to combine other recent ViTs with transformer based
detection heads as well to create pure ViT based designs
 , and we hope to see more such efforts in future.
Transformers for Segmentation
Self-attention can be leveraged for dense prediction tasks
like image segmentation that requires modeling rich interactions between pixels. Below, we discuss axial self-attention
 , a cross-modal approach that can segment regions
corresponding to a given language expression, and ViTs
based segmentation architectures , , .
Panoptic segmentation aims to jointly solve the
otherwise distinct tasks of semantic segmentation and instance segmentation by assigning each pixel a semantic label
and an instance id. Global context can provide useful cues
to deal with such a complex visual understanding task.
Self-attention is effective at modeling long-range contextual
information, albeit applying it to large inputs for a dense
prediction task like panoptic segmentation is prohibitively
expensive. A naive solution is to apply self-attention either
to downsampled inputs or to limited regions around each
pixel . Even after introducing these constraints, the selfattention still has quadratic complexity and sacriﬁces the
global context. To tackle these issues, Wang et al. 
propose the position-sensitive axial-attention where the 2D
self-attention mechanism is reformulated as two 1D axialattention layers, applied to height-axis and width-axis sequentially (see Fig. 8). The axial-attention is compute efﬁcient and enables models to capture the full-image context.
It achieves competitive performance for the panoptic segmentation task on COCO , Mapillary Vistas , and
Cityscapes benchmarks and for the image classiﬁcation
on ImageNet dataset .
Cross-modal Self-attention (CMSA) encodes longrange multi-modal dependencies between linguistic and
visual features for referring image segmentation task, that aims
to segment entities in an image referred by a language
description.For this purpose, a set of cross-modal features is
obtained by concatenating image features with each word
embedding and the spatial coordinate features. The selfattention operates on these features and generates attention
over the image corresponding to each word in the sentence.
The segmentation network then performs self-attention at
multiple spatial levels and uses a gated multi-level fusion
module to reﬁne segmentation masks via information exchange across multi-resolution features. A binary CE loss is
used to train the overall model that achieves good improvements on UNC , G-Ref and ReferIt datasets.
While the segmentation approaches discussed above insert self-attention in their CNN based architectures, some
recent works have proposed transformer based encoderdecoder architectures. Segmentation Transformer (SETR)
 has a ViT encoder, and two decoder designs based
upon progressive upsampling, and multi-level feature aggregation. SegFormer has a hierarchical pyramid ViT
 (without position encoding) as an encoder, and a simple
MLP based decoder with upsampling operation to get the
segmentation mask. Segmenter uses ViT encoder to
extract image features, and the decoder is a mask Transformer module which predicts segmentation masks, using
learnable mask tokens and image-patch tokens as inputs.
The authors also propose a baseline linear decoder which
projects the patch-embeddings to classiﬁcation space, thus
producing coarse patch-level labels.
Transformers for Image and Scene Generation
Here, we discuss Transformer-based architectures ,
 – for image synthesis, which is interesting from
Fig. 9: (a) Self-attention block in Image Transformer .
Given one channel for a pixel q, the block attends to the memory of previous synthesized pixels (mi), followed by a feedforward sub-network. Positional encodings pi are added in the
ﬁrst layer. (b) The operation performed in Local Self-Attention
(example of a 2D case is shown). The image is partitioned into
a grid of spatial blocks known as query blocks. In the selfattention operation, each pixel in a query block attends to all
pixels in the memory block (shown in cyan rectangle). White
grid locations show masked inputs that have zero-contribution
towards the self-attention.
the perspective of generative modeling and learning unsupervised representations for down-stream tasks.
Parmar et al. develop an image generation model
that can sequentially predict each pixel of an output image
given its previously generated pixels (Fig. 9). Their approach
models the joint distribution of the image pixels by factorizing it as a product of pixel-wise conditional distributions.
Previously developed auto-regressive models for this task,
such as the PixelCNN , suffer from a limited receptive
ﬁeld which hinders in modeling long term relationships in
an image e.g., part relationships or occlusions. Using selfattention, enhances the receptive ﬁeld without incurring a high computational cost (e.g., effective receptive ﬁeld
up to 256 pixels can be achieved as compared to 25 pixels
of PixelCNN ). The generative pipeline was also tested
on conditional generation tasks e.g., image super-resolution,
image completion, and denoising.
Inspired by the success of GPT model in the language domain, image GPT (iGPT) demonstrated that
such models can be directly used for image generation
tasks, and to learn strong features for downstream vision
tasks (e.g., image classiﬁcation). Speciﬁcally, iGPT trains
GPT v2 model on ﬂattened image sequences (1D pixel
arrays) and shows that it can generate plausible image
outputs without any external supervision. The generated
samples depict the model’s ability to understand spatial
relationships between pixels and high-level attributes such
as object classes, texture, and scale. Notably, the design
does not use any image-speciﬁc knowledge in the design
(e.g., the 2D position embeddings used in Image Transformer ). The features learned with iGPT’s unsupervised training mechanism compete impressively against
other unsupervised approaches, achieving state-of-the-art
performance on CIFAR-10/100 and STL datasets
while performing comparably to SimCLR (a contrastive
learning approach) on ImageNet dataset. This is an
astounding result, since the iGPT architecture is exactly the
same as used for language modeling tasks, and therefore it
does not incorporate any prior domain-speciﬁc knowledge.
Notably, the competing unsupervised CNN based solutions
widely adopt such priors in the form of architectural design,
attention mechanisms, loss functions, and regularization
 , – . However, on the downside, iGPT has a
high compute cost e.g., iGPT-L version has roughly 36×
high training cost compared to MoCo which is a
state of the art self-supervised feature learning approach.
For this reason, the training was generally limited to lowresolution of ≤64 × 64, while convolutional architectures
can effectively learn from high-resolution inputs.
Transformers typically incur a high compute cost when
applied on high-dimensional sequences. To overcome this
limitation, Esser et al. proposed to include inductive
biases (commonly used in the CNNs) alongside Transformers to improve their efﬁciency. Speciﬁcally, local connectivity
and spatial invariance biases inbuilt in the CNN structure
are leveraged by learning a rich dictionary of visual patterns
(using a Generative Adversarial approach). A Transformer
is then used to learn the long-range interactions between
the dictionary items to generate the outputs. In turn, they
develop a conditional image generation model capable of
producing very high-resolution images (up to megapixel
range) using Transformers. This is the ﬁrst work that
demonstrates the application of Transformers to generate
such high-resolution images.
Generative Adversarial Networks (GANs) with
CNNs as default backbone have been very successful for
visually appealing image synthesis – . TransGAN
 builds a strong GAN model, free of any convolution
operation, with both generator and discriminator based
upon the Transformer model . The architecture of both
generator and discriminator is based upon the encoder in
original Transformer model . For memory efﬁciency, the
generator contains multiple stages, with up-sampling modules in-between, which gradually increase the resolution
of feature maps (input sequence length) while reducing
the embedding dimension. The discriminator of TransGAN
takes ﬂattened image-patches as tokens similar to .
Authors introduce different training techniques including
data augmentation, training with an auxiliary task and
injecting locality to self-attention to scale-up their model
for high quality image synthesis . The TransGAN
model achieves state-of-the-art results in terms of Inception
Score and Fr´echet Inception Distance (FID) on STL-10 and
performs favorably compared with their CNN-based GAN
counterparts on other datasets.
Unlike previous image generation methods – ,
which directly predict image outputs, learns to generate
parameters of 3D objects to be placed in a given scene.
Speciﬁcally, SceneFormer studies the 3D room layout
conditioned scene generation task. Given the empty room
shape, can propose new object conﬁgurations in the
room while maintaining realism. Remarkably, the model
does not use any appearance information and only learns to
generate new scenes by modeling the inter-object relationships using self-attention in Transformers. Similar to how
a Transformer operates on a sentence, it is applied to a
sequence of objects to predict the next suitable object in a
scene. Speciﬁcally, the size, pose, location, and category of
the next object is predicted by the Transformer model. A
start token indicates the initiation of inference and the number of output token indicate the objects generated by the
model in a sequence. The authors also explore generating
new scenes given a textual description of the room layout.
The independence from the appearance makes the approach
efﬁcient, enabling interactive scene generation.
The task of generating realistic images from text is interesting and practically valuable (e.g., for artistic content creation), but at the same time highly challenging. Prior text-toimage synthesis approaches – are mostly based on
GANs . Although these methods produce encouraging
results, they are far from being photo-realistic. Ramesh et
al. recently proposed DALL·E which is a Transformer
model capable of generating high-ﬁdelity images from a
given text description. DALL·E model has 12 billion parameters and it is trained on a large set of text-image pairs taken
from the internet. Before training, images are ﬁrst resized
to 256×256 resolution, and subsequently compressed to
a 32×32 grid of latent codes using a pre-trained discrete
variational autoencoder , . DALL·E takes as input
a single stream of 1280 tokens (256 for the text and 1024
for the image), and is trained to generate all other tokens
autoregressively (one after another). It provides ﬂexibility
to generate images either from scratch (Fig. 10a) or by
extending existing images (Fig. 10b), while staying faithful
to the text caption.
The authors demonstrate the effectiveness of DALL·E by
creating images from text describing a wide variety of real
and ﬁctional concepts. While generating images purely from
textural captions, DALL·E shows impressive performance at
controlling multiple objects and their attributes (Fig. 10c),
rendering certain viewpoint (Fig. 10d), capturing object’s
internal structure (Fig. 10e), and combining unrelated objects (Fig. 10f). Furthermore, DALL·E can perform image-toimage translation (Fig. 10g) guided by the input text.
Transformers for Low-level Vision
After witnessing the success of Transformer models in highlevel vision problems, numerous Transformer-based methods have been proposed for low-level vision tasks, including
image super-resolution , , , denoising , ,
deraining , , and colorization . Image restoration
requires pixel-to-pixel correspondence from the input to the
output images. One major goal of restoration algorithms
is to preserve desired ﬁne image details (such as edges
and texture) in the restored images. CNNs achieve this by
employing a single-scale architecture design that does not
involve any downsampling operation. Since the computational complexity of self-attention in Transformer models
increases quadratically with number of image patches, it is
Fig. 10: Images generated by DALL·E from the following text prompts. (a) An armchair in the shape of an avocado. (b) A photo
of San Francisco’s golden gate bridge. Given a part of the image (in green box), DALL·E performs the image completion. (c) An emoji
of a baby penguin wearing a blue hat, red gloves, green shirt, and yellow pants. (d) An extreme close-up view of a capybara sitting in a ﬁeld.
(e) A cross-section view of a pomegranate. (f) A penguin made of watermelon. (g) The exact same cat on the top as a sketch on the bottom.
infeasible to develop Transformer model that can operate on
single-scale feature processing pipeline. Consequently, these
Transformer-based image restoration models make use of
various strategies to reduce the computational burden, such
as computing attention on local image windows , performing spatial reduction attention , and employing
encoder-decoder design , . Here, we brieﬂy discuss
a few image restoration Transformer models.
Transformers for Image Processing Tasks
Top performing algorithms for high-level computer vision
tasks such as object detection and semantic segmentation
often employ backbone models that are pre-trained on largescale datasets e.g., ImageNet. In contrast, algorithms for lowlevel vision tasks such as image denoising, super-resolution,
and deraining are directly trained on task-speciﬁc data,
thereby suffer from these limitations: (i) small number of images available in task-speciﬁc datasets , (ii) the model trained for one image
processing task does not adapt well to other related tasks.
Chen et al. propose a pre-trained model based on
Transformer architecture, named as Image Processing Transformer (IPT). It is capable of performing various image
restoration tasks such as super-resolution, denoising, and
deraining. The overall architecture of IPT consists of multiheads and multi-tails to deal with different tasks separately,
and a shared encoder-decoder Transformer body. Since exploiting Transformers at full potential requires training on
large-scale data, takes the clean (ground-truth) images
from the ImageNet benchmark and synthesize their degraded versions for different tasks. For example, bicubic interpolation is used for generating low-resolution images, additive white Gaussian noise is added to prepare noisy data,
and hand-crafted rain streaks are applied to obtain rainy
images. In total, 10 million images are used to pre-train the
IPT model. During training, each task-speciﬁc head takes as
input a degraded image and generates visual features. These
feature maps are divided into small crops and subsequently
ﬂattened before feeding them to the Transformer encoder
(whose architecture is the same as ). The outputs of the
encoder along with the task-speciﬁc embeddings are given
as input to the Transformer decoder. The features from the
decoder output are reshaped and passed to the multi-tail
that yields restored images. The IPT model is optimized
with L1 loss. Experimental results show that the pre-trained
IPT model, when ﬁne-tuned for a speciﬁc low-level vision
task, can provide signiﬁcant performance gains over the
state-of-the-art methods – .
Transformers for Super-Resolution
Recent years have seen major performance breakthroughs
for super-resolution (SR) due to convolutional neural networks (CNNs). Principally, the quality of super-resolved
images generated by CNNs is dependent on the choice of
optimization objective. While the SR methods , –
 that are based on pixel-wise loss functions (e.g., L1,
MSE, etc.) yield impressive results in terms of image ﬁdelity metrics such as PSNR and SSIM, they struggle to
recover ﬁne texture details and often produce images that
are overly-smooth and perceptually less pleasant. Further,
perceptual SR approaches , – , in addition to
per-pixel loss, employ adversarial loss and perceptual
loss based on deep features extracted from pre-trained
CNNs. While these methods generate images that are sharp,
visually pleasant, and perceptually plausible, they show a
substantial decrease in reconstruction accuracy measured in
PSNR/SSIM. Moreover, the perceptual SR algorithms have a
tendency to hallucinate fake textures and cause artifacts. The
above mentioned SR approaches follow two distinct (but
conﬂicting) research directions: one maximizing the reconstruction accuracy and the other maximizing the perceptual
quality, but never both.
To alleviate the trade-off between perceptual reproduction and accurate reproduction, Yang et al. propose a
Transformer network (TTSR) for super-resolution. During
training, TTSR uses paired LR-HR images, as well as reference (Ref) images with similar content as of LR images.
TTSR learns to search relevant regions in the Ref image and
transfers rich textures to help super-resolving the input LR
image. The texture Transformer module of TTSR method
(see Fig. 11) consists of four core components: (1) Learnable
texture extractor: takes as input LR↑, Ref↓↑, and Ref images,
and generates texture features query (Q), key (K), and value
(V), respectively. Here, ↑denotes bicubic upsampling operation, and ↓↑represents bicubic down-sampling followed by
an upsampling operation. (2) Relevance embedding: ﬁrst unfolds Q and K into patches and then computes the similarity
of each patch in Q with each patch in K in order to generate
hard and soft attention maps. (3) Hard-attention: transfers
HR texture features from V to (LR features) Q using the hard
attention map. (4) Soft-attention: further enhances relevant
features while suppressing less relevant ones.
While TTSR method deals with reference-based
image super-resolution, most of the research is conducted
Fig. 11: Diagram of the texture Transformer module. Q (query),
K (key) and V (value) represent texture features extracted from
a (bicubic upsampled) low-resolution image, a sequentially
down/upsampled reference image, and an original reference
image, respectively. The relevance embedding aims to estimate
similarity between low-resolution and reference images. H
and S respectively denote hard and soft attentions computed
from relevance embedding. T indicates high-resolution texture
features that are then transferred to the features F of lowresolution image. Figure is from .
on single image super-resolution problem in which only
LR-HR paired images are available. Since the computational complexity of the original self-attention operation
is prohibitively high for high-resolution images, recently
a few efﬁcient transformer models have been proposed
that employ window-based attention (SwinIR ) and
spatial resolution reduction operation in attention module
(ESRT ) to perform super-resolution.
Colorization Transformer
Given a grayscale image, colorization seeks to produce the
corresponding colorized sample. It is a one-to-many task as
for a given grayscale input, there exist many possibilities
in the colorized output space. The challenging nature of
this task requires probabilistic models capable of producing multiple colorized output samples. Colorization Transformer is a probabilistic model based on conditional
attention mechanism . It divides the image colorization
task into three sub-problems and proposes to solve each
task sequentially by a different Transformer network. The
authors ﬁrst train a Transformer network to map a lowresolution grey-scale image to a 3-bit low-resolution colored image. Low-resolution images in turn allow training
of larger models. The 3-bit low-resolution colored image
is then upsampled to an 8-bit RGB sample by another
Transformer network in the second stage of training. Finally,
a third stage Transformer is trained to increase the spatial
resolution of the 8-bit RGB sample produced by the secondstage Transformer. Self-attention used in the colorization
Transformer is based on row/column attention layers introduced in . These layers capture the interaction between
each pixel of an input image while being computationally less costly. The row-wise attention layer applies selfattention to all pixels in a given row, while the column-wise
attention layer considers pixels only in a given column of
an image. This work is the ﬁrst successful application
of Transformers trained to colorize grey-scale images at high
(256×256) resolution.
Transformers for Multi-Modal Tasks
Transformer models have also been extensively used for
vision-language tasks such as visual question answering
(VQA) , visual commonsense reasoning (VSR) ,
cross-modal retrieval and image captioning . Several works in this direction target effective vision-language
pre-training (VLP) on large-scale multi-modal datasets to
learn generic representations that effectively encode crossmodality relationships (e.g., grounding semantic attributes
of a person in a given image). These representations can
then be transferred to downstream tasks, often obtaining
state of the art results. Notably, several of these models
still use CNNs as vision backbone to extract visual features
while Transformers are used mainly used to encode text
followed by the fusion of language and visual features.
Such models generally apply the vanilla multi-layer Transformer with multi-modal inputs and do not introduce
fundamental changes to the core attention block. However,
their main distinction is in the conﬁguration of Transformers
and the loss functions, based on which we categorize them
into: (a) Multi-stream Transformers (see Sec. 3.7.1) and (b)
Single-stream Transformers (see Sec. 3.7.2). The single-stream
designs feed the multi-modal inputs to a single Transformer
while the multi-stream designs ﬁrst use independent Transformers for each modality and later learn cross-modal representations using another Transformer (see Fig. 12). Besides
these vision language pretraining methods, we also explain
visual grounding approaches towards the end of this section
(see Sec. 3.7.3).
Multi-stream Transformers
Vision and Language BERT (ViLBERT) was the ﬁrst
extension of the BERT model to the multi-modal domain.
The goal was to learn representations that can jointly model
images and natural language. For this purpose, ViLBERT
developed a two-stream architecture where each stream is
dedicated to model the vision or language inputs (Fig. 12-h).
The architecture of both parallel streams is a series of Transformer blocks similar to the BERT model. Subsequently, coattentional Transformer layers are applied to learn crossmodal relationships. The co-attentional framework is very
simple. Query, key, and value matrices are computed for
each modality in the standard way and then key-value
pairs for one modality are passed on to the other modality’s
attention head.
ViLBERT applies VLP on a set of proxy tasks deﬁned on
the Conceptual Concepts dataset (with 3.3M images with
weak captions) and later ﬁne-tune the model on downstream tasks such as VQA. The pre-training phase operates in a self-supervised manner, i.e., pretext tasks are created without manual labeling on the large-scale unlabelled
dataset. These pretext tasks include predicting whether the
text and image inputs are related and predicting the semantics of masked image regions and textual inputs (e.g., similar
Fig. 12: An overview of Transformer models used for multi-modal tasks in computer vision. The Transformer designs in this
category can be grouped into single-stream (UNITER , OSCAR , VideoBERT , Unicoder-VL , VisualBERT and
VL-BERT ) and dual-stream architectures (LXMERT , ViLBERT and PEMT ). A key distinction between models
is the choice of loss functions. While most of the multi-modal methods are focused on images as visual data, VideoBERT and
PEMT are designed to work on video streams and leverage unique modalities e.g., audio signals in videos .
to reconstructing masked words in text in the BERT model
 ). This way, the model learns the inherent structure in
the data during pre-training and also models cross-domain
associations. With evaluations on several tasks, demonstrated that a two-stream model can perform better than a
single-stream model that uses shared parameters to model
both language and vision domains .
Similar to ViLBERT , Learning Cross-Modality Encoder Representations from Transformers (LXMERT) 
also uses a two-stream architecture based on BERT framework. The main difference lies in the object-relationship
encoder that is used to model the visual features instead
of simple image-level features used in ViLBERT. The information in two streams is then fused across modalities using
cross-attention blocks similar to .
Compared to two pre-texts tasks used for VLP in ,
LXMERT uses ﬁve pre-training tasks including masked object and language prediction, cross-modality matching, and
visual question answering (Fig. 12-g). The pre-trained model
is ﬁne-tuned on the VQA task, however, a high similarity
between pre-training and ﬁne-tuned tasks raises questions
on the generalizability of the learned representations to new
tasks. To this end, the authors conducted generalization
experiments on Visual Reasoning for Real (NLVR) task 
demonstrating impressive improvements on novel tasks.
Lee et al. note that the multi-modal representation
learning approaches like VideoBERT and ViLBERT 
generally keep the language processing part ﬁxed to a pretrained model (e.g., BERT ) to reduce training complexity. For the ﬁrst time in the literature, they propose to
learn an end-to-end multi-modal bidirectional Transformer
model called PEMT on audio-visual data from unlabeled
videos. First, short-term (e.g., 1-3 seconds) video dynamics
are encoded using CNNs, followed by a modality-speciﬁc
Transformer (audio/visual) to model long-term dependencies (e.g., 30 seconds). A multi-modal Transformer is then
applied to the modality-speciﬁc Transformer outputs to exchange information across visual-linguistic domains. However, learning such a model in a naive form would incur
huge memory requirements. To reduce parametric complexity, the parameters are shared across layers within each
Transformer which leads upto 80% parameter reduction.
The Transformer is trained using a contrastive learning approach based on a content-aware negative sampling (Fig. 12i). Speciﬁcally, the model uses the features obtained from
CNNs learned during the training phase to select negative
samples that are visually similar to the positive instances.
This work also compares various fusion strategies adopted
in earlier works such as early (VideoBERT and VL-
BERT ), mid-level (ViL-BERT and LXMERT )
and late fusion mechanisms and shows that the mid-level
fusion is the optimal choice. The proposed model is pretrained on Kinetics-700 dataset and later ﬁne-tuned on
downstream video classiﬁcation tasks such as short video
classiﬁcation on UCF101 , audio classiﬁcation on ESC50
 and long-term action recognition on Charades 
and Kinetics-Sounds datasets.
Tan and Bansal introduce the concept of ‘vokens’
(images related to language tokens extracted from sentences). The vokens (visualized tokens) provide visual supervision to the language model to learn better features. The
motivation is that humans learn languages by correlating
visual information with semantic concepts. In a similar spirit
to other self-supervised language representation learning
methods , , they learn representations by deﬁning
an auxiliary task of voken-prediction task. Since the existing datasets encode limited visually grounded tokens, they
propose a vokenization method to map language tokens to
visual vokens, as illustrated in Fig. 13. The approach uses
language-based retrieval for such a mapping and transfers
a model trained on a small labeled dataset (MS-COCO) to a
large dataset (Wikipedia). Furthermore, it was ensured that
the sentence-wide context is considered to obtain the tokenvoken mapping. The resulting model trained using generated tokens outperforms the state of the art BERT model on
a diverse set of NLP tasks. In this sense, the proposed model
does not evaluate vision tasks, however, uses vision as a
useful grounding cue to train the language model, hence we
include it in the multi-modal representation learning group.
Vision-and-Language Navigation (VLN) aims to predict
a navigation plan on a map based on the vision and
language inputs. Transformer models were used earlier in
 , for VLN task. These works ﬁrst pre-train a crossmodal Transformer using self-supervision on vision and
language pairs and subsequently ﬁne-tune on the speciﬁc
VLN tasks. While these works learn attention between image region and language, Chen et al. propose to learn
cross-modal attention between language inputs and spatial
topological maps (to represent an agent’s environment as
a graph whose nodes denote places and the edges denote
their connectivity). Given the topological map and natural
language inputs, a VLN task using the Transformer model
bears resemblance to sequence prediction in NLP. Specifically, at each time instance, the cross-modal Transformer
predicts a single node of the topological map in the navigation plan. The individual language and map encodings
are ﬁrst processed using uni-modal encoders and later a
cross-modal encoder (similar to LXMERT ) is applied
to aggregate information across modalities. To denote positions in the map, a learned trajectory position encoding is
appended with the map features. Based on this Transformer
setup, reports a full navigation system that can freely
explore the environment and intelligently plan its actions.
CLIP is a contrastive approach to learn image representations from text, with a learning objective which maximizes similarity of correct text-image pairs embeddings in
a large batch size. Speciﬁcally, given a batch of N imagetext pairs, CLIP learns a multi-modal embedding space, by
jointly training an image-encoder and a text-encoder, such
that the cosine similarity of the valid N image-text pairs is
maximized, while the remaining N 2−N pairs is minimized.
The authors consider ResNet-50 and Vision Transformer
(ViT) for encoding images. The modiﬁed Transformer
model as in is employed for encoding text. CLIP is
trained on a large corpus of 400 million image-text pairs and
demonstrates excellent zero-shot transfer capabilities. At
inference, the names of classes are used as input to the textencoder, and similarity of the encoded image is computed
with all encoded texts (classes) to ﬁnd the image-text pair
with highest match. The CLIP achieves an astounding zeroshot classiﬁcation accuracy of 75% on ImageNet, without using an supervision from ImageNet training set. The authors
further demonstrate zero-shot transfer capabilities of the
CLIP model on 30 different computer vision benchmarks.
Note that CLIP with ResNet took 18 days to train on 592
V100 GPUs while CLIP with ViT took 12 days on 256 V100
GPUs. This highlights the computational cost of CLIP.
Single-stream Transformers
Different from two-stream networks like ViLBERT 
and LXMERT , VisualBERT uses a single stack of
Transformers to model both the domains (images and text).
The input sequence of text (e.g., caption) and the visual
features corresponding to the object proposals are fed to
the Transformer that automatically discovers relations between the two domains. Notably, VisualBERT architecture is
somewhat similar to VideoBERT (explained in Sec. 3.8),
but instead of only focusing on cooking videos, Visual-
BERT evaluates on various visual-linguistic tasks (e.g., VCR,
NLVR, VQA, and visual grounding). The VisualBERT model
ﬁrst applies task-agnostic pre-training using two objectives
(Fig. 12-e). The ﬁrst objective simply attempts to predict
missing text tokens using the image features and remaining
textual tokens. The second objective attempts to differentiate
between the true and false caption of a given image. After
task-agnostic pre-training, the authors propose to perform
task-speciﬁc pre-training to bridge the domain gap before
the ﬁnal ﬁne-tuning to the downstream task.
Su et al. propose a multi-modal pre-training approach to learn features that are generalizable to multimodal downstream tasks such as Visual Commonsense
Reasoning and Visual Question Answering. This endeavor
requires adequately aligning the visual and linguistic cues
so that an effective composite representation is learned. To
the end, builds on the BERT model and inputs both
the visual and language features. The language features
correspond to the token in the input sentence and the visual
features correspond to the region of interest (RoI) from
the input image (obtained via a standard Faster R-CNN).
Speciﬁcally, the model is pre-trained on both the visuallingual dataset (Conceptual Captions ) as well as the
language-only datasets (e.g., Wikipedia). The loss function is
identical to BERT, where the model is trained to predict the
masked out words or visual ROIs (Fig. 12-f). In contrary to
other works such as UNITER , VL-BERT claims that the
visual-linguistic matching tasks are not useful during pretraining, which is in contrast to evidence from later efforts
 . Their results on several multi-modal tasks show their
beneﬁt over the language-only pre-training (e.g., in BERT).
Universal Encoder for Vision and Language (Unicoder-
VL) learns multi-modal representations using largescale image-caption pairs. The language and image inputs
are fed to a single Transformer model (with multiple successive encoders) to learn joint embeddings. To this end,
it uses masked word prediction, masked object classiﬁcation, and visual-linguistic matching as self-supervision tasks
during pre-training (Fig. 12-d). Notably, the visual-linguistic
matching is carried out only at the global level (i.e., imagesentence alignment). The model is evaluated on imagetext retrieval, zero-shot learning, and visual commonsense
reasoning where it performs better than the previous models
such as ViLBERT and VisualBERT . This shows
the signiﬁcance of rich self-supervised tasks and advocates
for a uniﬁed Transformer architecture to learn multi-modal
features in a common framework.
The Uniﬁed Vision-Language Pre-training (VLP) 
model uses a single Transformer network for both encoding and decoding stages. This stands in contrast to BERT
inspired VLP models , , , which use independent encoder and decoder networks. Joint modeling
of encoding and decoding stages allows the Uniﬁed VLP
model to perform well for both image captioning and visualquestion answering tasks, when ﬁne-tuned on these individual tasks. The intuition for shared modeling of encoding and
decoding stage stems from the need to better share crosstask information during pre-training. The uniﬁed model
consists of a stack of 12 Transformer blocks, each with a selfattention layer followed by a feed-forward module. The selfsupervised objectives used for pre-training include masked
vision-language predictions. Here, the authors explore two
variants i.e., bidirectional and sequence-to-sequence prediction of masked works where different context encodings are
used for both types of objectives. The proposed approach is
evaluated on COCO Captions, Flick 30K Captions and VQA
2.0 and obtains encouraging results compared to previous
methods on image captioning and VQA .
Universal image-text representation (UNITER) performs pre-training on four large-scale visual-linguistic
datasets (MS-COCO , Visual Genome , Conceptual
Captions and SBU Captions ). The learned representations transfer well on downstream tasks such as VQA,
Multi-modal retrieval, Visual Commonsense reasoning, and
NLVR. In order to emphasize on learning the relationships
between visual and language domains, speciﬁcally designs pre-training tasks to predict masked visual or text
region conditioned on the other domain input, and align
language and visual inputs on both the global (image-text)
and local (word-region) levels (Fig. 12-a). These tasks are
beside the conventional masked language modeling task
used in BERT and explicitly include ﬁne-grained wordregion alignment alongside conditional masking of inputs
that were not considered in the earlier works such as VL-
BERT , Visual-BERT , Vilbert and Unicoder-
VL . Common to the other approaches, they adopt the
Transformer architecture proposed in BERT that operates
on both the visual and language embeddings. In contrast
to applying independent Transformers to the language and
visual inputs (as in ViLBERT and LXMERT ),
Fig. 13: Visualized tokens (Vokens) : A language model
is visually supervised using closely related images that leads
to better feature representations from the pretrained model.
Figure from .
UNITER adopts a single Transformer applied to the textual
and image inputs like , , .
VisualBert , Uniter , VL-BERT , VilBERT ,
and Unicoder-VL models for VLP concatenate image and text features and leave it to the self-attention to
automatically discover cross-modal relationships. This can
complicate the visual grounding of semantic concepts in an
image. To address this problem, Object-Semantics Aligned
Pre-Training (Oscar) ﬁrst uses an object detector to
obtain object tags (labels), which are then subsequently used
as a mechanism to align relevant visual features with the
semantic information (Fig. 12-b). The motivation is that the
textual content generally pertains to major objects in the
image, therefore by explicitly adding those image labels to
the input, visual features can be better attended. Similar to
BERT , Oscar uses a Masked Token Loss for VLP, where
different tokens in the textual input and image tags are randomly masked and the model predicts these missing tokens.
Further, it also uses a contrastive loss that discriminates
between the original and noisy/fake image-tag pairs. The
representations thus learned are ﬁne-tuned on VQA, crossmodality retrieval, natural language reasoning, and image
captioning tasks to obtain better performances compared to
VLP methods that do not use object tags. The recent VinVL
 approach extends Oscar for the object detection task
and learns object instance-centered relationships between
visual and language domains using an adapted pretraining
scheme. The model is trained on a collection of datasets
(MS-COCO, OpenImages, Visual Genome and Objects365)
and was demonstrated to precisely relate semantic attributes
with the visual information and provided better transferability to the downstream visual comprehension tasks.
Transformers for Visual Grounding
Modulated DETR (MDETR) has a CNN and BERT
backbone to extract features from image and text inputs,
respectively. The visual and text features are then separately
linearly projected to a shared space, concatenated and fed to
a transformer model (with an architecture similar to DETR)
to predict the bounding boxes for objects corresponding to
the queries in the grounding text. The model is trained by
using a loss which predicts a uniform distribution over all
relevant text query tokens speciﬁc to the predicted bounding
boxes. An additional contrastive loss term ensures correspondence between visual and text embedding. TransVG
 is a simple design, where visual and text features are
fused together in a transformer module, and the boundingbox corresponding to the query is directly regressed using a learnable token (input to the Transformer module,
along-with visual and text features). Referring Transformer
 is also a simple one stage design where the text
and image features are fused in a Transformer encoder,
and the Transformer based decoder then directly regresses
bounding boxes or segmentation masks. Visual Grounding
with Transformer has an encoder-decoder architecture,
where visual tokens (features extracted from a pretrained
CNN model) and text tokens (parsed through an RNN
module) are processed in parallel with two distinct branches
in the encoder, with cross-modality attention to generate
text-guided visual features. The decoder then computes
attention between the text queries and visual features and
predicts query-speciﬁc bounding boxes.
Video Understanding
Existing approaches for audio-video data analysis generally
learn representations on short-length videos (up to a few
seconds long), that allow them to encode only short-range
dependencies , . Long-range dependency modeling is
desirable in various uni-modal and multi-modal learning
tasks such as activity recognition , , – .
Below, we explain recent approaches that seek to resolve this
challenge using the expressivity of Transformer networks.
It is important to note that several of these works ,
 , , still employ (pretrained) CNNs to encode
image/frame-level features in the videos on top of which
Transformers are applied to model wide context. A few
exceptions include , – which obtain framelevel features also using the ViT based backbones.
Joint Video and Language Modeling
The VideoBERT model leverages Transformer networks
and the strength of self-supervised learning to learn effective multi-modal representations. Speciﬁcally, VideoBERT
uses the prediction of masked visual and linguistic tokens as
a pretext task (Fig. 12-c). This allows modeling high-level semantics and long-range temporal dependencies, important
for video understanding tasks. Given a video, converts
speech to text using off-the-shelf speech recognition systems
and applies vector quantization (clustering) to obtain visual
features from pre-trained video classiﬁcation models. The
BERT model is then directly applied to these concatenated
sequences of language and visual tokens to learn their
joint distribution. The model can be trained with only-text,
video-only, and video+text domains. The resulting model
showcases interesting capabilities for cross-modal predictions such as video generation from a given textual input
(e.g., captions or cooking recipe) and (video-based) future
forecasting. The video+text model uses a visual-linguistic
alignment task to learn cross-modality relationships. The
deﬁnition of this pre-text task is simple, given the latent
state of the [cls] token, the task is to predict whether the
sentence is temporally aligned with the sequence of visual
tokens. Further, the learned representations are shown to be
very useful for downstream tasks such as action classiﬁcation, zero-shot classiﬁcation, and video captioning.
Zhou et al. explore Masked Transformers for dense
video captioning. This requires generating language descriptions for all events occurring in a video. Existing works
on this problem generally operate sequentially i.e., ﬁrst
detect events and then generate captions in separate subblocks. proposes a uniﬁed Transformer network to
tackle both tasks jointly, thereby seamlessly integrating the
multi-modal tasks of event detection and captioning. First, a
video encoder is used to obtain frame-wise representations
followed by two decoder blocks focused on proposing the
video events and the captions. Since untrimmed videos are
considered, a masking network is used in the captioning
decoder to focus on describing a single event proposal.
Remarkably, was the ﬁrst approach to target dense
video captioning using non-recurrent models and used selfattention in the encoder(applied on CNN derived features)
to model broad range context between video frames. Experiments on ActivityNet Captions and YouCookII
 datasets showed good improvements over previous
recurrent network and two-stage based approaches.
Video Action Recognition
The traditional CNN based methods in video classiﬁcation
generally perform 3D spatio-temporal processing over limited intervals to understand videos. Neimark et al. 
propose Video Transformer Network (VTN) that ﬁrst obtains frame-wise features using 2D CNN and apply a Transformer encoder (Longformer ) on top to learn temporal
relationships. Longformer is an attractive choice to process
long sequences (with an arbitrary length n) due to its O(n)
complexity. The classiﬁcation token is passed through a
fully connected layer to recognize actions or events. The
advantage of using Transformer encoder on top of spatial
features is two fold: (a) it allows processing a complete video
in a single pass, and (b) considerably improves training and
inference efﬁciency by avoiding the expensive 3D convolutions. This makes VTN particularly suitable for modeling
long videos where interactions between entities are spread
throughout the video length. Their experiments on Kinetics-
400 dataset with various backbones (ResNet , ViT
 and DeiT ) shows competitive performance.
Girdhar et al. use a variant of Transformer architecture to aggregate person-speciﬁc contextual cues in a
video for action classiﬁcation and localization. Initially, the
model uses a Faster-RCNN style processing where a
backbone model generates features that are forwarded to the
Region Proposal Network to obtain object proposals. Then
RoI pooling is applied to generate object-speciﬁc features.
Multi-head self-attention is then applied on top of the
object features as a cascade of self-attention layers. In each
Transformer unit, a particular person feature is treated as
the ‘query’ (Q), while the features from the neighboring
video clip are used as ‘key’ (K) and ‘value’ (V). The location
information is explicitly encoded in the input feature map
from which K, V and Q are derived, thus incorporating
the positional information in the self-attention. For a given
400×400×64 video clip, the key and value tensors are of size
16×25×25×128, while the query is 128 dimensional vector.
Although uses only RGB stream, additional modalities
like optical ﬂow and audio signal (as in competing works)
would further increase the compute complexity. Further, the
Transformer model was found to be sub-optimal for action
localization, perhaps due to its tendency to incorporate
global information. Therefore, it is important to achieve
the right trade-off between the global and local context
for problems that demand precise delineation (e.g., action
localization and segmentation).
Human action recognition based on skeleton representation requires understanding relationships between different
joints of a body in a given frame as well as between different
frames of a video. Plizzari et al. proposed a two-stream
Transformer network to model such relationships. They
introduced spatial self-attention (SSA) to model relations
between different body-joints (Fig. 14a) while temporal selfattention (TSA) to capture long-range inter-frame dependencies (Fig. 14b). They ﬁrst used a small residual network
to extract features from skeleton data and then used SSA
and TSA modules to process those feature maps. SSA ﬁnds
the correlation between each pair of joints independently,
while TSA focuses on how features of a certain joint change
between frames along the temporal dimension. The purpose
of SSA is to discover relationships among the surrounding
joints in the same way as the Transformer relates different
words in a phrase. On the other hand, TSA ﬁnds long-range
relations between frames, similar to how relations among
phrases are built in NLP. The two streamed model achieves
state-of-the-art results on NTU-RGB+D 60 and NTU-
RGB+D 120 datasets.
Multiscale Vision Transformers (MViT) build a
feature hierarchy by progressively expanding the channel
capacity and reducing the spatio-temporal resolution in
videos. They introduce multi-head pooling attention to
gradually change the visual resolution in their pyramid
structure. TimeSFormer extends ViTs to videos,
by considering the video as a sequence of patches extracted from individual frames. To capture spatio-temporal
relationships, they propose divided attention i.e., spatial
and temporal attentions are separately applied within each
block. TimeSFormer demonstrates SoTA performance on
action recognition, and can be applied to clips over one
minute. Another notable pure-transformer based model is
the Video Vision Transformer (ViViT) . First, the spatiotemporal tokens are extracted and then efﬁcient factorised
versions of self-attention are applied to encode relationships
between tokens. However, they require initialization with
image-pretrained models to effectively learn the ViT models.
There has also been concurrent work on learning sound
pretrained models using self-supervised learning with ViTs.
An important recent effort is the long-short contrastive
learning (LSTCL) framework , which reconstructs representations from different time-scales (narrow and broad)
as auxiliary learning tasks and demonstrates good downstream performance.
Video Instance Segmentation
The Video Instance Segmentation Transformer (VisTR) 
model extends DETR for video object instance segmentation (VIS) task. Local features are obtained using a
(a) Spatial Self-Attention
(b) Temporal Self-Attention
Fig. 14: Spatial/Temporal Attention for Skeleton Data Representations. Relationships between body-joints and inter-frame
dependencies are modeled using two dedicated self-attention
modules. Figure is from .
backbone CNN on a collection of video frames. An encoder
and a decoder Transformer is used similar to DETR to
frame the instance segmentation problem as a sequence to
sequence prediction task. The input frame-level features are
concatenated to form clip representations and the Transformer outputs instance predictions in a order that is consistent across frames. This integrates the object detection and
tracking with-in a single uniﬁed architecture. The predicted
outputs are matched with the ground-truth using bipartitie
matching. Similar to Mask R-CNN , a separate head is
used to predict the instance mask based on self-attention
and 3D convolutions. The overall results are competitive
among the single model approaches on YouTube VIS dataset
 , but performs somewhat lower compared to more
complex CNN-based models such as MaskProp .
Transformers in Low-shot Learning
In the few-shot learning settings, a support set is provided
at the inference to adapt to a novel set of categories. Transformer models have been used to learn set-to-set mappings
on this support set or learn the spatial relationships
between a given input query and support set samples .
In terms of absolute performance, the patch-wise spatial
self-attention between query and support set images excels
compared to an image level association learned in .
However, the patch-wise attention computation is computationally expensive. We elaborate on these approaches below.
Doersch et al. explore the utility of self-supervision
and Transformer model for few-shot ﬁne-grained classiﬁcation, where distribution mismatch exists between training
Fig. 15: An overview of FEAT . Compared to the conventional instance embedding methods in FSL that keep the
embedding function same for all tasks (a), FEAT uses a setto-set function to adapt the embedding function to each FSL
task (b). It evaluates several set-to-set functions and found the
Transformer module to be the most suitable choice for FSL.
Figure from .
and evaluation phases. They develop Cross-Transformer
model to relate a given query image with the few-examples
available in the support set. To this end, the Transformer
ﬁnds spatially similar regions in the query and support
set images, and the corresponding features are then used
to obtain class decisions for the query. The queries in the
Transformer architecture are derived from the grid features
obtained using the query image. Similarly, grid features
from the support images are used to construct keys and
values which are in turn used to derive attended outputs.
This approach, besides a contrastive self-supervision based
training mechanism, leads to the best performance on the
challenging Meta-dataset .
Ye et al. propose to adapt the few-shot embeddings
learned on the base classes to the few-shot target classes
during inference using a Transformer module. This leads
to task-speciﬁc embeddings that perform better on the
discriminative tasks such as few-shot classiﬁcation. While
many other set-to-set functions are also evaluated, such as
Graph convolutional networks , Bidirectional LSTMs
 and DeepSets , the best performance is achieved
with the Transformer-based mapping. This is attributed to
the better contextualization, task interpolation and extrapolation capability of Transformers and their permutation
invariance while maintaining a relatively lower parameter
complexity. The Transformer architecture in follows the
standard model . The embeddings are adapted using
a contrastive loss function for preserving discriminative
properties (Fig. 15). The resulting model achieves strong
performance on inductive, transductive, and generalized
FSL tasks.
Liu et al. learn a multi-head self-attention based
module, to integrate the visual representation learned by the
models trained on different domains present in the metadataset . The Universal Representation Transformer
(URT) layer dynamically re-weights the representations
from different domain-speciﬁc backbones, and proves very
effective in handling few shot tasks across a variety of data
distributions.
Transformers for Clustering
Clustering aims to discover structure in the data by grouping similar data points together. It has numerous applications such as data visualization and interpretation, anomaly
detection, and open-set categorization. Neural networks
have been developed for set prediction problems ,
 , however, the setpoints are processed individually
which can lose information about inter-point relationships.
Recent works employ Transformers that operate on set
inputs called the Set Transformers (ST) for amortized
clustering. Amortized clustering is a challenging problem
that seeks to learn a parametric function that can map an
input set of points to their corresponding cluster centers. Lee
et al. propose to learn such a mapping function using
a Transformer architecture comprising of multi-head selfattention blocks . The Transformer model is permutation
invariant by design and allows encoding both pair-wise and
higher-order relationships between the input points. However, a full Transformer would lead to a high computational
cost of O(n2) in each self-attention layer, where n is the
number of points in the set. ST reduces this cost to O(mn)
by using an Induced Self-Attention Block that uses a lowrank projection (H ∈Rm) to allow operating on large sets.
The model was trained to learn optimal parameters that
maximize the likelihood of a mixture of Gaussians (MoGs).
Thus MoG parameters are estimated by the ST given a set
of data points. Beyond amortized clustering, ST is a generic
framework which can handle other set-input problems such
as counting unique elements in an input set, multi-instance
learning, set anomaly detection, and 3D point-cloud classiﬁcation. More recently, improves by taking a
sequential approach to cluster generation, thereby allowing
assignment to a variable number of clusters.
Transformers for 3D Analysis
Given the irregular (variable number of points) and permutation invariant nature of 3D point cloud representations,
Transformers provide a promising mechanism to encode
rich relationships between 3D data points. To this end,
recent works , are motivated by the capability of
Transformers to learn set-functions. Speciﬁcally, introduced a Point Transformer which uses vector attention to
learn weights for each channel, while suggest an alternate design where local 3D structure is explicitly encoded.
The non-local nature of Transformers is exploited in 
towards an accurate human pose and mesh reconstruction
algorithm. We discuss these approaches below.
Self-attention being a set-operator is ideally suited for
processing point clouds, a 3D data representation that demands invariance to number of points and their permutations. Zhao et al. propose a point Transformer layer that
applies self-attention in the local neighborhood of 3D points.
The proposed layer builds on vectorized self-attention network (SAN) where attention weights are represented
with vectors.Furthermore, a positional encoding is added
both to the attention vector and transformed features (value
vectors) to represent location information. The point Transformer layer is sandwiched between two linear layers to
create a point Transformer block that is stacked multiple
times in the developed network architecture. Their design
also included transition down/up blocks to reduce/increase
the number of points in the input (in a typical encodingdecoding pipeline style). The resulting architecture shows
promising results on the 3D classiﬁcation and segmentation
The Point Cloud Transformer (PCT) is a parallel
work to and motivated by the permutation invariance
property of Transformers. However, compared to , it
is more directly based on the conventional Transformer
architecture and does not involve vector attention. The
key modiﬁcations include a 3D coordinate-based position
encoding, an offset attention module, and a neighbor embedding that encodes local 3D structure in point-clouds.
Speciﬁcally, the offset attention layer calculates the difference between the self-attended features and the input
features using element-wise subtraction. The local neighbor
embedding simply ﬁnds self-attention relationships among
a group of points instead of individual 3D points. Explicitly
incorporating local neighbourhood information makes this
a more efﬁcient architecture compared to . The method
shows promising performance on 3D shape classiﬁcation,
normal estimation and segmentation tasks on ModelNet40
 and ShapeNet datasets.
The Mesh Transformer (METRO) model targets 3D
human pose and mesh reconstruction from a single 2D image. A key challenge here is to faithfully learn the non-local
interactions between body-joints and mesh vertices (e.g.,
hand and foot). The expressivity of Transformer network
is used to jointly model vertex to vertex relationships in a
mesh as well as the vertex to body-joint relationships. The
self-attention mechanism can attend to any combination of
vertices in the mesh, thereby encoding non-local relationships. The multi-layer Transformer architecture sequentially
performs dimensionality reduction to map the 2D image to
3D mesh. Position encoding is performed using the 3D coordinates (x,y,z) of each vertex and each body-joint. Similar to
masked language modeling in NLP, METRO uses masked
vertex modeling (MVM) which randomly masks some percentage of input queries (see Fig. 16). The Transformer is
tasked with regressing all the joints and vertices which helps
encode inter-dependencies between them. METRO obtains
state-of-the-art results on human mesh reconstruction on
Human3.6M and 3DPW datasets. Since the approach does not depends on a parametric mesh model, it
generalizes well to other reconstruction tasks such as 3D
hand reconstruction . Overall, this is the ﬁrst effort
to employ Transformers for 3D human reconstruction tasks
and leads to fairly good results.
OPEN CHALLENGES & FUTURE DIRECTIONS
Despite excellent performance from Transformer models
and their interesting salient features (Table 1), there exist several challenges associated with their applicability to
practical settings (Table 2). The most important bottlenecks
include requirement for large-amounts of training data and
associated high computational costs. There have also been
some challenges to visualize and interpret Transformer
models. In this section, we provide an overview of these
challenges, mention some of the recent efforts to address
those limitations and highlight the open research questions.
High Computational Cost
As discussed in Sec. 1, a strength of Transformer models
is their ﬂexibility to scale to high parametric complexity.
While this is a remarkable property that allows training
enormous sized models, this results in high training and
inference cost (a detailed comparison between CNN and
ViTs is shown in Table 3). As an example, the BERT 
basic model (with 109 million parameters) took around 1.89
peta-ﬂop days2 for training, while the latest GPT3 model
(175 billion parameters) took around 3640 peta-ﬂop days
for training . This comes
with a huge price tag, e.g., according to one estimate ,
GPT3 training might have cost OpenAI 4.6 million USD.
Additionally, these large-scale models require aggressive
compression (e.g., distillation) to make them feasible for realworld settings.
An empirical study on the scalability of Vision Transformers for number of parameters (ranging from ﬁve million
to two billion), size of the training datasets (ranging from 30
million to three billion training images), and compute budget (1-10000 TPU core-days) is presented in . From this
study, We can draw the following conclusions (a) scaling up
on compute, model and size of training samples improves
performance (b) only large models (with more parameters)
can beneﬁt from more training data, and the performance
of smaller models platueas quickly and can not leverage
from additional data. This indicates that large scale models
have the capacity to further enhance their representation
learning capabilities. However, with the current designs,
scaling upon Transformer models is expensive and compute
prohibitive, thus necessitating the need for efﬁcient designs.
In the language domain, recent works focus on reducing
the high complexity of Transformer models (basically arising from the self-attention mechanism where a token’s
representation is updated by considering all tokens from the
previous layer). For example, , explore selective
or sparse attention to previous layer tokens while updating
each next layer token. Linformer reduces complexity of
standard self-attention operation from O(n2) to O(n) (both
in time and memory requirements). The main idea is to
show that a low-rank matrix is sufﬁcient to model the selfattention mechanism. The Reformer model employed
locally-sensitive hashing (LSH) to minimize the complexity
of self-attention from O(n2) to O(nlog(n)). In similar pursuit, the recent Lambda Networks propose to model local
context as a linear function which helps reduce complexity
of self-attention . These linear function lambdas are
applied to the input query to model contextual relationships
between pixels.
Vyas et al. developed an efﬁcient cluster attention
to deal with large input sequences that approximates the
original self-attention. The cluster attention groups queries
into clusters and then computes attention between cluster
centers (instead of attention between all the queries that
leads to quadratic complexity). The main idea is that the
queries close in the Euclidean space should have similar
attention distributions. With a ﬁxed number of clusters, this
intuition helps reduce the quadratic complexity to linear
2. A peta-ﬂop day is a measure of computation and equals to performing 1015 neural net operations per second for one complete day.
Design Highlights (focus on differences
with the standard form)
Input Data Type
Label Type
Classiﬁcation
Directly adopted NLP Transformer Encoder for images, Mechanism to linearly
embed image patches with positional embedding suitable for the Encoder.
Class labels
Cross-entropy
Transformer as s student while CNN as
a teacher, Distillation tokens to produce
estimated labels from teacher, Attention
between class and distillation tokens.
Class labels
Cross-entropy,
Distillation loss
KL-divergence
CLIP 
Jointly train image and text encoders on
image-text pairs, to maximize similarity of
valid pairs and minimize otherwise
2D Images & texts
Image-text
cross-entropy
Linear projection layer to reduce CNN
feature dimension, Spatial positional embedding added to each multi-head selfattention layer of both encoder and decoder. Object queries (output positional
encoding) added to each multi-head selfattention layer of decoder.
Class labels
Hungarian loss
based on bipartite
matching between
predicted and
ground truths
D-DETR 
Deformable Transformer consists of deformable attention layers to introduce
sparse priors in Transformers, Multi-scale
attention module.
Class labels
Hungarian loss
Self-supervised
pretraining,
Queryaligned
prototypes
correspondence
support-set images and query image.
Pretraining
labels and
learning with
Class labels
Normalized
Cross-entropy
Colorization
ColTran 
Conditional Row/column multi-head attention layers, Progressive multi-scale colorization scheme.
log-likelihood of the
Recognition
ST-TR 
Spatial and Temporal self-attention to operates on graph data such as joints in skeletons.
Cross-entropy
Superresolution
Texture enhancing Transformer module,
Relevance embeddings to compute the relevance between the low-resolution and
reference image.
Reconstruction loss,
Perceptual loss
pretrained VGG19
Multi-Model
Oscar 
Transformer layer to jointly process triplet
representation of image-text [words, tags,
features], Masked tokens to represent text
Class labels,
Object tags
log-likelihood of
masked tokens,
Contrastive binary
cross-entropy
3D Classiﬁcation/Segmentation
Point Transformer block, Transition down
block to reduce cardinality of the point set,
Transition up for dense prediction tasks.
CAD models, 3D
object part
segmentation
Object and
categories
Cross-entropy
Reconstruction
METRO 
Progressive
dimensionality
Transformer
Positional
Encoding with 3D joint and 3D vertex
coordinates,
vertex/joint
Human Pose
L1 loss on mesh
vertices and joints in
projection.
Vision and
Navigation
Chen et al. 
Uni-modal encoders on language and map
inputs followed by a cross-modal transformer, Trajectory position encodings in
the map encoder.
Instruction text +
RGBD panorama +
Topological
Environment Map
Navigation
Cross-entropy over
nodes and [stop]
Segmentation
Multimodal
Cross-modal
selfattention on multiple levels and their fusion using learned gates.
2D Image +
Language expression
Segmentation
Binary cross-entropy
Classiﬁcation
Lee et al. 
Operates on real-valued audio-visual signals instead of tokens, Contrastive learning for pre-training, End-to-end multimodal transformer learning.
Audio-Visual
Contrastive InfoNCE
loss and Binary
cross-entropy
TABLE 1: A summary of key design choices adopted in different variants of transformers for a representative set of
computer vision applications. The main changes relate to speciﬁc loss function choices, architectural modiﬁcations, different
position embeddings and variations in input data modalities.
Performance
Highlights
Limitations
Classiﬁcation
Top-1 Acc.
a) First application of Transformer
(global self-attention) directly on
image patches, b) Convolution-free
network architecture, c) Outperforms CNN models such as ResNet.
a) Requires training on large-scale
data e.g., 300-Million images, b)
Requires careful transfer learning
to the new task, c) Requires large
model with 632-Million parameters
to achieve SOTA results.
Top-1 Acc.
a) Successfully trains Transformer
on ImageNet only, b) Introduces
attention-based distillation method.
competitive
performance with small (86-Million parameters) Transformers.
a) Requires access to pretrained
CNN based teacher model thus performance depends on the quality of
the teacher model.
Swin-T 
Top-1 Acc.
a) Provides a general purpose backbone for different vision tasks e.g.,
classiﬁcation, detection and segmentation b) A hierarchical design
using shifted-windows operation.
a) Hard to train from scratch on
smaller datasets b) Quadratic compute complexity inherent to the
self-attention operation.
NeurIPS’20
Top-1 Acc.
Self-supervised
pre-training
Transformer
achieving stat-of-the-art results.
Proposed algorithm is limited in its
capacity to perform on datasets that
lack spatial details such as texture.
a) Use of Transformer allows endto-end training pipeline for object
detection, b) Removes the need for
hand-crafted post-processing steps.
a) Performs poorly on small objects,
b) Requires long training time to
D-DETR 
a) Achieves better performance on
small objects than DETR , b)
Faster convergence than DETR 
Obtain SOTA results with 52.3 AP
but with two stage detector design
and test time augmentations.
Colorization
ColTran 
a) First successful application of
Transformer to image colorization,
b) Achieves SOTA FID score.
a) Lacks end-to-end training, b)
limited to images of size 256×256.
Recognition
ST-TR 
Top-1 Acc.
a) Successfully applies Transformer
to model relations between body
joints both in spatial and temporal
domain, b) Achieves SOTA results.
Proposed Transformers do not process joints directly rather operate on
features extracted by a CNN, thus
the overall model is based on handcrafted design.
Resolution
27.1 / 0.8
30.0 / 0.81
25.9 / 0.78
30.1 / 0.91
a) Achieves state-of-the-art superresolution by using attention, b)
Novel Transformer inspired architectures that can process multi-scale
a) Proposed Transformer does not
process images directly but features
extracted by a convolution based
network, b) Model with large number of trainable parameters, and c)
Compute intensive.
NeurIPS’19
VQA /
70.6/ 58.2
a) Proposed Transformer architecture can combine text and visual
information to understand intertask dependencies, b) Achieves pretraining on unlabelled dataset.
a) Requires large amount of data
for pre-training, b) Requires ﬁne
tuning to the new task.
Oscar 
VQA /
80.37/57.5
a) Exploit novel supervisory signal
via object tags to achieve text and
image alignment, b) Achieves stateof-the-art results.
Requires extra supervision through
pre-trained object detectors thus
performance is dependent on the
quality of object detectors.
UNITER 
(R@1/5/10)
VQA /
72.47/83.72 Learns ﬁne-grained relation alignment between text and images
Requires large multi-task datasets
for Transformer training which lead
to high computational cost.
Transformer 
Top-1 Acc.
ModelNet40
a) Transformer based attention capable to process unordered and unstructured point sets, b) Permutation invariant architecture.
a) Only moderate improvements
over previous SOTA, b) Large number of trainable parameters around
6× higher than PointNet++ .
METRO 
a) Does not depend on parametric
mesh models so easily extendable
to different objects, b) Achieves
SOTA results using Transformers.
Dependent on hand-crafted network design.
TABLE 2: A summary of advantages and limitations of different Transformers based methods in different Tasks. (CT: Cross
Transformers, AP: Average Precision, mAP: mean AP, IoU: Intersection over Union, FID: Fr´echet inception distance, MPJPE:
Mean Per Joint Position Error, MPVE: Mean Per Vertex Error).
Fig. 16: Mesh Transformer architecture. The joint and vertex queries are appended with positional embeddings and passed
through multiple self-attention layers to jointly regress 3D coordinates of joints and mesh vertices. Figure is from .
#Param (M)
Top-1 Acc (%)
ResNet18 ⋆
EfﬁcientNet-B3 ⋆
DeiT-T 
T2T-ViTt-7 
LocalViT-T 
CrossViT-T 
PVTv1-T 
ResT-Lite 
CaiT-XXX-24 
PVTv2-B1 
Lv-ViT-T 
RegionViT-T 
ResNet50 ⋆
ResNeXt50-32x4d ⋆
RegNetY-4G ⋆
EfﬁcientNet-B4 ⋆
DeiT-S 
PVTv1-S 
LocalViT-S 
CrossViT-S 
TNT-S 
Swin-T 
NesT-T 
T2T-ViTt-14 
CvT-13 
ResT-B 
Twins-SVT-S 
PVTv2-B2-Li 
RegionViT-S 
Lv-ViT-S 
#Param (M)
Top-1 Acc (%)
ResNet101 ⋆
ResNeXt101-32x4d ⋆
RegNetY-8G ⋆
EfﬁcientNet-B5 ⋆
CvT-21 
CaiT-S-24 
T2T-ViTt-19 
PVTv1-M 
PVTv2-B3 
NesT-S 
ResNet152 ⋆
CaiT-S-36 
T2T-ViTt-24 
PVTv1-L 
TNT-B 
Swin-S 
Twins-SVT-B 
RegionViT-B 
PVTv2-B4 
ResNeXt101-64x4d ⋆
RegNetY-16G ⋆
EfﬁcientNet-B6 ⋆
NesT-B 
ViT-B/16 
DeiT-B/16 
Swin-B 
Twins-SVT-L 
PVTv2-B5 
Lv-ViT-M 
TABLE 3: A Comparative analysis between different vision transformer and CNN models in terms of their parameter
complexity and top-1 (%) accuracy on ImageNet validation set. For a direct comparison, we consider models that are
trained on ImageNet from scratch on input of size 224x224. ⋆denotes pure CNN-based methods.
complexity of O(nc) with respect to the input sequence
length n (where c is the number of clusters). We refer
interested readers to a survey on efﬁcient Transformers in
Similar to the NLP domain, computer vision models
also suffer from the high computational cost of Transformer
models. For example, image generators that are based
on sequence-based Transformers (e.g., iGPT) have a high
compute cost limiting their applicability to high-resolution
inputs. The time and memory cost of core self-attention
operation in Transformers increases quadratically with the
number of patches, i.e. O(n2), for n image patches (in some
applications, e.g., low-level vision, n = H × W where
H, W denote the height and width of the image). This is a
major drawback of existing Transformers that hinders their
application to most tasks involving high-resolution (HR)
images, such as object detection and segmentation (in highlevel vision), and super-resolution, deblurring, denoising,
etc. (in low-level vision). Numerous methods have been
proposed that make special design choices to perform selfattention more ‘efﬁciently’, for instance employing pooling/downsampling in self-attention , , , local
window-based attention , , axial-attention ,
 , low-rank projection attention , , , ker-
nelizable attention , , and similarity-clustering
based methods , . However, almost all of these
approaches either come with a trade-off between complexity
and accuracy, require special hardware speciﬁcations or are
still not applicable to very large images. Therefore, there
is a pressing need to develop an efﬁcient self-attention
mechanism that can be applied to HR images on resourcelimited systems without compromising accuracy. It will be
interesting to explore how existing models can be extended
to high-dimensional cases e.g., using a multi-scale transformer design with a somewhat local context modeling. By
inducing inductive biases based on our understanding of
the visual learning tasks (e.g., spatial relationships in the
local neighbourhood), the high computational cost can be
reduced. Similarly, using sparse attention maps modeled
with low-rank factorization in the matrices can also help
towards reducing the computational cost .
Large Data Requirements
Since Transformer architectures do not inherently encode
inductive biases (prior knowledge) to deal with visual data,
they typically require large amount of training to ﬁgure
out the underlying modality-speciﬁc rules. For example, a
CNN has inbuilt translation invariance, weight sharing, and
partial scale invariance due to pooling operations or multiscale processing blocks. However, a Transformer network
needs to ﬁgure out these image-speciﬁc concepts on its own
from the training examples. Similarly, relationships between
video frames need to be discovered automatically by the
self-attention mechanism by looking at a large database
of video sequences. This results in longer training times,
a signiﬁcant increase in computational requirements, and
large datasets for processing. For example, the ViT 
model requires hundreds of millions of image examples to
obtain reasonable performance on the ImageNet benchmark
dataset. The question of learning a Transformer in a dataefﬁcient manner is an open research problem and recent
works report encouraging steps towards its resolution. For
example, DeiT uses a distillation approach to achieve
data efﬁciency while T2T (Tokens-to-Token) ViT models
local structure by combining spatially close tokens together,
thus leading to competitive performance when trained only
on ImageNet from scratch (without pre-training). By incorporating CNNs like feature hierarchies in ViTs to effectively
capture local image cues, ViTs (e.g., CCT , NesT )
can be trained from scratch even on small-scale datasets
(e.g., CIFAR-10). Another approach to data efﬁcient training
of ViTs is proposed in et al. . The authors show that
by smoothing the local loss surface using sharpness-aware
minimizer (SAM) , ViTs can be trained with simple
data augmentation scheme (random crop, and horizontal
ﬂip) , instead of employing compute intensive strong
data augmentation strategies, and can outperform their
counterpart ResNet models.
Vision Tailored Transformer Designs
We note that most of the existing works focused on vision
tasks tend to directly apply NLP Transformer models on
computer vision problems. These include architectures designed for image recognition , video understanding 
and especially multi-modal processing . Although the
initial results from these simple applications are quite encouraging and motivate us to look further into the strengths
of self-attention and self-supervised learning, current architectures may still remain better tailored for language problems (with a sequence structure) and need further intuitions
to make them more efﬁcient for visual inputs. For example,
vector attention from is a nice work in this direction
which attempts to speciﬁcally tailor self-attention operation
for visual inputs via learning channel-wise attentions. Similarly, uses a Jigsaw puzzle based self-supervision loss
as a parallel branch in the Transformers to improve person
re-identiﬁcation. A recent work rearranges the spatially close tokens to better model relationships in spatially
proximal locations. Token distillation from pre-trained
CNN models has also been used as a remedy to inject
domain biases in the representations. One may argue that
the architectures like Transformer models should remain
generic to be directly applicable across domains, we notice
that the high computational and time cost for pre-training
such models demands novel design strategies to make their
training more affordable on vision problems.
Neural Architecture Search for ViTs
While Nerual Architecuter Search (NAS) has been well
explored for CNNs to ﬁnd an optimized architecture, it
is relatively less explored in Transformers (even for language transformers , ). Chen et al. propose a
one-shot NAS for vision transformers, called AutoFormer.
BossNAS searches for a hybrid architecture (CNN
and Transformer). Another recent effort studies the tradeoff between global and local information in Transformers in
the context of vision applications . It will be insightful
to further explore the domain-speciﬁc design choices (e.g.,
the contrasting requirements between language and vision
domains) using NAS to design more efﬁcient and lightweight models similar to CNNs .
Interpretability of Transformers
Through an extensive set of carefully designed experiments,
Naseer et al. investigate multiple intriguing properties
of ViTs in terms of their generalization and robustness. They
show that, compared with CNNs, ViTs demonstrate strong
robustness against texture changes and severe occlusions,
e.g.ViTs retain upto 60% top-1 accuracy on ImageNet once
80% of the image content is randomly occluded. Given the
strong performance of Transformer architectures, it is interesting and critical to interpret their decisions, e.g., by visualizing relevant regions in an image for a given classiﬁcation
decision. The main challenge is that the attention originating
in each layer, gets inter-mixed in the subsequent layers in a
complex manner, making it difﬁcult to visualize the relative
contribution of input tokens towards ﬁnal predictions. This
is an open problem, however, some recent works – 
target enhanced interpretability of Transformers and report
encouraging results. Attention roll-out and attention ﬂow
methods were proposed in to estimate the accurate attentions. However, this method functions in an ad-hoc manner and makes simplistic assumptions e.g., input tokens are
linearly combined using attention weights across the layers.
Chefer et al. note that the attention scores obtained directly via the self-attention process (encoding relationships
between tokens) or reassignments in do not provide an
optimal solution. As an alternative, they propose to assign
and propagate relevancy scores in the Transformer network
such that the sum of relevancy is constant throughout the
network. Their design can handle both the positive and
negative attributions experienced in the self-attention layer.
The proposed framework has an added advantage of being
able to provide class-speciﬁc visualizations. Despite these
seminal works, visualizing and interpreting Transformers
is an unsolved problem and methods are needed to obtain
spatially precise activation-speciﬁc visualizations. Further
progress in this direction can help in better understanding
the Transformer models, diagnosing any erroneous behaviors and biases in the decision process. It can also help us
design novel architectures that can help us avoid any biases.
Hardware Efﬁcient Designs
Large-scale Transformer networks can have intensive power
and computation requirements, hindering their deployment
on edge devices and resource-constrained environments
such as internet-of-things (IoT) platforms. Some recent efforts have been reported to compress and accelerate NLP
models on embedded systems such as FPGAs . Li et
al. used an enhanced block-circulant matrix-based representation to compress NLP models and proposed a new
Field Programmable Gate Array (FPGA) architecture design
to efﬁciently manage resources for high throughput and low
latency. They could achieve 27x, 3x and 81x improvements
in performance (throughput measured in FPS), reduced
power consumption, and energy efﬁciency relative a CPU
for RoBERTa model . Towards this goal, proposed
to design Hardware-Aware Transformers (HAT) using neural architecture search strategies – . Speciﬁcally, a
SuperTransformer model is ﬁrst trained for performance
approximation which can estimate a model’s performance
without fully training it. This model comprises the largest
possible model in the search space while sharing weights
between common parts. Eventually, an evolutionary search
is performed considering the hardware latency constraints
to ﬁnd a suitable SubTransformer model for a target hardware platform (e.g., IoT device, GPU, CPU). However, such
hardware efﬁcient designs are currently lacking for the
vision Transformers to enable their seamless deployment
in resource-constrained devices. Further, the search cost of
the evolutionary algorithms remains signiﬁcant with the
associated impact of CO2 emissions on the environment.
Towards Integrating All Modalities
Since Transformers provide a uniﬁed design to process
different modalities, recent efforts also focus on proposing
more generic general purpose reasoning systems based on
Transformers. Inspired by the biological systems that can
process information from a diverse range of modalities,
Perceiver model aims to learn a uniﬁed model that
can process any given input modality without making
domain-speciﬁc architectural assumptions. In order to scale
to high-dimensional inputs, Perceiver uses an asymmetric
cross attention method to distill input information into lowdimensional latent bottleneck features. Once the features are
distilled in a compact and ﬁxed-dimensional form, regular
Transformer blocks are applied in the latent space. The
original Perceiver model shows performance competitive to
ResNets and ViTs on image classiﬁcation and can process 3D
data, audio, images, video or their combinations. However,
this model can only generate ﬁxed outputs e.g., class probabilities. A recent improvement called Perceiver IO 
aims to learn models with both ﬂexible inputs as well as
arbitrary sized outputs. This allows application to problems
which demand structured outputs such as natural language
tasks and visual comprehension. While these models avoid
modality dependent architectural choices, the learning itself
still involves modality dependent choices e.g., speciﬁc augmentations or positional encodings. An interesting and open
future direction is to achieve total modality-agnosticism in
the learning pipeline.
CONCLUSION
Attention has played a key role in delivering efﬁcient
and accurate computer vision systems, while simultaneously providing insights into the function of deep neural networks. This survey reviews the self-attention approaches and speciﬁcally focuses on the Transformer and bidirectional encoding architectures that are built on the principle of self-attention. We ﬁrst cover fundamental concepts
pertaining to self-attention architectures and later provide
an in-depth analysis of competing approaches for a broad
range of computer vision applications. Speciﬁcally, we include state of the art self-attention models for image recognition, object detection, semantic and instance segmentation,
video analysis and classiﬁcation, visual question answering,
visual commonsense reasoning, image captioning, visionlanguage navigation, clustering, few-shot learning, and 3D
data analysis. We systematically highlight the key strengths
and limitations of the existing methods and particularly
elaborate on the important future research directions. With
its speciﬁc focus on computer vision tasks, this survey provides a unique view of the recent progress in self-attention
and Transformer-based methods. We hope this effort will
drive further interest in the vision community to leverage
the potential of Transformer models and improve on their
current limitations e.g., reducing their carbon footprint.
ACKNOWLEDGMENTS
The authors would like to thank Tim Prangemeier (TU Darmstadt), Luowei Zhou (Microsoft Research), Jason Corso (University of Michigan),
Pichao Wang (Alibaba Group), Yuqing Wang (Meituan), Alex Meinke
(Uni-Tuebingen), Irwan Bello (Google Brain) and Manoj Kumar (Google
Brain) for their helpful feedback on the survey. We would also like to
thank Mohamed Afham for his help with a ﬁgure.