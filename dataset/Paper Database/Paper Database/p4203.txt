JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
Residual Attention U-Net for Automated
Multi-Class Segmentation of COVID-19 Chest CT
Xiaocong Chen, Lina Yao, Member, IEEE, and Yu Zhang, Senior Member, IEEE
Abstract—The novel coronavirus disease 2019 (COVID-19) has
been spreading rapidly around the world and caused signiﬁcant
impact on the public health and economy. However, there is
still lack of studies on effectively quantifying the lung infection
caused by COVID-19. As a basic but challenging task of the
diagnostic framework, segmentation plays a crucial role in
accurate quantiﬁcation of COVID-19 infection measured by
computed tomography (CT) images. To this end, we proposed
a novel deep learning algorithm for automated segmentation of
multiple COVID-19 infection regions. Speciﬁcally, we use the
Aggregated Residual Transformations to learn a robust and
expressive feature representation and apply the soft attention
mechanism to improve the capability of the model to distinguish
a variety of symptoms of the COVID-19. With a public CT image
dataset, we validate the efﬁcacy of the proposed algorithm in
comparison with other competing methods. Experimental results
demonstrate the outstanding performance of our algorithm for
automated segmentation of COVID-19 Chest CT images. Our
study provides a promising deep leaning-based segmentation tool
to lay a foundation to quantitative diagnosis of COVID-19 lung
infection in CT images.
Index Terms—Automated segmentation, COVID-19, Computed
tomography, Deep learning
I. INTRODUCTION
The novel coronavirus disease 2019, also known as COVID-
19 outbreak ﬁrst noted in Wuhan in the end of 2019, has been
spreading rapidly worldwide . As an infectious disease,
COVID-19 is caused by severe acute respiratory syndrome
coronavirus and presents with symptoms including fever, dry
cough, shortness of breath, tiredness and so on. As the April
9th, over 1.5 million people around the world have been
conﬁrmed as COVID-19 infection with a case fatality rate
of about 5.7 % according to the statistic of World Health
Organization1.
So far, no speciﬁc treatment has proven effective for
COVID-19. Therefore, accurate and rapid testing is extremely
crucial for timely prevention of COVID-19 spread. Real-time
reverse transcriptase polymerase chain reaction (RT-PCR) has
been referred as the standard approach for testing COVID-
19. However, RT-PCR testing is time-consuming and limited
by the lack of supply test kits , . Moreover, RT-PCR
X. Chen, L.Yao are with the School of Computer Science and Engineering at University of New South Wales, NSW 2052, Australia (e-mail:
 , ).
Y. Zhang is with the Department of Psychiatry and Behavioral Sciences at
Stanford University, CA 94305, USA (email: ).
1 
2019/situation-reports
has been reported to suffer from low sensitivity and repeated
checking is typically needed for accurate conﬁrmation of a
COVID-19 case. This indicates that many patients will not be
conﬁrmed timely , , thereby resulting in a high risk of
infecting a larger population.
In recent years, imaging technology has emerged as a
promising tool for automatic quantiﬁcation and diagnosis of
various diseases. As a routine diagnostic tool for pneumonia,
chest computed tomography (CT) imaging has been strongly
recommended in suspected COVID-19 cases for both initial
evaluation and follow-up. Chest CT scans were found very
useful in detecting typical radiographic features of COVID-
19 . A systematic review concluded that CT imaging
of chest was found sensitive for checking COVID-19 even
before some clinical symptoms were observed. Speciﬁcally,
the imaging features including ground class opaciﬁcation, consolidation, and pleural effusion have been frequently observed
in the chest CT images scanned from COVID-19 patients –
Accurate segmentation of these important radiographic features is crucial for a reliable quantiﬁcation of COVID-19 infection in chest CT images. Segmentation of medical imaging
needs to be manually annotated by well-trained expert radiologists. The rapidly increasing number of infected patients has
caused tremendous burden for radiologists and slowed down
the labeling of ground-truth mask. Thus, there is an urgent
need for automated segmentation of infection regions, which
is a basic but arduous task in the pipeline of computer-aided
disease diagnosis . However, automatically delineating the
infection regions from the chest CT scans is considerably
challenging because of the large variation in both position and
shape across different patients and low contrast of the infection
regions in CT images .
Machine learning-based artiﬁcial intelligence provides a
powerful technique for the design of data-driven methods in
medical imaging analysis . Developing advanced deep
learning models would bring unique beneﬁts to the rapid and
automated segmentation of medical images . So far, fully
convolutional networks have proven superiority over other
widely used registration-based approaches for segmentation
 . In particular, U-Net models work decently well for
most segmentation tasks in medical images , – .
However, several potential limitations of U-Net have not been
effectively addressed yet. For example, the U-Net model is
hard to capture the complex features such as multi-class
image segmentation and recover the complex feature into the
 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
segmentation image . There are also a few successful
applications that adopt U-Net or its variants to implement the
CT image segmentation, including heart segmentation ,
liver segmentation , or multi-organ segmentation .
However, segmentation of COVID-19 infection regions with
deep learning remains under explored. The COVID-19 is a
new disease but very similar with the common pneumonia in
the medical imaging side, which makes its accurate quantiﬁcation considerably challenging. Recent advancement of the
deep learning method provide heaps of insightful ideas about
improving the U-Net architecture. The most popular one is
the deep residual network (ResNet) . ResNet provided
an elegant way to stacked CNN layers and demonstrate the
strength when combined with U-Net . On the other hand,
attention was also applied to improve the U-Net and other
deep learning models to boost the performance , .
Accordingly, we propose a novel deep learning model
for rapid and accurate segmentation of COVID-19 infection
regions in chest CT scans. Our developed model is based on
the U-Net architecture, inspired with recent advancement in
the deep learning ﬁeld. We exploit both the residual network
and attention mechanism to improve the efﬁcacy of the U-Net.
Experimental analysis is conducted with a public CT image
dataset collected from patients infected with COVID-19 to
assess the efﬁcacy of the developed model. The outstanding
performance demonstrates that our study provides a promising
segmentation tool for the timely and reliable quantiﬁcation of
lung infection, toward to developing an effective pipeline for
precious COVID-19 diagnosis.
The rest of the paper is summarized as follows. We ﬁrst
review some related work about existing deep learning methods for CT image segmentation in Section II Related Work.
Our proposed new deep learning model is detailedly described
in Section III Methodology, including the U-Net structure,
the methods used to improve the encoder and decoder. The
experimental study and performance assessment are described
in Section IV, followed by discussion and summary of our
II. METHODOLOGY
This section will introduce our proposed Residual Attention
U-Net for the lung CT image segmentation in detail. We
start by describing the overall structure of the developed
deep learning model followed by explaining the two improved
components including aggregated residual block and locality
sensitive hashing attention, as well as the training strategy. The
overall ﬂowchart is illustrated in Fig. 1.
A. Overview
U-Net was ﬁrst proposed by Ronneberger et al. ,
which was basically a variant of fully convolutional networks
(FCN) . The traditional U-Net is a type of artiﬁcial
neural network (ANN) containing a set of convolutional layers
and deconvolutional layers to perform the task of biomedical
image segmentation. The structure of U-Net is symmetric with
two parts: encoder and decoder. The encoder is designed to
extract the spatial features from the original medical image.
The decoder is to construct the segmentation map from the
extracted spatial features. The encoder follows the similar style
like FCN with the combination of several convolutional layers.
To be speciﬁc, the encoder consists of a sequence of blocks for
down-sampling operations, with each block including two 3×3
convolution layers followed by a 2×2 max-pooling layers with
stride of 2. The number of ﬁlters in the convolutional layers is
doubled after each down-sampling operation. In the end, the
encoder adopts two 3 × 3 convolutional layers as the bridge
to connect with the decoder.
Differently, the decoder is designed for up-sampling and
constructing the segmentation image. The decoder ﬁrst utilizes
the a 2×2 deconvolutional layer to up-sample the feature map
generated by the encoder. The deconvolutional layer developed
by Zeiler et al. contains the transposed convolution
operation and will half the number of ﬁlters in the output.
It is followed by a sequence of up-sampling blocks which
consist two 3 × 3 convolution layers and a deconvolutional
layer. Then, a 1 × 1 convolutional layer is used as the ﬁnal
layer to generate the segmentation result. The ﬁnal layer
adopted Sigmoid function as the activation function while all
other layers used ReLU function.The ReLU and the Sigmoid
functions are deﬁned as:
ReLU: f(x) = max{0, x}
Sigmoid: f(x) =
1 + exp(−x)
In addition, the U-Net concatenates part of the encoder features with the decoder. For each block in encoder, the result
of the convolution before the max-pooling is transferred to
decoder symmetrically. In decoder, each block receives the
feature representation learned from encoder, and concatenates
them with the output of deconvolutional layer. The concatenated result is then forwardly propagated to the consecutive
block. This concatenation operation is useful for the decoder
to capture the possible lost features by the max-pooling.
B. Aggregated Residual Block
As mentioned in previous section, the U-Net only have four
blocks of convolution layers to conduct the feature extraction.
The conventional structure may not be sufﬁcient for the
complex medical image analysis such as multi-class image
segmentation in lung, which is the aim for this study. Although
U-Net can easily separate the lung in a CT image, it may have
limited ability to distinguish the difference infection regions of
the lung which infected by COVID-19. Based on this case, the
deeper network is needed with more layers, especially for the
encoding process. However, when deeper network converging,
a problem will be exposed: with increasing of the network
depth, accuracy gets very high and then decrease rapidly.
This problem is be deﬁned as degradation problem , .
He et al. proposed the ResNet to mitigate the effect of
network degradation on model learning. ResNet utilizes a skip
connection with residual learning to overcome the degradation
and avoid estimating a large number parameters generated by
the convolutional layer. The typical ResNet block is depicted
as Fig. 2. The function F can be deﬁned as:
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
ResNeXt Block
ResNeXt Block
ResNeXt Block
ResNeXt Block
ResNeXt Block
ResNeXt Block
ResNeXt Block
ResNeXt Block
ResNeXt Block
Convolution
Max- Pooling
Up-Sampling
Global Averaging
Skip Connection
Concatenate and
LSH Attention
Segmentation
Fig. 1: Illustration of our developed residual attention U-Net model. The aggregated ResNeXt blocks are used to capture the
complex feature from the original images. The left side of the U-Shape serves as encoder and the right side as decoder. Each
block of the decoder receives the feature representation learned from the encoder, and concatenates them with the output of
deconvolutional layer followed by LSH attention mechanism. The ﬁltered feature representation after the attention mechanism
is propagated through the skip connection.
Weight Layer
Weight Layer
Fig. 2: ResNet Block. Here the variable i is the D-dimension
representation of the input image or features map. The skip
connection is performed as the identity mapping, the output of
the identity mapping will be added to the output of the stacked
where i = [i1, i2, · · · , iD] and W = [w1, w2, · · · , wD] is the
trainable weight for the weight layer. Different from the U-
Net that concatenates the features map into decoding process,
ResNet adopts the shortcut to add the identity into the output
of each block. The stacked residual block is able to better
learn the latent representation of the input CT image. However,
the model comes more complex and hard to converge as the
increase in the number of layers.
Regarding this, Xie et al. proposed Aggregated Residual
Network(ResNeXt) and showed that increasing the cardinality
was more useful than increasing the depth or width . The
cardinality is deﬁned as the set of the Aggregated Residual
transformations with formulation as follows:
where C is the number of residual transformation to be
aggregated and Tj(i) can be any function. Considering a
simple neuron, Tj should be a transformation projecting i into
an low-dimensional embedding ideally and then transforming
it. Accordingly, we can extend it into the residual function:
where the y is the output. The ResNeXt block is visualized in
Fig. 3. Compared with the Fig. 2, the ResNeXt has a slightly
different structure. The weight layer’s size is smaller than
ResNet as ResNeXt use the cardinality to reduce the number of
layers but keep the performance. One thing is wroth to mention
that the three small blocks inside the ResNeXt block need to
have the some topology, in the other words, they should be
topologically equivalent.
Weight Layer
Weight Layer
Weight Layer
Weight Layer
Weight Layer
Weight Layer
Fig. 3: ResNeXt Block. The variable i is the D-dimension
representation of the input image or features map. Here, the
cardinality = 3.
Similar with the ResNet, after a sequence of blocks, the
learned features are feed into a global averaging pooling
layers to generate the ﬁnal feature map. Different from the
convolutional layers and normal pooling layers, the global
averaging pooling layers take the average of feature maps
derived by all blocks. It can sum up all the spatial information
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
Scaled Dot-Product Attention
Fig. 4: Attention Mechanism. The left ﬁgure shows the simple
scaled dot-prodct attention. The right ﬁgure depicts the multihand attention with the h head.
which captured by each step and is generally more robust
than directly make the spatial transformation to the input.
Mathematically, we can treat the global averaging pooling
layer as a structural regularizer that are helpful for driving
the desired feature maps .
Importantly, instead of using the encoder in the U-Net,
our proposed deep learning model adopts the ResNeXt block
(see Fig. 3) to conduct the features extraction. The ResNeXt
provides a solution which can prevent the network goes very
deeper but remain the performance. In addition, the training
cost of ResNeXt is better than ResNet.
C. Locality Sensitive Hashing Attention
The decoder in U-Net is used to up-sampling the extracted
feature map to generate the segmentation image. However,
due to the capability of the convolutional neural network, it
may not able to capture the complex features if the network
structure is not deep enough. In recent years, transformers 
have gained increasingly interest . The key of the success
is the attention mechanism . Attention includes two different mechanisms: soft attention and hard attention. We adopt
the soft attention to improve the model learning. Different
the hard attention, the soft attention can let model focus on
each pixel’s relative position, but the hard attention only can
focus on the absolute position. There are two different types
of soft attention: Scaled Dot-Product Attention and Multi-
Head Attention as shown in Fig. 4. The scaled dot-product
attention takes the inputs including a query Q, a key Kn of
the n-dimension and a value Vm of the m-dimension. The
dot-product attention is deﬁned as follows:
Attention(Q, Kn, Vm) = softmax(QKT
n represent to the transpose of the matrix Kn and
√n is a scaling factor. The softmax function σ(z) with z =
[z1, · · · , zn] ∈Rn is given by:
j=1 exp(zj) for i = 1, · · · , n
Vaswani et al. mentioned that, perform different linearly
project of the queries Q, keys K and values V in parallel
h layers will beneﬁt the attention score calculation. We can
assume that Q, K and V have been linearly projected to
dk, dk, dv dimensions, respectively. It is worth noting that
these linear projections are different and learnable. On each
projection p, we have a pair of query, key and value Qp, Kp, Vp
to conduct the attention calculation in parallel, which results
in a dv-dimensional output. The calculation can be formulated
MultiHead(Q, K, V ) = Concatenate(head1, · · · , headh)W O
where headi = Attention(QW Q
where the the projections W Q
Rdmodel×dk, W K
Rdmodel×dk, W V
∈Rdmodel×dvare parameter matrices and
W O ∈Rdmodel×hdv is the weight matrix used to balance the
results of h layers.
However, the multi-head attention is memory inefﬁcient due
to the size of Q, K and V . Assume that the Q, K, V have
the shape [|batch|, length, dmodel] where | · | represents the
size of the variable. The term QKT will produce a tensor in
shape [length, length, dmodel]. Given the standard image size,
the length × length will take most of the memory. Kitaev et
al. proposed a Locality Sensitive Hashing(LSH) based
Attention to address this issue. Firstly, we rewire the basic
attention formula into each query position i in the partition
exp(qi · kj −z(i, Pi))vj
where Pi = {j : i ≥j}
where the function z is the partition function, Pi is the set
which query position i attends to. During model training, we
normally conduct the batching and assume that there is a larger
i = {0, 1, · · · , l} ⊇Pi without considering elements not
exp(qi · kj −N(j, Pi) −z(i, Pi))vj
where N(j, Pi) =
Then, with a hash function h(·): h(qi) = h(kj), we can get
Pi = {j : h(qi) = h(kj)}
In order to guarantee that the number of keys can uniqually
match with the number of quires, we need to ensure that
h(qi) = h(ki) where ki =
∥qi∥. During the hashing process,
some similar items may fall in different buckets because of the
hashing. The multi-round hashing provides an effective way
to overcome this issue. Suppose there is nr round, and each
round has different hash functions {h1, · · · , hnr}, so we have:
i where P g
i = {j : hg(qi) = hg(qj)}
Considering the batching case, we need to get the P L
i for each
i = {j : ⌊i
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
where m = 2l
nr . The last step is to calculate the LSH attention
score in parallel. With the formula (1) and (3), we can derive:
exp(z(i, P g
i ) −z(i, Pi))ag
exp(qi · kj −mg
i,j −z(i, P g
log |{g′ : j ∈P g′
D. Training Strategy
The task of the lung CT image segmentation is to predict
if each pixel of the given image belongs to a predeﬁned class
or the background. Therefore, the traditional medical image
segmentation problem comes to a binary pixel-wise classiﬁcation problem. However, in this study, we are focusing on the
multi-class image segmentation, which can be concluded as a
multi-classes pixel-wise classiﬁcation. Hence, we choose the
multi-class cross entropy as the loss function:
yo,c log(po,c)
where yo,c is a binary value which use to compare the correct
class c and observation class o, po,c is a probability of the
observation o to correct class c and M is the number of classes.
III. EXPERIMENT AND EVOLUTION RESULT
A. Data Description
We used COVID-19 CT images collected by Italian Society
of Medical and Interventional Radiology (SIRM)2 for our
experimental study. The dataset included 110 axial CT images collected from 60 patients. These images were reversely
intensity-normalized by taking RGB-values from the JPGimages from areas of air (either externally from the patient
or in the trachea) and fat (subcutaneous fat from the chest
wall or pericardial fat) and used to establish the uniﬁed
Houndsﬁeld Unit-scale (the air was normalized to -1000, fat
to -100). The ground-truth segmentation was done by a trained
radiologist using MedSeg3 with three labels: 1 = ground class
opaciﬁcation, 2 = consolidations, and 3 = pleural effusions. A
total of 100 samples that have both preprocessed CT images
and masks were used for our experimental analysis. These data
are publicly available4.
B. Data Preprocessing and Augmentation
The original CT images have the size of 512 × 512. We
use the opencv25 to convert the images into size of 369 ×
369 and grey scale. This processing is helpful to automatically
2 
3 
4 
5 
minimize the effects of the black frame in the images and some
random noises (e.g., words) on the segmentation.
As our model is based on deep learning, the number of
samples will affect the performance signiﬁcantly. Consider
about the size of the dataset, data augmentation is necessary
for training the neural network to achieve high generalizability.
Our study implements parameterized transformations to realize
data augmentation in this study. We rotate the existing images
90 degrees, 180 degrees and 270 degrees to generate another
300 examples. We can easily generate the corresponding mask
by rotating with the same degrees. Scaling have the some
property with the rotation, so we just scale the image to 0.5
and 1.5 separately to generate another 200 images and its
corresponding masks.
C. Experiments setting and Measure Metrics
For the model training, we use the Adma as the
optimizer. For a fair comparison, we train our model and the
U-Net with the default parameter in 100 epochs. Both models
are trained under data augmentation and non-augmentation
cases. We conducted the experimental analyses on our own
server consisting of two 12-core/ 24-thread Intel(R) Xeon(R)
CPU E5-2697 v2 CPUs, 6 NVIDIA TITAN X Pascal GPUs,
2 NVIDIA TITAN RTX, a total 768 GiB memory.
In a segmentation task, especially for the multi-class image
segmentation, the target area of interest may take a trivial part
of the whole image. Thus, we adopt the Dice Score, accuracy,
and precision as the measure metrics. The dice score is deﬁned
DSC(X, Y ) = 2|X ∩Y |
|X| + |Y |
where X, Y are two sets, and | · | calculates the number of
element in a set. Assume Y is the correct result of the test and
X is the predicted result. We conduct the experimental comparison based on a 10-fold cross-validation for performance
assessment.
D. Results
The ﬁgure 5 provides two examples about the result images
which have data augmentation. The table I shows the measure
metric for our proposed model and the U-Net in with data
augmentation case and no data augmentation case. Based on
TABLE I: Comparison of segmentation performance between
our proposed model and U-Net. All the values are the average
value based on the 10-fold cross-validation.
With Augmentation
No Augmentation
this table, we can easily ﬁnd that our proposed method is outperformed than U-Net which the improvement is at least 10%
in all three measure metrics. As shown in ﬁgure 2(h), we ﬁnd
that the original U-Net almost failed to do the segmentation.
The most possible reason is that, the range of interest is
very small, and the U-Net do not have enough capability to
distinguish those trivial difference.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
Fig. 5: Visualization of segmentation results. The images (a) and (e) show the preprocessed chest CT images of two scans.
The images (b) and (f) are the ground-truth masks for these two scans, where the yellow represents the consolidation, blue
represents pleural effusion and green corresponds to ground-glass opacities. The images (c) and (g) are the segmentation results
generated by our model where the blue represents the consolidation and brown represents the pleural effusion and sky-blue
for the ground-glass opacities. The images (d) and (h) are the outputs of the U-Net. In order to make the visualization clear,
we choose the light gray as the color for the background segment.
E. Ablation Study
In addition to the above-mentioned results, we are also
interested in the effectiveness of each component in the
proposed model. Accordingly, we conduct the ablation study
about the ResNeXt and Attention separately to investigate how
these components would affect the segmentation performance.
To ensure a fair experimental comparison, we conduct the
ablation study in the exactly same experiment environment
with our main experiments presented in section III-C. We
implement the ablation study on two variants of our model:
Model without Attention and Model without ResNeXt. Our
model without ResNeXt is similar with literature . We
just use the M-R to represent it. The results are summarized
in Table II, where M-A represents the model without attention
and M-R represents the model without ResNeXt block. We
can observe that both the attention and ResNeXt blocks play
important roles in our model and contribute to derive improved
segmentation performance in comparison with U-Net.
IV. DISCUSSION AND CONCLUSIONS
Up to now, the most common screening tool for COVID-
19 is the CT imaging. It can help community to accelerate
the speed of diagnose and accurately evaluate the severity of COVID-19 . In this paper, we presented a novel
deep learning-based algorithm for automated segmentation of
TABLE II: Comparison result of ablation study. All the values
are the average value based on the 10-fold cross-validation.
With Augmentation
No Augmentation
COVID-19 CT images, which is proved to be plausible and
superior comparing to a series of baselines. We proposed
a modiﬁed U-Net model by exploiting residual network to
enhance the feature extraction. An efﬁcient attention mechanism was further embedded into the decoding process to
generate the high-quality multi-class segmentation results. Our
method gained more than 10% improvement in multi-class
segmentation when comparing against U-Net and a set of
baselines.
Recent study shows that the early detection of the COVID-
19 is very important . If the infection in chest CT image
can be detected at early stage, the patients would have the
higher chance to survive . Our study provides an effective
tool for the radiologist to precisely determine the lung’s infection percentage and diagnose the progression of COVID-19. It
also shed some light on how deep learning can revolutionize
the diagnosis and treatment in the midst of COVID-19.
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
Our future work would be generalizing the proposed model
into a wider range of practical scenarios, such as facilitating
with diagnosing more types of diseases from CT images.
In particularly, in the case of a new disease, such as the
coronavirus, the amount of ground truth data is usually limited
given the difﬁculty of data acquisition and annotation. The
model is capable of generalizing and adapting itself usingonly
a few available ground-truth samples. A knowledge-based
generative model will be integrated to enhance the ability
in handling new tasks. Another line of future work lies in the
interpretability, which is specially critical for the medical domain applications. Although deep learning is widely accepted
to its limitation in interpretability, the attention mechanism we
proposed in this work can produce the interpretation of internal
decision process at some levels. To gain deeper scientiﬁc
insights, we will keep working along with this direction and
explore the hybrid attention model for generating meaningfully
semantic explanations.