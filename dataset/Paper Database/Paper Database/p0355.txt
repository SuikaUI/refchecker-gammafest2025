Learning View-invariant Sparse Representations for Cross-view Action
Recognition
Jingjing Zheng†, Zhuolin Jiang§
†University of Maryland, College Park, MD, USA
§Noah’s Ark Lab, Huawei Technologies
 , 
We present an approach to jointly learn a set of viewspeciﬁc dictionaries and a common dictionary for crossview action recognition. The set of view-speciﬁc dictionaries is learned for speciﬁc views while the common dictionary is shared across different views. Our approach represents videos in each view using both the corresponding
view-speciﬁc dictionary and the common dictionary. More
importantly, it encourages the set of videos taken from different views of the same action to have similar sparse representations. In this way, we can align view-speciﬁc features in the sparse feature spaces spanned by the viewspeciﬁc dictionary set and transfer the view-shared features
in the sparse feature space spanned by the common dictionary. Meanwhile, the incoherence between the common
dictionary and the view-speciﬁc dictionary set enables us
to exploit the discrimination information encoded in viewspeciﬁc features and view-shared features separately. In
addition, the learned common dictionary not only has the
capability to represent actions from unseen views, but also
makes our approach effective in a semi-supervised setting
where no correspondence videos exist and only a few labels
exist in the target view. Extensive experiments using the
multi-view IXMAS dataset demonstrate that our approach
outperforms many recent approaches for cross-view action
recognition.
1. Introduction
Action recognition has many potential applications in
multimedia retrieval, video surveillance and human computer interaction. In order to accurately recognize human
actions, most existing approaches focus on developing different discriminative features, such as spatio-temporal interest point (STIP) based features , shape and optical ﬂow based features . These
features are effective for recognizing actions taken from
similar viewpoints, but perform poorly when viewpoints
vary signiﬁcantly. Extensive experiments in have
This research was supported by a MURI from the US Ofﬁce of Naval
Research under the Grant N00014-10-1-0934.
Source View
Target View
view-shared
view-specific
Figure 1. Joint learning of a view-speciﬁc dictionary pair and
a common dictionary. We not only learn a common dictionary
D to model view-shared features of corresponding videos in both
views, but also learn two view-speciﬁc dictionaries Ds and Dt that
are incoherent to D to align the view-speciﬁc features. The sparse
representations (x1 and x2, z1 and z2) share the same sparsity
patterns (selecting the same items).
shown that failing to handle feature variations caused by
viewpoints may yield inferior results. This is because the
same action looks quite different from different viewpoints
as shown in Figure 1. Thus action models learned from one
view become less discriminative for recognizing actions in
a much different view.
A very fruitful line of work for cross-view action recognition based on transfer learning is to construct the mappings or connections between different views, by using
videos taken from different views of the same action . exploited the frame-to-frame correspondence
in pairs of videos taken from two views of the same action by transferring the split-based features of video frames
in the source view to the corresponding video frames in
the target view. proposed to exploit the correspondence between the view-dependent codebooks constructed
by k-means clustering on videos in each view.
However, the frame-to-frame correspondence is computationally expensive, and the codebook-to-codebook correspondence is not accurate enough to guarantee that a
pair of videos observed in the source and target views will
have similar feature representations.
In order to overcome these drawbacks, we propose a dictionary learning framework to exploit the video-to-video
correspondence by encouraging pairs of videos taken in two
2013 IEEE International Conference on Computer Vision
1550-5499/13 $31.00 © 2013 IEEE
DOI 10.1109/ICCV.2013.394
2013 IEEE International Conference on Computer Vision
1550-5499/13 $31.00 © 2013 IEEE
DOI 10.1109/ICCV.2013.394
views to have similar sparse representations. Figure 1 illustrates our dictionary learning framework. Our approach
not only learns a common dictionary shared by different
views to model the view-shared features, but also learns
a dictionary pair corresponding to the source and target
views to model and align view-speciﬁc features in the two
views. Both the common dictionary and the corresponding
view-speciﬁc dictionary are used to represent videos in each
view. Instead of transferring the split-features as in , we
transfer the indices of the non-zero elements (i.e., the indices of selected dictionary items) in sparse codes of videos
from the source view to sparse codes of the corresponding
videos from the target view. In other words, we not only
use the same subset of dictionary items from the common
dictionary to represent view-shared features in correspondence videos from different views, but also use the same
subset of dictionary items from different view-speciﬁc dictionaries to represent view-speciﬁc features. In this way,
videos across different views of the same action tend to have
similar sparse representations. Note that our approach enforces the common dictionary to be incoherent with viewspeciﬁc dictionaries, so that the discrimination information
encoded in view-speciﬁc features and view-shared features
are exploited separately and makes view-speciﬁc dictionaries more compact.
Actions are categorized into two types: shared actions
observed in both views and orphan actions that are only observed in the source view. Note that only pairs of videos
taken from two views of the shared actions are used for
dictionary learning. In addition, we consider two scenarios for the shared actions: (1) shared actions in both views
are unlabeled. (2) shared actions in both views are labeled.
These two scenarios are referred to as unsupervised and supervised settings, respectively, in subsequent discussions.
1.1. Contributions
The main contributions of this paper are:
• We propose to simultaneously learn a set of viewspeciﬁc dictionaries to exploit the video-level correspondence across views and a common dictionary to
model the common patterns shared by different views.
• The incoherence between the common dictionary and
the view-speciﬁc dictionaries enables our approach to
drive the shared pattern to the common dictionary and
focus on exploiting the discriminative correspondence
information encoded by the view-speciﬁc dictionaries.
• With the separation of the common dictionary, our
approach not only learns more compact view-speciﬁc
dictionaries, but also bridges the gap of the sparse representations of correspondence videos taken from different views of the same action using a more ﬂexible
• Our framework is a general approach and can be applied to cross-view and multi-view action recognition
under both unsupervised and supervised settings.
2. Related Work
Recently, several transfer learning techniques have been
proposed for cross-view action recognition .
Speciﬁcally, proposed to generate the same split-based
features for correspondence video frames from both the
source and target views. It is computationally expensive
because it requires the construction of feature-to-feature
correspondence at the frame-level and learning an additional mapping from original features to the split-based
features. used a bipartite graph to model the relationship between two view-dependent codebooks.
though this approach exploits the codebook-to-codebook
correspondence between two views, it can not guarantee
that videos taken at different views of shared actions will
have similar features. used canonical correlation analysis to derive a correlation subspace as a joint representation from different bag-of-words models at different views
and incorporate a corresponding correlation regularizer into
the formulation of support vector machine. proposed a
dictionary learning framework for cross-view action recognition with the assumption that sparse representations of
videos from different views of the same action should be
strictly equal. However, this assumption is too strong to
ﬂexibly model the relationship between different views.
Many view-invariant approaches that use 2D image data
acquired by multiple cameras have also been proposed. proposed view-invariant representations based on
view-invariant canonical body poses and trajectories in 2D
invariance space. captured the structure of temporal
similarities and dissimilarities within an action sequence using a Self-Similarity Matrix. proposed a view-invariant
matching method based on epipolar geometry between actor silhouettes without tracking and explicit point correspondences. learned two view-speciﬁc transformations
for the source and target views, and then generated a sequence of linear transformations of action descriptors as the
virtual views to connect two views. proposed the Hankel matrix of a short tracklet which is a view-invariant feature to recognize actions across different viewpoints.
Another fruitful line of work for cross-view action recognition concentrates on using the 3D image data. The method
introduced in employed three dimensional occupancy
grids built from multi-view points to model actions. 
developed a 4D view-invariant action feature extraction to
encode the shape and motion information of actors observed
from multiple views. Both of these approaches lead to computationally intense algorithms because they need to ﬁnd
the best match between a 3D model and a 2D observation
over a large model parameter space. developed a robust
and view-invariant hierarchical classiﬁcation method based
on 3D HOG to represent a test sequence.
3. Learning View-invariant Sparse Representations via Dictionary Learning
3.1. Unsupervised Learning
In the unsupervised setting , our goal is to ﬁnd viewinvariant feature representations by making use of correspondence between videos of the shared actions taken from
different views. Let Y v = [yv
1, ..., yv
N] ∈Rd×N denote
d-dimensional feature representations of N videos of the
shared actions taken in the v-th view. Yi = [y1
i , ..., yV
are V action videos of the shared action yi taken from
V views, which are referred to as correspondence videos.
On one hand, we would like to learn a common dictionary
D ∈Rd×J with a size of J shared by different views to
represent videos from all views. On the other hand, for each
view, we learn Dv ∈Rd×Jvto model the view-speciﬁc features. The objective function for the unsupervised setting
+ λ||Xi||2,1 + λ||Zi||2,1} + η
||DT Dv||2
where Xi = [x1
i , ..., xV
i ], Zi = [z1
i , ..., zV
i ] are the joint
sparse representations for yi across V views. This objective
function consists of ﬁve terms:
1. The ﬁrst two terms are the reconstruction errors of
videos from different views using D only or using both
D and Dv. The minimization of the ﬁrst reconstruction error enables D to encode view-shared features as
much as possible while the minimization of the second
reconstruction error enables Dv to encode and align
view-speciﬁc features that can not be modeled by D.
2. The third and fourth terms are the sparse representations via L2,1-norm regularization using D and Dv respectively. The L2,1-norm minimization for X and Z
can make the entries in each row of the two matrices
to be all zeros or non-zeros at the same time. This
means that we not only encourage to use the same subset of dictionary items in D to represent the correspondence videos from different views, but also encourage
to use dictionary items from Dv with the same index of
selected dictionary items to further reduce the reconstruction error of videos in each view. Therefore the
testing videos taken from different views of the same
action will be encouraged to have similar sparse representations when using the learned D and Dv.
3. The last term regularizes the common dictionary to be
incoherent to the view-speciﬁc dictionaries. The incoherence between D and Dv enables our approach to
separately exploit the discriminative information encoded in the view-speciﬁc features and view-shared
3.2. Supervised Learning
Given the action categories of correspondence videos,
we can learn a discriminative common dictionary and discriminative views-speciﬁc dictionaries by leveraging the
category information. We partition the dictionary items in
each dictionary into disjoint subsets and associate each subset with one speciﬁc class label. For videos from action
class k, we aim to represent them using the same subset of
dictionary items associated with class k. For videos from
different classes, we represent them using disjoint subsets
of dictionary items. This is supported by the intuition that
action videos from the same class tend to have the similar
features and each action video can be well represented by
other videos from the same class . We incorporate the
discriminative sparse code error term introduced in to
achieve this goal.
Assume there are K
shared action classes,
D = [D1, ..., DK], Dv = [Dv
1, ..., Dv
K] where Dk ∈
k=1 Jk = J, and Dv
the objective function for the supervised setting is:
+ ||qi −Axv
2} + λ||Xi||2,1
+ λ||Zi||2,1} + η
||DT Dv||2
[qi1, ..., qiK]T
RJ×1 and qv
i1, ..., qv
iK]T ∈RJv×1 called ‘discriminative’ sparse coefﬁcients associated with D and Dv respectively. When a
i is from class k at the v-th view, then qik and qv
are ones and other entries in qi and qv
i are zeros. A ∈RJ×J
and B ∈RJv×Jv are called transformation matrices which
transform xv
i to approximate qi and qv
i respectively.
The discriminative sparse-code error terms ||qi −Axv
2 encourage the dictionary items with class
k to be selected to reconstruct those videos from class k.
Note that the L2,1-norm regularization only regularize the
relationship between the sparse codes of correspondence
videos, but can not regularize the relationship between the
sparse codes of videos from the same action class in each
view. The integration of discriminative sparse code error
term in the objective function can address this issue. In
other words, our approach not only encourages the videos
taken from different views of the same action to have similar sparse representations, but also encourages videos from
the same class in each view to have similar sparse representations.
3.3. Optimization
Here we only describe the optimization of the objective function in (2) while the optimization of (1) utilizes
the similar procedure except that A and B components are
excluded. This optimization problem is divided into three
subproblems: (1) computing sparse codes with ﬁxed Dv, D
and A, B; (2) updating Dv, D with ﬁxed sparse codes and
A, B; (3) updating A, B with ﬁxed Dv, D and sparse codes.
3.4. Computing Sparse Codes
Given ﬁxed Dv, D and A, B, we solve the sparse coding
problem of the correspondence videos set by set and (2) is
reduced to:
2 + ||qi −Axv
2} + λ||Xi||2,1 + λ||Zi||2,1}.
We rewrite (3) as follows:
2 + λ|| ˜Zi||2,1
i , ..., ˜zV
i ] and O1
RJ×Jv, O3 ∈RJv×J are matrices of all zeros. The minimization of (4) is known as a multi-task group lasso problem where each view is treated as a task. We use the
software SLEP in for computing sparse codes.
3.5. Updating Dictionaries
Given ﬁxed sparse codes and A, B, (2) is reduced to:
||DT Dv||2
We rewrite (5):
v=1{||Y v −DXv||2
F + ||Y v −
DXv −DvZv||2
F } + η V
v=1 ||DT Dv||2
F where Y v =
1, ..., yv
N], Xv = [xv
1, ..., xv
N], Zv = [zv
1, ..., zv
N]. Motivated by , we ﬁrst ﬁx Dv and then update D =
[d1, ..., dJ] atom by atom, i.e.
updating dj while ﬁxing
other column atoms in D. Speciﬁcally, let ˆY v = Y v −
(m) where xv
(m) corresponds to the m-th row
of Xv, we solve the following problem for updating dj in
D: arg mindj f(dj) = V
v=1{|| ˆY v −djxv
F + || ˆY v −
DvZv −djxv
F . Let the ﬁrst-order derivative of f(dj) with respect to dj equal to zero, i.e. ∂f(dj)
0, then we can update dj as:
2DvDvT )−1(2 ˆY v −DvZv)xvT
Now we ﬁx D and update Dv atom by atom. Each item dv
in Dv is updated as :
2DDT )−1 ¯Y vzvT
where ¯Y v = Y v −DXv −
3.6. Updating A, B
Given sparse codes and all the dictionaries, we employ
the multivariate ridge regression model to update A, B
with the quadratic loss and l2 norm regularization:
2 + λ1||A||2
2 + λ2||B||2
which yields the following solutions:
XvXvT + λ1I)−1,
Q = [q1, ..., qN], X = [x1, ..., xN],
ZvZvT + λ2I)−1,
1, ..., qv
N], Zv = [zv
1, ..., zv
Algorithm 1 summarizes our approach. The algorithm
converged after a few iterations in our experiments.
4. Experiments
We evaluated our approach for both cross-view and
multi-view action recognition on the IXMAS multi-view
dataset . This dataset contains 11 actions performed
three times by ten actors taken from four side views and
one top view. Figure 3 shows some example frames. We
follow the experiment setting in for extracting the local
STIP feature . We ﬁrst detect up to 200 interest points
from each action video and then extract a 100-dimensional
gradient-based descriptors around these interest points via
PCA. Then these interest points-based descriptors are clustered into 1000 visual words by k-mean clustering and
Algorithm 1 Learning View-invariant Sparse Representations for Cross-view Action Recognition
1: Input: Y v = [Y v
1 , ..., Y v
K], Q, Qv, v = 1, ..., V, λ, η
2: Initialize D and Dv
3: for k = 1 →K do
Initialize
class-speciﬁc
dictionary
arg minDk,αk ||[Y 1
k ] −Dkαk||2
F + λ||αk||1
Initialize class-speciﬁc dictionary Dv
by solving Dv
F + λ||βk||1
6: end for
Compute sparse codes xv
i of a set of correspondence videos yv
i by solving the multi-task group LASSO problem in (4) using the SLEP 
Update each atom dj in D and dv
j in Dv using (6) and (7) respectively
Update transformation matrices A, B using (8)
11: until convergence or certain rounds
12: Output: D = [D1, ..., DK], Dv = [Dv
1 , ..., Dv
each action video is represented by a 1000-dimensional histogram. For the global feature, we extract shape-ﬂow descriptors introduced in and learn a codebook of size
500 by k-means clustering on these shape-ﬂow descriptors.
Similarly, this codebook is used to encode shape-ﬂow descriptors and each action video is represented by a 500dimensional histogram. Then the local and global feature
descriptors are concatenated to form a 1500-dimensional
descriptor to represent an action video.
For fair comparison to , we use three evaluation modes: (1) unsupervised correspondence mode; (2) supervised correspondence mode ; (3) partially labeled mode.
For the ﬁrst two correspondence mode, we use the leaveone-action-class-out strategy for choosing the orphan action which means that each time we only consider one action class for testing in the target view. And all videos of
the orphan action are excluded when learning the quantized
visual words and constructing dictionaries. The only difference between the ﬁrst and the second mode is whether
the category labels of the correspondence videos are available or not. For the third mode, we follow to consider
a semi-supervised setting where a small portion of videos
from the target view is labeled and no matched correspondence videos exist. From this we want to show that our
framework can be applied to the domain adaptation problem. Two comparing methods for the third mode are two
types of SVMs used in . The ﬁrst one is AUGSVM,
which creates a feature-augmented version of each individual feature as the new feature. The second one is MIXSVM
which trains two SVM’s on the source and target views and
learns an optimal linear combination of them.
Note that the test actions from the source and target
views are not seen during dictionary learning whereas the
test action can be seen in the source view for classiﬁer training in the ﬁrst two evaluation modes. On the contrary, the
test action from different views can be seen during both dictionary learning and classiﬁer training in the third mode.
For all modes, we report the classiﬁcation accuracy by av-
Figure 3. Exemplar frames from the IXMAS multi-view
dataset. Each row shows one action viewed across different angles.
eraging the results over different combinations of selecting
orphan actions.
4.1. Beneﬁts of the Separation of the Common and
View-speciﬁc Dictionaries
In this section, we demonstrate the beneﬁts of the separation of the common and view-speciﬁc dictionaries. For visualization purpose, two action classes ”check-watch” and
”waving” taken by Camera0 and Camera2 from the IX-
MAS dataset was selected to construct a simple cross-view
dataset. We extract the shape descriptor for each video
frame and learn a common dictionary and two-view speciﬁc
dictionaries using our approach. We then reconstruct a pair
of frames taken from Camera0 and Camer2 views of the action ”waving” using two methods. The ﬁrst one is to use the
common dictionary only to reconstruct the frame pair. The
other one is use both the common dictionary and the viewspeciﬁc dictionary for reconstruction. Figure 2(b) shows the
original shape feature and the reconstructed shape features
of two frames of action ”waving” from two seen views and
one unseen view using the mentioned two methods. First,
comparing dictionary items in D and {Ds, Dt}, we see that
some items in D mainly encode the body and body outline
which are just shared by frames of the same action from two
view while items in {Ds, Dt} mainly encode different arm
poses that reﬂects the class information in the two views.
It demonstrates that the common dictionary has the ability
to exploit view-shared features from different views. Second, it can be observed that better reconstruction is achieved
by using both the common dictionary D and view-speciﬁc
dictionaries. This is because the common dictionary may
not reconstruct the more detailed view-speciﬁc features well
such as arm poses. The separation of the common dictionary enables the view-speciﬁc dictionaries to focus on exploiting and aligning view-speciﬁc features from different
views. Third, from the last row in Figure 2(b), we ﬁnd that
a good reconstruction of an action frame taken from the unseen view can be achieved by using the common dictionary
only. It demonstrates that the common dictionary learned
from two seen views has the capability to represent videos
of the same action from an unseen view. Moreover, two
(a) Visualization of all dictionary items from the common and view-speciﬁc dictionaries.
23.8769 15.4577 13.9989
-0.9751 0.3478 -0.5108 0.3478 -0.3326
-1.2799 -0.6488 0.6649 0.3771 -0.2953
-18.5509 16.2382 13.2468
23.8769 10.8952 -10.8353
Original recon1
-0.8508 -0.0350 0.3508 0.3243 -0.3089
(b) Reconstruction of shape features of action ”waving” from two seen views and one unseen view.
Figure 2. Illustration of the beneﬁts of the common dictionary. (a) Visualization of all dictionary atoms in D (green color), Ds (red
color) and Dt (purple color). (b) Figures from 2 ∼5 columns show the reconstruction result using D only. Figures from 6 ∼11 columns
show the reconstruction result using {D, Ds}, {D, Dt} and {D, Ds, Dt} respectively. Only at most top-3 dictionary items are shown.
(77.6, 79.9, 81.8, 99.1) (69.4, 76.8, 88.1, 90.9) (70.3, 76.8, 87.5, 88.7) (44.8, 74.8, 81.4, 95.5)
C1 (77.3, 81.2, 87.5, 97.8)
(73.9, 75.8, 82.0, 91.2) (67.3, 78.0, 92.3, 78.4) (43.9, 70.4, 74.2, 88.4)
C2 (66.1, 79.6, 85.3, 99.4) (70.6, 76.6, 82.6, 97.6)
(63.6, 79.8, 82.6, 91.2) (53.6, 72.8, 76.5, 100.0)
C3 (69.4, 73.0, 82.1, 87.6) (70.0, 74.4, 81.5, 98.2) (63.0, 66.9, 80.2, 99.4)
(44.2, 66.9, 70.0, 95.4)
C4 (39.1, 82.0, 78.8, 87.3) (38.8, 68.3, 73.8, 87.8) (51.8, 74.0, 77.7, 92.1) (34.2, 71.1, 78.7, 90.0)
Ave. (63.0, 79.0, 83.4, 93.0) (64.3, 74.7, 79.9, 95.6) (64.5, 75.2, 82.0, 93.4) (58.9, 76.4, 85.3, 87.1) (46.6, 71.2, 75.5, 95.1)
Table 1. Cross-view action recognition accuracies of different approaches on the IXMAS dataset using unsupervised correspondence
mode. Each row corresponds to a source (training) view and each column a target (test) view. The four accuracy numbers in the bracket
are the average recognition accuracies of , , and our unsupervised approach respectively.
(79, 98.5)
(79, 99.7)
(68, 99.7)
(76, 99.7)
(72, 100.0)
(74, 97.0)
(70, 89.7)
(66, 100.0)
(71, 99.1)
(82, 99.3)
(76, 100.0)
(72, 99.7)
(75, 90.0)
(75, 99.7)
(73, 98.2)
(76, 96.4)
(80, 99.7)
(73, 95.7)
(73, 100.0)
(79, 98.5)
(74, 97.2)
(77, 98.3)
(76, 98.7)
(73, 97.0)
(72, 98.9)
Table 2. Cross-view action recognition accuracies of different approaches on the IXMAS dataset using supervised correspondence
mode. Each row corresponds to a source (training) view and each column a target (test) view. The accuracy numbers in the bracket are the
average recognition accuracies of and our supervised approach respectively.
(42.8, 36.8, 63.6, 64.9) (45.2, 46.8, 60.0, 64.1) (47.2, 42.7, 61.2, 67.1) (30.5, 36.7, 52.6, 65.5)
C1 (44.1, 39.4, 61.0, 63.6)
(43.5, 51.8, 62.1, 60.2) (47.1, 45.8, 65.1, 66.7) (43.6, 40.2, 54.2, 66.8)
C2 (53.7, 49.1, 63.2, 65.4) (50.5, 49.4, 62.4, 63.2)
(53.5, 45.0, 71.7, 67.1) (39.1, 46.9, 58.2, 65.9)
C3 (46.3, 39.3, 64.2, 65.4) (42.5, 42.5, 71.0, 61.9) (48.8, 51.2, 64.3, 65.4)
(37.5, 38.9, 56.6, 61.6)
C4 (37.0, 40.3, 50.0, 65.8) (35.0, 42.5, 59.7, 62.7) (44.4, 40.4, 60.7, 64.5) (37.2, 40.7, 61.1, 61.9)
Ave. (45.3, 42.6, 59.6, 65.0) (42.7, 42.8, 64.2, 63.2) (45.4, 47.5, 61.9, 63.5) (46.2, 43.5, 64.8, 65.7) (37.6, 40.7, 55.4, 65.0)
Table 3. Cross-view action recognition accuracies of different approaches on the IXMAS dataset using partially labeling mode. Each
row corresponds to a source (training) view and each column a target (test) view. The accuracy numbers in the bracket are the average
recognition accuracies of AUGSVM, MIXSVM from , , and our approach respectively.
methods have nearly the same reconstruction performance
for frames of the same action from the unseen view. This
is because {Ds, Dt} are learned by exploiting features that
are speciﬁc for the two seen views. In addition, the separation of the common dictionary and view-speciﬁc dictionaries can enable us to learn more compact view-speciﬁc
dictionaries.
4.2. Cross-view Action Recognition
We evaluate our approach using three different modes.
We ﬁrst learn a common dictionary D and two view-speciﬁc
dictionaries {Ds, Dt} corresponding to the source and target views respectively. Both D and Ds are used to represent
the training videos in the source view. Similarly, for a test
video y in the target view, we encode it over ˆD = [D Dt],
β = arg minβ ||y −ˆDβ||2
2 + λ0||β||1 where λ0 is
a parameter to balance the reconstruction error and sparsity. For the ﬁrst two modes, a k-NN classiﬁer is used to
classify the test video in the sparse feature space. For the
third mode, we use SRC method to predict the label
of y, i.e. k∗= arg mink ||y −ˆDkβk||2
2 + λ0||βk||1 where
ˆDk = [Dk Dt
k] and βk is the associated sparse codes.
As shown in Tables 1 and 2, our approach yields a much
better performance for all 20 combinations for the ﬁrst two
modes. Moreover, the proposed approach achieves more
than 90% recognition accuracy for most combinations. The
higher recognition accuracy obtained by our supervised setting over our unsupervised setting demonstrates that the dictionaries learned using labeled information across views are
more discriminative.
For the partially labeled mode, our approach outperforms other approaches for most of source-target combinations in Table 3. It is interesting to note that for the case
where Camera4 is the source or target view, the recognition accuracies of comparing approaches are a little lower
than other combinations of piecewise views. This is because the Camera4 was set above the actors and different
actions look very similarly from the top view. However, our
approach still achieves a very high recognition accuracy for
these combinations, which further demonstrates the effectiveness of our approach.
Ours (mode1)
Ours (mode2)
 (mode1)
 (mode2)
Table 4. Multi-view action recognition results using the unsupervised and supervised correspondence modes. Each column
corresponds to one target view.
Ours (mode3)
Table 5. Multi-view action recognition results using the partially labeled mode. Each column corresponds to one target view.
4.3. Multi-view Action Recognition
We select one camera as a target view and use all other
four cameras as source views to explore the beneﬁts of combining multiple source views. Here we use the same classiﬁcation scheme used for cross-view action recognition.
Both D and the set of correspondence dictionaries Dv are
learned by aligning the sparse representations of shared action videos across all views. Since videos from all views are
aligned into a common view-invariant sparse feature space,
we do not need to differentiate the training videos from each
source view in this common view-invariant sparse feature
Table 4 shows the average accuracy of the proposed approach for the ﬁrst two evaluation modes. Note that the
comparing approaches are evaluated using the unsupervised
correspondence mode. Both our unsupervised and supervised approaches outperform other comparing approaches
and achieve nearly perfect performance for all target views.
Furthermore, and our unsupervised approach only
use training videos from four source views to train a classi-
ﬁer while other approaches used all the training videos from
all ﬁve views to train the classiﬁer. Table 5 shows the av-
erage accuracy of different approaches using the partially
labeled evaluation mode. The proposed approach outperforms on four out of ﬁve target views. Overall, we
accomplish a comparable accuracy with under the partially labeled mode.
5. Conclusion
We presented a novel dictionary learning framework to
learn view-invariant sparse representations for cross-view
action recognition. We propose to simultaneously learn a
common dictionary to model view-shared features and a set
of view-speciﬁc dictionaries to align view-speciﬁc features
from different views. Both the common dictionary and the
corresponding view-speciﬁc dictionary are used to represent videos from each view. We transfer the indices of nonzeros in the sparse codes of videos from the source view to
the sparse codes of the corresponding videos from the target view. In this way, the mapping between the source and
target views is encoded in the common dictionary and viewspeciﬁc dictionaries. Meanwhile, the associated sparse representations are view-invariant because non-zero positions
in the sparse codes of correspondence videos share the same
set of indices. Our approach can be applied to cross-view
and multi-view action recognition under unsupervised, supervised and domain adaptation settings.