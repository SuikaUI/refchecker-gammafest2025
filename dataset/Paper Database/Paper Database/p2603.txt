FEDformer: Frequency Enhanced Decomposed Transformer for Long-term
Series Forecasting
Tian Zhou * 1 Ziqing Ma * 1 Qingsong Wen 1 Xue Wang 1 Liang Sun 1 Rong Jin 1
Although Transformer-based methods have signiﬁcantly improved state-of-the-art results for
long-term series forecasting, they are not only
computationally expensive but more importantly,
are unable to capture the global view of time
series (e.g.
overall trend).
To address these
problems, we propose to combine Transformer
with the seasonal-trend decomposition method,
in which the decomposition method captures the
global proﬁle of time series while Transformers
capture more detailed structures. To further enhance the performance of Transformer for longterm prediction, we exploit the fact that most
time series tend to have a sparse representation
in well-known basis such as Fourier transform,
and develop a frequency enhanced Transformer.
Besides being more effective, the proposed
method, termed as Frequency Enhanced Decomposed Transformer (FEDformer), is more efﬁcient than standard Transformer with a linear
complexity to the sequence length. Our empirical studies with six benchmark datasets show that
compared with state-of-the-art methods, FEDformer can reduce prediction error by 14.8% and
22.6% for multivariate and univariate time series, respectively. Code is publicly available at
 
1. Introduction
Long-term time series forecasting is a long-standing challenge in various applications (e.g., energy, weather, traf-
ﬁc, economics). Despite the impressive results achieved
by RNN-type methods , they often suffer from the problem of gradi-
*Equal contribution
1Machine Intelligence Technology, Alibaba Group.. Correspondence to: Tian Zhou < >, Rong Jin < >.
Proceedings of the 39 th International Conference on Machine
Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).
ent vanishing or exploding , signiﬁcantly limiting their performance. Following the recent success in NLP and CV community ,
Transformer has been introduced to
capture long-term dependencies in time series forecasting
and shows promising results . Since high computational complexity and memory
requirement make it difﬁcult for Transformer to be applied
to long sequence modeling, numerous studies are devoted
to reduce the computational cost of Transformer .
overview of this line of works can be found in Appendix
Despite the progress made by Transformer-based methods for time series forecasting, they tend to fail in capturing the overall characteristics/distribution of time series in some cases.
In Figure 1, we compare the time
series of ground truth with that predicted by the vanilla
Transformer method in a real-world
ETTm1 dataset . It is clear that the predicted time series shared a different distribution from that
of ground truth. The discrepancy between ground truth
and prediction could be explained by the point-wise attention and prediction in Transformer. Since prediction for
each timestep is made individually and independently, it
is likely that the model fails to maintain the global property and statistics of time series as a whole. To address
this problem, we exploit two ideas in this work. The ﬁrst
idea is to incorporate a seasonal-trend decomposition approach , which is
widely used in time series analysis, into the Transformerbased method. Although this idea has been exploited before , we present a
special design of network that is effective in bringing the
distribution of prediction close to that of ground truth, according to Kologrov-Smirnov distribution test. Our second
idea is to combine Fourier analysis with the Transformerbased method. Instead of applying Transformer to the time
domain, we apply it to the frequency domain which helps
Transformer better capture global properties of time series.
Combining both ideas, we propose a Frequency Enhanced
Submission and Formatting Instructions for ICML 2022
Decomposition Transformer, or, FEDformer for short, for
long-term time series forecasting.
One critical question with FEDformer is which subset of
frequency components should be used by Fourier analysis
to represent time series. A common wisdom is to keep lowfrequency components and throw away the high-frequency
ones. This may not be appropriate for time series forecasting as some of trend changes in time series are related to important events, and this piece of information could be lost
if we simply remove all high-frequency components. We
address this problem by effectively exploiting the fact that
time series tend to have (unknown) sparse representations
on a basis like Fourier basis. According to our theoretical
analysis, a randomly selected subset of frequency components, including both low and high ones, will give a better
representation for time series, which is further veriﬁed by
extensive empirical studies. Besides being more effective
for long term forecasting, combining Transformer with frequency analysis allows us to reduce the computational cost
of Transformer from quadratic to linear complexity. We
note that this is different from previous efforts on speeding
up Transformer, which often leads to a performance drop.
In short, we summarize the key contributions of this work
as follows:
1. We propose a frequency enhanced decomposed Transformer architecture with mixture of experts for
seasonal-trend decomposition in order to better capture global properties of time series.
2. We propose Fourier enhanced blocks and Wavelet
enhanced blocks in the Transformer structure that
allows us to capture important structures in time
series through frequency domain mapping.
serve as substitutions for both self-attention and crossattention blocks.
3. By randomly selecting a ﬁxed number of Fourier components, the proposed model achieves linear computational complexity and memory cost. The effectiveness
of this selection method is veriﬁed both theoretically
and empirically.
4. We conduct extensive experiments over 6 benchmark
datasets across multiple domains (energy, trafﬁc, economics, weather and disease).
Our empirical studies show that the proposed model improves the performance of state-of-the-art methods by 14.8% and
22.6% for multivariate and univariate forecasting, respectively.
Figure 1. Different distribution between ground truth and forecasting output from vanilla Transformer in a real-world ETTm1
dataset. Left: frequency mode and trend shift. Right: trend shift.
2. Compact Representation of Time Series in
Frequency Domain
It is well-known that time series data can be modeled from
the time domain and frequency domain. One key contribution of our work which separates from other long-term forecasting algorithms is the frequency-domain operation with
a neural network. As Fourier analysis is a common tool
to dive into the frequency domain, while how to appropriately represent the information in time series using Fourier
analysis is critical. Simply keeping all the frequency components may result in inferior representations since many
high-frequency changes in time series are due to noisy inputs. On the other hand, only keeping the low-frequency
components may also be inappropriate for series forecasting as some trend changes in time series represent important events. Instead, keeping a compact representation of
time series using a small number of selected Fourier components will lead to efﬁcient computation of transformer,
which is crucial for modelling long sequences. We propose to represent time series by randomly selecting a constant number of Fourier components, including both highfrequency and low-frequency. Below, an analysis that justi-
ﬁes the random selection is presented theoretically. Empirical veriﬁcation can be found in the experimental session.
X1(t), . . . , Xm(t).
applying Fourier transform
to each time series, we turn each Xi(t) into a vector ai
(ai,1, . . . , ai,d)⊤
By putting all
the Fourier transform vectors into a matrix, we have
A = (a1, a2, . . . , am)⊤∈Rm×d, with each row corresponding to a different time series and each column
corresponding to a different Fourier component. Although
using all the Fourier components allows us to best preserve
the history information in the time series, it may potentially
lead to overﬁtting of the history data and consequentially
a poor prediction of future signals.
Hence, we need to
select a subset of Fourier components, that on the one hand
should be small enough to avoid the overﬁtting problem
and on the other hand, should be able to preserve most
of the history information.
Here, we propose to select
s components from the d Fourier components (s < d)
Submission and Formatting Instructions for ICML 2022
FEDformer Encoder
FEDformer Decoder
Trend Init
en (or X l
de ∈R(I/2+O)×D
de ∈R(I/2+O)×D
de (or X l
Figure 2. FEDformer Structure. The FEDformer consists of N encoders and M decoders. The Frequency Enhanced Block (FEB, green
blocks) and Frequency Enhanced Attention (FEA, red blocks) are used to perform representation learning in frequency domain. Either
FEB or FEA has two subversions (FEB-f & FEB-w or FEA-f & FEA-w), where ‘-f’ means using Fourier basis and ‘-w’ means using
Wavelet basis. The Mixture Of Expert Decomposition Blocks (MOEDecomp, yellow blocks) are used to extract seasonal-trend patterns
from the input data.
uniformly at random.
More speciﬁcally, we denote by
i1 < i2 < . . . < is the randomly selected components.
We construct matrix S ∈{0, 1}s×d, with Si,k = 1 if
i = ik and Si,k = 0 otherwise. Then, our representation
of multivariate time series becomes A′ = AS⊤∈Rm×s.
Below, we will show that, although the Fourier basis are
randomly selected, under a mild condition, A′ is able to
preserve most of the information from A.
In order to measure how well A′ is able to preserve information from A, we project each column vector of A into the
subspace spanned by the column vectors in A′. We denote
by PA′(A) the resulting matrix after the projection, where
PA′(·) represents the projection operator. If A′ preserves
a large portion of information from A, we would expect a
small error between A and PA′(A), i.e. |A −PA′(A)|. Let
Ak represent the approximation of A by its ﬁrst k largest
single value decomposition. The theorem below shows that
|A−PA′(A)| is close to |A−Ak| if the number of randomly
sampled Fourier components s is on the order of k2.
Theorem 1. Assume that µ(A), the coherence measure of
matrix A, is Ω(k/n). Then, with a high probability, we
|A −PA′(A)| ≤(1 + ǫ)|A −Ak|
if s = O(k2/ǫ2).
The detailed analysis can be found in Appendix C.
For real-world multivariate times series, the corresponding
matrix A from Fourier transform often exhibit low rank
property, since those univaraite variables in multivariate
times series depend not only on its past values but also
has dependency on each other, as well as share similar frequency components. Therefore, as indicated by the Theorem 1, randomly selecting a subset of Fourier components allows us to appropriately represent the information
in Fourier matrix A.
Similarly, wavelet orthogonal polynomials, such as Legendre Polynomials, obey restricted isometry property (RIP)
and can be used for capture information in time series as
well. Compared to Fourier basis, wavelet based representation is more effective in capturing local structures in time
series and thus can be more effective for some forecasting
tasks. We defer the discussion of wavelet based representation in Appendix B. In the next section, we will present the
design of frequency enhanced decomposed Transformer architecture that incorporate the Fourier transform into transformer.
3. Model Structure
In this section, we will introduce (1) the overall structure
of FEDformer, as shown in Figure 2, (2) two subversion
structures for signal process: one uses Fourier basis and
the other uses Wavelet basis, (3) the mixture of experts
mechanism for seasonal-trend decomposition, and (4) the
complexity analysis of the proposed model.
3.1. FEDformer Framework
Preliminary
Long-term time series forecasting is a sequence to sequence problem. We denote the input length
as I and output length as O. We denote D as the hidden
states of the series. The input of the encoder is a I × D
matrix and the decoder has (I/2 + O) × D input.
FEDformer Structure
Inspired by the seasonal-trend decomposition and distribution analysis as discussed in Section 1, we renovate Transformer as a deep decomposition
architecture as shown in Figure 2, including Frequency
Enhanced Block (FEB), Frequency Enhanced Attention
Submission and Formatting Instructions for ICML 2022
(FEA) connecting encoder and decoder, and the Mixture
Of Experts Decomposition block (MOEDecomp). The detailed description of FEB, FEA, and MOEDecomp blocks
will be given in the following Section 3.2, 3.3, and 3.4 respectively.
The encoder adopts a multilayer structure as:
Encoder(X l−1
en ), where l ∈{1, · · · , N} denotes the output of l-th encoder layer and X 0
en ∈RI×D is the embedded
historical series. The Encoder(·) is formalized as
en ,−= MOEDecomp(FEB
en ,−= MOEDecomp(FeedForward
where Sl,i
en, i ∈{1, 2} represents the seasonal component after the i-th decomposition block in the l-th layer respectively. For FEB module, it has two different versions
(FEB-f & FEB-w) which are implemented through Discrete
Fourier transform (DFT) and Discrete Wavelet transform
(DWT) mechanism respectively and can seamlessly replace
the self-attention block.
The decoder also
adopts a multilayer structure as:
de = Decoder(X l−1
de , T l−1
de ), where l ∈{1, · · · , M}
denotes the output of l-th decoder layer. The Decoder(·) is
formalized as
de , T l,1
de = MOEDecomp
de , T l,2
de = MOEDecomp
de , T l,3
de = MOEDecomp
FeedForward
de = T l−1
+ Wl,1·T l,1
de + Wl,2·T l,2
de + Wl,3·T l,3
where Sl,i
de , T l,i
de , i ∈{1, 2, 3} represent the seasonal and
trend component after the i-th decomposition block in the lth layer respectively. Wl,i, i ∈{1, 2, 3} represents the projector for the i-th extracted trend T l,i
de . Similar to FEB, FEA
has two different versions (FEA-f & FEA-w) which are implemented through DFT and DWT projection respectively
with attention design, and can replace the cross-attention
block. The detailed description of FEA(·) will be given in
the following Section 3.3.
The ﬁnal prediction is the sum of the two reﬁned decomposed components as WS · X M
de , where WS is to
project the deep transformed seasonal component X M
the target dimension.
3.2. Fourier Enhanced Structure
Fourier Enhanced Structures use discrete Fourier transform
(DFT). Let F denotes the Fourier transform and F−1 de-
Figure 3. Frequency Enhanced Block with Fourier transform
(FEB-f) structure.
F + Sampling
F + Sampling
F + Sampling
Padding + F−1
Figure 4. Frequency Enhanced Attention with Fourier transform
(FEA-f) structure, σ(·) is the activation function.
notes the inverse Fourier transform. Given a sequence of
real numbers xn in time domain, where n = 1, 2...N. DFT
is deﬁned as Xl = PN−1
n=0 xne−iωln, where i is the imaginary unit and Xl, l = 1, 2...L is a sequence of complex
numbers in the frequency domain. Similarly, the inverse
DFT is deﬁned as xn = PL−1
l=0 Xleiωln. The complexity of DFT is O(N 2). With fast Fourier transform (FFT),
the computation complexity can be reduced to O(N log N).
Here a random subset of the Fourier basis is used and
the scale of the subset is bounded by a scalar. When we
choose the mode index before DFT and reverse DFT operations, the computation complexity can be further reduced
Frequency Enhanced Block with Fourier Transform
The FEB-f is used in both encoder and decoder
as shown in Figure 2.
The input (x ∈RN×D) of the
FEB-f block is ﬁrst linearly projected with w ∈RD×D, so
q = x·w. Then q is converted from the time domain to the
frequency domain. The Fourier transform of q is denoted
as Q ∈CN×D. In frequency domain, only the randomly
selected M modes are kept so we use a select operator as
˜Q = Select(Q) = Select(F(q)),
where ˜Q ∈CM×D and M << N. Then, the FEB-f is
FEB-f(q) = F−1(Padding( ˜Q ⊙R)),
where R ∈CD×D×M is a parameterized kernel initialized
randomly. Let Y = Q ⊙C, with Y ∈CM×D. The production operator ⊙is deﬁned as: Ym,do = PD
di=0 Qm,di ·
Rdi,do,m, where di = 1, 2...D is the input channel and
do = 1, 2...D is the output channel. The result of Q ⊙R
is then zero-padded to CN×D before performing inverse
Fourier transform back to the time domain. The structure
is shown in Figure 3.
Submission and Formatting Instructions for ICML 2022
Frequency Enhanced Attention with Fourier Transform (FEA-f)
We use the expression of the canonical
transformer. The input: queries, keys, values are denoted
as q ∈RL×D, k ∈RL×D, v ∈RL×D. In cross-attention,
the queries come from the decoder and can be obtained by
q = xen · wq, where wq ∈RD×D. The keys and values
are from the encoder and can be obtained by k = xde · wk
and v = xde · wv, where wk, wv ∈RD×D. Formally, the
canonical attention can be written as
Atten(q, k, v) = Softmax( qk⊤
In FEA-f, we convert the queries, keys, and values with
Fourier Transform and perform a similar attention mechanism in the frequency domain, by randomly selecting M
modes. We denote the selected version after Fourier Transform as ˜Q ∈CM×D, ˜
K ∈CM×D, ˜V ∈CM×D. The
FEA-f is deﬁned as
Q = Select(F(q))
K = Select(F(k))
˜V = Select(F(v))
FEA-f(q, k, v) = F−1(Padding(σ( ˜Q · ˜
K⊤) · ˜V )),
where σ is the activation function. We use softmax or
tanh for activation, since their converging performance differs in different data sets. Let Y = σ( ˜Q · ˜
K⊤) · ˜V , and
Y ∈CM×D needs to be zero-padded to CL×D before performing inverse Fourier transform. The FEA-f structure is
shown in Figure 4.
3.3. Wavelet Enhanced Structure
Discrete Wavelet Transform (DWT)
While the Fourier
transform creates a representation of the signal in the frequency domain, the Wavelet transform creates a representation in both the frequency and time domain, allowing efﬁcient access of localized information of the signal. The multiwavelet transform synergizes the advantages of orthogonal polynomials as well as wavelets. For a given f(x),
the multiwavelet coefﬁcients at the scale n can be deﬁned
i=0 , respectively, w.r.t. measure µn with sn
l ∈Rk×2n. φn
wavelet orthonormal basis of piecewise polynomials. The
decomposition/reconstruction across scales is deﬁned as
l = H(0)sn+1
+ H(1)sn+1
l + G(0)T dn
l = G(0)sn+1
+ H(1)sn+1
2l+1 = Σ(1) 
l + G(1)T dn
 H(0), H(1), G(0), G(1)
are linear coefﬁcients for
multiwavelet decomposition ﬁlters. They are ﬁxed matrices
Decomposed
Lf :X(L+1)
Reconstruction
Decomposed
Lf :q(L+1)
Figure 5. Top Left: Wavelet frequency enhanced block decomposition stage. Top Right: Wavelet block reconstruction stage shared
by FEB-w and FEA-w. Bottom: Wavelet frequency enhanced
cross attention decomposition stage.
used for wavelet decomposition. The multiwavelet representation of a signal can be obtained by the tensor product
of multiscale and multiwavelet basis. Note that the basis
at various scales are coupled by the tensor product, so we
need to untangle it. Inspired by , we
adapt a non-standard wavelet representation to reduce the
model complexity. For a map function F(x) = x′, the map
under multiwavelet domain can be written as
ˆsl = Cndn
where (U n
l ) are the multiscale, multiwavelet
coefﬁcients, L is the coarsest scale under recursive decomposition, and An, Bn, Cn are three independent FEB-f
blocks modules used for processing different signal during
decomposition and reconstruction. Here ¯F is a single-layer
of perceptrons which processes the remaining coarsest signal after L decomposed steps. More designed detail is described in Appendix D.
Frequency Enhanced Block with Wavelet Transform
The overall FEB-w architecture is shown in Figure 5. It differs from FEB-f in the recursive mechanism:
the input is decomposed into 3 parts recursively and operates individually. For the wavelet decomposition part, we
implement the ﬁxed Legendre wavelets basis decomposition matrix. Three FEB-f modules are used to process the
resulting high-frequency part, low-frequency part, and remaining part from wavelet decomposition respectively. For
each cycle L, it produces a processed high-frequency tensor Ud(L), a processed low-frequency frequency tensor
Submission and Formatting Instructions for ICML 2022
Us(L), and the raw low-frequency tensor X(L+1). This is
a ladder-down approach, and the decomposition stage performs the decimation of the signal by a factor of 1/2, running for a maximum of L cycles, where L < log2(M) for
a given input sequence of size M. In practice, L is set as a
ﬁxed argument parameter. The three sets of FEB-f blocks
are shared during different decomposition cycles L. For the
wavelet reconstruction part, we recursively build up our output tensor as well. For each cycle L, we combine X(L+1),
Us(L), and Ud(L) produced from the decomposition part
and produce X(L) for the next reconstruction cycle. For
each cycle, the length dimension of the signal tensor is increased by 2 times.
Frequency Enhanced Attention with Wavelet Transform (FEA-w)
FEA-w contains the decomposition stage
and reconstruction stage like FEB-w. Here we keep the
reconstruction stage unchanged. The only difference lies
in the decomposition stage. The same decomposed matrix
is used to decompose q, k, v signal separately, and q, k, v
share the same sets of module to process them as well. As
shown above, a frequency enhanced block with wavelet decomposition block (FEB-w) contains three FEB-f blocks
for the signal process. We can view the FEB-f as a substitution of self-attention mechanism. We use a straightforward way to build the frequency enhanced cross attention
with wavelet decomposition, substituting each FEB-f with
a FEA-f module. Besides, another FEA-f module is added
to process the coarsest remaining q(L), k(L), v(L) signal.
3.4. Mixture of Experts for Seasonal-Trend
Decomposition
Because of the commonly observed complex periodic pattern coupled with the trend component on real-world data,
extracting the trend can be hard with ﬁxed window average
pooling. To overcome such a problem, we design a Mixture Of Experts Decomposition block (MOEDecomp). It
contains a set of average ﬁlters with different sizes to extract multiple trend components from the input signal and
a set of data-dependent weights for combining them as the
ﬁnal trend. Formally, we have
Xtrend = Softmax(L(x)) ∗(F(x)),
where F(·) is a set of average pooling ﬁlters and
Softmax(L(x)) is the weights for mixing these extracted
3.5. Complexity Analysis
For FEDformer-f, the computational complexity for time
and memory is O(L) with a ﬁxed number of randomly selected modes in FEB & FEA blocks. We set modes number M = 64 as default value.
Though the complexity
Table 1. Complexity analysis of different forecasting models.
Autoformer
O(L log L)
O(L log L)
O(L log L)
O(L log L)
Transformer
O(L log L)
O(L log L)
O(L log L)
of full DFT transformation by FFT is (O(L log(L)), our
model only needs O(L) cost and memory complexity with
the pre-selected set of Fourier basis for quick implementation. For FEDformer-w, when we set the recursive decompose step to a ﬁxed number L and use a ﬁxed number
of randomly selected modes the same as FEDformer-f, the
time complexity and memory usage are O(L) as well. In
practice, we choose L = 3 and modes number M = 64
as default value. The comparisons of the time complexity
and memory usage in training and the inference steps in
testing are summarized in Table 1. It can be seen that the
proposed FEDformer achieves the best overall complexity
among Transformer-based forecasting models.
4. Experiments
To evaluate the proposed FEDformer, we conduct extensive experiments on six popular real-world datasets, including energy, economics, trafﬁc, weather, and disease. Since
classic models like ARIMA and basic RNN/CNN models
perform relatively inferior as shown in 
and , we mainly include four state-of-theart transformer-based models for comparison, i.e., Autoformer , Informer , Log-
Trans and Reformer 
as baseline models. Note that since Autoformer holds the
best performance in all the six benchmarks, it is used as
the main baseline model for comparison.
More details
about baseline models, datasets, and implementation are
described in Appendix A.2, F.1, and F.2, respectively.
4.1. Main Results
For better comparison, we follow the experiment settings
of Autoformer in where the input length
is ﬁxed to 96, and the prediction lengths for both training
and evaluation are ﬁxed to be 96, 192, 336, and 720, respectively.
Multivariate Results
For the multivariate forecasting,
FEDformer achieves the best performance on all six benchmark datasets at all horizons as shown in Table 2. Compared with Autoformer, the proposed FEDformer yields an
overall 14.8% relative MSE reduction. It is worth noting
Submission and Formatting Instructions for ICML 2022
Table 2. Multivariate long-term series forecasting results on six datasets with input length I = 96 and prediction length O ∈
{96, 192, 336, 720} (For ILI dataset, we use input length I = 36 and prediction length O ∈{24, 36, 48, 60}). A lower MSE indicates better performance, and the best results are highlighted in bold.
Electricity
FEDformer-f
0.203 0.269 0.325 0.421 0.193 0.201 0.214 0.246 0.148 0.271 0.460 1.195 0.587 0.604
0.626 0.217 0.276 0.339 0.403 3.228 2.679 2.622 2.857
0.287 0.328 0.366 0.415 0.308 0.315 0.329 0.355 0.278 0.380 0.500 0.841 0.366 0.373
0.382 0.296 0.336 0.380 0.428 1.260 1.080 1.078 1.157
FEDformer-w
0.204 0.316 0.359 0.433 0.183 0.195 0.212 0.231 0.139 0.256 0.426 1.090 0.562 0.562
0.596 0.227 0.295 0.381 0.424 2.203 2.272 2.209 2.545
0.288 0.363 0.387 0.432 0.297 0.308 0.313 0.343 0.276 0.369 0.464 0.800 0.349 0.346
0.368 0.304 0.363 0.416 0.434 0.963 0.976 0.981 1.061
Autoformer
0.255 0.281 0.339 0.422 0.201 0.222 0.231 0.254 0.197 0.300 0.509 1.447 0.613 0.616
0.660 0.266 0.307 0.359 0.419 3.483 3.103 2.669 2.770
0.339 0.340 0.372 0.419 0.317 0.334 0.338 0.361 0.323 0.369 0.524 0.941 0.388 0.382
0.408 0.336 0.367 0.395 0.428 1.287 1.148 1.085 1.125
0.365 0.533 1.363 3.379 0.274 0.296 0.300 0.373 0.847 1.204 1.672 2.478 0.719 0.696
0.864 0.300 0.598 0.578 1.059 5.764 4.755 4.763 5.264
0.453 0.563 0.887 1.338 0.368 0.386 0.394 0.439 0.752 0.895 1.036 1.310 0.391 0.379
0.472 0.384 0.544 0.523 0.741 1.677 1.467 1.469 1.564
0.768 0.989 1.334 3.048 0.258 0.266 0.280 0.283 0.968 1.040 1.659 1.941 0.684 0.685 0.7337 0.717 0.458 0.658 0.797 0.869 4.480 4.799 4.800 5.278
0.642 0.757 0.872 1.328 0.357 0.368 0.380 0.376 0.812 0.851 1.081 1.127 0.384 0.390
0.396 0.490 0.589 0.652 0.675 1.444 1.467 1.468 1.560
0.658 1.078 1.549 2.631 0.312 0.348 0.350 0.340 1.065 1.188 1.357 1.510 0.732 0.733
0.755 0.689 0.752 0.639 1.130 4.400 4.783 4.832 4.882
0.619 0.827 0.972 1.242 0.402 0.433 0.433 0.420 0.829 0.906 0.976 1.016 0.423 0.420
0.596 0.638 0.596 0.792 1.382 1.448 1.465 1.483
Table 3. Univariate long-term series forecasting results on six datasets with input length I
= 96 and prediction length O ∈
{96, 192, 336, 720} (For ILI dataset, we use input length I = 36 and prediction length O ∈{24, 36, 48, 60}). A lower MSE indicates better performance, and the best results are highlighted in bold.
Electricity
FEDformer-f
0.072 0.102 0.130 0.178 0.253 0.282 0.346 0.422 0.154
0.286 0.511 1.301 0.207 0.205 0.219 0.244 0.0062 0.0060 0.0041 0.0055 0.708 0.584 0.717 0.855
0.206 0.245 0.279 0.325 0.370 0.386 0.431 0.484 0.304
0.420 0.555 0.879 0.312 0.312 0.323 0.344
0.627 0.617 0.697 0.774
FEDformer-w
0.063 0.110 0.147 0.219 0.262 0.316 0.361 0.448 0.131
0.277 0.426 1.162 0.170 0.173 0.178 0.187 0.0035 0.0054
0.693 0.554 0.699 0.828
0.189 0.252 0.301 0.368 0.378 0.410 0.445 0.501 0.284
0.420 0.511 0.832 0.263 0.265 0.266 0.286
0.629 0.604 0.696 0.770
Autoformer
0.065 0.118 0.154 0.182 0.341 0.345 0.406 0.565 0.241
0.300 0.509 1.260 0.246 0.266 0.263 0.269
0.0075 0.0063 0.0085 0.948 0.634 0.791 0.874
0.189 0.256 0.305 0.335 0.438 0.428 0.470 0.581 0.387
0.369 0.524 0.867 0.346 0.370 0.371 0.372
0.732 0.650 0.752 0.797
0.080 0.112 0.166 0.228 0.258 0.285 0.336 0.607 1.327
1.258 2.179 1.280 0.257 0.299 0.312 0.366
5.282 4.554 4.273 5.214
0.217 0.259 0.314 0.380 0.367 0.388 0.423 0.599 0.944
0.924 1.296 0.953 0.353 0.376 0.387 0.436
2.050 1.916 1.846 2.057
0.075 0.129 0.154 0.160 0.288 0.432 0.430 0.491 0.237
0.738 2.018 2.405 0.226 0.314 0.387 0.437 0.0046 0.0060 0.0060
3.607 2.407 3.106 3.698
0.208 0.275 0.302 0.322 0.393 0.483 0.483 0.531 0.377
0.619 1.070 1.175 0.317 0.408 0.453 0.491
1.662 1.363 1.575 1.733
0.077 0.138 0.160 0.168 0.275 0.304 0.370 0.460 0.298
0.777 1.833 1.203 0.313 0.386 0.423 0.378
3.838 2.934 3.755 4.162
0.214 0.290 0.313 0.334 0.379 0.402 0.448 0.511 0.444
0.719 1.128 0.956 0.383 0.453 0.468 0.433
1.720 1.520 1.749 1.847
that for some of the datasets, such as Exchange and ILI, the
improvement is even more signiﬁcant (over 20%). Note
that the Exchange dataset does not exhibit clear periodicity in its time series, but FEDformer can still achieve superior performance. Overall, the improvement made by
FEDformer is consistent with varying horizons, implying
its strength in long term forecasting. More detailed results
on ETT full benchmark are provided in Appendix F.3.
Univariate Results
The results for univariate time series
forecasting are summarized in Table 3. Compared with
Autoformer, FEDformer yields an overall 22.6% relative
MSE reduction, and on some datasets, such as trafﬁc and
weather, the improvement can be more than 30%. It again
veriﬁes that FEDformer is more effective in long-term forecasting. Note that due to the difference between Fourier
and wavelet basis, FEDformer-f and FEDformer-w perform
well on different datasets, making them complementary
choice for long term forecasting. More detailed results on
ETT full benchmark are provided in Appendix F.3.
4.2. Ablation Studies
In this section, the ablation experiments are conducted, aiming at comparing the performance of frequency enhanced
block and its alternatives. The current SOTA results of Autoformer which uses the autocorrelation mechanism serve
as the baseline. Three ablation variants of FEDformer are
tested: 1) FEDformer V1: we use FEB to substitute selfattention only; 2) FEDformer V2: we use FEA to substitute cross attention only; 3) FEDFormer V3: we use
FEA to substitute both self and cross attention. The ablated versions of FEDformer-f as well as the SOTA models
are compared in Table 4, and we use a bold number if the
ablated version brings improvements compared with Autoformer. We omit the similar results in FEDformer-w due to
space limit. It can be seen in Table 4 that FEDformer V1
brings improvement in 10/16 cases, while FEDformer V2
improves in 12/16 cases. The best performance is achieved
in our FEDformer with FEB and FEA blocks which improves performance in all 16/16 cases. This veriﬁes the
effectiveness of the designed FEB, FEA for substituting
self and cross attention. Furthermore, experiments on ETT
and Weather datasets show that the adopted MOEDecomp
(mixture of experts decomposition) scheme can bring an
average of 2.96% improvement compared with the single
decomposition scheme. More details are provided in Appendix F.5.
Submission and Formatting Instructions for ICML 2022
Table 4. Ablation studies: multivariate long-term series forecasting results on ETTm1 and ETTm2 with input length I = 96 and prediction length O ∈{96, 192, 336, 720}. Three variants of FEDformer-f are compared with baselines. The best results are highlighted in
Transformer
Autoformer
FEDformer V1
FEDformer V2
FEDformer V3
FEDformer-f
FEB-f(Eq. 4)
FEA-f(Eq. 7)
FEB-f(Eq. 4)
FEA-f(Eq. 7)
FEA-f(Eq. 7)
FEA-f(Eq. 7)
Mode number
Mode number
Figure 6. Comparison of two base-modes selection method
(Fix&Rand). Rand policy means randomly selecting a subset of
modes, Fix policy means selecting the lowest frequency modes.
Two policies are compared on a variety of base-modes number
M ∈{2, 4, 8...256} on ETT full-benchmark (h1, m1, h2, m2).
4.3. Mode Selection Policy
The selection of discrete Fourier basis is the key to effectively representing the signal and maintaining the model’s
linear complexity.
As we discussed in Section 2, random Fourier mode selection is a better policy in forecasting tasks. more importantly, random policy requires no
prior knowledge of the input and generalizes easily in new
tasks. Here we empirically compare the random selection
policy with ﬁxed selection policy, and summarize the experimental results in Figure 6. It can be observed that the
adopted random policy achieves better performance than
the common ﬁxed policy which only keeps the low frequency modes.
Meanwhile, the random policy exhibits
some mode saturation effect, indicating an appropriate
random number of modes instead of all modes would bring
better performance, which is also consistent with the theoretical analysis in Section 2.
4.4. Distribution Analysis of Forecasting Output
In this section, we evaluate the distribution similarity between the input sequence and forecasting output of different transformer models quantitatively. In Table 5, we
applied the Kolmogrov-Smirnov test to check if the forecasting results of different models made on ETTm1 and
Table 5. P-values of Kolmogrov-Smirnov test of different transformer models for long-term forecasting output on ETTm1 and
ETTm2 dataset. Larger value indicates the hypothesis (the input
sequence and forecasting output come from the same distribution)
is less likely to be rejected. The best results are highlighted.
Transformer
Autoformer
ETTm2 are consistent with the input sequences. In particular, we test if the input sequence of ﬁxed 96-time steps
come from the same distribution as the predicted sequence,
with the null hypothesis that both sequences come from the
same distribution. On both datasets, by setting the common P-value as 0.01, various existing Transformer baseline models have much less values than 0.01 except Autoformer, which indicates their forecasting output have a
higher probability to be sampled from the different distributions compared to the input sequence. In contrast, Autoformer and FEDformer have much larger P-value compared
to others, which mainly contributes to their seasonal-trend
decomposition mechanism. Though we get close results
from ETTm2 by both models, the proposed FEDformer has
much larger P-value in ETTm1. And it’s the only model
whose null hypothesis can not be rejected with P-value
larger than 0.01 in all cases of the two datasets, implying
that the output sequence generated by FEDformer shares a
more similar distribution as the input sequence than others
and thus justiﬁes the our design motivation of FEDformer
as discussed in Section 1. More detailed analysis are provided in Appendix E.
Submission and Formatting Instructions for ICML 2022
4.5. Differences Compared to Autoformer baseline
Since we use the decomposed encoder-decoder overall architecture as Autoformer, we think it is critical to emphasize the differences. In Autoformer, the authors consider
a nice idea to use the top-k sub-sequence correlation (autocorrelation) module instead of point-wise attention, and the
Fourier method is applied to improve the efﬁciency for subsequence level similarity computation. In general, Autoformer can be considered as decomposing the sequence
into multiple time domain sub-sequences for feature exaction. In contrast, We use frequency transform to decompose the sequence into multiple frequency domain modes
to extract the feature. In particular, we do not use a selective approach in sub-sequence selection. Instead, all frequency features are computed from the whole sequence,
and this global property makes our model engage better performance for long sequence.
5. Conclusions
This paper proposes a frequency enhanced transformer
model for long-term series forecasting which achieves
state-of-the-art performance and enjoys linear computational complexity and memory cost. We propose an attention mechanism with low-rank approximation in frequency
and a mixture of experts decomposition to control the distribution shifting. The proposed frequency enhanced structure decouples the input sequence length and the attention
matrix dimension, leading to the linear complexity. Moreover, we theoretically and empirically prove the effectiveness of the adopted random mode selection policy in frequency. Lastly, extensive experiments show that the proposed model achieves the best forecasting performance on
six benchmark datasets in comparison with four state-ofthe-art algorithms.