Author's version of De Fauw et al. "Clinically applicable deep learning for diagnosis and referral in retinal disease". ​Nature Medicine​ ​XX​,
pppp-pppp , DOI: ​10.1038/s41591-018-0107-6​ Under Embargo until 13 August 2018 at 1600 London time (DO NOT SHARE)
Clinically applicable deep learning for
diagnosis and referral in retinal disease
Jeffrey De Fauw​1​, Joseph R Ledsam​1​, Bernardino Romera-Paredes​1​, Stanislav Nikolov​1​, Nenad Tomasev​1​, Sam
Blackwell​1​, Harry Askham​1​, Xavier Glorot​1​, Brendan O'Donoghue​1​, Daniel Visentin​1​, George van den
Driessche​1​, Balaji Lakshminarayanan​1​, Clemens Meyer​1​, Faith Mackinder​1​, Simon Bouton​1​, Kareem Ayoub​1​,
Reena Chopra​2​, Dominic King​1​, Alan Karthikesalingam​1​, Cían O Hughes​1,3​, Rosalind Raine​3​, Julian Hughes​2​,
Dawn A Sim​2​, Catherine Egan​2​, Adnan Tufail​2​, Hugh Montgomery​3​, Demis Hassabis​1​, Geraint Rees​3​, Trevor
Back​1​, Peng T. Khaw​2​, Mustafa Suleyman​1​, Julien Cornebise​4,5​, Pearse A. Keane​2,5​*, Olaf Ronneberger​1,5​*
1 DeepMind, London, UK
2 NIHR Biomedical Research Centre at Moorfields Eye Hospital and UCL Institute of Ophthalmology, London,
3 University College London, London, UK
4 Work performed while employed at DeepMind
5 These authors contributed equally to this work
* e-mail: ; 
Abstract​: The volume and complexity of diagnostic imaging is increasing at a pace faster than the
availability of human expertise to interpret it. Artificial intelligence has shown great promise in
classifying two-dimensional photographs of some common diseases and typically relies on databases of
millions of annotated images. Until now, the challenge of reaching the performance of expert clinicians
in a real-world clinical pathway with three-dimensional diagnostic scans has remained unsolved. Here,
we apply a novel deep learning architecture to a clinically heterogeneous set of three-dimensional
optical coherence tomography (OCT) scans from patients referred to a major eye hospital. We
demonstrate performance in making a referral recommendation that reaches or exceeds that of experts
on a range of sight-threatening retinal diseases after training on only 14,884 scans. Moreover, we
demonstrate that the tissue segmentations produced by our architecture act as a device-independent
representation; referral accuracy is maintained when using tissue segmentations from a different type of
device. Our work removes previous barriers to wider clinical use without prohibitive training data
requirements across multiple pathologies in a real-world setting.
Introduction
Medical imaging is expanding globally at an unprecedented rate​1,2​, leading to an ever-expanding quantity of data
requiring human expertise and judgement to interpret and triage. In many clinical specialities there is a relative
shortage of this expertise to provide timely diagnosis and referral. For example, in ophthalmology, the
widespread availability of optical coherence tomography (OCT) has not been matched by the availability of
expert humans to interpret scans and refer patients to the appropriate clinical care​3​. This problem is exacerbated
by the dramatic increase in prevalence of sight-threatening diseases for which OCT is the gold standard of initial
assessment​4–7​.
Artificial intelligence (AI) provides a promising solution for such medical image interpretation and triage, but
despite recent breakthroughs demonstrating expert-level performance on two-dimensional photographs in
preclinical settings​8,9​, prospective clinical application of this technology remains stymied by three key
challenges. First, AI (typically trained on hundreds of thousands of examples from one canonical dataset) must
generalise to new populations and devices without a substantial loss of performance, and without prohibitive
data requirements for retraining. Second, AI tools must be applicable to real-world scans, problems and
pathways, and designed for clinical evaluation and deployment. Finally, AI tools must match or exceed the
performance of human experts in such real-world situations. Recent work applying AI to OCT has shown
promise in resolving some of these criteria in isolation, but has not yet shown clinical applicability by resolving
all three.
Author's version of De Fauw et al. "Clinically applicable deep learning for diagnosis and referral in retinal disease". ​Nature Medicine​ ​XX​,
pppp-pppp , DOI: ​10.1038/s41591-018-0107-6​ Under Embargo until 13 August 2018 at 1600 London time (DO NOT SHARE)
Clinical Application & AI architecture
We developed our architecture in the challenging context of optical coherence tomography (OCT) imaging for
ophthalmology. We tested this approach for patient triage in a typical ophthalmology clinical referral pathway,
comprising more than 50 common diagnoses for which OCT provides the definitive imaging modality
(​Supplementary Table 1​)​. OCT is a three-dimensional volumetric medical imaging technique analogous to 3D
ultrasonography but measuring the reflection of near-infrared light rather than sound waves at a resolution for
living human tissue of ~5 µm​10​. ​OCT is now one of the most common imaging procedures with 5.35 million
OCT scans performed in the U.S. Medicare population in 2014 alone (see
 
harge-Data/Physician-and-Other-Supplier.html​). It has been widely adopted across the UK National Health
Service (NHS) for comprehensive initial assessment and triage of patients requiring rapid non-elective
assessment of acute and chronic sight loss. Rapid access “virtual” OCT clinics have become the standard of
care​11,12​. In such clinics, expert clinicians interpret the OCT and clinical history to diagnose and triage patients
with pathology affecting the macula, the central part of the retina required for high-resolution, color vision.
Automated diagnosis of a medical image, even for a single disease, faces two main challenges: technical
variations in the imaging process (different devices, noise, ageing of the components, etc.), and
patient-to-patient variability in pathological manifestations of disease. Existing deep learning approaches​8,9​ seek
to deal with all combinations of these variations using a single end-to-end black-box network, thus typically
requiring millions of labeled scans. In contrast, our framework decouples the two problems (technical variations
in the imaging process, and pathology variants) and solves them independently (see ​Fig. 1​). A deep
segmentation network (​Fig. 1b​) creates a detailed device-independent tissue segmentation map. Subsequently, a
deep classification network (​Fig. 1d​) analyses this segmentation map and provides diagnoses and referral
suggestions.
Figure 1 | Our proposed AI framework.​ (​a​) Raw retinal OCT scan (6 x 6 x 2.3 mm³ around the macula). (​b​) Deep
segmentation network, trained with manually segmented OCT scans. (​c​) Resulting tissue segmentation map. (​d​) Deep
classification network, trained with tissue maps with confirmed diagnoses and optimal referral decisions. (​e​) Predicted diagnosis
probabilities and referral suggestions.
Author's version of De Fauw et al. "Clinically applicable deep learning for diagnosis and referral in retinal disease". ​Nature Medicine​ ​XX​,
pppp-pppp , DOI: ​10.1038/s41591-018-0107-6​ Under Embargo until 13 August 2018 at 1600 London time (DO NOT SHARE)
Figure 2 | Results of the segmentation network. ​Three selected 2D slices from the n=224 OCT scans in the segmentation
test set (left column) with manual segmentation (middle column) and automated segmentation (right column; detailed ​color
legend in ​Supplementary Table 2​)​. (​a​) A patient with diabetic macular edema. (​b​) A patient with choroidal neovascularization
resulting from age-related macular degeneration (AMD), demonstrating extensive fibrovascular pigment epithelium detachment
and associated subretinal fluid. ​(c)​ A patient with neovascular AMD with extensive subretinal hyperreflective material. Further
examples of the variation of pathology with model segmentation and diagnostic performance can be found in ​Supplementary
Videos 1-9​. In all examples the classification network predicted the correct diagnosis. Scale bars: 0.5mm
The segmentation network (​Fig. 1b​) uses a 3D U-Net architecture​13,14​ to translate the raw OCT scan into a tissue
map (​Fig. 1c​) with 15 classes including anatomy, pathology and image artefacts (​Supplementary Table 2​). It
was trained with 877 clinical OCT scans (Topcon 3D OCT, Topcon, Japan) with sparse manual segmentations
(Dataset #1 in ​Supplementary Table 3, ​see ​Online Methods “Manual Segmentation” and "Datasets" ​for
full breakdown of scan dataset). Only approximately 3 representative slices of the 128 slices of each scan were
manually segmented (see ​Supplementary Table 4​ for image sizes) . This sparse annotation procedure​14​ allowed
us to cover a large variety of scans and pathologies with the same workload as approximately 21 dense manual
segmentations. Examples of the output of our segmentation network for illustrative pathologies are shown in
The classification network (​Fig. 1d​) analyses the tissue segmentation map (​Fig. 1c​) and as a primary outcome
provides one of four referral suggestions currently used in clinical practice at Moorfields Eye Hospital (please
see ​Supplementary Table 1 ​for a list of retinal conditions associated with these referral suggestions).
Additionally, it reports the presence or absence of multiple, concomitant retinal pathologies (​Supplementary
Table 5​). To construct the training set for this network we assembled 14,884 OCT scan volumes of 7621
patients referred to the hospital with symptoms suggestive of macular pathology (see ​Online Methods
"Clinical Labeling"​ for details). These OCT scans were automatically segmented using our segmentation
network. The resulting segmentation maps with the clinical labels built the training set for the classification
network (Dataset #3 in ​Supplementary Table 3, ​illustrated in​ Fig. 1d​).
A central challenge in OCT image segmentation is the presence of ambiguous regions, where the true tissue type
cannot be deduced from the image, and thus multiple equally plausible interpretations exist. To address this
issue, we trained not one but multiple instances of the segmentation network. Each network instance creates a
full segmentation map for the given scan, resulting in multiple hypotheses (see ​Supplementary Fig. 1​).
Analogous to multiple human experts, these segmentation maps agree in areas with clear image structures but
Author's version of De Fauw et al. "Clinically applicable deep learning for diagnosis and referral in retinal disease". ​Nature Medicine​ ​XX​,
pppp-pppp , DOI: ​10.1038/s41591-018-0107-6​ Under Embargo until 13 August 2018 at 1600 London time (DO NOT SHARE)
may contain different (but plausible) interpretations in ambiguous low-quality regions. These multiple
segmentation hypotheses from our network can be displayed as a video, where the ambiguous regions and the
proposed interpretations become clearly visible (see ​Online Methods "Visualization of results in clinical
practice"; ​use of this viewer across a range of challenging macular diseases is illustrated in ​Supplementary
Videos 1-9​).
Achieving Expert Performance on Referral Decisions
To evaluate our framework, we first defined a gold standard. This used information not available at the first
patient visit and OCT scan, by examining the patient clinical records to determine the final diagnosis and
optimal referral pathway in the light of that (subsequently obtained) information. Such a gold standard can only
be obtained retrospectively. Gold standard labels were acquired for 997 patients not included in the training
dataset (Dataset #5 in ​Supplementary Table 5). ​We then tested our framework on this dataset. For each patient,
we obtained the referral suggestion of our framework plus an independent referral suggestion from eight clinical
experts, four of whom were retina specialists and four optometrists trained in medical retina; see
Supplementary Table 6​ for more information. Each expert provided two separate decisions, one (like our
framework) from the OCT scan alone (Dataset #7 in ​Supplementary Table 5​); and one from the OCT plus
fundus image and clinical notes (Dataset #8 in ​Supplementary Table 5, ​see ​Supplementary Fig. 2​), in two
separate sessions spaced at least two weeks apart. We compared each of these performances (framework and
two expert decisions) against the gold standard.
Our framework achieved and in some cases exceeded expert performance (​Fig. 3​). To illustrate this, ​Fig. 3a
displays performance on "Urgent referrals", the most important clinical referral decision (mainly due to
pathologies that cause choroidal neovascularization (CNV) -- see ​Supplementary Table 1​) versus all other
referral decisions as a receiver operating characteristic (ROC) plot (plots for the other decisions are shown in
Supplementary Fig. 3​). Performance of our framework matched our two best retina specialists and had a
significantly higher performance than the other two retinal specialists and all four optometrists when they used
only the OCT scans to make their referral suggestion. (​Fig. 3a​ filled markers). When experts had access to the
fundus image and patient summary notes to make their decision, their performance improved (​Fig. 3a​ empty
markers) but our framework remained as good as the five best experts and continued to significantly outperform
the other three (see Supplemental Material).
To provide a fuller picture, the overall performance of our framework on all four clinical referral suggestions
("urgent", "semi-urgent", "routine", and "observation only") is displayed in ​Fig. 3b​ compared to the two highest
performing retina specialists. The framework performed comparably to the two best-performing retina
specialists, and made no clinically serious wrong decisions (topright element of each matrix, i.e. referring a
patient who needs an urgent referral to observation only). Confusion matrices for the assessments of the other
human experts are shown in ​Supplementary Fig. 4​. The aggregated number of wrong referral decisions is
displayed as error rate (1 - accuracy) for our framework and all experts in ​Fig. 3c​. Our framework (5.5% error
rate) performed comparably to the two best retina specialists (6.7% and 6.8% error rate) and significantly
outperformed the other six experts in the "OCT only" setting. Significance thresholds (3.9% for higher
performance and 7.3% for lower performance) were derived by a two-sided exact binomial test, incorporating
uncertainty both from expert and from algorithm (see ​Online Methods​ ​"Statistical Analysis"​). When experts
additionally used the fundus image and the patient’s summary notes, five approached the performance of our
framework (three retina specialists and two optometrists), which continued to significantly outperform the
remaining three (one retina specialist and two optometrists).
Our framework uses an ensemble of five segmentation and five classification model instances (see
Supplementary Fig. 1​) to achieve these results. Beside the benefits of an uncertainty measure, ensembling also
significantly improves overall performance compared to a single model instance. Error rates for different
ensemble sizes are shown in ​Supplementary Fig. 5. ​With more segmentation model instances and more
classification model instances, performance increases. The bottom right cells in that table illustrate that
performance differences between 4 x 4 model instances and 5 x 5 model instances are only marginal, so we do
not expect significant changes by adding more instances.
Author's version of De Fauw et al. "Clinically applicable deep learning for diagnosis and referral in retinal disease". ​Nature Medicine​ ​XX​,
pppp-pppp , DOI: ​10.1038/s41591-018-0107-6​ Under Embargo until 13 August 2018 at 1600 London time (DO NOT SHARE)
Figure 3 | Results on the patient referral decision.​ ​Performance ​on an independent test set of n=997 patients (252 urgent,
230 semi-urgent, 266 routine, 249 observation only). (​a​) Receiver operating characteristic (ROC) diagram for "urgent referral"
(due to choroidal neovascularization (CNV)) versus all other referrals. The blue ROC curve is created by sweeping a threshold
over the predicted probability of a particular clinical diagnosis. Points outside the light blue area correspond to a significantly
different performance (95% confidence level, using a two-sided exact binomial test). The asterisk denotes the performance of
our model in the 'balanced performance' setting. Filled markers denote experts' performance using OCT only; emptyoutlined
markers denote their performance using OCT, fundus image and summary notes. Dashed lines connect the two performance
points of each expert. (​b​) Confusion matrices with patient numbers for referral decision for our framework and the two best
retina specialists. These show the number of patients for each combination of gold standard decision and predicted decision.
The numbers of correct decisions are found on the diagonal. Wrong decisions due to overdiagnosis are in the lower-left triangle,
and wrong decisions due to underdiagnosis are in the upper-right triangle. (​c​) Total error rate (1 - accuracy) on referral decision.
Values outside the light blue area (3.9% - 7.3%) are significantly different (95% confidence interval, using a two-sided exact
binomial test) to the framework performance (5.5%). AUC: area under curve.
The accumulated number of diagnostic errors does not fully reflect the clinical consequences that an incorrect
referral decision might have for patients, which depends also on the specific diagnosis missed. For example,
failing to diagnose sight-threatening conditions could result in rapid visual loss​3,15,16​ which is not the case for
many other diagnoses. For an initial quantitative estimation of these consequences, we weighted different types
of diagnostic errors according to our clinical experts’ judgement of the clinical impact of erroneous
classification (expressed as penalty points; see ​Supplementary Fig. 6a​). We derived a score for our framework
and each expert as a weighted average of all wrong diagnoses. This revealed that our framework achieved a
lower average penalty point score than any of our experts (​Supplementary Fig. 6b​). We further optimized our
framework decisions to minimise this specific score (see ​Online Methods "Optimizing the Ensemble Output
for Sensitivity, Specificity and Penalty Scores"​) which further improved performance , DOI: ​10.1038/s41591-018-0107-6​ Under Embargo until 13 August 2018 at 1600 London time (DO NOT SHARE)
6b​). Thus the expert performance of our framework is not achieved at the cost of missing clinically important
sight-threatening diagnoses.
To examine how our proposed two-stage architecture compared to a traditional single-stage architecture, we
trained an end-to-end classification network with the same architecture as our second stage to map directly from
a raw OCT scan to a referral decision (see ​Methods "End-to-end Classification Network"​). The error rate
achieved with an ensemble of five network instances was 5.5%, which was not significantly different from the
performance of the two-stage architecture. This validates our choice of the two-stage architecture that offers
several clinical advantages. See ​Supplementary Fig. 7​ for detailed results.
Achieving Expert Performance on Retinal Morphology
The referral decision recommended by our framework is determined by the most urgent diagnosis detected on
each scan (​Supplementary Table 1​). Patients may also have multiple concomitant retinal pathologies. These
additional pathologies do not change the referral decision, but may have implications for further investigations
and treatment. Our framework was therefore also trained to predict the probability of a patient having one or
more of several pathologies (​Supplementary Table 5​).
To evaluate performance on diagnosing multiple pathologies, a ‘silver standard’ for each scan was established
by majority vote from eight experts who evaluated the OCT scan, fundus image and patient summary notes
(Dataset #6 in ​Supplementary Table 3)​. This majority vote biases the assessment against our framework.
Nevertheless, our framework demonstrated an area under the ROC curve that was over 99% for most of the
pathologies (and over 96% for all of them; ​Supplementary Table 7​), on par with the experts' performance on
OCT only. As with earlier evaluations, experts’ performance improved when they were provided also with the
fundus image and patient summary notes. This improvement was most marked in pathologies classed as 'routine
referral' e.g. geographic atrophy and central serous retinopathy. Many of these are conditions where the fundus
photograph or demographic information would be expected to provide important information, indicating that
there is scope for future work to improve the model. However even in the worst case our framework still
performed on par with at least one retinal specialist and one optometrist (​Supplementary Table 6 ​and
Supplementary Fig. 8​).
Generalization to a New Scanning Device Type
A key benefit of our two-stage framework is the device independence of the second stage. ​Using our framework
on a new device generation ​thus only requires retraining of the segmentation stage to learn how each tissue type
appears in the new scan, while knowledge about patient-to-patient variability in pathological manifestation of
different diseases that it learned from the approximately 15,000 training cases can be reused. To demonstrate
this generalization, we collected an independent test set of clinical scans from 116 patients (plus confirmed
clinical outcomes) recorded with a different OCT scanner type from a different vendor (Spectralis, Heidelberg
Engineering, Germany; hereafter “device type 2”) . This dataset is listed as Dataset #11 in ​Supplementary
Table 3 ​(see also ​Online​ ​Methods "Datasets"​ for details). We selected this device type for several reasons. It
is the second most used device type at Moorfields Eye hospital for these examinations, giving rise to a sufficient
number of scans. It has a similar worldwide market share as device type 1. But most importantly, this device
type ​provides a large difference in scan characteristics compared to ​the original device type (see
Supplementary Fig. 9​).
Author's version of De Fauw et al. "Clinically applicable deep learning for diagnosis and referral in retinal disease". ​Nature Medicine​ ​XX​,
pppp-pppp , DOI: ​10.1038/s41591-018-0107-6​ Under Embargo until 13 August 2018 at 1600 London time (DO NOT SHARE)
Figure 4 | Generalization to a new scanning device type. ​(​a​) Low performance of original network on OCT scans from the
new device type 2. Left: The selected slice shows the different appearance of structures in device type 2. Middle: A poor quality
segmentation map created with our original segmentation network ​(​color legend in ​Supplementary Table 2​). Right: Resulting
performance on a new test set of n=116 patients. The confusion matrix shows patient numbers for the referral suggestion. (​b​)
All five segmentation hypotheses from our original network. The strong variations show the large uncertainty. (​c​) High
performance was attained on the device type 2 test set (n=116) after re-training the segmentation network with OCT scans from
device type 1 and device type 2. The classification network is unchanged. (​d​) All five segmentation hypotheses from the
re-trained segmentation network. The network is confident in the interpretation of most structures, and just highlights the
ambiguities in the sub-retinal pigment epithelium (RPE) space. Scale bars: 0.5mm
To evaluate the effect of a different scanning device type, we initially fed the OCT scans from device type 2 into
our framework trained only on scans from device type 1 (​Fig. 4a​). The segmentation network is clearly
confused by the changed appearance of these structures and attempted to explain them as additional retinal
layers (​Fig. 4a​ middle). Consequently, performance was poor with a total error rate for referral suggestions of
46.6% (​Fig. 4a​ right). Uncertainty of the segmentation network on these (never seen) types of images resulted in
five strongly different segmentation hypotheses (​Fig. 4b​).
We next collected an additional segmentation training set with 152 scans (527 manually segmented slices in
total) from this device (Dataset #9 in ​Supplementary Table 3)​, and retrained the segmentation network with
both the training scans from the original device type 1 and the new device type 2 (see ​Online Methods
"Segmentation Network" ​for details). The classification network was not modified.
Our retrained system (adapted segmentation network + unchanged classification network) now achieved a
similarly high level of performance on device type 2 as on the original device (​Fig. 4c​). It suggested incorrect
referral decisions in 4 of the 116 cases, a total error rate of 3.4%. Due to the small number of cases in the new
test set, this is not significantly different to the error rate of 5.5% on device type 1 (P(4 out of 116 < 55 out of
997) = 0.774, see ​Online Methods "Statistical Analysis"​). For continuity with our previous evaluation, we
also measured performance against retina specialists accessing OCT scans plus fundus images and clinical notes
(Dataset #12 in ​Supplementary Table 3​). Our experts achieved the following error rates (all with access to
imaging and clinical notes): retinal specialist one: 2 errors = 1.7% error rate; retinal specialist two: 2 errors =
1.7% error rate; retinal specialist three: 4 errors = 3.4% error rate; retinal specialist four: 3 errors = 2.6% error
rate; retinal specialist five: 3 errors = 2.6% error rate. These differences in performance between our framework
Author's version of De Fauw et al. "Clinically applicable deep learning for diagnosis and referral in retinal disease". ​Nature Medicine​ ​XX​,
pppp-pppp , DOI: ​10.1038/s41591-018-0107-6​ Under Embargo until 13 August 2018 at 1600 London time (DO NOT SHARE)
and the best human retina specialists did not reach statistical significance (P(4 out of 116 > 2 out of 116) =
To verify that device type 2 provides the greatest difference in scan characteristics, we performed a feasibility
study on the small number of OCT scans from Cirrus HD-OCT 5000 with AngioPlex (Carl Zeiss Meditec)
devices available in Moorfields Eye Hospital (dataset of 61 scans not included here). Applying our original
network to these images we already obtained an error rate of 16.4%. This rate was much lower than that
originally obtained with device type 2 (46.6%), consistent with the claim that device type 2 provides a larger
difference in scan characteristics from device type 1. Retraining of the segmentation network with 6 manually
segmented scans reduced the error rate to 9.8%.
Table 1​ summarizes our results: For device type 1 our architecture required 877 training scans with manual
segmentations and 14,884 training scans with gold standard referral decisions to achieve expert performance on
referral decisions (5.5% error rate). For device type 2 we only required 152 additional training scans with
manual segmentations and not a single additional training scan with gold standard referral decisions to achieve
the same performance on referral decisions on this device type (3.4% error rate).
Table 1 | Number of training scans and achieved performance on the two device types
Scans with sparse
manual segmentations
Scans with gold
standard referral
Performance on
referral decision
(error rate)
Performance on
urgent referral
Device type 1
55 out of 997 = ​5.5%
Device type 2
(+ 877 scans from
device type 1)
4 out of 116 = ​3.4%
Discussion
Recent work applying AI to the automated diagnosis of OCT scans shows encouraging results but until now
such studies have relied on selective and clinically unrepresentative OCT datasets. For example, several
authors​17–21​ report high performance on automated classification of age-related macular degeneration (AMD)
from OCT scans. However, they tested their algorithms on smaller datasets that exclude other pathologies. In
contrast, here we demonstrate expert performance on multiple clinical referral suggestions for two independent
test datasets of 997 and 116 clinical OCT scans that include a wide range of retinal pathologies.
Several recent studies used deep learning based architectures to deliver successful segmentation of OCT
scans​22–25​. This earlier work focused on a subset of diagnostically relevant tissues types (e.g. intraretinal fluid)
and applied 2D models in samples of between 10 and 42 patients. In the present work we go beyond these
earlier studies by applying 3D models, segmenting a much larger range of diagnostically relevant tissue types,
and connect such segmentation to clinically relevant real-world referral recommendations.
We evaluated our framework on a broad range of real-world images from routine clinical practice at 32 different
Moorfields Eye Hospital sites covering diverse populations within London and surrounding areas, using 37
individual OCT devices (28 device type 1 and 9 device type 2). The two device types we tested are both used
widely in routine clinical practice at Moorfields Eye Hospital, the largest eye hospital in Europe and North
America, and provided a large difference in scan characteristics.
A number of potential benefits extend from our framework. The derivation of a device-independent
segmentation of the OCT scan creates an intermediate representation that is readily viewable by a clinical expert
and integrates into clinical workflows (see ​Fig. 5​ for the clinical results viewer). Moreover, the use of an
ensemble of five segmentation network instances allows us to present ambiguities arising from the imaging
process to the decision network (and could potentially be used for automated quality control).
Author's version of De Fauw et al. "Clinically applicable deep learning for diagnosis and referral in retinal disease". ​Nature Medicine​ ​XX​,
pppp-pppp , DOI: ​10.1038/s41591-018-0107-6​ Under Embargo until 13 August 2018 at 1600 London time (DO NOT SHARE)
Figure 5 | Visualization of the segmentation results as thickness maps.​ (​a​) The average intensity projection of the OCT
scan along A-scan direction (frontal view of the eye) is overlaid with a thickness map of the fibrovascular pigment epithelium
detachment (PED,red segment). (​b​) Screenshot from our OCT viewer. (​Row 1 left​) Referral suggestion, tissue volumes and
diagnosis probabilities. The highlighted bars correspond to the selected segmentation model. (​Rows 1-3)​ Thickness maps of
the 10 relevant tissue types from segmentation model instance 2. The two healthy tissue types (high level retina and RPE) are
displayed in a black-blue-green-brown-white color map, the pathological tissues (all others) are displayed as overlay on a
projection of the raw OCT scan. The thin white line indicates the position of slice 80. (​Row 4​) Slice 80 from the OCT scan and
the segmentation map from segmentation model instance 2. Detailed tissue legend in ​Supplementary Table 2​. The slice and
model instance can be interactively selected (see ​Supplementary Video 1​).
The ‘black box’ problem has been identified as an impediment to the application of deep learning in
healthcare​26​. Here we created a framework whose structure closely matches the clinical decision-making
process, separating judgements about the scan itself from the subsequent referral decision. This allows a
clinician to inspect and visualize an interpretable segmentation, rather than simply being presented with a
diagnosis and referral suggestion. Such an approach to medical imaging AI offers potential insights into the
decision process, in a fashion more typical of clinical practice. For example, an interpretable representation is
particularly useful in difficult and ambiguous cases. Such cases are common in medicine and even expert
medical practitioners can find it difficult to reach consensus (for example, our eight experts only agreed on
63.5% of cases even when accessing all information).
Our segmentation map assigns only one label per pixel, and it may not be possible to use the framework directly
in other clinical pathways where the tissue segmentation map does not contain all required information for a
diagnosis (e.g. in certain radiomics applications). To keep the advantages of the intermediate
device-independent representation in such applications, future work can potentially augment the tissue
segmentation map with multiple labels per pixel to encode local tissue features, or with additional channels that
encode continuous features like inflammatory reaction. This may be of particular value for other components of
the retina such as the nerve fibre layer, and may be of importance for multiple ocular and brain disorders such as
glaucoma and dementia.
While we have demonstrated the performance of our framework in the domain of a clinical treatment pathway,
the approach has potential utility in clinical training where medical professionals must learn to read medical
images. In addition, a wide variety of non-medically qualified health professionals have an interest in
Author's version of De Fauw et al. "Clinically applicable deep learning for diagnosis and referral in retinal disease". ​Nature Medicine​ ​XX​,
pppp-pppp , DOI: ​10.1038/s41591-018-0107-6​ Under Embargo until 13 August 2018 at 1600 London time (DO NOT SHARE)
appropriately reading and understanding medical images. Our framework produces a visualisable segmentation
and achieves expert performance on diagnosis and referral decisions for a large number of scans and
pathologies. This therefore raises the intriguing possibility that such a framework could be evaluated as a tool
for effectively training health care professionals to expert levels.
Segmentation output itself can also be used to quantify retinal morphology and derive measurements of
particular pathologies (for example, the location and volume of fibrovascular pigment epithelium detachment
and macular edema). Some of these measurements (such as retinal thickness and intraretinal fluid) can currently
be derived automatically​27,28​, used to investigate correlations with visual outcomes​27​ and as an endpoint in
clinical trials of therapies for retinal disease​29–32​. Our framework can be used to define and validate a broader
range of automatically derived quantitative measurements.
Our framework can triage scans at first presentation of a patient into a small number of pathways used in routine
clinical practice with a performance matching or exceeding both expert retina specialists and optometrists who
staff virtual clinics in a UK NHS setting. Future work can now directly seek evidence for efficacy of such a
framework in a randomized controlled trial. The output of our framework can be optimized to penalize different
diagnostic errors, and thus for other clinically important metrics. For example, the potential improvement to
patient quality of life of different diagnostic decisions, or avoiding the harm of unnecessary investigation that
might come from a false-positive diagnosis, could all be incorporated into future work.
Globally, ophthalmology clinical referral pathways vary, and the range of diseases that can potentially be
diagnosed by OCT includes pathologies additional to those macular diseases studied here. We studied a major
clinical referral pathway in a global center of clinical excellence focusing on 53 key diagnoses relevant to the
national (NHS) referral pathways. Our work opens up the possibility of testing the clinical applicability of this
approach in other global settings and clinical pathways such as emergency macular assessment clinics in the UK
NHS, triage and assessment in community eye care centers and the monitoring of disease during treatment
regimes. Furthermore, devices such as binocular OCT​33​ have the potential to increase accessibility in emerging
economies. Images produced by such devices will differ in resolution, contrast and image quality from the
state-of-the-art devices studied here, and existing AI models trained on current state-of-the-art devices may
perform poorly on such new devices. Our proposed two-stage model offers exciting possibilities in deploying
models more efficiently in countries where state-of-the-art OCT devices are too costly for widespread adoption.
In conclusion, we present a novel framework that analyses clinical OCT scans and makes referral suggestions to
a standard comparable to clinical experts. While focused on one common type of medical imaging, future work
can address a much wider range of medical imaging techniques, and incorporate clinical diagnoses and tissue
types well outside the immediate application demonstrated here.
Author's version of De Fauw et al. "Clinically applicable deep learning for diagnosis and referral in retinal disease". ​Nature Medicine​ ​XX​,
pppp-pppp , DOI: ​10.1038/s41591-018-0107-6​ Under Embargo until 13 August 2018 at 1600 London time (DO NOT SHARE)