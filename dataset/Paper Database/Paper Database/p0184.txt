HAL Id: hal-01015140
 
Submitted on 17 May 2015
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Is object localization for free? – Weakly-supervised
learning with convolutional neural networks
Maxime Oquab, Léon Bottou, Ivan Laptev, Josef Sivic
To cite this version:
Maxime Oquab, Léon Bottou, Ivan Laptev, Josef Sivic. Is object localization for free? – Weaklysupervised learning with convolutional neural networks. IEEE Conference on Computer Vision and
Pattern Recognition, Jun 2015, Boston, United States. ￿hal-01015140v2￿
Is object localization for free? –
Weakly-supervised learning with convolutional neural networks
Maxime Oquab∗
INRIA Paris, France
L´eon Bottou†
MSR, New York, USA
Ivan Laptev*
INRIA, Paris, France
Josef Sivic*
INRIA, Paris, France
Successful methods for visual object recognition typically rely on training datasets containing lots of richly annotated images. Detailed image annotation, e.g. by object
bounding boxes, however, is both expensive and often subjective. We describe a weakly supervised convolutional neural network (CNN) for object classiﬁcation that relies only
on image-level labels, yet can learn from cluttered scenes
containing multiple objects. We quantify its object classi-
ﬁcation and object location prediction performance on the
Pascal VOC 2012 (20 object classes) and the much larger
Microsoft COCO (80 object classes) datasets. We ﬁnd that
the network (i) outputs accurate image-level labels, (ii) predicts approximate locations (but not extents) of objects, and
(iii) performs comparably to its fully-supervised counterparts using object bounding box annotation for training.
1. Introduction
Visual object recognition entails much more than determining whether the image contains instances of certain object categories. For example, each object has a location and
a pose; each deformable object has a constellation of parts;
and each object can be cropped or partially occluded.
Object recognition algorithms of the past decade can
roughly be categorized in two styles. The ﬁrst style extracts local image features (SIFT, HOG), constructs bag
of visual words representations, and runs statistical classiﬁers . Although this approach has been
shown to yield good performance for image classiﬁcation,
attempts to locate the objects using the position of the visual
words have been unfruitful: the classiﬁer often relies on visual words that fall in the background and merely describe
the context of the object.
The second style of algorithms detects the presence of
objects by ﬁtting rich object models such as deformable
part models . The ﬁtting process can reveal useful
∗WILLOW project, Departement d’Informatique de l’´Ecole Normale
Sup´erieure, ENS/INRIA/CNRS UMR 8548, Paris, France
†L´eon Bottou is now with Facebook AI Research, New York.
training images
train iter. 210
train iter. 510
train iter. 4200
Evolution of localization score maps for the
motorbike class over iterations of our weakly-supervised
CNN training. Note that the network learns to localize objects despite having no object location annotation at training, just object presence/absence labels. Note also that locations of objects with more usual appearance (such as the
motorbike shown in left column) are discovered earlier during training.
attributes of objects such as location, pose and constellations of object parts, but the model is usually trained from
images with known locations of objects or even their parts.
The combination of both styles has shown beneﬁts .
A third style of algorithms, convolutional neural networks (CNNs) construct successive feature vectors that progressively describe the properties of larger and
larger image areas. Recent applications of this framework
to natural images have been extremely successful for a
variety of tasks including image classiﬁcation , object detection , human pose estimation 
and others. Most of these methods, however, require detailed image annotation. For example bounding box super-
vision has been shown highly beneﬁtial for object classiﬁcation in cluttered and complex scenes .
Labelling a set of training images with object attributes
quickly becomes problematic. The process is expensive and
involves a lot of subtle and possibly ambiguous decisions.
For instance, consistently annotating locations and scales of
objects by bounding boxes works well for some images but
fails for partially occluded and cropped objects. Annotating object parts becomes even harder since the correspondence of parts among images in the same category is often
ill-posed.
In this paper, we investigate whether CNNs can be
trained from complex cluttered scenes labelled only with
lists of objects they contain and not their locations. This
is an extremely challenging task as the objects may appear
at different locations, different scales and under variety of
viewpoints, as illustrated in Figure 1 (top row). Furthermore, the network has to avoid overﬁtting to the scene clutter co-occurring with objects as, for example, motorbikes
often appear on the road. How can we modify the structure
of the CNN to learn from such difﬁcult data?
We build on the successful CNN architecture and
the follow-up state-of-the-art results for object classiﬁcation
and detection , but introduce the following modiﬁcations. First, we treat the last fully connected
network layers as convolutions to cope with the uncertainty
in object localization. Second, we introduce a max-pooling
layer that hypothesizes the possible location of the object in
the image, similar to [32, Section 4] and . Third, we
modify the cost function to learn from image-level supervision. Interestingly, we ﬁnd that this modiﬁed CNN architecture, while trained to output image-level labels only,
localizes objects or their distinctive parts in training images,
as illustrated in Figure 1. So, is object localization with convolutional neural networks for free? In this paper we set out
to answer this question and analyze the developed weakly
supervised CNN pipeline on two object recognition datasets
containing complex cluttered scenes with multiple objects.
2. Related work
The fundamental challenge in visual recognition is modeling the intra-class appearance and shape variation of objects. For example, what is the appropriate model of the
various appearances and shapes of “chairs”? This challenge
is usually addressed by designing some form of a parametric model of the object’s appearance and shape. The parameters of the model are then learnt from a set of instances
using statistical machine learning. Learning methods for visual recognition can be characterized based on the required
input supervision and the target output.
Unsupervised methods do not require any supervisory signal, just images. While unsupervised learning
is appealing, the output is currently often limited only to
frequently occurring and visually consistent objects. Fully
supervised methods require careful annotation of object location in the form of bounding boxes , segmentation or even location of object parts , which is costly
and can introduce biases. For example, should we annotate
the dog’s head or the entire dog? What if a part of the dog’s
body is occluded by another object? In this work, we focus on weakly supervised learning where only image-level
labels indicating the presence or absence of objects are required. This is an important setup for many practical applications as (weak) image-level annotations are often readily
available in large amounts, e.g. in the form of text tags ,
full sentences or even geographical meta-data .
The target output in visual recognition ranges from
image-level labels (object/image classiﬁcation) , location and extent of objects in the form of bounding
boxes (object detection) , to detailed object segmentation or even predicting an approximate 3D
pose and geometry of objects . In this work, we
focus on predicting accurate image-level labels indicating
the presence/absence of objects.
However, we also ﬁnd
that the weakly supervised network can predict the approximate location (in the form of a x, y position) of objects
in the scene, but not their extent (bounding box). Furthermore, our method performs on par with alternative fullysupervised methods both on the classiﬁcation and location
prediction tasks. We quantify these ﬁndings on the Pascal
VOC 2012 and Microsoft COCO datasets that both
depict objects in complex cluttered scenes.
Initial work on weakly supervised object localization has focused on learning from images containing prominent and centered objects in scenes with limited background clutter. More recent efforts attempt to learn
from images containing multiple objects embedded in complex scenes , from web images 
or from video . These methods typically aim to localize
objects including ﬁnding their extent in the form of bounding boxes. They attempt to ﬁnd parts of images with visually consistent appearance in the training data that often
contains multiple objects in different spatial conﬁgurations
and cluttered backgrounds. While these works are promising, their performance is still far from the fully supervised
methods such as .
Our work is related to recent methods that ﬁnd distinctive mid-level object parts for scene and object recognition
in unsupervised or weakly supervised settings.
The proposed method can also be seen as a variant of Multiple Instance Learning if we refer to each image
as a “bag” and treat each image window as a “sample”.
In contrast to the above methods we develop a weakly
supervised learning method based on end-to-end training
of a convolutional neural network (CNN) from
image-level labels.
Convolutional neural networks have
recently demonstrated excellent performance on a number of visual recognition tasks that include classiﬁcation
of entire images , predicting presence/absence
of objects in cluttered scenes or localizing objects by bounding boxes . However, most
of the current CNN architectures assume in training a single prominent object in the image with limited background
clutter or require fully annotated object locations in the image .
Learning from
images containing multiple objects in cluttered scenes with
only weak object presence/absence labels has been so far
mostly limited to representing entire images without explicitly searching for location of individual objects ,
though some level of robustness to the scale and position of
objects is gained by jittering. Recent concurrent effort 
also investigates CNNs for learning from weakly labelled
cluttered scenes. Their work conﬁrms some of our ﬁndings
but does not investigate location prediction. Our work is
also related to recent efforts aiming to extract object localization by examining the network output while masking different portions of the input image , but these
methods consider already pre-trained networks at test time.
Contributions.
contributions
First, we develop a weakly supervised convolutional neural network end-to-end learning pipeline that
learns from complex cluttered scenes containing multiple
objects by explicitly searching over possible object locations and scales in the image. Second, we perform an extensive experimental analysis of the network’s classiﬁcation
and localization performance on the Pascal VOC 2012 and
the much larger Microsoft COCO datasets. We ﬁnd that
our weakly-supervised network (i) outputs accurate imagelevel labels, (ii) predicts approximate locations (but not extents) of objects, and (iii) performs comparably to its fullysupervised counterparts that use object bounding box annotation for training.
3. Network architecture for weakly supervised
We build on the fully supervised network architecture
of that consists of ﬁve convolutional and four fully
connected layers and assumes as input a ﬁxed-size image
patch containing a single relatively tightly cropped object.
To adapt this architecture to weakly supervised learning we
introduce the following three modiﬁcations. First, we treat
the fully connected layers as convolutions, which allows us
to deal with nearly arbitrary-sized images as input. Second,
we explicitly search for the highest scoring object position
in the image by adding a single global max-pooling layer at
the output. Third, we use a cost function that can explicitly model multiple objects present in the image. The three
modiﬁcations are discussed next and the network architecture is illustrated in Figure 2.
Convolutional adaptation layers.
The network architecture of assumes a ﬁxed-size image patch of 224×224
[$0.7…1.4$]$
C1=C5$ FC6$
ConvoluBonal$feature$$
extracBon$layers$
AdaptaBon$layers$
Figure 2: Network architecture for weakly supervised training.
Figure 3: Multiscale object recognition.
RGB pixels as input and outputs a 1 × 1 × K vector of perclass scores as output, where K is the number of classes.
The aim is to apply the network to bigger images in a sliding window manner thus extending its output to n×m×K
where n and m denote the number of sliding window positions in the x- and y- direction in the image, respectively,
computing the K per-class scores at all input window positions. While this type of sliding was performed in 
by applying the network to independently extracted image
patches, here we achieve the same effect by treating the
fully connected adaptation layers as convolutions. For a
given input image size, the fully connected layer can be
seen as a special case of a convolution layer where the size
of the kernel is equal to the size of the layer input. With
this procedure the output of the ﬁnal adaptation layer FC7
becomes a 2 × 2 × K output score map for a 256 × 256
RGB input image. As the global stride of the network is
321 pixels, adding 32 pixels to the image width or height
increases the width or height of the output score map by
one. Hence, for example, a 2048 × 1024 pixel input would
lead to a 58 × 26 output score map containing the score of
the network for all classes for the different locations of the
input 224 × 224 window with a stride of 32 pixels. While
this architecture is typically used for efﬁcient classiﬁcation
at test time, see e.g. , here we also use it at training time
(as discussed in Section 4) to efﬁciently examine the entire
1or 36 pixels for the OverFeat network that we use on MS COCO
image for possible locations of the object during weakly supervised training.
Explicit search for object’s position via max-pooling.
The aim is to output a single image-level score for each
of the object classes independently of the input image size.
This is achieved by aggregating the n × m × K matrix of
output scores for n × m different positions of the input
window using a global max-pooling operation into a single 1 × 1 × K vector, where K is the number of classes.
Note that the max-pooling operation effectively searches for
the best-scoring candidate object position within the image,
which is crucial for weakly supervised learning where the
exact position of the object within the image is not given at
training. In addition, due to the max-pooling operation the
output of the network becomes independent of the size of
the input image, which will be used for multi-scale learning
in Section 4.
Multi-label classiﬁcation loss function.
The goal of object classiﬁcation is to tell whether an instance of an object class is present in the image, where the input image
may depict multiple different objects. As a result, the usual
multi-class mutually exclusive logistic regression loss, as
used in e.g. for ImageNet classiﬁcation, is not suited
for this set-up as it assumes only a single object per image.
To address this issue, we treat the task as a separate binary
classiﬁcation problem for each class. The loss function is
therefore a sum of K binary logistic regression losses, one
for each of the K classes k ∈{1 · · · K},
ℓ( fk(x) , yk ) =
log(1 + e−ykfk(x)) ,
where fk(x) is the output of the network for input image
x and yk ∈{−1, 1} is the image label indicating the absence/presence of class k in the input image x. Each class
score fk(x) can be interpreted as a posterior probability indicating the presence of class k in image x with transformation
1 + e−fk(x) .
Treating a multi-label classiﬁcation problem as K independent classiﬁcation problems is often inadequate because it
does not model label correlations. This is not an issue here
because the classiﬁers share hidden layers and therefore are
not independent. Such a network can model label correlations by tuning the overlap of the hidden state distribution
given each label.
4. Weakly supervised learning and classiﬁcation
In this section we describe details of the training procedure. Similar to we pre-train the convolutional feature extraction layers C1-C7 on images from the ImageNet
dataset and keep their weights ﬁxed. This pre-training procedure is standard and similar to . Next, the goal is to
train the adaptation layers Ca and Cb using the Pascal VOC
or MS COCO images in a weakly supervised manner, i.e.
from image-level labels indicating the presence/absence of
the object in the image, but not telling the actual position
and scale of the object. This is achieved by stochastic gradient descent training using the network architecture and cost
function described in Section 3, which explicitly searches
for the best candidate position of the object in the image using the global max-pooling operation. We also search over
object scales (similar to ) by training from images of
different sizes. The training procedure is illustrated in Figure 2. Details and further discussion are given next.
Stochastic gradient descent with global max-pooling.
The global max-pooling operation ensures that the training error backpropagates only to the network weights corresponding to the highest-scoring window in the image. In
other words, the max-pooling operation hypothesizes the location of the object in the image at the position with the
maximum score, as illustrated in Figure 4. If the imagelevel label is positive (i.e. the image contains the object)
the back-propagated error will adapt the network weights
so that the score of this particular window (and hence other
similar-looking windows in the dataset) is increased. On the
other hand, if the image-level label is negative (i.e. the image does not contain the object) the back-propagated error
adapts the network weights so that the score of the highestscoring window (and hence other similar-looking windows
in the dataset) is decreased. For negative images, the maxpooling operation acts in a similar manner to hard-negative
mining known to work well in training sliding window object detectors . Note that there is no guarantee the location of the score maxima corresponds to the true location
of the object in the image. However, the intuition is that the
erroneous weight updates from the incorrectly localized objects will only have limited effect as in general they should
not be consistent over the dataset.
Multi-scale sliding-window training.
The above procedure assumes that the object scale (the size in pixels) is
known and the input image is rescaled so that the object occupies an area that corresponds to the receptive ﬁeld of the
fully connected network layers (i.e. 224 pixels). In general,
however, the actual object size in the image is unknown. In
fact, a single image can contain several different objects of
different sizes. One possible solution would be to run multiple parallel networks for different image scales that share
parameters and max-pool their outputs. We opt for a different less memory demanding solution. Instead, we train
from images rescaled to multiple different sizes. The intuition is that if the object appears at the correct scale, the
max-pooling operation correctly localizes the object in the
image and correctly updates the network weights. When the
Figure 4: Illustration of the weakly-supervised learning procedure. At training time, given an input image with an aeroplane label
(left), our method increases the score of the highest scoring positive image window (middle), and decreases scores of the highest scoring
negative windows, such as the one for the car class (right).
object appears at the wrong scale the location of the maximum score may be incorrect. As discussed above, the network weight updates from incorrectly localized objects may
only have limited negative effect on the results in practice.
In detail, all training images are ﬁrst rescaled to have the
largest side of size 500 pixels and zero-padded to 500×500
pixels. Each training mini-batch of 16 images is then resized by a scale factor s uniformly sampled between 0.7
and 1.4. This allows the network to see objects in the image at various scales. In addition, this type of multi-scale
training also induces some scale-invariance in the network.
Classiﬁcation.
At test time we apply the same sliding window procedure at multiple ﬁnely sampled scales.
In detail, the test image is ﬁrst normalized to have its
largest dimension equal to 500 pixels, padded by zeros
to 500 × 500 pixels and then rescaled by a factor s ∈
{0.5, 0.7, 1, 1.4, 2.0, 2.8}.
Scanning the image at large
scales allows the network to ﬁnd even very small objects.
For each scale, the per-class scores are computed for all
window positions and then max-pooled across the image.
These raw per-class scores (before applying the soft-max
function (2)) are then aggregated across all scales by averaging them into a single vector of per-class scores. The
testing architecture is illustrated in Figure 3. We found that
searching over only six different scales at test time was suf-
ﬁcient to achieve good classiﬁcation performance. Adding
wider or ﬁner search over scale did not bring additional beneﬁts.
5. Classiﬁcation experiments
In this section we describe our classiﬁcation experiments
where we wish to predict whether the object is present /
absent in the image. Predicting the location of the object is
evaluated in section 6.
Experimental setup.
We apply the proposed method to
the Pascal VOC 2012 object classiﬁcation task and the recently released Microsoft COCO dataset. The Pascal VOC
2012 dataset contains 5k images for training, 5k for validation and 20 object classes. The much larger COCO dataset
contains 80k images for training, 40k images for validation
and 80 classes. On the COCO dataset, we wish to evaluate whether our method scales-up to much bigger data with
more classes.
We use Torch7 for our experiments.
For Pascal
VOC, we use a network pre-trained on 1512 classes of ImageNet following ; for COCO, we use the Overfeat 
network. Training the adaptation layers was performed with
stochastic gradient descent and summarized across all classes using mean average precision
(mAP). Our weakly supervised approach (G.WEAK SUP)
obtains the highest overall mAP among all single network
methods outperforming other CNN-based methods trained
from image-level supervision (C-G) as well as the comparable setup of (B) that uses object-level supervision.
Beneﬁts of sliding-window training.
Here we compare
the proposed weakly supervised method (G. WEAK SUP)
with training from full images (F. FULL IMAGES), where
no search for object location during training/testing is performed and images are presented to the network at a single scale. Otherwise the network architectures are identical. Results for Pascal VOC test data are shown in Table 1).
The results clearly demonstrate the beneﬁts of sliding window multi-scale training attempting to localize the objects
in the training data. The largest improvements are obtained
for small objects, such as bottles and potted plants, where
AP increases by 15-20%.
Similar results on the COCO
dataset are shown in the ﬁrst row of Figure 5, where sliding window weakly supervised training (blue) consistently
improves over the full image training (red) for all classes.
Beneﬁts of multi-scale training and testing.
COCO dataset, multi-scale training improves the classiﬁcation mAP by about 1% when compared to training at a
single-scale s = 1. The intuition is that the network gets to
Object-level sup.
plane bike
chair cow table
horse moto pers plant sheep sofa train
A.NUS-SCM 
84.2 80.8 85.3 60.8 89.9 86.8 89.3
73.4 94.5 80.7
B.OQUAB 
82.9 88.2 84.1 60.3 89.0 84.4 90.7
62.3 91.1 79.8
Image-level sup.
plane bike
chair cow table
horse moto pers plant sheep sofa train
C.Z&F 
77.1 88.4 85.5 55.8 85.8 78.6 91.2
61.1 91.8 76.1
D.CHATFIELD 
82.5 91.5 88.1 62.1 88.3 81.9 94.8
66.4 93.5 81.9
E.NUS-HCP 
84.3 93.0 89.4 62.5 90.2 84.6 94.8
61.8 94.4 78.0
F.FULL IMAGES
77.4 85.6 83.1 49.9 86.7 77.7 87.2
63.4 91.4 74.1
G.WEAK SUP
88.8 92.0 87.4 64.7 91.1 87.4 94.4
70.0 94.5 83.7
Table 1: Single method image classiﬁcation results on the VOC 2012 test set. Methods A,B use object-level supervision. Methods C to G
use image-level supervision only. The combination of methods A and E reaches 90.3% mAP , the highest reported result on this data.
Classiﬁcation
Location Prediction
H.FULL IMAGES
I.MASKED POOL
J.WEAK SUP
K.CENTER PRED.
Table 2: Classiﬁcation and location prediction mean Average Precision on the validation sets for Pascal VOC and COCO datasets.
*For R-CNN , which is an algorithm designed for object detection, we use only the most conﬁdent bounding box proposal per
class and per image for evaluation.
see objects at different scales, increasing the overall number of examples. Scanning at multiple scales at test time
provides an additional 3% increase in classiﬁcation mAP.
Does adding object-level supervision help classiﬁcation?
Here we investigate whether adding object-level supervision to our weakly supervised setup improves classiﬁcation
performance. In order to test this, we remove the global
max-pooling layer in our model and introduce a “masked
pooling” layer that indicates the location of individual objects during training. In detail, the masked pooling layer
uses ground truth maps of the same size as the output of
the network, signaling the presence or absence of an object
class to perform the global max-pooling, but now restricted
to the relevant area of the output. This provides learning
guidance to the network as the max-scoring object hypothesis has to lie within the ground truth object location in the
image. We have also explored a variant of this method,
that minimized the object score outside of the masked area
to avoid learning from the context of the object, but obtained consistently worse results. Classiﬁcation results for
the masked-pooling method (I. MASKED POOL) on both
the Pascal VOC and COCO datasets are provided in Table 2 and show that adding this form of object-level supervision does not bring signiﬁcant beneﬁts over the weaklysupervised learning.
6. Location prediction experiments
The proposed weakly supervised architecture outputs
score maps for different objects. In the previous section we
have shown that max-pooling on these maps provides excellent classiﬁcation performance. However, we have also
observed that these scores maps are consistent with the locations of objects in the input images. In this section we
investigate whether the output score maps can be used to
localize the objects.
Location prediction metric.
In order to provide quantitative evaluation of the localization power of our CNN architecture, we introduce a simple metric based on precisionrecall using the per-class response maps. We ﬁrst rescale
the maps to the original image size2. If the maximal response across scales falls within the ground truth bounding
box of an object of the same class within 18 pixels tolerance
(which corresponds to the pooling ratio of the network), we
label the predicted location as correct. If not, then we count
the response as a false positive (it hit the background), and
we also increment the false negative count (no object was
found). Finally, we use the conﬁdence values of the responses to generate precision-recall curves. Each p-r curve
is summarized by Average Precision (AP). The perfect performance (AP=1) means that the network has indicated the
presence / absence of the object correctly in all images and
for each image containing the object the predicted object location fell inside one of the ground truth bounding boxes of
that object (if multiple object instances were present). This
metric differs from the standard object detection bounding
box overlap metric as it does not take into account whether
the extent of the object is predicted correctly and it only
measures localization performance for one object instance
per image. Note however, that even this type of location
prediction is very hard for complex cluttered scenes considered in this work.
Location prediction results.
The summary of the location prediction results for both the Pascal VOC and Microsoft COCO datasets is given in Table 2. The per-class
results for the Pascal VOC and Microsoft COCO datasets,
are shown in Table 3 (J.WEAK SUP) and Figure 5 (green
bars), respectively.
Center prediction baseline.
We compare the location
prediction performance to the following baseline. We use
the max-pooled image-level per-class scores of our weakly
supervised setup (J.WEAK SUP), but predict the center of
the image as the location of the object. As shown in Table 2,
2We do simple interpolation in our experiments.
plane bike
chair cow table
horse moto pers plant sheep sofa train
I.MASKED POOL
76.9 83.2 68.3 39.8 88.1 62.2 90.2
51.9 86.8 64.1
J.WEAK SUP
77.4 81.4 79.2 41.1 87.8 66.4 91.0
50.8 86.8 66.5
K.CENTER PRED.
55.0 61.1 38.9 14.5 78.2 30.7 82.6
28.5 71.8 22.4
80.8 80.8 73.0 49.9 86.8 77.7 87.6
50.1 81.5 76.6
Table 3: Location prediction scores on the VOC12 validation set. Maximal responses are labeled as correct when they fall within a
bounding box of the same class, and count as false negatives if the class was present but its location was not predicted. We then use the
conﬁdence values of the responses to generate precision-recall values.
kitchenware
electronics
1. person : 97.5
2. bicycle : 55.9
3. car : 74.6
4. motorcycle : 80.9
5. airplane : 88.9
6. bus : 73.9
7. train : 86.2
8. truck : 58.8
9. boat : 73.1
10. traffic light : 70.0
11. fire hydrant : 61.9
13. stop sign : 65.1
14. parking meter : 47.9
15. bench : 43.2
16. bird : 63.3
17. cat : 86.0
18. dog : 73.4
19. horse : 77.8
20. sheep : 80.3
21. cow : 67.2
22. elephant : 93.6
23. bear : 83.7
24. zebra : 98.6
25. giraffe : 97.5
27. backpack : 30.7
28. umbrella : 60.7
31. handbag : 37.7
32. tie : 66.6
33. suitcase : 37.0
34. frisbee : 47.9
35. skis : 86.3
36. snowboard : 47.7
37. sports ball : 66.8
38. kite : 81.2
39. baseball bat : 78.4
40. baseball glove : 86.2
41. skateboard : 63.1
42. surfboard : 78.5
43. tennis racket : 91.3
44. bottle : 55.5
46. wine glass : 52.8
47. cup : 58.1
48. fork : 46.2
49. knife : 40.6
50. spoon : 42.1
51. bowl : 58.8
52. banana : 65.1
53. apple : 48.8
54. sandwich : 58.1
55. orange : 63.4
56. broccoli : 85.5
57. carrot : 54.1
58. hot dog : 51.2
59. pizza : 85.3
60. donut : 51.7
61. cake : 54.0
62. chair : 57.7
63. couch : 56.9
64. potted plant : 44.6
65. bed : 60.4
67. dining table : 66.8
70. toilet : 87.0
72. tv : 71.6
73. laptop : 69.2
74. mouse : 69.5
75. remote : 41.7
76. keyboard : 71.5
77. cell phone : 38.7
78. microwave : 51.7
79. oven : 67.9
80. toaster : 6.5
81. sink : 77.2
82. refrigerator : 54.2
84. book : 49.9
85. clock : 71.9
86. vase : 58.3
87. scissors : 19.0
88. teddy bear : 70.8
89. hair drier : 1.8
90. toothbrush : 28.6
Figure 5: Per-class barplots of the output scores on the Microsoft COCO validation set. From top to bottom : (a) weakly-supervised classiﬁcation AP (blue) vs. full-image classiﬁcation AP (red). (b) weakly-supervised classiﬁcation AP (blue) vs. weakly-supervised location
prediction AP (green). (c) weakly-supervised location prediction AP (green) vs. masked-pooling location prediction AP (magenta). At the
bottom of the ﬁgure, we provide the object names and weakly-supervised classiﬁcation AP values.
using the center prediction baseline (K.CENTER PRED.) results in a >50% performance drop on COCO, and >30%
drop on Pascal VOC, compared to our weakly supervised
method (J.WEAK SUP) indicating the difﬁculty of the location prediction task on this data.
Comparison with R-CNN baseline.
In order to provide a
baseline for the location prediction task, we used the bounding box proposals and conﬁdence values obtained with the
state-of-the-art object detection R-CNN algorithm on
the Pascal VOC 2012 validation set. Note that this algorithm was not designed for classiﬁcation, and its goal is to
ﬁnd all the objects in an image, while our algorithm looks
only for a single instance of a given object class. To make
the comparison as fair as possible, we process the R-CNN
results to be compatible with our metric, keeping for each
class and image only the best-scoring bounding box proposal and using the center of the bounding box for evaluation. Results are summarized in Table 2 and the detailed
per-class results are shown in Table 3. Interestingly, our
weakly supervised method (J.WEAK SUP) achieves comparable location prediction performance to the strong R-CNN
baseline, which uses object bounding boxes at training time.
Does adding object-level supervision help location prediction?
Here we investigate whether adding the objectlevel supervision (with masked pooling) helps to better predict the locations of objects in the image. The results on
the Pascal VOC dataset are shown in Table 3 and show a
very similar overall performance for our weakly supervised
(J.WEAK SUP) method compared to the object-level supervised (I.MASKED POOL) setup. This is interesting as it indicates that our weakly supervised method learns to predict
object locations and adding object-level supervision does
not signiﬁcantly increase the overall location prediction performance. Results on the COCO dataset are shown in Figure 5 (bottom) and indicate that for some classes with poor
location prediction performance in the weakly supervised
setup (green) adding object-level supervision (masked pooling, magenta) helps. Examples are small sports objects such
as frisbee, tennis racket, baseball bat, snowboard, sports
ball, or skis. While for classiﬁcation the likely presence of
these objects can be inferred from the scene context, objectlevel supervision can help to understand better the underlying concept and predict the object location in the image. We
examine the importance of the object context next.
The importance of object context.
To better assess the
importance of object context for the COCO dataset we directly compare the classiﬁcation (blue) and location prediction (green) scores in Figure 5 (middle). In this setup a high
classiﬁcation score but low location prediction score means
Figure 6: Example location predictions for images from the Microsoft COCO validation set obtained by our weakly-supervised method.
Note that our method does not use object locations at training time, yet can predict locations of objects in test images (yellow crosses). The
method outputs the most conﬁdent location per object per class. Please see additional results on the project webpage .
that the classiﬁcation decision was taken primarily based on
the object context. Fore example, the presence of a baseball
ﬁeld is a strong indicator for presence of a baseball bat and
a baseball glove. However, as discussed above these objects
are hard to localize in the image. The kitchenware (forks,
knives, spoons) and electronics (laptop, keyboard, mouse)
superclasses show a similar behavior. Nevertheless, a good
classiﬁcation result can still be informative and can guide a
more precise search for these objects in the image.
Predicting extent of objects.
To evaluate the ability to
predict the extent of objects (not just the location) we also
evaluate our method using the standard area overlap ratio as
used in object detection . We have implemented a simple extension of our method that aggregates CNN scores
within selective search object proposals. This procedure obtains on the Pascal VOC 2012 validation set the
mAP of 11.74, 27.47, 43.54% for area overlap thresholds
0.5, 0.3, 0.1, respectively. The relatively low performance
could be attributed to (a) the focus of the network on discriminative object parts (e.g. aeroplane propeller, as in Figure 4) rather than the entire extent of an object and (b) no
max-pooling over scales in our current training procedure.
Similar behavior on discriminative parts was recently observed in scene classiﬁcation .
7. Conclusion
So, is object localization with convolutional neural networks for free? We have shown that our weakly supervised
CNN architecture learns to predict the location of objects
in images despite being trained from cluttered scenes with
only weak image-level labels. We believe this is possible
because of (i) the hierarchical convolutional structure of
CNNs that appears to have a bias towards spatial localization combined with (ii) the extremely efﬁcient end-to-end
training that back-propagates loss gradients from imagelevel labels to candidate object locations. While the approximate position of objects can be predicted rather reliably, this is not true (at least with the current architecture)
for the extent of objects as the network tends to focus on
distinctive object parts. However, we believe our results are
signiﬁcant as they open-up the possibility of large-scale reasoning about object relations and extents without the need
for detailed object level annotations.
Acknowledgements. This work was supported by the MSR-
INRIA laboratory, ERC grant Activia (no. 307574), ERC grant
Leap (no. 336845) and the ANR project Semapolis (ANR-13-
CORD-0003).