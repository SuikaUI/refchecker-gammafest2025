Interpretability Beyond Feature Attribution:
Quantitative Testing with Concept Activation Vectors (TCAV)
Been Kim Martin Wattenberg Justin Gilmer Carrie Cai James Wexler
Fernanda Viegas
Rory Sayres
The interpretation of deep learning models is a
challenge due to their size, complexity, and often
opaque internal state. In addition, many systems,
such as image classiﬁers, operate on low-level features rather than high-level concepts. To address
these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net’s internal state in terms of
human-friendly concepts. The key idea is to view
the high-dimensional internal state of a neural net
as an aid, not an obstacle. We show how to use
CAVs as part of a technique, Testing with CAVs
(TCAV), that uses directional derivatives to quantify the degree to which a user-deﬁned concept
is important to a classiﬁcation result–for example, how sensitive a prediction of zebra is to the
presence of stripes. Using the domain of image
classiﬁcation as a testing ground, we describe how
CAVs may be used to explore hypotheses and generate insights for a standard image classiﬁcation
network as well as a medical application.
1. Introduction
Understanding the behavior of modern machine learning
(ML) models, such as neural networks, remains a signiﬁcant
challenge. Given the breadth and importance of ML applications, however, it is important to address this challenge.
In addition to ensuring accurate predictions, and giving scientists and engineers better means of designing, developing,
and debugging models, interpretability is also important to
ensure that ML models reﬂect our values.
One natural approach to interpretability is to describe an
ML model’s predictions in terms of the input features it
considers. For instance, in logistic regression classiﬁers,
coefﬁcient weights are often interpreted as the importance
of each feature. Similarly, saliency maps give importance
weights to pixels based on ﬁrst-order derivatives .
A key difﬁculty, however, is that most ML models operate on
features, such as pixel values, that do not correspond to highlevel concepts that humans easily understand. Furthermore,
a model’s internal values (e.g., neural activations) can seem
incomprehensible. We can express this difﬁculty mathematically, viewing the state of an ML model as a vector space
Em spanned by basis vectors em which correspond to data
such as input features and neural activations. Humans work
in a different vector space Eh spanned by implicit vectors
eh corresponding to an unknown set of human-interpretable
From this standpoint, an “interpretation” of an ML model
can be seen as function g : Em →Eh. When g is linear, we
call it a linear interpretability. In general, an interpretability function g need not be perfect ; it
may fail to explain some aspects of its input domain Em and
it will unavoidably not cover all possible human concepts in
In this work, the high-level concepts of Eh are deﬁned using sets of example input data for the ML model under
inspection. For instance, to deﬁne concept ‘curly’, a set of
hairstyles and texture images can be used. Note the concepts
of Eh are not constrained to input features or training data;
they can be deﬁned using new, user-provided data. Examples are shown to be effective means of interfacing with ML
models for both non-expert and expert users .
This work introduces the notion of a Concept Activation Vector (CAV) as a way of translating between Em and Eh. A
CAV for a concept is simply a vector in the direction of the
values (e.g., activations) of that concept’s set of examples.
In this paper, we derive CAVs by training a linear classiﬁer
between a concept’s examples and random counterexamples
and then taking the vector orthogonal to the decision boundary. This simple approach is supported by recent work using
local linearity .
The main result of this paper is a new linear interpretability
method, quantitative Testing with CAV (TCAV) (outlined in
Figure 1). TCAV uses directional derivatives to quantify
 
Testing with Concept Activation Vectors (TCAV)
Figure 1. Testing with Concept Activation Vectors: Given a user-deﬁned set of examples for a concept (e.g., ‘striped’), and random
examples a⃝, labeled training-data examples for the studied class (zebras) b⃝, and a trained network c⃝, TCAV can quantify the model’s
sensitivity to the concept for that class. CAVs are learned by training a linear classiﬁer to distinguish between the activations produced by
a concept’s examples and examples in any layer d⃝. The CAV is the vector orthogonal to the classiﬁcation boundary (vl
C, red arrow). For
the class of interest (zebras), TCAV uses the directional derivative SC,k,l(x) to quantify conceptual sensitivity e⃝.
the model prediction’s sensitivity to an underlying highlevel concept, learned by a CAV. For instance, given an ML
image model recognizing zebras, and a new, user-deﬁned
set of examples deﬁning ‘striped’, TCAV can quantify the
inﬂuence of striped concept to the ‘zebra’ prediction as
a single number. In addition, we conduct statistical tests
where CAVs are randomly re-learned and rejected unless
they show a signiﬁcant and stable correlation with a model
output class or state value. (This is detailed in Section 3.2).
Our work on TCAV was pursued with the following goals.
Accessibility: Requires little to no ML expertise of user.
Customization: Adapts to any concept (e.g., gender) and
is not limited to concepts considered during training.
Plug-in readiness: Works without any retraining or modi-
ﬁcation of the ML model.
Global quantiﬁcation: Can interpret entire classes or sets
of examples with a single quantitative measure, and
not just explain individual data inputs.
We perform experiments using TCAV to gain insights and
reveal dataset biases in widely-used neural network models and with a medical application (diabetic retinopathy),
conﬁrming our ﬁndings with a domain expert. We conduct human subject experiments to quantitatively evaluate
feature-based explanations and to contrast with TCAV.
2. Related work
In this section, we provide an overview of existing interpretability methods, methods speciﬁc to neural networks,
and methods that leverage the local linearity of neural networks.
2.1. Interpretability methods
To achieve interpretability, we have two options: (1) restrict
ourselves to inherently interpretable models or (2) postprocess our models in way that yields insights. While option
1 offers simplicity as the explanation is embedded in the
model , this option might be costly for users who already
have a working high performance model. With increasing
demands for more explainable ML , there is an growing need for methods that can be
applied without retraining or modifying the network.
One of many challenges of option 2 is to ensure that the
explanation correctly reﬂects the model’s complex internals.
One way to address this is to use the generated explanation as an input, and check the network’s output for validation. This is typically used in perturbation-based/sensitivity
analysis-based interpretability methods to either use data
points or features as a form of perturbation, and check
how the network’s response changes. They maintain the
consistency either locally (i.e., explanation is true for a data
point and its neighbors) or globally (i.e., explanation is true
for most data points in a class) by construction. TCAV is a
type of global perturbation method, as it perturbs data points
towards a human-relatable concept to generate explanations.
However, even a perturbation-based method can be inconsistent if the explanation is only true for a particular data point
and its neighbors (i.e., local explanation), and not for all inputs in the class. For example, they
may generate contradicting explanations for two data points
in the same class, resulting in decreased user trust. TCAV
produces explanations that are not only true for a single data
Testing with Concept Activation Vectors (TCAV)
point, but true for each class (i.e., global explanation).
2.2. Interpretability methods in neural networks
The goal of TCAV is to interpret high dimensional Em
such as that of neural network models. Saliency methods
are one of the most popular local explanation methods for
image classiﬁcation . These techniques typically produce a map
showing how important each pixel of a particular picture is
for its classiﬁcation, as shown in Figure 8. While a saliency
map often identiﬁes relevant regions and provides a type
of quantiﬁcation (i.e., importance for each pixel), there are
a couple of limitations: 1) since a saliency map is given
conditioned on only one picture (i.e., local explanation),
humans have to manually assess each picture in order to
draw a class-wide conclusion, and 2) users have no control
over what concepts of interest these maps pick up on (lack
of customization). For example, consider two saliency maps
of two different cat pictures, with one picture’s cat ears
having more brightness. Can we assess how important the
ears were in the prediction of “cats”?
Furthermore, some recent work has demonstrated that
saliency maps produced by randomized networks are similar
to that of the trained network , while
simple meaningless data processing steps, such as mean
shift, may cause saliency methods to result in signiﬁcant
changes . Saliency maps may also
be vulnerable to adversarial attacks .
2.3. Linearity in neural network and latent dimensions
There has been much research demonstrating that linear
combinations of neurons may encode meaningful, insightful
information .
Both and show
that meaningful directions can be efﬁciently learned via
simple linear classiﬁers. Mapping latent dimensions to
human concepts has also been studied in the context of
words , and in the context of GANs to
generate attribute-speciﬁc pictures . A similar idea to using such concept vectors in latent dimension in
the context of generative model has also been explored .
Our work extends this idea and computes directional derivatives along these learned directions in order to gather the
importance of each direction for a model’s prediction. Using TCAV’s framework, we can conduct hypothesis testing
on any concept on the ﬂy (customization) that make sense
to the user (accessibility) for a trained network (plug-in
readiness) and produce a global explanation for each class.
3. Methods
This section explains our ideas and methods: (a) how to
use directional derivatives to quantify the sensitivity of ML
model predictions for different user-deﬁned concepts, and
(b) how to compute a ﬁnal quantitative explanation (TCAVQ
measure) of the relative importance of each concept to each
model prediction class, without any model retraining or
modiﬁcation.
Without loss of generality, we consider neural network models with inputs x ∈Rn and a feedforward layer l with m
neurons, such that input inference and its layer l activations
can be seen as a function fl : Rn →Rm.
3.1. User-deﬁned Concepts as Sets of Examples
The ﬁrst step in our method is to deﬁne a concept of interest. We do this simply by choosing a set of examples
that represent this concept or ﬁnd an independent data set
with the concept labeled. The key beneﬁt of this strategy is
that it does not restrict model interpretations to explanations
using only pre-existing features, labels, or training data of
the model under inspection.
Instead, there is great ﬂexibility for even non-expert ML
model analysts to deﬁne concepts using examples and explore and reﬁne concepts as they test hypotheses during
analysis. Section 4 describes results from experiments with
small number of images (30) collected using a search engine.
For the case of fairness analysis (e.g., gender, protected
groups), curated examples are readily available (Huang et al.,
3.2. Concept Activation Vectors (CAVs)
Following the approach of linear interpretability, given a
set of examples representing a concept of human interest,
we seek a vector in the space of activations of layer l that
represents this concept. To ﬁnd such a vector, we consider
the activations in layer l produced by input examples that
in the concept set versus random examples. We then deﬁne
a “concept activation vector” (or CAV) as the normal to
a hyperplane separating examples without a concept and
examples with a concept in the model’s activations (see red
arrow in Figure 1).
This approach lends itself to a natural implementation.
When an analyst is interested in a concept C (say, striped
textures) they may gather a positive set of example inputs
PC (e.g., photos of striped objects) and and a negative set N
(e.g., a set of random photos). Then, a binary linear classiﬁer
can be trained to distinguish between the layer activations
of the two sets: {fl(x) : x ∈PC} and {fl(x) : x ∈N}.1
1For convnets, a layer must be ﬂattened so width w, height h,
and c channels becomes a vector of m = w × h × c activations.
Testing with Concept Activation Vectors (TCAV)
This classiﬁer vl
C ∈Rm is a linear CAV for the concept C.
3.3. Directional Derivatives and Conceptual Sensitivity
Interpretability methods like saliency maps use the gradients
of logit values with respect to individual input features, like
pixels, and compute
where hk(x) is the logit for a data point x for class k and
xa,b is a pixel at position (a, b) in x. Thus, saliency uses
the derivative to gauge the sensitivity of the output class k
to changes in the magnitude of pixel (a, b).
By using CAVs and directional derivatives, we instead gauge
the sensitivity of ML predictions to changes in inputs towards the direction of a concept, at neural activation layer l.
C ∈Rm is a unit CAV vector for a concept C in layer l,
and fl(x) the activations for input x at layer l, the “conceptual sensitivity” of class k to concept C can be computed as
the directional derivative SC,k,l(x):
hl,k(fl(x) + ϵvl
C) −hl,k(fl(x))
∇hl,k(fl(x)) · vl
where hl,k : Rm →R. This SC,k,l(x) can quantitatively
measure the sensitivity of model predictions with respect
to concepts at any model layer. It is not a per-feature metric (e.g., unlike per-pixel saliency maps) but a per-concept
scalar quantity computed on a whole input or sets of inputs.
3.4. Testing with CAVs (TCAV)
Testing with CAVs, or TCAV, uses directional derivatives
to compute ML models’ conceptual sensitivity across entire
classes of inputs. Let k be a class label for a given supervised learning task and let Xk denote all inputs with that
given label. We deﬁne the TCAV score to be
TCAVQC,k,l = |{x ∈Xk : SC,k,l(x) > 0}|
the fraction of k-class inputs whose l-layer activation vector was positively inﬂuenced by concept C,
TCAVQC,k,l ∈ . Note that TCAVQC,k,l only depends
on the sign of SC,k,l, one could also use a different metric
that considers the magnitude of the conceptual sensitivities. The TCAVQ metric allows conceptual sensitivities to
be easily interpreted, globally for all inputs in a label.
3.5. Statistical signiﬁcance testing
One pitfall with the TCAV technique is the potential for
learning a meaningless CAV. After all, using a randomly
chosen set of images will still produce a CAV. A test based
on such a random concept is unlikely to be meaningful.
To guard against spurious results from testing a class against
a particular CAV, we propose the following simple statistical
signiﬁcance test. Instead of training a CAV once, against a
single batch of random examples N, we perform multiple
training runs, typically 500. A meaningful concept should
lead to TCAV scores that behave consistently across training
Concretely we perform a two-sided t-test of the TCAV
scores based on these multiple samples. If we can reject the
null hypothesis of a TCAV score of 0.5, we can consider
the resulting concept as related to the class prediction in a
signiﬁcant way. Note that we also perform a Bonferroni
correction for our hypotheses (at p < α/m with m = 2) to
control the false discovery rate further. All results shown in
this paper are CAVs that passed this testing.
3.6. TCAV extensions: Relative TCAV
In practice, semantically related concepts (e.g., brown hair
vs. black hair) often yield CAVs that are far from orthogonal.
This natural, expected property may be beneﬁcially used to
make ﬁne-grained distinctions since relative comparisons
between related concepts are a good interpretative tool .
Relative CAVs allow making such ﬁne-grained comparisons.
Here the analyst selects two sets of inputs that represent
two different concepts, C and D. Training a classiﬁer on
fl(PC) and fl(PD) yields a vector vl
C,D ∈Rm. The vector
C,D intuitively deﬁnes a 1-d subspace in layer l where
the projection of an embedding fl(x) along this subspace
measures whether x is more relevant to concept C or D.
Relative CAVs may, for example, apply to image recognition, where we can hypothesize that concepts for ‘dotted’,
‘striped’, and ‘meshed’ textures are likely to exist as internal
representations, and be correlated or overlapping. Given
three positive example sets Pdot, Pstripe, and Pmesh, a relative
CAV can be derived by constructing, for each, a negative input set by complement (e.g., {Pdot ∪Pmesh} for the stripes).
The TCAVQ measures enabled by the resulting relative CAV
are used in many of the experiments in the following Section 4, e.g., to gauge the relative importance of stripes to
zebras and that of diagnostic concepts for diabetic retinopathy.
4. Results
We ﬁrst show evidence that CAVs align with intended concepts of interest, by sorting images based on how similar
they are to various concepts (Section 4.1.1) and by using an
activation maximization technique, empirical deep dream,
on the CAVs (Section 4.1.2). We then summarize gained
insights and revealed biases of two widely used networks
Testing with Concept Activation Vectors (TCAV)
using TCAV (Section 4.2.1). For further validation, we create a dataset and settings where we have an approximated
ground truth for TCAVQ. We show that TCAV closely tracks
the ground truth (Section 4.3.1) while saliency maps are
unable to communicate this ground truth to humans (Section 4.3.2). Finally we apply TCAV to help interpret a model
predicting diabetic retinopathy (DR) (Section 4.4), where
TCAV provided insights when the model diverged with the
domain expert’s knowledge.
4.1. Validating the learned CAVs
The ﬁrst step is to convince ourselves that the learned CAVs
are aligned with the intended concepts of interest. We ﬁrst
sort the images of any class with respect to CAVs for inspection. Then we learn patterns that maximally activate each
CAV using an activation maximization technique for further
visual conﬁrmation.
4.1.1. SORTING IMAGES WITH CAVS
We can use CAVs to sort images with respect to their relation
to the concept. This is useful for qualitative conﬁrmation
that the CAVs correctly reﬂect the concept of interest. As
a CAV encodes the direction of a concept in the vector
space of a bottleneck, vl
C ∈Rm using the activations of
the concept pictures, fl(xi) ∈Rm as described Section 3.2,
we can compute cosine similarity between a set of pictures
of interest to the CAV to sort the pictures. Note that the
pictures being sorted are not used to train the CAV.
Figure 2. The most and least similar pictures of stripes using ‘CEO’
concept (left) and neckties using ‘model women’ concept (right)
The left of Figure 2 shows sorted images of stripes with
respect to a CAV learned from a more abstract concept,
‘CEO’ (collected from ImageNet). The top 3 images are
pinstripes which may relate to the ties or suits that a CEO
may wear. The right of Figure 2 shows sorted images of
neckties with respect to a ‘model women’ CAV. All top 3
images show women in neckties.
This also suggests that CAVs can be as a standalone similarity sorter, to sort images to reveal any biases in the example
images from which the CAV is learned.
4.1.2. EMPIRICAL DEEP DREAM
Another way to visually conﬁrm our conﬁdence in a CAV is
to optimize for a pattern that maximally activates the CAV
and compare that to our semantic notions of the concept.
Activation maximization techniques, such as Deep Dream or
Lucid , are often
used to visualize patterns that would maximally activate a
neuron, set of neurons or random directions. This technique
is also applied to AI-aided art . As
is typically done, we use a random image as a starting point
for the optimization to avoid choosing an arbitrary image.
Using this technique, we show that CAVs do reﬂect their
underlying concepts of interest. Figure 3 shows the results
of deep dreamed patterns for knitted texture, corgis and
Siberian huskey CAVs. We include results from all layers and many other CAVs in the appendix. This suggests
that TCAV can be used to identify and visualize interesting
directions in a layer.
Figure 3. Empirical Deepdream using knitted texture, corgis and
Siberian huskey concept vectors (zoomed-in)
4.2. Insights and biases: TCAV for widely used image
classiﬁcations networks
In this section, we apply TCAV to two popular networks to
1) further conﬁrm TCAV’s utility, 2) reveal biases, and 3)
show where concepts are learned in these networks.
4.2.1. GAINING INSIGHTS USING TCAV
We applied TCAV for two widely used networks . We tried various types of CAVs, including color, texture, objects, gender and race. Note that
none of these concepts were in the set of the network’s class
labels; instead all were collected from or a popular
image search engine. We show TCAV results with CAVs
learned from all (for GoogleNet) or a subset (for Inception
V3) of layers.
As shown in Figure 4, some results conﬁrmed our commonsense intuition, such as the importance of the red concept for
ﬁre engines, the striped concept for zebras, and the Siberian
husky concept for dogsleds. Some results also conﬁrmed
our suspicion that these networks were sensitive to gender
and race, despite not being explicitly trained with these
categories. For instance, TCAV provides quantitative con-
ﬁrmations to the qualitative ﬁndings from that found ping-pong balls are highly correlated with
a particular race. TCAV also ﬁnds the ‘female’ concept
highly relevant to the ‘apron’ class. Note that the race con-
Testing with Concept Activation Vectors (TCAV)
Figure 4. Relative TCAV for all layers in GoogleNet and last three layers in Inception V3 for conﬁrmation (e.g., ﬁre engine), discovering biases (e.g.,
rugby, apron), and quantitative conﬁrmation for previously qualitative ﬁndings in 
(e.g., dumbbell, ping-pong ball). TCAVQs in layers close to the
logit layer (red) represent more direct inﬂuence on the prediction
than lower layers. ‘*’s mark CAVs omitted after statistical testing.
cept (ping-pong ball class) shows a stronger signal as it gets
closer to the ﬁnal prediction layer, while the texture concept
(e.g., striped) inﬂuences TCAVQ in earlier layers (zebra
We also observed that the statistical signiﬁcance testing
(Section 3.5) of CAVs successfully ﬁlters out spurious results. For instance, it successfully ﬁltered out spurious CAVs
where the ‘dotted’ concept returned high TCAVQ (e.g.,
mixed4a) for zebra classes. The statistical signiﬁcance testing of CAVs successfully eliminated CAVs in this layer; all
CAVs that passed this testing consistently returned ‘striped’
as the most important concept.
In some cases, it was sufﬁcient to use a small number of pictures to learn CAVs. For the ‘dumbbell’ class, we collected
30 pictures of each concept from a popular image search
engine. Despite the small number of examples, Figure 4
shows that TCAV successfully identiﬁed that the ‘arms’ concept was more important to predict dumbbell class than
other concepts. This ﬁnding is consistent with previous
qualitative ﬁndings from , where
a neuron’s DeepDream picture of a dumbbell showed an
arm holding it. TCAV allows for quantitative conﬁrmation
of this previously qualitative ﬁnding.
4.2.2. TCAV FOR WHERE CONCEPTS ARE LEARNED
In the process of learning CAVs, we train a linear classiﬁer
to separate each concept. We can use the performance of
these linear classiﬁers to obtain lower-bound approximates
for which layer each concept is learned.
Figure 5. The accuracies of CAVs at each layer. Simple concepts
(e.g., colors) achieve higher performance in lower-layers than more
abstract or complex concepts (e.g. people, objects)
Figure 5 shows that the accuracy of more abstract concepts
(e.g., objects) increases in higher layers of the network.
The accuracy of simpler concepts, such as color, is high
throughout the entire network. This is a conﬁrmation of
many prior ﬁndings that lower
layers operate as lower level feature detectors (e.g., edges),
while higher layers use these combinations of lower-level
features to infer higher-level features (e.g., classes). The
accuracies are measured by a held out test set of 1/3 the size
of the training set.
4.3. A controlled experiment with ground truth
Figure 6. A controlled training set: Regular images and images
with captions for the cab and cucumber class.
The goal of this experiment is demonstrate that TCAV can
be successfully used to interpret the function learned by
a neural network in a carefully controlled setting where
ground truth is known. We show quantitative results of
TCAV and compare these with our evaluation of saliency
Testing with Concept Activation Vectors (TCAV)
To this end we create a dataset of three arbitrary classes
(zebra, cab, and cucumber) with potentially noisy captions
written in the image (example shown in Figure 6). The
noise parameter p ∈[0, 1.0] controls the probability that
the image caption agrees with the image class. If there is
no noise (p = 0), the caption always agrees with the image
label, e.g. a picture of a cab always contains the word “cab”
at the bottom. At p = .3, each picture has a 30% chance
of having the correct caption replaced with a random word
(e.g. “rabbit”).
We then train 4 networks, each on a dataset with a different noise parameter p in . Each network may learn to
pay attention to either images or captions (or both) in the
classiﬁcation task. To obtain an approximated ground truth
for which concept each network paid attention, we can test
the network’s performance on images without captions. If
the network used the image concept for classiﬁcation, the
performance should remain high. If not, the network performance will suffer. We create image CAVs using each class’s
images, and caption CAVs using captions with other pixels
in the image randomly shufﬂed.
4.3.1. QUANTITATIVE EVALUATION OF TCAV
Figure 7. TCAV results with approximated ground truth: Both cab
and cucumber classes, TCAVQ closely matches the ground truth.
For the cab class, the network used image concept more than the
caption concept regardless of the models.
Overall, we ﬁnd that the TCAV score closely mirrors the
concept that the network paid attention to (Figure 7). Accuracy results suggest that, when classifying cabs, the network
used the image concept more than the caption concept, regardless of the noise parameter. However, when classifying
cucumbers, the network sometimes paid attention to the
caption concept and sometimes the image concept. Figure 7
shows that the TCAVQ closely matches this ground truth. In
the cab class, the TCAVQ for the image concept is high, consistent with its high test performance on caption-less images.
In the cucumber class, the TCAVQ for the image concept
increases as noise level increases, consistent with the observation that accuracy also increases as noise increases.
4.3.2. EVALUATION OF SALIENCY MAPS WITH HUMAN
Saliency maps are an alternative way to communicate the
same information, and are commonly used as an inter-
Figure 8. Saliency map results with approximated ground truth:
Models trained on datasets with different noise parameter p (rows)
and different saliency map methods (columns) are presented. The
approximated ground truth is that the network is paying a lot more
attention to the image than the caption in all cases, which is not
clear from saliency maps.
pretability method for image-based networks (see Section 2).
Qualitatively, as shown in Figure 8 for the cab class, it is
not clear that the four networks used the image concept
more than the caption concept. In this section, we quantitatively evaluate what information saliency maps are able to
communicate to humans, via a human subject experiment.
We took the saliency maps generated from the previous section to conduct a 50-person human experiment on Amazon
Mechanical Turk. For simplicity, we evaluated two of the
four noise levels (0% and 100% noise), and two types of
saliency maps and ).
Each worker did a series of six tasks (3 object classes × 2
saliency map types), all for a single model. Task order was
randomized. In each task, the worker ﬁrst saw four images
along with their corresponding saliency masks. They then
rated how important they thought the image was to the
model (10-point scale), how important the caption was to
the model (10-point scale), and how conﬁdent they were
in their answers (5-point scale). In total, turkers rated 60
unique images (120 unique saliency maps).
Overall, saliency maps correctly communicated which concept was more important only 52% of the time (random
chance is 50% for two options). Wilcox signed-rank tests
show that in more than half of the conditions, there was
either no signiﬁcant difference in the perceived importance
of the two concepts, or the wrong concept was identiﬁed
as being more important. Figure 9 (top) shows one example where saliency maps communicated the wrong concept
importance. In spite of this, the percent of correct answers
rated as very conﬁdent was similar to that of incorrect answers (Figure 9 bottom), suggesting that interpreting using
saliency maps alone could be misleading. Furthermore,
Testing with Concept Activation Vectors (TCAV)
Figure 9. For the cab class, the ground truth was that the image
concept was more important than the caption concept. However,
when looking at saliency maps, humans perceived the caption
concept as being more important (model with 0% noise), or did not
discern a difference (model with 100% noise). In contrast, TCAV
results correctly show that the image concept was more important.
Overall, the percent of correct answers rated as very conﬁdent was
similar to that of incorrect answers, indicating that saliency maps
may be misleading.
when one of the saliency map methods correctly communicated the more important concept, it was always the case
that the other saliency map method did not, and vice versa.
4.4. TCAV for a medical application
We now apply TCAV to the real-world problem of predicting
diabetic retinopathy (DR), a treatable but sight-threatening
condition, from retinal fundus images .
We consulted with a medical expert about our results.
The model of interest predicts DR level using a 5-point
grading scale based on complex criteria, from level 0 (no
DR) to 4 (proliferative). Doctors’ diagnoses of DR level
depend on evaluating a set of diagnostic concepts, such as
microaneurysms (MA) or pan-retinal laser scars (PRP), with
different concepts being more prominent at different DR
levels. We sought to test the importance of these concepts
to the model using TCAV.
For some DR levels, TCAV identiﬁed the correct diagnostic
concepts as being important. As shown in Figure 10 (top),
the TCAV score was high for concepts relevant to DR level
4, and low for a non-diagnostic concept.
For DR level 1, TCAV results sometimes diverge from doctors’ heuristics ( Figure 10 bottom). For example, aneurysms
(HMA) had a relatively high TCAV score, even though they
are diagnostic of a higher DR level (see HMA distribution in
Figure 10). However, consistent with this ﬁnding, the model
often over-predicted level 1 (mild) as level 2 (moderate).
Given this, the doctor said she would like to tell the model
to de-emphasize the importance of HMA for level 1. Hence,
TCAV may be useful for helping experts interpret and ﬁx
model errors when they disagree with model predictions.
Figure 10. Top: A DR level 4 image and TCAV results. TCAVQ is
high for features relevant for this level (green), and low for an
irrelevant concept (red). Middle: DR level 1 (mild) TCAV results.
The model often incorrectly predicts level 1 as level 2, a model
error that could be made more interpretable using TCAV: TCAVQs
on concepts typically related to level 1 (green, MA) are high in
addition to level 2-related concepts (red, HMA). Bottom: the HMA
feature appears more frequently in DR level 2 than DR level 1.
5. Conclusion and Future Work
The method presented here, TCAV, is a step toward creating
a human-friendly linear interpretation of the internal state
of a deep learning model, so that questions about model
decisions may be answered in terms of natural high-level
concepts. Crucially, these concepts do not need to be known
at training time, and may easily be speciﬁed during a post
hoc analysis via a set of examples.
Our experiments suggest TCAV can be a useful technique in
an analyst’s toolbox. We provided evidence that CAVs do indeed correspond to their intended concepts. We then showed
how they may be used to give insight into the predictions
made by various classiﬁcation models, from standard image
classiﬁcation networks to a specialized medical application.
There are several promising avenues for future work based
on the concept attribution approach. While we have focused
on image classiﬁcation systems, applying TCAV to other
types of data (audio, video, sequences, etc.) may yield new
insights. TCAV may also have applications other than interpretability: for example, in identifying adversarial examples
for neural nets (see appendix). Finally, one could ask for
ways to identify concepts automatically and for a network
that shows super-human performance, concept attribution
may help humans improve their own abilities.
Testing with Concept Activation Vectors (TCAV)
ACKNOWLEDGMENTS
We would like to thank Daniel Smilkov for helpful discussions. We thank Alexander Mordvintsev for providing tfzoo
code. We also thank Ethan R Elenberg, David Alvarez Melis
and an anonymous reviewer for helpful comments and discussions. We thank Alexander Mordvintsev, Chris Olah and
Ludwig Schubert for generously allowing us to use their
code for DeepDream. Thanks to Christopher for sharing
early work on doing attribution to semantically meaningful
channels. Work from Nicholas Carlini, on training linear
classiﬁers for non-label concepts on logit-layer activations,
was one of our motivations. Finally, we would like to thank
Dr. Zahra Rastegar for evaluating diabetic retinopathy results, and provided relevant medical expertise.
Testing with Concept Activation Vectors (TCAV)
In this appendix, we show other experiments we conducted
and additional results and ﬁgures.
A. TCAV on adversarial examples
prediction: zebra
Figure 11. Two types of adversarial images that are classiﬁed as
zebra. In both cases, the distribution of TCAVQare different from
that of a normal zebra.
Adversarial examples are small, often
visually imperceptible changes to an image which can cause
an arbitrarily change to a network’s predicted class. We
conduct a simple experiment to see whether TCAV is fooled
by adversarial examples. In Figure 11, TCAV returns a high
score for the striped concept for zebra pictures. We create
two sets of adversarial examples, both by performing a targeted attack using a single step of the Fast Gradient Sign
Method . We successfully make the
network believe that an essentially random noise image (top
middle in Figure 11) and a randomly sampled non-zebra
image with imperceptible changes (top right in Figure 11)
are zebras (100% for noise image, 99% for the latter). However, the distribution of TCAVQs for the regular zebra and
adversarial images remain different (bottom in Figure 11).
While this may not be sufﬁcient direction for building a
defense mechanism, one can imagine having a dictionary of
concepts where we know the usual distribution of TCAVQs
for each class. If we want an ’alert’ for potential adversarial
attacks, we can compare the ’usual’ distribution of TCAVQs
to that of the suspicious images.
B. Additional Results: Insights and biases:
TCAV for widely used image classiﬁcations
We provide further results on Section 4.2.1.
C. Additional Results: Empirical Deep
D. Additional Results: Sorting Images with
Testing with Concept Activation Vectors (TCAV)
Figure 12. TCAV results for each layer
Testing with Concept Activation Vectors (TCAV)
Figure 13. Empirical deepdream using CAVs for each layer in Googlenet.
Testing with Concept Activation Vectors (TCAV)
Figure 14. Empirical deepdream using CAVs for each layer in Googlenet.
Testing with Concept Activation Vectors (TCAV)
Figure 15. Additional Results: Sorting Images with CAVs
Figure 16. Additional Results: Sorting Images with CAVs
Testing with Concept Activation Vectors (TCAV)
Figure 17. Additional Results: Sorting Images with CAVs
Figure 18. Additional Results: Sorting Images with CAVs
Testing with Concept Activation Vectors (TCAV)
Figure 19. Additional Results: Sorting Images with CAVs
Testing with Concept Activation Vectors (TCAV)