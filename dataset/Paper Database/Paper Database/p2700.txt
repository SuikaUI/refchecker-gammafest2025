Improving Fairness in Machine Learning Systems:
What Do Industry Practitioners Need?
Kenneth Holstein
Carnegie Mellon University
Pittsburgh, PA
 
Jennifer Wortman Vaughan
Microsoft Research
New York, NY
 
Hal Daumé III
Microsoft Research &
University of Maryland
New York, NY
 
Miroslav Dudík
Microsoft Research
New York, NY
 
Hanna Wallach
Microsoft Research
New York, NY
 
The potential for machine learning (ML) systems to amplify
social inequities and unfairness is receiving increasing popular and academic attention. A surge of recent work has
focused on the development of algorithmic tools to assess
and mitigate such unfairness. If these tools are to have a
positive impact on industry practice, however, it is crucial
that their design be informed by an understanding of realworld needs. Through 35 semi-structured interviews and an
anonymous survey of 267 ML practitioners, we conduct the
first systematic investigation of commercial product teams’
challenges and needs for support in developing fairer ML
systems. We identify areas of alignment and disconnect between the challenges faced by teams in practice and the
solutions proposed in the fair ML research literature. Based
on these findings, we highlight directions for future ML and
HCI research that will better address practitioners’ needs.
CCS CONCEPTS
• Human-centered computing; • Social and professional
topics →Socio-technical systems; • Computing methodologies →Machine learning;
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. Copyrights
for components of this work owned by others than the author(s) must
be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. Request permissions from .
CHI 2019, May 4–9, 2019, Glasgow, Scotland UK
© 2019 Copyright held by the owner/author(s). Publication rights licensed
ACM ISBN 978-1-4503-5970-2/19/05...$15.00
 
algorithmic bias, fair machine learning, product teams, needfinding, empirical study, UX of machine learning
ACM Reference Format:
Kenneth Holstein, Jennifer Wortman Vaughan, Hal Daumé III, Miroslav
Dudík, and Hanna Wallach. 2019. Improving Fairness in Machine
Learning Systems: What Do Industry Practitioners Need?. In CHI
Conference on Human Factors in Computing Systems Proceedings
 , May 4–9, 2019, Glasgow, Scotland UK. ACM, New York,
NY, USA, 16 pages. 
INTRODUCTION
Machine learning (ML) systems increasingly influence every
facet of our lives, including the quality of healthcare and
education we receive , which news or social
media posts we see , who receives a job ,
who is released from jail , and who is subjected to
increased policing . With this growth, the potential of ML to amplify social inequities has received growing
attention across several research communities, as well as in
the popular press. It is now commonplace to see reports in
mainstream media of systemic unfair behaviors observed in
widely used ML systems—for example, an automated hiring
system that is more likely to recommend hires from certain
racial, gender, or age groups , or a search engine
that amplifies negative stereotypes by showing arrest-record
ads in response to queries for names predominantly given to
African American babies, but not for other names .
Substantial effort in the rapidly growing research literature on fairness in ML has centered on the development of
statistical definitions of fairness and algorithmic methods to assess and mitigate undesirable biases in
relation to these definitions . As the field matures,
integrated toolkits are being developed with the aim of making these methods more widely accessible and usable (e.g.,
 ). While some fair ML tools are already
 
being prototyped with practitioners, their initial design often
appears to be driven more by the availability of algorithmic
methods than by real-world needs (cf. ). If such tools
are to have a positive and meaningful impact on industry
practice, however, it is crucial that their design be informed
by an understanding of practitioners’ actual challenges and
needs for support in developing fairer ML systems .
We investigate the challenges faced by commercial ML
product teams—whose products affect the lives of millions
of users —in monitoring for unfairness and taking
appropriate action . Through semi-structured interviews with 35 practitioners, across 25 ML product teams from
10 major companies, we investigate teams’ existing practices
and challenges around fairness in ML, as well as their needs
for additional support. To better understand the prevalence
and generality of the key themes surfaced in our interviews,
we then conduct an anonymous survey of 267 industry ML
practitioners, across a broader range of contexts. To our
knowledge, this is the first systematic investigation of industry ML practitioners’ challenges and needs around fairness.
Through our investigation, we identify a range of realworld needs that have been neglected in the literature so
far, as well as several areas of alignment. For example, while
the fair ML literature has largely focused on “de-biasing”
methods and viewed the training data as fixed , most
of our interviewees report that their teams consider data
collection, rather than model development, as the most important place to intervene. Participants also often report
struggling to apply existing auditing and de-biasing methods
in their contexts. For instance, whereas previously proposed
methods typically require access to sensitive demographics at an individual level, such information is frequently
available only at coarser levels. The fair ML literature has
tended to focus on domains such as recidivism prediction,
automated hiring, and face recognition, where fairness can
be understood, at least partially, in terms of well-defined
quantitative metrics , whereas teams working
on applications involving richer interactions between the
user and the system (e.g., chatbots, web search, and adaptive
tutoring) brought up needs for more holistic, system-level
fairness auditing methods. Interviewees also stressed the
importance of explicitly considering biases and “blind spots”
that may be present in the humans embedded throughout
the ML development pipeline, such as crowdworkers or userstudy participants. Such concerns also extended to product
teams’ own blind spots. For instance, teams often struggled
to anticipate which subpopulations and forms of unfairness
they need to consider for specific kinds of ML applications.
Based on these findings, we highlight opportunities for the
ML and HCI research communities to have a greater impact
on industry and on the fairness of ML systems in practice.
BACKGROUND AND RELATED WORK
The design, prototyping, and maintenance of machine learning systems raises many unique challenges 
not commonly faced with other kinds of intelligent systems
or computing systems more broadly . The budding area of “UX for ML” has begun to explore new forms of
prototyping for ML systems, to provide earlier insights into
the complex, interacting UX impacts of particular dataset,
modeling, and system-design choices . In addition, a growing body of research focuses on the design and
development of programmer tools that can better support
developers in debugging and effectively monitoring the predictive performance of complex ML systems .
In parallel, the potential for undesirable biases in ML systems to exacerbate existing social inequities—or even generate new ones—has received considerable attention across
a range of academic disciplines, from ML to HCI to public policy, law, and ethics . Specialized research
conferences and initiatives are forming with a focus on biases and unfairness in data-driven algorithmic systems, such
as the Workshop on Fairness, Accountability, and Transparency in Machine Learning (FAT/ML) , the nascent
FAT* conference , AI Now , and the Partnership on
AI . Significant effort in the fair ML community has
focused on the development of statistical definitions of fairness and algorithmic methods to assess and
mitigate biases in relation to these definitions .
In contrast, the HCI community has studied unfairness in ML
systems through political, social, and psychological lenses,
among others (e.g., ). For example, HCI researchers have empirically studied users’ expectations and
perceptions related to fairness in algorithmic systems, finding that these do not always align with existing statistical
definitions . Other work has focused on auditing widely-used ML products from the outside ,
and often concluded with high-level calls to action aimed
at those responsible for developing and maintaining these
systems or for regulating their use . Lastly, Crawford , Springer et al. , and others have highlighted
an urgent need for internal processes and tools to support
companies in developing fairer systems in the first place.
Despite this widespread attention to biases and unfairness
in ML, to the best of our knowledge, only one prior study,
by Veale et al. , has investigated actual ML practitioners’ challenges and needs for support in creating fairer ML
systems. Veale et al. conducted exploratory interviews with
public-sector ML practitioners working across a range of
high-stakes contexts, such as predictive policing and
child mistreatment detection , to understand the challenges faced by practitioners in aligning the behavior of ML
systems with public values. Through these interviews, the authors uncovered several disconnects between the real-world
challenges that arise in public-sector ML practice compared
with those commonly presumed in the fair ML literature.
In the same spirit as Veale et al. , this work investigates ML practitioners’ needs for support, with the aim
of identifying fruitful opportunities for future research .
Whereas Veale et al. studied algorithm-assisted decision makers in high-stakes public-sector contexts—who are often experienced in thinking about fairness, yet relatively new to
working with ML systems—we study industry ML practitioners, who tend to be experienced in developing ML systems,
but relatively new to thinking about fairness. Supporting industry ML practitioners can also be viewed as a critical step
towards fairer algorithm-assisted decision making downstream, including in the public sector, where systems are
often built on top of ML products and APIs developed in
industry . Unlike the public-sector practitioners studied by Veale et al., the industry practitioners studied
here work on a much broader range of applications, such as
image captioning, web search, chatbots, speech recognition,
and personalized retail. In many of these applications, ethical
considerations may be even less clear cut than in high-stakes
public-sector contexts. Moreover, motivations and organizational priorities can differ considerably in industry contexts.
To better understand product teams’ needs for support in
developing fairer ML systems, we conducted a series of semistructured, one-on-one interviews with a total of 35 practitioners, across 25 ML product teams from 10 major companies. To investigate the prevalence and generality of the key
themes that emerged in these interviews, we then conducted
an anonymous survey with a broader sample of 267 industry
ML practitioners. For both the interviews and the survey,
“practitioners” were defined broadly as those who work in
any role on a team that develops products or services involving ML. The study went through an ethical review and was
IRB-approved. Semi-structured interview protocols and survey questions are provided in the supplementary materials.
Interview Study
Our interview study proceeded in two rounds. First, to get
a broad sense of challenges and needs, we conducted six
formative interviews. Building on emerging themes, we then
conducted more in-depth interviews with a larger sample.
The first six interviews were conducted with product managers (PMs) across different technology areas. Each interview
lasted 30 minutes and was conducted by teleconference because PMs were distributed across multiple countries. Each
PM was first asked to describe the products their team is
responsible for, who the customers of these products are,
and how their team is structured. Interviewees were then
asked whether fairness is something their team regularly discusses or incorporates into their workflow. The meaning of
“fairness” was intentionally left open as we were interested
in hearing PMs’ notions about what it might mean for their
products to be fair. However, if a PM requested clarification
at any point, we provided a broad definition: “Any case where
AI/ML systems perform differently for different groups in ways
that may be considered undesirable.” PMs were then asked
whether their team or customers had ever encountered issues
relating to fairness in their products. If so, PMs were asked for
concrete examples, and were asked high-level follow-up questions about these experiences. Otherwise, they were asked
whether they thought such issues might exist undetected,
and whether they had seen “other surprising or unexpected issues arise” (cf. ). These follow-ups sometimes led PMs to
realize they actually did have relevant experiences to share.
Our second, main round of interviews built on themes
that emerged during the initial round. We conducted more
detailed, semi-structured interviews to investigate teams’
current practices, challenges, and needs in greater depth.
Interviewees included 29 practitioners, across 19 ML product
teams from 10 major technology companies. As shown in
Table 1, we interviewed practitioners across a range of technology areas and team roles. Whenever possible, we tried
to interview people in different roles on the same team to
hear (potentially) different perspectives. Interviewees were
recruited using snowball sampling. We searched for news
articles related to ML biases and unfairness and contacted
members of teams whose products had previously received
relevant media coverage (i.e., news articles about unfair behavior observed in these products). In addition, we emailed
direct contacts across over 30 major companies. In both cases,
we asked contacts to share our interview invitation with any
colleagues working on ML products (in any role) at their own
company or others. At the end of each interview, interviewees were again encouraged to share any relevant contacts.
Although prospective interviewees were often eager to
participate and recruit colleagues, we encountered several
challenges resembling those discussed by Veale et al. .
For instance, given a recent trend of negative media coverage
calling out algorithmic biases and unfairness in widely-used
ML systems (e.g., ), our contacts often expressed strong fears that their team or company’s identity
might leak to the popular press, harming their reputation.
Some contacts revealed a general distrust of researchers,
citing cases where researchers have benefited by publicly
critiquing companies’ products from the outside instead of
engaging to help them improve their products. Finally, some
contacts worried that, in diving into the details of their teams’
Table 1: Interview demographics: Interviewees’ self-reported technology areas and team roles. Where multiple
people were interviewed from the same product team, interviewee identifiers are grouped in square brackets.
Technology Area
Roles of Interviewees
Interviewee IDs
Adaptive Tutoring & Mentoring
Chief Data Scientist, CTO, Data Scientist, Research Scientist
R10, [R13, R14], R30
CEO, Product Mgr., UX Researcher
[R17, R18], R35
Vision & Multimodal Sensing
CTO, ML Engineer, Product Mgr., Software Engineer
[R2, R3, R4], R6, R7, R9, R26
General-purpose ML (e.g., APIs)
Chief Architect, Director of ML, Product Mgr.
R25, R32, R34
NLP (e.g., Speech, Translation)
Data Mgr., Data Collector, Domain Expert, ML Engineer, Prod-
R1, [R15, R16, R19, R20, R21,
uct Mgr., Research Software Eng., Technical Mgr., UX Designer
R22], R24, [R27, R29], R28, R31
Recommender Systems
Chief Data Scientist, Data Scientist, Head of Diversity Analytics
R8, R12, R23, R33
Web Search
Product Mgr.
prior experiences, they might inadvertently reveal trade secrets. For these reasons, contacts often declined to be interviewed. To allay some of these concerns, we assured contacts
that the goal of these interviews was to help us learn about
teams’ current practices, challenges, and needs around fair
ML in general, and that findings would not be linked to specific individuals, teams, or companies. We also asked for
interviewees’ advance permission to audio record the interviews, noting that these recordings would not be shared
outside of the research team. Furthermore, we noted that
these recordings would be destroyed following transcription,
and that the resulting transcriptions would be de-identified.
Finally, we assured interviewees that we would allow them
to review any (de-identified) direct quotes before including
them in any research publications. All 29 interviewees in the
main round of interviews consented to be audio recorded.
Each interview in the main round lasted between 40 and
60 minutes. In each one, interviewees were first reminded
of the overall purpose of the research, and were then asked
a series of questions about fairness at each stage in their
team’s ML development pipeline—from collecting data to
designing datasets (e.g., curating training and test sets) and
developing an ML product to assessing and potentially mitigating fairness issues in that product. For each of these
stages, interviewees were asked a broad opening question
about critical episodes they had encountered (e.g., “Can you
recall times you or your team have discovered fairness issues in
your products?”), and a series of follow-up questions (where
applicable and not previously covered). While a few followup questions were specific to particular stages of the pipeline,
a core sequence of four follow-ups was used across all stages.
First, interviewees were asked to walk through how their
team navigated the episode (e.g., “When you decided there
were issues that needed to be addressed... how did your team
decide what course of action to take to address them?”). Next,
interviewees were instructed to imagine they could return
to these critical episodes, but this time with access to a magical oracle of which they could ask any question to help
them in the moment . We asked the question in this
way to encourage interviewees to speak freely about their
current challenges and needs without feeling constrained
to those for which they believed a solution was currently
available . After interviewees generated questions for
the oracle, they were then asked, for each question, whether
and how their team currently goes about trying to answer
this question. This follow-up often led interviewees to reflect
on gaps between their current and ideal practices. Finally,
interviewees were asked whether they saw any other areas
for improvement, or opportunities for support, related to the
relevant stage of their team’s ML development pipeline.
To analyze the interview data, we worked through transcriptions of approximately 25 hours of audio to synthesize
findings using standard methodology from contextual design:
interpretation sessions and affinity diagramming .
Specifically, we employed bottom-up affinity diagramming
(using MURAL ) to iteratively generate codes for various interviewee utterances and then grouped these codes
into successively higher-level themes concerning current
practices, challenges, and needs for support. Key themes are
presented in detail below, under Results and Discussion.
To validate our findings on a broader population, we conducted an anonymous online survey using Qualtrics .
Respondents were recruited using snowball sampling. Specifically, we emailed the survey to direct contacts at over 40 companies and invited them to pass the survey on to colleagues
(within or outside of their companies) who are part of a team
working on ML products (in any role). We also announced the
survey on social media (e.g., Twitter) and online communities
related to ML and AI, including Kaggle forums and special
interest groups on LinkedIn, Facebook, Reddit, and Slack.
Figure 1: Survey demographics: the top 10 self-reported technology areas (left) and team roles (right).
We structured the survey to act as a quantitative supplement to the interviews. As such, the high-level structure of
the survey mirrored that of the main round of interviews.
Based on our findings from the interviews, we developed survey questions to investigate the prevalence and generality of
emerging themes. First, we asked a set of demographic questions to understand our respondents’ backgrounds, including their technology area(s) and team role(s). In a branching
sequence of survey sections, respondents were then asked
about their team’s current practices, challenges, and needs
for support around fairness, with each section pertaining
to one stage of their team’s ML development pipeline. For
each question, closed-ended response options were provided
based on themes that emerged from the interviews through
affinity diagramming, in addition to free-response options
that allowed respondents to elaborate on their responses.
A total of 287 people started the survey. However, not
all respondents completed it, so we analyze only the 267
respondents who completed at least one section beyond demographics. The top 10 self-reported technology areas and
team roles are shown in Figure 1. In each case, respondents
were able to select multiple options. Additional survey demographics are available in the supplementary materials.
RESULTS AND DISCUSSION
Although participants spanned a diverse range of companies, team roles, and application areas, we observed many
commonalities. In the following, we discuss teams’ current
challenges and needs around fairness, organized by top-level
themes that emerged through affinity diagramming. These
include needs for support in fairness-aware data collection
and curation, overcoming teams’ blind spots, implementing
more proactive fairness auditing processes, auditing complex
ML systems, deciding how to address particular instances
of unfairness, and addressing biases in the humans embedded throughout the ML development pipeline. Within each
of these top-level themes, we present selected sub-themes
to highlight research and design opportunities that have
received little attention in the fair ML literature thus far.
We supplement our findings from the interviews with
corresponding survey responses. Interviewees are identified
with an “R,” and survey responses are accompanied with percentages. Some survey questions were completed by a subset
of respondents because the survey used branching logic, with
follow-up sections appearing only as applicable (e.g., respondents were only asked questions about addressing fairness issues if they reported that their team had previously detected
such issues in their products). In such cases, question-specific
response rates are provided in addition to the percentage
of respondents who were asked the question. A graphical
summary of selected survey responses is provided in the supplementary materials. To illustrate general themes, we share
direct quotes in cases where we received explicit permission
from interviewees, in accordance with our IRB approval and
consent form. Therefore, although the themes we present
are drawn from a wide range of application domains, the
domains represented by direct quotes may be narrower.
By recruiting interviewees using snowball sampling and
specifically targeting members of teams whose products had
previously received media coverage related to ML biases and
unfairness, we may have sampled practitioners who are unusually motivated to address fairness issues in their products.
Indeed, many interviewees reported having invested significant time and effort into trying to improve fairness in their
teams’ products, even when they felt unsupported in these
initiatives by their team or company leadership. As such,
many of the findings presented below may be taken to represent barriers that industry ML practitioners run up against
in improving fairness even when they are motivated to do so.
While much of our discussion focuses on needs for tools,
we stress that biases and unfairness are fundamentally sociotechnical problems, and that technically focused research
efforts must go hand-in-hand with efforts to improve organizational processes, policies, and education .
Fairness-aware Data Collection
While media coverage of biases and unfairness in ML systems
often uses a “bias in, bias out” framing, emphasizing the central role of dataset quality , the fair ML research literature has overwhelmingly focused on the development of
algorithmic methods to mitigate biases, viewing the dataset
as fixed . However, many of our interviewees reported
that their teams typically look to their training datasets, not
their ML models, as the most important place to intervene
to improve fairness in their products. Out of the 65% of survey respondents who reported that their teams have some
control over data collection and curation, a majority (58%) reported that they currently consider fairness at these stages of
their ML development pipeline. Furthermore, out of the 21%
of respondents whose teams had previously tried to address
fairness issues found in their products, the most commonly
attempted strategy (73%) was “collecting more training data.”
"It’s almost like the wild west." Most interviewees reported
that their teams do not currently have processes in place
to support the collection and curation of balanced or representative datasets. A software engineer (R7) described their
team’s current data collection practices as “almost like the
wild west” and a data scientist (R10) noted that “there isn’t
really a thought process surrounding... ‘Should [our team] ingest this data in?’ [...] If it is available to us, we ingest it.” An
ML engineer (R19) emphasized the value of supporting more
effective communication (e.g., around sampling practices
and useful metadata to include) between those responsible
for decisions about data collection and curation, and those
responsible for developing ML models. On a 5-point Likert
scale from “Not at all” to “Extremely” useful, 52% of the respondents who were asked the question (79%) indicated that
tools to facilitate communication between model developers
and data collectors would be “Very” or “Extremely” useful.
Interviewees also shared a range of needs for tools and
processes that can actively guide data collection as it occurs,
to support fairness in downstream ML models. For example, in response to the “oracle” interview question, an ML
engineer (R19) working on automated essay scoring noted:
“To score African American students fairly, they need
examples of African American students scoring highly.
But in the data [the data collection team] collect[s],
this is very rare. So, what is the right way to sample
[high-scorers] without having to score all the essays?
[...] So [we need] some kind of way... to indicate [which
schools] to collect from [...] or what to bother spending
the extra money to score.”
A majority (60%) of survey respondents, out of the 25% who
indicated their team has some control over data collection
processes, indicated having such active guidance would be at
least “Very” useful. By contrast, in contexts where training
data is collected via regular, “in-the-wild” users of a product,
challenges can arise when specific user populations are less
engaged with the product. To overcome such challenges,
a technical manager working on speech recognition (R31)
suggested it would help to know more effective strategies
to incentivize“in-the-wild” product usage within specific
populations, such as targeted gamification techniques .
Scaffolding fairness-aware test set design. Several interviewees stressed the central importance of careful test set
design to detecting potential fairness issues. For example,
R4’s team would discuss possible issues to watch out for, and
then “try to design the test set to capture those notions, if we
can.” R4 credited having “the test set be well constructed and
not biased” for the discovery of gender biases in their image
captioner (e.g., images of female doctors were often mislabeled as nurses) which they ultimately traced to imbalances
in their training data. An ML engineer working on gesture
recognition (R6) noted it would be helpful to have better
tools to scaffold the design of test sets, to make it easier to
“assign tags to data points based on certain characteristics that you want to make sure are fair [...and check]
that first of all, each of those tags has a significant number of samples [and] look at the measurements across
each slice and [check] if there’s a bias issue.”
A majority (66%) of survey respondents, out of the 70% who
were asked this question, indicated that such tools to scaffold
the design of test sets would be “Very” or “Extremely” useful.
Implications. Although the fair ML literature has focused
heavily on algorithmic “de-biasing” methods that assume a
fixed training dataset, practitioners in many industry contexts have some control over data collection and curation. Future research should support these practitioners in collecting
and curating high quality datasets in the first place (cf. ). In contrast to the fair ML literature, HCI research on
ML developer tools has often focused on the design of user
interfaces to scaffold data collection and curation processes
(e.g., ). However, this work has tended to focus
on improving ML models’ overall predictive accuracy, rather
than fairness. A promising direction for future research is
to explore how such tools might be explicitly designed to
support fairness and equity in downstream ML models by
interactively guiding data collection, curation, and augmentation, or by supporting the design and use of test sets that can
effectively surface biases and unfairness (cf. ).
Challenges Due to Blind Spots
Interviewees expressed many anxieties around their teams’
potential “blind spots,” which might stand in the way of
effectively addressing fairness issues, or even thinking to
monitor for some forms of unfairness in the first place.
Data collection and curation challenges due to blind spots.
Most of our interviewees highlighted needs for support in
identifying which subpopulations their team needs to consider when developing specific kinds of ML applications,
to ensure they collect sufficient data from these subpopulations or balance across them when curating existing datasets.
A technical director working on general-purpose ML tools
(R32) emphasized the challenges of anticipating which subpopulations to consider, noting that this can be highly context and application dependent , with potential subpopulations extending well beyond those commonly discussed in
the fair ML literature: “Most of the time, people start thinking
about attributes like [ethnicity and gender...]. But the biggest
problem I found is that these cohorts should be defined based on
the domain and problem. For example, for [automated writing
evaluation] maybe it should be defined based on [...whether
the person is] a native speaker.” A majority (62%) of survey
respondents, out of the 80% who were asked the question,
indicated that it would be at least “Very” useful to have additional support in identifying relevant subpopulations.
"You’ll know if there’s fairness issues if someone raises hell
online." Interviewees often reported that their teams do not
discover serious fairness issues until they receive customer
complaints about products (or, worse, by reading negative
media coverage about their products). As a software engineer working on image classification (R7) put it, “How do you
know the unknowns that you’re being unfair towards? [...] You
just have to put your model out there, and then you’ll know if
there’s fairness issues if someone raises hell online.” Several
interviewees expressed needs for support in detecting biases
and unfairness prior to deployment, even in cases where
they may not have anticipated all relevant subpopulations
or all kinds of fairness issues. Despite their efforts running
user studies, teams often discovered serious issues only after
deploying a system in the real world (51% of survey respondents marked this statement as at least “Very” accurate).
Team biases and limitations. Several of our interviewees’
teams have a practice of getting together and trying to imagine everything that could go wrong with their products, so
that they can make sure to proactively monitor for those issues. A few teams even reported including fairness-focused
quizzes in their interview processes, with the aim of hiring
employees who would be effective at spotting biases and unfairness. R2 described much of their team’s current process
as “just everyone collecting all the things that they can think
of that could be offensive and testing for [them].” But as R4
emphasized, “no one person on the team [has expertise] in all
types of bias [...] especially when you take into account different cultures.” Interviewees noted that it would be helpful to
somehow pool knowledge of potential fairness issues in specific application domains across teams with different backgrounds, who have complementary knowledge and blind
spots. A majority (67%) of survey respondents, out of the 71%
who were asked the question, indicated that tools to support
such knowledge pooling would be at least “Very” useful.
A few interviewees also shared experiences in which efforts to obtain additional training data to address a fairness
issue were hampered by their teams’ blind spots. A developer working on image captioning (R4) recalled cases where
many customers had complained that a globally deployed
system performed well for celebrities from some countries,
but routinely misidentified major celebrities from others:
“It sounds easy to just say like, ‘Oh, just add some more
images in there,’ but [...] there’s no person on the team
that actually knows what all of [these celebrities] look
like, for real [...] if I noticed that there’s some celebrity
from Taiwan that doesn’t have enough images in there,
I actually don’t know what they look like to go and fix
that. [...] But, Beyoncé, I know what she looks like.”
Implications. Assessing and mitigating unfairness in ML
systems can depend on nuanced cultural and domain knowledge that no single product team is likely to have. In cases
where ML products are deployed globally, efforts to recruit
more diverse teams may be helpful yet insufficient. A fruitful
area for future research is the design of processes and tools
to support effective sharing and re-use of such knowledge
across team or company boundaries—for example via shared
test sets (cf. ) or case studies from other teams who have
worked in specific application domains. Another promising
direction may be to support teams in the ad-hoc recruitment
of diverse, team-external “experts” for particular tasks .
Needs for More Proactive Auditing Processes
Detecting potential fairness issues presents many unique auditing challenges. Interviewees often described their teams’
current fairness auditing processes as reactive—with efforts
tightly focused around specific customer complaints—in contrast to their teams’ proactive approaches for detecting potential security risks. As a PM for web search (R11) put it,
“It’s a little bit of a... manual search to say, ‘hey, we
think this has a bias, let’s go take a look and see if
it does,’ which I don’t know is the right approach [...]
because there are a lot of strange ones that you wouldn’t
expect [...] that we just accidentally stumbled upon.”
Out of the 63% of survey respondents who were asked the
question, almost half (49%) reported that their team had
previously found potential fairness issues in their products.
Of the 51% who indicated that their team had not found any
issues, most (80%) suspected that there might be issues they
had not yet detected, with a majority (55%) reporting they
believe undetected issues “Probably” or “Definitely” exist.
Furthermore, most interviewees noted that they are not
generally rewarded within their organizations for their efforts around fairness. On a 5-point Likert scale from “Not
at all” to “A great deal,” only 21% of survey respondents reported that their team prioritizes fairness “A lot” or “A great
deal,” and 36% indicated “Not at all.” Given that interviewees
often engaged in ML fairness efforts on their own time and
initiative, several of the needs they highlighted focused on
ways to reduce the amount of manual labor required to effectively monitor for unfairness, or ways to persuade other team
members that a given fairness issue actually exists (e.g., by
showing them quantitative metrics) and should be addressed.
Interviewees revealed a range of needs for support in
implementing proactive auditing processes. These include
domain-specific auditing processes, including metrics and
tools; methods to effectively monitor for unfairness when
individual-level demographics are unavailable; more scalable and comprehensive auditing processes; and ways to
determine if a specific issue is part of a systemic problem.
Needs for (domain-specific) standard auditing processes. To
progress beyond “having each team do some sort of ad hoc
[testing]” (R4), several interviewees expressed desires for
greater sharing of guidelines and processes. As R30 stressed,
“If you’re developing [a model], there is a type of checklist that you go through for accuracy and so on. But
there isn’t anything like that [for fairness], or at least it
hasn’t been disseminated. What we need is a good way
of incorporating [fairness] as part of the workflow.”
Most of our interviewees’ teams do not currently have fairness metrics against which they can monitor performance
and progress. Only 23% and 20% of survey respondents, respectively, out of the 26% who were asked these questions,
marked as at least “Very” accurate the statements “We have
[fairness] metrics / key performance indicators (KPIs)” and
“We run automated tests [for fairness].” Yet, as R2 noted, “it’s
really hard to fix things that you can’t measure.” Similarly, R1
said, “it would be really nice to learn more about how unfair
we actually are, because only then can we start tackling that.”
Several of our interviewees reported that their team had
searched the fair ML literature for existing fairness metrics.
However, they often failed to find metrics that readily applied
to their specific application domains. For example, although
much of the fair ML literature has focused on metrics for
quantifying “allocative harms” (relating to the allocation of
limited opportunities, resources, or information), many of
the fairness issues that arise in domains such as web search,
conversational AI, or image captioning are “representational
harms” (e.g., perpetuating undesirable associations) .
Some interviewees reported that they had tried to hold
meetings with other teams within their company, to learn
from one another’s experiences and avoid duplicated effort.
However, as R2 explained, even when practitioners are working on different problems in the same application domain,
“It doesn’t necessarily result in a best practices list. [...]
We’ve all tried to make these ways to measure [unfairness ... but] with each problem comes nuances that
make it difficult to have one general way of testing.”
Given that most interviewees had limited time and resources
to devote to developing their own solutions, they often emphasized the value of resources that could help them learn
from others’ experiences more efficiently. For example, R21
suggested it would be ideal to have “a nice white paper that’s
just like... ‘Here’s a summary of research people have done on
fairness [specifically] in NLP models.’ ” Other interviewees
suggested it would be extremely helpful to have access to
tools and resources that would help their team anticipate
what kinds of fairness issues can arise in their specific application domain, together with domain-specific frameworks
that can help them navigate any associated complexities.
Fairness auditing without access to individual-level demographics. Although most auditing methods in the fair ML
literature assume access to sensitive demographics (such as
gender or race) at an individual level , many of our interviewees reported that their teams are only able to collect such
information at coarser levels, if at all (cf. ). For
example, companies working with K-12 student populations
in the US are typically prohibited from collecting such demographics by school or district policies and FERPA laws .
A majority (70%) of survey respondents, out of the 69% who
were asked the question, indicated that the availability of
tools to support fairness auditing without access to demographics at an individual level would be at least “Very” useful.
A few interviewees reported that their teams had attempted
to use coarse-grained demographic information (e.g., regionor organization-level demographics) for fairness auditing.
However, each of these teams had quickly abandoned these
efforts, citing limited time and resources to spend on building their own solutions. R21 said, “If we had more people
who we could throw at this... ‘Can we leverage this fuzzy data
to [audit]?’ that would be great [...] It’s a fairly intimidating
research problem I think, for us.” Other interviewees noted
that, while it would be helpful to have support in efficiently
using coarse-grained demographic information for fairness
auditing, several challenges would remain. For example, as
R14 noted, “even when you have those data [...] you may know
a bunch about the demographics of a school, but then, you
know, it turns out [our product] is only used by the gifted [or
remedial] students, and you may not have means [to check].”
Some interviewees shared that their teams had experimented with developing ML models to infer individual-level
demographics from available proxies, so that they could then
use these inferred demographics to audit their ML products.
However, interviewees worried that the use of proxies may
in itself introduce undesirable biases, introducing a need to
audit the auditing tool. A data scientist working on automated hiring (R23) recounted a time when their team had
developed such a tool, but ultimately decided against using it:
“We called it the SETHtimator, a sex and ethnicity estimator. [...with] one dataset, we [only] had a list of
people’s names and their IP addresses. So we were able
to sort of cross-reference their IP addresses with a name
database, and from there use a [classifier] to list a probability that someone with that name in that region
would have a certain gender or ethnicity. [...] It’s buggy.
If there was a tool [...to] do this automatically and with
a trusted data source... that would be super useful.”
Interviewees often commented that it would be ideal to
be able to “get the demographic information in the first place”
(R15). Recent work in the fair ML literature has proposed
encryption mechanisms that ensure any collected individuallevel demographics can only be used for auditing .
But interviewees emphasized that, while such technical solutions are an important prerequisite, half the battle would
lie in convincing stakeholders and policymakers that these
mechanisms are truly secure and that the benefits outweigh
the risks of a potential data leak. Anticipating such challenges, a couple of interviewees expressed interest in mechanisms that might allow local decision makers, such as healthcare professionals in hospitals, to use their “on the ground”
knowledge of individual-level demographics to improve fairness in an ML system without revealing these demographics.
Needs for greater scalability and comprehensiveness. Interviewees often complained about limitations of their current
auditing processes, given the enormous space of potential
fairness issues. For example, a UX researcher working on
chatbots (R18) highlighted the challenges of recruiting a sizable, diverse sample of user-study participants as one reason
their team sometimes fails to detect fairness issues early on:
“because of just logistics... we get [8 or 10] participants at a
time, and even though we recruit for a ‘diverse’ group... I mean,
we’re not representing everybody.” Similarly, R4 described
their team’s current user-testing practices as “more of a spot
check,” noting “what I would rather have is a more comprehensive... full bias scan.” Drawing parallels to existing, automated
tools that scan natural language datasets for potentially sensitive terms, R1 suggested having tools that “at least flag things
that seem potentially ‘unfair’ would be helpful.” While several
other interviewees generated similar ideas, they also often
pointed out that developing scalable, automated processes
would be a highly challenging research problem, given that
fairness can be so context dependent. R5 emphasized that
domains like image captioning or web search are particularly challenging, because fairness can depend jointly on the
system’s output and the user-provided input (e.g., a query).
Diagnosing systemic problems from specific issues. Interviewees also highlighted challenges in diagnosing whether
specific issues (e.g., complaints from customers) are symptomatic of broader, systemic problems or just “one-offs.” A
product and data manager for a translation system (R1) said
“If an oracle was able to tell me, ‘look, this is a severe
problem and I can give you a hundred examples [of this
problem],’ [...] then it’s much easier internally to get
enough people to accept this and to solve it. So having
a process which gives you more data points where you
mess up [in this way] would be really helpful.”
A majority (62%) of survey respondents, out of the 70% who
were asked this question, indicated that tools to help find
other instances of an issue would be at least “Very” useful.
Implications. Given that fairness can be highly context and
application dependent , there is an urgent need for
domain-specific educational resources, metrics, processes,
and tools to help practitioners navigate the unique challenges
that can arise in their specific application domains. Such resources might include, for example, accessible summaries of
state-of-the-art research around fairness in machine translation, or case studies of challenges faced by teams working
on particular kinds of recommender systems (e.g., ).
Future research should also explore processes and tools to
support fairness auditing with access to only coarse-grained
demographic information (e.g., neighborhood- or schoollevel demographics). Recent work has begun to
explore the design of encryption mechanisms that ensure
any collected individual-level demographics can only be used
for auditing. However, our interviewees noted that in certain
sensitive contexts, stakeholders may be unwilling to reveal
individual-level demographics, even with such mechanisms.
Although interviewees highlighted needs for more scalable and comprehensive auditing processes, they also often
expressed skepticism that fairness auditing could be fully
automated in their domain due to challenges in quantifying
fairness. Therefore, in addition to developing domain-specific
metrics and tools, a direction for future research may be to explore and evaluate human-in-the-loop approaches to fairness
auditing that combine the strengths of automated methods
and human inspection (cf. ), perhaps aided
by tools for visualization and guided exploration .
Finally, at a broader, organizational level, future research
should explore how teams and companies can best be motivated to adopt fairness as a central priority. Efforts to develop easily-trackable fairness metrics in particular application domains may help towards this goal in strongly metricdriven companies. Relatedly, future tools for fairness auditing should be designed to support practitioners in both (1)
determining whether specific issues are instances of broader,
systemic problems and (2) effectively persuading other team
members that there are issues that need to be addressed.
Needs for More Holistic Auditing Methods
Much of the existing fair ML literature has focused on domains such as recidivism prediction, automated hiring, and
face recognition, where fairness can be at least partially
understood in terms of well-defined quantitative metrics
(e.g., between-group parity in error rates or decisions ). However, interviewees working on applications involving richer, complex interactions between the user and
the system—such as chatbots, automated writing evaluation,
adaptive tutoring and mentoring, and web search—brought
up needs for more holistic, system-level auditing methods.
Fairness as a system-level property. Many interviewees
noted disconnects between the way they tend to think about
fairness in their application domain and the discourse they
have observed in both the popular press and academic literature. For example, a technical manager working on adaptive
learning technologies (R30) noted that their team does not
think about fairness in terms of monitoring individual ML
models for unfairness, but instead evaluating the real-world
impacts of ML systems, mentioning that “If we think about educational interventions as analogous to medical interventions
or drug trials [...] we know and [expect] a particular intervention will have different effects on different subpopulations.”
In a complex, multi-component ML system, there is not
always a clean mapping between performance metrics for
an individual ML model and the system’s utility for users.
System components may interact with one another 
and with other aspects of a system’s design in ways that can
be difficult to predict absent an empirical study with actual
users (cf. ). Furthermore, it may not be straightforward to
even define fair system behavior without first understanding
users’ expectations and beliefs about the system (cf. ).
For example, a PM working on web search (R11) shared that
their team had previously experimented with ways to address
a fairness issue involving image search (a search for the term
“CEO” resulted predominantly in images of white men). However, through user studies, the team learned that many users
were uncomfortable with the idea of the company “manipulating” search results, viewing this behavior as unethical:
“Users right now are seeing [image search] as ‘We show
you [an objective] window into [...] society,’ whereas we
do have a strong argument [instead] for, ‘We should
show you as many different types of images as possible,
[to] try to get something for everyone.”’
Needs for simulation-based approaches for complex systems.
In applications involving sequences of complex interactions
between the user and the system, fairness can be heavily
context dependent. R17 suggested it would be valuable to
have ways to prototype conversational agents more rapidly,
including methods for simulating conversational trajectories
(cf. ) “and then find[ing] ways to automate the identification of risky conversation patterns that emerge.” Similarly, a
data scientist working on adaptive mentoring software (R10)
suggested that because their product involves a long-term
feedback loop, it would be ideal to run it on a population
of “simulated mentees” (cf. ) to see if certain forms of
personalization might be harmful with respect to equity.
Implications. In applications involving richer, complex interactions between the users and the system, assessing fairness issues via de-contextualized quantitative metrics for
individual ML models may be insufficient (cf. ). Future
research should explore new approaches to auditing and prototyping ML systems (cf. ), perhaps aided by simulation
tools that can help developers anticipate sensitive contexts
or real-world impacts of using an ML system (cf. ).
Addressing Detected Issues
Interviewees revealed a range of challenges and needs around
debugging and remediation of fairness issues once detected.
These included, among others, needs for support in identifying the cheapest, most effective strategies to address particular issues; methods to estimate how much additional data to
collect for particular subpopulations; processes to anticipate
potential trade-offs between specific definitions of fairness
and other desiderata for an ML system (not limited to predictive accuracy); and frameworks to help navigate complex
ethical decisions (e.g., the fairness of fairness interventions).
Needs for support in strategy selection. Interviewees reported that their teams often struggle to isolate the causes
of unexpected fairness issues, especially when working with
ML models that the team consider to be “black boxes.” As a
result, it is often difficult for teams to decide where to focus
their efforts—switching to a different model, augmenting the
training data in some way, collecting more or different kinds
of data, post-processing outputs, changing the objective function, or something else (cf. ). Of the 21% of survey respondents whose teams had previously tried to address detected
issues, a majority (54%) indicated that it would be at least
“Very” useful to have better support in comparing specific
strategies for addressing particular fairness issues. Different
costs may be associated with different strategies in different
contexts. For example, a developer working on image captioning (R7) noted that collecting additional data is typically a
“last resort” option for their team, given data collection costs.
In contrast, a PM working on image captioning on a different
team (R2) cited data collection as their team’s default starting
place when trying to address a fairness issue. These interviewees and others also shared experiences where their teams
had wasted significant time and resources pursuing various
dead ends. As such, interviewees highlighted several needs
for “fair ML debugging” processes and tools (cf. )
to support their teams in identifying the cheapest, yet most
promising strategies to address particular issues. For example, R1 highlighted needs for support in “identify[ing] the
component where we mess up” in complex, multi-component
ML systems , and in deciding whether to focus their
mitigation efforts on training data or on models. A majority
(63%) of survey respondents indicated that tools to aid in
these decisions (cf. ), would be at least “Very” useful.
Avoiding unexpected side effects of fairness interventions. In
addition to costs, interviewees often cited fears of unexpected
side effects as a deterrent to addressing fairness issues. R4
shared prior experiences where, after making changes to
datasets or models to improve fairness, their system changed
in subtle, unexpected ways that harmed users’ experiences:
“Even if your [quantitative metrics] come out better...
at the end of the day, it’s really just different from what
you had before [...] and [users] notice that for their
particular scenario, it’s different in a negative way.”
A majority (71%) of survey respondents indicated that it
would be at least “Very” useful to have tools to help their team
understand potential UX side effects caused by a particular
strategy for addressing a fairness issue. To minimize the risk
of side effects, several teams had a practice of implementing
many local, “band-aid,” fixes rather than trying to address
the root cause of an issue. In some cases (e.g., when the issue
is a “one-off”), such local fixes may be sufficient. However,
interviewees also reported that these fixes, such as censoring
specific system outputs or responses to certain user inputs,
sometimes resulted in other forms of unfairness (e.g., harms
to certain user populations caused by the censoring itself).
How much more data would we need to collect? In cases
where teams had considered addressing a fairness issue by
collecting additional data, interviewees often shared needs
for support in estimating the minimum number of additional
data points per subpopulation that they would need to address the issue (66% of survey respondents indicated that
they would find such support at least “Very” useful). Most of
our interviewees’ teams currently rely on developer intuition.
For example, a PM working on image captioning (R2) said,
“It’s just hope and trial and error... [the developers have]
experimented so much with these models [...] that they
can say ‘generally, [we will need] this much data to
make an impact on this type of model to change things
this much.’ ”
However, a developer on the same team (R4) noted that their
initial estimates are often wrong, which can be costly,
“especially when it takes two weeks to get an answer.
[...] I always would just really want to know how much
was enough.”
Concerns about the fairness of fairness interventions. Interviewees often expressed unease with the idea that their
teams’ technical choices can have major societal impacts. For
example, a technical director working on general-purpose
ML tools (R32) said, “[ML] models’ main assumption [is] that
the past is similar to the future. [...] if I don’t want to have the
same future, am I in the position to define the future for society
or not?” Another interviewee (R6) expressed doubts about
the morality of targeting specific subpopulations for additional data collection, even if this data collection ultimately
serves to improve fairness for those subpopulations (cf. ):
“Targeting people based on certain aspects of their person... I don’t know how we would go about doing that
[in the] most morally and ethically and even vaguely
responsible way.”
Several interviewees suggested it would be helpful to have access to domain-specific resources, such as ethical frameworks
and case studies, to guide their teams’ ongoing efforts around
fairness (55% of survey respondents indicated that having
access to such resources would be at least “Very” useful).
Changes to the broader system design. The fair ML research
literature has tended to focus heavily on the development
of algorithmic methods to assess and mitigate biases in individual ML models. However, interviewees emphasized that
many fairness issues that arise in real-world ML systems may
be most effectively addressed through changes to the broader
system design. For example, R3 recalled a case where their
image captioner was systematically mislabeling images of female doctors as “nurses,” in accordance with historical stereotypes. The team resolved the issue by replacing the system
outputs “nurse” and “doctor” with the more generic “healthcare professional.” Several other interviewees described “failsoft” design strategies their team employs to try to ensure
that the worst-case harm is minimized (cf. ). As a
PM for web search (R5) put it, “Sometimes, you start with
what you know won’t cause more harm, and [then] iterate.”
R14 noted that when their team designs actions (e.g., personalized messages) taken in response to particular model
outputs, they try to imagine the impacts these actions might
have in specific false-positive and false-negative scenarios.
Indeed, 40% of survey respondents reported that they had
tried such fail-soft strategies to address fairness issues.
Implications. Future research should explore educational
resources, processes, and tools to help teams identify the
cheapest, most effective strategies to address particular fairness issues. This might include helping teams estimate the
minimum amount of additional data required to address a
fairness issue, or helping teams anticipate trade-offs between
definitions of fairness and other desiderata for an ML system, such as user satisfaction (moving beyond the fair ML
literature’s focus on trade-offs between fairness and predictive accuracy). In some cases, the best option available to a
team may be to refrain from applying a particular fairness
intervention (e.g., to avoid greater harms to users) .
Biases in the Humans in the Loop
Finally, several interviewees stressed the importance of explicitly considering biases that may be present in the humans embedded in the various stages of the ML development
pipeline, such as crowdworkers who annotate training data
or user-study participants tasked with surfacing undesirable
biases in ML systems . For example, a UX designer working on automated essay scoring (R20) noted that
their training data is collected by hiring human scorers to
evaluate essays according to a detailed rubric. However, their
team suspects that irrelevant factors may influence scorers’
judgments . An ML engineer on the team (R19) suggested it would be valuable to have support in auditing their
human scoring process. R19 proposed auditing scorers by
injecting artificially generated essays into the scoring pool,
using a hypothetical tool that can “paraphrase [an essay] in
another subgroup’s style [...] a different voice [or] vernacular
[...without] chang[ing] the linguistic content otherwise... and
say, ‘If you apply this linguistic feature, do the scores change?’ ”
Similarly, 68% of survey respondents marked tools to simulate counterfactuals (cf. ) as at least “Very” useful.
More broadly, 69% of survey respondents, out of the 79%
who were asked the question, marked tools to reduce the
influence of human biases on their labeling or scoring processes (cf. ) at least “Very” useful. Furthermore, 69% of
respondents, out of the 32% who were asked the question, reported that their teams already actively try to mitigate biases
in their labeling or scoring processes at least “Sometimes.”
Implications. Future research should explore ways to help
teams better understand biases that may be present in the
humans embedded throughout the ML development pipeline,
as well as ways to mitigate these biases (see ).
CONCLUSIONS AND FUTURE DIRECTIONS
Although industry practitioners are already grappling with
biases and unfairness in ML systems, research on fair ML is
rarely guided by an understanding of the challenges faced
by practitioners . In this work, we conducted the
first systematic investigation of industry teams’ challenges
and needs for support in developing fairer ML systems. Even
when practitioners are motivated to improve fairness in their
products, they often face various technical and organizational barriers. Below, we highlight just a few broad directions for future research to reduce some of these barriers:
• Although the fair ML literature has overwhelmingly focused on algorithmic “de-biasing,” future research should
also support practitioners in collecting and curating highquality datasets in the first place, with an eye towards
fairness in downstream ML models (cf. ).
• Because fairness can be context and application dependent , domain-specific educational resources, metrics, processes, and tools (e.g., ) are urgently needed.
• Although most fairness auditing methods assume access to
individual-level demographics, many teams are only able
to collect such information at coarser levels, if at all. Future
research should explore ways to support fairness auditing
with access to only coarse-grained demographic information (e.g., neighborhood- or school-level demographics).
• Another rich area for future research is the development of
processes and tools for fairness-focused debugging .
For instance, it can be highly challenging to determine
whether specific issues (e.g., complaints from customers)
are “one-offs” or symptomatic of broader, systemic problems that might require deeper investigation, let alone to
identify the most effective strategies for addressing them.
• Finally, our findings point to needs for automated auditing tools and new approaches to prototyping ML systems
(cf. ). Interviewees highlighted limitations of existing
UX prototyping methods for surfacing fairness issues in
complex, multi-component ML systems (e.g., chatbots),
where fairness can be heavily context dependent ,
and the space of potential contexts is often very large.
The rapidly growing area of fairness in ML presents many
new challenges. ML systems are increasingly widespread,
with demonstrated potential to amplify social inequities, or
even to create new ones . Therefore, as research in this area progresses, it is urgent that research agendas be aligned with the challenges and needs of those who
affect and are affected by ML systems. We view the directions
outlined in this paper as critical opportunities for the ML and
HCI research communities to play more active, collaborative
roles in mitigating unfairness in real-world ML systems.
ACKNOWLEDGMENTS
We thank all our interviewees and survey respondents for
their participation. We also thank Mary Beth Kery, Bogdan
Kulynych, Michael Madaio, Alexandra Olteanu, Rebekah
Overdorf, Ronni Sadovsky, Joseph Seering, Ben Shneiderman,
Michael Veale, and our anonymous CHI 2019 reviewers for
their insightful feedback. This research was initiated while
the first author was a summer intern at Microsoft Research.