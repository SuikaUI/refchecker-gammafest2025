Recent Advances in Convolutional Neural Networks
Jiuxiang Gua,∗, Zhenhua Wangb,∗, Jason Kuenb, Lianyang Mab, Amir Shahroudyb, Bing Shuaib, Ting
Liub, Xingxing Wangb, Li Wangb, Gang Wangb, Jianfei Caic, Tsuhan Chenc
aROSE Lab, Interdisciplinary Graduate School, Nanyang Technological University, Singapore
bSchool of Electrical and Electronic Engineering, Nanyang Technological University, Singapore
cSchool of Computer Science and Engineering, Nanyang Technological University, Singapore
In the last few years, deep learning has led to very good performance on a variety of problems, such as
visual recognition, speech recognition and natural language processing.
Among diﬀerent types of deep
neural networks, convolutional neural networks have been most extensively studied.
Leveraging on the
rapid growth in the amount of the annotated data and the great improvements in the strengths of graphics
processor units, the research on convolutional neural networks has been emerged swiftly and achieved stateof-the-art results on various tasks.
In this paper, we provide a broad survey of the recent advances in
convolutional neural networks. We detailize the improvements of CNN on diﬀerent aspects, including layer
design, activation function, loss function, regularization, optimization and fast computation. Besides, we
also introduce various applications of convolutional neural networks in computer vision, speech and natural
language processing.
Convolutional Neural Network, Deep learning
1. Introduction
Convolutional Neural Network (CNN) is a well-known deep learning architecture inspired by the natural
visual perception mechanism of the living creatures. In 1959, Hubel & Wiesel found that cells in animal
visual cortex are responsible for detecting light in receptive ﬁelds. Inspired by this discovery, Kunihiko
Fukushima proposed the neocognitron in 1980 , which could be regarded as the predecessor of CNN. In
1990, LeCun et al. published the seminal paper establishing the modern framework of CNN, and later
improved it in . They developed a multi-layer artiﬁcial neural network called LeNet-5 which could classify
handwritten digits. Like other neural networks, LeNet-5 has multiple layers and can be trained with the
backpropagation algorithm . It can obtain eﬀective representations of the original image, which makes it
possible to recognize visual patterns directly from raw pixels with little-to-none preprocessing. A parallel
study of Zhang et al. used a shift-invariant artiﬁcial neural network (SIANN) to recognize characters
from an image. However, due to the lack of large training data and computing power at that time, their
networks can not perform well on more complex problems, e.g., large-scale image and video classiﬁcation.
Since 2006, many methods have been developed to overcome the diﬃculties encountered in training deep
CNNs . Most notably, Krizhevsky et al.proposed a classic CNN architecture and showed signiﬁcant
improvements upon previous methods on the image classiﬁcation task. The overall architecture of their
method, i.e., AlexNet , is similar to LeNet-5 but with a deeper structure. With the success of AlexNet,
many works have been proposed to improve its performance. Among them, four representative works are
∗equal contribution
Email addresses: (Jiuxiang Gu), (Zhenhua Wang), 
(Jason Kuen), (Lianyang Ma), (Amir Shahroudy), (Bing Shuai),
 (Ting Liu), (Xingxing Wang), (Li Wang),
 (Gang Wang), (Jianfei Cai), (Tsuhan Chen)
 
Figure 1: Hierarchically-structured taxonomy of this survey
ZFNet , VGGNet , GoogleNet and ResNet . From the evolution of the architectures, a typical
trend is that the networks are getting deeper, e.g., ResNet, which won the champion of ILSVRC 2015, is
about 20 times deeper than AlexNet and 8 times deeper than VGGNet. By increasing depth, the network can
better approximate the target function with increased nonlinearity and get better feature representations.
However, it also increases the complexity of the network, which makes the network be more diﬃcult to
optimize and easier to get overﬁtting. Along this way, various methods have been proposed to deal with
these problems in various aspects. In this paper, we try to give a comprehensive review of recent advances
and give some thorough discussions.
In the following sections, we identify broad categories of works related to CNN. Figure 1 shows the
hierarchically-structured taxonomy of this paper. We ﬁrst give an overview of the basic components of
CNN in Section 2. Then, we introduce some recent improvements on diﬀerent aspects of CNN including
convolutional layer, pooling layer, activation function, loss function, regularization and optimization in Section 3 and introduce the fast computing techniques in Section 4. Next, we discuss some typical applications
of CNN including image classiﬁcation, object detection, object tracking, pose estimation, text detection
and recognition, visual saliency detection, action recognition, scene labeling, speech and natural language
processing in Section 5. Finally, we conclude this paper in Section 6.
2. Basic CNN Components
There are numerous variants of CNN architectures in the literature. However, their basic components
are very similar. Taking the famous LeNet-5 as an example, it consists of three types of layers, namely
convolutional, pooling, and fully-connected layers. The convolutional layer aims to learn feature representations of the inputs. As shown in Figure 2(a), convolution layer is composed of several convolution kernels
which are used to compute diﬀerent feature maps. Speciﬁcally, each neuron of a feature map is connected to
a region of neighbouring neurons in the previous layer. Such a neighbourhood is referred to as the neuron’s
receptive ﬁeld in the previous layer. The new feature map can be obtained by ﬁrst convolving the input with
a learned kernel and then applying an element-wise nonlinear activation function on the convolved results.
Note that, to generate each feature map, the kernel is shared by all spatial locations of the input. The
complete feature maps are obtained by using several diﬀerent kernels. Mathematically, the feature value at
location (i, j) in the k-th feature map of l-th layer, zl
i,j,k, is calculated by:
i,j,k = wl
(a) LeNet-5 network
(b) Learned features
Figure 2: (a) The architecture of the LeNet-5 network, which works well on digit classiﬁcation task.
(b) Visualization of
features in the LeNet-5 network. Each layer’s feature maps are displayed in a diﬀerent block.
k are the weight vector and bias term of the k-th ﬁlter of the l-th layer respectively, and
i,j is the input patch centered at location (i, j) of the l-th layer. Note that the kernel wl
k that generates
the feature map zl
:,:,k is shared. Such a weight sharing mechanism has several advantages such as it can
reduce the model complexity and make the network easier to train. The activation function introduces
nonlinearities to CNN, which are desirable for multi-layer networks to detect nonlinear features. Let a(·)
denote the nonlinear activation function. The activation value al
i,j,k of convolutional feature zl
i,j,k can be
computed as:
i,j,k = a(zl
Typical activation functions are sigmoid, tanh and ReLU . The pooling layer aims to achieve
shift-invariance by reducing the resolution of the feature maps. It is usually placed between two convolutional
layers. Each feature map of a pooling layer is connected to its corresponding feature map of the preceding
convolutional layer. Denoting the pooling function as pool(·), for each feature map al
:,:,k we have:
i,j,k = pool(al
m,n,k), ∀(m, n) ∈Rij
where Rij is a local neighbourhood around location (i, j).
The typical pooling operations are average
pooling and max pooling . Figure 2(b) shows the feature maps of digit 7 learned by the ﬁrst two
convolutional layers. The kernels in the 1st convolutional layer are designed to detect low-level features such
as edges and curves, while the kernels in higher layers are learned to encode more abstract features. By
stacking several convolutional and pooling layers, we could gradually extract higher-level feature representations.
After several convolutional and pooling layers, there may be one or more fully-connected layers which
aim to perform high-level reasoning . They take all neurons in the previous layer and connect them
to every single neuron of current layer to generate global semantic information. Note that fully-connected
layer not always necessary as it can be replaced by a 1 × 1 convolution layer .
The last layer of CNNs is an output layer. For classiﬁcation tasks, the softmax operator is commonly
used . Another commonly used method is SVM, which can be combined with CNN features to solve
diﬀerent classiﬁcation tasks . Let θθθ denote all the parameters of a CNN (e.g., the weight vectors
and bias terms). The optimum parameters for a speciﬁc task can be obtained by minimizing an appropriate
loss function deﬁned on that task. Suppose we have N desired input-output relations {(xxx(n),yyy(n)); n ∈
[1, · · · , N]}, where xxx(n) is the n-th input data, yyy(n) is its corresponding target label and ooo(n) is the output
of CNN. The loss of CNN can be calculated as follows:
ℓ(θθθ;yyy(n),ooo(n))
Training CNN is a problem of global optimization. By minimizing the loss function, we can ﬁnd the best
ﬁtting set of parameters. Stochastic gradient descent is a common solution for optimizing CNN network [21,
(a) Convolution
(b) Tiled Convolution
(c) Dilated Convolution
(d) Deconvolution
Figure 3: Illustration of (a) Convolution, (b) Tiled Convolution, (c) Dilated Convolution, and (d) Deconvolution
3. Improvements on CNNs
There have been various improvements on CNNs since the success of AlexNet in 2012. In this section, we
describe the major improvements on CNNs from six aspects: convolutional layer, pooling layer, activation
function, loss function, regularization, and optimization.
3.1. Convolutional Layer
Convolution ﬁlter in basic CNNs is a generalized linear model (GLM) for the underlying local image
It works well for abstraction when instances of latent concepts are linearly separable.
introduce some works which aim to enhance its representation ability.
3.1.1. Tiled Convolution
Weight sharing mechanism in CNNs can drastically decrease the number of parameters. However, it
may also restrict the models from learning other kinds of invariance. Tiled CNN is a variation of CNN
that tiles and multiples feature maps to learn rotational and scale invariant features. Separate kernels are
learned within the same layer, and the complex invariances can be learned implicitly by square-root pooling
over neighbouring units. As illustrated in Figure 3(b), the convolution operations are applied every k unit,
where k is the tile size to control the distance over which weights are shared. When the tile size k is 1,
the units within each map will have the same weights, and tiled CNN becomes identical to the traditional
CNN. In , their experiments on the NORB and CIFAR-10 datasets show that k = 2 achieves the best
results. Wang et al. ﬁnd that Tiled CNN performs better than traditional CNN on small time series
3.1.2. Transposed Convolution
Transposed convolution can be seen as the backward pass of a corresponding traditional convolution.
It is also known as deconvolution and fractionally strided convolution . To stay consistent
with most literature , we use the term “deconvolution”. Contrary to the traditional convolution
that connects multiple input activations to a single activation, deconvolution associates a single activation
with multiple output activations. Figure 3(d) shows a deconvolution operation of 3 × 3 kernel over a 4 × 4
input using unit stride and zero padding. The stride of deconvolution gives the dilation factor for the input
feature map. Speciﬁcally, the deconvolution will ﬁrst upsample the input by a factor of the stride value with
padding, then perform convolution operation on the upsampled input. Recently, deconvolution has been
widely used for visualization , recognition , localization , semantic segmentation , visual
question answering , and super-resolution .
3.1.3. Dilated Convolution
Dilated CNN is a recent development of CNN that introduces one more hyper-parameter to the
convolutional layer. By inserting zeros between ﬁlter elements, Dilated CNN can increase the network’s
(a) Linear convolution layer
(b) Mlpconv layer
Figure 4: The comparison of linear convolution layer and mlpconv layer.
receptive ﬁeld size and let the network cover more relevant information. This is very important for tasks
which need a large receptive ﬁeld when making the prediction. Formally, a 1-D dilated convolution with
dilation l that convolves signal F with kernel k of size r is deﬁned as (F ∗l k)t = P
τ kτFt−lτ, where ∗l
denotes l-dilated convolution. This formula can be straightforwardly extended to 2-D dilated convolution.
Figure 3(c) shows an example of three dilated convolution layers where the dilation factor l grows up
exponentially at each layer. The middle feature map F2 is produced from the bottom feature map F1 by
applying a 1-dilated convolution, where each element in F2 has a receptive ﬁeld of 3×3. F3 is produced from
F2 by applying a 2-dilated convolution, where each element in F3 has a receptive ﬁeld of (23 −1) × (23 −1).
The top feature map F4 is produced from F3 by applying a 4-dilated convolution, where each element in
F4 has a receptive ﬁeld of (24 −1) × (24 −1). As can be seen, the size of receptive ﬁeld of each element in
Fi+1 is (2i+2 −1) × (2(i+2) −1). Dilated CNNs have achieved impressive performance in tasks such as scene
segmentation , machine translation , speech synthesis , and speech recognition .
3.1.4. Network in Network
Network In Network (NIN) is a general network structure proposed by Lin et al. . It replaces the
linear ﬁlter of the convolutional layer by a micro network, e.g., multilayer perceptron convolution (mlpconv)
layer in the paper, which makes it capable of approximating more abstract representations of the latent
concepts. The overall structure of NIN is the stacking of such micro networks. Figure 4 shows the diﬀerence
between the linear convolutional layer and the mlpconv layer. Formally, the feature map of convolution
layer (with nonlinear activation function, e.g., ReLU ) is computed as:
ai,j,k = max(wT
k xi,j + bk, 0)
where ai,j,k is the activation value of k-th feature map at location (i, j), xi,j is the input patch centered at
location (i, j), wk and bk are weight vector and bias term of the k-th ﬁlter. As a comparison, the computation
performed by mlpconv layer is formulated as:
i,j,kn = max(wT
i,j,: + bkn, 0)
where n ∈[1, N], N is the number of layers in the mlpconv layer, a0
i,j,: is equal to xi,j. In mlpconv layer,
1×1 convolutions are placed after the traditional convolutional layer. The 1×1 convolution is equivalent to
the cross-channel parametric pooling operation which is succeeded by ReLU . Therefore, the mlpconv
layer can also be regarded as the cascaded cross-channel parametric pooling on the normal convolutional
layer. In the end, they also apply a global average pooling which spatially averages the feature maps of the
ﬁnal layer, and directly feed the output vector into softmax layer. Compared with the fully-connected layer,
global average pooling has fewer parameters and thus reduces the overﬁtting risk and computational load.
3.1.5. Inception Module
Inception module is introduced by Szegedy et al. which can be seen as a logical culmination of
NIN. They use variable ﬁlter sizes to capture diﬀerent visual patterns of diﬀerent sizes, and approximate
Figure 5: (a) Inception module, naive version. (b) The inception module used in . (c) The improved inception module used
in where each 5 × 5 convolution is replaced by two 3 × 3 convolutions. (d) The Inception-ResNet-A module used in .
the optimal sparse structure by the inception module. Speciﬁcally, inception module consists of one pooling
operation and three types of convolution operations (see Figure 5(b)), and 1 × 1 convolutions are placed
before 3×3 and 5×5 convolutions as dimension reduction modules, which allow for increasing the depth and
width of CNN without increasing the computational complexity. With the help of inception module, the
network parameters can be dramatically reduced to 5 millions which are much less than those of AlexNet
(60 millions) and ZFNet (75 millions).
In their later paper , to ﬁnd high-performance networks with a relatively modest computation cost,
they suggest the representation size should gently decrease from inputs to outputs as well as spatial aggregation can be done over lower dimensional embeddings without much loss in representational power. The
optimal performance of the network can be reached by balancing the number of ﬁlters per layer and the
depth of the network. Inspired by the ResNet , their latest Inception-V4 combines the inception architecture with shortcut connections (see Figure 5(d)). They ﬁnd that shortcut connections can signiﬁcantly
accelerate the training of inception networks. Their Inception-v4 model architecture (with 75 trainable layers) that ensembles three residual and one Inception-v4 can achieve 3.08% top-5 error rate on the validation
dataset of ILSVRC 2012.
3.2. Pooling Layer
Pooling is an important concept of CNN. It lowers the computational burden by reducing the number of
connections between convolutional layers. In this section, we introduce some recent pooling methods used
3.2.1. Lp Pooling
Lp pooling is a biologically inspired pooling process modelled on complex cells . It has been theoretically analyzed in , which suggest that Lp pooling provides better generalization than max pooling. Lp
pooling can be represented as:
yi,j,k = [
(am,n,k)p]1/p
where yi,j,k is the output of the pooling operator at location (i, j) in k-th feature map, and am,n,k is the
feature value at location (m, n) within the pooling region Rij in k-th feature map. Specially, when p = 1,
Lp corresponds to average pooling, and when p = ∞, Lp reduces to max pooling.
3.2.2. Mixed Pooling
Inspired by random Dropout and DropConnect , Yu et al. propose a mixed pooling method
which is the combination of max pooling and average pooling.
The function of mixed pooling can be
formulated as follows:
yi,j,k = λ
(m,n)∈Rij am,n,k + (1 −λ)
where λ is a random value being either 0 or 1 which indicates the choice of either using average pooling or
max pooling. During forward propagation process, λ is recorded and will be used for the backpropagation
operation. Experiments in show that mixed pooling can better address the overﬁtting problems and it
performs better than max pooling and average pooling.
3.2.3. Stochastic Pooling
Stochastic pooling is a dropout-inspired pooling method. Instead of picking the maximum value
within each pooling region as max pooling does, stochastic pooling randomly picks the activations according
to a multinomial distribution, which ensures that the non-maximal activations of feature maps are also
possible to be utilized. Speciﬁcally, stochastic pooling ﬁrst computes the probabilities p for each region Rj
by normalizing the activations within the region, i.e., pi = ai/ P
k∈Rj(ak). After obtaining the distribution P(p1, ..., p|Rj|), we can sample from the multinomial distribution based on p to pick a location l within
the region, and then set the pooled activation as yj = al, where l ∼P(p1, ..., p|Rj|). Compared with max
pooling, stochastic pooling can avoid overﬁtting due to the stochastic component.
3.2.4. Spectral Pooling
Spectral pooling performs dimensionality reduction by cropping the representation of input in frequency domain. Given an input feature map x ∈Rm×m, suppose the dimension of desired output feature
map is h × w, spectral pooling ﬁrst computes the discrete Fourier transform (DFT) of the input feature
map, then crops the frequency representation by maintaining only the central h × w submatrix of the frequencies, and ﬁnally uses inverse DFT to map the approximation back into spatial domain. Compared
with max pooling, the linear low-pass ﬁltering operation of spectral pooling can preserve more information
for the same output dimensionality. Meanwhile, it also does not suﬀer from the sharp reduction in output
map dimensionality exhibited by other pooling methods. What is more, the process of spectral pooling is
achieved by matrix truncation, which makes it capable of being implemented with little computational cost
in CNNs (e.g., ) that employ FFT for convolution kernels.
3.2.5. Spatial Pyramid Pooling
Spatial pyramid pooling (SPP) is introduced by He et al. . The key advantage of SPP is that it can
generate a ﬁxed-length representation regardless of the input sizes. SPP pools input feature map in local
spatial bins with sizes proportional to the image size, resulting in a ﬁxed number of bins. This is diﬀerent
from the sliding window pooling in the previous deep networks, where the number of sliding windows depends
on the input size. By replacing the last pooling layer with SPP, they propose a new SPP-net which is able
to deal with images with diﬀerent sizes.
3.2.6. Multi-scale Orderless Pooling
Inspired by , Gong et al. use multi-scale orderless pooling (MOP) to improve the invariance of
CNNs without degrading their discriminative power. They extract deep activation features for both the
whole image and local patches of several scales. The activations of the whole image are the same as those of
previous CNNs, which aim to capture the global spatial layout information. The activations of local patches
are aggregated by VLAD encoding , which aim to capture more local, ﬁne-grained details of the image
as well as enhancing invariance. The new image representation is obtained by concatenating the global
activations and the VLAD features of the local patch activations.
3.3. Activation Function
A proper activation function signiﬁcantly improves the performance of a CNN for a certain task. In this
section, we introduce the recently used activation functions in CNNs.
3.3.1. ReLU
Rectiﬁed linear unit (ReLU) is one of the most notable non-saturated activation functions. The
ReLU activation function is deﬁned as:
ai,j,k = max(zi,j,k, 0)
where zi,j,k is the input of the activation function at location (i, j) on the k-th channel. ReLU is a piecewise
linear function which prunes the negative part to zero and retains the positive part (see Figure 6(a)).
The simple max(·) operation of ReLU allows it to compute much faster than sigmoid or tanh activation
functions, and it also induces the sparsity in the hidden units and allows the network to easily obtain sparse
representations. It has been shown that deep networks can be trained eﬃciently using ReLU even without
pre-training . Even though the discontinuity of ReLU at 0 may hurt the performance of backpropagation,
many works have shown that ReLU works better than sigmoid and tanh activation functions empirically [54,
3.3.2. Leaky ReLU
A potential disadvantage of ReLU unit is that it has zero gradient whenever the unit is not active. This
may cause units that do not active initially never active as the gradient-based optimization will not adjust
their weights. Also, it may slow down the training process due to the constant zero gradients. To alleviate
this problem, Mass et al. introduce Leaky ReLU (LReLU) which is deﬁned as:
ai,j,k = max(zi,j,k, 0) + λ min(zi,j,k, 0)
where λ is a predeﬁned parameter in range (0, 1).
Compared with ReLU, Leaky ReLU compresses the
negative part rather than mapping it to constant zero, which makes it allow for a small, non-zero gradient
when the unit is not active.
3.3.3. Parametric ReLU
Rather than using a predeﬁned parameter in Leaky ReLU, e.g., λ in Eq.(10), He et al. propose
Parametric Rectiﬁed Linear Unit (PReLU) which adaptively learns the parameters of the rectiﬁers in order
to improve accuracy. Mathematically, PReLU function is deﬁned as:
ai,j,k = max(zi,j,k, 0) + λk min(zi,j,k, 0)
where λk is the learned parameter for the k-th channel. As PReLU only introduces a very small number of
extra parameters, e.g., the number of extra parameters is the same as the number of channels of the whole
network, there is no extra risk of overﬁtting and the extra computational cost is negligible. It also can be
simultaneously trained with other parameters by backpropagation.
3.3.4. Randomized ReLU
Another variant of Leaky ReLU is Randomized Leaky Rectiﬁed Linear Unit (RReLU) . In RReLU,
the parameters of negative parts are randomly sampled from a uniform distribution in training, and then
ﬁxed in testing (see Figure 6(c)). Formally, RReLU function is deﬁned as:
i,j,k = max(z(n)
i,j,k, 0) + λ(n)
where z(n)
i,j,k denotes the input of activation function at location (i, j) on the k-th channel of n-th example,
denotes its corresponding sampled parameter, and a(n)
i,j,k denotes its corresponding output. It could
reduce overﬁtting due to its randomized nature. Xu et al. also evaluate ReLU, LReLU, PReLU and
RReLU on standard image classiﬁcation task, and concludes that incorporating a non-zero slop for negative
part in rectiﬁed activation units could consistently improve the performance.
(b) LReLU/PReLU
Figure 6: The comparison among ReLU, LReLU, PReLU, RReLU and ELU. For Leaky ReLU, λ is empirically predeﬁned.
For PReLU, λk is learned from training data. For RReLU, λ(n)
is a random variable which is sampled from a given uniform
distribution in training and keeps ﬁxed in testing. For ELU, λ is empirically predeﬁned.
3.3.5. ELU
Clevert et al. introduce Exponential Linear Unit (ELU) which enables faster learning of deep neural
networks and leads to higher classiﬁcation accuracies. Like ReLU, LReLU, PReLU and RReLU, ELU avoids
the vanishing gradient problem by setting the positive part to identity. In contrast to ReLU, ELU has a
negative part which is beneﬁcial for fast learning.
Compared with LReLU, PReLU, and RReLU which also have unsaturated negative parts, ELU employs
a saturation function as negative part. As the saturation function will decrease the variation of the units if
deactivated, it makes ELU more robust to noise. The function of ELU is deﬁned as:
ai,j,k = max(zi,j,k, 0) + min(λ(ezi,j,k −1), 0)
where λ is a predeﬁned parameter for controlling the value to which an ELU saturate for negative inputs.
3.3.6. Maxout
Maxout is an alternative nonlinear function that takes the maximum response across multiple channels at each spatial position. As stated in , the maxout function is deﬁned as: ai,j,k = maxk∈[1,K] zi,j,k,
where zi,j,k is the k-th channel of the feature map. It is worth noting that maxout enjoys all the beneﬁts of
ReLU since ReLU is actually a special case of maxout, e.g., max(wT
1 x + b1, wT
2 x + b2) where w1 is a zero
vector and b1 is zero. Besides, maxout is particularly well suited for training with Dropout.
3.3.7. Probout
Springenberg et al. propose a probabilistic variant of maxout called probout.
They replace the
maximum operation in maxout with a probabilistic sampling procedure. Speciﬁcally, they ﬁrst deﬁne a
probability for each of the k linear units as: pi = eλzi/ Pk
j=1 eλzj, where λ is a hyperparameter for controlling
the variance of the distribution. Then, they pick one of the k units according to a multinomial distribution
{p1, ..., pk} and set the activation value to be the value of the picked unit. In order to incorporate with
dropout, they actually re-deﬁne the probabilities as:
ˆp0 = 0.5, ˆpi = eλzi/(2.
The activation function is then sampled as:
where i ∼multinomial{ˆp0, ..., ˆpk}. Probout can achieve the balance between preserving the desirable properties of maxout units and improving their invariance properties. However, in testing process, probout is
computationally expensive than maxout due to the additional probability calculations.
3.4. Loss Function
It is important to choose an appropriate loss function for a speciﬁc task. We introduce four representative
ones in this subsection: Hinge loss, Softmax loss, Contrastive loss, Triplet loss.
3.4.1. Hinge Loss
Hinge loss is usually used to train large margin classiﬁers such as Support Vector Machine (SVM). The
hinge loss function of a multi-class SVM is deﬁned in Eq.(16), where w is the weight vector of classiﬁer and
yyy(i) ∈[1, . . . , K] indicates its correct class label among the K classes.
Lhinge = 1
[max(0, 1 −δ(yyy(i), j)wT xi)]p
where δ(yyy(i), j) = 1 if yyy(i) = j, otherwise δ(yyy(i), j) = −1. Note that if p = 1, Eq.(16) is Hinge-Loss (L1-
Loss), while if p = 2, it is the Squared Hinge-Loss (L2-Loss) . The L2-Loss is diﬀerentiable and imposes
a larger loss for point which violates the margin comparing with L1-Loss.
 investigates and compares
the performance of softmax with L2-SVMs in deep networks. The results on MNIST demonstrate the
superiority of L2-SVM over softmax.
3.4.2. Softmax Loss
Softmax loss is a commonly used loss function which is essentially a combination of multinomial logistic
loss and softmax. Given a training set {(xxx(i),yyy(i)); i ∈1, . . . , N,yyy(i) ∈1, . . . , K}, where xxx(i) is the i-th input
image patch, and yyy(i) is its target class label among the K classes. The prediction of j-th class for i-th input
is transformed with the softmax function: p(i)
l , where z(i)
is usually the activations of a
densely connected layer, so z(i)
can be written as z(i)
j a(i) + bj. Softmax turns the predictions into
non-negative values and normalizes them to get a probability distribution over classes. Such probabilistic
predictions are used to compute the multinomial logistic loss, i.e., the softmax loss, as follows:
Lsoftmax = −1
1{yyy(i) = j}logp(i)
Recently, Liu et al. propose the Large-Margin Softmax (L-Softmax) loss, which introduces an angular
margin to the angle θj between input feature vector a(i) and the j-th column wj of weight matrix. The
prediction p(i)
for L-Softmax loss is deﬁned as:
e∥wj∥∥a(i)∥ψ(θj)
e∥wj∥∥a(i)∥ψ(θj) + P
l̸=j e∥wl∥∥a(i)∥cos(θl)
ψ(θj) = (−1)k cos(mθj) −2k, θj ∈[kπ/m, (k + 1)π/m]
where k ∈[0, m −1] is an integer, m controls the margin among classes. When m = 1, the L-Softmax
loss reduces to the original softmax loss. By adjusting the margin m between classes, a relatively diﬃcult
learning objective will be deﬁned, which can eﬀectively avoid overﬁtting. They verify the eﬀective of L-
Softmax on MNIST, CIFAR-10, and CIFAR-100, and ﬁnd that the L-Softmax loss performs better than the
original softmax.
3.4.3. Contrastive Loss
Contrastive loss is commonly used to train Siamese network which is a weakly-supervised scheme
for learning a similarity measure from pairs of data instances labelled as matching or non-matching. Given
the i-th pair of data (xxx(i)
β ), let (zzz(i,l)
) denotes its corresponding output pair of the l-th (l ∈
[1, · · · , L]) layer. In and , they pass the image pairs through two identical CNNs, and feed the
feature vectors of the ﬁnal layer to the cost module. The contrastive loss function that they use for training
samples is:
Lcontrastive =
(y)d(i,L) + (1 −y) max(m −d(i,L), 0)
where d(i,L) = ||zzz(i,L)
2, and m is a margin parameter aﬀecting non-matching pairs. If (xxx(i)
a matching pair, then y = 1. Otherwise, y = 0.
Lin et al. ﬁnd that such a single margin loss function causes a dramatic drop in retrieval results when
ﬁne-tuning the network on all pairs. Meanwhile, the performance is better retained when ﬁne-tuning only
on non-matching pairs. This indicates that the handling of matching pairs in the loss function is responsible
for the drop. While the recall rate on non-matching pairs alone is stable, handling the matching pairs is the
main reason for the drop in recall rate. To solve this problem, they propose a double margin loss function
which adds another margin parameter to aﬀect the matching pairs. Instead of calculating the loss of the ﬁnal
layer, their contrastive loss is deﬁned for every layer l and the backpropagations for the loss of individual
layers are performed at the same time. It is deﬁned as:
Ld−contrastive =
(y)max(d(i,l) −m1, 0) + (1 −y)max(m2 −d(i,l), 0)
In practice, they ﬁnd that these two margin parameters can set to be equal (m1 = m2 = m) and be learned
from the distribution of the sampled matching and non-matching image pairs.
3.4.4. Triplet Loss
Triplet loss considers three instances per loss function.
The triplet units (xxx(i)
n ) usually
contain an anchor instance xxx(i)
as well as a positive instance xxx(i)
from the same class of xxx(i)
and a negative
instance xxx(i)
n . Let (zzz(i)
n ) denote the feature representation of the triplet units, the triplet loss is
deﬁned as:
Ltriplet = 1
(a,p) −d(i)
(a,n) + m, 0}
where d(i)
(a,p) = ∥zzz(i)
2 and d(i)
(a,n) = ∥zzz(i)
2. The objective of triplet loss is to minimize the
distance between the anchor and positive, and maximize the distance between the negative and the anchor.
However, randomly selected anchor samples may judge falsely in some special cases. For example, when
(n,p) < d(i)
(a,p) < d(i)
(a,n), the triplet loss may still be zero. Thus the triplet units will be neglected during the
backward propagation. Liu et al. propose the Coupled Clusters (CC) loss to solve this problem. Instead
of using the triplet units, the coupled clusters loss function is deﬁned over the positive set and the negative
set. By replacing the randomly picked anchor with the cluster center, it makes the samples in the positive
set cluster together and samples in the negative set stay relatively far away, which is more reliable than the
original triplet loss. The coupled clusters loss function is deﬁned as:
2 max{∥zzz(i)
2 −∥zzz(∗)
where N p is the number of samples per set, zzz(∗)
is the feature representation of xxx(∗)
which is the nearest
negative sample to the estimated center point cp = (PN p
p )/N p. Triplet loss and its variants have been
widely used in various tasks, including re-identiﬁcation , veriﬁcation , and image retrieval .
3.4.5. Kullback-Leibler Divergence
Kullback-Leibler Divergence (KLD) is a non-symmetric measure of the diﬀerence between two probability
distributions p(x) and q(x) over the same discrete variable x (see Figure 7(a)). The KLD from q(x) to p(x)
(a) KL divergence
(b) Autoencoders variants and GAN variants
Figure 7: The illustration of (a) the KullbackLeibler divergence for two normal Gaussian distributions, (b) AE variants (AE,
VAE , DVAE , and CVAE ) and GAN variants (GAN , CGAN ).
is deﬁned as:
DKL(p||q) = −H(p(x)) −Ep[log q(x)]
p(x) log p(x) −
p(x) log q(x) =
p(x) log p(x)
where H(p(x)) is the Shannon entropy of p(x), Ep(log q(x)) is the cross entropy between p(x) and q(x).
KLD has been widely used as a measure of information loss in the objective function of various Autoencoders (AEs) . Famous variants of AE include sparse AE , Denoising AE and Variational
AE (VAE) . VAE interprets the latent representation through Bayesian inference. It consists of two
parts: an encoder which “compresses” the data sample x to the latent representation z ∼qφ(z|x); and a
decoder, which maps such representation back to data space ˜x ∼pθ(x|z) which as close to the input as
possible, where φ and θ are the parameters of encoder and decoder respectively. As proposed in , VAEs
try to maximize the variational lower bound of the log-likelihood of log p(x|φ, θ):
Lvae = Ez∼qφ(z|x)[log pθ(x|z)] −DKL(qφ(z|x)∥p(z))
where the ﬁrst term is the reconstruction cost, and the KLD term enforces prior p(z) on the proposal
distribution qφ(z|x). Usually p(z) is the standard normal distribution , discrete distribution , or
some distributions with geometric interpretation . Following the original VAE, many variants have been
proposed . Conditional VAE (CVAE) generates samples from the conditional distribution
with ˜x ∼pθ(x|y, z). Denoising VAE (DVAE) recovers the original input x from the corrupted input
Jensen-Shannon Divergence (JSD) is a symmetrical form of KLD. It measures the similarity between
p(x) and q(x):
DJS(p||q) = 1
p(x) + q(x)
p(x) + q(x)
By minimizing the JSD, we can make the two distributions p(x) and q(x) as close as possible. JSD has
been successfully used in the Generative Adversarial Networks (GANs) . In contrast to VAEs
that model the relationship between x and z directly, GANs are explicitly set up to optimize for generative
tasks . The objective of GANs is to ﬁnd the discriminator D that gives the best discrimination between
the real and generated data, and simultaneously encourage the generator G to ﬁt the real data distribution.
The min-max game played between the discriminator D and the generator G is formalized by the following
objective function:
D Lgan(D, G) = Ex∼p(x)[log D(x)] + Ez∼q(z)[log(1 −D(G(z)))]
The original GAN paper shows that for a ﬁxed generator G∗, we have the optimal discriminator D∗
p(x)+q(x). Then the Equation 28 is equivalent to minimize the JSD between p(x) and q(x). If G and D
have enough capacity, the distribution q(x) converges to p(x). Like Conditional VAE, the Conditional GAN
(CGAN) also receives an additional information y as input to generate samples conditioning on y. In
practice, GANs are notoriously unstable to train .
3.5. Regularization
Overﬁtting is an unneglectable problem in deep CNNs, which can be eﬀectively reduced by regularization.
In the following subsection, we introduce some eﬀective regularization techniques: ℓp-norm, Dropout, and
DropConnect.
3.5.1. ℓp-norm Regularization
Regularization modiﬁes the objective function by adding additional terms that penalize the model complexity. Formally, if the loss function is L(θ, x, y), then the regularized loss will be:
E(θ, x, y) = L(θ, x, y) + λR(θ)
where R(θ) is the regularization term, and λ is the regularization strength.
ℓp-norm regularization function is usually employed as R(θ) = P
p. When p ≥1, the ℓp-norm
is convex, which makes the optimization easier and renders this function attractive . For p = 2, the
ℓ2-norm regularization is commonly referred to as weight decay. A more principled alternative of ℓ2-norm
regularization is Tikhonov regularization , which rewards invariance to noise in the inputs. When p < 1,
the ℓp-norm regularization more exploits the sparsity eﬀect of the weights but conducts to non-convex
3.5.2. Dropout
Dropout is ﬁrst introduced by Hinton et al. , and it has been proven to be very eﬀective in reducing
overﬁtting. In , they apply Dropout to fully-connected layers. The output of Dropout is y = r∗a(WT x),
where x = [x1, x2, . . . , xn]T is the input to fully-connected layer, W ∈Rn×d is a weight matrix, and r is a
binary vector of size d whose elements are independently drawn from a Bernoulli distribution with parameter
p, i.e. ri ∼Bernoulli(p). Dropout can prevent the network from becoming too dependent on any one (or
any small combination) of neurons, and can force the network to be accurate even in the absence of certain
information. Several methods have been proposed to improve Dropout. Wang et al. propose a fast
Dropout method which can perform fast Dropout training by sampling from or integrating a Gaussian
approximation. Ba et al. propose an adaptive Dropout method, where the Dropout probability for
each hidden variable is computed using a binary belief network that shares parameters with the deep
In , they ﬁnd that applying standard Dropout before 1 × 1 convolutional layer generally
increases training time but does not prevent overﬁtting. Therefore, they propose a new Dropout method
called SpatialDropout, which extends the Dropout value across the entire feature map. This new Dropout
method works well especially when the training data size is small.
3.5.3. DropConnect
DropConnect takes the idea of Dropout a step further. Instead of randomly setting the outputs
of neurons to zero, DropConnect randomly sets the elements of weight matrix W to zero. The output of
DropConnect is given by y = a((R ∗W)x), where Rij ∼Bernoulli(p). Additionally, the biases are also
masked out during the training process. Figure 8 illustrates the diﬀerences among No-Drop, Dropout and
DropConnect networks.
3.6. Optimization
In this subsection, we discuss some key techniques for optimizing CNNs.
(a) No-Drop
(b) DropOut
(c) DropConnect
Figure 8: The illustration of No-Drop network, DropOut network and DropConnect network.
3.6.1. Data Augmentation
Deep CNNs are particularly dependent on the availability of large quantities of training data. An elegant
solution to alleviate the relative scarcity of the data compared to the number of parameters involved in CNNs
is data augmentation . Data augmentation consists in transforming the available data into new data
without altering their natures. Popular augmentation methods include simple geometric transformations
such as sampling , mirroring , rotating , shifting , and various photometric transformations .
Paulin et al. propose a greedy strategy that selects the best transformation from a set of candidate
transformations. However, their strategy involves a large number of model re-training steps, which can
be computationally expensive when the number of candidate transformations is large. Hauberg et al. 
propose an elegant way for data augmentation by randomly generating diﬀeomorphisms. Xie et al. 
and Xu et al. oﬀer additional means of collecting images from the Internet to improve learning in
ﬁne-grained recognition tasks.
3.6.2. Weight Initialization
Deep CNN has a huge amount of parameters and its loss function is non-convex , which makes it
very diﬃcult to train. To achieve a fast convergence in training and avoid the vanishing gradient problem,
a proper network initialization is one of the most important prerequisites . The bias parameters
can be initialized to zero, while the weight parameters should be initialized carefully to break the symmetry
among hidden units of the same layer. If the network is not properly initialized, e.g., each layer scales
its input by k, the ﬁnal output will scale the original input by kL where L is the number of layers. In
this case, the value of k > 1 leads to extremely large values of output layers while the value of k < 1
leads a diminishing output value and gradients. Krizhevsky et al. initialize the weights of their network
from a zero-mean Gaussian distribution with standard deviation 0.01 and set the bias terms of the second,
fourth and ﬁfth convolutional layers as well as all the fully-connected layers to constant one. Another famous
random initialization method is “Xavier”, which is proposed in . They pick the weights from a Gaussian
distribution with zero mean and a variance of 2/(nin +nout), where nin is the number of neurons feeding into
it, and nout is the number of neurons the result is fed to. Thus “Xavier” can automatically determine the
scale of initialization based on the number of input and output neurons, and keep the signal in a reasonable
range of values through many layers. One of its variants in Caﬀe 1 uses the nin-only variant, which makes
it much easier to implement. “Xavier” initialization method is later extended by to account for the
rectifying nonlinearities, where they derive a robust initialization method that particularly considers the
ReLU nonlinearity. Their method , allows for the training of extremely deep models (e.g., ) to converge
while the “Xavier” method cannot.
1 
Independently, Saxe et al. show that orthonormal matrix initialization works much better for
linear networks than Gaussian initialization, and it also works for networks with nonlinearities. Mishkin et
al. extend to an iterative procedure. Speciﬁcally, it proposes a layer-sequential unit-variance
process scheme which can be viewed as an orthonormal initialization combined with batch normalization (see
Section 3.6.4) performed only on the ﬁrst mini-batch. It is similar to batch normalization as both of them take
a unit variance normalization procedure. Diﬀerently, it uses ortho-normalization to initialize the weights
which helps to eﬃciently de-correlate layer activities. Such an initialization technique has been applied
to with a remarkable increase in performance.
3.6.3. Stochastic Gradient Descent
The backpropagation algorithm is the standard training method which uses gradient descent to update
the parameters. Many gradient descent optimization algorithms have been proposed . Standard
gradient descent algorithm updates the parameters θθθ of the objective L(θθθ) as θθθt+1 = θθθt −η∇θθθE[L(θθθt)],
where E[L(θθθt)] is the expectation of L(θθθ) over the full training set and η is the learning rate. Instead of
computing E[L(θθθt)], Stochastic Gradient Descent (SGD) estimates the gradients on the basis of a single
randomly picked example (xxx(t),yyy(t)) from the training set:
θθθt+1 = θθθt −ηt∇θθθL(θθθt;xxx(t),yyy(t))
In practice, each parameter update in SGD is computed with respect to a mini-batch as opposed to a single
example. This could help to reduce the variance in the parameter update and can lead to more stable
convergence. The convergence speed is controlled by the learning rate ηt. However, mini-batch SGD does
not guarantee good convergence, and there are still some challenges that need to be addressed. Firstly,
it is not easy to choose a proper learning rate. One common method is to use a constant learning rate
that gives stable convergence in the initial stage, and then reduce the learning rate as the convergence
slows down.
Additionally, learning rate schedules have been proposed to adjust the learning
rate during the training. To make the current gradient update depend on historical batches and accelerate
training, momentum is proposed to accumulate a velocity vector in the relevant direction. The classical
momentum update is given by:
vvvt+1 = γvvvt −ηt∇θθθL(θθθt;xxx(t),yyy(t))
θθθt+1 = θθθt + vvvt+1
where vvvt+1 is the current velocity vector, γ is the momentum term which is usually set to 0.9. Nesterov
momentum is another way of using momentum in gradient descent optimization:
vvvt+1 = γvvvt −ηt∇θθθL(θθθt + γvvvt;xxx(t),yyy(t))
Compared with the classical momentum which ﬁrst computes the current gradient and then moves
in the direction of the updated accumulated gradient, Nesterov momentum ﬁrst moves in the direction of
the previous accumulated gradient γvvvt, calculates the gradient and then makes a gradient update. This
anticipatory update prevents the optimization from moving too fast and achieves better performance .
Parallelized SGD methods improve SGD to be suitable for parallel, large-scale machine learning.
Unlike standard (synchronous) SGD in which the training will be delayed if one of the machines is slow, these
parallelized methods use the asynchronous mechanism so that no other optimizations will be delayed except
for the one on the slowest machine. Jeﬀrey Dean et al. use another asynchronous SGD procedure called
Downpour SGD to speed up the large-scale distributed training process on clusters with many CPUs. There
are also some works that use asynchronous SGD with multiple GPUs. Paine et al. basically combine
asynchronous SGD with GPUs to accelerate the training time by several times compared to training on
a single machine. Zhuang et al. also use multiple GPUs to asynchronously calculate gradients and
update the global model parameters, which achieves 3.2 times of speedup on 4 GPUs compared to training
on a single GPU.
Note that SGD methods may not result in convergence. The training process can be terminated when
the performance stops improving. A popular remedy to over-training is to use early stopping in which
optimization is halted based on the performance on a validation set during training. To control the duration
of the training process, various stopping criteria can be considered. For example, the training might be
performed for a ﬁxed number of epochs, or until a predeﬁned training error is reached . The stopping
strategy should be done carefully , a proper stopping strategy should let the training process continue
as long as the network generalization ability is improved and the overﬁtting is avoided.
3.6.4. Batch Normalization
Data normalization is usually the ﬁrst step of data preprocessing. Global data normalization transforms
all the data to have zero-mean and unit variance. However, as the data ﬂows through a deep network, the
distribution of input to internal layers will be changed, which will lose the learning capacity and accuracy
of the network. Ioﬀe et al. propose an eﬃcient method called Batch Normalization (BN) to partially
alleviate this phenomenon. It accomplishes the so-called covariate shift problem by a normalization step
that ﬁxes the means and variances of layer inputs where the estimations of mean and variance are computed
after each mini-batch rather than the entire training set. Suppose the layer to normalize has a d dimensional
input, i.e., x = [x1, x2, ..., xd]T . We ﬁrst normalize the k-th dimension as follows:
ˆxk = (xk −µB)/
where µB and δ2
B are the mean and variance of mini-batch respectively, and ϵ is a constant value. To enhance
the representation ability, the normalized input ˆxk is further transformed into:
yk = BNγ,β(xk) = γˆxk + β
where γ and β are learned parameters. Batch normalization has many advantages compared with global data
normalization. Firstly, it reduces internal covariant shift. Secondly, BN reduces the dependence of gradients
on the scale of the parameters or of their initial values, which gives a beneﬁcial eﬀect on the gradient
ﬂow through the network.
This enables the use of higher learning rate without the risk of divergence.
Furthermore, BN regularizes the model, and thus reduces the need for Dropout.
Finally, BN makes it
possible to use saturating nonlinear activation functions without getting stuck in the saturated model.
3.6.5. Shortcut Connections
As mentioned above, the vanishing gradient problem of deep CNNs can be alleviated by normalized
initialization and BN . Although these methods successfully prevent deep neural networks from
overﬁtting, they also introduce diﬃculties in optimizing the networks, resulting in worse performances than
shallower networks . Such an optimization problem suﬀered by deeper CNNs is regarded as
the degradation problem.
Inspired by Long Short Term Memory (LSTM) networks which use gate functions to determine
how much of a neuron’s activation value to transform or just pass through. Srivastava et al. propose
highway networks which enable the optimization of networks with virtually arbitrary depth. The output of
their network is given by:
xl+1 = φl+1(xl, WH) · τl+1(xl, WT ) + xl · (1 −τl+1(xl, WT ))
where xl and xl+1 correspond to the input and output of lth highway block, τ(·) is the transform gate
and φ(·) is usually an aﬃne transformation followed by a non-linear activation function (in general it may
take other forms). This gating mechanism forces the layer’s inputs and outputs to be of the same size and
allows highway networks with tens or hundreds of layers to be trained eﬃciently. The outputs of gates vary
signiﬁcantly with the input examples, demonstrating that the network does not just learn a ﬁxed structure,
but dynamically routes data based on speciﬁc examples.
Independently, Residual Nets (ResNets) share the same core idea that works in LSTM units. Instead
of employing learnable weights for neuron-speciﬁc gating, the shortcut connections in ResNets are not gated
and untransformed input is directly propagated to the output which brings fewer parameters. The output
of ResNets can be represented as follows:
xl+1 = xl + fl+1(xl, WF )
where fl is the weight layer, it can be a composite function of operations such as Convolution, BN, ReLU,
or Pooling. With residual block, activation of any deeper unit can be written as the sum of the activation
of a shallower unit and a residual function. This also implies that gradients can be directly propagated to
shallower units, which makes deep ResNets much easier to be optimized than the original mapping function
and more eﬃcient to train very deep nets. This is in contrast to usual feedforward networks, where gradients
are essentially a series of matrix-vector products, that may vanish as networks grow deeper.
After the original ResNets, He et al. follow up with another preactivation variant of ResNets, where
they conduct a set of experiments to show that identity shortcut connections are the easiest for networks to
learn. They also ﬁnd that bringing BN forward performs considerably better than using BN after addition.
In their comparisons, the residual net with BN + ReLU pre-activation gets higher accuracies than their
previous ResNets . Inspired by , Shen et al. introduce a weighting factor for the output from
the convolutional layer, which gradually introduces the trainable layers. The latest Inception-v4 paper 
also reports that training is accelerated and performance is improved by using identity skip connections
across Inception modules. The original ResNets and preactivation ResNets are very deep but also very thin.
By contrast, Wide ResNets proposes to decrease the depth and increase the width, which achieves
impressive results on CIFAR-10, CIFAR-100, and SVHN. However, their claims are not validated on the
large-scale image classiﬁcation task on Imagenet dataset2. Stochastic Depth ResNets randomly drop a subset
of layers and bypass them with the identity mapping for every mini-batch. By combining Stochastic Depth
ResNets and Dropout, Singh et al. generalize dropout and networks with stochastic depth, which
can be viewed as an ensemble of ResNets, Dropout ResNets, and Stochastic Depth ResNets. The ResNets
in ResNets (RiR) paper describes an architecture that merges classical convolutional networks and
residual networks, where each block of RiR contains residual units and non-residual blocks. The RiR can
learn how many convolutional layers it should use per residual block. ResNets of ResNets (RoR) is a
modiﬁcation to the ResNets architecture which proposes to use multi-level shortcut connections as opposed
to single-level shortcut connections in the prior work on ResNets . DenseNet can be seen as an
architecture takes the insights of the skip connection to the extreme, in which the output of a layer is
connected to all the subsequent layer in the module. In all of the ResNets , Highway and
Inception networks , we can see a pretty clear trend of using shortcut connections to help train very deep
4. Fast Processing of CNNs
With the increasing challenges in the computer vision and machine learning tasks, the models of deep
neural networks get more and more complex. These powerful models require more data for training in order
to avoid overﬁtting. Meanwhile, the big training data also brings new challenges such as how to train the
networks in a feasible amount of time. In this section, we introduce some fast processing methods of CNNs.
Mathieu et al. carry out the convolutional operation in the Fourier domain with FFTs.
FFT-based methods has many advantages.
Firstly, the Fourier transformations of ﬁlters can be reused
as the ﬁlters are convolved with multiple images in a mini-batch. Secondly, the Fourier transformations
of the output gradients can be reused when backpropagating gradients to both ﬁlters and input images.
Finally, the summation over input channels can be performed in the Fourier domain, so that inverse Fourier
transformations are only required once per output channel per image. There have already been some GPUbased libraries developed to speed up the training and testing process, such as cuDNN and fbﬀt .
2 
However, using FFT to perform convolution needs additional memory to store the feature maps in the Fourier
domain, since the ﬁlters must be padded to be the same size as the inputs. This is especially costly when
the striding parameter is larger than 1, which is common in many state-of-art networks, such as the early
layers in and . While FFT can achieve faster training and testing process, the rising prominence
of small size convolutional ﬁlters have become an important component in CNNs such as ResNet and
GoogleNet , which makes a new approach specialized for small ﬁlter sizes: Winograd’s minimal ﬁltering
algorithms . The insight of Winograd is like FFT, and the Winograd convolutions can be reduced
across channels in transform space before applying the inverse transform and thus makes the inference more
4.2. Structured Transforms
Low-rank matrix factorization has been exploited in a variety of contexts to improve the optimization
problems. Given an m×n matrix C of rank r, there exists a factorization C = AB where A is an m×r full
column rank matrix and B is an r ×n full row rank matrix. Thus, we can replace C by A and B. To reduce
the parameters of C by a fraction p, it is essential to ensure that mr + rn < pmn, i.e., the rank of C should
satisfy that r < pmn/(m + n). By applying this factorization, the space complexity reduces from O(mn)
to O(r(m + n)), and the time complexity reduces from O(mn) to O(r(m + n)). To this end, Sainath et
al. apply the low-rank matrix factorization to the ﬁnal weight layer in a deep CNN, resulting about
30-50% speedup in training time with little loss in accuracy. Similarly, Xue et al. apply singular value
decomposition on each layer of a deep CNN to reduce the model size by 71% with less than 1% relative
accuracy loss.
Inspired by which demonstrates the redundancy in the parameters of deep neural
networks, Denton et al. and Jaderberg et al. independently investigate the redundancy within
the convolutional ﬁlters and develop approximations to reduced the required computations.
Novikov et
al. generalize the low-rank ideas, where they treat the weight matrix as multi-dimensional tensor and
apply a Tensor-Train decomposition to reduce the number of parameters of the fully-connected layers.
Adaptive Fastfood transform is generalization of the Fastfood transform for approximating matrix. It reparameterizes the weight matrix C ∈Rn×n in fully-connected layers with an Adaptive Fastfood
transformation: Cx = ( ˜D1H ˜D2ΠH ˜D3)x, where ˜D1, ˜D2 and ˜D3 are diagonal matrices of parameters, Π
is a random permutation matrix, and H denotes the Walsh-Hadamard matrix. The space complexity of
Adaptive Fastfood transform is O(n), and the time complexity is O(n log n).
Motivated by the great advantages of circulant matrix in both space and computation eﬃciency ,
Cheng et al. explore the redundancy in the parametrization of fully-connected layers by imposing the
circulant structure on the weight matrix to speed up the computation, and further allow the use of FFT for
faster computation. With a circulant matrix C ∈Rn×n as the matrix of parameters in a fully-connected
layer, for an input vector x ∈Rn, the output of Cx can be calculated eﬃciently using the FFT and inverse
IFFT: CDx = iﬀt(ﬀt(v)) ◦ﬀt(x), where ◦corresponds to elementwise multiplication operation, v ∈Rn
is deﬁned by C, and D is a random sign ﬂipping matrix for improving the capacity of the model. This
method reduces the time complexity from O(n2) to O(n log n), and space complexity from O(n2) to O(n).
Moczulski et al. further generalize the circulant structures by interleaving diagonal matrices with
the orthogonal Discrete Cosine Transform (DCT). The resulting transform, ACDC−1, has O(n) space
complexity and O(n log n) time complexity.
4.3. Low Precision
Floating point numbers are a natural choice for handling the small updates of the parameters of CNNs.
However, the resulting parameters may contain a lot of redundant information . To reduce redundancy,
Binarized Neural Networks (BNNs) restrict some or all the arithmetics involved in computing the outputs
to be binary values.
There are three aspects of binarization for neural network layers: binary input activations, binary synapse
weights, and binary output activations. Full binarization requires all the three components are binarized,
and the cases with one or two components are considered as partial binarization. Kim et al. consider full
binarization with a predetermined portion of the synapses having zero weight, and all other synapses with a
weight of one. Their network only needs XNOR and bit count operations, and they report 98.7% accuracy on
the MNIST dataset. XNOR-Net applies convolutional BNNs on the ImageNet dataset with topologies
inspired by AlexNet, ResNet and GoogLeNet, reporting top-1 accuracies of up to 51.2% for full binarization
and 65.5% for partial binarization. DoReFa-Net explores reducing precision during the forward pass
as well as the backward pass. Both partial and full binarization are explored in their experiments and
the corresponding top-1 accuracies on ImageNet are 43% and 53%. The work by Courbariaux et al. 
describes how to train fully-connected networks and CNNs with full binarization and batch normalization
layers, reporting competitive accuracies on the MNIST, SVHN, and CIFAR-10 datasets.
4.4. Weight Compression
Many attempts have been made to reduce the number of parameters in the convolution layers and fullyconnected layers. Here, we brieﬂy introduce some methods under these topics: vector quantization, pruning,
and hashing.
Vector Quantization (VQ) is a method for compressing densely connected layers to make CNN models
smaller. Similar to scalar quantization where a large set of numbers is mapped to a smaller set , VQ
quantizes groups of numbers together rather than addressing them one at a time. In 2013, Denil et al. 
demonstrate the presence of redundancy in neural network parameters, and use VQ to signiﬁcantly reduce
the number of dynamic parameters in deep models. Gong et al. investigate the information theoretical vector quantization methods for compressing the parameters of CNNs, and they obtain parameter
prediction results similar to those of . They also ﬁnd that VQ methods have a clear gain over existing matrix factorization methods, and among the VQ methods, structured quantization methods such as
product quantization work signiﬁcantly better than other methods (e.g., residual quantization , scalar
quantization ).
An alternative approach to weight compression is pruning. It reduces the number of parameters and
operations in CNNs by permanently dropping less important connections , which enables smaller networks to inherit knowledge from the large predecessor networks and maintains comparable of performance.
Han et al. introduce ﬁne-grained sparsity in a network by a magnitude-based pruning approach.
If the absolute magnitude of any weight is less than a scalar threshold, the weight is pruned. Gao et al. 
extend the magnitude-based approach to allow restoration of the pruned weights in the previous iterations,
with tightly coupled pruning and retraining stages, for greater model compression. Yang et al. take the
correlation between weights into consideration and propose an energy-aware pruning algorithm that directly
uses energy consumption estimation of a CNN to guide the pruning process. Rather than ﬁne-grained pruning, there are also works that investigate coarse-grained pruning. Hu et al. propose removing ﬁlters
that frequently generate zero output activations on the validation set. Srinivas et al. merge similar
ﬁlters into one, while Mariet et al. merge ﬁlters with similar output activations into one.
Designing a proper hashing technique to accelerate the training of CNNs or save memory space also an
interesting problem. HashedNets is a recent technique to reduce model sizes by using a hash function
to group connection weights into hash buckets, and all connections within the same hash bucket share
a single parameter value. Their network shrinks the storage costs of neural networks signiﬁcantly while
mostly preserves the generalization performance in image classiﬁcation. As pointed out in Shi et al. 
and Weinberger et al. , sparsity will minimize hash collision making feature hashing even more eﬀective.
HashNets may be used together with pruning to give even better parameter savings.
4.5. Sparse Convolution
Recently, several attempts have been made to sparsify the weights of convolutional layers . Liu et
al. consider sparse representations of the basis ﬁlters, and achieve 90% sparsifying by exploiting both
inter-channel and intra-channel redundancy of convolutional kernels. Instead of sparsifying the weights of
convolution layers, Wen et al. propose a Structured Sparsity Learning (SSL) approach to simultaneously
optimize their hyperparameters (ﬁlter size, depth, and local connectivity). Bagherinezhad et al. propose
a lookup-based convolutional neural network (LCNN) that encodes convolutions by few lookups to a rich
set of dictionary that is trained to cover the space of weights in CNNs. They decode the weights of the
convolutional layer with a dictionary and two tensors. The dictionary is shared among all weight ﬁlters in a
layer, which allows a CNN to learn from very few training examples. LCNN can achieve a higher accuracy
in a small number of iterations compared to standard CNN.
5. Applications of CNNs
In this section, we introduce some recent works that apply CNNs to achieve state-of-the-art performance,
including image classiﬁcation, object tracking, pose estimation, text detection, visual saliency detection,
action recognition, scene labeling, speech and natural language processing.
5.1. Image Classiﬁcation
CNNs have been applied in image classiﬁcation for a long time . Compared with other methods,
CNNs can achieve better classiﬁcation accuracy on large scale datasets due to their capability
of joint feature and classiﬁer learning. The breakthrough of large scale image classiﬁcation comes in 2012.
Krizhevsky et al. develop the AlexNet and achieve the best performance in ILSVRC 2012. After the
success of AlexNet, several works have made signiﬁcant improvements in classiﬁcation accuracy by either
reducing ﬁlter size or expanding the network depth .
Building a hierarchy of classiﬁers is a common strategy for image classiﬁcation with a large number of
classes . The work of is one of the earliest attempts to introduce category hierarchy in CNN,
in which a discriminative transfer learning with tree-based priors is proposed.
They use a hierarchy of
classes for sharing information among related classes in order to improve performance for classes with very
few training examples. Similarly, Wang et al. build a tree structure to learn ﬁne-grained features
for subcategory recognition. Xiao et al. propose a training method that grows a network not only
incrementally but also hierarchically. In their method, classes are grouped according to similarities and
are self-organized into diﬀerent levels. Yan et al. introduce a hierarchical deep CNNs (HD-CNNs) by
embedding deep CNNs into a category hierarchy. They decompose the classiﬁcation task into two steps.
The coarse category CNN classiﬁer is ﬁrst used to separate easy classes from each other, and then those
more challenging classes are routed downstream to ﬁne category classiﬁers for further prediction.
architecture follows the coarse-to-ﬁne classiﬁcation paradigm and can achieve lower error at the cost of an
aﬀordable increase of complexity.
Subcategory classiﬁcation is another rapidly growing subﬁeld of image classiﬁcation. There are already
some ﬁne-grained image datasets (such as Birds , Dogs , Cars , and Plants ). Using
object part information is beneﬁcial for ﬁne-grained classiﬁcation . Generally, the accuracy can be
improved by localizing important parts of objects and representing their appearances discriminatively. Along
this way, Branson et al. propose a method which detects parts and extracts CNN features from
multiple pose-normalized regions. Part annotation information is used to learn a compact pose normalization
space. They also build a model that integrates lower-level feature layers with pose-normalized extraction
routines and higher-level feature layers with unaligned image features to improve the classiﬁcation accuracy.
Zhang et al. propose a part-based R-CNN which can learn whole-object and part detectors. They
use selective search to generate the part proposals, and apply non-parametric geometric constraints to
more accurately localize parts. Lin et al. incorporate part localization, alignment, and classiﬁcation
into one recognition system which is called Deep LAC. Their system is composed of three sub-networks:
localization sub-network is used to estimate the part location, alignment sub-network receives the location as
input and performs template alignment , and classiﬁcation sub-network takes pose aligned part images
as input to predict the category label. They also propose a value linkage function to link the sub-networks
and make them work as a whole in training and testing.
As can be noted, all the above-mentioned methods make use of part annotation information for supervised
training. However, these annotations are not easy to collect and these systems have diﬃculty in scaling up
and to handle many types of ﬁne-grained classes. To avoid this problem, some researchers propose to ﬁnd
localized parts or regions in an unsupervised manner. Krause et al. use the ensemble of localized
learned feature representations for ﬁne-grained classiﬁcation, they use co-segmentation and alignment to
generate parts, and then compare the appearance of each part and aggregate the similarities together. In
their latest paper , they combine co-segmentation and alignment in a discriminative mixture to generate
parts for facilitating ﬁne-grained classiﬁcation. Zhang et al. use the unsupervised selective search to
generate object proposals, and then select the useful parts from the multi-scale generated part proposals.
Xiao et al. apply visual attention in CNN for ﬁne-grained classiﬁcation. Their classiﬁcation pipeline
is composed of three types of attentions: the bottom-up attention proposes candidate patches, the objectlevel top-down attention selects relevant patches of a certain object, and the part-level top-down attention
localizes discriminative parts. These attentions are combined to train domain-speciﬁc networks which can
help to ﬁnd foreground object or object parts and extract discriminative features. Lin et al. propose
a bilinear model for ﬁne-grained image classiﬁcation. The recognition architecture consists of two feature
extractors. The outputs of two feature extractors are multiplied using the outer product at each location of
the image, and are pooled to obtain an image descriptor.
5.2. Object Detection
Object detection has been a long-standing and important problem in computer vision . Generally, the diﬃculties mainly lie in how to accurately and eﬃciently localize objects in images or video frames.
The use of CNNs for detection and localization can be traced back to 1990s . However, due to the lack
of training data and limited processing resources, the progress of CNN-based object detection is slow before
2012. Since 2012, the huge success of CNNs in ImageNet challenge rekindles interest in CNN-based object
detection . In some early works , they use the sliding window based approaches to densely
evaluate the CNN classiﬁer on windows sampled at each location and scale. Since there are usually hundreds
of thousands of candidate windows in a image, these methods suﬀer from highly computational cost, which
makes them unsuitable to be applied on the large-scale dataset, e.g., Pascal VOC , ImageNet and
MSCOCO .
Recently, object proposal based methods attract a lot of interests and are widely studied in the literature . These methods usually exploit fast and generic measurements to test whether
a sampled window is a potential object or not, and further pass the output object proposals to more sophisticated detectors to determine whether they are background or belong to a speciﬁc object class. One of
the most famous object proposal based CNN detector is Region-based CNN (R-CNN) . R-CNN uses
Selective Search (SS) to extract around 2000 bottom-up region proposals that are likely to contain
objects. Then, these region proposals are warped to a ﬁxed size (227 × 227), and a pre-trained CNN is used
to extract features from them. Finally, a binary SVM classiﬁer is used for detection.
R-CNN yields a signiﬁcant performance boost. However, its computational cost is still high since the
time-consuming CNN feature extractor will be performed for each region separately.
To deal with this
problem, some recent works propose to share the computation in feature extraction . Over-
Feat computes CNN features from an image pyramid for localization and detection. Hence the computation can be easily shared between overlapping windows. Spatial pyramid pooling network (SPP net) 
is a pyramid-based version of R-CNN , which introduces an SPP layer to relax the constraint that
input images must have a ﬁxed size. Unlike R-CNN, SPP net extracts the feature maps from the entire
image only once, and then applies spatial pyramid pooling on each candidate window to get a ﬁxed-length
representation. A drawback of SPP net is that its training procedure is a multi-stage pipeline, which makes
it impossible to train the CNN feature extractor and SVM classiﬁer jointly to further improve the accuracy. Fast RCNN improves SPP net by using an end-to-end training method. All network layers
can be updated during ﬁne-tuning, which simpliﬁes the learning process and improves detection accuracy.
Later, Faster R-CNN introduces a region proposal network (RPN) for object proposals generation
and achieves further speed-up. Beside R-CNN based methods, Gidaris et al. propose a multi-region
and semantic segmentation-aware model for object detection. They integrate the combined features on an
iterative localization mechanism as well as a box-voting scheme after non-max suppression. Yoo et al. 
treat the object detection problem as an iterative classiﬁcation problem. It predicts an accurate object
boundary box by aggregating quantized weak directions from their detection network.
Another important issue of object detection is how to explore eﬀective training sets as the performance is
somehow largely depends on quantity and quality of both positive and negative samples. Online bootstrap-
ping (or hard negative mining ) for CNN training has recently gained interest due to its importance
for intelligent cognitive systems interacting with dynamically changing environments . proposes
a novel bootstrapping technique called online hard example mining (OHEM) for training detection models
based on CNNs. It simpliﬁes the training process by automatically selecting the hard examples. Meanwhile, it only computes the feature maps of an image once, and then forwards all region-of-interests (RoIs)
of the image on top of these feature maps. Thus it is able to ﬁnd the hard examples with a small extra
computational cost.
More recently, YOLO and SSD allow single pipeline detection that directly predicts class
labels. YOLO treats object detection as a regression problem to spatially separated bounding boxes
and associated class probabilities. The whole detection pipeline is a single network which predicts bounding
boxes and class probabilities from the full image in one evaluation, and can be optimized end-to-end directly
on detection performance. SSD discretizes the output space of bounding boxes into a set of default boxes
over diﬀerent aspect ratios and scales per feature map location. With this multiple scales setting and their
matching strategy, SSD is signiﬁcantly more accurate than YOLO. With the beneﬁts from super-resolution,
Lu et al. propose a top-down search strategy to divide a window into sub-windows recursively, in which
an additional network is trained to account for such division decisions.
5.3. Object Tracking
The success in object tracking relies heavily on how robust the representation of target appearance is
against several challenges such as view point changes, illumination changes, and occlusions . There
are several attempts to employ CNNs for visual tracking. Fan et al. use CNN as a base learner. It
learns a separate class-speciﬁc network to track objects. In , the authors design a CNN tracker with
a shift-variant architecture. Such an architecture plays a key role so that it turns the CNN model from a
detector into a tracker. The features are learned during oﬄine training. Diﬀerent from traditional trackers
which only extract local spatial structures, this CNN based tracking method extracts both spatial and
temporal structures by considering the images of two consecutive frames. Because the large signals in the
temporal information tend to occur near objects that are moving, the temporal structures provide a crude
velocity signal to tracking.
Li et al. propose a target-speciﬁc CNN for object tracking, where the CNN is trained incrementally
during tracking with new examples obtained online. They employ a candidate pool of multiple CNNs as a
data-driven model of diﬀerent instances of the target object. Individually, each CNN maintains a speciﬁc set
of kernels that favourably discriminate object patches from their surrounding background using all available
low-level cues. These kernels are updated in an online manner at each frame after being trained with just one
instance at the initialization of the corresponding CNN. Instead of learning one complicated and powerful
CNN model for all the appearance observations in the past, Li et al. use a relatively small number of
ﬁlters in the CNN within a framework equipped with a temporal adaptation mechanism. Given a frame,
the most promising CNNs in the pool are selected to evaluate the hypothesises for the target object. The
hypothesis with the highest score is assigned as the current detection window and the selected models are
retrained using a warm-start backpropagation which optimizes a structural loss function.
In , a CNN object tracking method is proposed to address limitations of handcrafted features and
shallow classiﬁer structures in object tracking problem. The discriminative features are ﬁrst automatically
learned via a CNN. To alleviate the tracker drifting problem caused by model update, the tracker exploits the
ground truth appearance information of the object labeled in the initial frames and the image observations
obtained online. A heuristic schema is used to judge whether updating the object appearance models or
Hong et al. propose a visual tracking algorithm based on a pre-trained CNN, where the network is
trained originally for large-scale image classiﬁcation and the learned representation is transferred to describe
target. On top of the hidden layers in the CNN, they put an additional layer of an online SVM to learn a
target appearance discriminatively against background. The model learned by SVM is used to compute a
target-speciﬁc saliency map by back-projecting the information relevant to target to input image space. And
they exploit the target-speciﬁc saliency map to obtain generative target appearance models and perform
tracking with understanding of spatial conﬁguration of target.
5.4. Pose Estimation
Since the breakthrough in deep structure learning, many recent works pay more attention to learn
multiple levels of representations and abstractions for human-body pose estimation task with CNNs . DeepPose is the ﬁrst application of CNNs to human pose estimation problem. In this work,
pose estimation is formulated as a CNN-based regression problem to body joint coordinates. A cascade of
7-layered CNNs are presented to reason about pose in a holistic manner. Unlike the previous works that
usually explicitly design graphical model and part detectors, the DeepPose captures the full context of each
body joint by taking the whole image as the input.
Meanwhile, some works exploit CNN to learn representation of local body parts. Ajrun et al. 
present a CNN based end-to-end learning approach for full-body human pose estimation, in which CNN
part detectors and an Markov Random Field (MRF)-like spatial model are jointly trained, and pair-wise
potentials in the graph are computed using convolutional priors. In a series of papers, Tompson et al. 
use a multi-resolution CNN to compute heat-map for each body part. Diﬀerent from , Tompson et
al. learn the body part prior model and implicitly the structure of the spatial model. Speciﬁcally, they
start by connecting every body part to itself and to every other body part in a pair-wise fashion, and use a
fully-connected graph to model the spatial prior. As an extension of , Tompson et al. propose a CNN
architecture which includes a position reﬁnement model after a rough pose estimation CNN. This reﬁnement
model, which is a Siamese network , is jointly trained in cascade with the oﬀ-the-shelf model . In a
similar work with , Chen et al. also combine graphical model with CNN. They exploit a CNN
to learn conditional probabilities for the presence of parts and their spatial relationships, which are used in
unary and pairwise terms of the graphical model. The learned conditional probabilities can be regarded as
low-dimensional representations of the body pose. There is also a pose estimation method called dual-source
CNN that integrates graphical models and holistic style. It takes the full body image and the holistic
view of the local parts as inputs to combine both local and contextual information.
In addition to still image pose estimation with CNN, recently researchers also apply CNN to human
pose estimation in videos. Based on the work , Jain et al. also incorporate RGB features and
motion features to a multi-resolution CNN architecture to further improve accuracy. Speciﬁcally, The CNN
works in a sliding-window manner to perform pose estimation. The input of the CNN is a 3D tensor which
consists of an RGB image and its corresponding motion features, and the output is a 3D tensor containing
response-maps of the joints. In each response map, the value of each location denote the energy for presence
the corresponding joint at that pixel location. The multi-resolution processing is achieved by simply down
sampling the inputs and feeding them to the network.
5.5. Text Detection and Recognition
The task of recognizing text in image has been widely studied for a long time . Traditionally,
optical character recognition (OCR) is the major focus. OCR techniques mainly perform text recognition
on images in rather constrained visual environments (e.g., clean background, well-aligned text). Recently,
the focus has been shifted to text recognition on scene images due to the growing trend of high-level visual
understanding in computer vision research . The scene images are captured in unconstrained environments where there exists a large amount of appearance variations which poses great diﬃculties to existing
OCR techniques. Such a concern can be mitigated by using stronger and richer feature representations such
as those learned by CNN models. Along the line of improving the performance of scene text recognition
with CNN, a few works have been proposed. The works can be coarsely categorized into three types: (1)
text detection and localization without recognition, (2) text recognition on cropped text images, and (3)
end-to-end text spotting that integrates both text detection and recognition:
5.5.1. Text Detection
One of the pioneering works to apply CNN for scene text detection is . The CNN model employed
by learns on cropped text patches and non-text scene patches to discriminate between the two. The text
are then detected on the response maps generated by the CNN ﬁlters given the multiscale image pyramid of
the input. To reduce the search space for text detection, Xu et al. propose to obtain a set of character
candidates via Maximally Stable Extremal Regions (MSER) and ﬁlter the candidates by CNN classiﬁcation.
Another work that combines MSER and CNN for text detection is . In , CNN is used to distinguish
text-like MSER components from non-text components, and cluttered text components are split by applying
CNN in a sliding window manner followed by Non-Maximal Suppression (NMS). Other than localization
of text, there is an interesting work that makes use of CNN to determine whether the input image
contains text, without telling where the text is exactly located. In , text candidates are obtained using
MSER which are then passed into a CNN to generate visual features, and lastly the global features of the
images are constructed by aggregating the CNN features in a Bag-of-Words (BoW) framework.
5.5.2. Text Recognition
Goodfellow et al. propose a CNN model with multiple softmax classiﬁers in its ﬁnal layer, which is
formulated in such a way that each classiﬁer is responsible for character prediction at each sequential location
in the multi-digit input image.
As an attempt to recognize text without using lexicon and dictionary,
Jaderberg et al. introduce a novel Conditional Random Fields (CRF)-like CNN model to jointly
learn character sequence prediction and bigram generation for scene text recognition.
The more recent
text recognition methods supplement conventional CNN models with variants of recurrent neural networks
(RNN) to better model the sequence dependencies between characters in text. In , CNN extracts rich
visual features from character-level image patches obtained via sliding window, and the sequence labelling
is carried out by LSTM . The method presented in is very similar to , except that in ,
lexicon can be taken into consideration to enhance text recognition performance.
5.5.3. End-to-end Text Spotting
For end-to-end text spotting, Wang et al. apply a CNN model originally trained for character
classiﬁcation to perform text detection.
Going in a similar direction as , the CNN model proposed
in enables feature sharing across the four diﬀerent subtasks of an end-to-end text spotting system:
text detection, character case-sensitive and insensitive classiﬁcation, and bigram classiﬁcation. Jaderberg et
al. make use of CNNs in a very comprehensive way to perform end-to-end text spotting. In , the
major subtasks of its proposed system, namely text bounding box ﬁltering, text bounding box regression,
and text recognition are each tackled by a separate CNN model.
5.6. Visual Saliency Detection
The technique to locate important regions in imagery is referred to as visual saliency prediction. It
is a challenging research topic, with a vast number of computer vision and image processing applications
facilitated by it. Recently, a couple of works have been proposed to harness the strong visual modeling
capability of CNNs for visual saliency prediction.
Multi-contextual information is a crucial prior in visual saliency prediction, and it has been used concurrently with CNN in most of the considered works . Wang et al. introduce a novel saliency
detection algorithm which sequentially exploits local context and global context. The local context is handled by a CNN model which assigns a local saliency value to each pixel given the input of local image patches,
while the global context (object-level information) is handled by a deep fully-connected feedforward network.
In , the CNN parameters are shared between the global-context and local-context models, for predicting
the saliency of superpixels found within object proposals. The CNN model adopted in is pre-trained
on large-scale image classiﬁcation dataset and then shared among diﬀerent contextual levels for feature extraction. The outputs of the CNN at diﬀerent contextual levels are then concatenated as input to be passed
into a trainable fully-connected feedforward network for saliency prediction. Similar to , the CNN
model used in for saliency prediction are shared across three CNN streams, with each stream taking
input of a diﬀerent contextual scale. He et al. derive a spatial kernel and a range kernel to produce two
meaningful sequences as 1-D CNN inputs, to describe color uniqueness and color distribution respectively.
The proposed sequences are advantageous over inputs of raw image pixels because they can reduce the
training complexity of CNN, while being able to encode the contextual information among superpixels.
There are also CNN-based saliency prediction approaches that do not consider multi-contextual
information. Instead, they rely very much on the powerful representation capability of CNN. In , an
ensemble of CNNs is derived from a large number of randomly instantiated CNN models, to generate good
features for saliency detection. The CNN models instantiated in are however not deep enough because
the maximum number of layers is capped at three. By using a pre-trained and deeper CNN model with 5
convolutional layers, (Deep Gaze) learns a separate saliency model to jointly combine the responses
from every CNN layer and predict saliency values. is the only work making use of CNN to perform
visual saliency prediction in an end-to-end manner, which means the CNN model accepts raw pixels as input
and produces saliency map as output. Pan et al. argue that the success of the proposed end-to-end
method is attributed to its not-so-deep CNN architecture which attempts to prevent overﬁtting.
5.7. Action Recognition
Action recognition, the behaviour analysis of human subjects and classifying their activities based on
their visual appearance and motion dynamics, is one of the challenging problems in computer vision . Generally, this problem can be divided to two major groups: action analysis in still images and in
videos. For both of these two groups, eﬀective CNN based methods have been proposed. In this subsection
we brieﬂy introduce the latest advances on these two groups.
5.7.1. Action Recognition in Still Images
The work of has shown the output of last few layers of a trained CNN can be used as a general visual
feature descriptor for a variety of tasks. The same intuition is utilized for action recognition by ,
in which they use the outputs of the penultimate layer of a pre-trained CNN to represent full images of
actions as well as the human bounding boxes inside them, and achieve a high level of performance in action
classiﬁcation. Gkioxari et al. add a part detection to this framework. Their part detector is a CNN
based extension to the original Poselet method.
CNN based representation of contextual information is utilized for action recognition in . They
search for the most representative secondary region within a large number of object proposal regions in the
image and add contextual features to the description of the primary region (ground truth bounding box of
human subject) in a bottom-up manner. They utilize a CNN to represent and ﬁne-tune the representations
of the primary and the contextual regions. After that, they move a step forward and show that it is possible
to locate and recognize human actions in images without using human bounding boxes . However, they
need to train human detectors to guide their recognition at test time. In , they propose a method that
segments out the action mask of underlying human-object interactions with minimum annotation eﬀorts.
5.7.2. Action Recognition in Video Sequences
Applying CNNs on videos is challenging because traditional CNNs are designed to represent two dimensional pure spatial signals but in videos a new temporal axis is added which is essentially diﬀerent from the
spatial variations in images . The sizes of the video signals are also in higher orders in comparison
to those of images which makes it more diﬃcult to apply convolutional networks on. Ji et al. propose
to consider the temporal axis in a similar manner as other spatial axes and introduce a network of 3D convolutional layers to be applied on video inputs. Recently Tran et al. study the performance, eﬃciency,
and eﬀectiveness of this approach and show its strengths compared to other approaches.
Another approach to apply CNNs on videos is to keep the convolutions in 2D and fuse the feature maps
of consecutive frames, as proposed by . They evaluate three diﬀerent fusion policies: late fusion, early
fusion, and slow fusion, and compare them with applying the CNN on individual single frames. One more
step forward for better action recognition via CNNs is to separate the representation to spatial and temporal
variations and train individual CNNs for each of them, as proposed by Simonyan and Zisserman . First
stream of this framework is a traditional CNN applied on all the frames and the second receives the dense
optical ﬂow of the input videos and trains another CNN which is identical to the spatial stream in size and
structure. The output of the two streams are combined in a class score fusion step. Ch´eron et al. utilize
the two stream CNN on the localized parts of the human body and show the aggregation of part-based local
CNN descriptors can eﬀectively improve the performance of action recognition. Another approach to model
the dynamics of videos diﬀerently from spatial variations, is to feed the CNN based features of individual
frames to a sequence learning module e.g., a recurrent neural network. Donahue et al. study diﬀerent
conﬁgurations of applying LSTM units as the sequence learner in this framework.
5.8. Scene Labeling
Scene labeling aims to relate one semantic class (road, water, sea etc.) to each pixel of the input image . CNNs are used to model the class likelihood of pixels directly from local image patches. They
are able to learn strong features and classiﬁers to discriminate the local visual subtleties. Farabet et al. 
have pioneered to apply CNNs to scene labeling tasks. They feed their Multi-scale ConvNet with diﬀerent
scale image patches, and they show that the learned network is able to perform much better than systems
with hand-crafted features. Besides, this network is also successfully applied to RGB-D scene labeling .
To enable the CNNs to have a large ﬁeld of view over pixels, Pinheiro et al. develop the recurrent
CNNs. More speciﬁcally, the identical CNNs are applied recurrently to the output maps of CNNs in the
previous iterations. By doing this, they can achieve slightly better labeling results while signiﬁcantly reduces
the inference times. Shuai et al. train the parametric CNNs by sampling image patches, which
speeds up the training time dramatically. They ﬁnd that patch-based CNNs suﬀer from local ambiguity
problems, and solve it by integrating global beliefs. and use the recurrent neural networks to
model the contextual dependencies among image features from CNNs, and dramatically boost the labeling
performance.
Meanwhile, researchers are exploiting to use the pre-trained deep CNNs for object semantic segmentation.
Mostajabi et al. apply the local and proximal features from a ConvNet and apply the Alex-net 
to obtain the distant and global features, and their concatenation gives rise to the zoom-out features.
They achieve very competitive results on the semantic segmentation tasks. Long et al. train a fully
convolutional Network to directly predict the input images to dense label maps. The convolution layers of the
FCNs are initialized from the model pre-trained on ImageNet classiﬁcation dataset, and the deconvolution
layers are learned to upsample the resolution of label maps. Chen et al. also apply the pre-trained
deep CNNs to emit the labels of pixels. Considering that the imperfectness of boundary alignment, they
further use fully connected CRF to boost the labeling performance.
5.9. Speech Processing
5.9.1. Automatic Speech Recognition
Automatic Speech Recognition (ASR) is the technology that converts human speech into spoken words .
Before applying CNN to ASR, this domain has long been dominated by the Hidden Markov Model and Gaussian Mixture Model (GMM-HMM) methods , which usually require extracting hand-craft features on
speech signals, e.g., the most popular Mel Frequency Cepstral Coeﬃcients (MFCC) features. Meanwhile,
some researchers have applied Deep Neural Networks (DNNs) in large vocabulary continuous speech recognition (LVCSR) and obtained encouraging results , however, their networks are susceptible to
performance degradations under mismatched condition , such as diﬀerent recording conditions etc.
CNNs have shown better performance over GMM-HMMs and general DNNs , since they are
well suited to exploit the correlations in both time and frequency domains through the local connectivity
and are capable of capturing frequency shift in human speech signals. In , they achieve lower speech
recognition errors by applying CNN on Mel ﬁlter bank features. Some attempts use the raw waveform with
CNNs, and to learn ﬁlters to process the raw waveform jointly with the rest of the network . Most
of the early applications of CNN in ASR only use fewer convolution layers. For example, Abdel-Hamid et
al. use one convolutional layer in their network, and Amodei et al. use three convolutional
layers as the feature preprocessing layer. Recently, very deep CNNs have shown impressive performance in
ASR . Besides, small ﬁlters have successfully applied in acoustic modeling in hybrid NN-HMM
ASR system, and pooling operations are replaced by densely connected layers for ASR tasks . Yu et
al. propose a layer-wise context expansion with attention model for ASR. It is a variant of time-delay
neural network in which lower layers focus on extracting simple local patterns while higher layers
exploit broader context and extract complex patterns than the lower layers. A similar idea can be found
5.9.2. Statistical Parametric Speech Synthesis
In addition to speech recognition, the impact of CNNs has also spread to Statistical Parametric Speech
Synthesis (SPSS). The goal of speech synthesis is to generate speech sounds directly from the text and
possibly with additional information. It has been known for many years that the speech sounds generated
by shallow structured HMM networks are often muﬄed compared with natural speech. Many studies have
adopted deep learning to overcome such deﬁciency .
One advantage of these methods is their
strong ability to represent the intrinsic correlation by using a generative modeling framework. Inspired by
the recent advances in neural autoregressive generative models that model complex distributions such as
images and text , WaveNet makes use of the generative model of the CNN to represent the
conditional distribution of the acoustic features given the linguistic features, which can be seen as a milestone
in SPSS. In order to deal with long-range temporal dependencies, they develop a new architecture based
on dilated causal convolutions to capture very large receptive ﬁelds. By conditioning linguistic features on
text, it can be used to directly synthesize speech from text.
5.10. Natural Language Processing
5.10.1. Statistical Language Modeling
For statistical language modeling, the input typically consists of incomplete sequences of words rather
than complete sentences . Kim et al. use the output of character-level CNN as the input to an
LSTM at each time step. genCNN is a convolutional architecture for sequence prediction, which uses
separate gating networks to replace the max-pooling operations. Recently, Kalchbrenner et al. propose a
CNN-based architecture for sequence processing called ByteNet, which is a stack of two dilated CNNs. Like
WaveNet , ByteNet also beneﬁts from convolutions with dilations to increase the receptive ﬁeld size, thus
can model sequential data with long-term dependencies. It also has the advantage that the computational
time only linearly depends on the length of the sequences. Compared with recurrent neural networks, CNNs
not only can get long-range information but also get a hierarchical representation of the input words. Gu et
al. and Yann et al. share a similar idea that both of them use CNN without pooling to model the
input words. Gu et al. combine the language CNN with recurrent highway networks and achieve a huge
improvement compared to LSTM-based methods. Inspired by the gating mechanism in LSTM networks,
the gated CNN in uses a gating mechanism to control the path through which information ﬂows in the
network, and achieves the state-of-the-art on WiKiText-103. However, the frameworks in and 
are still under the recurrent framework, and the input window size of their network are of limited size. How
to capture the speciﬁc long-term dependencies as well as hierarchical representation of history words is still
an open problem.
5.10.2. Text Classiﬁcation
Text classiﬁcation is a crucial task for Natural Language Processing (NLP). Natural language sentences
have complicated structures, both sequential and hierarchical, that are essential for understanding them.
Owing to the powerful capability of capturing local relations of temporal or hierarchical structures, CNNs
have achieved top performance in sentence modeling. A proper CNN architecture is important for text
classiﬁcation. Collobert et al. and Yu et al. apply one convolutional layer to model the sentence,
while Kalchbrenner et al. stack multiple layers of convolution to model sentences.
In , they
use multichannel convolution and variable kernels for sentence classiﬁcation.
It is shown that multiple
convolutional layers help to extract high-level abstract features, and multiple linear ﬁlters can eﬀectively
consider diﬀerent n-gram features. Recently, Yin et al. extend the network in by hierarchical
convolution architecture and further exploration of multichannel and variable size feature detectors. The
pooling operation can help the network deal with variable sentence lengths. In , they use maxpooling to keep the most important information to represent the sentence. However, max-pooling cannot
distinguish whether a relevant feature in one of the rows occurs just one or multiple times and it ignores
the order in which the features occur. In , they propose the k-max pooling which returns the top
k activations in the original order in the input sequence. Dynamic k-max pooling is a generalization of
the k-max pooling operator where the k value is depended on the input feature map size.
architectures mentioned above are rather shallow compared with the deep CNNs which are very successful
in computer vision. Recently, Conneau et al. implement a deep convolutional architecture which is up
to 29 convolutional layers. They ﬁnd that shortcut connections give better results when the network is very
deep (49 layers). However, they do not achieve state-of-the-art under this setting.
6. Conclusions and Outlook
Deep CNNs have made breakthroughs in processing image, video, speech and text. In this paper, we
have given an extensive survey on recent advances of CNNs. We have discussed the improvements of CNN
on diﬀerent aspects, namely, layer design, activation function, loss function, regularization, optimization
and fast computation. Beyond surveying the advances of each aspect of CNN, we have also introduced the
application of CNN on many tasks, including image classiﬁcation, object detection, object tracking, pose
estimation, text detection, visual saliency detection, action recognition, scene labeling, speech and natural
language processing.
Although CNNs have achieved great success in experimental evaluations, there are still lots of issues that
deserve further investigation. Firstly, since the recent CNNs are becoming deeper and deeper, they require
large-scale dataset and massive computing power for training. Manually collecting labeled dataset requires
huge amounts of human eﬀorts. Thus, it is desired to explore unsupervised learning of CNNs. Meanwhile,
to speed up training procedure, although there are already some asynchronous SGD algorithms which have
shown promising result by using CPU and GPU clusters, it is still worth to develop eﬀective and scalable
parallel training algorithms. At testing time, these deep models are highly memory demanding and timeconsuming, which makes them not suitable to be deployed on mobile platforms that have limited resources.
It is important to investigate how to reduce the complexity and obtain fast-to-execute models without loss
of accuracy.
Furthermore, one major barrier for applying CNN on a new task is that it requires considerable skill and
experience to select suitable hyperparameters such as the learning rate, kernel sizes of convolutional ﬁlters,
the number of layers etc. These hyper-parameters have internal dependencies which make them particularly
expensive for tuning. Recent works have shown that there exists a big room to improve current optimization
techniques for learning deep CNN architectures .
Finally, the solid theory of CNNs is still lacking.
Current CNN model works very well for various
applications. However, we do not even know why and how it works essentially. It is desirable to make more
eﬀorts on investigating the fundamental principles of CNNs. Meanwhile, it is also worth exploring how to
leverage natural visual perception mechanism to further improve the design of CNN. We hope that this
paper not only provides a better understanding of CNNs but also facilitates future research activities and
application developments in the ﬁeld of CNNs.
Acknowledgment
This research was carried out at the Rapid-Rich Object Search (ROSE) Lab at the Nanyang Technological University, Singapore. The ROSE Lab is supported by the Infocomm Media Development Authority,
Singapore.