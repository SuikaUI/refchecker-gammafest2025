Learning Deep Transformer Models for Machine Translation
Qiang Wang1, Bei Li1, Tong Xiao1,2∗, Jingbo Zhu1,2, Changliang Li3,
Derek F. Wong4, Lidia S. Chao4
1NLP Lab, Northeastern University, Shenyang, China
2NiuTrans Co., Ltd., Shenyang, China
3Kingsoft AI Lab, Beijing, China
4NLP2CT Lab, University of Macau, Macau, China
 , libei ,
{xiaotong,zhujingbo}@mail.neu.edu.com,
 , {derekfw,lidiasc}@um.edu.mo
Transformer is the state-of-the-art model in
recent machine translation evaluations. Two
strands of research are promising to improve models of this kind:
the ﬁrst uses
wide networks (a.k.a. Transformer-Big) and
has been the de facto standard for the development of the Transformer system, and
the other uses deeper language representation
but faces the difﬁculty arising from learning deep networks.
Here, we continue the
line of research on the latter. We claim that
a truly deep Transformer model can surpass
the Transformer-Big counterpart by 1) proper
use of layer normalization and 2) a novel
way of passing the combination of previous
layers to the next.
On WMT’16 English-
German, NIST OpenMT’12 Chinese-English
and larger WMT’18 Chinese-English tasks,
our deep system (30/25-layer encoder) outperforms the shallow Transformer-Big/Base
baseline (6-layer encoder) by 0.4∼2.4 BLEU
points. As another bonus, the deep model is
1.6X smaller in size and 3X faster in training
than Transformer-Big1.
Introduction
Neural machine translation (NMT) models have
advanced the previous state-of-the-art by learning mappings between sequences via neural networks and attention mechanisms .
The earliest of
these read and generate word sequences using a
series of recurrent neural network (RNN) units,
and the improvement continues when 4-8 layers
are stacked for a deeper model . More recently, the system
based on multi-layer self-attention (call it Transformer) has shown strong results on several large-
∗Corresponding author.
1The source code is available at 
com/wangqiangneu/dlcl
scale tasks .
In particular, approaches of this kind beneﬁt greatly from
a wide network with more hidden states (a.k.a.
Transformer-Big), whereas simply deepening the
network has not been found to outperform the
“shallow” counterpart . Do
deep models help Transformer? It is still an open
question for the discipline.
For vanilla Transformer, learning deeper networks is not easy because there is already a relatively deep model in use2. It is well known that
such deep networks are difﬁcult to optimize due
to the gradient vanishing/exploding problem . We note that,
despite the signiﬁcant development effort, simply
stacking more layers cannot beneﬁt the system and
leads to a disaster of training in some of our experiments.
A promising attempt to address this issue is
Bapna et al. ’s work. They trained a 16layer Transformer encoder by using an enhanced
attention model. In this work, we continue the line
of research and go towards a much deeper encoder
for Transformer. We choose encoders to study because they have a greater impact on performance
than decoders and require less computational cost
 . Our contributions are threefold:
• We show that the proper use of layer normalization is the key to learning deep encoders.
The deep network of the encoder can be
optimized smoothly by relocating the layer
normalization unit.
While the location of
layer normalization has been discussed in recent systems , as far as we know,
its impact has not been studied in deep Trans-
2For example, a standard Transformer encoder has 6 layers. Each of them consists of two sub-layers. More sub-layers
are involved on the decoder side.
 
(a) post-norm residual unit
(b) pre-norm residual unit
Figure 1: Examples of pre-norm residual unit and postnorm residual unit. F = sub-layer, and LN = layer normalization.
• Inspired by the linear multi-step method
in numerical analysis , we propose an approach based on dynamic linear combination of layers (DLCL)
to memorizing the features extracted from all
preceding layers. This overcomes the problem with the standard residual network where
a residual connection just relies on the output
of one-layer ahead and may forget the earlier
• We successfully train a 30-layer encoder, far
surpassing the deepest encoder reported so
far . To our best knowledge, this is the deepest encoder used in
English-German,
Chinese-English,
Chinese-English
translation
we show that our deep system (30/25-layer
encoder) yields a BLEU improvement of 1.3∼2.4
points over the base model (Transformer-Base
with 6 layers). It even outperforms Transformer-
Big by 0.4∼0.6 BLEU points, but requires 1.6X
fewer model parameters and 3X less training time.
More interestingly, our deep model is 10% faster
than Transformer-Big in inference speed.
Post-Norm and Pre-Norm Transformer
The Transformer system and its variants follow the
standard encoder-decoder paradigm. On the encoder side, there are a number of identical stacked
Each of them is composed of a selfattention sub-layer and a feed-forward sub-layer.
The attention model used in Transformer is multihead attention, and its output is fed into a fully
connected feed-forward network.
Likewise, the
decoder has another stack of identical layers. It
has an encoder-decoder attention sub-layer in addition to the two sub-layers used in each encoder
layer. In general, because the encoder and the decoder share a similar architecture, we can use the
same method to improve them. In the section, we
discuss a more general case, not limited to the encoder or the decoder.
Model Layout
For Transformer, it is not easy to train stacked layers on neither the encoder-side nor the decoderside. Stacking all these sub-layers prevents the ef-
ﬁcient information ﬂow through the network, and
probably leads to the failure of training. Residual
connections and layer normalization are adopted
for a solution. Let F be a sub-layer in encoder or
decoder, and θl be the parameters of the sub-layer.
A residual unit is deﬁned to be :
xl + F(xl; θl)
where xl and xl+1 are the input and output of the
l-th sub-layer, and yl is the intermediate output followed by the post-processing function f(·). In this
way, xl is explicitly exposed to yl (see Eq. (2)).
Moreover, layer normalization is adopted to reduce the variance of sub-layer output because hidden state dynamics occasionally causes a much
longer training time for convergence. There are
two ways to incorporate layer normalization into
the residual network.
• Post-Norm. In early versions of Transformer
 , layer normalization is
placed after the element-wise residual addition (see Figure 1(a)), like this:
xl+1 = LN(xl + F(xl; θl))
where LN(·) is the layer normalization function, whose parameter is dropped for simplicity. It can be seen as a post-processing step of
the output (i.e., f(x) = LN(x)).
• Pre-Norm. In recent implementations , layer normalization is applied to the
input of every sub-layer (see Figure 1(b)):
xl+1 = xl + F(LN(xl); θl)
Eq. (4) regards layer normalization as a part
of the sub-layer, and does nothing for postprocessing of the residual connection (i.e.,
f(x) = x).3
Both of these methods are good choices for implementation of Transformer. In our experiments,
they show comparable performance in BLEU for a
system based on a 6-layer encoder (Section 5.1).
On the Importance of Pre-Norm for Deep
Residual Network
The situation is quite different when we switch to
deeper models. More speciﬁcally, we ﬁnd that prenorm is more efﬁcient for training than post-norm
if the model goes deeper. This can be explained by
seeing back-propagation which is the core process
to obtain gradients for parameter update. Here we
take a stack of L sub-layers as an example. Let
E be the loss used to measure how many errors
occur in system prediction, and xL be the output
of the topmost sub-layer. For post-norm Transformer, given a sub-layer l, the differential of E
with respect to xl can be computed by the chain
rule, and we have
1 + ∂F(xk; θk)
where QL−1
means the backward pass of
the layer normalization, and QL−1
k=l (1+ ∂F(xk;θk)
means the backward pass of the sub-layer with the
residual connection. Likewise, we have the gradient for pre-norm 4:
∂F(LN(xk); θk)
Obviously, Eq. (6) establishes a direct way to
pass error gradient
∂xL from top to bottom. Its
merit lies in that the number of product items on
the right side does not depend on the depth of the
In contrast, Eq. (5) is inefﬁcient for passing gradients back because the residual connection is not
3We need to add an additional function of layer normalization to the top layer to prevent the excessively increased
value caused by the sum of unnormalized output.
4For a detailed derivation, we refer the reader to Appendix
a bypass of the layer normalization unit (see Figure 1(a)).
Instead, gradients have to be passed
through LN(·) of each sub-layer. It in turn introduces term QL−1
into the right hand side
of Eq. (5), and poses a higher risk of gradient vanishing or exploring if L goes larger. This was con-
ﬁrmed by our experiments in which we successfully trained a pre-norm Transformer system with
a 20-layer encoder on the WMT English-German
task, whereas the post-norm Transformer system
failed to train for a deeper encoder (Section 5.1).
Dynamic Linear Combination of
The residual network is the most common approach to learning deep networks, and plays an
important role in Transformer. In principle, residual networks can be seen as instances of the ordinary differential equation (ODE), behaving like
the forward Euler discretization with an initial
value . Euler’s method is probably the most popular ﬁrstorder solution to ODE. But it is not yet accurate enough. A possible reason is that only one
previous step is used to predict the current value
5 . In MT, the single-step property
of the residual network makes the model “forget”
distant layers . As a result,
there is no easy access to features extracted from
lower-level layers if the model is very deep.
Here, we describe a model which makes direct links with all previous layers and offers ef-
ﬁcient access to lower-level representations in a
deep stack. We call it dynamic linear combination of layers (DLCL). The design is inspired by
the linear multi-step method (LMM) in numerical
ODE . Unlike Euler’s
method, LMM can effectively reuse the information in the previous steps by linear combination to
achieve a higher order. Let {y0, ..., yl} be the output of layers 0 ∼l. The input of layer l + 1 is
deﬁned to be
xl+1 = G(y0, . . . , yl)
where G(·) is a linear function that merges previously generated values {y0, ..., yl} into a new
value. For pre-norm Transformer, we deﬁne G(·)
5Some of the other single-step methods, e.g. the Runge-
Kutta method, can obtain a higher order by taking several
intermediate steps . Higher order generally
means more accurate.
y0 y1 y2 y3
y0 y1 y2 y3
y0 y1 y2 y3
y0 y1 y2 y3
Figure 2: Connection weights for 3-layer encoder: (a) residual connection , (b) dense residual connection , (c) multi-layer representation fusion /transparent
attention and (d) our approach. y0 denotes the input embedding. Red denotes the weights are
learned by model.
G(y0, . . . , yl) =
where W l+1
∈R is a learnable scalar and weights
each incoming layer in a linear manner. Eq. (8)
provides a way to learn preference of layers in different levels of the stack. Even for the same incoming layer, its contribution to succeeding layers
could be different (e.g. W i
k ) . Also, the
method is applicable to the post-norm Transformer
model. For post-norm, G(·) can be redeﬁned as:
G(y0, . . . , yl) = LN
Comparison to LMM. DLCL differs from LMM
in two aspects, though their fundamental model is
the same. First, DLCL learns weights in an endto-end fashion rather than assigning their values
deterministically, e.g. by polynomial interpolation.
This offers a more ﬂexible way to control the model behavior. Second, DLCL has an
arbitrary size of the past history window, while
LMM generally takes a limited history into account . Also, recent work shows
successful applications of LMM in computer vision, but only two previous steps are used in their
LMM-like system .
Comparison to existing neural methods. Note
that DLCL is a very general approach. For example, the standard residual network is a special
case of DLCL, where W l+1
= 1, and W l+1
for k < l. Figure (2) compares different methods of connecting a 3-layer network. We see that
the densely residual network is a fully-connected
network with a uniform weighting schema . Multi-layer representation fusion and transparent attention (call it TA) 
methods can learn a weighted model to fuse layers but they are applied to the topmost layer only.
The DLCL model can cover all these methods. It
provides ways of weighting and connecting layers in the entire stack.
We emphasize that although the idea of weighting the encoder layers
by a learnable scalar is similar to TA, there are
two key differences: 1) Our method encourages
earlier interactions between layers during the encoding process, while the encoder layers in TA
are combined until the standard encoding process
is over; 2) For an encoder layer, instead of learning a unique weight for each decoder layer like
TA, we make a separate weight for each successive encoder layers. In this way, we can create
more connections between layers6.
Experimental Setup
We ﬁrst evaluated our approach on WMT’16
English-German (En-De) and NIST’12 Chinese-
English (Zh-En-Small) benchmarks respectively.
To make the results more convincing, we also experimented on a larger WMT’18 Chinese-English
dataset (Zh-En-Large) with data augmentation by
back-translation .
Datasets and Evaluation
For the En-De task, to compare with Vaswani
et al. ’s work, we use the same 4.5M preprocessed data 7, which has been tokenized and
6Let the encoder depth be M and the decoder depth be
N (M > N for a deep encoder model). Then TA newly
adds O(M × N) connections, which are fewer than ours of
7 
download&id=0B_bZck-ksdkpM25jRUN2X2UxMm8
Vaswani et al. (Base)
Bapna et al. -deep (Base, 16L)
Vaswani et al. (Big)
Chen et al. (Big)
He et al. (Big)
Shaw et al. (Big)
Dou et al. (Big)
Ott et al. (Big)
Transformer (Base)
Transformer (Big)
Transformer-deep (Base, 20L)
DLCL (Base)
DLCL-deep (Base, 25L)
Transformer (Base)
Transformer (Big)
Transformer-deep (Base, 20L)
DLCL (Base)
DLCL-deep (Base, 30L)
Table 1: BLEU scores [%] on English-German translation. Batch indicates the corresponding batch size if
running on 8 GPUs. Times ∝Batch×Updates, which can be used to approximately measure the required
training time. † denotes an estimate value. Note that “-deep” represents the best-achieved result as depth changes.
jointly byte pair encoded (BPE) with 32k merge operations using a shared
vocabulary 8. We use newstest2013 for validation
and newstest2014 for test.
For the Zh-En-Small task, we use parts of the
bitext provided within NIST’12 OpenMT9. We
choose NIST MT06 as the validation set, and
MT04, MT05, MT08 as the test sets. All the sentences are word segmented by the tool provided
within NiuTrans . We remove
the sentences longer than 100 and end up with
about 1.9M sentence pairs. Then BPE with 32k
operations is used for both sides independently,
resulting in a 44k Chinese vocabulary and a 33k
English vocabulary respectively.
For the Zh-En-Large task, we use exactly the
same 16.5M dataset as Wang et al. ,
composing of 7.2M-sentence CWMT corpus,
4.2M-sentence UN and News-Commentary combined corpus, and back-translation of 5M-sentence
monolingual data from NewsCraw2017. We refer
the reader to Wang et al. for the details.
8The tokens with frequencies less than 5 are ﬁltered out
from the shared vocabulary.
9LDC2000T46,
LDC2000T47,
LDC2000T50,
LDC2003E14, LDC2005T10, LDC2002E18, LDC2007T09,
LDC2004T08
For evaluation, we ﬁrst average the last 5 checkpoints, each of which is saved at the end of an
epoch. And then we use beam search with a beam
size of 4/6 and length penalty of 0.6/1.0 for En-
De/Zh-En tasks respectively. We measure casesensitive/insensitive tokenized BLEU by multibleu.perl for En-De and Zh-En-Small respectively, while case-sensitive detokenized BLEU is
reported by the ofﬁcial evaluation script mtevalv13a.pl for Zh-En-Large. Unless noted otherwise
we run each experiment three times with different
random seeds and report the mean of the BLEU
scores across runs10.
Model and Hyperparameters
All experiments run on fairseq-py11 with 8
NVIDIA Titan V GPUs. For the post-norm Transformer baseline, we replicate the model setup of
Vaswani et al. . All models are optimized
by Adam with β1 = 0.9,
β2 = 0.98, and ϵ = 10−8.
In training warmup
(warmup = 4000 steps), the learning rate linearly
increases from 10−7 to lr =7×10−4/5×10−4 for
10Due to resource constraints, all experiments on Zh-En-
Large task only run once.
11 
Model (Base, 16L)
Bapna et al. 
Transformer
Transformer
Compare with Bapna et al. on
WMT’16 English-German translation under a 16-layer
Transformer-Base/Big respectively, after which it
is decayed proportionally to the inverse square
root of the current step. Label smoothing εls=0.1
is used as regularization.
For the pre-norm Transformer baseline, we follow the setting as suggested in tensor2tensor12.
More speciﬁcally, the attention dropout Patt = 0.1
and feed-forward dropout Pff = 0.1 are additionally added. And some hyper-parameters for optimization are changed accordingly: β2 = 0.997,
warmup = 8000 and lr = 10−3/7×10−4 for
Transformer-Base/Big respectively.
For both the post-norm and pre-norm baselines,
we batch sentence pairs by approximate length
and restrict input and output tokens per batch
to batch = 4096 per GPU. We set the update
steps according to corresponding data sizes. More
speciﬁcally, the Transformer-Base/Big is updated
for 100k/300k steps on the En-De task as Vaswani
et al. , 50k/100k steps on the Zh-En-Small
task, and 200k/500k steps on the Zh-En-Large
In our model, we use the dynamic linear combination of layers for both encoder and decoder. For
efﬁcient computation, we only combine the output of a complete layer rather than a sub-layer. It
should be noted that for deep models (e.g. L ≥
20), it is hard to handle a full batch in a single GPU
due to memory size limitation. We solve this issue
by accumulating gradients from two small batches
 before each update . In our primitive experiments, we observed
that training with larger batches and learning rates
worked well for deep models. Therefore all the results of deep models are reported with batch =
8192, lr = 2×10−3 and warmup = 16,000 unless
otherwise stated. For fairness, we only use half of
the updates of baseline (e.g. update = 50k) to
ensure the same amount of data that we actually
12 
tensor2tensor
see in training. We report the details in Appendix
Results on the En-De Task
In Table 1, we ﬁrst report results on WMT En-De
where we compare to the existing systems based
on self-attention. Obviously, while almost all previous results based on Transformer-Big (marked
by Big) have higher BLEU than those based on
Transformer-Base (marked by Base), larger parameter size and longer training epochs are required.
As for our approach, considering the post-norm
case ﬁrst, we can see that our Transformer baselines are superior to Vaswani et al. in both
Base and Big cases. When increasing the encoder depth, e.g. L = 20, the vanilla Transformer
failed to train, which is consistent with Bapna et al.
 . We attribute it to the vanishing gradient
problem based on the observation that the gradient norm in the low layers (e.g. embedding layer)
approaches 0. On the contrary, post-norm DLCL
solves this issue and achieves the best result when
The situation changes when switching to prenorm. While it slightly underperforms the postnorm counterpart in shallow networks, pre-norm
Transformer beneﬁts more from the increase in encoder depth.
More concretely, pre-norm Transformer achieves optimal result when L=20 (see
Figure 3(a)), outperforming the 6-layer baseline
by 1.8 BLEU points. It indicates that pre-norm
is easier to optimize than post-norm in deep networks. Beyond that, we successfully train a 30layer encoder by our method, resulting in a further improvement of 0.4 BLEU points.
is 0.6 BLEU points higher than the pre-norm
Transformer-Big. It should be noted that although
our best score of 29.3 is the same as Ott et al.
 , our approach only requires 3.5X fewer
training epochs than theirs.
To fairly compare with transparent attention
(TA) , we separately list the
results using a 16-layer encoder in Table 2.
can be seen that pre-norm Transformer obtains the
same BLEU score as TA without the requirement
of complicated attention design. However, DLCL
in both post-norm and pre-norm cases outperform
TA. It should be worth that TA achieves the best
result when encoder depth is 16, while we can fur-
Model (pre-norm)
Transformer (Base)
Transformer (Big)
Transformer-deep (Base, 25L)
DLCL (Base)
DLCL-deep (Base, 25L)
Table 3: BLEU scores [%] on NIST’12 Chinese-English translation.
newstest17
newstest18
Wang et al. (post-norm, Base)
pre-norm Transformer (Base)
pre-norm Transformer (Big)
pre-norm DLCL-deep (Base, 25L)
pre-norm DLCL-deep (Base, 30L)
Table 4: BLEU scores [%] on WMT’18 Chinese-English translation.
Transformer
1620 25 30 35
BLEU Score
(a) WMT En-De
16 20 25 30
BLEU Score
(b) NIST Zh-En
Figure 3: BLEU scores [%] against the encoder depth
for pre-norm Transformer and pre-norm DLCL on
English-German and Chinese-English tasks.
ther improve performance by training deeper encoders.
Results on the Zh-En-Small Task
Seen from the En-De task, pre-norm is more effective than the post-norm counterpart in deep networks. Therefore we evaluate our method in the
case of pre-norm on the Zh-En task. As shown
in Table 3, ﬁrstly DLCL is superior to the baseline when the network’s depth is shallow. Interestingly, both Transformer and DLCL achieve the best
results when we use a 25-layer encoder. The 25layer Transformer can approach the performance
of Transformer-Big, while our deep model outperforms it by about 0.5 BLEU points under the
equivalent parameter size.
It conﬁrms that our
approach is a good alternative to Transformer no
matter how deep it is.
Results on the Zh-En-Large Task
While deep Transformer models, in particular
the deep pre-norm DLCL, show better results
Figure 4: GPU generation speed (target tokens/sec.)
against the depth of encoder for pre-norm DLCL on
English-German task (batch size = 32, beam size = 4).
than Transformer-Big on En-De and Zh-En-Small
tasks, both data sets are relatively small, and
the improved performance over Transformer-Big
might be partially due to over-ﬁtting in the wider
model. For a more challenging task , we report
the results on Zh-En-Large task in Table 4. We
can see that the 25-layer pre-norm DLCL slightly
surpassed Transformer-Big, and the superiority is
bigger when using a 30-layer encoder. This result
indicates that the claiming of the deep network defeating Transformer-Big is established and is not
affected by the size of the data set.
Effect of Encoder Depth
In Figure 3, we plot BLEU score as a function
of encoder depth for pre-norm Transformer and
DLCL on En-De and Zh-En-Small tasks. First of
all, both methods beneﬁt from an increase in encoder depth at the beginning. Remarkably, when
the encoder depth reaches 20, both of the two deep
models can achieve comparable performance to
Transformer-Big, and even exceed it when the encoder depth is further increased in DLCL. Note that
pre-norm Transformer degenerates earlier and is
less robust than DLCL when the depth is beyond
20. However, a deeper network (>30 layers) does
not bring more beneﬁts. Worse still, deeper networks consume a lot of memory, making it impossible to train efﬁciently.
We also report the inference speed on GPU in
Figure 4. As expected, the speed decreases linearly with the number of encoder layers. Nevertheless, our system with a 30-layer encoder is still
faster than Transformer-Big, because the encoding
process is independent of beam size, and runs only
once. In contrast, the decoder suffers from severe
autoregressive problems.
Effect of Decoder Depth
Enc. Depth
Dec. Depth
Table 5: Tokenized BLEU scores [%] and GPU generation speed (target tokens per second) in pre-norm
Transformer (Base) on the test set of WMT English-
German (batch size = 32, beam size = 4).
Table 5 shows the effects of decoder depth on
BLEU and inference speed on GPU. Different
from encoder, increasing the depth of decoder only
yields a slight BLEU improvement, but the cost is
high: for every two layers added, the translation
speed drops by approximate 500 tokens evenly.
It indicates that exploring deep encoders may be
more promising than deep decoders for NMT.
Ablation Study
We report the ablation study results in Table 6. We
ﬁrst observe a modest decrease when removing the
introduced layer normalization in Eq. (8). Then
we try two methods to replace learnable weights
with constant weights: All-One (W i
j = 1) and Average (W i
j = 1/(i+1)). We can see that these two
methods consistently hurt performance, in particular in the case of All-One. It indicates that making
the weights learnable is important for our model.
Moreover, removing the added layer normalization in the Average model makes BLEU score drop
by 0.28, which suggests that adding layer normalization helps more if we use the constant weights.
In addition, we did two interesting experiments on
big models. The ﬁrst one is to replace the base en-
pre-norm DLCL-20L
- layer norm.
- learnable weight (ﬁx 1)
- learnable weight (ﬁx 1/N)
- layer norm.
pre-norm Transformer-Base
+ big encoder
pre-norm Transformer-Big
+ 12-layer encoder (DLCL)
Table 6: Ablation results by tokenized BLEU [%] on
the test set of WMT English-German translation.
coder with a big encoder in pre-norm Transformer-
Base. The other one is to use DLCL to train a
deep-and-wide Transformer (12 layers). Although
both of them beneﬁt from the increased network
capacity, the gain is less than the “thin” counterpart in terms of BLEU, parameter size, and training efﬁciency.
Visualization on Learned Weights
We visually present the learned weights matrices of the 30-layer encoder (Figure 5(a)) and its
6-layer decoder (Figure 5(b)) in our pre-norm
DLCL-30L model on En-De task. For a clearer
contrast, we mask out the points with an absolute
value of less than 0.1 or 5% of the maximum per
row. We can see that the connections in the early
layers are dense, but become sparse as the depth
increases. It indicates that making full use of earlier layers is necessary due to insufﬁcient information at the beginning of the network. Also, we ﬁnd
that most of the large weight values concentrate on
the right of the matrix, which indicates that the impact of the incoming layer is usually related to the
distance between the outgoing layer. Moreover,
for a ﬁxed layer’s output yi, it is obvious that its
contribution to successive layers changes dynamically (one column). To be clear, we extract the
weights of y10 in Figure 5(c). In contrast, in most
previous paradigms of dense residual connection,
the output of each layer remains ﬁxed for subsequent layers.
Related Work
Deep Models.
Deep models have been explored in the context of neural machine translation since the emergence of RNN-based models.
To ease optimization, researchers tried to reduce
the number of non-linear transitions (Zhou et al.,
(b) 6-layer decoder of DLCL
(a) 30-layer encoder of DLCL
(c) Weight distribution of y10 in the encoder
Figure 5: A visualization example of learned weights in our 30-layer pre-norm DLCL model.
2016; Wang et al., 2017). But these attempts are
limited to the RNN architecture and may not be
straightforwardly applicable to the current Transformer model. Perhaps, the most relevant work
to what is doing here is Bapna et al. ’s
work. They pointed out that vanilla Transformer
was hard to train if the depth of the encoder was
beyond 12. They successfully trained a 16-layer
Transformer encoder by attending the combination of all encoder layers to the decoder.
their approach, the encoder layers are combined
just after the encoding is completed, but not during the encoding process. In contrast, our approach allows the encoder layers to interact earlier, which has been proven to be effective in machine translation and text match
 . In addition to machine translation, deep Transformer encoders are also used for
language modeling . For example, Al-Rfou et al. 
trained a character language model with a 64layer Transformer encoder by resorting to auxiliary losses in intermediate layers. This method
is orthogonal to our DLCL method, though it is
used for language modeling, which is not a very
heavy task.
Densely Residual Connections.
residual connections are not new in NMT. They
have been studied for different architectures, e.g.,
RNN and Transformer . Some of the previous studies ﬁx
the weight of each layer to a constant, while
others learn a weight distribution by using either the self-attention model 
or a softmax-normalized learnable vector . They focus more on learning connections from lower-level layers to the topmost
layer. Instead, we introduce additional connectivity into the network and learn more densely connections for each layer in an end-to-end fashion.
Conclusion
We have studied deep encoders in Transformer.
We have shown that the deep Transformer models
can be easily optimized by proper use of layer normalization, and have explained the reason behind
it. Moreover, we proposed an approach based on
a dynamic linear combination of layers and successfully trained a 30-layer Transformer system.
It is the deepest encoder used in NMT so far. Experimental results show that our thin-but-deep encoder can match or surpass the performance of
Transformer-Big.
Also, its model size is 1.6X
smaller. In addition, it requires 3X fewer training
epochs and is 10% faster for inference.
Acknowledgements
This work was supported in part by the National
Natural Science Foundation of China (Grant Nos.
61876035, 61732005, 61432013 and 61672555),
the Fundamental Research Funds for the Central Universities (Grant No.
N181602013),
the Joint Project of FDCT-NSFC , the MYRG from the University of
Macau (Grant No. MYRG2017-00087-FST).