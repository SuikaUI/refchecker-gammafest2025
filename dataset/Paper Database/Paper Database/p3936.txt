IMPROVING EVENT DETECTION FOR AUDIO SURVEILLANCE USING GABOR
FILTERBANK FEATURES
J¨urgen T. Geiger and Karim Helwani
 
Huawei European Research Center, Munich, Germany
Acoustic event detection in surveillance scenarios is an important but difﬁcult problem. Realistic systems are struggling
with noisy recording conditions. In this work, we propose to
use Gabor ﬁlterbank features to detect target events in different noisy background scenes. These features capture spectrotemporal modulation frequencies in the signal, which makes
them suited for the detection of non-stationary sound events.
A single-class detector is constructed for each of the different
target events. In a hierarchical framework, the separate detectors are combined to a multi-class detector. Experiments are
performed using a database of four different target sounds and
four background scenarios. On average, the proposed features
outperform conventional features in all tested noise levels, in
terms of detection and classiﬁcation performance.
Index Terms— Audio surveillance, event detection, Gabor features, noise robustness
1. INTRODUCTION
Automatic surveillance systems are becoming more and more
ubiquitous in public spaces. Audio analysis can complement
video-based systems, which are exposed to several vulnerabilities, such as occlusions. Systems that analyse audio signals can successfully be combined with video solutions, or
used in a stand-alone manner . Relevant tasks that can be
solved by audio analysis are abnormal event detection (such
as gunshots or explosions) and classiﬁcation, as well as source
localisation and tracking. The problems that an audio analysis system has to face include high amounts of non-stationary
background noise and a strong diversity of potential interesting sound events.
This paper deals with sound event detection in highly
realistic noisy environments.
Several previous studies addressed the problem of detecting and classifying acoustic
events such as gunshots, explosions, or screams. Most of the
proposed systems rely on the traditional approach of modelling Mel-frequency cepstral coefﬁcient (MFCC) features
with Gaussian mixture models (GMMs) or hidden Markov
The research leading to these results has received funding from the European Commision Union Seventh Framework Programme 
under grant agreement 607480 LASIE.
models (HMMs) and explore different system setups or different additional audio features. Our work goes in the same
direction, with the goal of creating a robust system that can
operate in realistic environment.
1.1. Related Work
Over the last years, several studies evaluated systems for
event detection in surveillance scenarios. Several of the proposed systems use classical spectral features in a GMM or
HMM framework. In , six sound event classes (including
human screams, explosions, and gunshots) are detected with a
median ﬁlter and classiﬁed using linear spectral band features
and either a GMM or HMM classiﬁer. The system showed
solid recognition rates in white and musical background
noise. Clavel et al. used MFCCs and other spectral features
(spectral centroid and spread) together with a GMM classiﬁer
to detect gunshot sounds in recordings of public places .
A similar system is used in to detect scream and gunshot
sounds, and small improvements were obtained by adding
more features, most notably spectral distribution features
(e. g. spectral slope or spectral roll-off) and correlation-based
features. A two-stage approach is proposed in : an audio
signal is ﬁrst classiﬁed as normal or abnormal, followed by
a maximum-likelihood classiﬁcation to determine the class.
This work relies again on MFCC features and an HMM
classiﬁer. In , different gunshot detection algorithms are
compared, with the conclusion that correlation and waveletbased detection algorithms give higher performance. A bag
of aural words classiﬁer was used in to classify acoustic
events in surveillance scenarios. In , wavelet features are
proposed for environmental sound classiﬁcation. The general
problem of event detection in surveillance scenarios is that
almost no realistic databases are available. In all of the mentioned studies, databases were created by mixing target sound
events into background recordings. Furthermore, most of the
previous studies rely on techniques that were originally designed for speech processing. There is still a lack of features
and classiﬁcation models that are speciﬁcally tailored to the
underlying problem.
Acoustic event detection systems are also used in other
environments. In the CLEAR and D-CASE evaluations, the goal was to detect acoustic events in a domenstic environment. In , acoustic event detection was per-
23rd European Signal Processing Conference (EUSIPCO)
978-0-9928626-3-3/15/$31.00 ©2015 IEEE
formed on real-life recordings. The UrbanSound dataset, another database of real-life recordings is described in . An
interesting approach for event detection that goes beyond the
classical spectral features are the spectrogram image features
proposed by Dennis et al. . Relevant information is extracted by regarding the spectrogram as an image. These features achieved good results in the similar problem of noiserobust acoustic event classiﬁcation.
1.2. Contributions
The goal of the present study is to construct a noise-robust
event detection system for surveillance scenarios. While most
of the previous studies in the ﬁeld rely on classical spectral
and cepstral audio features (mostly MFCCs), we investigate
the suitability of a Gabor ﬁlterbank feature set.
The employed Gabor ﬁlterbank features are physiologically inspired
and were originally proposed for noise-robust speech recognition . These features extract spectro-temporal modulation frequencies from the signal by ﬁltering the Mel spectrogram with different Gabor ﬁlters. The use of such features
is motivated by the ﬁnding that a similar processing is performed in the primary auditory cortex of mammals . In
a recent challenge for acoustic event detection in an ofﬁce
environment, these features achieved a good detection performance .
Acoustic events are modelled with GMMs, and singleclass detectors for noisy environments are created. A hierarchical system setup is used to distinguish between different
event classes, in order to arrive at multi-class detection system. Experiments are carried out using recordings of breaking
glass, explosion, gunshot, and scream sounds. Target sounds
were mixed into realistic background scene recordings. The
experimental evaluations reveal that the proposed GBFB features achieve better results than MFCC features, in terms of
event detection and classiﬁcation.
The rest of the paper is organised as follows. In Section 2,
the framework of the event detection system is delineated.
The employed audio features are described in detail in Section 3. Experimental results are presented in Section 4, followed by some conclusions in Section 5.
2. EVENT DETECTION SYSTEM
The proposed event detection system is composed of singleevent detectors. For each target event, a detector is trained.
Each event detector consists of a two-class GMM classiﬁer,
one model, θ1, for the target event and one, θ2, for the background noise. The GMMs are trained with diagonal covariances, and the number of mixture components is ﬁxed to 16
(following preliminary experiments). For a given unknown
sample X = x1, . . . , xT , where T is the length in frames, the
log-likelihood for both models
Li = log P(X|θi), i ∈{1, 2}
is evaluated. The log-likelihoods are used to derive a detection score
φ = L1 −L2.
Together with a threshold, this score can be converted to a
detection decision.
The same detection framework can be used for singleevent and multi-event detection. For single-event detection,
a detection score is obtained as described above. In order
to perform multi-event detection, a hierarchical system setup
is used. After obtaining the scores for each single-event detector, maximum-likelihood classiﬁcation between all target
event models is performed to obtain one result.
The given problem of event detection in surveillance scenarios differs from other acoustic event detection scenarios.
In most other event detection scenarios, precise timing information is important, regarding the onset and offset of events.
On the other hand, in surveillance scenarios, exact timing (in
the order of several frames) is not required. Therefore, we
evaluate the system with pre-segmented recordings, instead
of detecting events in longer background recordings.
3. AUDIO FEATURES
As a baseline, MFCCs are used as features. In previous studies about event detection for acoustic surveillance, MFCCs
achieved a good performance in combination with GMM
classiﬁers.
13 MFCC coefﬁcients are computed for each
frame of 25 ms length (frame shift 10 ms). Together with delta
and delta-delta coefﬁcients, this results in a 39-dimensional
feature vector per frame.
As an alternative, we propose to use Gabor ﬁlterbank
(GBFB) features.
This feature set models the spectrotemporal modulation frequencies in the signal and it was
recently proposed for noise-robust speech recognition .
The selection of features based on Gabor ﬁlterbank is motivated by the fact that it provides a systematic approach to
describe the spectro-temporal characteristics of a signal and
it offers the beneﬁts of the wavelet analysis by analysing the
signal at different scales. This results in an optimal timefrequency localization . Therefore, global as well as localised characteristics of the temporal as well as the spectral structure of the signal can be gathered. Since the Gabor ﬁlterbank is deﬁned in the spectro-temporal domain, it
can be adjusted to capture features in the time domain only,
the spectral domain or features related to dependencies of the
spectral excitation with respect to the time. This is useful to
characterise many sound events where the frequency excitation structure follows a speciﬁc chronology. For example, a
detonation causes shock waves which have speciﬁc spectrotemporal structure caused by increased pressure of the air, its
local temperature and the local speed of sound. Hence, in
an initially plane sinusoidal wave of a single frequency, the
peaks of the wave travel faster than the troughs, and the pulse
becomes cumulatively more like a sawtooth wave . As
23rd European Signal Processing Conference (EUSIPCO)
another example, although the sound of a gun shot depends
on the gun, however, the evolvement of the excited frequencies has similar time dependency for the majority of guns.
To extract GFB features, ﬁrst, the log Mel-spectrum of a
signal (25 ms frames with 10 ms frame shift) is computed.
This spectrum is ﬁltered by a Gabor ﬁlterbank. Each Gabor
ﬁlter is deﬁned as the product of a 2-dimensional sinusoid carrier (3) with corresponding temporal modulation frequency
ωk and spectral modulation frequency ωn, and an envelope
function (4):
sω(x) = exp(iωx),
0.5 −0.5 cos
The parameter b controls the width of the carrier function.
Each of the sinusoid carriers corresponds to a speciﬁc temporal and spectral modulation frequency. The maximum size
of the ﬁlters is limited to 69 frequency channel and 40 time
frames. The ﬁlterbank is designed to consist of 41 Gabor ﬁlters (with different temporal and spectral modulation frequencies). Each of these ﬁlters can be applied to each of the 23
frequency channels. From the 943 possible combinations, a
number of representative channels is selected. This reduces
the ﬁlterbank output to 311 dimensions. These settings correspond to the original deﬁnition of the GBFB features and
are used throughout the present work.
Figure 1 illustrates the log Mel-spectrogram and the output of one Gabor ﬁlter for two exemplary recordings of the
classes breaking glass and gunshot.
The breaking glass
recording has only few low-frequency components, while
the gunshot recording reveals a considerable amount of lowfrequency components. In addition to the spectrograms, the
output of the Gabor ﬁlter corresponding to a spectral modulation frequency of 0.06 cycles per channel and a temporal
modulation frequency of 6.2 Hz is shown in order to illustrate the extracted features. Considerable differences between
the two different classes are visible in the ﬁgure in terms of
the spectral distribution, as well as characteristic properties
within the recording of the same class.
Applying the 2-dimensional Gabor ﬁlterbank can also be
understood as an image ﬁltering process on the spectrogram.
With the spectro-temporal extent of the ﬁlters, spectral and
temporal context is incorporated in the resulting features.
Spectral modulation frequencies of up to 0.25 cycles per
channel and temporal modulation frequencies of up to 25
Hz are captured with the ﬁlterbank. Exploiting this information seems to be promising for the task of event detection
in surveillance scenarios, since the target sounds are not
assumed to be stationary.
In order to compare GBFB features to MFCCs, a principal
component analysis is applied to reduce the dimensionality of
the GBFB features to 39. As a consequence, the same model
order can be applied for both feature sets. In the experiments,
Breaking glass
Fig. 1. log Mel-spectrogram (top) and output for one Gabor
ﬁlter (bottom) for two recordings. The x-axis represents the
time in s and the y-axis the Mel-frequency channels.
the PCA basis is always computed from the training data, and
test data are projected onto this basis.
4. EXPERIMENTS
4.1. Database
There are no standard publicly available databases for acoustic surveillance scenarios. As in most of the previous studies
on event detection for audio surveillance, we created our
own evaluation database. Different classes of target sounds
were mixed into realistic background recordings at various
signal-to-noise ratios (SNRs). As target sounds, we considered the four classes breaking glass, explosion, gunshot, and
scream. For each of these classes, we collected 100 samples
from the public repository www.findsounds.com. Background sounds were chosen from the database of acoustic
scenes from the D-CASE challenge . The classes busystreet, openairmarket, park, and tubestation were selected
as potential scenarios for audio surveillance. Although the
background recordings in the D-CASE database are available
as binaural recordings, only the left channel was used, to
simulate a simple, realistic single-microphone setup. Target
sounds were mixed into background recordings at different
SNR values from 20 dB to 0 dB, in steps of 5 dB. The detection models are trained with matched noise settings, i. e., for
each background noise, a separate model is trained.
In order to provide negative samples for the detection
experiments, consisting only of background sounds, extracts
are cut from the background recordings. It was found that
the length of the target sounds follows a Gamma distribution.
Background samples were extracted with a length randomly
23rd European Signal Processing Conference (EUSIPCO)
Table 1. Event detection equal error rate, comparing MFCC
and GBFB features
drawn from a Gamma distribution with shape and scale parameters adjusted to the length distribution of the target
In total, the created database consists of 8 800 samples:
four target classes with 100 samples each, mixed with four
different backgrounds at ﬁve SNR values, together with the
clean recordings; furthermore, four background classes with
100 samples each. The database is divided into a training
set (60 of each of the 100 samples) and a test set (the other
40 samples). For training, only clean recordings of the target events as well as background recordings are used, while
tests are performed for all SNR values in addition to the clean
recordings.
4.2. Event Detection
Firstly, event detection is evaluated separately for each class
of acoustic event. The task is to detect target events in a background recording. Therefore, evaluation can be carried out in
terms of false detections and false rejections. For each recording, an event detector yields a detection score, which can be
used, together with a detection threshold, to trade off false
detections and false rejections. Results for different detection
threshold can be plotted in a detection error trade-off (DET)
curve. The equal error rate (EER) is used as a universal performance measure in this work. It is deﬁned as the operating
point with equal false detection and false rejection rates.
Detection experiments are performed separately for each
of the backgrounds, in matched conditions. This means that
the detector is trained and tested with the same background
class. The results can be averaged over all backgrounds and
over all target sounds, to obtain one averaged EER per SNR
value. Table 1 shows these results, for MFCC and GBFB features. GBFB features achieve better results than MFCCs for
all SNR values. On average, using GBFB instead of MFCC
leads to a relative performance improvement of 13 %.
Table 2 reports results (for GBFB features) separately for
each of the target classes. As could be expected, breaking
glass and scream are easier to detect, since for the other two
classes, confusions with background sounds are more likely
because of the low-frequency noise-like structure.
Results for the separate background classes are given in
Table 3. The worst results are obtained with the background
class openairmarket. This class contains a wide range of diverse sounds, which could lead to the relatively high error
Table 2. Event detection results (EER) separately for each
target event, using GBFB features
Table 3. Event detection results (EER) separately for each
background class, using GBFB features
rates. For the background class park, the error rates are also
relatively high. On average, the park recordings are relatively
quiet, which means that they have to be ampliﬁed unnaturally
strong in order to arrive at certain SNR values. Realistically,
the SNR values would be much higher in a park environment
compared to, for example, busystreet.
To illustrate the detection result of one exemplary experiment, Figure 2 shows the DET curves for the detection of
the class gunshot in tubestation noise. For an SNR value of
20 dB, the EER is 7.5 %, and for 0 dB, the EER goes up to
4.3. Event Classiﬁcation
In order to perform multi-event detection, the same framework as for single-event detection is used. Results for event
classiﬁcation are shown in Table 4. The comparison shows
again that GBFB features achieve a better performance than
Only in the case of clean sounds, MFCCs are
slightly better, while for all other SNR values, GBFB features
perform consistently better. On average, the relative performance improvement from MFCCs to GBFBs is 6 %.
practical advantage of the multiclass detection system is that
it uses the same models as the single-class detectors, which
are trained with clean data only. Further improvements in
classiﬁcation accuracy are expected with the introduction of
concepts such as multi-condition training.
5. CONCLUSIONS
We proposed an event detection system for audio surveillance scenarios. Acoustic events are modelled with GMMs
and single-class detectors are trained for different realistic
23rd European Signal Processing Conference (EUSIPCO)
False detection rate
False rejection rate
Fig. 2. DET curves for the class gunshot in tubestation noise,
for the SNR values of 20, 10, 5, and 0 dB.
Table 4. Event classiﬁcation accuracy
background noise conditions. As an alternative to the classical MFCC features, we evaluated Gabor ﬁlterbank features,
which extract spectral and temporal modulation frequencies
from the signal. In an evaluation with realistic background
recordings in noisy conditions, the proposed Gabor features
achieved a better detection and classiﬁcation performance
than the MFCCs. In particular, in the classiﬁcation experiments, where MFCCs performed slightly better in clean
conditions, GBFB features showed a better noise robustness.
In this work, matched models with known background sound
are assumed.
In order to run the system automatically in
different background scenes, a combination with a system
for acoustic scene recognition makes sense, such as the one
presented in . The evaluated GMMs are well suited to
model stationary sounds such as scream, while for other nonstationary sounds, better models need to be found in future