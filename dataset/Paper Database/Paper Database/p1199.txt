The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence (AAAI-20)
Measuring and Relieving the Over-Smoothing
Problem for Graph Neural Networks from the Topological View
Deli Chen,1 Yankai Lin,2 Wei Li,1 Peng Li,2 Jie Zhou,2 Xu Sun1
1MOE Key Lab of Computational Linguistics, School of EECS, Peking University
2Pattern Recognition Center, WeChat AI, Tencent Inc., China
{chendeli, liweitj47, xusun}@pku.edu.cn, {yankailin, patrickpli, withtomzhou}@tencent.com
Graph Neural Networks (GNNs) have achieved promising
performance on a wide range of graph-based tasks. Despite their success, one severe limitation of GNNs is the
over-smoothing issue (indistinguishable representations of
nodes in different classes). In this work, we present a systematic and quantitative study on the over-smoothing issue of GNNs. First, we introduce two quantitative metrics,
MAD and MADGap, to measure the smoothness and oversmoothness of the graph nodes representations, respectively.
Then, we verify that smoothing is the nature of GNNs and
the critical factor leading to over-smoothness is the low
information-to-noise ratio of the message received by the
nodes, which is partially determined by the graph topology. Finally, we propose two methods to alleviate the oversmoothing issue from the topological view: (1) MADReg
which adds a MADGap-based regularizer to the training objective; (2) AdaEdge which optimizes the graph topology
based on the model predictions. Extensive experiments on
7 widely-used graph datasets with 10 typical GNN models
show that the two proposed methods are effective for relieving the over-smoothing issue, thus improving the performance of various GNN models.
Introduction
Graph Neural Networks form an effective framework for
learning graph representation, which have proven powerful
in various graph-based tasks . Despite their success in graph modeling, oversmoothing is a common issue faced by GNNs , which means that the representations of the graph nodes of different classes would become
indistinguishable when stacking multiple layers, which seriously hurts the model performance (e.g., classiﬁcation accuracy). However, there is limited study on explaining why
and how over-smoothing happens. In this work, we conduct
a systematic and quantitative study of the over-smoothing
issue of GNNs on 7 widely-used graph datasets with 10 typical GNN models, aiming to reveal what is the crucial factor
Copyright c⃝2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
Figure 1: The prediction accuracy (Acc) and MADGap of
GCNs on the CORA dataset. We
can observe a signiﬁcantly high correlation between the accuracy and MADGap in two different situations: (a) Different models: Results of GCNs with different number of layers; (b) Different training periods: Results after each epoch
in the 2-layer GCN. The Pearson correlation coefﬁcient is
shown in the title and ** means statistically signiﬁcant with
bringing in the over-smoothing problem of GNNs and ﬁnd
out a reasonable direction to alleviate it.
We ﬁrst propose a quantitative metric Mean Average Distance (MAD), which calculates the mean average distance
among node representations in the graph to measure the
smoothness of the graph (smoothness means similarity of
graph nodes representation in this paper). We observe that
the MAD values of various GNNs become smaller as the
number of GNN layers increases, which supports the argument that smoothing is the essential nature of GNNs. Hence,
the node interaction through the GNN message propagation would make their representations closer, and the whole
graph representation would inevitably become smoothing
when stacking multiple layers.
Furthermore, we argue that one key factor leading to the
over-smoothing issue is the over-mixing of information and
noise. The interaction message from other nodes may be either helpful information or harmful noise. For example, in
the node classiﬁcation task, intra-class interaction can bring
useful information, while inter-class interaction may lead
to indistinguishable representations across classes. To measure the quality of the received message by the nodes, we
deﬁne the information-to-noise ratio as the proportion of
intra-class node pairs in all node pairs that have interactions through GNN model. Based on our hypothesis, we extend MAD to MADGap to measure the over-smoothness of
graph (over-smoothness means similarity of representations
among different classes’ nodes in this paper). We notice that
two nodes with close topological distance (can reach with a
few hops) are more likely to belong to the same class, and
vice versa. Therefore, we differentiate the role between remote and neighboring nodes and calculate the gap of MAD
values (MADGap) between remote and neighboring nodes
to estimate the over-smoothness of graph representation. Experimental results prove that MADGap does have a signiﬁcantly high correlation with the model performance in general situations, and an example is shown in Figure 1. Further experiments show that both the model performance and
the MADGap value rise as the information-to-noise ratio increases, which veriﬁes our assumption that the informationto-noise ratio affects the smoothness of graph representation
to a great extent.
After more in-depth analysis, we propose that low
information-to-noise ratio is caused by the discrepancy between the graph topology and the objective of the downstream task. In the node classiﬁcation task, if there are too
many inter-class edges, the nodes will receive too much message from nodes of other classes after several propagation
steps, which would result in over-smoothing. To prove our
assumption, we optimize the graph topology by removing
inter-class edges and adding intra-class edges based on the
gold labels, which proves very effective in relieving oversmoothing and improving model performance. Hence, the
graph topology has a great inﬂuence on the smoothness of
graph representation and model performance. That is to say,
there is a deviation from the natural graph to the downstream task. However, in the previous graph-related studies , researchers mainly focus on designing novel GNN architectures but pay less attention to improve the established
graph topology.
Based on our observations, we propose two methods to
relieve the over-smoothing issue from the topological view:
(a) MADReg: We add a MADGap-based regularizer to
the training objective to directly increase received information and reduce noise; (b) Adaptive Edge Optimization
 
Convolution
ChebGCN 
Convolution
HyperGraph 
Convolution
&Attention
FeaSt 
Convolution
GraphSAGE 
Convolution
GAT 
ARMA 
Convolution
GraphSAGE 
Convolution
HighOrder 
GGNN 
Table 1: Introduction of baseline GNN models. The information propagation method is also displayed.
• We design two quantitative metrics: MAD for smoothness
and MADGap for over-smoothness of graph representation. Statistical analysis shows that MADGap has a signiﬁcantly high correlation with model performance.
• We propose two methods: MADReg and AdaEdge to relieve the over-smoothing issue of GNNs. Experimental results show that our proposed methods can signiﬁcantly
reduce over-smoothness and improve the performance of
multiple GNNs on various datasets.
Datasets and Models
node classiﬁcation task, one of the most basic graphbased tasks, is usually conducted to verify the effectiveness of GNN architectures or analyze the characteristics of GNNs .
Therefore, we select the node classiﬁcation task for our experiments. We conduct experiments on 7 public datasets
in three types, namely, (1) citation network: CORA, Cite-
Seer, PubMed ; (2) coauthor network:
CS, Physics;1 (3) Amazon product network: Computers,
Photo . We conduct our detailed analysis on the three citation networks, which are usually taken
as the benchmarks for graph-related studies and verify the effectiveness of the
proposed method on all these datasets.
To guarantee the generalizability of our conclusion, we
conduct experiments with 10 typical GNN models in this
work. The GNN models and their propagation methods are
listed in Table 1, in which the propagation taxonomy follows Zhou et al. . The implementation of the baselines is partly based on Fey and Lenssen and Maehara . More details about the datasets and experiments
are given in the arXiv version2 due to limited space.
Measuring Over-smoothing Problem from the
Topological View
In this section, we aim to investigate what is the key factor
leading to the over-smoothing problem. To this end, we propose two quantitative metrics MAD and MADGap to measure the smoothness and over-smoothness of graph repre-
1 
2 
sentation, which are further used to analyze why and how
the over-smoothing issue happens.
MAD: Metric for Smoothness
To measure the smoothness of the graph representation, we
ﬁrst propose a quantitative metric: Mean Average Distance
(MAD). MAD reﬂects the smoothness of graph representation by calculating the mean of the average distance from
nodes to other nodes. Formally, given the graph representation matrix H ∈Rn×h (we use the hidden representation of
the ﬁnal layer of GNN. Term h is the hidden size), we ﬁrst
obtain the distance matrix D ∈Rn×n for H by computing
the cosine distance between each node pair:
Hi,: · Hj,:
|Hi,:| · |Hj,:|
i, j ∈[1, 2, · · · , n],
where Hk,: is the k-th row of H. The reason to use cosine
distance is that cosine distance is not affected by the absolute
value of the node vector, thus better reﬂecting the smoothness of graph representation. Then we ﬁlter the target node
pairs by element-wise multiplication D with a mask matrix
Dtgt = D ◦M tgt,
where ◦denotes element-wise multiplication; M tgt
{0, 1}n×n; M tgt
= 1 only if node pair (i, j) is the target
one. Next we access the average distance ¯Dtgt for non-zero
values along each row in Dtgt:
where 1(x) = 1 if x > 0 otherwise 0. Finally, the MAD
value given the target node pairs is calculated by averaging
the non-zero values in ¯Dtgt:
Li, Han, and Wu perform a theoretical analysis
on the graph convolution network (GCN), and conclude that
performing smoothing operation on node representations is
the key mechanism why GCN works. We extend the conclusion empirically to the 10 typical GNNs listed in Table 1
with the help of the proposed MAD metric. To this end, for
each GNN model with different number of layers, we compute the MAD value MADglobal by taking all node pairs into
account, i.e., all values in M tgt are 1, to measure the global
smoothness of the learned graph representation.
The results on the CORA dataset are shown in Figure 2.
We can observe that as the number of GNN layers increases,
the MAD values become smaller. Apart from this, the MAD
value of high-layer GNNs gets close to 0, which means
that all the node representations become indistinguishable.
GNN models update the node representation based on the
features from neighboring nodes. We observe that the interaction between nodes makes their representations similar to each other. Similar phenomenons that the smoothness
rises as the layer increases are also observed in other datasets
as presented in arXiv version. Therefore, we conclude that
smoothing is an essential nature for GNNs.
Figure 2: The MAD values of various GNNs with different layers on the CORA dataset. Darker color means larger
MAD value. We can ﬁnd that the smoothness of graph representation rises as the model layer increases.
Information-to-noise Ratio Largely Affects
Over-smoothness
With the help of MAD, we can quantitatively measure the
smoothness of graph representation. Here come two new
questions: Since smoothing is the nature of GNNs, what is
over-smoothing, and what results in over-smoothing?
We assume that the over-smoothing problem is caused by
the over-mixing of information and noise, which is inﬂuenced by the quality of the nodes received message. The interaction message from other nodes by GNN operation may
be either helpful information or interference noise. For example, in the node classiﬁcation task, interaction between
nodes of the same class brings useful information, which
makes their representations more similar to each other and
the probability of being classiﬁed into the same class is increased. On the contrary, the contact of nodes from other
classes brings the noise. Hence, the reason why GNNs work
is that the received useful information is more than noise.
On the other hand, when the noise is more than the information, the learned graph representation will become oversmoothing.
To quantitatively measure the quality of the received message of the nodes, we deﬁne the information-to-noise ratio
as the proportion of intra-class node pairs in all contactable
node pairs that have interactions through the GNN model.
For example, at the second-order, the information-to-noise
ratio for each node is the proportion of nodes of the same
class in all the ﬁrst-order and second-order neighbors; the
information-to-noise ratio for the whole graph is the proportion of the intra-class pairs in all the node pairs that can be
contacted in 2 steps. In Figure 3, we display the informationto-noise ratio of the whole graph for the CORA, CiteSeer
and Pubmed datasets. We can ﬁnd that there are more intraclass node pairs at low order and vice versa. When the model
layer number gets large where the information-to-noise ra-
Figure 3: The information-to-noise ratio at different neighbor orders (accumulated) for the CORA/CiteSeer/PubMed
datasets. We can ﬁnd that the information-to-noise ratio declines as the orders increases in all these three datasets.
tio is small, the interaction between high-order neighbors
brings too much noise and dilutes the useful information,
which is the reason for the over-smoothing issue. Based on
this observation, we extend MAD to MADGap to measure
the over-smoothness in the graph representation. From Figure 3 we notice that two nodes with small topological distance (low-order neighbours) are more likely to belong to the
same category. Hence, we propose to utilize the graph topology to approximate the node category, and calculate the gap
of MAD values differentiating remote and neighbour nodes
to estimate the over-smoothness of the graph representation,
MADGap = MADrmt −MADneb,
where MADrmt is the MAD value of the remote nodes in
the graph topology and MADneb is the MAD value of the
neighbouring nodes.
According to our assumption, large MADGap value indicates that the useful information received by the node is
more than noise. At this time, GNNs perform reasonable
extent of smoothing, and the model would perform well.
On the contrary, small or negative MADGap means oversmoothing and inferior performance. To verify the effectiveness of MADGap, we calculate the MADGap value3 and
compute the Pearson coefﬁcient between the MADGap and
the prediction accuracy for various GNN models. We report
the Pearson coefﬁcient for GNNs with different layers on
CORA, CiteSeer and PubMed datasets in Table 2. According to the table, we can ﬁnd that there exists a signiﬁcantly
high correlation between MADGap and the model performance, which validates that MADGap is a reliable metric
to measure graph representation over-smoothness. Besides,
MADGap can also be used as an observation indicator to estimate the model performance based on the graph topology
without seeing the gold label. It is worth noting that 1-layer
3In this work, we calculate MADneb based on nodes with orders ≤3 and MADrmt based on nodes with orders ≥8.
HyperGraph
Table 2: The Pearson coefﬁcient between accuracy and
MADGap for various models on CORA/CiteSeer/PubMed
datasets. Pearson coefﬁcient is calculated based on the results of models with different layers (1-6). * means statistically signiﬁcant with p < 0.05 and ** means p < 0.01.
GNN usually has small MADGap and prediction accuracy
(Figure 1), which is caused by the insufﬁcient information
transfer, while the over-smoothing issue of high-layer GNN
is caused by receiving too much noise.
In Figure 4, we show the MADGap and prediction accuracy for node sets with different information-to-noise ratios in the same model. We can ﬁnd that even with the
same model and propagation step, nodes with higher rate of
information-to-noise ratio generally have higher prediction
accuracy with smaller over-smoothing degree. We also observe similar phenomena on other datasets, which are shown
in the arXiv version. This way, we further verify that it is the
information-to-noise ratio that affects the graph representation over-smoothness to a great extent, thus inﬂuencing the
model performance.
Topology Affects the Information-to-noise Ratio
From the previous analysis, we can ﬁnd that the key factor inﬂuencing the smoothness of graph representation is the
information-to-noise ratio. Then the following question is:
What affects the information-to-noise ratio? We argue that
it is the graph topology that affects the information-to-noise
ratio. The reason for the node receiving too much noise is
related to the discordance between the natural graph and
the task objective. Take node classiﬁcation as an example. If
there are too many inter-class edges, the nodes will receive
too much noise after multiple steps of message propagation,
which results in over-smoothing and bad performance.
The graph topology is constructed based on the natural
links. For example, the edges in the citation network represent the citing behavior between papers and edges in the
product network represent the products co-purchasing relations. GNN models rely on these natural links to learn node
representations. However, natural links between nodes of
different classes are harmful to the node classiﬁcation task.
Therefore, we propose to alleviate the over-smoothing issue
of GNNs and improve their performance by optimizing the
graph topology to match the downstream task.
To verify our assumption, we optimize the graph topology
Figure 4: Performance (accuracy) and over-smoothness (MADGap) of node sets with different information-to-noise ratio (e.g.,
0.1 means ratio≤0.1) on the CORA dataset (We display 4 out of 10 models results due to the limited space. We observe similar
results in other models). All models have 2 layers. Results prove that nodes with higher information-to-noise ratio would have
less over-smoothness degree and better prediction result.
Figure 5: The gold label based topology adjustment experiment on the CORA dataset. We show the results of both removing
inter-class edges (ﬁrst row, where the X-axis represents the removing rate) and adding intra-class edges (second row, where the
X-axis represents the intra-class edge ratio compared to the raw graph) on GCN, GAT, GraphSAGE and ChebGCN. Results
show that both of these methods are very helpful for relieving the over-smoothing issue and improving model performance.
by removing inter-class edges and adding intra-class edges
based on the gold labels. The results on the CORA dataset
are shown in Figure 5. We can ﬁnd that the MADGap value
rises consistently as more inter-class edges are removed and
more intra-class edges are added, resulting in better model
performance. Therefore, optimizing graph topology is helpful in relieving the over-smoothing problem and improving
model performance.
In summary, we ﬁnd that the graph topology has a great
inﬂuence on the smoothness of graph representation and
model performance. However, there is still discordance between the natural links and the downstream tasks. Most existing works mainly focus on designing novel GNN architectures but pay less attention to the established graph topology.
Hence, we further investigate to improve the performance of
GNNs by optimizing the graph topology.
Relieving Over-smoothing Problem from the
Topological View
Inspired by the previous analysis, we propose two methods to relieve the over-smoothing issue from the topological view: (1) MADReg: We add a MADGap-based regularizer to the training objective; (2) Adaptive Edge Optimization (AdaEdge): We adjust the graph topology adaptively
by iteratively training GNN models and conducting edge remove/add operations based on the prediction result. Neither
of these two methods is restricted to speciﬁc model architectures and can be used in the training process of general
GNN models. Experiments demonstrate their effectiveness
in a variety of GNNs.
MADReg: MADGap as Regularizer
In the previous experiments, we ﬁnd that MADGap shows
a signiﬁcantly high correlation with model performance.
Hence, we add MADGap to the training objective to make
the graph nodes receive more useful information and less
interference noise:
−l log p( ˆl |X, A, Θ) −λMADGap,
where X is the input feature matrix, A is the adjacency matrix, ˆl and l are the predicted and gold labels of the node
respectively. Θ is the parameters of GNN and λ is the regularization coefﬁcient to control the inﬂuence of MADReg.
We calculate MADGap on the training set to be consistent
with the cross-entropy loss.
Figure 6: MADReg and AdaEdge results on the CORA/CiteSeer/PubMed datasets. The number of GNN layers is 4, where the
over-smoothing issue is severe. The box plot shows the mean value and the standard deviation of the prediction accuracy and
the MADGap values of 50 turns results and Sun, Koniusz, and Wang . And we can ﬁnd that the two proposed methods can effectively relieve the
over-smoothing issue and improve model performance in most cases.
AdaEdge: Adaptive Edge Optimization
As discussed in the previous section, after optimizing the
topology based on gold label (adding the intra-class edges
and removing the inter-class edges), the over-smoothing
issue is notably alleviated, and the model performance
is greatly improved. Inspired by this, we propose a selftraining algorithm called AdaEdge to optimize the graph
topology based on the prediction result of the model to adaptively adjust the topology of the graph to make it more reasonable for the speciﬁc task objective. Speciﬁcally, we ﬁrst
train GNN on the original graph and adjust the graph topology based on the prediction result of the model by deleting
inter-class edges and adding intra-class edges. Then we retrain the GNN model on the updated graph from scratch.
We perform the above graph topology optimization operation multiple times. The details of the AdaEdge algorithm
are introduced in the arXiv version.
Relieving Over-smoothing in High-order Layers
To verify the effectiveness of the two proposed methods, we
conduct controlled experiments for all the 10 baseline GNN
models on CORA/CiteSeer/PubMed datasets. We calculate
the prediction accuracy and MADGap value for the GNN
models with 4 layers, where the over-smoothing issue is serious. The results are shown in Figure 6. We present 6 out
of 10 models results due to the space limit; the other models
can be found in the arXiv version. We can ﬁnd that in the
high-order layer situation where the over-smoothing issue
is severe, the MADReg and AdaEdge methods can effectively relieve the over-smoothing issue and improve model
performance for most models in all three datasets. The effectiveness of MADReg and AdaEdge further validates our
assumption and provides a general and effective solution to
relieve the over-smoothing problem.
Improving Performance of GNNs
In Table 3, we show the controlled experiments for GNN
models trained on the original graph and the updated graph
obtained by the AdaEdge method on all the 7 datasets. We
select the best hyper-parameters when training GNN on the
original graph and ﬁx all these hyper-parameters when training on the updated graph. Experimental results show that
the AdaEdge method can effectively improve the model performance in most cases, which proves that optimizing the
graph topology is quite helpful for improving model performance. We analyze the cases of the AdaEdge method with
little or no improvement and ﬁnd that this is caused by the
incorrect operations when adjusting the topology. Therefore,
when the ratio of incorrect operations is too large, it will
bring serious interference to the model training and bring in
little or no improvement. Due to the space limit, the results
of MADReg are shown in the arXiv version. Typically, the
baselines achieve their best performance with small number of GNN layers, where the over-smoothing issue is not
severe. Under this condition, MADReg can hardly improve
the performance by enlarging the MADGap value. However,
when the over-smoothing issue becomes more severe while
the GNN layer number grows larger, MADReg is still capable of improving the performance of the baselines signiﬁcantly. Above all, both AdaEdge and MADReg are effective
for improving GNNs performance, and AdaEdge generalizes better when the over-smoothing issue is not severe.
Related Work
Graph Neural Networks (GNNs)
GNNs have proven effective in various non-Euclidean graph
structures, such as social network , biology network (Zitnik and Leskovec
Amazon Photo
Amazon Comp.
Coauthor CS
Coauthor Phy.
HyperGraph
Table 3: Controlled experiments of AdaEdge (+AE) on all the 7 datasets. We show the mean value, the standard deviation and
the t-test signiﬁcance of 50 turns results. * means statistically signiﬁcance with p < 0.05 and ** means p < 0.01. Bold result
means improvement compared to baseline. The missing results are due to the huge consumption of GPU memory.
2017), business graph and academic
graph . Recently, many novel GNN architectures have been developed for graph-based tasks. Veliˇckovi´c
et al. propose the graph attention network to use
self-attention to aggregate information from neighboring
nodes. Hamilton, Ying, and Leskovec propose a
general inductive framework to generate node embedding
by sampling and aggregating features from the neighboring nodes. There are also other GNNs proposed, such as
ARMA , FeaSt , HyperGraph and so
on. Xu et al. propose jumping knowledge networks to
help the GNN model to leverage the information from highorder neighbours for a better node representation. However,
all these models focus on improving the information propagation and aggregation operation on the static graph while
paying less attention to the graph topology. In this work, we
propose to explicitly optimize the graph topology to make it
more suitable for the downstream task.
Pareja et al. propose the EvolveGCN that uses the
RNN to evolve the graph model itself over time. Fey 
allow for a selective and node-adaptive aggregation of
the neighboring embeddings of potentially differing locality. Liang Yang and Zesheng Kang and Xiaochun Cao and
Di Jin and Bo Yang and Yuanfang Guo propose a new
variation of GCN by jointly reﬁning the topology and training the fully connected network. These existing works about
dynamic graph rely on the adaptive ability of the model itself and focus on special GNN architecture (e.g., GCN),
while our AdaEdge method optimizes the graph topology
with a clear target (adding intra-class edges and removing
inter-class edges) and can be used in general GNN architectures. Rong et al. propose DropEdge method to
drop edges randomly at each training epoch for data augmentation while our AdaEdge method adjusts edges before
training to optimize the graph topology.
Over-smoothing Problem in GNNs
Previous works 
have proven that over-smoothing is a common phenomenon
in GNNs. Li, Han, and Wu prove that the graph convolution of the GCN model is actually a special form of
Laplacian smoothing.
Deng, Dong, and Zhu propose that smoothness is helpful for node classiﬁcation and
design methods to encourage the smoothness of the output
distribution, while Yang et al. propose that nodes
may be mis-classiﬁed by topology based attribute smoothing and try to overcome this issue. In this work, we prove
that smoothing is the essential feature of GNNs, and then
classify the smoothing into two kinds by the information-tonoise ratio: Reasonable smoothing that makes GNN work,
and over-smoothing that causes the bad performance. From
this view, the methods from Deng, Dong, and Zhu 
and Yang et al. can be regarded as improving reasonable smoothing and relieve over-smoothing, respectively.
Besides, Li et al. propose to use LSTM in GNN
to solve over-smoothing issue in text classiﬁcation. However, existing works usually mention the over-smoothing
phenomenon, but there lacks systematic or quantitative research about it.
Conclusion and Future Work
In this work, we conduct a systematic and quantitative study
of the over-smoothing issue faced by GNNs. We ﬁrst design two quantitative metrics: MAD for smoothness and
MADGap for over-smoothness. From the quantitative measurement results on multiple GNNs and graph datasets, we
ﬁnd that smoothing is the essential nature of GNNs; oversmoothness is caused by the over-mixing of information and
the noise. Furthermore, we ﬁnd that there is a signiﬁcantly
high correlation between the MADGap and the model performance. Besides, we prove that the information-to-noise
ratio is related to the graph topology, and we can relieve the
over-smoothing issue by optimizing the graph topology to
make it more suitable for downstream tasks. Followingly,
we propose two methods to relieve the over-smoothing issue in GNNs: The MADReg and the AdaEdge methods. Extensive results prove that these two methods can effectively
relieve the over-smoothing problem and improve model performance in general situations.
Although we have shown optimizing graph topology is
an effective way of improving GNNs performance, our proposed AdaEdge method still suffers from the wrong graph
adjustment operation problem. How to reduce these operations is a promising research direction.
Acknowledgement
This work was supported in part by a Tencent Research
Grant and National Natural Science Foundation of China
(No. 61673028). Xu Sun is the corresponding author of this