On Data Augmentation for GAN Training
Ngoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen, Trung-Kien Nguyen, Ngai-Man Cheung
Abstract—Recent successes in Generative Adversarial Networks (GAN) have afﬁrmed the importance of using more data
in GAN training. Yet it is expensive to collect data in many
domains such as medical applications. Data Augmentation (DA)
has been applied in these applications. In this work, we ﬁrst
argue that the classical DA approach could mislead the generator
to learn the distribution of the augmented data, which could
be different from that of the original data. We then propose
a principled framework, termed Data Augmentation Optimized
for GAN (DAG), to enable the use of augmented data in GAN
training to improve the learning of the original distribution. We
provide theoretical analysis to show that using our proposed DAG
aligns with the original GAN in minimizing the Jensen–Shannon
(JS) divergence between the original distribution and model
distribution. Importantly, the proposed DAG effectively leverages
the augmented data to improve the learning of discriminator and generator. We conduct experiments to apply DAG to
different GAN models: unconditional GAN, conditional GAN,
self-supervised GAN and CycleGAN using datasets of natural
images and medical images. The results show that DAG achieves
consistent and considerable improvements across these models.
Furthermore, when DAG is used in some GAN models, the
system establishes state-of-the-art Fr´echet Inception Distance
(FID) scores. Our code is available1.
Index Terms—Generative Adversarial Networks, GAN, Data
Augmentation, Limited Data, Conditional GAN, Self-Supervised
GAN, CycleGAN
I. INTRODUCTION
ENERATIVE Adversarial Networks (GANs) is an
active research area of generative model learning. GAN
has achieved remarkable results in various tasks, for example: image synthesis , , , , image transformation
 , , , super-resolution , , text to image ,
 , video captioning , image dehazing , domain
adaptation , anomaly detection , . GAN aims
to learn the underlying data distribution from a ﬁnite number of (high-dimensional) training samples. The learning is
achieved by an adversarial minimax game between a generator G and a discriminator D . The minimax game is:
minG maxD V(D, G),
V(D, G) = Ex∼Pd log
+ Ex∼Pg log
Here, V(.) is the value function, Pd is the real data distribution
of the training samples, Pg is the distribution captured by the
generator (G) that maps from the prior noise z ∼Pz to the
data sample G(z) ∼Pg. Pz is often Uniform or Gaussian
distribution. It is shown in that given the optimal discriminator D∗, minG V(D∗, G) is equivalent to minimizing the
Jensen-Shannon (JS) divergence JS(Pd||Pg). Therefore, with
more samples from Pd (e.g., with a larger training dataset),
1 
the empirical estimation of JS(Pd||Pg) can be improved while
training a GAN. This has been demonstrated in recent works
 , , , where GAN beneﬁts dramatically from more
However, it is widely known that data collection is an
extremely expensive process in many domains, e.g. medical
images. Therefore, data augmentation, which has been applied
successfully to many deep learning-based discriminative tasks
 , , , could be considered for GAN training. In fact,
some recent works (e.g. ) have applied label-preserving
transformations (e.g. rotation, translation, etc.) to enlarge the
training dataset to train a GAN.
However, second thoughts about adding transformed data
to the training dataset in training GAN reveal some issues.
Some transformed data could be infrequent or non-existence
w.r.t. the original data distribution (Pd(T(x)) ≈0, where
T(x) is some transformed data by a transformation T). On the
other hand, augmenting the dataset may mislead the generator
to learn to generate these transformed data. For example,
if rotation is used for data augmentation on a dataset with
category “horses”, the generator may learn to create rotated
horses, which could be inappropriate in some applications.
The fundamental issue is that: with data augmentation (DA),
the training dataset distribution becomes P T
which could
be different from the distribution of the original data Pd.
Following , it can be shown that generator learning is
minimizing JS(P T
d ||Pg) instead of JS(Pd||Pg).
In this work, we conduct a comprehensive study to understand the issue of applying DA for GAN training. The main
challenge is to utilize the augmented dataset with distribution
d to improve the learning of Pd, distribution of the original
dataset. We make the following novel contributions:
• We reveal the issue that the classical way of applying DA
for GAN could mislead the generator to create infrequent
samples w.r.t. Pd.
• We propose a new Data Augmentation optimized for
GAN (DAG) framework, to leverage augmented samples
to improve the learning of GAN to capture the original
distribution. We discuss invertible transformation and its
JS preserving property. We discuss discriminator regularization via weight-sharing. We use these as principles
to build our framework.
• Theoretically, we provide convergence guarantee of our
framework under invertible transformations; empirically,
we show that both invertible and non-invertible transformations can be used in our framework to achieve
improvement.
• We show that our proposed DAG overcomes the issue
in classical DA. When DAG is applied to some existing
GAN model, we could achieve state-of-the-art performance.
 
II. RELATED WORKS
The standard GAN connects the learning of the discriminator and the generator via the single feedback (real or fake)
to ﬁnd the Nash equilibrium in high-dimensional parameter
space. With this feedback, the generator or discriminator may
fall into ill-pose settings and get stuck at bad local minimums
(i.e. mode collapse) though still satisfying the model constraints. To overcome the problems, different approaches of
regularizing models have been proposed.
Lipschitzness based Approach. The most well-known approach is to constrain the discriminator to be 1-Lipschitz. Such
GAN relies on methods like weight-clipping , gradient
penalty constraints , , , , and spectral
norm . This constraint mitigates gradient vanishing 
and catastrophic forgetting . However, this approach often
suffers the divergence issues , .
Inference Models based Approach. Inference models enable to infer compact representation of samples, i.e., latent
space, to regularize the learning of GAN. For example, using auto-encoder to guide the generator towards resembling
realistic samples ; however, computing reconstruction via
auto-encoder often leads to blurry artifacts. VAE/GAN 
combines VAE and GAN, which enables the generator
to be regularized via VAE to mitigate mode collapse, and
blur to be reduced via the feature-wise distance. ALI 
and BiGAN take advantage of the encoder to infer the
latent dimensions of the data, and jointly train the data/latent
samples in the GAN framework. InfoGAN improves the
generator learning via maximizing variational lower bound
of the mutual information between the latent and its ensuing
generated samples. , used auto-encoder to regularize
both learning of discriminator and generator. Infomax-GAN
 applied contrastive learning and mutual information for
GAN. It is worth-noting auto-encoder based methods ,
 , , are likely good to mitigate catastrophic forgetting
since the generator is regularized to resemble the real ones.
The motivation is similar to EWC or IS , except
the regularization is obtained via the output. Although using feature-wise distance in auto-encoder could reconstruct
sharper images, it is still challenging to produce realistic detail
of textures or shapes.
Multiple Feedbacks based Approach. The learning via
multiple feed-backs has been proposed. Instead of using only
one discriminator or generator like standard GAN, the mixture
models are proposed, such as multiple discriminators ,
 , , the mixture of generators , or an attacker
applied as a new player for GAN training . , ,
 train GAN with auxiliary self-supervised tasks via multi
pseudo-classes that enhance stability of the optimization
Data-Scale based Approach. Recent work , suggests that GAN beneﬁts from large mini-batch sizes and
the larger dataset , as many other deep learning
models. Unfortunately, it is costly to obtain a large-scale
collection of samples in many domains. This motivates us to
study Data Augmentation as a potential solution. Concurrent
with our work, , , independently propose data
augmentation for training GANs very recently. Our work
and all these works are based on different approaches and
experiments. We recommend the readers to check out their
works for more details. Here, we want to highlight that our
work is fundamentally different from these concurrent works:
our framework is based on theoretical JS divergence preserving
of invertible transformation. Furthermore, we apply the ideas
of multiple discriminators and weight sharing to design our
framework. Empirically, we show that both invertible and noninvertible transformations can be used in our framework to
achieve improvement especially in setups with limited data.
III. NOTATIONS
We deﬁne some notations to be used in our paper:
• X denotes the original training dataset; x ∈X has the
distribution Pd.
• X T , X Tk denote the transformed datasets that are transformed by T, Tk, resp. T(x) ∈X T has the distribution
d ; Tk(x) ∈X Tk has the distribution P Tk
d . We use T1
to denote an identity transform. Therefore, X T1 is the
original data X.
• X T = X T1 ∪X T2 · · · ∪X TK denotes the augmented
dataset, where T = {T1, T2, . . . , TK}. Sample in X T
has the mixture distribution P T
IV. ISSUE OF CLASSICAL DATA AUGMENTATION FOR GAN
Data Augmentation (DA) increases the size of the dataset
to reduce the over-ﬁtting and generalizes the learning of deep
neural networks , , . The goal is to improve the
classiﬁcation performance of these networks on the original
dataset. In this work, we study whether applying DA for
GAN can improve learning of the generator and modeling
of the distribution Pd of the original dataset. The challenge
here is to use additional augmented data but have to keep
the learning of the original distribution. To understand this
problem, we ﬁrst investigate how the classical way of using
DA (increasing diversity of X via transformations T and use
augmented dataset X T as training data for GAN) inﬂuences
the learning of GAN.
V(D, G) = Ex∼P T
+Ex∼Pg log
Toy example. We set up the toy example with the MNIST
dataset for the illustration. In this experiment, we augment
the original MNIST dataset (distribution Pd) with some
widely-used augmentation techniques T
(rotation, ﬂipping,
and cropping) (Refer to Table. I for details) to obtain new
dataset (distribution P T
d ). Then, we train the standard GAN
 (objectives is shown in Eq. 2) on this new dataset. We
construct two datasets with two different sizes: 100% and
25% randomly selected MNIST samples. We denote the GAN
model trained on the original dataset as Baseline, and GAN
trained on the augmented dataset as DA. We evaluate models
by FID scores. We train the model with 200K iterations using
small DCGAN architecture similar to . We compute the
10K-10K FID (using a pre-trained MNIST classiﬁer) to
measure the similarity between the generator distributions and
THE LIST OF DA TECHNIQUES IN OUR EXPERIMENTS. : INVERTIBLE, : NON-INVERTIBLE. INVERTIBLE: THE ORIGINAL IMAGE CAN BE EXACTLY
REVERTED BY THE INVERSE TRANSFORMATION. EACH ORIGINAL IMAGE IS TRANSFORMED INTO K −1 NEW TRANSFORMED IMAGES. THE ORIGINAL
IMAGE IS ONE CLASS AS THE IDENTITY TRANSFORMATION. FLIPROT = FLIPPING + ROTATION.
Invertible
Description
Rotating images with 0◦, 90◦, 180◦and 270◦degrees.
Flipping the original image with left-right, bottom-up and the combination of left-right and bottom-up.
Translation
Shifting images Nt pixels in directions: up, down, left and right. Zero-pixels are padded for missing parts caused by
the shifting.
Cropping at four corners with scales Nc of original size and resizing them into the same size as the original image.
Combining ﬂipping (left-right, bottom-up) + rotation of 90◦.
Generated examples of the full MNIST dataset (100%). Top row:
the real samples, the generated samples of Baseline model. Bottom row: the
rotated real samples and the generated samples of DA with rotation.
BEST FID (10K-10K) OF GAN BASELINE WITH CLASSICAL DA ON
MNIST DATASET.
the distribution of the original dataset. For a fair comparison,
we use K = 4 for all augmentation methods.
Some generated examples of Baseline and DA methods are
visualized in Fig. 1. Top row: real samples, the generated
samples of the Baseline model. Bottom row: the rotated real
samples and the generated samples of DA with rotation. See
more examples of DA with ﬂipping and cropping in Fig. 10
of Appendix B in the supplementary material. We observe
that the generators trained with DA methods create samples
similar to the augmented distribution P T
d . Therefore, many
generated examples are out of Pd. To be precise, we measure
the similarity between the generator distribution Pg and Pd
with FID scores as in Table. II. The FIDs of DA methods are
much higher as compared to that of Baseline for both cases
100% and 25% of the dataset. This suggests that applying
DA in the classical way would misguide the generator to
learn a rather different distribution compared to that of the
original data. Comparing different augmentation techniques,
it makes sense that the distributions of DA with ﬂipping and
DA with cropping are most similar and different from the
original distribution respectively. Training DA on small/full
dataset results in FID difference for Baseline. It means there
are some impacts of data size on the learning of GAN (to
be discussed further). We further support these observations
with the theoretical analysis in Sec. IV-A. This experiment
illustrates that applying DA in a classical way for GAN could
encounter an issue: infrequent samples may be generated more
due to alternation in the data distribution Pd. Therefore, the
classical way of applying DA may not be suitable for GAN.
To apply data augmentation in GAN, the methods of applying
DA need to ensure the learning of Pd. We propose a new DA
framework to achieve this.
A. Theoretical Analysis on DA
Generally, let T = {T1, T2, . . . , TK} be the set of augmentation techniques to apply on the original dataset. Pd is the
distribution of original dataset. P T
is the distribution of the
augmented dataset. Training GAN on this new dataset, the
generator is trained via minimizing the JS divergence between
its distribution Pg and P T
d as following (The proof is similar
V(D∗, G) = −log(4) + 2 · JS(P T
where D∗is the optimal discriminator. Assume that the
optimal solution can be obtained: Pg = P T
V. PROPOSED METHOD
The previous section illustrates the issue of classical DA
for GAN training. The challenge here is to use the augmented
dataset X T to improve the learning of the distribution of
original data, i.e. Pd instead of P T
d . To address this,
1) We ﬁrst discuss invertible transformations and their
invariance for JS divergence.
2) We then present a simple modiﬁcation of the vanilla
GAN that is capable to learn Pd using transformed samples X Tk, provided that the transformation is invertible
as discussed in (1).
3) Finally, we present our model which is a stack of
the modiﬁed GAN in (2); we show that this model
is capable to use the augmented dataset X T , where
T = {T1, T2, . . . , TK}, to improve the learning of Pd.
A. Jensen-Shannon (JS) Preserving with Invertible Transformation
Invertible mapping function . Considering two distributions px(x) and qx(x) in space X. Let T: X →Y denote
the differentiable and invertible (bijective) mapping function
(linear or non-linear) that converts x into y, i.e. y = T(x).
Then we have the following theorem:
Theorem 1: The Jensen-Shannon (JS) divergence between
two distributions is invariant under differentiable and invertible
transformation T:
JS(px(x)||qx(x)) = JS(py(y)||qy(y))
Proof. Refer to our proof in Appendix A-A. In our case, we
have px(.), qx(.), py(.), qy(.) to be Pd, Pg, P T
g resp. Thus,
if an invertible transformation is used, then JS(Pd||Pg) =
g ). Note that, if T is non-invertible, JS(P T
may approximate JS(Pd||Pg) to some extent. The detailed
investigation of this situation is beyond the scope of our work.
However, the take-away from this theorem is that JS preserving
can be guaranteed if invertible transformation is used.
B. GAN Training with Transformed Samples
Motivated by this invariant property of JS divergence, we
design the GAN training mechanism to utilize the transformed
data, but still, preserve the learning of Pd by the generator.
Figure 2 illustrates the vanilla GAN (left) and this new design
(right). Compared to the vanilla GAN, the change is simple:
the real and fake samples are transformed by Tk before feeding
into the discriminator Dk. Importantly, generator’s samples are
transformed to imitate the transformed real samples, thus the
generator is guided to learn the distribution of the original data
samples in X. The mini-max objective of this design is same
as that of the vanilla GAN, except that now the discriminator
sees the transformed real/fake samples:
V(Dk, G) = Ex∼P
where P Tk
be the distributions of transformed real and
fake data samples respectively, Tk ∈T . For ﬁxed generator
G, the optimal discriminator D∗
k of V(Dk, G) is that in Eq. 6
(the proof follows the arguments as in ). With the invertible
transformation Tk, Dk is trained to achieve exactly the same
optimal as D:
k(Tk(x)) =
d (Tk(x)) + pTk
pd(x)|J Tk(x)|−1
pd(x)|J Tk(x)|−1 + pg(x)|J Tk(x)|−1
pd(x) + pg(x) = D∗(x)
where |J Tk(x)| is the determinant of Jacobian matrix of Tk.
Given optimal D∗
k, training generator with these transformed
samples is equivalent to minimizing JS divergence between
k, G) = −log(4) + 2 · JS(P Tk
Furthermore, if an invertible transformation is chosen for
Tk, then V(D∗
k, G) = −log(4)+2·JS(Pd||Pg) (using Theorem
1). Therefore, this mechanism guarantees the generator to learn
to create the original samples, not transformed samples. The
convergence of GAN with transformed samples has the same
JS divergence as the original GAN if the transformation Tk is
invertible. Note that, this design has no advantage over the
original GAN: it performs the same as the original GAN.
However, we explore a design to stack them together to utilize
augmented samples with multiple transformations. This will be
discussed next.
C. Data Augmentation Optimized for GAN
Improved DA (IDA). Building on the design of the previous
section, we make the ﬁrst attempt to leverage the augmented
samples for GAN training as shown in Fig. 3a, termed Improved DA (IDA). Speciﬁcally, we transform fake and real
samples with {Tk} and feed the mixture of those transformed
real/fake samples as inputs to train a single discriminator D
(recall that T1 denotes the identity transform). Training the
discriminator (regarded as a binary classiﬁer) using augmented
samples tends to improve generalization of discriminator
learning: i.e., by increasing feature invariance (regarding real
vs. fake) to speciﬁc transformations, and penalizing model
complexity via a regularization term based on the variance of
the augmented forms . Improving feature representation
learning is important to improve the performance of GAN
 , . However, although IDA can beneﬁt from invertible
transformation as in Section V-B, training all samples with
a single discriminator does not preserve JS divergence of
original GAN (Refer to Theorem 2 of Appendix A for proofs).
IDA is our ﬁrst attempt to use augmented data to improve
GAN training. Since it is not JS preserving, it does not
guarantee the convergence of GAN. We state the issue of IDA
in Theorem 2.
Theorem 2: Considering two distributions p, q: p
m=1 wmpm and q = PK
m=1 wmqm, where PK
m=1 wm = 1.
If distributions pm and qm are distributions of p0 and q0
transformed by invertible transformations Tm respectively, we
JS(p||q) ≤JS(p0||q0)
Proofs. From Theorem 1 of invertible transformation Tm,
we have JS(p0||q0) = JS(pm||qm). Substituting this into the
Lemma 2 (in Appendix): JS(p||q) ≤PK
m=1 wmJS(p0||q0) =
m=1 wm)JS(p0||q0) = JS(p0||q0). It concludes the proof.
In our case, we assume that p, q are mixtures of distributions
that are inputs of IDA method (discussed in Section V-C)
Fig. 2. The original (vanilla) GAN model (left) and our design to train GAN with transformed data (right).
Fig. 3. (a) IDA model with single discriminator (b) Our ﬁnal proposed model (DAG) with multiple discriminators Dk. In these models, both real samples
(violet paths) and fake samples (red paths) are used to train discriminators. Only fake samples (red paths) are used to train the generator.
and p0 = Pd, q0 = Pg. In fact, the mixture of transformed
samples has the form of distributions as discussed in Theorem
2 (Refer to Lemma 1 in Appendix). According to Theorem 2,
IDA method is minimizing the lower-bound of JS divergence
instead of the exact divergence of JS(Pd||Pg). Due to this
issue, although using more augmented samples, but IDA (FID
= 29.7) does not out-perform the Baseline (FID = 29.6) (Refer
to Table III in Section VI).
Data Augmentation optimized for GAN (DAG). In what
follows, we discuss another proposed framework to overcome
the above problem. The proposed Data Augmentation optimized for GAN (DAG) aims to utilize an augmented dataset
X T with samples transformed by T = {T1, T2, . . . , TK} to
improve learning of the distribution of original data (Fig. 3b).
DAG takes advantage of the different transformed samples by
using different discriminators D, {Dk} = {D2, D3, . . . DK}.
The discriminator Dk is trained on samples transformed by
D,{Dk}V(D, {Dk}, G) = V(D, G) +
discriminator
augmenting
the original GAN discriminator objective V(D, G) with
V({Dk}, G) = PK
k=2 V(Dk, G), see Eq. 9. Each objective
V(Dk, G) is given by Eq. 5, i.e., similar to original GAN
objective except that the inputs to discriminator are now
transformed, as discussed previously. Dk is trained to distinguish transformed real samples vs. transformed fake samples
(both transformed by same Tk).
G V(D, {Dk}, G) = V(D, G) +
Our generator objective is shown in Eq. 10. The generator
G learns to create samples to fool the discriminators D and
{Dk} simultaneously. The generator takes the random noise
z as input and maps into G(z) to confuse D as in standard
GAN. It is important as we want the generator to generate
only original images, not transformed images. Then, G(z)
is transformed by Tk to confuse Dk in the corresponding
task V(Dk, G). Here, V({Dk}, G) = PK
k=2 V(Dk, G). When
leveraging the transformed samples, the generator receives K
feed-back signals to learn and improve itself in the adversarial
mini-max game. If the generator wants its created samples
to look realistic, the transformed counterparts need to look
realistic also. The feedbacks are computed from not only
JS(Pd||Pg) of the original samples but also JS(P Tk
of the transformed samples as discussed in the next section.
In Eq. 9 and 10, λu and λv are constants.
1) Analysis on JS preserving: The invertible transformations ensure no discrepancy in the optimal convergence of
discriminators, i.e., Dk are trained to achieve the same optimal
k(Tk(x)) = D∗(x), ∀k (Refer to Eq. 6). Given these
optimal discriminators {D∗
k} at equilibrium point. For generator learning, minimizing V({Dk}, G) in Eq. 10 is equivalent
to minimizing Eq. 11:
k}, G) = const + 2
Furthermore, if all Tk are invertible, the r.h.s. of Eq. 11
becomes: const + 2(K −1) · JS(Pd||Pg). In this case, the
convergence of GAN is guaranteed. In this attempt, D, {Dk}
do not have any shared weights. Refer to Table III in Section
VI: when we use multiple discriminators D, {Dk} to handle
transformed samples by {Tk} respectively, the performance is
slightly improved to FID = 28.6 (“None” DAG) from Baseline
(FID = 29.6). This veriﬁes the advantage of JS preserving of
our model in generator learning. To further improve the design,
we propose to apply weight sharing for D, {Dk}, so that we
can take advantage of data augmentation, i.e. via improving
feature representation learning of discriminators.
2) Discriminator regularization via weight sharing: We
propose to regularize the learning of discriminators by enforcing weights sharing between them. Like IDA, discriminator
gets beneﬁt from the data augmentation to improve the representation learning of discriminator and furthermore, the model
preserves the same JS objective to ensure the convergence
of the original GAN. Note that the number of shared layers
between discriminators does not inﬂuence the JS preserving
property in our DAG (the same proofs about JS as in Section
V-B). The effect of number of shared layers will be examined
via experiments (i.e., in Table III in Section VI). Here, we
highlight that with discriminator regularization (on top of
JS preserving), the performance is substantially improved. In
practical implementation, D, {Dk} shared all layers except the
last layers to implement different heads for different outputs.
See Table III in Section VI for more details.
In this work, we focus on invertible transformation in image
domains. In the image domain, the transformation is invertible
if its transformed sample can be reverted to the exact original
image. For example, some popular afﬁne transformations in
image domain are rotation, ﬂipping or ﬂiprot (ﬂipping +
rotation), etc.; However, empirically, we ﬁnd out that our DAG
framework works favorably with most of the augmentation
techniques (even non-invertible transformation) i.e., cropping
and translation. However, if the transformation is invertible,
the convergence property of GAN is theoretically guaranteed.
Table I represents some examples of invertible and noninvertible transformations that we study in this work.
The usage of DAG outperforms the baseline GAN models
(refer to Section VI-B for details). Our DAG framework can
apply to various GAN models: unconditional GAN, conditional GAN, self-supervised GAN, CycleGAN. Speciﬁcally,
the same ideas can be applied: train the discriminator with
transformed real/fake samples as inputs; train the generator
with transformed fake samples to learn Pd; stack such modiﬁed models and apply weight sharing to leverage different
transformed samples. We select one state-of-the-art GAN system recently published and apply DAG. We refer to this
as our best GAN system; this system advances state-of-the-art
performance on benchmark datasets, as will be discussed next.
3) Difference from existing works with multiple discriminators: We highlight the difference between our work and
existing works that also uses multiple discriminators , ,
 : i) we use augmented data to train multiple discriminators,
ii) we propose the DAG architecture with invertible transformations that preserve the JS divergence as the original GAN.
Furthermore, our DAG is simple to implement on top of any
GAN models and potentially has no limits of augmentation
techniques or number discriminators to some extent. Empirically, the more augmented data DAG uses (adhesive to the
higher number of discriminators), the better FID scores it gets.
VI. EXPERIMENTS
We ﬁrst conduct the ablation study on DAG, then investigate
the inﬂuence of DAG across various augmentation techniques
on two state-of-the-art baseline models: Dist-GAN for
unconditional GAN and SS-GAN for self-supervised
GAN. Then, we introduce our best system by making use of
DAG on top of a recent GAN system to compare to the state
of the art.
Model training. We use batch size of 64 and the latent
dimension of dz = 128 in most of our experiments (except
in Stacked MNIST dataset, we have to follow the latent
dimension as in ). We train models using Adam optimizer
with learning rate lr = 2 × 10−4, β1 = 0.5, β2 = 0.9 for
DCGAN backbone and β1 = 0.0, β2 = 0.9 for Residual
Network (ResNet) backbone . We use linear decay over
300K iterations for ResNet backbone as in . We use our
best parameters: λu = 0.2, λv = 0.2 for SS-GAN and
λu = 0.2, λv = 0.02 for Dist-GAN. We follow to train the
discriminator with two critics to obtain the best performance
for SS-GAN baseline. For fairness, we implement DAG with
K = 4 branches for all augmentation techniques, and the
number of samples in each training batch are equal for DA
and DAG. In our implementation, Nt = 5 pixels for translation
and the cropping scale Nc = 0.75 for cropping (Table I).
Evaluation. We perform extensive experiments on datasets:
CIFAR-10, STL-10, and Stacked MNIST. We measure the
diversity/quality of generated samples via FID for CIFAR-
10 and SLT-10. FID is computed with 10K real samples and
5K generated samples as in if not precisely mentioned.
We report the best FID attained in 300K iterations as in ,
 , , . In FID ﬁgures, The horizontal axis is the
number of training iterations, and the vertical axis is the FID
score. We report the number of modes covered (#modes) and
the KL divergence score on Stacked MNIST similar to .
A. Ablation study
We conduct the experiments to verify the importance of
discriminator regularization, and JS preserving our proposed
DAG. In this study, we mainly use Dist-GAN and SS-GAN
as baselines and train on full (100%) CIFAR-10 dataset. For
DAG, we use K = 4 rotations. As the study requires expensive
computation, we prefer the small DC-GAN network (Refer to
Appendix C for details). The network backbone has four convlayers and 1 fully-connect (FC) layer.
1) The impacts of discriminator regularization: We validate
the importance of discriminator regularization (via shared
weights) in DAG. We compare four variants of DAG: i)
discriminators share no layers (None), ii) discriminators share
a half number of conv-layers (Half), which is two conv-layers
in current model, iii) discriminators share all layers (All), iv)
discriminators share all layers but FC (All but heads). Note
that the number of shared layers counts from the ﬁrst layer of
the discriminator network in this study. As shown in Table III,
comparing to Baseline, DA, and IDA, we can see the impacts
of shared weights in our DAG. This veriﬁes the importance of
discriminator regularization in DAG. In this experiment, two
settings: “Half” and “All but heads”, achieve almost similar
performance, but the latter is more memory-efﬁcient, cheap
and consistent to implement in any network conﬁgurations.
Therefore, we choose “All but heads” for our DAG setting
for the next experiments. Dist-GAN is the baseline for this
experiment.
THE ABLATION STUDY ON DISCRIMINATOR REGULARIZATION VIA THE NUMBER OF SHARED LAYERS (COUNTS FROM THE FIRST LAYER) IN OUR DAG
MODEL. ”NONE”: SHARING NO LAYERS AT ALL. ”HALF”: SHARING A HALF NUMBER OF LAYERS OF THE NETWORK. ”ALL BUT HEADS”: SHARING ALL
CONVOLUTIONAL LAYERS BUT DIFFERENT FC LAYERS. ”ALL”: SHARING ALL LAYERS. BASELINE: DIST-GAN.
Shared layers
All but heads
Fig. 4. The modiﬁed models with K branches from DAG: k = 2,...,K without
data augmentation in generator learning (represented by dot lines – note that
these are used in training Dk).
2) The importance of JS preserving and the role of transformations in generator learning of DAG: First, we compare
our DAG to IDA (see Table III). The results suggest that IDA
is not as good as DAG, which means that when JS divergence
is not preserved (i.e., minimizing lower-bounds in the case of
IDA), the performance is degraded. Second, when training the
generator, we remove branches Tk, i.e. in generator training,
no augmented sample is used (Fig. 4). We use DAG models
with rotation as the baselines and others are kept exactly the
same as DAG. Substantial degradation occurs as shown in
Table IV. This conﬁrms the signiﬁcance of augmented samples
in generator learning.
3) The importance of data augmentation in our DAG:
Tables V and VI represent the additional results of other
DAG methods comparing to the multiple discriminators (MD)
variant. MD is exactly the same as DAG as in Fig. 3b,
except that all the transformations Tk are removed, i.e. MD
does not apply augmented data. We use K = 4 branches for
all DAG methods. The experiments are with two baseline
models: DistGAN and SSGAN. We train DistGAN + MD and
SS-GAN + MD on full (100%) CIFAR-10 dataset. Using MD
indeed slightly improves the performance of Baseline, but
the performance is substantially improved further as adding
any augmentation technique (DAG). This study veriﬁes the
importance of augmentation techniques and our DAG in the
improvement of GAN baseline models. We use small DCGAN
(Appendix C) for this experiment.
4) The ablation study on the number of branches K of DAG:
We conduct the ablation study on the number of branches
K in our DAG, we note that using large K is adhesive to
combine more augmentations since each augmentation has the
limit number of invertible transformations in practice, i.e. 4 for
rotations (Table I). The Dist-GAN + DAG model is used for
this study. In general, we observe that the larger K is (by
simply combining with other augmentations on top of the
current ones), the better FID scores DAG gets as shown in
Table VII. However, there is a trade-off between the accuracy
and processing time as increasing the number of branches K.
(Refer to more details about the training time in Section VI-E).
We use small DCGAN (Appendix C) for this experiment.
B. Data Augmentation optimized for GAN
In this study, experiments are conducted mainly on the
CIFAR-10 dataset. We use small DC-GAN architecture (Refer
to Appendix C for details) to this study. We choose two
state-of-the-art models: SS-GAN , Dist-GAN as the
baseline models. The common augmentation techniques in
Table. I are used in the experiment. In addition to the full
dataset (100%) of the CIFAR-10 dataset, we construct the
subset with 25% of CIFAR-10 dataset (randomly selected) as
another dataset for our experiments. This small dataset is to
investigate how the models address the problem of limited
data. We compare DAG to DA and Baseline. DA is the
classical way of applying GAN on the augmented dataset
(similar to Section IV of our toy example) and Baseline is
training GAN models on the original dataset. Fig. 5 and Fig.
6 present the results on the full dataset (100%) and 25% of
datasets respectively. Figures in the ﬁrst row are with the SS-
GAN, and ﬁgures in the second row are with the Dist-GAN.
SS-GAN often diverges at about 100K iterations; therefore,
we report its best FID within 100K. We summarize the best
FID of these ﬁgures into Tables VIII.
First, we observe that applying DA for GAN does not
support GAN to learn Pd better than Baseline, despite few
exceptions with SS-GAN on the 25% dataset. Mostly, the
distribution learned with DA is too different from the original one; therefore, the FIDs are often higher than those of
the Baselines. In contrast, DAG improves the two Baseline
models substantially with all augmentation techniques on both
Second, all of the augmentation techniques used with DAG
improve both SS-GAN and Dist-GAN on two datasets. For
100% dataset, the best improvement is with the Fliprot. For
the 25% dataset, Fliprot is competitive compared to other
techniques. It is consistent with our theoretical analysis, and
invertible methods such as Fliprot can provide consistent
improvements. Note that, although cropping is non-invertible,
utilizing this technique in our DAG still enables reasonable
improvements from the Baseline. This result further corroborates the effectiveness of our proposed framework using a
range of data augmentation techniques, even non-invertible
Third, GAN becomes more fragile when training with fewer
data, i.e., 25% of the dataset. Speciﬁcally, on the full dataset
GAN models converge stably, on the small dataset they both
FID OF DISTGAN + DAG (ROTATION) AND SS-GAN + DAG (ROTATION) WITH AND WITHOUT AUGMENTED SAMPLES IN GENERATOR LEARNING. “-G”:
NO AUGMENTED SAMPLES IN G LEARNING.
DistGAN+DAG
DistGAN+DAG (-G)
SSGAN+DAG (-G)
Iterations
Rotation (100%)
SS-GAN + DA
SS-GAN + DAG
Iterations
Flipping (100%)
SS-GAN + DA
SS-GAN + DAG
Iterations
Cropping (100%)
SS-GAN + DA
SS-GAN + DAG
Iterations
Translation (100%)
SS-GAN + DA
SS-GAN + DAG
Iterations
Flipping+rotation (100%)
SS-GAN + DA
SS-GAN + DAG
Iterations
Rotation (100%)
Dist-GAN + DA
Dist-GAN + DAG
Iterations
Flipping (100%)
Dist-GAN + DA
Dist-GAN + DAG
Iterations
Cropping (100%)
Dist-GAN + DA
Dist-GAN + DAG
Iterations
Translation (100%)
Dist-GAN + DA
Dist-GAN + DAG
Iterations
Flipping+Rotation (100%)
Dist-GAN + DA
Dist-GAN + DAG
Fig. 5. Comparing DA and our proposed DAG with SS-GAN (ﬁrst row) and Dist-GAN (second row) baselines on full dataset (100%). Left to right
columns: rotation, ﬂipping, cropping, translation, and ﬂipping+rotation. The horizontal axis is the number of training iterations, and the vertical axis is the
FID score.
Iterations
Rotation (25%)
SS-GAN + DA
SS-GAN + DAG
Iterations
Flipping (25%)
SS-GAN + DA
SS-GAN + DAG
Iterations
Cropping (25%)
SS-GAN + DA
SS-GAN + DAG
Iterations
Translation (25%)
SS-GAN + DA
SS-GAN + DAG
Iterations
Flipping+rotation (25%)
SS-GAN + DA
SS-GAN + DAG
Iterations
Rotation (25%)
Dist-GAN + DA
Dist-GAN + DAG
Iterations
Flipping (25%)
Dist-GAN + DA
Dist-GAN + DAG
Iterations
Cropping (25%)
Dist-GAN + DA
Dist-GAN + DAG
Iterations
Translation (25%)
Dist-GAN + DA
Dist-GAN + DAG
Iterations
Flipping+Rotation (25%)
Dist-GAN + DA
Dist-GAN + DAG
Fig. 6. Comparing DA and our proposed DAG with SS-GAN (ﬁrst row) and Dist-GAN (second row) baselines on 25% of dataset. Left to right
columns: rotation, ﬂipping, cropping, translation, and ﬂipping+rotation. The horizontal axis is the number of training iterations, and the vertical axis is the
FID score.
FID OF DISTGAN + MD COMPARED WITH DISTGAN BASELINE AND
OUR DISTGAN + DAG METHODS.
DistGAN + MD
DistGAN + DAG (rotation)
DistGAN + DAG (ﬂipping)
DistGAN + DAG (cropping)
DistGAN + DAG (translation)
DistGAN + DAG (ﬂipping+rotation)
suffer divergence and mode collapse problems, especially
SS-GAN. This is consistent with recent observations ,
 , , : the more data GAN model trains on, the
higher quality it can achieve. In the case of limited data, the
performance gap between DAG versus DA and Baseline is
FID OF SSGAN + MD COMPARED WITH SSGAN BASELINE AND OUR
SSGAN + DAG METHODS.
SSGAN + MD
SSGAN + DAG (rotation)
SSGAN + DAG (ﬂipping)
SSGAN + DAG (cropping)
SSGAN + DAG (translation)
SSGAN + DAG (ﬂipping+rotation)
even larger. Encouragingly, with only 25% of the dataset, Dist-
GAN + DAG with FlipRot still achieves similar FID scores
as that of Baseline trained on the full dataset. DAG brings
more signiﬁcant improvements with Dist-GAN over SS-GAN.
Therefore, we use Dist-GAN as the baseline in comparison
THE ABLATION STUDY ON THE NUMBER OF BRANCHES K IN OUR DAG
MODEL. WE USE DIST-GAN + DAG AS THE BASELINE FOR THIS STUDY.
Number of branches
K = 4 (1 identity + 3 rotations)
K = 7 (1 identity + 3 rotations + 3 ﬂippings)
K = 10 (1 identity + 3 rotations + 3 ﬂippings + 3 croppings)
with state of the art in the next section.
We also test our best version with limited data (10%
dataset). Our best DAG (K = 10, see Table VII) archives
FID (=30.5) which is much better than baseline (=54.6) and
comparable to the baseline on 100% dataset (=29.6) in Table.
C. Comparison to state-of-the-art GAN
1) Self-supervised GAN + our proposed DAG: In this section, we apply DAG (FlipRot augmentation) to SS-DistGAN
 , a self-supervised extension of DistGAN. We indicate this
combination (SS-DistGAN + DAG) with FlipRot as our best
system to compare to state-of-the-art methods. We also report
(SS-DistGAN + DAG) with rotation to compare with previous
works , for fairness. We highlight the main results as
We report our performance on natural images datasets:
CIFAR-10, STL-10 (resized into 48 × 48 as in ). We
investigate the performance of our best system. We use ResNet
 , (refer to Appendix C) with “hinge” loss as it attains
better performance than standard “log” loss . We compare
our proposed method to other state-of-the-art unconditional
and conditional GANs. We emphasize that our proposed
method is unconditional and does not use any labels.
Main results are shown in Table IX. The best FID attained
in 300K iterations are reported as in , , , .
The ResNet is used for the comparison. We report our best
system (SS-DistDAN + DAG) with Rotation and FlipRot. The
improved performance over state-of-the-art GAN conﬁrms the
effectiveness of our proposed system.
In addition, in Table IX, we also compare our FID to those
of SAGAN and BigGAN (the current state-of-theart conditional GANs). We perform the experiments under
the same conditions using ResNet backbone on the CIFAR-
10 dataset. The FID of SAGAN is extracted from . For
BigGAN, we extract the best FID from the original paper.
Although our method does not use labeled data, our best FID
approaches these state-of-the-art conditional GANs which use
labeled data. Our system SS-DistDAN + DAG combines selfsupervision as in and optimized data augmentation to
achieve outstanding performance. Generated images using our
system can be found in Figures 7 of Appendix B.
2) Conditional GAN + our proposed DAG: We demonstrate
that our DAG can also improve the state-of-the-art conditional
GAN model, BigGAN . For this experiment, we apply
rotations (0, 90, 180, 270 degrees) as transformations, and
the model is trained with 60K iterations on limited datasets
of CIFAR-10 (10%, 20%), CIFAR-100 (10%, 20%), and
ImageNet (25%). As shown in Table X, when DAG is applied
to BigGAN, it can boost the performance of BigGAN considerably on these limited-size datasets. These experimental
results for comparison were obtained with a single Titan RTX
GPU with a batch size of 50.
3) Image-image translation + our proposed DAG: In this
experiment, we apply DAG to CycleGAN , a GAN method
for image-image translation. We use the public Pytorch code2
for our experiments. We follow the evaluation on Per-pixel
accuracy, Per-class accuracy, and Class IOU as in on the
Cityscapes dataset. Table XI shows that CycleGAN + DAG
achieves signiﬁcantly better performance than the baseline
for all three scores following the same setup. Note that we
use DAG with only four rotations. We believe using more
transformations will achieve more improvement. Note that (*)
means the results reported in the original paper. The others are
produced by the publicly available Pytorch code. The more
detail implementation can be found in Appendix A-B2.
4) Mode collapse on Stacked MNIST: We evaluate the
stability of SS-DistGAN + DAG and the diversity of its
generator on Stacked MNIST . Each image of this dataset
is synthesized by stacking any three random MNIST digits.
We follow the same setup with tiny architectures K = { 1
and evaluation protocol of . K indicates the size of the
discriminator relative to the generator. We measure the quality
of methods by the number of covered modes (higher is better)
and KL divergence (lower is better) . For this dataset, we
report for our performance and compare to previous works
as in Table. XII. The numbers show our proposed system
outperforms the state of the art for both metrics. The results are
computed from eight runs with the best parameters obtained
via the same parameter as previous experiments.
D. Medical images with limited data
We verify the effectiveness of our DAG on medical images with a limited number of samples. The experiment is
conducted using the IXI dataset3, a public MRI dataset. In
particular, we employ the T1 images of the HH subset (MRI
of the brain). We extract two subsets: (i) 1000 images from
125 random subjects (8 slices per subject) (ii) 5024 images
from 157 random subjects (32 slices per subject). All images
are scaled to 64x64 pixels. We use DistGAN baseline with
DCGAN architecture , DAG with 90-rotation, and report
the best FID scores. The results in Table XIII suggest that
DAG improves the FID score of the baseline substantially and
is much better than the baseline on the limited data.
E. Training time comparison
Our GAN models are implemented with the Tensorﬂow
deep learning framework . We measure the training time
of DAG (K=4 branches) on our machine: Ubuntu 18.04, CPU
Core i9, RAM 32GB, GPU GTX 1080Ti. We use DCGAN
baseline (in Section VI-B) for the measurement. We compare
models before and after incorporating DAG with SS-GAN and
2 
3 
TABLE VIII
BEST FID OF SS-GAN (ABOVE) AND DIST-GAN (BELOW) BASELINE, DA AND DAG METHODS ON THE CIFAR-10 DATASET. FLIPROT = FLIPPING +
ROTATION. WE USE K = 4 FOR ALL EXPERIMENTS (INCLUDING FLIPROT) AS DISCUSSED IN TABLE I FOR A FAIR COMPARISON.
Translation
Fig. 7. Real (left) and generated (right) examples by our best system on CIFAR-10 (two ﬁrst columns) and STL-10 (two last columns).
FID SCORES WITH RESNET ON CIFAR-10 AND STL-10 DATASETS.
THE FID SCORES ARE EXTRACTED FROM THE RESPECTIVE PAPERS WHEN
AVAILABLE. ‘*’: 10K-10K FID IS COMPUTED AS IN . ’+’: 50K-50K
FID IS COMPUTED. ALL COMPARED GANS ARE UNCONDITIONAL,
EXCEPT SAGAN AND BIGGAN. R: ROTATION AND F+R: FLIPROT.
SN-GAN 
21.70 ± .21
40.10 ± .50
SS-GAN 
DistGAN 
17.61 ± .30
28.50 ± .49
GN-GAN 
16.47 ± .28
MMD GAN+ 
Auto-GAN+ 
MS-DistGAN 
13.90 ± .22
27.10 ± .34
SAGAN (cond.)
BigGAN (cond.)
13.72 ± .15
25.69 ± .15
Ours (F+R)
13.20 ± .19
25.56 ± .15
Dist-GAN. SS-GAN: 0.14 (s) per iteration. DistGAN: 0.11 (s)
per iteration. After incorporating DAG, we have these training
times: SS-GAN + DAG: 0.30 (s) per iteration and DistGAN-
DAG: 0.23 (s) per iteration. The computation time is about
2× higher with adding DAG (K = 4) and about 5× higher
with adding DAG (K = 10). Because of that, we propose to
use K = 4 for most of the experiments which have a better
trade-off between the FID scores and processing time and also
is fair to compare to other methods. With K = 4, although the
processing 2× longer, DAG helps achieve good quality image
generation, e.g. 25% dataset + DAG has the same performance
as 100% dataset training, see our results of Dist-GAN + DAG
with ﬂipping+rotation. For most experiments in Section VI-B,
we train our models on 8 cores of TPU v3 to speed up the
VII. CONCLUSION
We propose a Data Augmentation optimized GAN (DAG)
framework to improve GAN learning to capture the distribution of the original dataset. Our DAG can leverage the
various data augmentation techniques to improve the learning stability of the discriminator and generator. We provide
theoretical and empirical analysis to show that our DAG
preserves the Jensen-Shannon (JS) divergence of original GAN
with invertible transformations. Our theoretical and empirical
analyses support the improved convergence of our design. Our
proposed model can be easily incorporated into existing GAN
models. Experimental results suggest that they help boost the
performance of baselines implemented with various network
architectures on the CIFAR-10, STL-10, and Stacked-MNIST
datasets. The best version of our proposed method establishes
state-of-the-art FID scores on all these benchmark datasets.
Our method is applicable to address the limited data issue for
GAN in many applications, e.g. medical applications.