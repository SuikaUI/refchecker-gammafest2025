Deep-Anomaly: Fully Convolutional Neural Network
for Fast Anomaly Detection in Crowded Scenes
M. Sabokroua,‚àó, M. Fayyazb,‚àó, M. Fathyc, Z. Moayedd, and R. Kletted
aSchool of Computer Science, Institute for Research in Fundamental Sciences (IPM)
P.o.Box 19395-5746, Tehran, Iran
bDi Hub, Brussels, Belgium
cIran University of Science and Technology, Tehran, Iran
dSchool of Engineering, Computer and Mathematical Sciences, EEE Department
Auckland University of Technology, Auckland, New Zealand
The detection of abnormal behaviours in crowded scenes has to deal with many
challenges. This paper presents an eÔ¨Écient method for detection and localization of anomalies in videos. Using fully convolutional neural networks (FCNs)
and temporal data, a pre-trained supervised FCN is transferred into an unsupervised FCN ensuring the detection of (global) anomalies in scenes. High performance in terms of speed and accuracy is achieved by investigating the cascaded
detection as a result of reducing computation complexities. This FCN-based
architecture addresses two main tasks, feature representation and cascaded outlier detection. Experimental results on two benchmarks suggest that detection
and localization of the proposed method outperforms existing methods in terms
of accuracy.
Video anomaly detection, CNN, transfer learning, real-time
processing
‚àóEqual Collaboration
Email addresses: (M. Sabokrou), (M. Fayyaz),
 (M. Fathy), (Z. Moayed), 
(and R. Klette)
 
May 2, 2017
 
1. Introduction
The use of surveillance cameras requires that computer vision technologies
need to be involved in the analysis of very large volumes of video data. The
detection of anomalies in captured scenes is one of the applications in this area.
Anomaly detection and localization is a challenging task in video analysis
already due to the fact that the deÔ¨Ånition of ‚Äúanomaly‚Äù is subjective, or contextdependent. In general, an event is considered to identify an ‚Äúanomaly‚Äù when it
occurs rarely, or unexpected; for example, see .
Compared to the previously published deep-cascade method in , this paper
proposes and evaluates a diÔ¨Äerent and new method for anomaly detection. Here
we introduce and study a modiÔ¨Åed pre-trained convolutional neural network
(CNN) for detecting and localizing anomalies. In diÔ¨Äerence to , the considered
CNN is not trained from scratch but ‚Äújust‚Äù Ô¨Åne-tuned.
More in detail, for
processing a video frame outlined a method where the frame was Ô¨Årst divided
into a set of patches, then the anomaly detection was organised based on levels of
patches. In diÔ¨Äerence to that, the input of the proposed CNN algorithm is a full
video frame in this paper. As a brief preview, the new method is methodically
simpler but faster in both the training and testing phase where the accuracy of
anomaly detection is comparable to the accuracy of the method presented in .
In the context of crowd scene videos, anomalies are formed by rare shapes
or rare motions. Due to the fact that looking for unknown shapes or motions
is a time-consuming task, state-of-the-art approaches learn regions or patches
of normal frames as reference models. Indeed, these reference models include
normal motion or shapes of every region of the training data. In the testing
phase, those regions which diÔ¨Äer from the normal model are considered to be
abnormal. Classifying these regions into normal and abnormal requires extensive sets of training samples in order to describe the properties of each region
eÔ¨Éciently.
There are numerous ways to describe region properties. Trajectory-based
methods have been used to deÔ¨Åne behaviours of objects. Recently, for mod-
eling spatio-temporal properties of video data, low-level features such as histogram of gradients (HoG) and histogram of optic Ô¨Çow (HoF) are used. These
trajectory-based methods have two main disadvantages. They cannot handle
occlusion problems, and they also suÔ¨Äer from high complexity, especially in
crowded scenes.
CNNs proved recently to be useful for deÔ¨Åning eÔ¨Äective data analysis techniques for various applications.
CNN-based approaches outperformed stateof-the-art methods in diÔ¨Äerent areas including image classiÔ¨Åcation , object
detection , or activity recognition . It is argued that handcrafted features
cannot eÔ¨Éciently represent normal videos . In spite of these beneÔ¨Åts,
CNNs are computationally slow, especially when considering block-wise methods . Thus, dividing a video into a set of patches and representing them
by using CNNs, should be followed by a further analysis about possible ways of
speed-ups.
Major problems in anomaly detection using CNNs are as follows:
1. Too slow for patch-based methods; thus, CNN is considered as being a
time-consuming procedure.
2. Training a CNN is totally supervised learning; thus, the detection of
anomalies in real-world videos suÔ¨Äers from a basic impossibility of training
large sets of samples from non-existing classes of anomalies.
Due to these diÔ¨Éculties, there is a recent trend to optimize CNN-based algorithms in order to be applicable in practice. Faster-RCNN takes advantage
of convolutional layers to have a feature map of every region in the input data,
in order to detect the objects. For semantic segmentation, methods such as
 use fully convolutional networks (FCNs) for traditional CNNs to extract
regional features. Making traditional classiÔ¨Åcation CNNs to work as a fully convolutional network and using a regional feature extractor reduces computation
costs. In general, as CNNs or FCNs are supervised methods, neither CNNs nor
FCNs are capable for solving anomaly detection tasks,
To overcome aforementioned problems, we propose a new FCN-based struc-
ture to extract the distinctive features of video regions. This new approach includes several initial convolutional layers of a pre-trained CNN using an AlexNet
model and an additional convolutional layer. AlexNet, similar to , is a
pre-trained model proposed for image classiÔ¨Åcation by using ImageNet 
and the MIT places dataset . Extracted features, by following this approach,
are suÔ¨Éciently discriminative for anomaly detection in video data.
In general, entire frames are fed to the proposed FCN. As a result, features
of all regions are extracted eÔ¨Éciently. By analysing the output, anomalies in
the video are extracted and localized. The processes of convolution and pooling,
in all of the CNN layers, run concurrently. A standard NVIDIA TITAN GPU
processes ‚âà370 frames per second (fps) when analyzing (low-resolution) frames
of size 320 √ó 240. This is considered to be ‚Äúvery fast‚Äù.
Convolution and pooling operations in CNNs are responsible for extracting
regions from input data using a speciÔ¨Åc stride and size.
These patch-based
operations provide a description for each extracted region. Detected features in
the output and the corresponding descriptors distinguish a potential region in
a set of video frames. Both convolution and pooling operations are invertible.
However, a roll-back operation generates a receptive Ô¨Åeld (a region in a frame)
from deeper layers to more shallow layers of the network. This receptive Ô¨Åeld
results in the generation of feature vectors.
In this paper, we propose a method for detecting and localizing abnormal
regions in a frame by analyzing the output of deep layers in an FCN. The idea of
localizing a receptive Ô¨Åeld is inspired by the faster-RCNN in , and OverFeat
in .
This paper uses the structure of a CNN for patch-based operations in order
to extract and represent all patches in a set of frames. A generated feature
vector, while using the CNN for each detected region, is Ô¨Åtted to the given
image classiÔ¨Åcation task.
Similar to , we use a transfer learning method to gain a better description
for each region.
We evaluate our method for Ô¨Ånding the best intermediate
convolutional layer of the CNN. Then, a new convolutional layer is added after
the best-performing layer of the CNN. The kernels of a pre-trained CNN are
adjusted based on pre-training, and considered to be constant in our FCN; the
parameters of the Ô¨Ånal new convolutional layer are trained based on our training
In other words, all regions generated by the pre-trained CNN are represented
by a sparse-auto-encoder as a feature vector of length h which is the hidden size
of the auto-encoder. We Ô¨Ånd that the feature set, generated by a pre-trained
CNN, is suÔ¨Éciently discriminative for modeling ‚Äúmany‚Äù regions. To make the
process more accurate, those regions which are classiÔ¨Åed with low conÔ¨Ådence, are
given to the Ô¨Ånal convolutional layer for further representation and classiÔ¨Åcation.
In fact, two Gaussian models are deÔ¨Åned based on the description of all
normal training regions. The Ô¨Årst model is generated by the kth layer of the
CNN, while the second model is based on its transformation by the (k + 1)th
convolutional layer.
In the testing phase, those regions which diÔ¨Äer signiÔ¨Åcantly from the Ô¨Årst
Gaussian model, are labeled as being a conÔ¨Ådent anomaly. Those regions which
Ô¨Åt completely to the Ô¨Årst model are labeled as being normal. The rest of the
regions, being by a minor diÔ¨Äerence below the threshold, are represented by
a sparse-auto-encoder and evaluated more carefully by the second Gaussian
model. This approach is similar to a cascade classiÔ¨Åer deÔ¨Åned by two stages; it
is explained in the next sections.
The main contributions of this paper are as follows:
‚Ä¢ To the best of our knowledge, this is the Ô¨Årst time that an FCN is used
for anomaly detection.
‚Ä¢ We adapt a pre-trained classiÔ¨Åcation CNN to an FCN for generating video
regions to describe motion and shape concurrently.
‚Ä¢ We propose a new FCN architecture for time-eÔ¨Écient anomaly detection
and localization.
‚Ä¢ The proposed method performs as well as state-of-the-art methods, but
our method outperforms those with respect to time; we have real-time for
typical applications.
‚Ä¢ We achieved a processing speed of 370 fps on a standard GPU; this is
about three times faster than the fastest existing method reported so far.
Section 2 provides a brief survey on existing work. We present the proposed
method in Section 3 including the overall scheme of our method, and also details for anomaly detection and localization, and for the evaluation of diÔ¨Äerent
layers of the CNN for performance optimization. Qualitative and quantitative
experiments are described in Section 4. Section 5 concludes the paper.
2. Related Work
Object trajectory estimation is often of interest in cases of anomaly detection; see . An object shows an anomaly if it
does not follow learned normal trajectories. This approach usually suÔ¨Äers from
many weaknesses, such as disability to eÔ¨Éciently handle occlusions, and being
too complex for processing crowded scenes.
To avoid these two weaknesses, it is proposed to use spatio-temporal low level
features such as optical Ô¨Çow or gradients. Zhang et al. use a Markov random
Ô¨Åeld (MRF) to model the normal patterns of a video with respect to a number
of features, such as rarity, unexpectedness, and relevance. Boiman and Irani 
consider an event as being abnormal if its reconstruction is impossible by using
previous observations only. Adam et al. use an exponential distribution for
modeling the histograms of optical Ô¨Çow in local regions.
A mixture of dynamic textures (MDT) is proposed by Mahadevan et al. 
for representing a video.
In this method, the represented features Ô¨Åt into a
Gaussian mixture model. In , the MDT is extended and explained in more
details. Kim and Grauman exploit a mixture of probabilistic PCA (MPPCA)
model for representing local optical Ô¨Çow patterns. They also use an MRF for
learning the normal patterns.
A method based on motion properties of pixels for behavior modeling is
proposed by Benezeth et al.
They described the video by learning a
co-occurrence matrix for normal events across space-time. In , a Gaussian
model is Ô¨Åtted into spatio-temporal gradient features, and a hidden Markov
model (HMM) is used for detecting the abnormal events.
Mehran et al. introduce social force (SF) as an eÔ¨Écient technique for
abnormal motion modeling of crowds. Detection of abnormal behaviors using
a method based on spatial-temporal oriented energy Ô¨Åltering in proposed by
Cong et al. construct an over-complete normal basis set from normal
data. A patch is considered to be abnormal if reconstructing it with this basis
set is not possible.
In , a scene parsing approach is proposed by Antic et al. All object hypotheses for the foreground of a frame are explained by normal training. Those
hypotheses, which cannot be explained by normal training, are considered to
show anomaly. Saligrama et al. in propose a method based on the clustering of the test data using optic-Ô¨Çow features. Ullah et al. introduced an
approach based on a cut/max-Ô¨Çow algorithm for segmenting the crowd motion.
If a Ô¨Çow does not follow the regular motion model, it is considered as being an
anomaly. Lu et al. propose a fast (140-150 fps) anomaly detection method
based on sparse representation.
In , an extension of the bag of video words (BOV) approach is used by
Roshtkhari et al.
A context-aware anomaly detection algorithm is proposed
in , where the authors represent the video using motions and the context
of videos. In , a method for modeling both motion and shape with respect
to a descriptor (named ‚Äúmotion context‚Äù) is proposed; they consider anomaly
detection as a matching problem.
Roshkhari et al. introduce a method
for learning the events of a video by using the construction of a hierarchical
codebook for dominant events in a video. Ullah et al. learn an MLP neural network using trained particles to extract the video behavior. A Gaussian
mixture model (GMM) is exploited for learning the behavior of particles using
extracted features. In addition, in , an MLP neural network for extracting
the corner features from normal training samples is proposed; authors also label
the test samples using that MLP.
Authors of extract corner features and analyze them based on their
properties of motion by an enthalpy model, a random forest with corner features for detecting abnormal samples. Xu et al. propose a uniÔ¨Åed anomaly
energy function based on a hierarchical activity-pattern discovery for detecting
anomalies.
Work reported in models normal events based on a set of representative
features which are learned on auto-encoders . They use a one-class classiÔ¨Åer
for detecting anomalies as being outliers compared to the target (normal) class.
See also the beginning of Section 1 where we brieÔ¨Çy reviewed work reported in
 ; this paper proposes a cascaded classiÔ¨Åer which takes advantage of two deep
neural networks for anomaly detection. Here, challenging patches are identiÔ¨Åed
at Ô¨Årst by using a small deep network; then the neighboring patches are passed
into another deep network for further classiÔ¨Åcation.
In , the histogram of oriented tracklets (HOT) is used for video representation and anomaly detection. A new strategy for improving HOT is also
introduced in this paper.
Yuan et al. propose an informative structural
context descriptor (SCD) to represent a crowd individually.
In this work, a
(spatial-temporal) SCD variation of a crowd is analyzed to localize the anomaly
An unsupervised deep learning approach is used in for extracting anomalies in crowded scenes. In this approach, shapes and features are extracted using a PCANet from 3D gradients. Then, a deep Gaussian mixture model
(GMM) is used to build a model that deÔ¨Ånes the event patterns. A PCANet
is also used in .
In this study, authors exploit the human visual system
(HVS) to deÔ¨Åne features in the spatial domain. On the other hand, a multiscale histogram of optical Ô¨Çow (MHOF) is used to represent motion features of
the video. PCANet is adopted to exploit these spatio-temporal features in order
to distinguish abnormal events.
A hierarchical framework for local and global anomaly detection is proposed
in . Normal interactions are extracted by Ô¨Ånding frequent geometric relationships between sparse interest points; authors model the normal interaction
template by Gaussian process regression. Xiao et al. exploit sparse seminonnegative matrix factorization (SSMF) for learning the local pattern of pixels.
Their method learns a probability model by using local patterns of pixels for
considering both the spatial and temporal context.
Their method is totally
unsupervised. Anomalies are detected by the learned model.
In , an eÔ¨Écient method for representing human activities in video data
with respect to motion characteristics is introduced and named as motion inÔ¨Çuence map. Those blocks of a frame which have a low occurrence are labelled as
being abnormal. A spatio-temporal CNN is developed in [? ] to deÔ¨Åne anomalies in crowded scenes; this CNN model is designed to detect features in both
spatial and temporal dimensions using spatio-temporal convolutions.
Li et al. propose an unsupervised framework for detecting the anomalies
based on learning global activity patterns and local salient behavior patterns
via clustering and sparse coding.
3. Proposed Method
This section explains at Ô¨Årst the overall outline of the method.
detailed description of the proposed method is given.
3.1. Overall Scheme
Abnormal events in video data are deÔ¨Åned in terms of irregular shapes or
motion, or possibly a combination of both. As a result of this deÔ¨Ånition, identifying the shapes and motion is an essential task for anomaly detection and
localization. In order to identify the motion properties of events, we need a series of frames. In other words, a single frame does not include motion properties;
it only provides shape information of that speciÔ¨Åc frame.
For analyzing both shape and motion, we consider the pixel-wise average
of frame It and previous frame It‚àí1, denoted by I‚Ä≤
t (not to be confused with a
derivative),
t(p) = It(p) + It‚àí1(p)
where It is tth frame in the video. For detecting anomalies in It, we use the
sequence Dt = ‚ü®I‚Ä≤
We start with this sequence Dt when representing video frames on grids of
decreasing size w√óh. Dt is deÔ¨Åned on a grid ‚Ñ¶0 of size w0√óh0. The sequence Dt
is subsequently passed on to an FCN, deÔ¨Åned by the kth intermediate convolutional layer, for k = 0, 1, . . . , L, each deÔ¨Åned on a grid ‚Ñ¶k of size wk √óhk, where
wk > wk+1, and hk > hk+1. We use L = 3 for the number of convolutional
The output of the kth intermediate convolutional layer of the FCN are feature
vectors fk ‚ààRmk (i.e. each containing mk real feature values), satisfying mk ‚â§
mk+1, starting with m0 = 1. For the input sequence Dt, the output of the kth
convolutional layer is a matrix of vector values:
k(i, j, 1 : mk)
(i,j)=(1,1) =
k(i, j, 1), . . . , f t
k(i, j, mk)
‚ä§o (wk,hk)
(i,j)=(1,1)
Each feature vector f t
k(i, j, 1 : mk) is derived from a speciÔ¨Åc receptive Ô¨Åeld (i.e.
a sub-region of input Dt).
In other words, Ô¨Årst, a high-level description of Dt is provided for the tth
frame of the video. Second, Dt is represented subsequently by the kth intermediate convolutional layer of the FCN, for k = 1, . . . , L. This representation is
used for identifying a set of partially pairwise overlapping regions in ‚Ñ¶k, called
the receptive Ô¨Åelds. Hence, we represent frame It at Ô¨Årst by sequence Dt on ‚Ñ¶0,
and then by mk maps
k(i, j, l)
(i,j)=(1,1) , for l = 1, 2, . . . , mk
on ‚Ñ¶k, for k = 1, . . . , L. Recall that the size wk √ó hk decreases with increases
of k values.
Suppose that we have q training frames from a video which are considered
to be normal. To represent these normal frames with respect to the kth convolutional layer of the FCN (AlexNet without its fully connected layers), we have
wk √ó hk √ó q vectors of length mk, deÔ¨Åning our 2D normal region descriptions;
they are generated automatically by a pre-trained FCN . For modeling the normal behavior, a Gaussian distribution is Ô¨Åtted as a one-class classiÔ¨Åer to the
descriptions of normal regions so that it deÔ¨Ånes our normal reference model. In
the testing phase, a test frame It is described in a similar way by a set of regional features. Those regions which diÔ¨Äer from the normal reference model are
labeled as being abnormal. In particular, the features generated by a pre-trained
CNN (2nd layer of AlexNet) are suÔ¨Éciently discriminative. These features are
learned based on a set of independent images which are not necessarily related
to video surveillance applications only.
Consequently, suspicious regions are represented by a ‚Äúmore discriminant‚Äù
feature set. This new representation leads to a better performance for distinguishing abnormal regions from normal ones. In other words, we transform the
generated features by AlexNet into an anomaly detection problem. This work
is done by an auto-encoder which is trained on all normal regions. As a result,
those suspicious f t
k(i, j, 1 : mk) regions are passed to an auto-encoder to have a
better representation. This is done by the (k + 1)st convolutional layer whose
kernels are learned by a sparse auto-encoder.
k(i, j, 1 : mk) be the transformed representation of f t
k(i, j, 1 : mk) by
a sparse auto-encoder; see Figure 1.
The abnormal region is visually more
distinguishable in the heat-map when the regional descriptors are represented
again by the auto-encoder (i.e. the Ô¨Ånal convolutional layer).
Then, for the new feature space, those regions which diÔ¨Äer from the normal
reference model are labeled as being abnormal. This proposed approach ensures
both accuracy and speed.
Suppose that f(i, j, 1 : mk) ‚ààRmk is the description of an abnormal region.
By moving backward from the kth to the 1st layer of the FCN, we can identify
regions in input frames with descriptions f t
k(i, j, 1 : mk). This is due to the fact
that convolution and mean pooling operator of the FCN (from 1st to 2nd layer)
are approximately invertible.
For instance, the 1st and 2nd convolutional layer, and the 1st sub-sampling
Figure 1: EÔ¨Äect of representing receptive Ô¨Åelds with an added convolutional layer.
Input frame. Middle: Heat-map visualisation of the 2nd layer of a pre-trained FCN. Right:
Heat-map visualisation of the 3nd layer of a pre-trained FCN with added convolutional layer.
layer are called C1, C2, and S1, respectively. As usual, (.)‚àí1 identiÔ¨Åes below the
inverse of a function. The exact location of description f t
k(i, j, 1 : mk) in the Dt
sequence (the input of the FCN) is located at C‚àí1
2 (f(i, j, 1 : mk))).
See the following sections for more details.
Figure 2 shows the work-Ô¨Çow of the proposed detection method. First, input
frames are passed on to a pre-trained FCN. Then, hk √ó wk regional feature
vectors are generated in the output of the kth layer. These feature vectors are
veriÔ¨Åed using Gaussian classiÔ¨Åer G1. Those patches, which diÔ¨Äer signiÔ¨Åcantly
ùë°(ùëñ, ùëó)1..ùëöùëò
ùë°(1,1)1..ùëöùëò
ùë°(ùëò, ùëò)1..ùëöùëò
ùë°(1,1,1: ùëöùëò)
ùë°(ùëñ, ùëó, 1: ùëöùëò)
ùë°(ùë§ùëò, ‚Ñéùëò, 1: ùëöùëò)
ùë°(1,1,1:‚Ñé)
ùë°(ùëñ, ùëó, 1:‚Ñé)
ùë°(ùë§ùëò, ‚Ñéùëò, 1: ‚Ñé)
Transferring description
of suspicious proposal
ùë°(ùëñ‚Ä≤, ùëó‚Ä≤,1:ùëöùëò))>Œ±
ùë°(ùëñ‚Ä≤, ùëó‚Ä≤,1:‚Ñé))>œï
Œ± >ùëë(ùê∫1, ùëìùëò
ùë°(ùëñ, ùëó, 1: ùëöùëò))>Œ≤
Localizing the (ùëñ‚Ä≤, ùëó‚Ä≤)
Sparse auto-encoder
Figure 2: Schematic sketch of the proposed method. (1) Input video frame of size w0 √ó h0.
(2,3) Description of regions of size hk √ó wk generated by the kth layer of the FCN. (4)
Transformed feature domain using a sparse auto-encoder. (5) Joint anomaly detector. (6)
Location of descriptions which identify anomalies.
from G1 as a normal reference model, are labeled as being abnormal. More
speciÔ¨Åcally, G1 is a Gaussian distribution which is Ô¨Åtted to all of the normal
extracted regional feature vectors; regions which completely diÔ¨Äer from G1 are
considered to be an anomaly.
Those suspicious regions which are Ô¨Åtted with low conÔ¨Ådence are given to a
sparse auto-encoder. At this stage, we also label these regions based on Gaussian
classiÔ¨Åer G2 which works similar to G1. G2 is also a Gaussian classiÔ¨Åer, trained
on all extracted regional feature vectors from training video data which are
represented by an auto-encoder. Finally, the location of those abnormal regions
can be annotated by a roll-back on the FCN.
3.2. Anomaly Detection
In this paper, the video is represented using a set of regional features. These
features are extracted densely and their description is given by feature vectors
in the output of the kth convolutional layer. See Equ. (2).
Gaussian classiÔ¨Åer G1(.) is Ô¨Åtted to all normal regional features generated by
the FCN. Those regional features for which their distance to G1(.) is bigger than
threshold Œ± are considered to be abnormal. Those ones that are compatible to
G1 (i.e. their distance is less than threshold Œ≤) are labeled as being normal. A
region is suspicious if it has a distance to G1 being between Œ± and Œ≤.
All suspicious regions are given to the next convolutional layer which is
trained on all normal regions generated by the pre-trained FCN. The new representation of these suspicious regions is more discriminative and denoted by
k(i, j, n)
(i,j)=(1,1) , for n = 1, 2, . . . , h
where h is the size of the feature vectors generated by the auto-encoder, which
equals the size of the hidden layers.
In this step, only the suspicious regions are processed. Thus, some points
(i, j) in grid (wk, hk) are ignored and not analysed in the grid (w‚Ä≤
k). Similar
to G1, we create a Gaussian classiÔ¨Åer G2 on all of the normal training regional
features which are represented by our auto-encoder. Those regions which are
not suÔ¨Éciently Ô¨Åtted to G2 are considered to be abnormal.
Equations (5) and (6) summarize anomaly detection by using two Ô¨Åtted
Gaussian classiÔ¨Åers. First, we have that
k(i, j, 1 : mk)) =
if d(G1, f t
k(i, j, 1 : mk)) ‚â§Œ≤
Suspicious
if Œ≤ < d(G1, f t
k(i, j, 1 : mk)) < Œ±
if d(G1, f t
k(i, j, 1 : mk)) ‚â•Œ±
Then, for a suspicious region represented by T t
k(i, j, 1 : h), we have that:
k(i, j, 1 : hk)) =
if d(G2, T t
k(i, j, 1 : h)) ‚â•œÜ
Here, d(G, x) is the Mahalanobis distance of a regional feature vector x from
the G-model.
3.3. Localization
The Ô¨Årst convolutional layer has m1 kernels of size x1 √ó y1. They are convolved on sequence Dt for considering the tth frame. As a result of this convolution, a feature is extracted.
Recall that each region for the input of the FCN is described by a feature
vector of length m1. In this continuous process, we have mk maps as output
for the kth layer.
Consequently, a point in the output of the kth layer is a
description for a subset of overlapping (x1 √ó y1)th receptive Ô¨Åelds in the input
of the FCN.
The order of layers in the modiÔ¨Åed version of AlexNet is denoted by
AlexNet Order ‚Üí[C1, S1, C2, S2, C3, fc1, fc2]
where C and S are a convolutional layer and a sub-sampling layer, respectively.
The two Ô¨Ånal layers are fully connected.
Assume that n regional feature vectors (i1, j1) ¬∑ ¬∑ ¬∑ (in, jn), generated in layer
Ck on grid ‚Ñ¶k, are identiÔ¨Åed as showing an anomaly. The location (i, j) in ‚Ñ¶k
corresponds to
1 (¬∑ ¬∑ ¬∑ S‚àí1
k (i, j)))
as the rectangular region in the original frame.
Suppose we have mk kernels of size xk √ó yk which are then convolved with
stride d on the output of the previous layer of Ck. C‚àí1
k (i, j) is the (rectangular)
set of all locations in ‚Ñ¶k‚àí1 which are mapped in the FCN on (i, j) in ‚Ñ¶k.
Function S‚àí1
is deÔ¨Åned in an analogous way.
The sub-sampling (mean pooling) layer can also be considered as a convolutional layer which has only one kernel. Any region, detected as being an
abnormal region in the original frame (i.e. in ‚Ñ¶0), is then a combination of some
overlapping and large patches. This leads to a poor localization performance.
As a case in point, a detection in the 2nd layer causes 51 √ó 51 overlapping
receptive Ô¨Åelds. To achieve more accuracy in anomaly detection, those pixels in
‚Ñ¶0 are identiÔ¨Åed to show an anomaly which are covered by more than Œ∂ related
receptive Ô¨Åelds (we decided for Œ∂=3 experimentally).
3.4. FCN Structure for Anomaly Detection
This section analyses the quality of diÔ¨Äerent layers of a pre-trained CNN
for generating regional feature vectors. We adapt (in this paper in general) a
classiÔ¨Åcation by CNN into an FCN by solely using convolutional layers. Selecting
the best layer for representing the video is crucial considering the following two
(1) Although deeper features are usually more discriminative, using these
deeper features is time-consuming. In addition, since the CNN is trained
for image classiÔ¨Åcation, going deeper may create over-Ô¨Åtted features for
image classiÔ¨Åcation.
(2) Going deeper leads to larger receptive Ô¨Åelds in the input data; as a result,
the likelihood of inaccurate localization increases which then has inverse
eÔ¨Äects on performance.
For the Ô¨Årst two convolutional layers of our FCN model, we use a modiÔ¨Åed
version of AlexNet named CaÔ¨Äe reference model.1
This model is trained on 1183 categories, each with 205 scene categories from
the MIT places database , and 978 object categories from the train data of
ILSVRC2012 (ImageNet) having 3.6 million images.
The implemented FCN has three convolutional layers. For Ô¨Ånding the best
convolutional layer k, we set initially k to 1, and then increase it to 3. When
the best k is decided, deeper layers are ignored.
The general Ô¨Åndings are described at an abstract level. First we use the output of layer C1. For distinguishing abnormal from normal regions, corresponding receptive Ô¨Åelds are small in size, and generated features are not capable of
achieving the suitable results. Therefore, here we have lots of false positives.
Later, the output of C2 is used as a deeper layer. At this stage, we achieve better performance compared to C1 due to the following reasons: A corresponding
receptive Ô¨Åeld in the input frames of C1 is now suÔ¨Éciently large, and the deeper
features are more discriminative.
At k = 3, we have the results in layer C3 as output. Although the capacity
of the network increases, results are not as good as for the 2nd convolutional
layer. It seems that by adding one more layer, we achieved deeper features;
however, these features are also likely to over-Ô¨Åt the image classiÔ¨Åcation tasks
since the network is trained for ImageNet.
Consequently, we decided for the C2 output for extracting regional features.
Similar to , we transformed the description of each generated regional feature
using a convolutional layer; the kernels of the layer are learned using a sparse
auto-encoder. This new layer is called CT that is on top of the C2 layer of the
CNN. The combination of three (initial) layers of a pre-trained CNN (i.e. C1, S1,
and C2) with an additional (new) convolutional layer is our new architecture for
detecting anomalies. Figure 3 shows the proposed FCN structure. To emphasise
further the eÔ¨Äects of using this structure, see Tables 1 to 3.
1 CaÔ¨Äe is a framework maintained by UC Berkeley .
Mean Pooling
st Convolution Output
nd Convolution Output
rd Convolution Output
Fixed part
Trainable part
Proposed FCN structure for detecting anomalies.
This FCN is only used for
regional feature extraction. At later stages, two Gaussian classiÔ¨Åers are embedded for labeling
abnormal regions.
Table 1 shows the performance of diÔ¨Äerent layers of the pre-trained CNN.
Table 1: Evaluating CNN convolutional layers for anomaly detection
Output in C1
Output in C2
Output in C3
Proposed size
Frame-level EER
Pixel-level EER
Table 2 reports the performance of using the proposed architecture with
diÔ¨Äerent numbers of kernels in the (k + 1)th convolutional layer. We represent
video frames with our FCN. A Gaussian classiÔ¨Åer is exploited at the Ô¨Ånal stage
of the FCN (see the performance for 100, 256, and 500 kernels in Table 2).
We also evaluated the performance when two Gaussian classiÔ¨Åers are used in a
similar approach to a cascade. The frame-level and pixel-level EER measures
Table 2: EÔ¨Äect of the number of kernels in the (k + 1)th convolutional layer, used for representing regional features when using C2 as outputs
Number of kernels
500 & two classiÔ¨Åers
Frame-level EER
are introduced in the next section. Recall that the smaller values of EER, the
‚Äúbetter‚Äù performance.
Table 3 reports the performance of processing the network outputs in C2
output and CT output with cascaded classiÔ¨Åers.
Table 3: EÔ¨Äect of adding the (k + 1)th convolutional layer, used for representing regional
features when using C2 for outputs
CT and two classiÔ¨Åers
Frame-level EER
Our results when evaluating the diÔ¨Äerent CNNs conÔ¨Årm that the proposed
CNN architecture is the best architecture for the studied data.
4. Experimental Results
We evaluate the performance of the proposed method on UCSD and
Subway benchmarks . We show that our proposed method detects anomalies
at high speed, similar to a real-time method in video surveillance, with equal or
even better performance than other state-of-the-art methods.
For implementing our deep-anomaly architecture we use the CaÔ¨Äe library
 . All experiments are done using a standard NVIDIA TITAN GPU with
MATLAB 2014a.
4.1. UCSD and Subway Datasets
To evaluate and compare our experimental results, we use two datasets.
UCSD Ped2 . Dominant dynamic objects in this dataset are walkers
where crowd density varies from low to high. An appearing object such as a
car, skateboarder, wheelchair, or bicycle is considered to create an anomaly. All
training frames in this dataset are normal and contain pedestrians only. This
dataset has 12 sequences for testing, and 16 video sequences for training, with
320 √ó 240 resolution. For evaluating the localization, the ground truth of all
test frames is available. The total numbers of abnormal and normal frames are
‚âà2,384 and ‚âà2,566, respectively.
Subway . This dataset contains two sequences recorded at the entrance
(1 h and 36 min, 144,249 frames) and exit (43 min, 64,900 frames) of a subway
station. People entering and exiting the station usually behave normally. Abnormal events are deÔ¨Åned by people moving in the wrong direction (i.e. exiting
the entrance or entering the exit), or avoiding payment. This dataset has two
limitations: The number of anomalies is low, and there are predictable spatial
localizations (at entrance or exit regions).
4.2. Evaluation Methodology
We compare our results with state-of-the-art methods using a receiver operating characteristic (ROC) curve, the equal error rate (EER), and the area under
curve (AUC). Two measures at frame level and pixel level are used, which are
introduced in and often exploited in later work. According to these measures, frames are considered to be abnormal (positive) or normal (negative).
These measures are deÔ¨Åned as follows:
(1) Frame-level: In this measure, if one pixel detects an anomaly then it is
considered to be abnormal.
(2) Pixel-level: If at least 40 percent of anomaly ground truth pixels are covered by pixels that are detected by the algorithm, then the frame is considered to show an anomaly.
4.3. Qualitative and Quantitative Results
Figure 4 illustrates the output of the proposed system on the samples of
the UCSD and Subway dataset. The proposed method detects and localizes
anomalies correctly in these samples. The main problem of an anomaly detection
system is a high rate of false-positives.
Figure 5 shows regions which are wrongly detected as being an anomaly
using our method. Actually, false-positives occur in two situations: too crowded
Output of the proposed method on Ped2 UCSD and Subway dataset. A-left and
B-left: Original frames. A-Right and B-Right: Anomaly regions are indicated by red.
Some examples of false-positives in our system. Left: A pedestrian walking in
opposite direction to other people. Middle: A crowded region is wrongly detected as being an
anomaly. Right: People walking in diÔ¨Äerent directions.
scenes, and when people walk in diÔ¨Äerent directions. Since walking in opposite
direction of other pedestrians is not observed in the training video, this action
is also considered as being abnormal using our algorithm.
Frame-level and pixel-level ROCs of the proposed method in comparison to
state-of-the-art methods are provided in Figure 6; left and middle for frame-level
and pixel-level EER on UCSD Ped2 dataset, respectively. The ROCs show that
the proposed method outperforms the other considered methods in the UCSD
Table 4 compares the frame-level and pixel-level EER of our method and
other state-of-the-art methods. Our frame-level EER is 11%, where the best
result in general is 10%, achieved by Tan Xiao et al. . We outperform all
other considered methods except . On the other hand, the pixel-level EER of
the proposed approach is 15%, where the next best result is 17%. As a result, our
method achieved a better performance than any other state-of-the-art method
in the pixel-level EER metric by 2%.
ROC comparison with state-of-the-art methods. Upper left: Frame-level of UCSD
Ped2. Bottom left: Pixel-level of UCSD Ped2. Upper right: Subway dataset.
The frame-level ROC of the Subway dataset is shown in Figure 6 (right). In
this dataset, we evaluate our method in both the entrance and exit scenes. The
ROC conÔ¨Årms that our method has a better performance than MDT and
SRC methods. We also discuss the comparison of AUC and EER in this
dataset in Table 5.
For the exit scene, we outperform the other considered methods in respect
to both AUC and EER measures; we outperform by 0.5% and 0.4% in AUC and
EER, respectively. For the entrance scenes, the AUC of the proposed method
achieves better results compared to all other methods by 0.4%. The proposed
method gains better outcomes in terms of EER for all methods except Saligrama
et al. ; they achieve better results by 0.3%.
Table 4: EER for frame and pixel level comparisons on Ped2; we only list Ô¨Årst author in this
table for reasons of available space
Frame-level
Pixel-level
Frame-level
Pixel-level
Reddy 
Bertini 
Saligrama 
MPCCA 
Dan Xu 
MPCCA+SF 
Zaharescu 
Tan Xiao 
Sabokrou 
Table 5: AUC-EER comparison on Subway dataset
Saligrama et al. 
4.4. Run-time Analysis
For processing a frame, three steps need to be performed: Some pre-processing
such as resizing the frames and constructing the input of the FCN, and representing the input by the FCN are considered as the Ô¨Årst and second step,
respectively. In the Ô¨Ånal step, the regional descriptors must be checked by a
Gaussian classiÔ¨Åer.
With respect to these three steps, run-time details of our proposed method
for processing a single frame are provided in Table 6. The total time for detecting
Table 6: Details of run-time (second/frame)
Pre-processing
Representation
Classifying
Time (in sec)
an anomaly in a frame is ‚âà0.0027 sec. Thus, we achieve 370 fps, and this is
much faster than any of the other considered state-of-the-art methods.
Table 7 shows the speed of our method in comparison to other approaches.
There are some key points which make our system fast. The proposed method
beneÔ¨Åts from fully convolutional neural networks.
These types of networks
perform feature extraction and localization concurrently. This property leads
to less computations.
Table 7: Run-time comparison on Ped2 (in sec)
Roshtkhari et al.
Xiao et al.
Furthermore, by combining six frames into a three-channel input, we process
a cubic patch of video frames at just one forward-pass. As mentioned before,
for detecting abnormal regions, we only process two convolutional layers, and
for some regions we classify them using a sparse auto-encoder. Processing these
shallow layers results in reduced computations. Considering these tricks, besides
processing fully convolutional networks in parallel, results in faster processing
for our system compared to other methods.
5. Conclusions
This paper presents a new FCN architecture for generating and describing
abnormal regions for videos.
By using the strength of FCN architecture for
patch-wise operations on input data, the generated regional features are contextfree. Furthermore, the proposed FCN is a combination of a pre-trained CNN
(an AlexNet version) and a new convolutional layer where kernels are trained
with respect to the chosen training video. This Ô¨Ånal convolutional layer of the
proposed FCN needs to be trained. The proposed approach outperforms existing
methods in processing speed. Besides, it is a solution for overcoming limitations
in training samples used for learning a complete CNN. This method enables us
to run a deep learning-based method at a speed of about 370 fps. Altogether,
the proposed method is both fast and accurate for anomaly detection in video
Acknowledgement
This research was in part supported by a grant from IPM. (No. CS1396-5-01)
References