IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 22, NO. 5, MAY 2014
Natural Language Generation as Incremental
Planning Under Uncertainty: Adaptive Information
Presentation for Statistical Dialogue Systems
Verena Rieser, Oliver Lemon, and Simon Keizer
Abstract—We present and evaluate a novel approach to natural
language generation (NLG) in statistical spoken dialogue systems
(SDS) using a data-driven statistical optimization framework
for incremental information presentation (IP), where there is a
trade-off to be solved between presenting “enough" information
to the user while keeping the utterances short and understandable.
The trained IP model is adaptive to variation from the current
generation context (e.g. a user and a non-deterministic sentence
planner), and it incrementally adapts the IP policy at the turn
level. Reinforcement learning is used to automatically optimize
the IP policy with respect to a data-driven objective function.
In a case study on presenting restaurant information, we show
that an optimized IP strategy trained on Wizard-of-Oz data
outperforms a baseline mimicking the wizard behavior in terms of
total reward gained. The policy is then also tested with real users,
and improves on a conventional hand-coded IP strategy used in
a deployed SDS in terms of overall task success. The evaluation
found that the trained IP strategy signiﬁcantly improves dialogue
task completion for real users, with up to a 8.2% increase in
task success. This methodology also provides new insights into
the nature of the IP problem, which has previously been treated
as a module following dialogue management with no access to
lower-level context features (e.g. from a surface realizer and/or
speech synthesizer).
Index Terms—Information presentation, natural language generation, natural language user interfaces, reinforcement learning.
I. INTRODUCTION
ATURAL language allows us to achieve the same communicative goal (“what to say”) using many different expressions (“how to say it”). In a spoken dialogue system (SDS),
an abstract communicative goal (CG) can be generated in many
different ways. For example, the CG to present search results to
the user can be realized as a summary , , or by comparing
Manuscript received January 18, 2013; revised September 11, 2013;
accepted October 21, 2013. Date of publication April 03, 2014; date of
current version April 24, 2014. This work was supported by the European
Community’s FP7 programmes: under grant agreement no.
216594 (CLASSiC); under grant agreement no. 270019
(SPACEBOOK); under grant agreement no. 269427 (STAC);
and supported in part by the Engineering and Physical Sciences Research
Council, U.K., (EPSRC) under project no. EP/G069840/1. The associate editor
coordinating the review of this manuscript and approving it for publication was
Dr. Tatsuya Kawahara.
The authors are with the School of Mathematical and Computer Sciences
(MACS), Heriot-Watt University, Edinburgh EH14 4AS, U.K. (e-mail:
 ).
Color versions of one or more of the ﬁgures in this paper are available online
at 
Digital Object Identiﬁer 10.1109/TASL.2014.2315271
items , or by picking one item and recommending it to the
user . Previous work has shown that it is useful to adapt the
generated output to certain features of the dialogue context, for
example user preferences, e.g. , , user knowledge, e.g. ,
 , or predicted TTS quality, e.g. , .
With spoken dialogue systems now employing more statistical, e.g. , and incremental techniques, e.g. , language
generation faces new challenges. First, in fully statistical
dialogue systems, all components can introduce uncertainty,
i.e. other components cannot know for sure how “higher up”
or “lower down” components in the dialogue system pipeline
will perform, but may only have an estimate of their likely behavior. See for example the system developed by the CLASSiC
project1 . As such, the uncertainty the system has to deal
with is not only restricted to automatic speech recognition
(ASR), but all modules can introduce uncertainty. Strategy
optimization therefore needs to also consider adaptation to
uncertainty from input modules and the variation that is introduced by “lower level” modules such as utterance realization
and speech synthesis. For example, the NLG system might
consider recognition conﬁdence in slot values as well as
variations in predicted text-to-speech (TTS) quality , .
Secondly, NLG for incremental dialogue systems needs to
be able to have the ability to accommodate user barge-ins .
Note that the timing of when to start generation is usually determined by the dialogue manager. However, barge-ins are controlled by the user and thus, incremental systems need to consider possible user barge-ins any time. If the user decides not
to interrupt the system, NLG content selection has the choice
whether to continue generation or whether to stop. As such, incremental planning for generation requires an estimate of “ what
would happen if we stop generating now?,” i.e. an estimate of
dynamically changing next user actions.
To deal with these requirements, we propose a new model
which treats NLG as a statistical incremental planning problem,
enabling automatic adaptation to the dialogue context, analogously to current statistical approaches to dialogue management
(DM), e.g. , , , .
In NLG we have similar trade-offs and unpredictability as in
dialogue management, and in some systems the content planning and DM tasks are combined. For example, on the one hand,
very long system utterances should be avoided, because users
may become confused or impatient, but on the other hand, each
individual part of the utterance will convey some (potentially)
1 
This work is licensed under a Creative Commons Attribution 3.0 License. For more information, see 
IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 22, NO. 5, MAY 2014
EXAMPLE REALIZATIONS, GENERATED WHEN THE USER PROVIDED
, AND WHERE THE
WIZARD HAS ALSO SELECTED THE ADDITIONAL ATTRIBUTE
FOR PRESENTATION TO THE USER.
useful information to the user. There is therefore an optimization problem to be solved. Moreover, this optimization has to
deal with uncertainty, as the user judgements or the (most likely)
user reaction to each NLG action are unpredictable, and the behavior of subsequent modules, such as the surface realizer, may
also be variable.
In this paper we present and evaluate a novel framework
for adaptive natural language generation where the problem
is formulated as incremental decision making under uncertainty, which can be approached using statistical planning
methods , , , . The presented model is also
being explored by other researchers , , , . We
have applied the theory to a variety of NLG problems, such
as referring expression generation , and incremental
dialogue phenomena, such as barge-in , . Here, we
focus on adaptive information presentation (IP) in statistical
spoken dialogue systems.
The remainder of the paper proceeds as follows. In Section II
information
presentation
Section III we present the general framework of NLG as
planning under uncertainty. In Section IV we describe a
Wizard-of-Oz (WoZ) data collection. In Section V we explain
how we build a simulated training environment from this data.
In Section VI we present results from training and testing
in simulation and in Section VII we present results from a
user study. Section VIII concludes with a discussion of future
directions.
II. PREVIOUS WORK: INFORMATION PRESENTATION IN SDS
Work on evaluating SDS suggests that the information presentation (IP) phase is the primary contributor to dialogue duration , and as such, is a central aspect of SDS design. During
this phase the system returns a set of items (“hits”) from a database, which match the user’s current search constraints. An inherent problem in this task is the trade-off between presenting
“enough" information to the user (for example helping them to
feel conﬁdent that they have a good overview of the search results) versus keeping the utterances short and understandable,
see for example , .
Fig. 1. Possible NLG policies (
generation).
Broadly speaking, IP for SDS can be divided into two main
steps: 1) IP strategy selection and 2) Content or attribute selection, which again can be divided into attribute ranking (according to a user model) and the number of attributes to be selected. In this work we will concentrate on strategy selection as
well as selecting the number of attributes from a list of ranked
attributes, which we assume to be already provided by a given
user model.
Prior work has presented a variety of IP strategies for structuring information (see examples in Table I). For example, the
SUMMARY strategy is used to guide the user’s attention to a
relevant subset of search results, i.e. it draws the user’s attention to relevant attributes by grouping the current results from
the database into clusters, e.g. , . Other studies investigate
a COMPARE strategy, e.g. , , while much work in SDS
uses a RECOMMEND strategy, e.g. , .
In a previous proof-of-concept study we showed that
each of these strategies has its own strengths and drawbacks,
dependent on the particular context in which information needs
to be presented to a user. Here, we will also explore possible
(incremental) combinations of the strategies, for example SUM-
MARY followed by RECOMMEND, as also explored by ,
see Fig. 1.
Prior work on content or attribute selection has used a “Summarize and Reﬁne" approach , to determine which attributes should be used. This method employs utility-based attribute selection with respect to how each attribute (e.g. price
or food type in restaurant search) of a set of items helps to
narrow down the user’s goal to a single item. Related work explores a user modelling approach, where attributes are ranked
RIESER et al.: NLG AS INCREMENTAL PLANNING UNDER UNCERTAINTY: ADAPTIVE IP FOR STATISTICAL DIALOGUE SYSTEMS
according to user preferences , , . Our data collection
(see Section IV) and training environment (Section V) incorporate these approaches.
The work in this paper is the ﬁrst to apply a data-driven
method to this whole decision space, i.e. combinations of information presentation strategies as well as selecting the number
of attributes. We also show the utility of both lower-level features (e.g. from the surface realizer) and higher-level features
(e.g. from dialogue management) for this problem. Previous
work has focused on individual aspects of the problem (e.g.
how many attributes to generate, or when to use a SUMMARY),
using a pipeline model for SDS with DM features as input,
and where NLG has no knowledge of lower-level features, e.g.
lower-level surface realization and associated features such as
sentence length.
III. NATURAL LANGUAGE GENERATION AS PLANNING
UNDER UNCERTAINTY
We adopt the general framework of NLG as planning under
uncertainty (see , for the initial version of this approach). Some aspects of NLG have been treated as planning,
e.g. , but not as statistical planning. Within an SDS architecture, NLG actions take place in a stochastic environment, consisting for example of a user and a stochastic realizer, where the
individual NLG actions have uncertain effects on the environment. For example, presenting differing numbers of attributes to
the user, makes the user more or less likely to choose an item,
as shown by for multi-modal interaction.
Most SDSs employ ﬁxed template-based generation. Our
model, however, employs a non-deterministic sentence planner
and surface realizer for SDS, based on . This introduces
additional variation, to which higher level NLG decisions will
need to react. In our framework, the NLG component must
achieve a high-level Communicative Goal from the Dialogue
Manager (e.g. to present a number of items) through planning
a sequence of lower-level generation steps or actions, for example ﬁrst to summarize all the items and then to recommend
the highest ranking one. Each such action has uncertain effects
due to the stochastic realizer. For example, the realizer might
generate a different sentence structure or employ different numbers of attributes depending on its own processing constraints
(see e.g. the realizer used to collect the MATCH project data,
see and ). Likewise, the user may be likely to choose
an item after hearing a summary, or they may wish to hear
more. Generating appropriate language in the context of an
interactive, stochastic, system, thus has the following important
characteristics in general:
• NLG is goal driven behavior
• NLG must plan a sequence of actions
• Each action changes the environment state or context
• The effect of each action is uncertain.
As such, the problem of planning how to (incrementally) generate an utterance for SDS falls naturally into the class of statistical planning problems, rather than rule-based approaches
such as , , or supervised learning as explored in previous work, such as classiﬁer learning and re-ranking, e.g. ,
 , . These supervised approaches involve the ranking of
Fig. 2. Example RL-NLG action sequence for Table II.
a set of completed plans/utterances and do not adapt online to
the context or the user. reinforcement learning (RL) provides
a principled, data-driven optimization framework for our type
of planning problem maximizing some notion of long-term reward or utility . Note that there also is closely related work
which also explores NLG as a process of maximizing utility
using game theory , .
In the following we further illustrate this approach using a
worked example of information presentation.
A. Worked Example
The input to the NLG planning module is a Communicative
Goal supplied by the Dialogue Manager. The CG consists of
a Dialogue Act to be generated, for example
), and a System Goal (
) which is the
desired user reaction, e.g. to make the user choose one of the
RL-NLG module must plan a sequence of lower-level NLG
actions that achieve the goal (at lowest cost) in the current
context. The context consists of a user (who may remain silent,
supply more constraints, choose an item, or quit), and, in the
case of fully stochastic systems, variation from a stochastic
sentence realizer, e.g. systems such as SPaRKy .
plan for IP strategy selection only, as carried out by
) from the system’s DM.
This initializes the NLG state (init). In this example, the policy
chooses the action SUMMARY and this transitions us to state
, where we observe that in this example 2 attributes and 1
sentence have been generated by lower level modules. Also, the
user is predicted to remain silent. In this state, the current NLG
policy chooses to next RECOMMEND the top ranked item (
for this user), which takes us to state
, where, by now, a total
of 4 attributes have been generated in a total of 4 sentences,
and the user is now predicted to choose an item. As such, the
policy determines that in states like
the best thing to do is
“stop” and pass the turn to the user. This takes us to the state
end, where the total reward of this action sequence is computed
(see Section V-C), and used to update the NLG policy in each
of the visited state-action pairs via back-propagation.
In Section VI we present a model which integrates strategy
and attribute selection into a hierarchical learning architecture,
i.e. the number of attributes selected is also optimized with respect to the expected reward.
IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 22, NO. 5, MAY 2014
EXAMPLE UTTERANCE PLANNING SEQUENCE FOR FIG. 2.
Fig. 3. Wizard interface.
IV. WIZARD-OF-OZ DATA COLLECTION
In a previous proof-of-concept study using two data
sets made available by the MATCH project, see and 
we showed that each of the IP strategies in Table I has its own
strengths and drawbacks, dependent on the particular context
in which information needs to be presented to a user. In the
following we explore possible combinations of the strategies
in a WoZ data collection. In this WoZ study, we asked humans
(our “wizards”) to produce appropriate IP actions in different
dialogue contexts, when interacting with other humans (the
“users”), who were told that they were talking to an automated SDS. The wizards were experienced researchers in SDS
and were familiar with the search domain (i.e. restaurants in
Edinburgh). They were instructed to select IP structures and
attributes for NLG so as to most efﬁciently allow users to ﬁnd
a restaurant matching their search constraints, where we used a
database of 340 Edinburgh restaurants2 provided by TheList3 .
2Note that our approach is scalable to larger database sizes. The number of
retrieved database items currently serves as a feature in the state space as well as
a reward feature, where presenting large sub-sets of database items is negatively
rewarded (see Section V-C). By increasing the number of items in the database,
the size of the state space therefore increases, and learning would take more iterations. This feature could be quantized, or automatic state-space compression
techniques could be used to maintain tractability .
3 
The task for the wizards was to decide which IP structure
to use next (see Section IV-B for a list of IP strategies to
choose from), how many attributes to mention (e.g. cuisine,
price range, location, food quality, and/or service quality),
and whether to stop generating. Our hypothesis is that these
choices depend on varying numbers of database matches,
varying prompt realizations, and varying user behavior. Wizard
utterances were synthesized using the Cereproc text-to-speech
engine4 . The user speech input was delivered to the wizard
using Voice Over IP.
In order to make the wizards’ decisions comparable with real
system decisions, the wizard only sees what the systems would
have as input, i.e. the experimenter will listen to the user’s utterance and transcribe it into a simple semantic representation of
slot-value pairs. We also experimented with introducing noise to
simulate speech recognition errors, following a similar method
to . However, noise did not seem to have a signiﬁcant effect
on either wizard or user behavior .
Fig. 3 shows the web-based interface for the wizard. The
wizard GUI contains 5 main panels:
A: The wizard receives the user’s query as attribute values.
The experimenter has a similar input panel. There are 5
4 
RIESER et al.: NLG AS INCREMENTAL PLANNING UNDER UNCERTAINTY: ADAPTIVE IP FOR STATISTICAL DIALOGUE SYSTEMS
searchable attributes in total, which can also be negative
(“not expensive”).
B: The retrieved database items are presented in an ordered
list. We use a user modelling approach for ranking the
restaurants, where attributes are ranked according to user
preferences. We assume a default user who cares about
cheap food with high quality and good service.
C: The wizard then chooses which strategy and which attributes to generate next, by clicking radio buttons. The
attribute/s speciﬁed in the last user query are pre-selected
by default. The strategies can only be combined in the orders as speciﬁed in Fig. 1.
D: An utterance is automatically generated by the NLG stochastic surface realizer (see Section IV-B) every time the
wizard selects a strategy, and is displayed in an intermediate text panel.
E: The wizard can decide to add the generated utterance
to the ﬁnal output panel. The text in the ﬁnal panel is
sent to the user via TTS, once the wizard decides to stop
generating.
A. Experimental Setup and Data collection
We collected 213 dialogues with 18 subjects and 2
wizards . Each user performed a total of 12 tasks, where no
task set was seen twice by any one wizard. Note that 3 dialogues
were discharged due to technical difﬁculties. The majority of
users were from a range of backgrounds in a higher education
institute, in the age range 20-30, native speakers of English,
and none had any prior experience of spoken dialogue systems.
After each task, the user answered a questionnaire on a 6 point
Likert scale, regarding the perceived generation quality in that
task. The wizards’ IP strategies were highly ranked by the users
on average (4.7), and users were able to select a restaurant in
98.6% of the cases. Also, no signiﬁcant difference between the
wizards was observed.
The data contains 2236 utterances in total: 1465 wizard utterances and 771 user utterances. We automatically extracted
81 features (e.g #sentences, #DBhits, #turns, #ellipsis)5 from
the XML logﬁles after each dialogue. These features can be
categorized into 7 broader categories: (1) general information
(containing 9 features), e.g. user information; (2) turn information (4 features), e.g. turn number; (3) NLG related information
(14 features), e.g. the chosen IP strategy; (4) task-based information (22 features), e.g. slot values; (5) features from the annotated user reply (4 features), e.g. user dialogue act; (6) features describing the artiﬁcially introduced noise (16 features)
and (7) user questionnaire ratings per task & objective measures (12 features). The user questionnaire was similar to the
one described in Section VII-B, with additional questions focussing on the NLG quality, such as “The way the system presented information was good. ”, “The system’s utterances had
the right length.”, “The system gave me a good overview of
all the available options.”. A full list of annotated features is
described in .
5The full corpus and a list of features is available from the CLASSiC project
data repository, see 
Data/myaccount.php
B. NLG Realiser
We implemented a NLG realizer for the chosen IP structures
and attribute choices, in order to realize the wizards’ choices in
real time. This generator is based on data from the stochastic
sentence planner SPaRKy . We replicated the variation observed in SPaRKy by analyzing high-ranking example outputs
(given the highest possible score by the SPaRKy judges) and
implemented the variance using stochastic sentence generation
templates: 6 The realizations vary in sentence aggregation, aggregation operators (e.g. ‘and’, period, or ellipsis), contrasts
(e.g. ‘however’, ‘on the other hand’) and referring expressions
(e.g. ‘it’, ‘this restaurant’) used. The length of an utterance also
depends on the number of attributes chosen, i.e. the more attributes the longer the utterance. All of these variations were
The templates pre-deﬁne the sentence structure with placeholders for the actual values. For example, for comparing two
, the following templates could be used:
restaurant
restaurant
price range and it is located in
the restaurant called
restaurant which
price range and it is located in
restaurant
restaurant.
pricerange.
is located in
randomly chosen from the following set of contrastive connectives:
following . In total, we pre-deﬁned over 90 templates, with
about 30 for each of the following three IP strategies, where
one was chosen at random during generation.
• SUMMARY of all matching restaurants using a user model
(UM) approach, following , where attributes are ranked
and clustered according to (pre-deﬁned) user preferences.
• COMPARE the top 2 restaurants by Item (i.e. listing all
the attributes for the ﬁrst item and then for the other) or
by Attribute (i.e. directly comparing the different attribute
• RECOMMEND the top-ranking restaurant (according to a
Note that there was no discernible pattern in the data about
the wizards’ decisions between the UM/no UM and the byItem/
byAttribute versions of the strategies. In this study we therefore concentrate on the higher level decisions (SUMMARY VS.
6Note that we decided against using the freely available Java implementation
of SPaRKy (jSPaRKy v 2.0). First, jSPaRKy generates utterances in past tense (presumably because it is trained on the Penn Tree Bank). Second, the introduced
disﬂuencies might have had a negative impact on the user ratings.
IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 22, NO. 5, MAY 2014
Fig. 4. Rules learned by JRip for the wizard model (‘
database matches, ‘
NLG action).
COMPARE VS. RECOMMEND) and model UM/no UM and
the byItem/byAttribute versions as variations introduced by the
C. Supervised Baseline strategy
We analyzed the WoZ data to explore the strategies that
human wizards explored for this task. Note that while the wizards showed signiﬁcantly different behavior in terms of overall
frequency, however no signiﬁcant difference in user ratings
was detected . We therefore hypothesize that wizards select
different strategies according to different contexts. Observing
signiﬁcantly different strategies which are not signiﬁcantly
different in terms of user satisfaction, we conjecture that the
wizards converged on strategies which were appropriate in
certain contexts. We also observed that there was a wide spread
user ratings for different dialogues. We therefore took the top
rated 50% of the IP instances (
) to build a baseline
model which reﬂects “good” wizard behavior.
We used a variety of supervised learning methods to create a
model of the highly rated wizard behavior. Please see for
further details. The best performing method was Rule Induction
(JRip).7 The model achieved an accuracy of 43.19% which is
signiﬁcantly (
) better than the majority baseline of
always choosing SUMMARY (34.65%).8 The resulting rule set
is shown in Fig. 4, where the determining features are number
of database hits retrieved
and the IP strategy realized in
the previous turn
The features selected by this model were only “high-level”
features, i.e. the features that an IP module receives as input
from a Dialogue Manager. We further analyzed the importance of different features using feature ranking and selection
methods , ﬁnding that the human wizards (in this speciﬁc
setup) did not pay signiﬁcant attention to any lower level
features, e.g. variation from surface realization. Despite the
simplicity of the learned supervised policy, it achieves up
to 87.6% of the possible reward on this task, as we show in
Section VI-B, and so can be considered a serious baseline
against which to measure performance. Below, we will show
that reinforcement learning produces a signiﬁcant improvement
over the strategies present in the original data, especially in
cases where RL has access to “lower level" features of the
7The WEKA implementation of RIPPER .
8We believe that the relatively low model accuracy is due to data sparsity and
diverse behavior of the wizards. We hypothesize that multiple different generation actions are viable at each point and so while the model does not predict
the exact pattern in the data it predicts a reasonable pattern. This hypothesis is
supported later on by the high reward it gains. Note that the reward measures
how “appropriate” an action is in a speciﬁc context (see Section V-C).
context. This conﬁrms previous ﬁndings that human wizard
behavior is not necessarily “optimal” .
We also learn a supervised model for selecting the number
of attributes, trying a variety of SL techniques. However, the
learned models did not show any signiﬁcant improvements over
a majority or a random baseline, with the majority baseline performing best. We therefore implemented a majority baseline
(randomly choosing between 3 or 4 attributes) as a baseline
V. SIMULATED ENVIRONMENT FOR LEARNING
Here we “bootstrap" a simulated training environment from
the WoZ data, following , .
A. User Simulations
User simulations are commonly used to train strategies for
Dialogue Management, see for example . A user simulation
for (incremental) NLG is very similar, in that it is a predictive
model of the most likely next user act.9 However, this NLG
predicted user act does not actually change the overall dialogue
state (e.g. by ﬁlling slots) but it only changes the generator state.
In other words, the NLG user simulation tells us what the user
is most likely to do next, if we were to stop generating now.
We are mainly interested in the following user reactions:
the user chooses one of the presented items,
e.g. “Yes, I’ll take that one.” This reply type indicates that
the information presentation was sufﬁcient for the user to
make a choice.
The user provides more attributes, e.g. “I want
something cheap." This reply type indicates that the user
has more speciﬁc requests, which s/he wants to specify
after being presented with the current information.
The user asks for more information, e.g. “Can you recommend me one?", “What is the
price range of the last item?" This reply type indicates that
the system failed to present the information the user was
looking for.
The user asks the system to repeat the same
message again, e.g. “Can you repeat?" This reply type indicates that the utterance was either too long or confusing
for the user to remember, or the TTS quality was not good
enough, or both.
The user does not say anything. In this case it
is up to the system to take initiative.
The user closes the interaction.
We built user simulations using n-gram models of system ( )
and user ( ) acts, as ﬁrst introduced by . In order to account
for data sparsity, we apply different discounting (“smoothing”)
techniques including back-off, using the CMU Statistical
Language Modelling toolkit , see Table III. We construct
a bi-gram model for the users’ reactions to the system’s IP
structure decisions (
), and an extended bi-gram
(i.e. IP structure
attribute choice) model for predicting user
reactions to the system’s combined IP structure and attribute
9Similar to the internal user models applied in recent work on POMDP (Partially Observable Markov decision process) dialogue managers , , 
for estimation of user act probabilities.
RIESER et al.: NLG AS INCREMENTAL PLANNING UNDER UNCERTAINTY: ADAPTIVE IP FOR STATISTICAL DIALOGUE SYSTEMS
KULLBACK-LEIBLER DIVERGENCE FOR THE DIFFERENT USER SIMULATIONS
(US), HIGHLIGHTING WORST SCORING AND BEST SCORING METHODS.
selection decisions:
the predicted next user action at time ,
was the system’s
information presentation action at , and
attributes selected by the system at .
We evaluated the performance of these models by measuring dialogue similarity to the original data, based on the
Kullback-Leibler (KL) divergence, as also used by, e.g. ,
 , . We compare the raw probabilities as observed
in the data with the probabilities generated by our n-gram
models using different discounting techniques, including
Witten-Bell, Good-Turing, absolute and linear discounting
 , see Table III. All the models have a small divergence from
the original data (especially the bi-gram model), suggesting
that they are reasonable simulations for training and testing
NLG policies.
The absolute discounting method for the bi-gram model is
most dissimilar to the data, as is the Witten-Bell method for
the extended bi-gram model, i.e. the models using these discounting methods have the highest KL score. The best performing methods (i.e. most similar to the original data), are
linear discounting for the bi-gram model and Good-Turing for
the extended bi-gram. We use the most similar user simulations
for system training, and the most dissimilar user simulations for
testing NLG policies, in order to test whether the learned policies are robust and adaptive to unseen dialogue contexts.
B. Database Matches and Attention Modelling
An important task of information presentation is to support
the user in choosing between all the available items (and ultimately in selecting the most suitable one) by structuring the
current information returned from the database, as explained in
Section II. We therefore represent the items brought to the user’s
attention as a feature in our learning experiments. In particular,
attention modelling is used to determine the number of database
hits (# DBhits) used in the reward function, see Section V-C.
This feature reﬂects how the different IP strategies structure information with different numbers of attributes, similar to the
clustering approach discussed in . We implement this shift of
the user’s attention focus analogously to discovering the user’s
goal in dialogue management: every time the predicted next user
act is to add information (
), we infer that the user is
therefore only interested in a subset of the previously presented
results and so the system will focus on this new subset of database items in the rest of the generated utterance. For example in
Table I, the user’s attention reduces from a total of 23 DBhits to
10 after the SUMMARY (with UM) since the user is only interested in cheap, Indian places.
C. Data-Driven Reward Function
The reward/evaluation function
is constructed from the
WoZ data, using a stepwise linear regression, following the paradise framework . This model selects the context features
(see Section IV-A) which signiﬁcantly inﬂuenced the users’ ratings for the NLG strategy in the WoZ questionnaire. We also
assign a value to the user’s reactions (valueUserReaction), similar to optimizing task success for DM . In we showed
that different user replies are positively or negatively correlated
with user ratings. The numerical values were manually assigned
to reﬂect this. In principle it would also be possible to learn the
user reaction value function. The agent receives
if the user selects an item,
if the user adds further constraints to the search,
if the user does something else
The agent is encouraged to choose those sequences of actions
that lead to the user selecting a restaurant as quickly as possible.
We then run a stepwise linear regression on the annotated WoZ
data (relating contextual features to user task ratings), resulting
in the model in Equation (2) (
). The chosen features
indicate that users’ ratings are inﬂuenced by higher level and
lower dialogue level features: Users like to be focused on a small
set of database hits (where # DBhits ranges over 1-100), which
will enable them to choose an item (valueUserReaction), while
keeping the IP utterances short (where # sentence is in the range
 – ):
The reward is calculated at the end of an episode, which for
this task constitutes a full NLG utterance, also see Section III-A.
Note that the worst possible reward for a NLG move is therefore
achieved by presenting 100 items to the user in 18 sentences10,
in such a way that the user immediately ends the conversation
unsuccessfully. The top possible reward is achieved in the rare
cases where the system can immediately present one item to the
user using a minimum of two sentences, and the user then selects
that item, i.e.
VI. TRAINING AND TESTING THE LEARNED
POLICIES IN SIMULATION
We now formulate the problem as a Markov decision process
(MDP), where states are NLG dialogue contexts and actions are
NLG decisions. Each state-action pair is associated with a transition probability, which is the probability of moving from state
after having performed action
when in state . This transition probability is computed
10Note that the maximum possible number of sentences generated by the
realizer is 18 for the full IP sequence
using all the attributes.
IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 22, NO. 5, MAY 2014
Fig. 5. State-action space for the RL-NLG problem.
by the environment model (i.e. the user simulation and realizer), and explicitly captures the uncertainty in the generation
environment. This is a major difference to other, non-statistical,
planning approaches. Each transition is also associated with a
reinforcement signal (or “reward”)
as deﬁned above, describing how good the result of action
was when performed in
state . The aim of the MDP is to maximise the long-term expected reward of its decisions, resulting in a policy which maps
each possible state to an appropriate action in that state.
We then treat IP as a hierarchical joint optimization problem,
where ﬁrst one of the possible single IP strategies (1-3) is chosen
and then the number of attributes is decided, as shown in Fig. 5.
At each generation step, the MDP can choose 1-5 attributes
(e.g. cuisine, price range, location, food quality, and/or service
quality). Generation stops as soon as the user is predicted to select an item, i.e. the IP task is successful. (Note that the same
constraint is operational for the WoZ baseline.)
States are represented as sets of NLG dialogue context
features, see Fig. 5. The state space comprises “lower-level"
features about the realizer behavior, including two features
representing the number of attributes (# attr) and sentences
generated so far (# sentence), and three binary features
representing the user’s predicted next action, as well as
“high-level" features provided by the DM, including the current database hits brought to the user’s attention (# dbHits),
the slots ﬁlled so far (statusSlot) and a subset of user actions
(also see Section V-A). According to the reward function
(Section V-C), we mainly care about whether the user selects
) or adds information (
). All other user
mapped to one state feature (userElse).
hierarchical
algorithm with linear function approximation ,
and the simulation environment described in Section V. The
policy was trained for 60,000 iterations.
A. Experimental Set-Up
We compare the learned strategies against the supervised
wizard baseline as described in Section IV-C. For training, we
use the user simulation model most similar to the data. For
testing, we use a different user simulation model (the one which
is most dissimilar to the data), see Section V-A.
We ﬁrst investigate how well IP structure (without attribute
choice) can be learned in increasingly complex generation scenarios, with respect to action choices and uncertainty in the environment. A generation scenario is a combination of a particular kind of NLG surface realizer (template vs. stochastic) along
with different levels of variation introduced by certain features
of the dialogue system. In general, the stochastic realizer introduces more variation in sentence length than the template-based
realizer. We therefore investigate the following cases, starting
with optimizing IP structure choice only:
1.1 IP structure choicewrt. templaterealizer: Predicted
next user action varies according to the bi-gram model
); Number of sentences and attributes per IP
strategy is set by defaults, reﬂecting a template-based realizer.
1.2 IP structure choicewrt. stochasticrealizer: IP structure where number of attributes per NLG turn is given at
the beginning of each episode (e.g. set by the DM); Sentence generation according to the SPaRKy stochastic realizer model as described in Section IV-B.
We then investigate different scenarios for jointly optimizing IP
structure and attribute selection (Attr) decisions.
2.1 IP structure choice + #attribute choicewrt. templaterealizer: Predicted next user action varies according
to extended bi-gram (
Number of sentences per IP structure set to default.
2.2IP structure choice + #attribute choicewrt.templaterealizer+ Attention: Extended bi-gram user simulation
with Template realizer and attention model with respect to
#DBhits and #attributes as described in Section V-B.
2.3IP structure choice + #attribute choicewrt. stochasticrealizer Extended bo-gram user simulation with sentence/attribute relationship according to stochastic realizer
as described in Section IV-B.
2.4 IP structure choice + #attribute choicewrt.stochasticrealizer+ Attention: i.e. the full model = Predicted
next user action varies according to extended bi-gram
model+ attention model + Sentence/attribute relationship
according to stochastic realizer.
B. Results
We compare the average ﬁnal reward (see Equation (2))
gained by the baseline against the trained RL policies in the different scenarios for each 1000 test runs, using a paired samples
t-test. The results are shown in Table IV. In 5 out of 6 scenarios
the RL policy signiﬁcantly (
) outperforms the supervised baseline. We also report on the percentage of the top
possible reward, and the raw percentage improvement of the RL
policy. Note that the best possible reward (
can only be gained in rare cases (see Section V-C).
The more complex the scenario, the harder it is to gain higher
rewards for the policies in general (as more variation is introduced). However, the relative improvement in rewards also increases with complexity: RL learns to adapt to the more challenging scenarios. Note that the baseline does reasonably well in
scenarios with variation introduced by only higher level features
RIESER et al.: NLG AS INCREMENTAL PLANNING UNDER UNCERTAINTY: ADAPTIVE IP FOR STATISTICAL DIALOGUE SYSTEMS
TEST RESULTS FOR 1000 DIALOGUES, WHERE *** DENOTES THAT THE RL POLICY IS SIGNIFICANTLY (
THAN THE BASELINE POLICY, WITH THE STANDARD DEVIATION (
) IS SHOWN IN BRACKETS.
IP STRATEGIES LEARNED FOR THE DIFFERENT SCENARIOS, WHERE
DENOTES THE NUMBER OF ATTRIBUTES GENERATED.
(e.g. scenario 2.2), however it fails to adapt well to lower level
features (see Section IV-C). From these results we can infer that
adapting to lower level features is important in gaining signiﬁcant improvements over the baseline.
An overview of the range of different IP strategies learned for
each scenario can be found in Table V, which is generated by
analyzing the respective test runs.
C. Learned Behavior
Note that these strategies are context-dependent: the learner
chooses how to proceed dependent on the features in the state
space at each generation step. In the following we provide illustrative examples of what was learned for each of the generation
scenarios, which we extracted from the respective test runs.
a) Scenario 1.1-1.2: IP Strategy Selection: The RL policy
for Scenario 1.1 learned to start with a SUMMARY if the initial
number of items returned from the database is high (
will then stop generating if the user is predicted to select an item.
Otherwise, it continues with a RECOMMEND. If the number of
database items is low, it will start with a COMPARE and then
continue with a RECOMMEND, unless the user selects an item.
In addition, the RL policy for Scenario 1.2 learns to
adapt to a more complex scenario: the number of attributes
requested by the DM and produced by the stochastic sentence realizer. It learns to generate the whole sequence
) if #attributes is low (
), when the overall generated utterance
(ﬁnal #sentences) is still relatively short. Otherwise the policy
is similar to the one for Scenario 1.1.
b) Scenario 2.1-2.4: IP
RL policies for jointly optimizing IP strategy and attribute selection learn to select the number of attributes according to the generation Scenarios 2.1-2.4. For example, the RL policy learned
for Scenario 2.1 generates a RECOMMEND with 5 attributes if
the number of database hits is low (
). Otherwise, it will start
with a SUMMARY using 2 attributes. If the user is predicted to
narrow down his focus after the SUMMARY, the policy continues with a COMPARE using 1 attribute only, otherwise it
helps the user by presenting 4 attributes. It then continues with
RECOMMEND (5), and stops as soon as the user is predicted
to select one item.
The learned policy for Scenario 2.1 generates 5.85 attributes
per NLG turn on average (i.e. the cumulative number of attributes generated in the whole NLG sequence, where the same
attribute may be repeated within the sequence). This strategy
primarily adapts to the variations from the user simulation (extended bi-gram model). For Scenario 2.2 the average number of
attributes is higher (7.5) since the number of attributes helps to
narrow down the user’s focus via the DBhits/attribute relationship speciﬁed in Section V-B. For Scenario 2.3 fewer attributes
are generated on average (3.14), since here the number of attributes inﬂuences the sentence realizer, i.e. fewer attributes result in fewer sentences, but does not impact the user’s focus. In
Scenario 2.4 all the conditions mentioned above inﬂuence the
learned policy. The average number of attributes selected is still
low (3.19).
In comparison, the average (cumulative) number of attributes
for the WoZ baseline is 7.10. The WoZ baseline generates all the
possible IP structures (with 3 or 4 attributes) but is restricted
to use only “high-level" features (see Fig. 4). By beating this
baseline we show the importance of the “lower-level" features.
Nevertheless, this wizard policy achieves up to 87.6% of the
IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 22, NO. 5, MAY 2014
possible reward on this task, and so can be considered a serious
baseline against which to measure performance.
The only case (Scenario 2.2) where RL does not improve signiﬁcantly over the baseline is where lower level features do not
play an important role for learning good strategies: Scenario 2.2
is only sensitive to higher level features (DBhits).
VII. EVALUATION WITH REAL USERS
After obtaining satisfactory results in simulation, we now test
whether these results transfer to a real user setting, using a richer
set of evaluation metrics. In general, natural language generation for spoken dialogue systems serves two goals: On the one
hand the local NLG task is to present “enough" information to
the user while keeping the utterances short and understandable.
In the previous section, we demonstrated that the learned IP
model locally outperforms the WoZ baseline.
On the other hand, better information presentation should also
contribute to the global/ overall dialogue task, so as to maximise
task completion. In order to test this hypothesis, the learned
policy was integrated into a telephone-based spoken dialogue
system, and evaluated with real users. In particular, we test its
ability to contribute to overall dialogue task success.
A. System Integration
In order to evaluate our NLG strategy with real users, it
was integrated into the CamInfo system , a fully statistical
spoken dialogue system providing tourist information for real
locations in Cambridge. This baseline system has been made
accessible by phone using VoIP technology, enabling out-of-lab
evaluation with large numbers of users. Apart from practical advantages in managing evaluation campaigns, this development
effort was also intended as a step towards evaluating spoken
dialogue systems under more realistic conditions. Please note,
however, that the users in this evaluation were still recruited
and asked to complete predeﬁned tasks (see Section VII-B),
and therefore the evaluation might not be as realistic as an
evaluation of a ﬁnal deployed application with real-world users
having real goals .
The speech recognizer, semantic parser and dialogue manager have all been developed at Cambridge University. For
speech synthesis, the Baratinoo synthesizer developed at
France Telecom, was used.
The DM uses a POMDP (Partially Observable Markov decision process) framework, allowing it to process N-Best lists
of ASR hypotheses and keep track of multiple dialogue state
hypotheses. The DM policy is trained to select system dialogue
acts given a probability distribution over possible dialogue
states. It has been shown that such dialogue managers can
exploit the information in the N-Best lists (as opposed to only
using the top ASR hypothesis) and are therefore particularly
effective in noisy conditions .
The natural language generation component of this baseline
system is a standard rule-based surface realizer covering the full
range of system dialogue acts that the dialogue manager can produce. It has only one IP strategy, i.e., the system only provides
information about database entries in the form of single venue
recommendations (the recommend strategy, see Table I). The attributes of the venue to be presented are selected heuristically.
This baseline represents the information presentation strategies
in conventional slot-ﬁlling systems, e.g. , . In the extended version of the system, the IP strategy is replaced by our
trained NLG component, which is optimized to decide between
different IP strategies.
We follow a hybrid between statistical and rule-based approaches in order to integrate our trained IP policy: higher-level
hand-coded rules impose a set of constraints on the statistical
policy. Note that the possibility of constraining statistical policies with hard-coded rules is increasingly required for developing commercial dialogue systems . We follow a modular
approach for integration, where the NLG and dialogue management strategies were trained separately (we discuss this issue
further below).
We impose the following rule-based constraints on our policy
in order to make it compatible with the (separately trained) DM
• The chosen IP strategy must end with a RECOMMEND
action, since the DM expects (exactly one) named entity to
be mentioned.
• COMPARE actions are excluded in order to not introduce
new named entities that the user may refer to later (since
the DM was not optimized under this condition).
• The attribute selection is forced to present at least the attributes chosen by the DM.
The remaining decision points are: choosing between REC-
OMMEND and
, as well as selecting additional attributes to present to the user. Although this
is a somewhat limited version of the fully optimized IP strategy,
it is still interesting to discover whether even a limited amount
of NLG optimization (in terms of more elaborate IP strategies
and attribute selection) has an effect on overall global system
performance.
Hence, in this real user evaluation, we compared the baseline
system, incorporating a single recommendation IP strategy only,
with the extended system, incorporating our trained IP policy.
In a previous proof-of-concept study a similar rule-based
baseline NLG strategy (RECOMMEND only) was shown to be
outperformed in simulation. We now evaluate whether these results transfer to real user settings. In the remainder of the paper
we will refer to the baseline system as BASE system and to the
system with the integrated trained IP strategy as TIP.
Table VI shows an abbreviated example dialogue from the
evaluation corpus. The dialogue contains two IP turns where the
system selects a strategy based on the trained IP policy: Sys2
and Sys3. In Sys2, the system decides to ﬁrst summarize the retrieved restaurants, using location as a distinguishing feature,
and then recommend the top ranking one. In turn Usr2, the user
decides not to select the recommended restaurant, but to ask for
a restaurant in a different location, which was previously mentioned in the SUMMARY. Note that even though the system
failed to recognize “address” in User2, it did understand “riverside area” and was able to progress the dialogue. This example
illustrates one of the beneﬁts of generating summaries for task
based dialogue systems: it helps the user to gain an overview of
the available options .
The dialogue also shows how the system varies between
(Sys2) and RECOMMEND
RIESER et al.: NLG AS INCREMENTAL PLANNING UNDER UNCERTAINTY: ADAPTIVE IP FOR STATISTICAL DIALOGUE SYSTEMS
EXAMPLE DIALOGUE WITH THE CAMBRIDGE RESTAURANT
INFORMATION DIALOGUE SYSTEM.
only (Sys3) based on the progress of the dialogue task, which
is reﬂected in the state space by how many “slots” are ﬁlled.
In this case, the trained IP policy decides to only perform a
RECOMMEND after 3 slots were speciﬁed by the user (cuisine, price, and area). SUMMARY, in its function of providing
an overview, was chosen at the beginning of the dialogue,
when fewer slots were ﬁlled. The two following user utterances
(Usr3, Usr4) indicate the success of this policy.
B. Experimental Design
Next, we empirically evaluated the two systems using two approaches to recruit and manage subjects. In the ﬁrst approach,
subjects were recruited using mail-shots and web-based advertising amongst people from Cambridge and Edinburgh, mostly
students. From the resulting pool of subjects, people were gradually invited to start the tasks, in their own time, and within a
given trial period of around two weeks. After the trial period,
they were paid (using PayPal) per completed task, with a required minimum of 15 tasks, and a maximum of 40 tasks. For
the two systems, this resulted in a corpus of 304 dialogues. In
the second approach, an alternative method of managing subjects was used, using Amazon Mechanical Turk . In this
setup, tasks are published as so-called HITs (Human Intelligence Tasks) on a web-server and registered workers can complete them. This setup resulted in 532 collected dialogues for
the two systems compared11 . In the remainder of this paper, we
will refer to the corpus obtained with ‘locally’ managed subjects
as LOC and to the corpus obtained using Amazon Mechanical
Turk as AMT.
In both of the above-mentioned approaches, the subjects were
directed to a webpage with detailed instructions and for each
11This evaluation was part of a larger evaluation campaign, in which 2046 dialogues were collected in total. This data is available from the CLASSiC project
data repository, see 
Data/myaccount.php
OVERVIEW OF COLLECTED DATA, WITH FOR EACH CORPUS THE
NUMBER OF DIALOGUES (#DIALS), THE AVERAGE NUMBER OF USER
TURNS PER DIALOGUE (AVGTURNS), THE NUMBER OF UNIQUE
USERS (#USERS), THE AVERAGE NUMBER OF DIALOGUES PER
USER (#DSUSR), AND THE WORD ERROR RATE (WER).
task, a phone number to call and the scenario to follow. The
subjects were randomly assigned to interact with one of the systems (BASE or TIP). A scenario describes a place to eat in town,
with some constraints, for example: “You want to ﬁnd a moderately priced restaurant and it should be in the Riverside area.
You want to know the address, phone number, and type of food.”
After the dialogue, the subjects were asked to ﬁll in a short questionnaire, assessing the impact of IP strategies on the users’ perception of various system components:
Table VII summarizes the two corpora of collected data. For
the AMT corpus, considerably more subjects were used, although many of them did only a small number of tasks. For the
LOC corpus, it was more difﬁcult to recruit many subjects, but
in this setup, the subjects could be asked to complete a minimum
number of tasks, hence the higher average number of dialogues
Also, note that the word error rate (WER) is relatively high in
both corpora. This is partly due to the fact that the ASR module
had not been trained speciﬁcally for this particular domain due
to lack of training data. Furthermore, some of the subjects were
non-native speakers and some subjects used Skype to call the
systems, which causes distortion of the audio signal. These conditions are the same for both BASE and TIP systems. Despite
the high ASR error rates, overall task completion rates were relatively high .
The overall most frequently employed IP strategy is
Also, note that the trained policy never employed more than
3 attributes, and always chose to use the same number of
attributes for its combined IP strategies.
C. Results
After processing the log ﬁles and completed user questionnaires, both objective and subjective performance measures
were computed in order to compare the systems.
IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 22, NO. 5, MAY 2014
TABLE VIII
FREQUENCY OF OCCURRENCES OF EACH IP STRATEGY OBSERVED IN THE
EVALUATION WITH NUMBER OF ATTRIBUTES IN BRACKETS.
1) Objective Evaluation: For the objective evaluation of the
two dialogue systems we focused on measuring goal completion rates, which can be done in different ways. First, we can
take the task speciﬁcation assigned to the user for each dialogue
and then analyze the system dialogue acts. Partial completion
(ObjSucc-PC) is achieved when the system has offered a venue
that matches the constraints as speciﬁed in the assigned goal, for
example it has provided the name of a cheap Chinese restaurant
in the riverside area. Full completion (ObSucc-FC) is achieved
when the system has also provided the required additional information about that venue, for example the phone number and
Table IX shows all success rates obtained from the evaluation,
for the corpus with data from locally recruited subjects (LOC),
and the corpus with data from Amazon Mechanical Turk (AMT)
workers, as well as both corpora pooled together (TOT). The results show that the system with our NLG component (TIP) outperforms the baseline system (BASE) on all objective success
rates in both corpora. Despite an overall low completion rate
(which as mainly due to a high WER rate as explained earlier),
relative improvements of up to 30% for full completion on the
AMT corpus were obtained. After pooling the two corpora together, we have a sufﬁcient number of dialogues to show that
the improvement from our NLG strategy is statistically signiﬁcant on both partial and full completion (using a 2-tailed z-test
for two proportions).
It is also interesting to note that the average number of user
turns per dialogue is not signiﬁcantly different between systems
in both corpora, suggesting that the contribution of the trained IP
policy to system performance manifests itself primarily in terms
of effectiveness rather than efﬁciency. By providing more useful
information to the user, the system might help them to ﬁnd an
appropriate venue in fewer turns, but due to the more lengthy
system prompts, more turns might be needed to recover from
any speech recognition errors (see WER in Table VII).
2) Subjective Evaluation: Table X summarizes the subjective
user scores from the questionnaire (see Section VII-B). In terms
of subjective success rates (Q1), the baseline system (BASE)
obtains slightly higher scores on both corpora, although no statistically signiﬁcant differences were found. We will discuss
these results further in Section VII-D.
When comparing the other subjective scores (Q2–Q4) on a
scale of – , using a Mann-Whitney test, the only case where
a statistically signiﬁcant difference is found between the two
systems is the score for Q4:VoiceQuality in the LOC corpus,
where the baseline system is signiﬁcantly better. Since the TTS
voice is exactly the same for both systems, the difference in perceived voice quality might be inﬂuenced by the longer system
prompts for the TIP system. However, we did not observe this
pattern in the AMT corpus.
We also compared the Mechanical Turk setup to the setup
where subjects where recruited locally (AMT vs. LOC for
both systems). For the TIP system, Q2:Understanding and
Q3:Phrasing are signiﬁcantly higher in the AMT corpus compared to the LOC corpus. Similarly, the BASE system performs
signiﬁcantly better for Q3:Phrasing under the Mechanical
Turk setting. However, when combining the results for all the
subjective scores (similar to the objective scores), none of the
differences are signiﬁcant.
In sum, there is no difference in user ratings between the
original BASE system and the TIP system with the integrated
trained NLG strategy, except for Q4:VoiceQuality, which is
better rated for the BASE system in the LOC corpus, even
though the systems had identical TTS. The difference in ratings
between the LOC and AMT corpora suggests that the way in
which subjects are recruited, instructed and paid, as well as the
user population targeted, has an impact on subjective ratings
D. Discussion of Results
The results showed that the trained information presentation
model signiﬁcantly improves objective dialogue task completion, with up to a 23% increase (
raw improvement) compared to hand-coded presentation prompts often used in conventional slot-ﬁlling dialogue systems. It also shows that the
choice between generating a SUMMARY or generating a REC-
OMMEND only, as well as a principled way for selecting attributes, has a signiﬁcant effect on task success. The subjective
scores however were quite similar between the two systems, and
in terms of perceived success rate, the baseline system scored
slightly better, though not statistically signiﬁcantly.
An important factor that may have inﬂuenced the results,
was that the word error rate was relatively high throughout the
data. The more elaborate information presentation prompts from
the integrated system (TIP) might have exacerbated the many
speech recognition problems, where the DM might have falsely
initiated a lengthy information presentation prompt after a misrecognition error. This is also suggested by the analysis of dialogue length, which turned out to be very similar between the
two systems. By providing more useful information to the user,
the TIP system might help them to ﬁnd an appropriate venue in
fewer turns, but due to the lengthy system prompts, more turns
might be needed to recover from speech recognition errors.
Also note that the results only show signiﬁcant improvements for objective task success metrics, even though the NLG
strategy was optimized for user satisfaction (using a linear regression model for estimation, see Section V-C). One possible
explanation for this is the presence of ASR errors in the real
user evaluation, which can negatively impact user satisfaction.
So even though the NLG was optimized for user satisfaction,
this may have been over-ridden by ASR errors. A logistic
regression analysis showed a strong correlation between WER
and subjective task success .
RIESER et al.: NLG AS INCREMENTAL PLANNING UNDER UNCERTAINTY: ADAPTIVE IP FOR STATISTICAL DIALOGUE SYSTEMS
OVERVIEW OF ALL SUCCESS RATES (%) OBTAINED FOR THE TWO CORPORA, INCLUDING SUBJECTIVE SUCCESS OBTAINED FROM Q1 OF
THE USER QUESTIONNAIRE(SUBJSUCC), OBJECTIVE SUCCESS BASED ON ASSIGNED GOALS (OBJSUCC-PC FOR PARTIAL COMPLETION
AND OBJSUCC-FC FOR FULL COMPLETION). 95% CONFIDENCE INTERVALS FOR ALL SUCCESS RATES ARE INDICATED IN BRACKETS;
STATISTICALLY SIGNIFICANT IMPROVEMENTS (
USING A Z-TEST) ARE INDICATED WITH AN ASTERISK ( ).
SUBJECTIVE EVALUATION RESULTS, BASED ON THE QUESTIONNAIRE [Q1-Q4],
WHERE AN ASTERISK ( ) DENOTES A SIGNIFICANT DIFFERENCE AT
(USING A Z-TEST FOR Q1 AND A MANN-WHITNEY TEST FOR Q2–Q4).
Although these evaluation results are positive, a system setup
which combines separately trained dialogue manager and NLG
components is not ideal. In this case the dialogue manager was
trained in a setup where only the single item recommendation
strategy for IP is used. Therefore, for the dialogue manager state
update, only dialogue acts for such IP prompts are expected. If
the trained NLG model decides to use an alternative IP strategy,
a mismatch is then potentially caused between what the dialogue manager planned and what is actually presented to the real
user. Therefore, the NLG module might result in user behavior
that the dialogue manager is not optimized for. As a practical
compromise it was therefore decided (as explained above) to
require all IP prompts to end with a single item recommendation, and the compare strategy was blocked during the evaluation. Therefore, neither DM nor NLG were trained for the ﬁnal
operating conditions that they would experience in this application, though the constraints on NLG mentioned above meant
that the DM’s chosen actions were maintained. In future work
we therefore plan to jointly optimize the DM and NLG strategies (see also ), and it is likely that full use of an optimized
IP strategy would lead to an even greater performance boost in
the overall system. We expect that a joint optimization of DM
and NLG policies would prevent the DM from initiating long IP
prompts after likely mis-recognitions. We predict that the results
obtained in this study would be even stronger for a jointly-optimized
strategy, and we pursue this in current work.
Finally we note that in the above user evaluation the fact that
users were assigned a task may in some cases have limited the
beneﬁcial effects of providing a summary. The beneﬁts might be
greater if users have their own genuine goals, or more specifically, develop their goals during the dialogue based on possible attributes that the system mentions. For example, when
the system gives a summary using different food types, the user
may then be inclined to decide which food type they want. The
beneﬁts of summaries may therefore be greater when subjects
are free to choose their own goals while browsing.
VIII. CONCLUSION AND FUTURE WORK
In this paper we present and evaluate a novel framework
for adaptive natural language generation where the problem
is formulated as stochastic incremental planning under uncertainty, which can be approached using reinforcement learning
methods. At the time of writing, this approach is being actively
explored by a variety of researchers , , , , ,
 , , , , and there is closely related work which
also explores NLG as a process of maximizing utility using
Game Theory , .
We apply this framework to adaptive information presentation (IP) in spoken dialogue systems. For the IP problem a
statistical optimization framework was developed for content
structure planning and attribute selection. This work was the
ﬁrst to apply a data-driven optimization method to this decision space–from data collection to user testing. The IP model
is adaptive to variability observed in a stochastic SDS, and it
incrementally adapts the IP policy at the turn level. Reinforcement learning is used to automatically optimize the IP policy
with respect to a data-driven objective function. An evaluation
found that the trained information presentation strategy significantly improves dialogue task completion for real users, with
up to a 8.2% increase (23% relative) compared to a deployed
dialogue system which uses conventional, hand-coded presentation prompts.
This methodology provides new insights into the nature of
the IP problem, which has previously been treated as a sequential module following dialogue management with no access to
lower-level context features. Our results suggest that features
from modules which traditionally follow NLG strategy and attribute selection decisions, e.g. features from surface realization
or predicted TTS quality, play an important role for IP policy
optimization. As such, we argue that the traditional pipeline
approach for SDS is not appropriate for SDS that include stochastic modules.
It is also interesting to note that all the user studies show
that an adaptive NLG component signiﬁcantly contributes to the
(perceived or objective) task success of the system. Thus, such
IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 22, NO. 5, MAY 2014
data-driven adaptive NLG strategies have “global” effects on
overall system performance. The data-driven planning methods
applied here therefore promise signiﬁcantly upgraded performance of generation modules, and thereby of Natural Language
interaction in general.
Note that all data sets collected in this work are available from
the CLASSIC project data repository .12
A. Future Work
There are several directions in which this research can be developed. An interesting challenge for NLG in general, is that
of ‘generation under uncertainty’ , , , where language must be generated for users even though there is some
uncertainty about their state. This uncertainty can be about their
location, their gaze direction and objects in their ﬁeld of view,
or even about their goals and preferences. Regarding generation
under uncertainty, an interesting research direction will be to explicitly represent uncertainty about the generation context using
techniques such as belief states in Partially Observable Markov
decision processes (POMDPS).
We have currently only investigated a small number of “lower
level” features, such as sentence length. Future work could also
include the predicted TTS quality as a feature for optimizing
NLG decisions.
In addition, the issue of incremental surface generation in
spoken dialogue systems must be tackled for more natural and
efﬁcient dialogue systems , . Here, phenomena such
as split utterances and barge-in are a research focus, which we
are currently targeting using statistical approaches to NLG in
the EC FP7 PARLANCE project (www.parlance-project.eu)
 , .
Finally, in the ERC “Strategic Conversation” (STAC)
project , we currently investigate how this methodology scales to more complex generation scenarios, in particular
to multi-agent adversarial, non-collaborative, settings where
NLG utterances contain complex rhetorical relations and can
be planned so as to hide information or mislead hearers.
ACKNOWLEDGMENT
We would like to thank Xingkun Liu, Helen Hastie, and Paul
Crook for their collaboration on the data collections.