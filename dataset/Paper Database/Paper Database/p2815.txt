Reinforcement Learning in a large scale photonic Recurrent Neural Network
J. Bueno1, S. Maktoobi2, L. Froehly2, I. Fischer1, M. Jacquot2, L. Larger2, D. Brunner2,∗
1 Instituto de F´ısica Interdisciplinar y Sistemas Complejos, IFISC (UIB-CSIC),
Campus Universitat de les Illes Balear, E-07122 Palma de Mallorca, Spain. and
2 FEMTO-ST Institute/Optics Department, CNRS & University Bourgogne Franche-Comt´e,
15B avenue des Montboucons, 25030 Besanon Cedex, France.
∗Corresponding author: 
Photonic Neural Network implementations have been gaining considerable attention as a potentially disruptive future technology. Demonstrating learning in large scale neural networks is essential
to establish photonic machine learning substrates as viable information processing systems. Realizing photonic Neural Networks with numerous nonlinear nodes in a fully parallel and eﬃcient learning
hardware was lacking so far. We demonstrate a network of up to 2500 diﬀractively coupled photonic
nodes, forming a large scale Recurrent Neural Network. Using a Digital Micro Mirror Device, we
realize reinforcement learning. Our scheme is fully parallel, and the passive weights maximize energy
eﬃciency and bandwidth. The computational output eﬃciently converges and we achieve very good
performance.
INTRODUCTION
Multiple concepts of Neural Networks have initiated
a revolution in the way we process information. Deep
Neural Networks outperform humans in challenges previously deemed unsolvable by computers . Among others, these systems are now capable to solve non-trivial
computational problems in optics . At the same time,
Reservoir Computing (RC) emerged as a Recurrent Neural Network (RNN) concept . Initially, RC received
substantial attention due to excellent prediction performance achieved with minimal optimization eﬀort. However, quickly it was realized that RC is highly attractive
for analogue hardware implementations .
As employed by the machine learning community, Neural Networks (NNs) consist of a large number of nonlinear nodes interacting with each other. Evolving the
NNs’ state requires performing vector-matrix products
with possibly millions of entries. Neural Network concepts therefore fundamentally beneﬁt from parallelism.
Consequently, photonics was identiﬁed as an attractive
alternative to electronic implementation .
implementations were bulky and suﬀered from lack of
adequate technology and NN concepts.
This recently
started to change, ﬁrstly because RC enabled a tremendous complexity-reduction of analog electronic and photonic RNNs’ . In addition, integrated photonic
platforms have matured and integrated photonic Neural
Networks are now feasible . Various demonstrations
how a particular network of neurons can be implemented
have been realized in hardware. Yet, Neural Networks
consisting of numerous photonic nonlinear nodes combined with photonically implemented learning were so
far only demonstrated in a delay systems controlled by a
Field Programmable Gate Array . Due to the timemultiplexing, delay system NNs fundamentally require
such auxiliary infrastructure and computational speed
suﬀers due to their serial nature.
While networks with multiple nodes are more challenging to implement, they oﬀer key advantages in terms of
parallelism, speed and for realizing the essential vectormatrix products. Here, we demonstrated a network of
up to 2025 nonlinear network nodes, where each node
is a pixel of a Spatial Light Modulator (SLM). Recurrent and complex network connections are implemented
using a Diﬀractive Optical Element (DOE), an intrinsically parallel and passive device . Simulations based
on the angular spectrum of plane waves show that the
concept is scalable to well over 20.000 nodes. In a photonic RNN with N=900 nodes we implement learning
using a digital micro-mirror device (DMD). The DMD is
intrinsically parallel as well and, once weights have been
trained, passive and energy eﬃcient. Both, the coupling
and learning concepts’ bandwidth and power consumption is not impacted by the system’s size, oﬀering attractive scaling properties. Here we apply such a passive
and parallel readout layer to an analogue hardware RNN,
and introduce learning strategies improving performance
of such systems. Using reinforcement learning we implement timeseries prediction with excellent performance.
Our ﬁndings open the door to novel and versatile photonic Neural Network concepts.
NONLINEAR NODES AND DIFFRACTIVE
Figure 1(a) conceptually illustrates our RNN. Information enters the system via a single input node, from
where it is injected into a recurrently connected network
of nonlinear nodes. The computational result is provided
at the single output node after summing the network’s
state according to weight matrix WDMD. Following the
RC concept, one can choose the input and recurrent internal weights randomly . Here, we create a complex
and recurrently connected network using imaging which
is spatially structured via a DOE, resulting in the internal connectivity matrix WDOE .
1(b) we schematically illustrate our experimental setup. The polarization of an illumination laser
 
(a) Schematic illustration of a recurrent neural network. (b) Experimental setup. The NN state encoded on the
SLM is imaged onto the camera (CAM) through a polarizing
beam splitter (PBS) and a diﬀractive optical element (DOE).
In this way, we realize a recurrently coupled network of Ikeda
maps. A digital micromirror device (DMD) creates a spatially
modulated image of the SLM’s state. The integrated state,
obtained via superimposing the modulated intensities, is detected, creating the photonic RNN’s output. (c) The RNN’s
coupling matrix established by the DOE, with the inset showing a zoom into a smaller region.
(Thorlabs LP660-SF20, λ=661.2 nm, Ibias =89.69 mA,
T=23 ℃) is adjusted to s-polarization and the polarizing beam splitter cube (PBS) therefore reﬂects all light
towards the SLM. By focusing the illumination laser
onto the ﬁrst microscope objective’s (MO1, Nikon CFI
Plan Achro 10X) back focal plane, the SLM (Hamamatsu
X13267-01) is illuminated by a plane wave. The λ
in front of MO1 is adjusted such that the SLM operates
in intensity modulation mode. Consequently the optical
ﬁeld transmitted through the PBS (p-polarization) for
each SLM pixel i is modulated according to
i and xSLM
are the illumination and gray scale
value of pixel i, respectively. κSLM is the SLM’s conversion factor between pixel gray scale and polarization
rotation angle.
Ignoring for now the DOE’s eﬀect for explanatory purposes, the transmitted ﬁeld is imaged (MO2, Nikon CFI
Plan Achro 10X) on a mirror, and double passing through
4 -plate results in a s-polarized ﬁeld. The PBS therefore reﬂects the entire optical ﬁeld, which is consecutively
imaged (MO2, Nikon CFI Plan Fluor 4X) on the camera (CAM, Thorlabs DCC1545M). We rescale the camera
image via linear interpolation to ﬁt the number of pixels
of the SLM. This step is necessary due to (i) an imaging magniﬁcation of 2.5, and (ii) the diﬀerent pixel sizes
of SLM (12.5 µm) and camera (5.2 µm). The detected
state is xi = (αEi)2 with α =
Isat · ND. Here, Isat is
the camera’s saturation intensity and ND the transmission through a neutral density ﬁlter (ND) always selected
such that the the dynamical range of the camera is fully
exploited, while avoiding over-exposure (maximum gray
scale GS=255). After multiplication with scalar β, we
add a constant phase oﬀset θi and send the resulting matrix back to the SLM. Ignoring the DOE’s eﬀect, each
pixel therefore corresponds to an Ikeda map:
xi(n + 1) = GS cos2(βxi(n) + θi).
We write the SLM state as vector x(n + 1), yet in the
experiment this state corresponds to a square array of
SLM pixels.
Illumination wavelength, DOE (HOLOOR MS-443-
650-Y-X), as well as MO1 and MO2 were chosen such
that the spacing between diﬀractive orders matches the
pixel-spacing of the SLM. Therefore, upon adding the
DOE to the beam path, the optical ﬁeld on the camera becomes EC
Ej, where WDOE is the
networks coupling matrix created by the DOE. In Fig.
1(c) we show the experimentally obtained W DOE for a
network of 30×30 nodes. Upon inspection of the inset
one can see that locally connectivity strengths vary signiﬁcantly. This is due to each pixel illuminating a DOE
area comparable to the DOE’s lowest spatial frequency.
As this area shifts slightly from pixel to pixel, the intensity distribution between diﬀractive orders varies. This
intended eﬀect inherently creates the heterogeneous photonic network topology needed for computation . Finally, the photonic RNN’s state x(n + 1) is given by
xi(n + 1) = GS · cos2(β · α
u(n + 1) + θi).
Here u(n + 1) is the information to be injected into the
RNN and γ is the signal injection strength. Matlab is
used to control all instruments, update the network state
and to inject the input information weighted by matrix
. The overall update rate of the entire system is
Currently, the maximum size of networks we
can realize consist of ∼2500 nodes, which is limited by
the imaging setup’s ﬁeld of view and not by the concept
NETWORK READOUT WEIGHTS
Having created a recurrent photonic neural network
driven by external data, the ﬁnal to information processing is to adjust the system such that it performs the
required computation. This is typically achieved by modifying connection weights according to some learning routine. Inspired by the RC concept, we constrain learning
(a) Learning curves for a photonic RNN with two
phase oﬀsets φi across the network.
A fraction of (1 −µ)
nodes have a response function with a negative slope (Θ0 =
0.17π), the remaining nodes experience a response function
with a positive slope (Θ0 = 0.43π). (b) An almost symmetric
distribution of phase oﬀsets, resulting in positive and negative
node responses, beneﬁts computational performance.
induced weight adjustment to the readout layer. Our 900
RNN nodes are spatially distributed, and we therefore
can use a simple lens (Thorlabs AC254-400-B) to image
the SLM’s state onto a commercial array of micro mirrors (DLi4120 XGA, pitch 13.68 µm). Micro mirrors can
be ﬂipped between ±12 °, such that for -12 °the optical
signal is directed to a detector. The detectors photo current then corresponds to the RNN output. With WDMD
as the readout weight vector, the RNN output becomes
yout(n + 1) = δ
 WDMD(1 −x(n + 1))
Here, δ relates the power recorded by the power meter
(Thorlabs S150C and PM100A) to the SLM state. The
signal directed towards the DMD is orthogonally polarized compared to the one directed to the camera, resulting in xDMD(n) = 1 −x(n). In the experiment weight
vector WDMD corresponds to a square matrix, which can
be seen in Fig. 1(b). The image labeled DMD shows a
typical conﬁguration of the DMD with WDMD. As the
contribution of each node is either on or oﬀ, WDMD consists of Boolean entries only. Weights are not temporal
modulations as in delay system implementations of RC
 , and therefore can be implemented by passive attenuations in reﬂection or transmission. Such passive processes are energy eﬃcient and typically do not results in
a bandwidth limitation. In this speciﬁc implementation,
once trained, mirrors can simply remain in their position,
and if mechanically ﬁxed, would not consume additional
energy. Also, readout eq. 4 is optically performed for all
elements in parallel.
PHOTONIC LEARNING
It is now the task of a learning algorithm to tailor
WDMD such that signal yout(n + 1) approximates a target value as good as possible.
In our experiment, we
employ a version of reinforcement learning. The learning
input signal is injected after inverting the weight assigned
to one node. The error εk of signal yout
(n + 1) obtained
for conﬁguration WDMD
is then compare to the error
εk−1, where k is the index of learning iterations. If the
error is reduced, we keep DMD conﬁguration WDMD
if not, we revert back to WDMD
and invert a diﬀerent
weight. The weight to be updated is determined by the
largest entrys W select,max
position lk according to
=rand(N) · Wbias,
lk, W select,max
= max(Wselect
N + Wbias,
rand(N) creates a random vector with N entries, and
at the start WDMD
and Wbias are randomly initialized.
Wbias acts as a bias vector, whose values are increased
N each learning iteration while the bias belonging to
the most recently updated weight is set to zero. This results in a randomized selection process with a bias away
from inverting recently updated weights. In simulations
we found that reinforcement learning including such a
bias showed signiﬁcantly faster learning convergence. As
a task to be performed by our system, we chose nonlinear time series prediction. The injected signal u(n + 1)
is the chaotic Mackey-Glass (MG) sequence , and the
RNN’s learning target is yT (n + 1) = u(n + 2), the onetime-step-prediction of the MG system. Parameters of
the temporal MG sequence where identical to , using
an integration step size of 0.1. For determining the error
εk we discarded the ﬁrst 30 data points due to their transient nature. The RNNs remaining output sequence was
then inverted, its oﬀset subtracted and normalized by its
standard deviation, creating signal ˜yout
. The error was
measured by εk = σ(yT −˜yout
), where σ is the standard
deviation and εk therefore corresponds to the normalized
mean square error (NMSE).
At this stage we would like to stress a signiﬁcant diﬀerence between neural networks emulated on digital electronic computers and our photonic hardware implementation. In our system, all connection weights are positive, and WDMD is boolean. This restricts the functional space available for approximating the targeted
input-output transformation.
As a result, ﬁrst evaluations of the learning procedure and prediction of the
MG series suﬀered from minor performance. However,
we were able to mitigate this restriction by harnessing the non-monotonous slope of the cos2 nonlinearity.
We randomly divided the oﬀest phases θi|i=1...N,
resulting in nodes with negative and positive slope of
their response function. We chose Θ0 = 42ˆ=0.17π and
Θ0 + ∆Θ = 106ˆ=0.43π, respectively, with a probability of 1 −µ for θi = Θ0. As RNN-states and WDOE
entries are exclusively positive, the nonlinear transformation of nodes with θi = Θ0 is predominantly along a
positive slope, for θi = Θ0 + ∆Θ along a negative slope.
(a) Learning performance at optimal parameters
(β=0.8, γ=0.4, µ=0.45). (b) The photonic RNN’s predicted
output in µW (blue line) can hardly be diﬀerentiated from
the prediction target (red dots). Prediction error ε is given
by the yellow dashed data.
This enables the reinforcement learning procedure to select from nonlinear transformations with positive and
negative slopes. We used feedback β = 0.8 and injection gain γ = 0.4, and learning curves for various ratios
(µ = [0.25, 0.35, 0.45, 0.5]) are shown in Fig. 2 (a). They
reveal a strong impact of this symmetry breaking. Optimum performance for each µ is shown in 2(b). Best
performance is found for an RNN operating around almost equally distributed operating points at µ = 0.45.
This demonstrates that the absence of negative weights
in WDMD, WDOE and x can be partially compensated
for by incorporating nonlinear transformations with positive as well as negative slopes.
This result is of high
signiﬁcance for optical neural networks, which, e.g. motivated by robustness considerations, renounce making
use of the optical phase to implement negative weights.
We further optimized our system’s performance by
scanning the remaining parameters β and γ. In Fig. 3
(a) we show the error convergence under optimized global
conditions for a training sample size of 500 steps (blue
stars). The error eﬃciently reduces, and ﬁnally stabilizes
at ε ≈0.013. Considering learning is limited to Boolean
readout weights this is an excellent result. After training, the prediction performance is evaluated further on
a sequence of 4500 datapoints consecutive data points
which were not part of the training dataset.
As indicated by the red line in the same panel, the testing error
matches the training error. We can therefore conclude
that our photonic RNN successfully generalized the underlying target system’s properties. The excellent prediction performance can be appreciated in Fig.
Data belonging to the left y-axis (blue line) shows the
recorded output power, while on the right y-axis (red
dots) we show the normalized prediction target signal. A
diﬀerence between both is hardly visible, and the prediction error ε (yellow dashed line) is small. Down-sampling
the injected signals by 3 creates condition identical to
 . Under these conditions our error (ε = 0.042) is
larger by a factor of 2.2 relative to a delay RC based on
a semiconductor laser and by 6.5 relative to a Mach-
Zehnder modulator based setup . These comparisons
have to be evaluated in the light of the signiﬁcantly increased level of hardware implementation in our current
setup. In , readout weights were applied digitally
in an oﬀ-line procedure using weights with double precision. In a strong impact of digitization resolution on
the computational performance was identiﬁed, suggesting that ε can be signiﬁcantly reduced by increasing the
resolution of WDMD.
CONCLUSION
We demonstrated a photonic RNN consisting of hundreds of photonic nonlinear nodes and the implementation of photonic reinforcement learning. Using a simple
Boolean valued readout implemented with a DMD, we
trained our system to predict the chaotic MG sequence.
The resulting prediction error is very low despite of the
Boolean readout weights.
In our work we demonstrate how symmetry breaking inside the RNN can compensate for exclusively positive intensities in our analogue neural networks systems.
These results resolve a complication of general
importance to neural networks implemented in analog
hardware. Hardware-implemented networks and readout
weights based on physical devices open the door to a
new class of experiments, i.e. evaluating the robustness
and eﬃciency of learning strategies in fully implemented
analogue neural networks. The ﬁnal step, a photonic realization of the input, should be straight forward, as it
only requires a complex spatial distribution of the input information. Finally, our system is not limited to
the reported slow opto-electronic system. Extremely fast
all-optical systems can be realized employing the same
concept since we intentionally implemented a 4f architecture to allow for self-coupling .
FUNDING INFORMATION
The authors acknowledge the support of the Region
Bourgogne Franche-Comt´e.
This work was supported
by Labex ACTION program (contract ANR-11-LABX-
0001-0) and via the Volkswagen Foundation NeuroQNet
ACKNOWLEDGMENTS
The authors would like to thank Christian Markus Dietrich for valuable contributions to earlier versions of the
 Y. LeCun, Y. Bengio, and G. Hinton, Nature 521, 436
 A. Sinha, J. Lee, S. Li, and G. Barbastathis, Optica 4,
1117 .
 H. Jaeger and H. Haas, Science (New York, N.Y.) 304,
78 .
 K. Vandoorne, W. Dierckx, B. Schrauwen, D. Verstraeten, R. Baets, P. Bienstman,
and J. Van Campenhout, Optics Express 16, 11182 .
 L. Appeltant, M. C. Soriano, G. Van der Sande, J. Danckaert, S. Massar, J. Dambre, B. Schrauwen, C. R. Mirasso,
and I. Fischer, Nature Communications 2, 468 .
 K. Wagner and D. Psaltis, Applied Optics 26, 5061
 C. Denz, Optical Neural Networks .
 F. Duport, B. Schneider, A. Smerieri, M. Haelterman,
and S. Massar, Optics Express 20, 22783 .
B. Schrauwen, M. Haelterman,
and S. Massar, Scientiﬁc Reports 2, 287 .
 L. Larger, M. C. Soriano, D. Brunner, L. Appeltant, J. M.
Gutierrez, L. Pesquera, C. R. Mirasso,
and I. Fischer,
Optics Express 20, 3241 .
 D. Brunner, M. C. Soriano, C. R. Mirasso, and I. Fischer,
Nature Communications 4, 1364 .
 Y. Shen, N. C. Harris, S. Skirlo, M. Prabhu, T. Baehr-
Jones, M. Hochberg, X. Sun, S. Zhao, H. Larochelle,
D. Englund, and M. Soljacic, Nature Photonics 11, 441
 , arXiv:1610.02365.
 P. Antonik, M. Haelterman,
and S. Massar, Cognitive
Computation , 1 .
 D. Brunner and I. Fischer, Optics Letters 40, 3854
 J. Bueno, D. Brunner, M. Soriano, and I. Fischer, Optics
Express 25, 2401 .
 M. C. Ort´ın, S.and Soriano, L. Pesquera, D. Brunner,
D. San-Mart´ın, I. Fischer, C. R. Mirasso,
Guti´errez, Scientiﬁc Reports 5, 14945 .