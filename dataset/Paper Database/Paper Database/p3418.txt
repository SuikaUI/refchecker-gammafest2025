Vol.:(0123456789)
Minds and Machines 30:99–120
 
The Ethics of AI Ethics: An Evaluation of Guidelines
Thilo Hagendorff1
Received: 1 October 2019 / Accepted: 21 January 2020 / Published online: 1 February 2020
© The Author(s) 2020
Current advances in research, development and application of artificial intelligence
(AI) systems have yielded a far-reaching discourse on AI ethics. In consequence, a
number of ethics guidelines have been released in recent years. These guidelines
comprise normative principles and recommendations aimed to harness the “disruptive” potentials of new AI technologies. Designed as a semi-systematic evaluation,
this paper analyzes and compares 22 guidelines, highlighting overlaps but also omissions. As a result, I give a detailed overview of the field of AI ethics. Finally, I also
examine to what extent the respective ethical principles and values are implemented
in the practice of research, development and application of AI systems—and how
the effectiveness in the demands of AI ethics can be improved.
Keywords  Artificial intelligence · Machine learning · Ethics · Guidelines ·
Implementation
1  Introduction
The current AI boom is accompanied by constant calls for applied ethics, which are
meant to harness the “disruptive” potentials of new AI technologies. As a result, a
whole body of ethical guidelines has been developed in recent years collecting principles, which technology developers should adhere to as far as possible. However,
the critical question arises: Do those ethical guidelines have an actual impact on
human decision-making in the field of AI and machine learning? The short answer
is: No, most often not. This paper analyzes 22 of the major AI ethics guidelines and
issues recommendations on how to overcome the relative ineffectiveness of these
guidelines.
AI ethics—or ethics in general—lacks mechanisms to reinforce its own normative claims. Of course, the enforcement of ethical principles may involve
* Thilo Hagendorff
thilo.hagendorff@uni‑tuebingen.de
Cluster of Excellence “Machine Learning: New Perspectives for Science”, University
of Tuebingen, Tübingen, Germany
T. Hagendorff
reputational losses in the case of misconduct, or restrictions on memberships in
certain professional bodies. Yet altogether, these mechanisms are rather weak and
pose no eminent threat. Researchers, politicians, consultants, managers and activists have to deal with this essential weakness of ethics. However, it is also a reason
why ethics is so appealing to many AI companies and institutions. When companies
or research institutes formulate their own ethical guidelines, regularly incorporate
ethical considerations into their public relations work, or adopt ethically motivated
“self-commitments”, efforts to create a truly binding legal framework are continuously discouraged. Ethics guidelines of the AI industry serve to suggest to legislators that internal self-governance in science and industry is sufficient, and that no
specific laws are necessary to mitigate possible technological risks and to eliminate
scenarios of abuse . And even when more concrete laws concerning AI
systems are demanded, as recently done by Google , these demands remain
relatively vague and superficial.
Science- or industry-led ethics guidelines, as well as other concepts of self-governance, may serve to pretend that accountability can be devolved from state authorities and democratic institutions upon the respective sectors of science or industry.
Moreover, ethics can also simply serve the purpose of calming critical voices from
the public, while simultaneously the criticized practices are maintained within the
organization. The association “Partnership on AI” which brings together
companies such as Amazon, Apple, Baidu, Facebook, Google, IBM and Intel is
exemplary in this context. Companies can highlight their membership in such associations whenever the notion of serious commitment to legal regulation of business
activities needs to be stifled.
This prompts the question as to what extent ethical objectives are actually implemented and embedded in the development and application of AI, or whether merely
good intentions are deployed. So far, some papers have been published on the subject of teaching ethics to data scientists but by and large very little to
nothing has been written about the tangible implementation of ethical goals and values. In this paper, I address this question from a theoretical perspective. In a first
step, 22 of the major guidelines of AI ethics will be analyzed and compared. I will
also describe which issues they omit to mention. In a second step, I compare the
principles formulated in the guidelines with the concrete practice of research and
development of AI systems. In particular, I critically examine to what extent the
principles have an effect. In a third and final step, I will work out ideas on how AI
ethics can be transformed from a merely discursive phenomenon into concrete directions for action.
2  Guidelines in AI Ethics
2.1  Method
Research in the field of AI ethics ranges from reflections on how ethical principles
can be implemented in decision routines of autonomous machines over meta-studies about
AI ethics or the empirical analysis on how
trolley problems are solved to reflections on specific problems
 and comprehensive AI guidelines . This paper mainly deals
with the latter issue. The list of ethics guidelines considered in this article therefore
includes compilations that cover the field of AI ethics as comprehensively as possible. To the best of my knowledge, a few preprints and papers are currently available, which also deal with the comparison of different ethical guidelines . While especially the paper from Jobin
et al. is a systematic scoping review of all the existing literature on AI ethics, this paper does not aim at a full analysis of every available soft-law or non-legal
norm document on AI, algorithm, robot, or data ethics, but rather a semi-systematic
overview of issues and normative stances in the field, demonstrating how the details
of AI ethics relate to a bigger picture.
The selection and compilation of 22 major ethical guidelines were based on a
literature analysis. This selection was undertaken in two phases. In the first phase,
I searched different databases, namely Google, Google Scholar, Web of Science,
ACM Digital Library, arXiv, and SSRN for hits or articles on “AI ethics”, “artificial intelligence ethics”, “AI principles”, “artificial intelligence principles”, “AI
guidelines”, and “artificial intelligence guidelines, following every link in the first
25 search results, while at the same time ignoring duplicates in the search process.
During the analysis of the search results, I also sifted through the references in order
to manually find further relevant guidelines. Furthermore, I used Algorithm Watch’s
AI Ethics Guidelines Global Inventory, a crowdsourced, comprehensive list of ethics
guidelines, to check whether I missed relevant guidelines. Via the list, I found three
further guidelines that meet the criteria for the selection. In this context, a shortcoming one has to consider is that my selection is biased towards documents which are
western/northern in nature, excluding guidelines which are not written in English.
I rejected all documents older than 5  years in order to only take guidelines
into account that are relatively new. Documents that only refer to a national context—such as for instance position papers of national interest groups , the report of the British House of Lords , or the Nordic engineers’ stand on Artificial Intelligence and Ethics
(Podgaiska and Shklovski)—were excluded from the compilation. Nevertheless,
I included the European Commission’s “Ethics Guidelines for Trustworthy AI”
 , the Obama administration’s “Report on the Future of Artificial Intelligence” , and the “Beijing AI Principles” , which are backed by the Chinese Ministry of Science and Technology. I have included these three guidelines because
they represent the three largest AI “superpowers”. Furthermore, I included the
“OECD Principles on AI” due to their supranational character. Scientific papers or texts that
fall into the category of AI ethics but focus on one or more specific aspects of
the topic were not considered either. The same applies to guidelines or toolkits,
T. Hagendorff
which are not specifically about AI but rather about big data, algorithms or robotics . I further excluded corporate policies, with the exception of the “Information Technology Industry AI
Policy Principles” , the principles of the “Partnership on AI” , the
IEEE first and second version of the document on “Ethically Aligned Design”
 , as well as the brief principle lists of Google , Microsoft
 , DeepMind (DeepMind), OpenAI , and IBM 
which have become well-known through media coverage. Other large companies
such as Facebook or Twitter have not yet published any systematic AI guidelines,
but only isolated statements of good conduct. Paula Boddington’s book on ethical
guidelines funded by the Future of Life Institute was also not considered
as it merely repeats the Asilomar principles .
The decisive factor for the selection of ethics guidelines was not the depth of
detail of the individual document, but the discernible intention of a comprehensive mapping and categorization of normative claims with regard to the field of
AI ethics. In Table 1, I only inserted green markers if the corresponding issues
were explicitly discussed in one or more paragraphs. Isolated mentions without
further explanations were not considered, unless the analyzed guideline is so
short that it consists entirely of brief mentions altogether.
Table 1   Overview of AI ethics guidelines and the different issues they cover
The European Commission's High-Level Expert
Group on Arﬁcial Intelligence
Report on the Future of Arﬁcial Intelligence
Beijing AI Principles
OECD Recommendaon of the Council on Arﬁcial
Intelligence
The Malicious Use of Arﬁcial Intelligence
The Asilomar AI Principles
AI Now 2016 Report
AI Now 2017 Report
AI Now 2018 Report
AI Now 2019 Report
Principles for Accountable Algorithms and a Social
Impact Statement for Algorithms
Montréal Declaraon for Responsible
Development of Arﬁcial Intelligence
OpenAI Charter
Ethically Aligned Design: A Vision for Priorizing
Human Well-being with Autonomous and
Intelligent Systems (Version for Public Discussion)
Ethically Aligned Design: A Vision for Priorizing
Human Well-being with Autonomous and
Intelligent Systems (First Edion)
ITI AI Policy Principles
Microso AI principles
DeepMind Ethics & Society Principles
Arﬁcial Intelligence at Google
Everyday Ethics for Arﬁcial Intelligence
Partnership on AI
number of menons
 
 
Iniave on
Intelligent
Iniave on
Intelligent
(Informaon
Technology
Corporaon
(DeepMind)
 
AI principles
AI principles
AI prinicples
AI principles
of the OECD
analysis of
scenarios of
metaanalysis
principles
principles
statements
statements
statements
statements
principles of
the FAT ML
released by
Université
de Montréal
principles
ethical use
of ethical
aspects in
the context
of ethical
aspects in
the context
about basic
principles
short list of
ethical use
principles
ethical use
principles
ethical use
IBM’s short
ethical use
principles of
associaon
privacy protecon
fairness, non-discriminaon, jusce
accountability
transparency, openness
safety, cybersecurity
common good, sustainability, well-being
human oversight, control, auding
solidarity, inclusion, social cohesion
explainability, interpretabiliy
science-policy link
legislave framework, legal status of AI systems
future of employment/worker rights
responsible/intensiﬁed research funding
public awareness, educaon about AI and its risks
dual-use problem, military, AI arms race
ﬁeld-speciﬁc deliberaons (health, military, mobility etc.)
human autonomy
diversity in the ﬁeld of AI
cerﬁcaon for AI products
protecon of whistleblowers
cultural diﬀerences in the ethically aligned design of AI
hidden costs (labeling, clickwork, contend moderaon,
energy, resources)
notes on technical implementaons
yes, but very
proporon of women among authors (f/m)
each chapter
each chapter
length (number of words)
aﬃliaon (government, industry, science)
government government
government
number of ethical aspects
The Ethics of AI Ethics: An Evaluation of Guidelines﻿
2.2  Multiple Entries
As shown in Table  1, several issues are unsurprisingly recurring across various guidelines. Especially the aspects of accountability, privacy or fairness
appear altogether in about 80% of all guidelines and seem to provide the minimal requirements for building and using an “ethically sound” AI system. What is
striking here is the fact that the most frequently mentioned aspects are those for
which technical fixes can be or have already been developed. Enormous technical
efforts are undertaken to meet ethical targets in the fields of accountability and
explainable AI , fairness and discrimination aware data
mining , as well as privacy . Many
of those endeavors are unified under the FAT ML or XAI community . Several tech-companies already offer tools for
bias mitigation and fairness in machine learning. In this context, Google, Microsoft and Facebook have issued the “AI Fairness 360” tool kit, the “What-If Tool”,
“Facets”, “fairlern.py” and “Fairness Flow”, respectively .
Accountability, explainability, privacy, justice, but also other values such as
robustness or safety are most easily operationalized mathematically and thus tend
to be implemented in terms of technical solutions. With reference to the findings of psychologist Carol Gilligan, one could argue at this point that the way AI
ethics is performed and structured constitutes a typical instantiation of a maledominated justice ethics . In the 1980s, Gilligan demonstrated in
empirical studies that women do not, as men typically do, address moral problems
primarily through a “calculating”, “rational”, “logic-oriented” ethics of justice,
but rather interpret them within a wider framework of an “empathic”, “emotionoriented” ethics of care. In fact, no different from other parts of AI research, the
discourse on AI ethics is also primarily shaped by men. My analysis of the distribution of female and male authors of the guidelines, as far as authors were indicated in the documents, showed that the proportion of women was 41.7%. This
ratio appears to be close to balance. However, it should be considered that the
ratio of female to male authors is reduced to a less balanced 31.3% if the four AI
Now Reports are discarded, which come from an organization that is deliberately
led by women. The proportion of women is lowest at 7.7% in the FAT ML community’s guidelines which are focused predominantly on technical solutions (Diakopoulos et al.). Accordingly, the “male way” of thinking about ethical problems
is reflected in almost all ethical guidelines by way of mentioning aspects such as
accountability, privacy or fairness. In contrast, almost no guideline talks about
AI in contexts of care, nurture, help, welfare, social responsibility or ecological
networks. In AI ethics, technical artefacts are primarily seen as isolated entities
that can be optimized by experts so as to find technical solutions for technical
problems. What is often lacking is a consideration of the wider contexts and the
comprehensive relationship networks in which technical systems are embedded.
In accordance with that, it turns out that precisely the reports of AI Now , an organization primarily led by women, do not conceive AI applications in isolation, but
within a larger network of social and ecological dependencies and relationships
T. Hagendorff
 , corresponding most closely with the ideas and tenets
of an ethics of care .
What are further insights from my analysis of the ethics guidelines, as summarized in Table 1? On the one hand, it is noticeable that guidelines from industrial
contexts name on average 9.1 distinctly separated ethical aspects, whereas the average for ethics codes from science is 10.8. The principles of Microsoft’s AI ethics are
the most brief and minimalistic . The OpenAI Charta
names only four points and is thus situated at the bottom of the list .
Conversely, the IEEE guideline contains the largest volume with more than 100.000
words . Finally, yet importantly, it is noteworthy that almost all guidelines suggest that technical solutions exist for many of the problems described. Nevertheless, there are only two guidelines which contain genuinely technical explanations at
all—albeit only very sparsely. The authors of the guideline on the “Malicious Use of
AI” provide the most extensive commentary here .
2.3  Omissions
Despite the fact that the guidelines contain various parallels and several recurring
topics, what are issues the guidelines do not discuss at all or only very occasionally? Here, I want to give a (non-exhaustive) overview of issues that are missing.
Two things should be considered in this context. First, the sampling method used
to select the AI ethics guidelines has an effect on the list of issues and omissions.
When deliberately excluding for instance robot ethics guidelines, this has the effect
that the list of entries lacks issues that are connected with robotics. Second, not all
omissions can be treated equally. There are omissions which are missing or severely
underrepresented without any good reason—for instance the aspect of political abuse or “hidden” social and ecological costs of AI systems—, and omissions
that can be justified—for instance deliberations on artificial general intelligence or
machine consciousness, since those technologies are purely speculative.
Nevertheless, in view of the fact that significant parts of the AI community
see the emergence of artificial general intelligence as well as associated dangers
for humanity or existential threats as a likely scenario , one could argue that those topics could be discussed in ethics guidelines under the umbrella of potential prohibitions to pursue certain research strands in this area . The fact that
artificial general intelligence is not discussed in the guidelines may be due to the
fact that most of the guidelines are not written by research groups from philosophy
or other speculative disciplines, but by researchers with a background directly in
computer science or its application. In this context, it is noteworthy that the fear
of the emergence of superintelligence is more frequently expressed by people who
lack technical experience in the field of AI—one just has to think of people like
Stephen Hawking, Elon Musk or Bill Gates—while “real” experts generally regard
the idea of a strong AI as rather absurd . Perhaps the same holds true
for the question of machine consciousness and the ethical problems associated with
The Ethics of AI Ethics: An Evaluation of Guidelines﻿
it , as this topic is also omitted from all examined ethical guidelines.
What is also striking is the fact that only the Montréal Declaration for Responsible
Development of Artificial Intelligence as well as the AI Now 2019 Report
 explicitly addresses the aspect of democratic control, governance and political deliberation of AI systems. The mentioned documents are also the only guidelines that explicitly prohibits imposing certain lifestyles or concepts of “good living”
on people by AI systems, as it is for example demonstrated in the Chinese scoring
system . The former document further criticizes the application of AI systems for the reduction of social cohesion, for example by isolating
people in echo chambers . In addition, hardly any guideline
discusses the possibility for political abuse of AI systems in the context of automated
propaganda, bots, fake news, deepfakes, micro targeting, election fraud, and the like.
What is also largely absent from most guidelines is the issue of a lack in diversity
within the AI community. This lack of diversity is prevailing in the field of artificial
intelligence research and development, as well as in the workplace cultures shaping
the technology industry. In the end, a relatively small group of predominantly white
men determines how AI systems are designed, for what purposes they are optimized,
what is attempted to realize technically, etc. The famous AI startup “nnaisense” run
by Jürgen Schmidhuber, which aims at generating an artificial general intelligence,
to name just one example, employs only two women—one scientist and one office
manager—in its team, but 21 men. Another matter, which is not covered at all or
only very rarely mentioned in the guidelines, are aspects of robot ethics. As mentioned in the methods chapter, specific guidelines for robot ethics exist, most prominently represented by Asimov’s three laws of robotics , but those
guidelines were intentionally excluded from the analysis. Nonetheless, advances
in AI research contribute, for instance, to increasingly anthropomorphized technical devices. The ethical question that arises in this context echoes Immanuel Kant’s
“brutalization argument” and states that the abuse of anthropomorphized agents—
as, for example, is the case with language assistants —also promotes the likelihood of violent actions between people . Apart from
that, the examined ethics guidelines pay little attention to the rather popular trolley
problems and their alleged relation to ethical questions surrounding self-driving cars or other autonomous vehicles. In connection to this, no guideline deals in detail with the obvious question where systems of algorithmic decision making are superior or inferior, respectively, to human decision routines. And
finally, virtually no guideline deals with the “hidden” social and ecological costs
of AI systems. At several points in the guidelines, the importance of AI systems for
approaching a sustainable society is emphasized . However, it
is omitted—with the exception of the AI Now 2019 Report —that producer
and consumer practices in the context of AI technologies may in themselves contradict sustainability goals. Issues such as lithium mining, e-waste, the one-way use of
rare earth minerals, energy consumption, low-wage “clickworkers” creating labels
for data sets or doing content moderation are of relevance here . Although “clickwork” is a
necessary prerequisite for the application of methods of supervised machine learning, it is associated with numerous social problems , such as low wages, work conditions and psychological
work consequences, which tend to be ignored by the AI community. Finally, yet
importantly, not a single guideline raises the issue of public–private partnerships
and industry-funded research in the field of AI. Despite the massive lack of transparency regarding the allocation of research funds, it is no secret that large parts of
university AI research are financed by corporate partners. In light of this, it remains
questionable to what extent the ideal of freedom of research can be upheld—or
whether there will be a gradual “buyout” of research institutes.
3  AI in Practice
3.1  Business Versus Ethics
The close link between business and science is not only revealed by the fact that all
of the major AI conferences are sponsored by industry partners. The link between
business and science is also well illustrated by the AI Index 2018 . Statistics show that, for example, the number of corporate-affiliated AI papers
has grown significantly in recent years. Furthermore, there is a huge growth in the
number of active AI startups, each supported by huge amounts of annual funding
from Venture Capital firms. Tens of thousands of AI-related patents are registered
each year. Different industries are incorporating AI applications in a broad variety of
fields, ranging from manufacturing, supply-chain management, and service development, to marketing and risk assessment. All in all, the global AI market comprises
more than 7 billion dollars .
A critical look at this global AI market and the use of AI systems in the economy and other social systems sheds light primarily on unwanted side effects of the
use of AI, as well as on directly malevolent contexts of use. These occur in various
areas . Leading, of course, is
the military use of AI in cyber warfare or regarding weaponized unmanned vehicles
or drones . According to
media reports, the US government alone intends to invest two billion dollars in military AI projects over the next 5 years . Moreover, governments
can use AI applications for automated propaganda and disinformation campaigns
 , social control , surveillance , face recognition or sentiment analysis , social sorting , or improved interrogation techniques . Notwithstanding the above, companies can cause massive job losses due to AI implementation , conduct unmonitored forms of AI experiments on
society without informed consent , suffer from data breaches
 , use unfair, biased algorithms , provide unsafe AI
products , use trade secrets to disguise harmful or flawed AI
functionalities , rush to integrate and put immature AI applications on the market and many more. Furthermore, criminal or black-hat hackers
can use AI to tailor cyberattacks, steal information, attack IT infrastructures, rig
elections, spread misinformation for example through deepfakes, use voice synthesis
The Ethics of AI Ethics: An Evaluation of Guidelines﻿
technologies for fraud or social engineering , or disclose personal
traits that are actually secret or private via machine learning applications . All in all, only a very small number of
papers is published about the misuse of AI systems, even though they impressively
show what massive damage can be done with those systems .
3.2  AI Race
While the United States currently has the largest number of start-ups, China claims
to be the “world leader in AI” in 2030 . This claim is supported by
the sheer amount of data that China has at its disposal to train its own AI systems,
as well as by the large label companies that take over the manual preparation of
data sets for supervised machine learning . Conversely, China is seen
to have a weakness vis-à-vis the USA in that the investments of the market leaders Baidu, Alibaba and Tencent are too application-oriented comprising areas such
as autonomous driving, finance or home appliances, while important basic research
on algorithm development, chip production or sensor technology is neglected . The constant comparison between China, the USA and Europe renders the
fear of being inferior to each other an essential motive for efforts in the research and
development of artificial intelligence.
Another justification for competitive thinking is provided by the military context.
If the own “team”, framed in a nationalist way, does not keep pace, so the consideration, it will simply be overrun by the opposing “team” with superior AI military
technology. In fact, potential risks emerge from the AI race narrative, as well as
from an actual competitive race to develop AI systems for technological superiority . One risk of this rhetoric is that “impediments”
in the form of ethical considerations will be eliminated completely from research,
development and implementation. AI research is not framed as a cooperative global
project, but as a fierce competition. This competition affects the actions of individuals and promotes a climate of recklessness, repression, and thinking in hierarchies,
victory and defeat. The race for the best AI, whether a mere narrative or a harsh
reality, reduces the likelihood of the establishment of technical precaution measures
as well as of the development of benevolent AI systems, cooperation, and dialogue
between research groups and companies. Thus, the AI race stands in stark contrast to
the idea of developing an “AI4people” . The same holds true for
the idea of an “AI for Global Good”, as was proposed at the 2017’s ITU summit, or
the large number of leading AI researchers who signed the open letter of the “Future
of Life Institute”, embracing the norm that AI should be used for prosocial purposes.
Despite the downsides, in less public discourses and in concrete practice, an AI
race has long since established itself. Along with that development, in- and outgroup-thinking has intensified. Competitors are seen more or less as enemies or at
least as threats against which one has to defend oneself. Ethics, on the other hand, in
its considerations and theories always stresses the danger of an artificial differentiation between in- and outgroups . Constructed outgroups are subject
T. Hagendorff
to devaluation, are perceived de-individualized and in the worst case can become
victims of violence simply because of their status as “others” . I argue that only by abandoning such thinking in- and outgroups
may the AI race be reframed into a global cooperation for beneficial and safe AI.
3.3  Ethics in Practice
Do ethical guidelines bring about a change in individual decision-making regardless of the larger social context? In a recent controlled study, researchers critically
reviewed the idea that ethical guidelines serve as a basis for ethical decision-making
for software engineers . In brief, their main finding was that
the effectiveness of guidelines or ethical codes is almost zero and that they do not
change the behavior of professionals from the tech community. In the survey, 63
software engineering students and 105 professional software developers were scrutinized. They were presented with eleven software-related ethical decision scenarios,
testing whether the influence of the ethics guideline of the Association for Computing Machinery (ACM) in fact influences ethical decisionmaking in six vignettes, ranging from responsibility to report, user data collection,
intellectual property, code quality, honesty to customer to time and personnel management. The results are disillusioning: “No statistically significant difference in the
responses for any vignette were found across individuals who did and did not see the
code of ethics, either for students or for professionals.” .
Irrespective of such considerations on the microsociological level, the relative ineffectiveness of ethics can also be explained at the macrosociological level.
Countless companies are eager to monetize AI in a huge variety of applications.
This strive for a profitable use of machine learning systems is not primarily framed
by value- or principle-based ethics, but obviously by an economic logic. Engineers
and developers are neither systematically educated about ethical issues, nor are they
empowered, for example by organizational structures, to raise ethical concerns. In
business contexts, speed is everything in many cases and skipping ethical considerations is equivalent to the path of least resistance. Thus, the practice of development,
implementation and use of AI applications has very often little to do with the values
and principles postulated by ethics. The German sociologist Ulrich Beck once stated
that ethics nowadays “plays the role of a bicycle brake on an intercontinental airplane” . This metaphor proves to be particularly true in the context
of AI, where huge sums of money are invested in the development and commercial
utilization of systems based on machine learning , while ethical
considerations are mainly used for public relations purposes .
In their AI Now 2017 Report, Kate Crawford and her team state that ethics and
forms of soft governance “face real challenges” . This is mainly
due to the fact that ethics has no enforcement mechanisms reaching beyond a voluntary
and non-binding cooperation between ethicists and individuals working in research and
industry. So what happens is that AI research and development takes place in “closeddoor industry settings”, where “user consent, privacy and transparency are often overlooked in favor of frictionless functionality that supports profit-driven business models”
The Ethics of AI Ethics: An Evaluation of Guidelines﻿
 . Despite this dispensation of ethical principles, AI systems
are used in areas of high societal significance such as health, police, mobility or education. Thus, in the AI Now Report 2018, it is repeated that the AI industry “urgently
needs new approaches to governance”, since, “internal governance structures at most
technology companies are failing to ensure accountability for AI systems” . Thus, ethics guidelines often fall into the category of a “’trust us’ form
of [non-binding] corporate self-governance” and people
should “be wary of relying on companies to implement ethical practices voluntarily”
 .
The tension between ethical principles and wider societal interests on the one hand,
and research, industry, and business objectives on the other can be explained with
recourse to sociological theories. Especially on the basis of system theory it can be
shown that modern societies differ in their social systems, each working with their own
codes and communication media . Structural couplings
can lead decisions in one social system to influence other social systems. Such couplings, however, are limited and do not change the overall autonomy of social systems.
This autonomy, which must be understood as an exclusive, functionalist orientation
towards the system’s own codes is also manifested in the AI industry, business and science. All these systems have their own codes, their own target values, and their own
types of economic or symbolic capital via which they are structured and based upon
which decisions are made . Ethical intervention in those systems is
only possible to a very limited extent . A certain hesitance exists
towards every kind of intervention as long as these lie beyond the functional laws of
the respective systems. Despite that, unethical behavior or unethical intentions are not
solely caused by economic incentives. Rather, individual character traits like cognitive
moral development, idealism, or job satisfaction play a role, let alone organizational
environment characteristics like an egoistic work climate or (non-existent) mechanisms
for the enforcement of ethical codes . Nevertheless, many
of these factors are heavily influenced by the overall economic system logic. Ethics is
then, so to speak, “operationally effectless” .
And yet, such system-theoretical considerations apply only on a macro level of
observation and must not be generalized. Deviations from purely economic behavioral
logics in the tech industry occur as well, for example when Google withdrew from the
military project “Maven” after protests from employees or when people at
Microsoft protested against the company’s cooperation with Immigration and Customs
Enforcement (ICE) . Nevertheless, it must also be kept in mind here that,
in addition to genuine ethical motives, the significance of economically relevant reputation losses should not be underestimated. Hence, the protest against unethical AI projects can in turn be interpreted in an economic logic, too.
3.4  Loyalty to Guidelines
As indicated in the previous sections, the practice of using AI systems is poor in terms
of compliance with the principles set out in the various ethical guidelines. Great progress has been made in the areas of privacy, fairness or explainability. For example,
T. Hagendorff
many privacy-friendly techniques for the use of data sets and learning algorithms have
been developed, using methods where AI systems’ “sight” is “darkened” via cryptography, differential or stochastic privacy . Nevertheless, this contradicts the observation that
AI has been making such massive progress for several years precisely because of the
large amounts of (personal) data available. Those data are collected by privacy-invasive
social media platforms, smartphone apps, as well as Internet of Things devices with its
countless sensors. In the end, I would argue that the current AI boom coincides with
the emergence of a post-privacy society. In many respects, however, this post-privacy
society is also a black box society , in which, despite technical and
organizational efforts to improve explainability, transparency and accountability, massive zones of non-transparency remain, caused both by the sheer complexity of technological systems and by strategic organizational decisions.
For many of the issues mentioned in the guidelines, it is difficult to assess the extent
to which efforts to meet the set objectives are successful or whether conflicting trends
prevail. This is the case in the areas of safety and cybersecurity, the science-policy link,
future of employment, public awareness about AI risks, or human oversight. In other
areas, including the issue of hidden costs and sustainability, the protection of whistleblowers, diversity in the field of AI, the fostering of solidarity and social cohesion, the
respect for human autonomy, the use of AI for the common good or the military AI
arms race, it can certainly be stated that the ethical goals are being massively underachieved. One only has to think of the aspect of gender diversity: Even though ethical
guidelines clearly demand its improvement, the state of affairs is that on average 80%
of the professors at the world’s leading universities such as Stanford, Oxford, Berkeley
or the ETH are male . Furthermore, men make up more than 70%
of applicants for AI jobs in the U.S. . Alternatively, one can take
human autonomy: As repeatedly demanded in various ethical guidelines, people should
not be treated as mere data subjects, but as individuals. In fact, however, countless
examples show that computer decisions, regardless of their susceptibility to error, are
attributed a strong authority which results in the ignorance of individual circumstances
and fates . Furthermore, countless companies strive for the opposite of
human autonomy, employing more and more subtle techniques for manipulating user
behavior via micro targeting, nudging, UX-design and so on . Another example is that of cohesion: Many of the major scandals of the last
years would have been unthinkable without the use of AI. From echo chamber effects
 to the use of propaganda bots , or the spread
of fake-news , AI always played a key role to the effect of diminishing social cohesion, fostering instead radicalization, the decline of reason in public
discourse and social divides .
The Ethics of AI Ethics: An Evaluation of Guidelines﻿
4  Advances in AI Ethics
4.1  Technical Instructions
Given the relative lack of tangible impact of the normative objectives set out in
the guidelines, the question arises as to how the guidelines could be improved to
make them more effective. At first glance, the most obvious potential for improvement of the guidelines is probably to supplement them with more detailed technical explanations—if such explanations can be found. Ultimately, it is a major
problem to deduce concrete technological implementations from the very abstract
ethical values and principles. What does it mean to implement justice or transparency in AI-systems? What does a “human-centered” AI look like? How can
human oversight be obtained? The list of questions could easily be continued.
The ethics guidelines examined refer exclusively to the term “AI”. They never or
very seldom use more specific terminology. However, “AI” is just a collective term
for a wide range of technologies or an abstract large-scale phenomenon. The fact that
not a single prominent ethical guideline goes into greater technical detail shows how
deep the gap is between concrete contexts of research, development, and application
on the one side, and ethical thinking on the other. Ethicists must partly be capable
of grasping technical details with their intellectual framework. That means reflecting
on the ways data are generated, recorded, curated, processed, disseminated, shared,
and used , on the ways of designing algorithms and code,
respectively , or on the ways training data
sets are selected . In order to analyze all this in sufficient depth,
ethics has to partially transform to “microethics”. This means that at certain points,
a substantial change in the level of abstraction has to happen insofar as ethics aims
to have a certain impact and influence in the technical disciplines and the practice of
research and development of artificial intelligence . On the way
from ethics to “microethics”, a transformation from ethics to technology ethics, to
machine ethics, to computer ethics, to information ethics, to data ethics has to take
place. As long as ethicists refrain from doing so, they will remain visible in a general public, but not in professional communities.
A good example of such a microethical work which can be implemented easily and concretely in practice is the paper by Gebru et al. . The researchers propose the introduction of standardized datasheets listing the properties of
different training data sets, so that machine learning-practitioners can check to
what extent certain data sets are best suitable for their purposes, what the original intention was when the data set was created, what data the data set is composed of, how the data was collected and pre-processed, etc. The paper by Gebru
et al. makes it possible for practitioners to obtain a more informed decision on the
selection of certain training data sets, so that supervised machine learning ultimately becomes fairer, and more transparent, and avoids cases of algorithmic discrimination . Such work is, however, an exception.
In general, ethical guidelines postulate very broad, overarching principles which are then supposed to be implemented in a widely diversified set of
T. Hagendorff
scientific, technical and economic practices, and in sometimes geographically
dispersed groups of researchers and developers with different priorities, tasks and
fragmental responsibilities. Ethics thus operates at a maximum distance from the
practices it actually seeks to govern. Of course, this does not remain unnoticed
among technology developers. In consequence, the generality and superficiality
of ethical guidelines in many cases not only prevents actors from bringing their
own practice into line with them, but rather encourages the devolution of ethical
responsibility to others.
4.2  Virtue Ethics
Regardless of the fact that normative guidelines should be accompanied by in-depth
technical instructions—as far as they can reasonably be identified—, the question
still arises how the precarious situation regarding the application and fulfillment of
AI ethics guidelines can be improved. To address this question, one needs to take a
step back and look at ethical theories in general. In ethics, several major strands of
theories were created and shaped by various philosophical traditions. Those theories
range from deontological to contractualistic, utilitarian, or virtue ethical approaches
 . In the following, two of
these approaches—deontology and virtue ethics—will be selected to illustrate different approaches in AI ethics. The deontological approach is based on strict rules,
duties or imperatives. The virtue ethics approach, on the other hand, is based on
character dispositions, moral intuitions or virtues—especially “technomoral virtues”
 . In the light of these two approaches, the traditional type of AI ethics
can be assigned to the deontological concept . Ethics guidelines
postulate a fixed set of universal principles and maxims which technology developers should adhere to . The virtue ethics approach, on the other
hand, focuses more on “deeper-lying” structures and situation-specific deliberations,
on addressing personality traits and behavioral dispositions on the part of technology developers . Virtue ethics does not define codes of conduct but
focusses on the individual level. The technologists or software engineers and their
social context are the primary addressees of such an ethics , not technology itself.
I argue that the prevalent approach of deontological AI ethics should be augmented with an approach oriented towards virtue ethics aiming at values and character dispositions. Ethics is then no longer understood as a deontologically inspired
tick-box exercise, but as a project of advancing personalities, changing attitudes,
strengthen responsibilities and gaining courage to refrain from certain actions,
which are deemed unethical. When following the path of virtue ethics, ethics as a
scientific discipline must refrain from wanting to limit, control, or steer .
Very often, ethics or ethical guidelines are perceived as something whose purpose is
to stop or prohibit activity, to hamper valuable research and economic endeavors
 . I want to resign this negative notion of ethics. It should not be
the objective of ethics to stifle activity, but to do the exact opposite, i.e. broadening
The Ethics of AI Ethics: An Evaluation of Guidelines﻿
the scope of action, uncovering blind spots, promoting autonomy and freedom, and
fostering self-responsibility.
In view of AI ethics, approaches that focus on virtues aim at cultivating a moral
character, expressing technomoral virtues such as honesty, justice, courage, empathy, care, civility, or magnanimity, to name just a few . Those virtues
are supposed to raise the likelihood of ethical decision-making practices in organizations that develop and deploy AI applications. Cultivating a moral character, in
terms of virtue ethics, means to educate virtues in families, schools, communities,
as well as companies. At best, every individual, every member of a society should
encourage this cultivation, by generating the motivation to adopt and habituate practices that influence technology development and use in a positive manner. Especially
the subject of responsibility diffusion can only be circumvented when virtue ethics is
adopted on a broad and collective level in communities of tech professionals. Simply
every person involved in data science, data engineering and data economies related
to applications of AI has to take at least some responsibility for the implications
of their actions . This is why researchers such as Floridi argue that
every actor who is causally relevant for bringing about the collective consequence or
impacts in question, has to be held accountable . Interestingly, Floridi
uses the backpropagation method known from Deep Learning to describe the way
in which responsibilities can be assigned, except that here backpropagation is used
in networks of distributed responsibility. When working in groups, actions that are
on first glance allegedly morally neutral can nevertheless have consequences or
impacts—intended or non-intended—that are morally wrong. This means that practitioners from AI communities always need to discern the overarching, short- and
long-term consequences of the technical artefacts they are building or maintaining,
as well as to explore alternative ways of developing software or using data, including the option of completely refraining from carrying out particular tasks, which are
considered unethical.
In addition to the endorsement of virtue ethics in tech communities, several institutional changes should take place. They include the adoption of legal framework
conditions, the establishment of mechanisms for an independent auditing of technologies, the establishment of institutions for complaints, which also compensate for
harms caused by AI systems, and the expansion of university curricula in particular through content from ethics of technology, media, and information . So
far, however, hardly any of these demands have been met.
5  Conclusion
Currently, AI ethics is failing in many cases. Ethics lacks a reinforcement mechanism. Deviations from the various codes of ethics have no consequences. And
in cases where ethics is integrated into institutions, it mainly serves as a marketing strategy. Furthermore, empirical experiments show that reading ethics guidelines has no significant influence on the decision-making of software developers.
In practice, AI ethics is often considered as extraneous, as surplus or some kind
T. Hagendorff
of “add-on” to technical concerns, as unbinding framework that is imposed from
institutions “outside” of the technical community. Distributed responsibility in conjunction with a lack of knowledge about long-term or broader societal technological consequences causes software developers to lack a feeling of accountability or
a view of the moral significance of their work. Especially economic incentives are
easily overriding commitment to ethical principles and values. This implies that the
purposes for which AI systems are developed and applied are not in accordance with
societal values or fundamental rights such as beneficence, non-maleficence, justice,
and explicability .
Nevertheless, in several areas ethically motivated efforts are undertaken to
improve AI systems. This is particularly the case in fields where technical “fixes”
can be found for specific problems, such as accountability, privacy protection, antidiscrimination, safety, or explainability. However, there is also a wide range of ethical aspects that are significantly related to the research, development and application of AI systems, but are not or very seldomly mentioned in the guidelines. Those
omissions range from aspects like the danger of a malevolent artificial general intelligence, machine consciousness, the reduction of social cohesion by AI ranking
and filtering systems on social networking sites, the political abuse of AI systems,
a lack of diversity in the AI community, links to robot ethics, the dealing with trolley problems, the weighting between algorithmic or human decision routines, “hidden” social and ecological costs of AI, to the problem of public–private-partnerships
and industry-funded research. Again, as mentioned earlier, the list of omissions is
not exhaustive and not all omissions can be justified equally. Some omissions, like
deliberations on artificial general intelligence, can be justified by pointing at their
purely speculative nature, while other omissions are less valid and should be a reason to update or improve existing and upcoming guidelines.
Checkbox guidelines must not be the only “instruments” of AI ethics. A transition
is required from a more deontologically oriented, action-restricting ethic based on
universal abidance of principles and rules, to a situation-sensitive ethical approach
based on virtues and personality dispositions, knowledge expansions, responsible
autonomy and freedom of action. Such an AI ethics does not seek to subsume as
many cases as possible under individual principles in an overgeneralizing way, but
behaves sensitively towards individual situations and specific technical assemblages.
Further, AI ethics should not try to discipline moral actors to adhere to normative
principles, but emancipate them from potential inabilities to act self-responsibly on
the basis of comprehensive knowledge, as well as empathy in situations where morally relevant decisions have to be made.
These considerations have two consequences for AI ethics. On the one hand, a
stronger focus on technological details of the various methods and technologies in
the field of AI and machine learning is required. This should ultimately serve to
close the gap between ethics and technical discourses. It is necessary to build tangible bridges between abstract values and technical implementations, as long as these
bridges can be reasonably constructed. On the other hand, however, the consequence
of the presented considerations is that AI ethics, conversely, turns away from the
description of purely technological phenomena in order to focus more strongly on
genuinely social and personality-related aspects. AI ethics then deals less with AI
The Ethics of AI Ethics: An Evaluation of Guidelines﻿
as such, than with ways of deviation or distancing oneself from problematic routines
of action, with uncovering blind spots in knowledge, and of gaining individual selfresponsibility. Future AI ethics faces the challenge of achieving this balancing act
between the two approaches.
Acknowledgements  Open Access funding provided by Projekt DEAL.
Funding  This research was supported by the Cluster of Excellence “Machine Learning – New Perspectives for Science” funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)
under Germany’s Excellence Strategy – Reference Number EXC 2064/1 – Project ID 390727645.
Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License,
which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as
you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article
are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the
material. If material is not included in the article’s Creative Commons licence and your intended use is
not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission
directly from the copyright holder. To view a copy of this licence, visit 
ses/by/4.0/.