Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 3730–3740,
Hong Kong, China, November 3–7, 2019. c⃝2019 Association for Computational Linguistics
Text Summarization with Pretrained Encoders
Yang Liu and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
 , 
Bidirectional Encoder Representations from
Transformers represents the latest incarnation of pretrained language models which have recently advanced
a wide range of natural language processing
tasks. In this paper, we showcase how BERT
can be usefully applied in text summarization and propose a general framework for both
extractive and abstractive models. We introduce a novel document-level encoder based on
BERT which is able to express the semantics
of a document and obtain representations for
its sentences. Our extractive model is built on
top of this encoder by stacking several intersentence Transformer layers. For abstractive
summarization, we propose a new ﬁne-tuning
schedule which adopts different optimizers for
the encoder and the decoder as a means of alleviating the mismatch between the two (the
former is pretrained while the latter is not). We
also demonstrate that a two-staged ﬁne-tuning
approach can further boost the quality of the
generated summaries. Experiments on three
datasets show that our model achieves stateof-the-art results across the board in both extractive and abstractive settings.1
Introduction
Language model pretraining has advanced the
state of the art in many NLP tasks ranging from
sentiment analysis, to question answering, natural language inference, named entity recognition,
and textual similarity. State-of-the-art pretrained
models include ELMo , GPT
 , and more recently Bidirectional Encoder Representations from Transformers . BERT combines
both word and sentence representations in a single
very large Transformer ; it is
1Our code is available at 
nlpyang/PreSumm.
pretrained on vast amounts of text, with an unsupervised objective of masked language modeling
and next-sentence prediction and can be ﬁne-tuned
with various task-speciﬁc objectives.
In most cases, pretrained language models have
been employed as encoders for sentence- and
paragraph-level natural language understanding
problems involving various
classiﬁcation tasks (e.g., predicting whether any
two sentences are in an entailment relationship; or
determining the completion of a sentence among
four alternative sentences). In this paper, we examine the inﬂuence of language model pretraining on text summarization. Different from previous tasks, summarization requires wide-coverage
natural language understanding going beyond the
meaning of individual words and sentences. The
aim is to condense a document into a shorter version while preserving most of its meaning. Furthermore, under abstractive modeling formulations, the task requires language generation capabilities in order to create summaries containing
novel words and phrases not featured in the source
text, while extractive summarization is often de-
ﬁned as a binary classiﬁcation task with labels indicating whether a text span (typically a sentence)
should be included in the summary.
We explore the potential of BERT for text summarization under a general framework encompassing both extractive and abstractive modeling paradigms.
We propose a novel documentlevel encoder based on BERT which is able to
encode a document and obtain representations
for its sentences.
Our extractive model is built
on top of this encoder by stacking several intersentence Transformer layers to capture documentlevel features for extracting sentences.
Our abstractive model adopts an encoder-decoder architecture, combining the same pretrained BERT encoder with a randomly-initialized Transformer de-
coder . We design a new
training schedule which separates the optimizers
of the encoder and the decoder in order to accommodate the fact that the former is pretrained while
the latter must be trained from scratch. Finally,
motivated by previous work showing that the combination of extractive and abstractive objectives
can help generate better summaries , we present a two-stage approach
where the encoder is ﬁne-tuned twice, ﬁrst with
an extractive objective and subsequently on the abstractive summarization task.
We evaluate the proposed approach on three
single-document news summarization datasets
representative of different writing conventions
(e.g., important information is concentrated at the
beginning of the document or distributed more
evenly throughout) and summary styles (e.g., verbose vs. more telegraphic; extractive vs. abstractive).
Across datasets, we experimentally show
that the proposed models achieve state-of-the-art
results under both extractive and abstractive settings. Our contributions in this work are threefold: a) we highlight the importance of document
encoding for the summarization task; a variety
of recently proposed techniques aim to enhance
summarization performance via copying mechanisms , reinforcement learning , and multiple communicating encoders . We achieve better results
with a minimum-requirement model without using
any of these mechanisms; b) we showcase ways to
effectively employ pretrained language models in
summarization under both extractive and abstractive settings; we would expect any improvements
in model pretraining to translate in better summarization in the future; and c) the proposed models
can be used as a stepping stone to further improve
summarization performance as well as baselines
against which new proposals are tested.
Background
Pretrained Language Models
Pretrained language models have recently
emerged as a key technology for achieving impressive gains in a wide variety of natural language tasks.
These models extend the idea of
word embeddings by learning contextual representations from large-scale corpora using a language modeling objective. Bidirectional Encoder
Representations from Transformers is a new language representation
model which is trained with a masked language
modeling and a “next sentence prediction” task on
a corpus of 3,300M words.
The general architecture of BERT is shown in
the left part of Figure 1. Input text is ﬁrst preprocessed by inserting two special tokens. [CLS] is
appended to the beginning of the text; the output
representation of this token is used to aggregate information from the whole sequence (e.g., for classiﬁcation tasks). And token [SEP] is inserted after
each sentence as an indicator of sentence boundaries. The modiﬁed text is then represented as a
sequence of tokens X = [w1, w2, · · · , wn]. Each
token wi is assigned three kinds of embeddings:
token embeddings indicate the meaning of each
token, segmentation embeddings are used to discriminate between two sentences (e.g., during a
sentence-pair classiﬁcation task) and position embeddings indicate the position of each token within
the text sequence.
These three embeddings are
summed to a single input vector xi and fed to a
bidirectional Transformer with multiple layers:
˜hl = LN(hl−1 + MHAtt(hl−1))
hl = LN(˜hl + FFN(˜hl))
where h0 = x are the input vectors; LN is the layer
normalization operation ; MHAtt
is the multi-head attention operation ; superscript l indicates the depth of
the stacked layer. On the top layer, BERT will generate an output vector ti for each token with rich
contextual information.
Pretrained language models are usually used to
enhance performance in language understanding
Very recently, there have been attempts
to apply pretrained models to various generation
problems .
When ﬁne-tuning for a speciﬁc task, unlike ELMo
whose parameters are usually ﬁxed, parameters in
BERT are jointly ﬁne-tuned with additional taskspeciﬁc parameters.
Extractive Summarization
Extractive summarization systems create a summary by identifying (and subsequently concatenating) the most important sentences in a document. Neural models consider extractive sum-
Transformer Layers
Em beddings
Em beddings
Em beddings
Contextual
Em beddings
Transformer Layers
Original BERT
BERT for Summarization
Figure 1: Architecture of the original BERT model (left) and BERTSUM (right). The sequence on top is the input
document, followed by the summation of three kinds of embeddings for each token. The summed vectors are used
as input embeddings to several bidirectional Transformer layers, generating contextual vectors for each token.
BERTSUM extends BERT by inserting multiple [CLS] symbols to learn sentence representations and using interval
segmentation embeddings (illustrated in red and green color) to distinguish multiple sentences.
marization as a sentence classiﬁcation problem:
a neural encoder creates sentence representations
and a classiﬁer predicts which sentences should be
selected as summaries. SUMMARUNNER is one of the earliest neural
approaches adopting an encoder based on Recurrent Neural Networks. REFRESH is a reinforcement learning-based system
trained by globally optimizing the ROUGE metric.
More recent work achieves higher performance
with more sophisticated model structures.
TENT frames extractive summarization as a latent variable inference problem;
instead of maximizing the likelihood of “gold”
standard labels, their latent model directly maximizes the likelihood of human summaries given
selected sentences. SUMO capitalizes on the notion of structured attention to induce a multi-root dependency tree representation
of the document while predicting the output summary. NEUSUM scores and selects sentences jointly and represents the state of
the art in extractive summarization.
Abstractive Summarization
Neural approaches to abstractive summarization
conceptualize the task as a sequence-to-sequence
problem, where an encoder maps a sequence of
tokens in the source document x = [x1, ..., xn]
to a sequence of continuous representations z =
[z1, ..., zn], and a decoder then generates the target
summary y = [y1, ..., ym] token-by-token, in an
auto-regressive manner, hence modeling the conditional probability: p(y1, ..., ym|x1, ..., xn).
Rush et al. and Nallapati et al. 
were among the ﬁrst to apply the neural encoderdecoder architecture to text summarization. See
et al. enhance this model with a pointergenerator network (PTGEN) which allows it to
copy words from the source text, and a coverage
mechanism (COV) which keeps track of words that
have been summarized. Celikyilmaz et al. 
propose an abstractive system where multiple
agents (encoders) represent the document together
with a hierarchical attention mechanism (over the
agents) for decoding. Their Deep Communicating Agents (DCA) model is trained end-to-end
with reinforcement learning. Paulus et al. 
also present a deep reinforced model (DRM) for
abstractive summarization which handles the coverage problem with an intra-attention mechanism
where the decoder attends over previously generated words. Gehrmann et al. follow a
bottom-up approach (BOTTOMUP); a content selector ﬁrst determines which phrases in the source
document should be part of the summary, and a
copy mechanism is applied only to preselected
phrases during decoding. Narayan et al. 
propose an abstractive model which is particularly suited to extreme summarization (i.e., single
sentence summaries), based on convolutional neural networks and additionally conditioned on topic
distributions (TCONVS2S).
Fine-tuning BERT for Summarization
Summarization Encoder
Although BERT has been used to ﬁne-tune various NLP tasks, its application to summarization
is not as straightforward. Since BERT is trained
as a masked-language model, the output vectors
are grounded to tokens instead of sentences, while
in extractive summarization, most models manipulate sentence-level representations. Although
segmentation embeddings represent different sentences in BERT, they only apply to sentencepair inputs, while in summarization we must encode and manipulate multi-sentential inputs. Figure 1 illustrates our proposed BERT architecture
for SUMmarization (which we call BERTSUM).
In order to represent individual sentences, we
insert external [CLS] tokens at the start of each
sentence, and each [CLS] symbol collects features
for the sentence preceding it.
We also use interval segment embeddings to distinguish multiple sentences within a document. For senti we
assign segment embedding EA or EB depending
on whether i is odd or even.
For example, for
document [sent1, sent2, sent3, sent4, sent5], we
would assign embeddings [EA, EB, EA, EB, EA].
This way, document representations are learned
hierarchically where lower Transformer layers
represent adjacent sentences, while higher layers, in combination with self-attention, represent
multi-sentence discourse.
Position embeddings in the original BERT
model have a maximum length of 512; we overcome this limitation by adding more position embeddings that are initialized randomly and ﬁnetuned with other parameters in the encoder.
Extractive Summarization
Let d denote a document containing sentences
[sent1, sent2, · · · , sentm], where senti is the i-th
sentence in the document. Extractive summarization can be deﬁned as the task of assigning a label
yi ∈{0, 1} to each senti, indicating whether the
sentence should be included in the summary. It
is assumed that summary sentences represent the
most important content of the document.
With BERTSUM, vector ti which is the vector
of the i-th [CLS] symbol from the top layer can
be used as the representation for senti. Several
inter-sentence Transformer layers are then stacked
on top of BERT outputs, to capture document-level
features for extracting summaries:
˜hl = LN(hl−1 + MHAtt(hl−1))
hl = LN(˜hl + FFN(˜hl))
where h0 = PosEmb(T); T denotes the sentence vectors output by BERTSUM, and function PosEmb adds sinusoid positional embeddings to T, indicating the
position of each sentence.
The ﬁnal output layer is a sigmoid classiﬁer:
ˆyi = σ(WohL
i is the vector for senti from the top
layer (the L-th layer ) of the Transformer.
experiments, we implemented Transformers with
L = 1, 2, 3 and found that a Transformer with
L = 2 performed best.
We name this model
BERTSUMEXT.
The loss of the model is the binary classiﬁcation entropy of prediction ˆyi against gold label yi.
Inter-sentence Transformer layers are jointly ﬁnetuned with BERTSUM. We use the Adam optimizer with β1 = 0.9, and β2 = 0.999). Our learning rate schedule follows 
with warming-up (warmup = 10, 000):
lr = 2e−3 · min (step −0.5, step · warmup −1.5)
Abstractive Summarization
We use a standard encoder-decoder framework for
abstractive summarization . The
encoder is the pretrained BERTSUM and the decoder is a 6-layered Transformer initialized randomly.
It is conceivable that there is a mismatch between the encoder and the decoder, since
the former is pretrained while the latter must be
trained from scratch. This can make ﬁne-tuning
unstable; for example, the encoder might overﬁt
the data while the decoder underﬁts, or vice versa.
To circumvent this, we design a new ﬁne-tuning
schedule which separates the optimizers of the encoder and the decoder.
We use two Adam optimizers with β1 = 0.9 and
β2 = 0.999 for the encoder and the decoder, respectively, each with different warmup-steps and
learning rates:
lrE = ˜lrE · min(step−0.5, step · warmup−1.5
lrD = ˜lrD · min(step−0.5, step · warmup−1.5
where ˜lrE = 2e−3, and warmupE = 20, 000 for
the encoder and ˜lrD = 0.1, and warmupD =
10, 000 for the decoder.
This is based on the
assumption that the pretrained encoder should
be ﬁne-tuned with a smaller learning rate and
smoother decay (so that the encoder can be trained
with more accurate gradients when the decoder is
becoming stable).
# docs (train/val/test)
avg. doc length
avg. summary length
% novel bi-grams
in gold summary
90,266/1,220/1,093
196,961/12,148/10,397
96,834/4,000/3,452
204,045/11,332/11,334
Table 1: Comparison of summarization datasets: size of training, validation, and test sets and average document
and summary length (in terms of words and sentences). The proportion of novel bi-grams that do not appear in
source documents but do appear in the gold summaries quantiﬁes corpus bias towards extractive methods.
In addition, we propose a two-stage ﬁne-tuning
approach, where we ﬁrst ﬁne-tune the encoder on
the extractive summarization task (Section 3.2)
and then ﬁne-tune it on the abstractive summarization task (Section 3.3). Previous work suggests that using
extractive objectives can boost the performance
of abstractive summarization.
Also notice that
this two-stage approach is conceptually very simple, the model can take advantage of information
shared between these two tasks, without fundamentally changing its architecture. We name the
default abstractive model BERTSUMABS and the
two-stage ﬁne-tuned model BERTSUMEXTABS.
Experimental Setup
In this section, we describe the summarization
datasets used in our experiments and discuss various implementation details.
Summarization Datasets
We evaluated our model on three benchmark
datasets, namely the CNN/DailyMail news highlights dataset , the New
York Times Annotated Corpus , and XSum . These
datasets represent different summary styles ranging from highlights to very brief one sentence
summaries. The summaries also vary with respect
to the type of rewriting operations they exemplify
(e.g., some showcase more cut and paste operations while others are genuinely abstractive). Table 1 presents statistics on these datasets (test set);
example (gold-standard) summaries are provided
in the supplementary material.
CNN/DailyMail
contains news articles and associated highlights, i.e., a few bullet points giving
a brief overview of the article. We used the standard splits of Hermann et al. for training,
validation, and testing (90,266/1,220/1,093 CNN
documents and 196,961/12,148/10,397 DailyMail
documents). We did not anonymize entities. We
ﬁrst split sentences with the Stanford CoreNLP
toolkit and pre-processed
the dataset following See et al. . Input documents were truncated to 512 tokens.
contains 110,540 articles with abstractive
summaries. Following Durrett et al. , we
split these into 100,834/9,706 training/test examples, based on the date of publication . We used 4,000 examples from the
training as validation set. We also followed their
ﬁltering procedure, documents with summaries
less than 50 words were removed from the dataset.
The ﬁltered test set (NYT50) includes 3,452 examples. Sentences were split with the Stanford
CoreNLP toolkit and preprocessed following Durrett et al. . Input
documents were truncated to 800 tokens.
contains 226,711 news articles accompanied with a one-sentence summary, answering the
question “What is this article about?”. We used the
splits of Narayan et al. for training, validation, and testing (204,045/11,332/11,334) and followed the pre-processing introduced in their work.
Input documents were truncated to 512 tokens.
Aside from various statistics on the three
datasets, Table 1 also reports the proportion of
novel bi-grams in gold summaries as a measure
of their abstractiveness. We would expect models with extractive biases to perform better on
datasets with (mostly) extractive summaries, and
abstractive models to perform more rewrite operations on datasets with abstractive summaries.
CNN/DailyMail and NYT are somewhat extractive, while XSum is highly abstractive.
Implementation Details
For both extractive and abstractive settings, we
used PyTorch, OpenNMT and
the ‘bert-base-uncased’2 version of BERT to implement BERTSUM. Both source and target texts
2 
were tokenized with BERT’s subwords tokenizer.
Extractive Summarization
All extractive models were trained for 50,000 steps on 3 GPUs (GTX
1080 Ti) with gradient accumulation every two
steps. Model checkpoints were saved and evaluated on the validation set every 1,000 steps. We
selected the top-3 checkpoints based on the evaluation loss on the validation set, and report the averaged results on the test set. We used a greedy algorithm similar to Nallapati et al. to obtain
an oracle summary for each document to train extractive models. The algorithm generates an oracle
consisting of multiple sentences which maximize
the ROUGE-2 score against the gold summary.
When predicting summaries for a new document, we ﬁrst use the model to obtain the score
for each sentence. We then rank these sentences
by their scores from highest to lowest, and select
the top-3 sentences as the summary.
During sentence selection we use Trigram
Blocking to reduce redundancy ; we wish to minimize
the similarity between the sentence being considered and sentences which have been already selected as part of the summary.
Abstractive Summarization
In all abstractive
models, we applied dropout (with probability 0.1)
before all linear layers; label smoothing with smoothing factor 0.1 was also
used. Our Transformer decoder has 768 hidden
units and the hidden size for all feed-forward layers is 2,048. All models were trained for 200,000
steps on 4 GPUs (GTX 1080 Ti) with gradient accumulation every ﬁve steps. Model checkpoints
were saved and evaluated on the validation set every 2,500 steps. We selected the top-3 checkpoints
based on their evaluation loss on the validation set,
and report the averaged results on the test set.
During decoding we used beam search (size 5),
and tuned the α for the length penalty between 0.6 and 1 on the validation set; we
decode until an end-of-sequence token is emitted
and repeated trigrams are blocked , despite their popularity in abstractive summarization. This is mainly because
52.59 31.24 48.87
40.42 17.62 36.67
Extractive
SUMMARUNNER 39.60 16.20 35.30
REFRESH 
40.00 18.20 36.60
LATENT 
41.05 18.77 37.54
NEUSUM 
41.59 19.01 37.98
SUMO 
41.00 18.40 37.20
TransformerEXT
40.90 18.02 37.17
Abstractive
PTGEN 
36.44 15.66 33.42
PTGEN+COV 
39.53 17.28 36.38
DRM 
39.87 15.82 36.90
BOTTOMUP 
41.22 18.68 38.34
DCA 
41.69 19.47 37.92
TransformerABS
40.21 17.76 37.09
BERT-based
BERTSUMEXT
43.25 20.24 39.63
BERTSUMEXT w/o interval embeddings
43.20 20.22 39.59
BERTSUMEXT (large)
43.85 20.34 39.90
BERTSUMABS
41.72 19.39 38.76
BERTSUMEXTABS
42.13 19.60 39.18
Table 2: ROUGE F1 results on CNN/DailyMail test
set (R1 and R2 are shorthands for unigram and bigram
overlap; RL is the longest common subsequence). Results for comparison systems are taken from the authors’ respective papers or obtained on our data by running publicly released software.
we focus on building a minimum-requirements
model and these mechanisms may introduce additional hyper-parameters to tune. Thanks to the
subwords tokenizer, we also rarely observe issues with out-of-vocabulary words in the output; moreover, trigram-blocking produces diverse
summaries managing to reduce repetitions.
Automatic Evaluation
We evaluated summarization quality automatically using ROUGE .
unigram and bigram overlap (ROUGE-1 and
ROUGE-2) as a means of assessing informativeness and the longest common subsequence
(ROUGE-L) as a means of assessing ﬂuency.
summarizes
CNN/DailyMail dataset. The ﬁrst block in the table includes the results of an extractive ORACLE
system as an upper bound. We also present the
LEAD-3 baseline (which simply selects the ﬁrst
three sentences in a document).
The second block in the table includes various
extractive models trained on the CNN/DailyMail
dataset (see Section 2.2 for an overview).
Extractive
COMPRESS 
SUMO 
TransformerEXT
Abstractive
PTGEN 
PTGEN + COV 
DRM 
TransformerABS
BERT-based
BERTSUMEXT
BERTSUMABS
BERTSUMEXTABS
Table 3: ROUGE Recall results on NYT test set. Results for comparison systems are taken from the authors’ respective papers or obtained on our data by running publicly released software. Table cells are ﬁlled
with — whenever results are not available.
comparison to our own model, we also implemented a non-pretrained Transformer baseline
(TransformerEXT) which uses the same architecture as BERTSUMEXT, but with fewer parameters.
It is randomly initialized and only trained on the
summarization task. TransformerEXT has 6 layers, the hidden size is 512, and the feed-forward
ﬁlter size is 2,048. The model was trained with
same settings as in Vaswani et al. .
The third block in Table 2 highlights the performance of several abstractive models on the
CNN/DailyMail dataset (see Section 2.3 for an
overview). We also include an abstractive Transformer baseline (TransformerABS) which has the
same decoder as our abstractive BERTSUM models; the encoder is a 6-layer Transformer with 768
hidden size and 2,048 feed-forward ﬁlter size.
The fourth block reports results with ﬁne-tuned
BERT models: BERTSUMEXT and its two variants (one without interval embeddings, and one
with the large version of BERT), BERTSUM-
ABS, and BERTSUMEXTABS. BERT-based models outperform the LEAD-3 baseline which is not
a strawman; on the CNN/DailyMail corpus it
is indeed superior to several extractive and abstractive models 
9.21 23.24
PTGEN+COV 
8.02 21.72
TCONVS2S 
31.89 11.54 25.75
TransformerABS
9.77 23.01
BERT-based
BERTSUMABS
38.76 16.33 31.15
BERTSUMEXTABS
38.81 16.50 31.27
ROUGE F1 results on the XSum test set.
Results for comparison systems are taken from the authors’ respective papers or obtained on our data by running publicly released software.
CNN/DailyMail summaries are somewhat extractive and even abstractive models are prone to copying sentences from the source document when
trained on this dataset . Perhaps
unsurprisingly we observe that larger versions of
BERT lead to performance improvements and that
interval embeddings bring only slight gains.
Table 3 presents results on the NYT dataset.
Following the evaluation protocol in Durrett et al.
 , we use limited-length ROUGE Recall,
where predicted summaries are truncated to the
length of the gold summaries. Again, we report
the performance of the ORACLE upper bound and
LEAD-3 baseline. The second block in the table
contains previously proposed extractive models as
well as our own Transformer baseline.
PRESS is an ILP-based model
which combines compression and anaphoricity
constraints. The third block includes abstractive
models from the literature, and our Transformer
BERT-based models are shown in the
fourth block.
Again, we observe that they outperform previously proposed approaches. On this
dataset, abstractive BERT models generally perform better compared to BERTSUMEXT, almost
approaching ORACLE performance.
Table 4 summarizes our results on the XSum
dataset. Recall that summaries in this dataset are
highly abstractive (see Table 1) consisting of a single sentence conveying the gist of the document.
Extractive models here perform poorly as corroborated by the low performance of the LEAD baseline (which simply selects the leading sentence
from the document), and the ORACLE (which selects a single-best sentence in each document) in
As a result, we do not report results
for extractive models on this dataset. The second
Model perplexity (CNN/DailyMail; validation set) under different combinations of encoder and
decoder learning rates.
block in Table 4 presents the results of various abstractive models taken from Narayan et al. 
and also includes our own abstractive Transformer
baseline. In the third block we show the results
of our BERT summarizers which again are superior to all previously reported models (by a wide
Model Analysis
Learning Rates
Recall that our abstractive
model uses separate optimizers for the encoder
and decoder.
In Table 5 we examine whether
the combination of different learning rates (˜lrE
and ˜lrD) is indeed beneﬁcial. Speciﬁcally, we report model perplexity on the CNN/DailyMail validation set for varying encoder/decoder learning
rates. We can see that the model performs best
with ˜lrE = 2e −3 and ˜lrD = 0.1.
Position of Extracted Sentences
In addition to
the evaluation based on ROUGE, we also analyzed in more detail the summaries produced by
our model. For the extractive setting, we looked at
the position (in the source document) of the sentences which were selected to appear in the summary. Figure 2 shows the proportion of selected
summary sentences which appear in the source
document at positions 1, 2, and so on. The analysis
was conducted on the CNN/DailyMail dataset for
Oracle summaries, and those produced by BERT-
SUMEXT and the TransformerEXT. We can see
that Oracle summary sentences are fairly smoothly
distributed across documents, while summaries
created by TransformerEXT mostly concentrate on
the ﬁrst document sentences. BERTSUMEXT outputs are more similar to Oracle summaries, indicating that with the pretrained encoder, the model
relies less on shallow position features, and learns
deeper document representations.
Novel N-grams
We also analyzed the output of
abstractive systems by calculating the proportion
of novel n-grams that appear in the summaries but
not in the source texts. The results are shown in
Figure 3. In the CNN/DailyMail dataset, the pro-
9 10 11 12 13 14 15 16 17 18
Sentence position (in source document)
Proportion of selected sentences
TRANSFORMEREXT
BERTSUMEXT
Figure 2: Proportion of extracted sentences according
to their position in the original document.
portion of novel n-grams in automatically generated summaries is much lower compared to reference summaries, but in XSum, this gap is much
smaller. We also observe that on CNN/DailyMail,
BERTEXTABS produces less novel n-ngrams than
BERTABS, which is not surprising. BERTEXTABS
is more biased towards selecting sentences from
the source document since it is initially trained as
an extractive model.
The supplementary material includes examples
of system output and additional ablation studies.
Human Evaluation
In addition to automatic evaluation, we also evaluated system output by eliciting human judgments.
We report experiments following a questionanswering (QA) paradigm which quantiﬁes
the degree to which summarization models retain
key information from the document. Under this
paradigm, a set of questions is created based on
the gold summary under the assumption that it
highlights the most important document content.
Participants are then asked to answer these questions by reading system summaries alone without
access to the article. The more questions a system can answer, the better it is at summarizing the
document as a whole.
Moreover, we also assessed the overall quality of the summaries produced by abstractive systems which due to their ability to rewrite content
may produce disﬂuent or ungrammatical output.
Speciﬁcally, we followed the Best-Worst Scaling method
where participants were presented with the output
of two systems (and the original document) and
Proportion of novel n-grams
BERTSUMEXTABS
BERTSUMABS
(a) CNN/DailyMail Dataset
Proportion of novel n-grams
(b) XSum dataset
Figure 3: Proportion of novel n-grams in model generated summaries.
Extractive
Transformer
Table 6: QA-based evaluation. Models with † are signiﬁcantly different from BERTSUM (using a paired student t-test; p < 0.05). Table cells are ﬁlled with —
whenever system output is not available.
asked to decide which one was better according to
the criteria of Informativeness, Fluency, and Succinctness.
Both types of evaluation were conducted on
the Amazon Mechanical Turk platform. For the
CNN/DailyMail and NYT datasets we used the
same documents (20 in total) and questions from
previous work from the release
of Narayan et al. .
We elicited 3 responses per HIT. With regard to QA evaluation,
we adopted the scoring mechanism from Clarke
and Lapata ; correct answers were marked
with a score of one, partially correct answers
with 0.5, and zero otherwise. For quality-based
evaluation, the rating of each system was computed as the percentage of times it was chosen as
better minus the times it was selected as worse.
Ratings thus range from -1 (worst) to 1 (best).
Abstractive
33.3† -0.24†
30.5† -0.27†
23.7† -0.36†
40.6† -0.16†
QA-based and ranking-based evaluation.
Models with † are signiﬁcantly different from BERT-
SUM (using a paired student t-test; p < 0.05). Table
cells are ﬁlled with — whenever system output is not
available. GOLD is not used in QA setting, and LEAD
is not used in Rank evaluation.
Results for extractive and abstractive systems
are shown in Tables 6 and 7, respectively.
compared the best performing BERTSUM model
in each setting (extractive or abstractive) against
various state-of-the-art systems (whose output is
publicly available), the LEAD baseline, and the
GOLD standard as an upper bound.
in both tables participants overwhelmingly prefer the output of our model against comparison
systems across datasets and evaluation paradigms.
All differences between BERTSUM and comparison models are statistically signiﬁcant (p < 0.05),
with the exception of TCONVS2S (see Table 7;
XSum) in the QA evaluation setting.
Conclusions
In this paper, we showcased how pretrained BERT
can be usefully applied in text summarization. We
introduced a novel document-level encoder and
proposed a general framework for both abstractive and extractive summarization. Experimental
results across three datasets show that our model
achieves state-of-the-art results across the board
under automatic and human-based evaluation protocols.
Although we mainly focused on document encoding for summarization, in the future,
we would like to take advantage the capabilities of
BERT for language generation.
Acknowledgments
This research is supported by a Google PhD Fellowship to the ﬁrst author. We gratefully acknowledge the support of the European Research Council (Lapata, award number 681760, “Translating
Multiple Modalities into Text”). We would also
like to thank Shashi Narayan for providing us with
the XSum dataset.