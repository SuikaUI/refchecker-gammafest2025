Regionlets for Generic Object Detection
Xiaoyu Wang
Shenghuo Zhu
Yuanqing Lin
NEC Laboratories America, Inc.
{xwang,myang,zsh,ylin}@nec-labs.com
Generic object detection is confronted by dealing with
different degrees of variations in distinct object classes with
tractable computations, which demands for descriptive and
ﬂexible object representations that are also efﬁcient to evaluate for many locations. In view of this, we propose to
model an object class by a cascaded boosting classiﬁer
which integrates various types of features from competing
local regions, named as regionlets. A regionlet is a base
feature extraction region deﬁned proportionally to a detection window at an arbitrary resolution (i.e. size and aspect ratio). These regionlets are organized in small groups
with stable relative positions to delineate ﬁne-grained spatial layouts inside objects. Their features are aggregated to
a one-dimensional feature within one group so as to tolerate deformations. Then we evaluate the object bounding box proposal in selective search from segmentation
cues, limiting the evaluation locations to thousands. Our
approach signiﬁcantly outperforms the state-of-the-art on
popular multi-class detection benchmark datasets with a
single method, without any contexts. It achieves the detection mean average precision of 41.7% on the PASCAL VOC
2007 dataset and 39.7% on the VOC 2010 for 20 object categories. It achieves 14.7% mean average precision on the
ImageNet dataset for 200 object categories, outperforming
the latest deformable part-based model (DPM) by 4.7%.
1. Introduction
Despite the success of face detection where the target objects are roughly rigid, generic object detection remains an
open problem mainly due to the challenge of handling all
possible variations with tractable computations. In particular, different object classes demonstrate a variable degree of
deformation in images, either due to their nature, e.g., living
creatures like cats are generally more deformable than manmade objects like vehicles, or due to viewing distances or
angles, e.g., deformable objects may appear somehow rigid
at a distance and even rigid objects may show larger variations in different view angles. These pose a fundamental
Candidate detection bounding boxes
Regionlet based model
Applied to candidate boxes
Figure 1: Illustration of the regionlet representation. Regionlet representation can be applied to candidate bounding boxes that have different sizes and aspect ratios.
regionlet-based model is composed of a number of regions
(denoted by blue rectangles), and then each region is represented by a group of regionlets (denoted by the small orange
rectangles inside each region).
dilemma to object class representations: on one hand, a delicate model describing rigid object appearances may hardly
handle deformable objects; on the other hand, a high tolerance of deformation may result in imprecise localization or
false positives for rigid objects.
Prior arts in object detection cope with object deformation efﬁciently with primarily three typical strategies. First,
if spatial layouts of object appearances are roughly rigid
such as faces or pedestrians at a distance, the classical Adaboost detection mainly tackles local variations with
an ensemble classiﬁer of efﬁcient features. Then a sliding
window search with cascaded classiﬁers is an effective way
to achieve precise and efﬁcient localization. Second, the deformable part model (DPM) method inherits the HOG
window template matching but explicitly models deformations by latent variables, where an exhaustive search
of possible locations, scales, and aspect ratios are critical
to localize objects. Later on, the DPM has been accelerated by coarse-to-ﬁne search , branch and bound ,
and cross-talk approaches .
Third, object recognition
methods using spatial pyramid matching (SPM) of bag-ofwords (BoW) models are adopted for detection ,
and they inherently can tolerate large deformations. These
sophisticated detectors are applied to thousands of objectindependent candidate regions , instead of millions
of sliding windows. In return, little modeling of local spatial appearances leaves these recognition classiﬁers unable
to localize rigid objects precisely, e.g., bottles. These successful detection approaches inspire us to investigate a descriptive and ﬂexible object representation, which delivers
the modeling capacity for both rigid and deformable objects
in a uniﬁed framework.
In this paper, we propose a new object representation
strategy for generic object detection, which incorporates
adaptive deformation handling into both object classiﬁer
learning and basic feature extraction. Each object bounding box is classiﬁed by a cascaded boosting classiﬁer, where
each weak classiﬁer takes the feature response of a region
inside the bounding box as its input and then the region is in
turn represented by a group of small sub-regions, named as
regionlets. The sets of regionlets are selected from a huge
pool of candidate regionlet groups by boosting. On one
hand, the relative spatial positions of the regionlets within
a region and the region within an object bounding box are
stable. Therefore, the proposed regionlet representation can
model ﬁne-grained spatial appearance layouts. On the other
hand, the feature responses of regionlets within one group
are aggregated to a one dimensional feature, and the resulting feature is generally robust to local deformation. Also,
our regionlet model is designed to be ﬂexible to take bounding boxes with different sizes and aspect ratios.
Therefore our approach is ready to utilizes the selective search
strategy to evaluate on merely thousands of candidate
bounding boxes rather than hundreds of thousands (if not
millions) of sliding windows as in the exhaustive search.
An illustration of the regionlet representation is shown
in Figure 1, where the regionlets drawn as orange boxes
are grouped within blue rectangular regions. The regionlets
and their groups for one object class are learned in boosting with stable relative positions to each other. When they
are applied to two candidate bounding boxes, the feature
responses of regionlets are obtained at the their respective
scales and aspect ratios without enumerating all possible
spatial conﬁgurations.
The major contribution of this paper lies in two-fold.
1) It introduces the regionlet concept which is ﬂexible to
extract features from arbitrary bounding boxes.
regionlet-based representation for an object class, which not
only models relative spatial layouts inside an object but also
accommodates variations especially deformations by the regionlet group selection in boosting and the aggregation of
feature responses in a regionlet group. As validated in the
experiment, the proposed representation adaptively models
a varying degree of deformation in diverse object classes.
2. Related Work
Object detection is arguably an indispensable component
for most of vision tasks, and it has achieved prominent successes for some speciﬁc targets such as faces and
pedestrians . Complete survey of object detection is certainly beyond the scope of this paper. Instead,
we brieﬂy review related generic object detection methods
that do not focus on a particular type of object.
One of the most inﬂuential methods in generic object detection is the deformable part model (DPM) and its extensions . The DPM object detector consists
of a root ﬁlter and several part ﬁlters. Deformations among
parts are inferred with latent variables. Since the resolutions
of the object templates are ﬁxed, an exhaustive sliding window search is required to ﬁnd objects at different scales
and different aspect ratios. The exhaustive search can be accelerated by more efﬁcient search .
In contrast, our regionlet-based detection handles object deformation directly in feature extraction, and it is ﬂexible
to deal with different scaling and aspect ratios without the
need of an exhaustive search.
Recently, a new detection strategy is to use
multi-scale image segmentation to propose a couple thousands of candidate bounding boxes for each image and then
the object categories of the bounding boxes are determined
by strong object classiﬁers, e.g., using bag-of-words (BoW)
model with spatial pyramid match (SPM) . Because the
BoW models ignore spatial relations among local features,
they are able to tolerate large deformations. However, because of the lack of local spatial relations, they may not
localize rigid object precisely. Our method borrows the candidate window proposing procedure in to speed up the
detection, however, it is fundamentally different from 
in both feature extraction and classiﬁer learning.
Contexts from local or global appearance have been explored to improve object detection . We
do not use any context cues in this paper and leave it as a
future work.
3. Regionlets for Detection
Object detection is composed of two key components:
determing where the candidate locations are in images and
discerning whether they are the objects of interests. Beyond
the straightforward exhaustive search of all locations, our
regionlet detection approach screens the candidate windows
derived from the selective search . For selective search,
given an image, it ﬁrst over-segments the image into superpixels, and then those superpixels are grouped in a bottomup manner to propose candidate bounding boxes. The work
in shows that such proposing bounding boxes, about
1,000∼2,000 each image, achieve very high recall. After
this, the task of detection boils down to extraction of an
appropriate object representation on each proposed box and
learning of a scoring function to rank the boxes. To that end,
we introduce regionlet features for each candidate bounding
box. In our proposed method, we construct a largely overcomplete regionlet feature pool and then design a cascaded
boosting learning process to select the most discriminative
regionlets for detection.
In the following, Section 3.1 describes what the regionlets are and explains how they are designed to handle deformation. Section 3.2 presents how to construct a largely
over-complete regionlet pool and learn a cascaded boosting
classiﬁer for an object category by selecting the most discriminative regionlets.
3.1. Regionlets
Regionlet deﬁnition
In object detection, an object category is essentially deﬁned
by a classiﬁer where both object appearance and the spatial layout inside an object shall be taken into account. For
simplicity, appearance features are mostly extracted from
some rectangular sub-regions within an object, which we
refer as feature extraction regions in the paper. Features
extracted from a small region often provide a good localization ability, but are vulnerable to variations; a big region
tends to tolerate more variations but may not be sensitive
enough for accurate localization. When large variations especially deformations occur, a large rectangle region may
not be appropriate for extracting descriptive features of an
object. Because some parts or the regions may not be informative or even distractive. This motivates us to deﬁne
sub-parts of a region, i.e., the regionlets, as the basic units
to extract appearance features, and organize them into small
groups which are more ﬂexible to describe distinct object
categories with different degrees of deformation.
We would like to introduce the regionlets with an example illustrated in Figure 2. The ﬁrst column in Figure 2
shows three samples of a person that are the target object to
detect and they are cropped by black bounding boxes in the
second column. A rectangle feature extraction region inside
the bounding box is denoted as R, which will contribute a
weak classiﬁer to the boosting classiﬁer. Within this region
R, we further spot some small sub-regions (e.g., r1,r2 and
r3) and deﬁne them as a group of regionlets. We employ
the term regionlet, because the features of these sub-regions
will be aggregated to a single feature for R, and they are
below the level of a standalone feature extraction region in
an object classiﬁer. In short, in the proposed method, a detection bounding box is represented by a number of regions,
each of which is composed of a small set of regionlets.
This example also illustrates how regionlets are designed
to handle deformation. Hand, as a supposingly informative
Figure 2: Illustration of the relationship among a detection
bounding box, a feature extraction region and regionlets. A
feature extraction region R, shown as a light blue rectangle,
is cropped from a ﬁxed position from 3 samples of a person.
Inside R, several small sub-regions denoted as r1, r2 and r3
(in orange small rectangules) are the regionlets to capture
the possible locations of the hand for person detection.
part for a person, may appear at different locations within
the bounding box of a person. If we extract the feature for
a hand from the whole region R which roughly covers the
possible locations of the hand, the appearance of some nonhand regions on the torso or background clearly are also included in the feature. An ideal deformation handling strategy is to extract features only from the hand region in all
three cases. To that end, we introduce three regionlets inside
R (In general, a region can contain many regionlets. Here
“three” is mainly for illustration purpose). Each regionlet
r covers a possible location of hand. Then only features
from the regionlets are extracted and aggregated to generate a compact representation for R. Irrelevant appearance
from backgrounds are largely discarded. More regionlets
in R will increase the capacity to model deformations, e.g.,
hand surely may appear in more positions than three. On
the other hand, rigid objects may only require one regionlet
from a feature extraction region.
Region feature extraction
Feature extraction from R takes two steps: 1) extracting
appearance features, e.g., the HOG and LBP descriptors from each regionlet respectively; and 2) generating
the representation of R based on regionlets’ features. The
ﬁrst step is straightforward. For the second step, we de-
ﬁne a permutation invariant feature operation on features
extracted from regionlets, and such an operation also assumes an exclusive relation among regionlets. Let’s denote
T(R) as the feature representation for region R, T(rj) as
the feature extracted from the jth regionlet rj in R, then the
operation is deﬁned as following:
subject to αj ∈{0, 1},
where NR is the total number of regionlets in region R, αj
is a binary variable, either 0 or 1. This operation is permutation invariant, namely, the occurrence of the appearance
cues in any of regionlets is equivalent, which allows deformations among these regionlet locations. The operation
also assumes the exclusiveness within a group of regionlets, namely, one and only one regionlet will contribute to
the region feature representation. The exclusive assumption is that when deformation occurs, the discriminative
sub-region appears at only one position in a speciﬁc training/testing sample.
In our framework, we simply apply max-pooling over
regionlet features. So Eq. 1 is instantiated as:
T(R) = max
The max-pooling happens for each feature dimension independently. For each regionlet rj, we ﬁrst extract low-level
feature vectors, such as HOG or LBP histograms. Then, we
pick a 1D feature from the same dimension of these feature
vectors in each regionlet and apply Eq. 2 to form the feature for region R. We have millions of such 1D features
in a detection window and the most discriminative ones are
determined through a boosting type learning process (to be
described in Section 3.2.2).
Figure 3 illustrates the process to extract T(R), the 1-D
feature for a region R. Here we again use the example in
Figure 2, where the blue region R is the one covering the
variation of hand locations. Assuming the ﬁrst dimension
of the concatenated low-level features is the most distinctive
feature dimension learned for hand, we collect this dimension from all the three regionlets and represent T(R) by the
strongest feature response from the top regionlet.
Regionlets normalized by detection windows
In this work, the proposed regionlet representations are
evaluated on the candidate bounding boxes derived from
selective search approach . In principle, they are also
applicable for sliding windows. The selective search approach ﬁrst over-segments an images into superpixels, and
then the superpixel are grouped in a bottom-up manner to
propose some candidate bounding boxes. This approach
typically produces 1000 to 2000 candidate bounding boxes
for an object detector to evaluate on, compared to millions
of windows in an exhaustive sliding window search.
1D feature for
The learned dimension
Regionlets’ features
Figure 3: Example of regionlet-based feature extraction.
However, these proposed bounding boxes have arbitrary
sizes and aspect ratios. As a result, it is not feasible to use
template regions (or template regionlets) with ﬁxed absolute sizes that are widely used in sliding window search.
We address this difﬁculty by using the relative positions and
sizes of the regionlets and their groups to an object bounding box. Figure 4 shows our way of deﬁning regionlets in
contrast to ﬁxed regions with absolute sizes. When using
a sliding window search, a feature extraction region is often deﬁned by the top-left (l, t) and the bottom-right corner
(r, b) w.r.t. the anchor position of the candidate bounding
box. In contrast, our approach normalizes the coordinates
by the width w and height h of the box and records the relative position of a region (l′, t′, r′, b′) = ( l
These relative region deﬁnitions allow us to directly evaluate the regionlets-based representation on candidate windows at different sizes and aspect ratios without scaling
images into multiple resolutions or using multiples components for enumerating possible aspect ratios.
3.2. Learning the object detection model
We follow the boosting framework to learn the discriminative regionlet groups and their conﬁgurations from a huge
pool of candidate regions and regionlets.
Regions/regionlets pool construction
Deformation may occur at different scales. For instance,
in person detection, deformation can be caused by a moving ﬁnger or a waving hand. A set of small regionlets that
is effective to capture ﬁnger-level deformation may hardly
handle deformation caused by hand movements. In order
to deal with diverse variations, we build a largely overcomplete pool for regions and regionlets with various positions, aspect ratios, and sizes. Before regionlet learning,
(݈, ݐ, ݎ, ܾ)
Traditional
Normalized
(݈, ݐ, ݎ, ܾ)
Figure 4: Relative regions normalized by a candidate window that are robust to scale and aspect ratio changes.
a region R′ or a regionlet r′ are not applied to a detection
window yet, so we call R′ a feature region prototype and r′
a regionlet prototype.
We ﬁrst explain how the pool of region feature prototypes is constructed. Using the deﬁnition in Section 3.1.3,
we denote the 1D feature of a region relative to a bounding box as R′ = (l′, t′, r′, b′, k) where k denotes the kth
element of the low-level feature vector of the region. R′
represents a feature prototype. The region pool is spanned
by X × Y × W × H × F, where X and Y are respectively the space of horizontal and vertical anchor position
of R in the detection window, W and H are the width and
height of the feature extraction region R′, and F is the space
of low-level feature vector (e.g., the concatenation of HOG
and LBP). Enumerating all possible regions is impractical
and not necessary. We employ a sampling process to reduce
the pool size. Algorithm 1 describes how we sample multiple region feature prototypes. In our implementation, we
generate about 100 million feature prototypes.
Afterwards, we propose a set of regionlets with random
positions inside each region. Although the sizes of regionlets in a region could be arbitrary in general, we restrict regionlets in a group to have the identical size because our
regionlets are designed to capture the same appearance in
different possible locations due to deformation. The sizes
of regionlets in different groups could be different. A region may contain up to 5 regionlets in our implementation.
So the ﬁnal feature space used as the feature pool for
boosting is spanned by R × C, where R is the region feature prototype space, C is the conﬁguration space of regionlets. Therefore, we augment a region feature prototype
R′ = (l′, t′, r′, b′, k, c) with a regionlet conﬁguration c.
Algorithm 1: Generation of region feature prototypes
Input: Region width step sw and height step sh;
maximum width W and height H of region
prototypes; horizontal step px and vertical step
py for the region anchor position; minimum
width wmin and height hmin of region
prototypes; the number of features N to extract
from one region
w ←wmin, h ←hmin, i ←0
for w < W do
for h < H do
l ←0, t ←0
for l < W −w do
for t < H −h do
for k=1,. . . N do
r ←l + w, b ←t + h
R′ = (l/w, t/h, r/w, b/h, k)
R ←R ∪{R′}
t ←t + py, i ←i + 1
Output: Region feature prototype pool R
Training with boosting regionlet features
We use RealBoost to train cascaded classiﬁers for our
object detector. One boosting classifer consists of a set of
selected weak classiﬁers. Similar to , we deﬁne the
weak classiﬁer using a lookup table:
vo1(B(x) = o),
where h(x) is a piece-wise linear function deﬁned by a
lookup table, vo is the table value for the oth entry, B(x)
quantizes the feature value x into a table entry, and 1(·)
is an indicator function. In each round of the training, vo
is computed based on the sample weight distribution as
−), where U o
+ is the summation of the weights
of the positive examples whose feature values fall into the
oth entry of the table. The U o
−is deﬁned in a similar manner
for the weights of negative examples.
Let’s denote Q as a candidate bounding box, R′(Q)
as a rectangular region in Q, and T(R′(Q)) as the onedimensional feature computed on R′(Q) (similar notation
as in Eq. 1). Substituting x in Eq. 3 with the extracted feature, we can get the weak classiﬁer in the tth round of training for the bounding box Q:
ht(T(R′(Q))) =
t 1(Bt(T(R′(Q))) = o),
t is the table value of the oth entry at the tth round
of training. Then, for each boosting classiﬁer, the learning
process obtains a set of weak classiﬁers H for separating
the positive samples from negative ones:
where it is the index of the region selected in the tth round
of training, Nit is the total number of regionlets in Rit, and
βt is the weight of the selected weak classiﬁer. The classiﬁcation result of the candidate bounding box Q is determined
by the ﬁnal round of cascade if it passes all previous ones,
and it is expressed as f(Q) = sign(H∗(Q)) where H∗denotes the last stage of cascade.
In each cascade training, we generate 100 millions of
candidate weak classiﬁers. To feed into memory, a reduced
set of 20K weak classiﬁers are sampled uniformly.
training terminates once the error rates (37.5% for negative and 1% for positive samples) are achieved except the
last cascade. The last round stops until it collects 5K weak
classiﬁers. The training results in 6-7 cascades and 5K-6K
weak classiﬁers.
Given a test image, we ﬁrst propose a number of candidate bounding boxes using the selective search . Then,
each candidate bounding box is passed along the cascaded
classiﬁers learned in the boosting process.
Because of
early rejections, only a small number of candidate bounding
boxes reach the last stage of the cascade. Therefore, except
the time spent on proposing bounding boxes, our method
yields a very fast testing speed.
4. Experiments
Following most of recent work on generic object detection, we evaluate our object detection performance on the
challenging PASCAL VOC 2007 and VOC 2010 datasets.
The PASCAL VOC datasets contain 20 categories of objects
including rigid objects such as cars and deformable objects
like cats. They are popular benchmark datasets for generic
multi-class object detection. Detection performance of each
category is measured by the average precision. The overall
performance is reported by the mean of average precisions
(mAP) over all classes. The proposed method is further validated on the much larger ImageNet object detection dataset
(ILSVRC2013) , including 200 object categories. We
also use mAP as the metric for performance evaluation.
4.1. Experiment on PASCAL VOC datasets
In our implementation of regionlet-based detection, we
utilize the selective search bounding boxes from to
train our detector. The HOG , LBP and covariance
features are adopted as candidate features for the regionlets. To validate the advantages of the proposed approach, we compare it with three baselines: deformable
part-based model which is one of the most effective
sliding window based detectors, and two recent approaches
based on selective search .
Accuracy: Table 1 presents the performance on the
PASCAL VOC 2007 dataset, from which we learn some interesting insights from the comparison between the ﬁrst two
baselines. In features from each small 8×8 patches are
aligned inside the detection window for the root ﬁlter, allowing local deformation for parts with misplacement costs.
While builds spatial pyramids over detection windows.
So enforces a strict spatial constraint in the root ﬁlter but allows for small local deformations, while ignores spatial conﬁguration to a large extent. From Table 1,
we observe that is excellent at detecting rigid objects
(such as bus and car) and objects with well deﬁned contours
but abundant local deformations (such as horse and person).
In contrast, performs better for objects with signiﬁcant global deformations such as cat, cow and sheep, as expected. It is very interesting that signiﬁcantly outperforms in the aeroplane and tvmonitor categories, both
of which seem to be rigid objects. After taking a close look
at the data we ﬁgure out it is because these categories have
very diverse viewpoints or rich sub-categories.
Our method signiﬁcantly outperforms all three baselines,
as we won 16 out of 20 categories. Our approach performs
decently for both rigid objects and those objects with local
or global deformations. Compared to , our edge comes
from our regionlet representation encoding object’s spatial
conﬁguration. Compared to , our improvement on accuracy is led by the joint deformation and misalignment handling powered by the regionlets representation with multiple resolution features. If we limit the number of regionlets
in a region to be 1, our method obtained a mean average precision of 36.8%. Allowing multiple regionlets consistently
improves the object detection accuracy for each class and
pushes the number to 41.7%, which to our best knowledge,
is the best performance reported on the VOC 2007. The
results of are based on the old version of DPM. Direct
comparsion to other methods may not be fair. It combines
the output of and objectness measurement which gives
0.6% improvement. It suggests selective search itself does
not beneﬁt detection too much in terms of accuracy. A similar conclusion has been drawn in .
Table 3 presents the effectiveness of different feature
conﬁgurations. Figure 5 shows the average number of regionlets used per region for each class. Deformable objects
Table 1: Performance comparison with the baselines on the PASCAL VOC 2007 dataset (average precision %). DPM:
deformable part based model. SS SPM: selective search with spatial pyramid features. Regionlets-S: our regionlets approach
with a single regionlet per region. Regionlets-M: our regionlets approach allowing for multiple regionlets per region. mAP
is the mean average precision over all the 20 categories.
aero bike bird boat bottle bus
chair cow table dog horse mbike person plant sheep sofa train
DPM 1 33.2 60.3 10.2 16.1
54.3 58.2 23.0
SS SPM 2 43.5 46.5 10.4 12.0
49.4 53.7 39.4
Objectness 28.6 54.5
42.0 50.2 18.2
Regionlets-S 50.8 44.6 17.0 23.5
48.9 67.6 39.1
Regionlets-M 54.2 52.0 20.3 24.0
55.5 68.7 42.6
Table 2: Performance comparison with the baselines on the PASCAL VOC 2010 dataset (average precision %).
aero bike bird boat bottle bus
chair cow table dog horse mbike person plant sheep sofa train
DPM 45.6 49.0 11.0 11.6
50.5 43.1 23.6
SS SPM 58.2 41.3 19.2 14.0
44.8 36.7 48.8
Regionlets-M 65.0 48.9 25.9 24.6
56.1 54.5 51.2
Figure 5: Statistics of number of regionlets used for each
class. Deformable objects generally prefer more regionlets.
such as bird, cat, sheep, person and dog, etc. prefer more
regionlets than rigid objects like bicycle, bus, diningtable,
motorbike, sofa and train. An interesting yet consistent phenomenon has been observed for rigid objects like aeroplane
and tvmonitor, as in the comparison of and : our
algorithm selects even more regionlets than those for other
deformable objects. We speculate the regionlets in these
two cases may help to handle misalignment due to multiple viewpoints and sub-categories. Table 2 shows our de-
Table 3: Performance of different features on the PASCAL
VOC 2007 dataset.
Feature HOG LBP COV HOG+LBP HOG+COV LBP+COV
tection performance compared with baselines on the VOC
2010 dataset. Our regionlet approach again achieves the
best mean average precision.
Table 4 compares our approach with other state-of-the-arts methods on both VOC
2007 and VOC 2010 datasets in terms of mean average precision over the 20 categories. While the accuracies of 
and are close to ours, both of them are based on the
DPM with different ways to explore the context information. In contrast, our approach does not utilize any
context cues and the context information will likely further
beneﬁt our detection approach.
Table 4: Comparison with state of the arts using mAP over
20 classes. “WC” means the method utilizes context cues.
We do not use any context information in our method.
VOC 2007 VOC 2010 Results year
DPM(WC) 
UCI 2009 
INRIA 2009 
NLPR(WC) 
MITUCLA(WC) 
MIT 2010 
Song et al. (WC) 
Li et al. (WC) 
SS SPM 
Cinbis et al. (WC) 
Ours (Regionlets)
 
2We read the performance from the ﬁgure in .
Speed: We conducted our experiments on 12-core Xeon
2.1GHz blade servers.
Multiple threading is utilized to
speed up the training and testing procedure. Training for
one category on a single machine takes 4 to 8 hours, depending on how difﬁcult the category is. The detection runs
at 50 frames per second on a server or 5 frames per second
using a single core.
4.2. Experiment on ImageNet Object Detection
We demonstrate the scalability of the regionlet-based detection on the ImageNet object detection task, using the
training set for training and validation set for testing. The
DPM performance is obtained by applying the DPM v5
(version 5) code on the ImageNet1. The mAP over the 200
object categories is reported in Table 5. Our approach outperforms the latest DPM by 4.7%(mAP) across the 200 categories which is indeed a signiﬁcant improvement. The objects in ImageNet have large variations in terms of deformation and sub-categories. Due to the regionlets representation
and enforced spatial layout learning, our proposed approach
performs perfectly in both cases.
Table 5: Comparison with DPM on the ImageNet dataset.
ImageNet 2013 (mAP) Results year
DPM v5 
Ours (Regionlets)
5. Conclusion and Future Work
In this paper, we propose a regionlet-based approach for
generic object detection. Regionlets provide a radically different way to model object deformation compared to existing BoW approaches with selective search and DPM approaches. Our regionlet model can well adapt itself for detecting rigid objects, objects with small local deformations
as well as long-range deformations. Validated on the challenging PASCAL VOC datasets and ImageNet object detection dataset, the proposed regionlet approach demonstrates
superior performance compared to the existing approaches.
As a future work, we plan to improve the way of proposing bounding boxes in term of recall and speed. Indeed, this
step currently is the computation bottleneck in our method.
Second, we will investigate how the context information can
be integrated into the boosting learning process for further
improving detection performance.