IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 49, NO. 1, JANUARY 2001
Adaptive Subspace Detectors
Shawn Kraut, Member, IEEE, Louis L. Scharf, Fellow, IEEE, and L. Todd McWhorter
Abstract‚ÄîIn this paper, we use the theory of generalized likelihood ratio tests (GLRTs) to adapt the matched subspace detectors
(MSDs) of and to unknown noise covariance matrices. In
so doing, we produce adaptive MSDs that may be applied to signal
detection for radar, sonar, and data communication. We call the resulting detectors adaptive subspace detectors (ASDs). These include
Kelly‚Äôs GLRT and the adaptive cosine estimator (ACE) of and
 for scenarios in which the scaling of the test data may deviate
from that of the training data. We then present a unified analysis
of the statistical behavior of the entire class of ASDs, obtaining statistically identical decompositions in which each ASD is simply decomposed into the nonadaptive matched filter, the nonadaptive cosine or -statistic, and three other statistically independent random
variables that account for the performance-degrading effects of
limited training data.
Index Terms‚ÄîAdaptive signal detection, adaptive subspace detector, data communication, matched subspace detector, radar detection, sonar detection.
I. INTRODUCTION
UR AIM in this paper is to adapt the four matched subspace detectors (MSDs) of and to unknown noise
covariance in order to produce adaptive subspace detectors
(ASDs) that may be applied to signal detection for radar, sonar,
and data communication. Whenever we speak of an MSD
problem, we assume that the noise covariance matrix
known. When we speak of an ASD problem, we assume the
covariance matrix is unknown and is estimated from training
There are four (nonadaptive) matched subspace detectors that
form the basis for the adaptive subspace detectors of interest
to us here. They arise from two types of generalizations of the
matched filter detector. First, the inner product of the matched
filter may be generalized to a projection of the measurement
onto a higher dimensional signal subspace, thus producing a
subspace detector . Second, the detector may be normalized
by an estimate of the noise power to make it have a constant
false alarm rate (CFAR) with respect to the noise power. The
four detectors are thus
Manuscript received September 1, 1998; revised August 22, 2000. This
work was supported by the Office of Naval Research under Contracts
N00014-89-J-1070 and N00014-00-1-0033, and by the National Science
Foundation under Contracts MIP-9529050 and ECS 9979400. The associate
editor coordinating the review of this paper and approving it for publication
was Prof. Victor A. N. Barroso.
S. Kraut is with the Department of Electrical and Computer Engineering,
Duke University, Durham, NC 27706 USA.
L. L. Scharf was with the University of Colorado, Boulder, CO 80309
USA. He is now with the Department of Electrical and Computer Engineering, Colorado State University, Fort Collins, CO 80523 USA (e-mail:
 ).
L. T. McWhorter is with Mission Research Corp., Fort Collins, CO
80522-0466 USA.
Publisher Item Identifier S 1053-587X(01)00066-6.
1) the coherent MSD (i.e., matched filter), which is a normally distributed statistic that detects coherent signals
by resolving the inner product of the measurement and
2) the MSD, which is a
statistic that detects subspace
signals (including noncoherent signals) by computing the
energy of the measurement in the signal subspace;
3) the coherent CFAR MSD, which is a or ‚Äúcosine‚Äù statistic
that detects coherent signals in noise of unknown variance by resolving the cosine of the angle the measurement
makes with the signal;
the CFAR MSD, which is an
or ‚Äúcosine-squared‚Äù
statistic that detects subspace signals (including noncoherent signals) in noise of unknown variance by
measuring the fraction of energy the measurement has in
the signal subspace.
Each of the resulting four detectors is a GLRT for a concrete
problem, and each is UMP-invariant, uniformly most powerful
over the entire class of detectors invariant to an appropriate
transformation group. This was one of the main points of ,
namely, that the GLRTs have the same invariances as the UMPinvariant tests of , and therefore, they inherit the optimality
properties of the UMP-invariant tests for an interesting class of
multivariate Gaussian detection problems.
All of these detectors are compelling. They have clearly
stated optimalities and invariances, and they have evocative
geometrical interpretations. The MSDs use extra knowledge
of the noise variance for some performance gain against the
CFAR MSDs, which do not assume this knowledge (the gain
is slight unless the SNR exceeds the measurement dimension).
On the other hand, the CFAR MSDs (or ‚Äúcosine‚Äù statistics)
compensate for this lack of knowledge by providing an extra
invariance to data scaling, a property that the MSDs do not
have. A consequence of this invariance is that the CFAR
MSDs are CFAR over the whole class of elliptically contoured
distributions (a result that is obvious for the special case of
compound-Gaussian noise, multivariate Gaussian with random
amplitude scaling; see Section II). The scale invariance sacrifices some high-SNR performance gain in return for robustness
to tenuous and changeable prior information about channel
noise variances, filter gains, and noise statistics.
The MSDs and CFAR MSDs all assume prior knowledge of
noise covariance matrices. However, this information is often
not known, meaning that, in practice, it must be estimated and
then used correctly in an adaptive detector. In this paper, we
address this problem by adapting the MSDs and CFAR MSDs
to unknown noise covariance in order to derive ASDs and CFAR
ASDs. To adapt the MSDs, we appeal to the fundamental results
of Kelly on GLRTs, and to adapt the CFAR MSDs, we use
the results of .
1053‚Äì587X/01$10.00 ¬© 2001 IEEE
IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 49, NO. 1, JANUARY 2001
In order to clarify the adaptive problems we are studying and
to establish the meaning of the abbreviations we use, we offer
the following taxonomy of adaptive problems and detectors.
1) The coherent ASD is the adaptive GLRT generalization
of the coherent MSD, wherein the training data for estimating the unknown covariance matrix is scaled the same
as the test data.
2) The ASD is the adaptive GLRT generalization of the
MSD, and it equals the Kelly GLRT , wherein the
training and test data are scaled the same.
3) The coherent CFAR ASD is the adaptive GLRT generalization of the coherent CFAR MSD, and it is used for detecting a coherent signal when the training data is not constrained to be scaled the same as the test data.
4) The CFAR ASD is the adaptive GLRT generalization of
the CFAR MSD, wherein the training and test data are
not constrained to be uniformly scaled, and it equals the
adaptive cosine/coherence estimator (ACE) of , ,
In the nomenclature of the nonadaptive detection literature,
‚ÄúCFAR‚Äù is with respect to noise level or variance
in the test
data. In the adaptive detection literature (see, e.g., and ),
‚ÄúCFAR‚Äù is with respect to the noise covariance
, assumed to
be uniform over test and training data. Retaining the assumption that the noise is uniform in covariance structure, we say
a detector is ‚ÄúCFAR‚Äù when it is insensitive to variation in the
overall scale. In other words, we allow the noise level to vary
between training and test data with covariance
, respectively. We mean ‚ÄúCFAR‚Äù with respect to both the shared
noise covariance structure
and independent scaling
noise in the test data. This generalizes the meaning of ‚ÄúCFAR‚Äù
in both the nonadaptive and adaptive detection literature, where
‚ÄúCFAR‚Äù is respect to a presumed noise level or shared covariance between test and training data, respectively.
The CFAR ASDs have the remarkable property that they are
just the ‚Äúsample versions‚Äù of their corresponding CFAR MSD‚Äôs,
with a known covariance
replaced by the sample covariance
. The coherent ASD and the ASD are not sample versions of
their corresponding MSDs. However, these two ASDs may be
approximated as in and to obtain adaptive matched filters (AMFs). The coherent AMF is the sample version of the coherent MSD and the AMF is a sample version of the MSD. The
AMFs are not GLRTs, but they may be used when the training
and test data are scaled uniformly.
In the following treatment of matched and adaptive subspace
detectors, we compare and contrast invariances and performances. Under ideal conditions, the coherent ASD and ASD
typically outperform the coherent CFAR ASD and CFAR ASD,
although the performance gain is small for low SNR and small
sample support. However, the CFAR ASDs enjoy an extra
invariance with respect to data scaling that makes them robust
against system gains and deviations from the standard Gaussian
model. Consequently, we expect the CFAR ASDs to find
application in radar, sonar, data communication, time series
analysis, and array processing, where this extra invariance to
scaling will be desirable for operations in channels and systems
with variable gains or non-Gaussian statistics. This point will
be developed more fully in the sections to follow.
We conclude the paper with identical statistical decompositions for the ASDs, AMFs, and CFAR ASDs. These decompositions allow us to gain insight into the structure of adaptive detection statistics by decomposing them in terms of their nonadaptive counterparts and corruptive noise terms attributable to finite
training-data support. They also allow us to quantitatively characterize the random effects of adaptation, simplify Monte-Carlo
simulations, more simply derive exact distributions, and compute receiver operating characteristic (ROC) curves.
II. PROBLEM OF DETECTING A SUBSPACE SIGNAL
The problems we study are these. The parameter
in the signal subspace
of dimension
. That is,
, which is the linear space of (
complex matrices. This signal is scaled by
, and the channel
adds scaled noise
to the signal to produce the measurement
, which is distributed as
denotes the complex-normal density of the
-dimensional complex measurement
. The problem is to test hypotheses about the parameter
, indicating presence or absence
of the signal, under various assumptions about the parameters
Throughout our developments, we define the whitened signal
mode matrix
and the whitened measurement
as follows:
Then, when the detectors are written in terms of whitened variables, these relations may be used to express the detectors in the
original coordinates.
Whenever we refer to the ‚Äúrank-1 case,‚Äù we are discussing
the situation where the dimension of the signal subspace
1. We denote this one-dimensional (1-D) complex subspace by
. In this case, the parameter
is the complex
phase term
, and the signal
is the phased vector
. The whitened signal vector
The matched subspace detection problem is as follows.
, with the signal modes
and the noise covariance
known. Depending on the problem
specification, the noise variance
and the location parameter
may be known or unknown. Test the hypothesis
(noise only) versus the alternative
(signal plus noise).
Throughout this paper, we use the notation
to mean ‚Äúis
distributed as.‚Äù
A. MSD for Coherent Detection
A coherent signal can be written as
the matched subspace problem for coherent detection, the noise
and the location parameter
are both known. When
is completely known, then both
are known, which
is the coherent rank-1 case. A slight modification of standard
results (see, for example, and ) produces the MSD for
coherent detection:
KRAUT et al.: ADAPTIVE SUBSPACE DETECTORS
is the whitened matched-filter statistic
In the original coordinates, this has the form
Thus, the whitened matched filter statistic is complex normal,
with noncentrality parameter
which is the output ‚Äúvoltage‚Äù SNR (which equals signal-tonoise ratio when squared). This means the detector statistic
is an unbiased estimator of the output voltage SNR, where the
estimator may be thought of as
is the unbiased, maximum-likelihood estimator of the
signal level
. We call this an SNR representation for
. This detector resolves the whitened measurement
whitened signal subspace
. The statistic is invariant to translation of
in the perpendicular subspace
B. MSD for Subspace Detection
In the matched subspace detection problem for noncoherent
detection, the noise variance
is known, but the location parameter
is unknown. The MSD for noncoherent detection is
is the matched subspace detector statistic
is the orthogonal projection onto the subspace
. In this formula,
denotes the density
for a noncentral complex chi-squared (or gamma) random variable. By this, we mean a chi-squared random variable, scaled
complex degrees of freedom (or
real degrees
of freedom), and with a noncentrality parameter
Thus, by measuring the energy
in the whitened signal
, the detector
is estimating the output SNR
. It does this by estimating
is the pseudo-inverse of
. This gives the unbiased, maximumlikelihood estimate of
, which is then squared to estimate
the output SNR. Alternatively, we can write the noncoherent
MSD statistic in its SNR representation as
This detector resolves the energy of the measurement
in the whitened signal subspace
. This energy is invariant to
rotation of the measurement in the signal subspace
translation in the perpendicular subspace
C. CFAR MSD for Coherent Detection
In the CFAR matched subspace detection problem for coherent detection, the location parameter
is known, but the
noise variance
is unknown. The CFAR MSD for coherent
detection is
is the ‚Äúcosine‚Äù form of the coherent CFAR MSD ,
The square of the
statistic has a noncentral beta distribution,
which is beta under
statistic is a monotone function
of the CFAR MSD in its
form , . That is
is a complex -distributed statistic
In this formula,
-distribution with
degrees of freedom
noncentrality
in place of the unknown scaling
. The -form of this statistic estimates the output voltage SNR
, by estimating both
is an unbiased estimate of the noise level
. The cosine form measures the cosine of the angle that the test vector
makes with the signal vector
 .
Compound Gaussian noise is a special case of ‚Äúelliptically contoured‚Äù (EC) random vectors, whose distribution depends on the
Geometry and invariances of the CFAR matched subspace detector
(CFAR MSD) for noncoherent detection. The statistic is invariant to
transformations g(z) that include scaling and rotations in the subspaces hi
measurement
only through the quadratic form
contours of constant probability density for
are ellipsoids, and
contours of the density for
are spheres, meaning
is a spherically invariant random vector.
A result from the study of spherically invariant distributions
is that the
(or cosine-squared) and
(or cosine) statistics
presented in this section have the same distribution for any
zero-mean EC distribution on
and are therefore CFAR
across the class of such distributions (see, for example, [14,
pp. 38‚Äì39]). This result is a consequence of the scale-invariance property of these statistics; that is, a zero-mean
elliptical random vector has the stochastic representation
means ‚Äúequal in distribution to,‚Äù
is uniformly distributed on the
-dimensional unit spherical
incorporates the radial dependence
that distinguishes different EC distributions (see, for example,
[15, p. 55‚Äì57]). Any scale-invariant statistic will not depend
and will thus be distribution free within the class of
zero-mean EC distributions on
(note that this is not true of
statistics). This point is critical and not widely
appreciated in the detection literature; with the CFAR MSDs,
a given threshold will give the same the probability of false
alarm for any multivariate density with ellipsoidal contours
parameterized by
constant. The density could
be Gaussian, compound-Gaussian, uniform on an ellipsoid,
multivariate-Cauchy, etc.
Richmond uses this type of scale invariance argument to show
that the adaptive Kelly GLRT (ASD) and AMF statistics are distribution free under
over a whole class of EC distributions
on the test and training data [15, p. 70], in which the concatenated vector consisting of stacked training and test data vectors
is EC distributed, meaning that these vectors are uncorrelated
but statistically dependent. He uses the fact that the ASD and
AMF are invariant to uniform scaling of the test and training
data. Richmond‚Äôs argument can also be applied to the adaptive
cosine (ACE) statistic of the next section because it has this invariance, in addition to being more generally invariant to independent scaling of test and training data.
KRAUT et al.: ADAPTIVE SUBSPACE DETECTORS
TAXONOMY OF RESULTS FOR MATCHED SUBSPACE DETECTORS
III. PROBLEM OF ADAPTIVELY DETECTING A SUBSPACE
How does one make the MSDs and CFAR MSDs of the previous section adaptive to unknown covariance structure
order to derive ASDs and CFAR ASDs? To make this question meaningful, we must be clear about the assumptions under
which the detector is applied. There are many variations on the
experiment we describe, and each produces a different detector.
We will depart slightly, but significantly, from Kelly‚Äôs lead 
and design the experiment as follows.
A sequence of
independent and identically distributed
training vectors
, each distributed as
is measured in the training phase of the experiment. In the
statistically
independent
is measured, and from it, the hypothesis
(noise only) is tested against the alternative
(signal plus noise).
In this experiment, we generalize Kelly‚Äôs original experiment
by allowing the covariance matrix for the test vector, namely
, to differ by a scale constant
from the covariance matrix for the training data
. This generalization produces new
adaptive detectors, with extra invariances that Kelly‚Äôs detector
 , and its derivative AMF forms , , do not have.
We organize the training vectors into the data matrix
the composite measurement. The
joint density function for the composite measurement, under the
alternative
, the density is
. The density
may be rewritten as
is the composite sample covariance matrix constructed from both the training and test data:
In the section to follow, we extend the GLRT methodologies of
 , , and to determine the GLRT tests
where the carets denote the generalized likelihood ratios that result from maximizing the likelihood with respect to parameters
which are unknown, such as
When the noise scaling
is assumed to be known, this
GLRT procedure yields coherent and multirank versions of
the noncoherent detector of Kelly . These detectors are
not sample-matrix versions of the coherent MSD and MSD,
meaning that a sample covariance does not simply replace a
known covariance in the detector statistic. However, when
is unknown, maximizing the likelihood functions over this additional parameter yields CFAR ASDs that are sample-matrix
versions of the CFAR MSDs . Thus, we have the remarkable
fact that the CFAR ASDs retain the same form as the CFAR
IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 49, NO. 1, JANUARY 2001
is unknown. We proceed case by case to outline
these results.
In our discussion of the GLRT, we will find that the following
approximately whitened signal modes
, whitened signal vector
, and measurement
arise naturally in the theory:
A. ASD for Coherent Detection
The adaptive subspace detection problem for coherent detection is the MSD problem outlined in Section II-A, with the modification that the noise covariance matrix
is unknown. When
the maximum-likelihood estimates of
are used (under
), together with the maximum-likelihood estimate of
(constrained to be real and positive), the resulting generalized likelihood ratio (GLR) is 
Constructing a monotonic function of this gives the GLRT
is the ASD statistic for coherent detection
where we have set the known parameter
equal to unity to be
consistent with the adaptive detection literature. This statistic
may also be written as a monotone function of
, which is defined through
Then, an alternate representation for the coherent ASD is
where we employ the double-hat notation to indicate that
is an adaptive estimator of the signal amplitude
affected by
the training data. The quadratic form
would be proportional to the maximum-likelihood estimator of the relative noise
not constrained to be unity (we will discuss
this further in Section III-C).
The coherent ASD statistic
extends Kelly‚Äôs detector to
coherent problems. It is not quite the sample-matrix detector that
one obtains by just replacing
in (4). However, it does
become this detector if the denominator term
approximated by
. Then, it generalizes the ‚Äúadaptive matched
filter‚Äù (AMF) of , to coherent problems, as follows:
The detector
resolves the projection of the adaptively whitened
measurement
in the adaptively whitened signal subspace
The interpretation and invariances remain those of Section II-A.
B. ASD for Subspace Detection
The adaptive MSD problem for noncoherent detection is the
problem outlined in Section II-B, with the modification that the
noise covariance matrix
is unknown. The maximum-likelihood estimates of
), and the product
yield the GLR 
A monotone function of this gives the GLRT
is the ASD statistic for noncoherent detection
In terms of
, this statistic has the SNR representation
This detector generalizes Kelly‚Äôs test to multidimensional
subspaces , and when the denominator is well-approximated by
it generalizes the AMF of Robey et al. and
Chen and Reed to multidimensional subspaces , 
measures the energy of
in the subspace
interpretations and invariances remain those of Section II-B.
C. CFAR ASD for Coherent Detection
The CFAR ASD problem for coherent detection is the
problem outlined in Section II-C, with the modification that the
noise covariance matrix
is unknown. The maximum-likelihood estimates of
), as well as
(constrained to be real and positive), yield the GLR 
A monotone function of this gives the GLRT
KRAUT et al.: ADAPTIVE SUBSPACE DETECTORS
is the CFAR ASD statistic for coherent detection:
This detector generalizes the detector of and to coherent problems. It is just the ‚Äúsample-matrix‚Äù version of the
coherent CFAR MSD detector, and its
version is the sample
version of
This statistic may be written as
is an adaptive estimator of the signal amplitude
quadratic form
is the maximumlikelihood estimate of
, which is inserted into the
likelihood-ratio to obtain the CFAR ASD, or ACE detector, as
a GLRT in . The interpretation and invariances remain those
of Section II-C.
D. CFAR ASD for Subspace Detection
The CFAR ASD problem for noncoherent detection is the
problem outlined in Section II-D, with the modification that the
noise covariance matrix
is unknown. The maximum-likelihood estimates of
), and the product
, yield the GLR 
A monotone function of this gives the GLRT
is the CFAR ASD statistic for noncoherent detection
This detector generalizes the ACE of and to multidimensional subspaces. It is just the sample version of the CFAR
MSD, and its
version is just the sample version of
This statistic may be rewritten in its SNR form as
The interpretation and invariances remain those of Fig. 1.
E. Summary and Taxonomy
The results for ASDs and CFAR ASDs are summarized in
Table II. This time, the noise structure
is unknown. Each of
the ASD and CFAR ASD detectors is a GLRT. The CFAR ASDs
are sample versions of their CFAR MSD counterparts and thus
enjoy the same invariances. The approximations to the ASDs,
which are termed ‚Äúadaptive matched filters‚Äù (AMFs) to be consistent with the terminology of , are not GLRT. However,
they are sample versions of their MSD counterparts.
The ASD and AMF statistics are invariant to the transformation group
is a positive scalar
for the coherent detectors, complex for the subspace detectors,
meaning the test and training data may be scaled identically
without changing these statistics. In contrast, the CFAR ASD
statistics are invariant to the transformation group
, which means the training data and the test data may
be scaled differently without affecting them. This is the key point
when comparing ASDs, AMFs, and CFAR ASDs.
IV. STOCHASTIC REPRESENTATIONS FOR MULTIRANK
ADAPTIVE SUBSPACE DETECTORS
In this section, we will analyze how the ASDs in Section III
are distributed by using statistically identical decompositions
 . Using this approach, it is possible to represent any one of the
adaptive detectors in Table II as a simple function of the same
set of five independent random variables. We will carry out our
derivation, in detail, for the AMF, when the signal subspace
has dimension . For this analysis, we make use of insights obtained from a similar, and simpler, analysis of the rank-1 case in
 and . To compare the robustness of the adaptive statistics,
their distribution is analyzed in the general case where the measurement
has covariance
, even though
is assumed to
be unity in the standard derivations of the ASD and AMF detection statistics.
The derivation of statistical decompositions for the multirank
ASD, AMF, and CFAR ASD can be outlined in six steps. The
first four steps are analogous to those presented in and 
for the rank-1 detectors.
1) Apply the whitening transformation
training and test data to generate the transformed signal
and test vector
2) Next, apply a unitary transformation to rotate to a coordinate system in which the first
basis vectors are set
in the direction of
is the number of
signal modes or columns of
3) Resolve the inverse of the sample covariance matrix
4) Perform a change of variables on the elements of the resulting
covariance matrix so that these
variables are now statistically independent.
For the 1-D case, where
, the procedure terminates here.
For higher rank ASDs, these steps must be followed by two
more steps: 5) rotation and 6) matrix partitioning. Those less
interested in the details of the derivation may wish to skip to
The analysis will be based on the statistical behavior of the
sample correlation matrix
, where the
IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 49, NO. 1, JANUARY 2001
TAXONOMY OF RESULTS FOR ADAPTIVE SUBSPACE DETECTORS. NOTE THAT THE LOWER FOUR PANELS HAVE THE SAME FORM AS THOSE OF TABLE I, WITH S
REPLACING R. THE UPPER TWO PANELS ARE GENERALIZATIONS OF THE KELLY DETECTOR . ALL FOUR ASDS ARE GLRTS. THE AMFS ARE NOT
training data used to build
is complex Gaussian distributed as
. Then, the scaled covariance estimate
has a complex Wishart distribution
This puts the noncoherent AMF statistic into the normalized
Throughout the derivation, we will follow the lead of Reed et al.
 by making use of the following theorem mentioned in :
is nonsingular, then
. We will also make use of
the following theorem, which describes how to construct statistically independent random matrices from partitioned Wishart
A. Partitioned Wishart Matrices
1: Consider
, which we partition as
. Now, suppose we construct a data
consisting of
realizations
of . Partition the Wishart matrix
as follows:
Then, the following change of variables produces random matrices that are statistically independent
are Wishart with
degrees of
freedom, and
is a matrix with complex normal columns.
This can be verified by substituting the change of variables
into the density of (48) and computing the Jacobian of the transformation by using the techniques employed in , for example. Then, the joint density of
KRAUT et al.: ADAPTIVE SUBSPACE DETECTORS
The interpretation of this is as follows. The data matrices
are uncorrelated (and because they are Gaussian, statistically independent). Further, we can apply the following projection operator to the rows of
, which projects onto the row
. This allows us to decompose
as a superposition of two parts (
is a sample version of the Wiener-filter estimator of
is the residual adaptive error
are the adaptive estimators of
, respectively. Then
gives the adaptive estimate of the error covariance, and
gives the estimate of
remaining matrix
can be attributed to
by constructing the unitary transformation
. (Theorem 1
is a little more general than the standard theorem on Schur
complements of Wishart matrices in its treatment of the matrix
. It generalizes to arbitrary
case is all that
is needed for the results of this paper. Please note that
are used differently in this section than in the rest of this paper.)
B. Step 1: Whitening
We now return to (47) and apply a series of transformations to simplify its form. The whitening transformation
generates the transformed signal modes
, test vector
, and whitened sample covariance matrix
. The multirank
AMF can now be written as
C. Step 2: Rotation Into the Signal and Measurement Subspace
Next, we rotate into a new coordinate system by applying the
unitary transformation
. In the new coordinate
system, the signal matrix
determines the direction of the
basis vectors:
th basis vector is determined by that part of
the test vector
that does not lie in the signal basis; therefore, the rotated test vector
nonzero components:
illustrated
the same vector that arises in the (nonadaptive) MSD statistic:
. The component
contributes to the estimated
noise scaling, that is,
is an estimate of
The rotation into the signal and measurement subspace U sets the first
p + 1 basis vectors in the directions of  and P z.
has a complex chi-squared distribution
degrees of freedom:
. Note that the
-test version of
the CFAR MSD in (17):
The transformed sample covariance
also an estimate of identity and is Wishart distributed as
. In this new coordinate system, the
multirank AMF statistic is
D. Step 3: Partitioning the Covariance Matrix
Since the signal and test vectors in the new coordinate system
have, at most,
nonzero components, we are really only
concerned with the upper left
We partition
and use Theorem 1 to identify
Wishart distributed
block, with reduced degrees
of freedom,
The multirank AMF can now be written in terms of the
Wishart matrix
contains the first
elements of
We now apply Theorem 1 for a second time to identify the
upper-left
IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 49, NO. 1, JANUARY 2001
Then, we can write the multirank AMF as the quadratic form
E. Step 4: Transformation to Independent Covariance
Coefficients
is an estimate of identity, the covariance partitions
) are not independent, as can be seen from the
Wishart distribution of
. However, application of Theorem 1
generates new variables ( ,
) that are statistically independent:
(a matrix)
(a vector)
(a scalar)
This gives the following representation for the multirank
as a quadratic form in the Wishart matrix
are Rayleigh-distributed random variables (square-root of a chi-squared).
F. Step 5: Another Rotation
For 1-D subspaces with
, the stochastic representation of (58) is completely simplified since
scalars, which we will later denote as
. However,
for multirank subspaces, rotation and matrix partitioning operations need to be applied once again in order to reduce
scalar. Letting
, we apply the unitary transformation
. The transformation
rotates to a coordinate
system in which the first basis vector is set in the direction of
STATISTICAL DECOMPOSITIONS FOR THE MULTIRANK ASD, AMF, AND
CFAR ASD STATISTICS
, that is,
operation transforms the multirank AMF to
is the transformed sample covariance
G. Step 6: Another Matrix Partitioning
We can again use Theorem 1 to identify the upper-left element
where the scalar
This gives the final decomposition for the multirank AMF
compared with the multirank MSD:
(Recall from Section IV-C that the nonadaptive MSD is
.) In this formula, the normal random vectors
the Rayleigh random variables ,
, are all statistically
independent.
The decompositions for the remaining multirank adaptive detectors, namely, the ASD
and the CFAR ASD
found by following the same sequence of six steps. The results
are summarized in Table III.
H. Observations
Table III is organized so that decompositions for the MSD
and CFAR MSD statistics are recorded in the left-hand column,
KRAUT et al.: ADAPTIVE SUBSPACE DETECTORS
STATISTICAL DECOMPOSITIONS FOR THE COHERENT ASD, AMF, AND
CFAR ASD STATISTICS
and decompositions for the ASD, AMF, and CFAR ASD statistics are recorded in the right-hand column. To illustrate the
heuristic value of these decompositions, consider the multirank
MSD statistic
and the multirank AMF statistic
. It is as if the statistic
undergoes the linear transformation
its norm-squared is computed to obtain
. This linear transformation is of the form
are independent noise terms. The influence of these additive and multiplicative noise terms decreases as the available training data
increases. When
in distribution,
in distribution.
Similarly, consider the multirank CFAR MSD
and the multirank CFAR ASD
. It is as if the statistic
undergoes the linear transformation
before its normsquared is computed. Again, the influence of the noise terms
decreases as
The ASD has a similar interpretation.
The rank-1 versions of Table III, together with the decompositions for all of the rank-1 coherent detectors, have been obtained in and . The rank-1 versions that apply to coherent
problems are summarized in Table IV. All of the observations
just made about ASDs being equivalent to noise-corrupted versions of their MSD counterparts apply here as well.
It should be noted that these decompositions are not merely
‚Äústochastic representations‚Äù in the sense of being ‚Äúequal in distribution to‚Äù (or ‚Äú ‚Äù). They are identical decompositions on a
realization by realization basis, i.e., for a given realization of
, the adaptive statistics are expressed in terms of the corresponding realizations of
In addition to the intuitive insight into the structure of the
adaptive detectors that these decompositions bring, they also
help in quantitative analysis in several ways. First, they make
it possible to directly calculate the moments, such as mean and
Densities for the statistics
K ^n, K , cos, and cos . Densities are
shown under both H
(symmetric about zero for the coherent statistics and
weighted toward zero for the noncoherent statistics) and H
hypotheses. As
the number of training samples K increases, the hypotheses become better
separated. These densities were obtained from a Monte Carlo simulation using
a million realizations of the statistical decompositions of Tables III and IV.
Other parameters: dimension N = 10; SNR = ( = ) R
variance, of the adaptive detectors, without the need to find analytical expressions for their densities or characteristic functions.
To see how the complexity of the density or characteristic functions can be bypassed, consider the decomposition for the coherent AMF
in Table IV. It is written as sums and products of
. It is possible to write the moments
exactly in terms of the moments of these five random variables, which are distributed as normal, Rayleigh, or the reciprocal of a Rayleigh. In this way, we can analyze how the output
SNR of the adaptive statistics improves as the number of available training samples increases. This is discussed in full detail
in and .
A second advantage comes in performing Monte Carlo
simulations. With the statistical decompositions, the generation
of random realizations of an adaptive statistic can be achieved
much more efficiently. If the gamma random variables are
generated by summing normals, only
random variables need to be generated for a realization of a
rank-1 detector in Table IV, compared with
if the training and test data were generated explicitly. This
is a significant reduction: about a factor of
The results of Monte Carlo simulations performed in this
manner are shown in Figs. 3‚Äì5. Fig. 3 shows how the densities
separate as the available training data
increases. Densities are shown for the rank-1 case
of the ASD and CFAR ASD statistics, which in this figure
and all subsequent figures are labelled Kelly and ACE. Fig. 4
shows the corresponding improvement in the receiver operating
characteristics (ROCs) as the training data increases. Fig. 5
shows the same plots, but they are grouped to compare the
detection performance of the ASD statistics
IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 49, NO. 1, JANUARY 2001
Receiver operating characteristics (ROCs) for the ^n;  ; cos, and
statistics. As the number of training samples K increases, the hypotheses
become better separated. These curves are obtained from the same Monte Carlo
simulation discussed in the caption to Fig. 3; they are plotted as though on
‚Äúnormal probability paper.‚Äù
ROCs when the number of training vectors K is 13, 20, 100, and
infinite. These are the same curves as those in Fig. 4 but plotted to compare
the behavior of the ^n;  ; cos and ~^
statistics.
CFAR ASD statistics
, for various amounts of
training data. As a general rule, the detectors that correspond
to increased knowledge of system parameters (phase
) have better performance. However, as seen in Fig. 5, the
performance loss of the CFAR ASD detectors compared with
the ASD detectors becomes negligible for small training sets
(even becoming a slight performance gain in places), where the
CFAR ASD becomes a better approximation of the ASD.
A third advantage of the statistical decompositions is that they
simplify the derivation of analytical expressions for the density
functions of the ASDs, a topic that we discuss in detail in the
next section.
V. OBTAINING DISTRIBUTIONS FROM THE STATISTICAL
DECOMPOSITIONS
In this section, we use the statistical decompositions of
Table III to find analytical expressions for the distributions of
the ASD, AMF, and CFAR ASD statistics. Those less interested
in some of the details may wish to skip to (68).
A. Multirank ASD, AMF, and CFAR ASD
We will start with the ASD
. Recall from Table III that
the equivalent decomposition writes
The first step here is to separate out the mean of the vector
. We denote by VSNR the mean of
. Then, from Table III,
may be written
are distributed as
‚Äúequal in distribution to.‚Äù The sum
is distributed
the same as
true, irrespective of how
are distributed, assuming they
are independent of
). Therefore, we can write
At this point, we can follow a line of reasoning similar to that of
Kelly, et al. , , and find the distribution conditioned
on the ratio
In the multidimensional case,
have different degrees
of freedom, and
is distributed as
[interestingly, in the rank-1 case (
), this has the same distribution as the Reed et al. normalized output SNR , ].
KRAUT et al.: ADAPTIVE SUBSPACE DETECTORS
With this identification, the factor on the right can be
rewritten with the following algebra:
can be rewritten as
Conditioned on
, the left-hand factor of
-distributed random variable with
degrees of
freedom scaled by
is a complex normal random
vector whose mean is conditioned on . We denote the distribution function of the scaled noncentral
we define this to be the probability that the ratio of a complex-chi-square with
degrees of freedom and a noncentrality
, divided by a complex chi-square with
of freedom is less than
. The conditional distribution of
then given by
. Again, an expression for the final distribution can be obtained by integrating this distribution over the
beta density of .
Using the same procedure for the AMF
yields, from
Conditioning on , we obtain the following statistical equivalent
is defined as above. Then, the distribution of
Finally, we can go through the same procedure for the
of the CFAR ASD,
. From Table III
Conditioned on , the statistical equivalent of
Then, the distribution of
is given by
which notably does not depend on
. The probability of detection (PD) for one of these statistics is given by one minus the
distribution function evaluated at the threshold. The probability
of false alarm (PFA) is the PD when SNR
B. Coherent ASD, AMF, and CFAR ASD
Using the decompositions in Table IV, we can apply the same
techniques to find distributions for the coherent versions of the
ASD, AMF, and CFAR ASD. Conditioned on the beta parameter
, they are all related to the noncentral
distribution
Here, we denote the distribution function of the scaled noncentral by
; this we define to be the probability that the
ratio of the real part of a complex normal with mean
by the square root of a complex chi-square with
degrees of
freedom, is less than . Note that the distribution of Re
not depend on
In summary, the statistical decompositions of Table IV may
be used to get statistically equivalent random variables conditioned on the beta-distributed random variable . Each of these
conditional random variables is a linear transformation of either
a -distributed or an
-distributed random variable. This means
that their distributions may be obtained by integrating a noncentral - or
-distribution against a beta density to get integral representations for the distributions of the coherent and multirank
IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 49, NO. 1, JANUARY 2001
Detection probabilities for (left) K = 100 and (right) K = 25; PFA = 10
, and N = 20.
versions of the ASD, AMF, and CFAR ASD statistics. The distribution results for the coherent ASD, AMF, and CFAR ASD
statistics are original with this paper. The distributions for the
rank-1 noncoherent detectors (in , , and ) arise when
the noise scaling
is set to 1, and the signal subspace rank
is set equal to 1, in the expressions obtained here for the general
multirank case.
VI. NUMERICAL PERFORMANCE COMPARISONS
Now, let us compare the detection performance of the ASD,
AMF, and CFAR ASD statistics. We will consider the rank-1
case of these statistics (
), referring to them in this case
as the Kelly, AMF, and ACE to be consistent with the standard nomenclature of the radar literature. In the rank-1 case, the
ASD (Kelly), AMF, and CFAR ASD (ACE) and associated statistical decompositions (which are obtained by setting
Table III, or by magnitude squaring the expressions in Table IV)
take on the form
We use expressions for the distribution functions obtained in
Section V conditioned on
. We also make use of a useful
finite-sum expression for the noncentraldistribution, which
was found by Kelly . Instead of numerical integration, we
approximate the integral of the distribution, over the densities
of , by averaging realizations of distribution given by 1000 realizations of . (Based on examination of these realizations, we
estimate the uncertainty to be about
the height of the
plots; the qualitative behavior discussed here is still seen with
far fewer realizations.)
The resulting detection curves for the Kelly, AMF, and ACE
are shown in Fig. 6 for
equal to unity and
are more easily interpreted by recognizing that the probability
of detection of the nonadaptive cosine detector (CFAR MSD)
is always upperbounded by that of the MSD; this is the price
paid for estimating the noise scaling
. The performance of the
CFAR MSD only approaches that of the MSD when
compared with the SNR, resulting in a relatively good estimate
of the noise scaling, as given by (13)
(see ; this can also be verified, in terms of the variance and
expectation of
, using the analysis
approach of ).
in Fig. 6, the training data support is relatively high, and consequently, the adaptive detectors behave
close to their nonadaptive counterparts. Consistent with this observation, we can see that the AMF does well against the ACE
at high SNRs, with the difference becoming negligible at low
in Fig. 6, the training data support is relatively low. Here, the adaptive detectors rely on poor sample covariance estimates
. ACE is not only invariant to scaling of the
measurement
but is separately invariant to global scaling of
the training data set
(and, thus, to scaling of
). For this
reason, we expect it to be more robust under conditions of small
sample support. For
, the ACE begins to take advantage
of its scale invariance to
and overtakes the AMF at low SNR.
By comparing (78) with (79) and (80), one can see that the
Kelly GLRT approaches the AMF for very high values of the
sample support
and more closely approximates ACE for very
low values of sample support. In Fig. 6, this can be observed in
how close the Kelly curve is to that of the AMF for
to the ACE for
. In the very regime of small
the ACE performs relatively well, the Kelly begins to behave
more like the ACE, which would be expected by considering
the normalization term in (78) in the small
A. False Alarm Stability
These comparisons have been made under the idealized condition of
, which the Kelly and AMF assume but the
ACE does not. When
, which is the true relative scaling of the
measurement, deviates from unity, the probability of false alarm
KRAUT et al.: ADAPTIVE SUBSPACE DETECTORS
Change in false alarm probability for a constant threshold as the
variance of the noise scaling increases; K = 80, and N = 20.
(PFA) of the Kelly and AMF will be affected, but the PFA of
the ACE will not be affected. In Fig. 7, we illustrate this effect
by introducing a randomly fluctuating noise scaling. Under
this is equivalent to the random amplitude scaling of the compound-Gaussian noise model of . We chose a simple distribution for
: complex chi-squared but normalized to have unit
mean and with a variance equal to 1 divided by the number of
degrees of freedom. Because ACE is invariant to scaling of
then under
, its PFA is completely insensitive to fluctuations
in the noise scaling.
Conversely, while the Kelly and AMF may do well against the
ACE in terms of plots of PD versus PFA, their thresholds cannot
be set to achieve a desired PFA without requiring the user to have
some knowledge of the scaling distribution. The ACE does not
require this information; for a set threshold, its PFA does not
depend on the statistics of the noise scaling, whereas the PFA
of the AMF and Kelly may vary considerably, as illustrated in
In summary, ACE sacrifices a small amount of SNR performance (about 2 dB in Fig. 6, for a PFA of
; results are
similar at other PFAs), for low SNRs (less than
), or for small
sample support. In exchange, it has scaling invariance and true
CFAR performance against scale fluctuations in the test data.
VII. CONCLUSION
In this paper, we have offered a unified treatment of two
classes of generalized likelihood ratio tests: the MSDs, which
use a known noise covariance structure, and the ASDs, which
use training data to estimate an unknown noise covariance structure. Both matched and adaptive subspace detectors may be further classified according to whether the test signal is completely
specified (coherent) or parameterized (subspace) and according
to whether the noise level is known or unknown. In the adaptive
case, the unknown noise-level problem translates to an unknown
scaling between the noise in the training data and test data; it is
assumed that the training data accurately represents the noise
structure but may not accurately represent the noise level.
Maximizing the likelihood ratios over this additional scaling
parameter produces the cosine-based CFAR MSDs and the
CFAR ASDs, which are invariant with respect to arbitrary
scaling of the test data. In addition, the CFAR MSDs are
CFAR with respect to the entire class of elliptically contoured
distributions, which include compound-Gaussian distributions.
It is interesting that the CFAR ASDs, which include the ACE
statistic, have the same form as the CFAR MSDs, with the
sample covariance replacing the known covariance. This is
not true of the ASDs for known noise scaling, such as the
Kelly GLRT , which does not take the form of the matched
subspace detector.
The CFAR ASDs suffer some performance loss under the
idealized scenario of homogeneity between training-data and
test-data noise statistics. However, their invariance to test-data
scaling makes them CFAR with respect to variation in the noise
level between training and test data; other researchers have
shown them to have robustness to more complicated inhomogeneities, such as changes in the power of clutter discretes .
We have presented a unified description of the statistical behavior of the class of ASDs, including those parameterized by
multidimensional signal subspaces. We have shown that they
each have an identical statistical decomposition, which is a simplified function of the same set of five statistically independent random variables. These same random variables appear in
all such representations; they include the nonadaptive matched
filter and the /cosine statistic, plus three perturbing variables
attributable to training data. In addition to their heuristic value,
these representations provide some computational advantages.
We have used them here for more efficient Monte Carlo simulations and to present in detail a simplified derivation of analytical
expressions for the probability distributions of the ASDs.
VIII. POSTSCRIPT
This paper traces its heritage to the collaboration of LLS and
D. W. Lytle, who in applied the theory of invariance in hypothesis testing to the problem of CFAR signal detection. These
ideas were then generalized to incorporate multidimensional detectors in the collaborations of LLS and M. J. Dunn , resulting in the treatment of MSDs given in . The collaboration
of LLS and B. J. Friedlander led to the GLRT interpretation of
 . The work of Kelly , a major contribution to adaptive detection, was followed by the work of Chen and Reed and
Robey et al. . These papers are the natural predecessors of
this paper.
About the time of , R. L. Spooner and G. Vezzozi
and B. Picinbono derived CFAR detectors for spherically
invariant noise. These papers are predecessors of the work on
adaptive detection for spherically invariant noise by Conte et al.
 , , who suggest the rank-1 version of the noncoherent
CFAR ASD derived in . Conte et al. , slightly predate
the rank-1 version of the CFAR ASD presented in , , and
 . However, as we show in this paper, the detector of ,
 , and is just one of a large class of adaptive detectors
one can derive from a maximum likelihood principle, beginning
with the MSDs of , , and . In fact, it was not until the
publication of that we had a convincing derivation for the
CFAR ASD, based on asymptotic arguments in and 
and based on heuristic arguments in .
IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 49, NO. 1, JANUARY 2001
ACKNOWLEDGMENT
The authors acknowledge C. Richmond for helpful discussions, particularly on obtaining analytical expressions for density functions. They also acknowledge helpful discussions with
D. W. Tufts and I. S. Reed, which encouraged them to clarify the
significance of arbitrary scaling between test and training data.