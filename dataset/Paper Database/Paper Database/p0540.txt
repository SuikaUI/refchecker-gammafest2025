Learning Classiﬁers from
Only Positive and Unlabeled Data
Charles Elkan
Computer Science and Engineering
University of California, San Diego
La Jolla, CA 92093-0404
 
Keith Noto
Computer Science and Engineering
University of California, San Diego
La Jolla, CA 92093-0404
 
The input to an algorithm that learns a binary classiﬁer
normally consists of two sets of examples, where one set
consists of positive examples of the concept to be learned,
and the other set consists of negative examples. However,
it is often the case that the available training data are an
incomplete set of positive examples, and a set of unlabeled
examples, some of which are positive and some of which are
negative. The problem solved in this paper is how to learn
a standard binary classiﬁer given a nontraditional training
set of this nature.
Under the assumption that the labeled examples are selected randomly from the positive examples, we show that
a classiﬁer trained on positive and unlabeled examples predicts probabilities that diﬀer by only a constant factor from
the true conditional probabilities of being positive. We show
how to use this result in two diﬀerent ways to learn a classi-
ﬁer from a nontraditional training set. We then apply these
two new methods to solve a real-world problem: identifying
protein records that should be included in an incomplete
specialized molecular biology database. Our experiments in
this domain show that models trained using the new methods perform better than the current state-of-the-art biased
SVM method for learning from positive and unlabeled examples.
Categories and Subject Descriptors
H.2.8 [Database management]: Database applications—
data mining.
General Terms
Algorithms, theory.
Supervised learning, unlabeled examples, text mining, bioinformatics.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
KDD’08, August 24–27, 2008, Las Vegas, Nevada, USA.
Copyright 2008 ACM 978-1-60558-193-4/08/08 ...$5.00.
INTRODUCTION
The input to an algorithm that learns a binary classiﬁer
consists normally of two sets of examples. One set is positive
examples x such that the label y = 1, and the other set is
negative examples x such that y = 0. However, suppose the
available input consists of just an incomplete set of positive
examples, and a set of unlabeled examples, some of which
are positive and some of which are negative. The problem
we solve in this paper is how to learn a traditional binary
classiﬁer given a nontraditional training set of this nature.
Learning a classiﬁer from positive and unlabeled data, as
opposed to from positive and negative data, is a problem of
great importance. Most research on training classiﬁers, in
data mining and in machine learning assumes the availability
of explicit negative examples. However, in many real-world
domains, the concept of a negative example is not natural. For example, over 1000 specialized databases exist in
molecular biology . Each of these deﬁnes a set of positive examples, namely the set of genes or proteins included
in the database. In each case, it would be useful to learn a
classiﬁer that can recognize additional genes or proteins that
should be included. But in each case, the database does not
contain any explicit set of examples that should not be included, and it is unnatural to ask a human expert to identify
such a set. Consider the database that we are associated
with, which is called TCDB .
This database contains
information about over 4000 proteins that are involved in
signaling across cellular membranes. If we ask a biologist
for examples of proteins that are not involved in this process, the only answer is “all other proteins.” To make this
answer operational, we could take all proteins mentioned in
a comprehensive unspecialized database such as SwissProt
 . But these proteins are unlabeled examples, not negative
examples, because some of them are proteins that should be
in TCDB. Our goal is precisely to discover these proteins.
This paper is organized as follows. First, Section 2 formalizes the scenario of learning from positive and unlabeled
examples, presents a central result concerning this scenario,
and explains how to use it to make learning from positive and
unlabeled examples essentially equivalent to learning from
positive and negative examples. Next, Section 3 derives how
to use the same central result to assign weights to unlabeled
examples in a principled way, as a second method of learning using unlabeled examples. Then, Section 4 describes a
synthetic example that illustrates the results of Section 2.
Section 5 explains the design and ﬁndings of an experiment
showing that our two new methods perform better than the
best previously suggested method for learning from positive
and unlabeled data. Finally, Section 6 summarizes previous work in the same area, and Section 7 summarizes our
LEARNING A TRADITIONAL
CLASSIFIER FROM
NONTRADITIONAL INPUT
Let x be an example and let y ∈{0, 1} be a binary label.
Let s = 1 if the example x is labeled, and let s = 0 if x is
unlabeled. Only positive examples are labeled, so y = 1 is
certain when s = 1, but when s = 0, then either y = 1 or
y = 0 may be true.
Formally, we view x, y, and s as random variables. There
is some ﬁxed unknown overall distribution p(x, y, s) over
triples ⟨x, y, s⟩.
A nontraditional training set is a sample
drawn from this distribution that consists of unlabeled examples ⟨x, s = 0⟩and labeled examples ⟨x, s = 1⟩.
fact that only positive examples are labeled can be stated
formally as the equation
p(s = 1|x, y = 0) = 0.
In words, the probability that an example x appears in the
labeled set is zero if y = 0.
There is a subtle but important diﬀerence between the scenario considered here, and the scenario considered in .
The scenario here is that the training data are drawn randomly from p(x, y, s), but for each tuple ⟨x, y, s⟩that is
drawn, only ⟨x, s⟩is recorded. The scenario of is that
two training sets are drawn independently from p(x, y, s).
From the ﬁrst set all x such that s = 1 are recorded; these
are called “cases” or “presences.” From the second set all
x are recorded; these are called the background sample, or
contaminated controls, or pseudo-absences.
The single-training-set scenario considered here provides
strictly more information than the case-control scenario. Obviously, both scenarios allow p(x) to be estimated. However,
the ﬁrst scenario also allows the constant p(s = 1) to be estimated in an obvious way, while the case-control scenario
does not. This diﬀerence turns out to be crucial: it is possible to estimate p(y = 1) only in the ﬁrst scenario.
The goal is to learn a function f(x) such that f(x) =
p(y = 1|x) as closely as possible. We call such a function
f a traditional probabilistic classiﬁer.
Without some assumption about which positive examples are labeled, it is
impossible to make progress towards this goal. Our basic
assumption is the same as in previous research: that the
labeled positive examples are chosen completely randomly
from all positive examples. What this means is that if y = 1,
the probability that a positive example is labeled is the same
constant regardless of x. We call this assumption the “selected completely at random” assumption. Stated formally,
the assumption is that
p(s = 1|x, y = 1) = p(s = 1|y = 1).
Here, c = p(s = 1|y = 1) is the constant probability that
a positive example is labeled. This “selected completely at
random”assumption is analogous to the“missing completely
at random” assumption that is often made when learning
from data with missing values .
Another way
of stating the assumption is that s and x are conditionally
independent given y.
So, a training set is a random sample from a distribution
p(x, y, s) that satisﬁes Equations (1) and (2). Such a training set consists of two subsets, called the “labeled” (s = 1)
and “unlabeled” (s = 0) sets. Suppose we provide these two
sets as inputs to a standard training algorithm. This algorithm will yield a function g(x) such that g(x) = p(s = 1|x)
approximately. We call g(x) a nontraditional classiﬁer. Our
central result is the following lemma that shows how to obtain a traditional classiﬁer f(x) from g(x).
Suppose the “selected completely at random”
assumption holds. Then p(y = 1|x) = p(s = 1|x)/c where
c = p(s = 1|y = 1).
Remember that the assumption is p(s = 1|y =
1, x) = p(s = 1|y = 1).
Now consider p(s = 1|x).
p(s = 1|x)
p(y = 1 ∧s = 1|x)
p(y = 1|x)p(s = 1|y = 1, x)
p(y = 1|x)p(s = 1|y = 1).
The result follows by dividing each side by p(s = 1|y = 1).
Although the proof above is simple, the result has not been
published before, and it is not obvious. The reason perhaps
that the result is novel is that although the learning scenario
has been discussed in many previous papers, including , and these papers do make the“selected completely
at random” assumption either explicitly or implicitly, the
scenario has not previously been formalized using a random
variable s to represent the fact of an example being selected.
Several consequences of the lemma are worth noting. First,
f is an increasing function of g. This means that if the classiﬁer f is only used to rank examples x according to the
chance that they belong to class y = 1, then the classiﬁer g
can be used directly instead of f.
Second, f = g/p(s = 1|y = 1) is a well-deﬁned probability
f ≤1 only if g ≤p(s = 1|y = 1). What this says is that
g > p(s = 1|y = 1) is impossible. This is reasonable because
the “positive” (labeled) and “negative” (unlabeled) training
sets for g are samples from overlapping regions in x space.
Hence it is impossible for any example x to belong to the
“positive” class for g with a high degree of certainty.
The value of the constant c = p(s = 1|y = 1) can be
estimated using a trained classiﬁer g and a validation set of
examples. Let V be such a validation set that is drawn from
the overall distribution p(x, y, s) in the same manner as the
nontraditional training set. Let P be the subset of examples
in V that are labeled (and hence positive). The estimator
of p(s = 1|y = 1) is the average value of g(x) for x in P.
Formally the estimator is e1 = 1
x∈P g(x) where n is the
cardinality of P.
We shall show that e1 = p(s = 1|y = 1) = c if it is the
case that g(x) = p(s = 1|x) for all x. To do this, all we need
to show is that g(x) = c for x ∈P. We can show this as
p(s = 1|x)
p(s = 1|x, y = 1)p(y = 1|x)
+ p(s = 1|x, y = 0)p(y = 0|x)
p(s = 1|x, y = 1) · 1 + 0 · 0 since x ∈P
p(s = 1|y = 1).
A second estimator of c is e2 = P
x∈P g(x)/ P
This estimator is almost equivalent to e1, because
g(x)] = p(s = 1) · m = E[n|m]
where m is the cardinality of V .
A third estimator of c is e3 = maxx∈V g(x). This estimator is based on the fact that g(x) ≤c for all x.
Which of these three estimators is best? The ﬁrst estimator is exactly correct if g(x) = p(s = 1|x) precisely for all
x, but of course this condition never holds in practice. We
can have g(x) ̸= p(s = 1|x) for two reasons: because g is
learned from a random ﬁnite training set, and/or because
the family of models from which g(x) is selected does not
include the true model. For example, if the distributions of
x given y = 0 and x given y = 1 are Gaussian with diﬀerent
covariances, as in Section 4 below, then logistic regression
can model p(y = 1|x) and p(s = 1|x) approximately, but not
exactly. In the terminology of statistics, logistic regression
is mis-speciﬁed in this case.
In practice, when g(x) ̸= p(s = 1|x), the ﬁrst estimator is
still the best one to use. Compared to the third estimator,
it will have much lower variance because it is based on averaging over m examples instead of on just one example. It
will have slightly lower variance than the second estimator
because the latter is exposed to additional variance via its
denominator. Note that in principle any single example from
P is suﬃcient to determine c, but that in practice averaging
over all members of P is preferable.
WEIGHTING UNLABELED EXAMPLES
There is an alternative way of using Lemma 1. Let the
goal be to estimate Ep(x,y,s)[h(x, y)] for any function h, where
p(x, y, s) is the overall distribution. To make notation more
concise, write this as E[h]. We want an estimator of E[h]
based on a nontraditional training set of examples of the
form ⟨x, s⟩.
Clearly p(y = 1|x, s = 1) = 1. Less obviously,
p(y = 1|x, s = 0)
p(s = 0|x, y = 1)p(y = 1|x)
p(s = 0|x)
[1 −p(s = 1|x, y = 1)]p(y = 1|x)
1 −p(s = 1|x)
(1 −c)p(y = 1|x)
1 −p(s = 1|x)
(1 −c)p(s = 1|x)/c
1 −p(s = 1|x)
p(s = 1|x)
1 −p(s = 1|x).
By deﬁnition
h(x, y)p(x, y, s)
p(y|x, s)h(x, y)
p(s = 1|x)h(x, 1)
+ p(s = 0|x)[p(y = 1|x, s = 0)h(x, 1)
+ p(y = 0|x, s = 0)h(x, 0)]
The plugin estimate of E[h] is then the empirical average
w(x)h(x, 1) + (1 −w(x))h(x, 0)
w(x) = p(y = 1|x, s = 0) = 1 −c
p(s = 1|x)
1 −p(s = 1|x)
and m is the cardinality of the training set.
says is that each labeled example is treated as a positive
example with unit weight, while each unlabeled example is
treated as a combination of a positive example with weight
p(y = 1|x, s = 0) and a negative example with complementary weight 1−p(y = 1|x, s = 0). The probability p(s = 1|x)
is estimated as g(x) where g is the nontraditional classiﬁer
explained in the previous section.
There are two ways in which the result above on estimating E[h] can be used to modify a learning algorithm in order
to make it work with positive and unlabeled training data.
The ﬁrst method is to express the learning algorithm so that
it uses the training set only via the computation of averages,
and then to use the result above to estimate each of these
averages. The second method is to modify the learning algorithm so that training examples have individual weights.
Then, positive examples are given unit weight and unlabeled
examples are duplicated; one copy of each unlabeled example
is made positive with weight p(y = 1|x, s = 0) and the other
copy is made negative with weight 1 −p(y = 1|x, s = 0).
This second method is the one used in experiments below.
As a special case of the result above, consider h(x, y) = y.
w(x)1 + (1 −w(x))0
where n is the cardinality of the labeled training set, and
the sum is over the unlabeled training set. This result solves
an open problem identiﬁed in , namely how to estimate
p(y = 1) given only the type of nontraditional training set
considered here and the “selected completely at random” assumption.
There is an alternative way to estimate p(y = 1).
c = p(s = 1|y = 1) = p(s = 1 ∧y = 1)
p(y = 1) = p(s = 1 ∧y = 1)
The obvious estimator of p(s = 1 ∧y = 1) is n/m, which
yields the estimator
x∈P g(x) =
for p(y = 1).
Note that both this estimator and (4) are
greater than n/m, as is expected for p(y = 1).
The expectation E[y] = p(y = 1) is the prevalence of
y among all x, both labeled and unlabeled.
The reasoning above shows how to estimate this prevalence assum-
Figure 1: Data points lie in two dimensions. Blue pluses are positive examples, while red circles are negative
examples. The large ellipse is the result of logistic regression trained on all the data. It shows the set of
points x for which p(y|x) = 0.5 is estimated. The small ellipse is the result of logistic regression trained on
positive labeled data versus all other data, then transformed following Lemma 1. The two ellipses represent
similar classiﬁers in practice since they agree closely everywhere that data points of both classes have high
ing the single-training-set scenario. In the case-control scenario, where labeled training examples are obtained separately from unlabeled training examples, it can be proved
that p(y = 1) cannot be identiﬁed .
The arguments of this section and the preceding one are
derivations about probabilities, so they are only applicable
to the outputs of an actual classiﬁer if that classiﬁer produces correct probabilities as its output. A classiﬁer that
produces approximately correct probabilities is called wellcalibrated. Some learning methods, in particular logistic regression, do give well-calibrated classiﬁers, even in the presence of mis-speciﬁcation and ﬁnite training sets. However,
many other methods, in particular naive Bayes, decision
trees, and support vector machines (SVMs), do not. Fortunately, the outputs of these other methods can typically
be postprocessed into calibrated probabilities.
The two most common postprocessing methods for calibration are isotonic regression , and ﬁtting a one-dimensional logistic regression function . We apply the latter
method, which is often called Platt scaling, to SVM classi-
ﬁers in Section 5 below.
AN ILLUSTRATION
To illustrate the method proposed in Section 2 above, we
generate 500 positive data points and 1000 negative data
points, each from a two-dimensional Gaussian as shown in
Figure 1. We then train two classiﬁers: one using all the
data, and one using 20% of the positive data as labeled positive examples, versus all other data as negative examples.
Figure 1 shows the ideal trained classiﬁer as a large ellipse. Each point on this ellipse has predicted probability
0.5 of belonging to the positive class. The transformed nontraditional classiﬁer is the small ellipse; the transformation
following Lemma 1 uses the estimate e1 of p(s = 1|y = 1).
Based on a validation set of just 20 labeled examples, this
estimated value is e1 = 0.1928, which is very close to the
true value 0.2. Although the two ellipses are visually diﬀerent, they correspond closely in the area where both positive
and negative data points have high density, so they represent
similar classiﬁers for this application.
Given a data point ⟨x1, x2⟩, both classiﬁers use the representation ⟨x1, x2, x2
2⟩as input in order to allow the contours p(y = 1|x) = 0.5 to be quadratic sections, as they are
in Figure 1. Expanding the input representation in this way
is similar to using a nonlinear kernel with a support vector machine. Because the product x1x2 is not part of the
input representation, the ellipses are constrained to be axisparallel. Logistic regression with this input representation
is therefore mis-speciﬁed, i.e. not capable of representing exactly the distributions p(y = 1|x) and p(s = 1|x). Analogous
mis-speciﬁcation is likely to occur in real-world domains.
APPLICATION TO REAL-WORLD DATA
One common real-world application of learning from positive and unlabeled data is in document classiﬁcation. Here
we describe experiments that use documents that are records
from the SwissProt database.
We call the set of positive examples P. This set consists
of 2453 records obtained from a specialized database named
TCDB .
The set of unlabeled examples U consists of
4906 records selected randomly from SwissProt excluding its
intersection with TCDB, so U and P are disjoint. Domain
knowledge suggests that perhaps about 10% of the records
in U are actually positive.
This dataset is useful for evaluating methods for learning from positive and unlabeled examples because in previous work we did in fact manually identify the subset of
actual positive examples inside U; call this subset Q. The
procedure used to identify Q, which has 348 members, is
explained in . Let N = U\Q so the cardinality of N is
4558. The three sets of records N, P, and Q are available
at www.cs.ucsd.edu/users/elkan/posonly.
The P and U datasets were obtained separately, and U
is a sample from the whole population, as opposed to P ∪
U. Hence this experiment is an example of the case-control
scenario explained in Section 2, not of the single-training-set
scenario. However, we can still apply the methods suggested
in that section and evaluate their success. When applied in
practice, P ∪U will be all of SwissProt, so the scenario will
be the single-training-set one.
Our experiments compare four approaches: (i) standard
learning from P ∪Q versus N, (ii) learning from P versus U with adjustment of output probabilities, (iii) learning
from P and U after double weighting of U, and (iv) the biased SVM method from explained in Section 6 below.
Approach (i) is the baseline, that is the ideal: learning a
standard classiﬁer from fully labeled positive and negative
training sets.
We expect its accuracy to be the highest;
we hope that approaches (ii) and (iii) can achieve almost as
good accuracy. Approach (iv) is the most successful method
described in previous research. To make comparisons fair,
all four methods are based on soft-margin SVMs with linear
Each of the four methods yields a classiﬁer that assigns
numerical scores to test examples, so for each method we
can plot its receiver operating characteristic (ROC) curve.
Each method also has a natural threshold that can be used
to convert numerical scores into yes/no predictions for test
examples. For methods (i) and (iii) this natural threshold is
0.5, since these methods yield scores that are well-calibrated
probabilities. For method (iv) the natural threshold is zero,
since this method is an SVM. For method (ii) the natural
threshold is 0.5c where c = p(s = 1|y = 1). As in Section 4
above, we use the estimator e1 for c.
For each of the four methods, we do cross-validation with
ten folds, and obtain a combined confusion matrix from the
ten testing subsets.
With one confusion matrix for each
approach, we compute its recall and precision.
numbers are expected to be somewhere around 95%. Crossvalidation proceeds as follows. Partition P, Q, and N randomly into ten subsets each of size as equal as possible. For
example Q will have eight subsets of size 35 and two of size
34. For each of the ten trials, reserve one subset of P, Q,
and N for testing. Use the other nine subsets of each for
training. In every trial, each approach gives one ﬁnal classi-
ﬁer. In trial number i, this classiﬁer is applied to the testing
subsets Pi, Qi, and Ni, yielding a confusion matrix of the
following form:
The combined confusion matrix reported for each approach
has entries a = P10
i=1 ai etc. In this matrix a is the number
of true positives, b is the number of false negatives, c is
the number of false positives, and d is the number of true
negatives. Finally, precision is deﬁned as p = a/(a + c) and
recall as r = a/(a + b).
The basic learning algorithm for each method is an SVM
with a linear kernel as implemented in libSVM . For approach (ii) we use Platt scaling to get probability estimates
which are then adjusted using Lemma 1. For approach (iii)
we run libSVM twice. The ﬁrst run uses Platt scaling to get
probability estimates, which are then converted into weights
following Equation (3) at the end of Section 3. Next, these
weights are used for the second run. Although the oﬃcial
version of libSVM allows examples to be weighted, it requires all examples in one class to have the same weight.
We use the modiﬁed version of libSVM by Ming-Wei Chang
and Hsuan-Tien Lin, available at www.csie.ntu.edu.tw/-
∼cjlin/libsvmtools/#15, that allows diﬀerent weights for
diﬀerent examples.
For approach (iv) we run libSVM many times using varying weights for the negative and unlabeled examples. The
details of this method are summarized in Section 6 below and
are the same as in the paper that proposed the method originally . With this approach diﬀerent weights for diﬀerent
examples are not needed, but a validation set to choose the
best settings is needed. Given that the training set in each
trial consists of 90% of the data, a reasonable choice is to
use 70% of the data for training and 20% for validation in
each trial. After the best settings are chosen in each trial,
libSVM is rerun using all 90% for training, for that trial.
Note that diﬀerent settings may be chosen in each of the
ten trials.
The results of running these experiments are shown in
Table 5. Accuracies and F1 scores are calculated by thresholding trained models as described above, while areas under
the ROC curve do not depend on a speciﬁc threshold. “Relative time” is the number of times an SVM must be trained
for one fold of cross-validation.
As expected, training on P ∪Q versus N, method (i), performs the best, measured both by F1 and by area under the
True positive rate (2801 positive examples)
False positive rate (4558 negative examples)
(i) P union Q versus N
(ii) P versus U
(iii) P versus weighted U
(iv) Biased SVM
Contour lines at F1=0.92,0.93,0.94,0.95
100 false positive examples
Figure 2: Results for the experiment described in Section 5. The four ROC curves are produced by four
diﬀerent methods. (i) Training on P ∪Q versus N; the point on the line uses the threshold p(y = 1|x) = 0.5.
(ii) Training on P versus U; the point shown uses the threshold p(s = 1|x)/e1 = 0.5 where e1 is an estimate
of p(s = 1|y = 1).
(iii) Training on P versus U, where each example in U is labeled positive with a weight
of p(y = 1|x, z = 0) and also labeled negative with complementary weight. (iv) Biased SVM training, with
penalties CU and CP chosen using validation sets. Note that, in order to show the diﬀerences between methods
better, only the important part of the ROC space is shown.
ROC curve. However, this is the ideal method that requires
knowledge of all true labels. Our two methods that use only
positive and unlabeled examples, (ii) and (iii), perform better than the current state-of-the-art, which is method (iv).
Although methods (ii) and (iv) yield almost the same area
under the ROC curve, Figure 2 shows that the ROC curve
for method (ii) is better in the region that is important from
an application perspective. Method (iv) is the slowest by
far because it requires exhaustive search for good algorithm
Since methods (ii) and (iii) are mathematically
well-principled, no search for algorithm settings is needed.
The new methods (ii) and (iii) do require estimating c =
p(s = 1|y = 1). We do this with the e1 estimator described
in Section 2. Because of cross-validation, a diﬀerent estimate
e1 is computed for each fold. All these estimates are within
0.15% of the best estimate we can make using knowledge of
the true labels, which is p(s = 1|y = 1) =
|P ∪Q| = 0.876.
Consider the ROC curves in Figure 2; note that only part
of the ROC space is shown in this ﬁgure. Each of the four
curves is the result of one of the four methods. The points
highlighted on the curves show the false positive/true positive trade-oﬀat the thresholds described above. While the
diﬀerences in the ROC curves may seem small visually, they
represent a substantial practical diﬀerence.
Suppose that
a human expert will tolerate 100 negative records. This is
represented by the black line in Figure 2. Then the expert
will miss 9.6% of positive records using the biased SVM, but
only 7.6% using the reweighting method, which is a 21% reduction in error rate, and a diﬀerence of 55 positive records
in this case.
RELATED WORK
Several dozen papers have been published on the topic of
learning a classiﬁer from only positive and unlabeled training
examples. Two general approaches have been proposed previously. The more common approach is (i) to use heuristics
to identify unlabeled examples that are likely to be negative,
Table 1: Measures of performance for each of four methods.
area under ROC curve
relative time
(i) Ideal: Training on P ∪Q versus N
(ii) Training on P versus U
(iii) Training on P versus weighted U
(iv) Biased SVM
and then (ii) to apply a standard learning method to these
examples and the positive examples; steps (i) and (ii) may
be iterated. Papers using this general approach include , and the idea has been rediscovered independently a
few times, most recently in [22, Section 2.4]. The approach
is sometimes extended to identify also additional positive examples in the unlabeled set . The less common approach
is to assign weights somehow to the unlabeled examples,
and then to train a classiﬁer with the unlabeled examples
interpreted as weighted negative examples. This approach
is used for example by .
The ﬁrst approach can be viewed as a special case of the
second approach, where each weight is either 0 or 1. The
second approach is similar to the method we suggest in Section 3 above, with three important diﬀerences.
view each unlabeled example as being both a weighted negative example and a weighted positive example. Second, we
provide a principled way of choosing weights, unlike previous papers. Third, we assign diﬀerent weights to diﬀerent
unlabeled examples, whereas previous work assigns the same
weight to every unlabeled example.
A good paper that evaluates both traditional approaches,
using soft-margin SVMs as the underlying classiﬁers, is .
The ﬁnding of that paper is that the approach of heuristically identifying likely negative examples is inferior. The
weighting approach that is superior solves the following SVM
optimization problem:
minimize 1
2||w||2 + CP
subject to yi(w · x + b) ≥1 −zi and zi ≥0 for all i.
Here P is the set of labeled positive training examples and U
is the set of unlabeled training examples. For each example i
the hinge loss is zi. In order to make losses on P be penalized
more heavily than losses on U, CP > CU. No direct method
is suggested for setting the constants CP and CU. Instead, a
validation set is used to select empirically the best values of
CP and CU from the ranges CU = 0.01, 0.03, 0.05, . . . , 0.61
and CP /CU = 10, 20, 30, . . . , 200. This method, called biased SVM, is the current state-of-the-art for learning from
only positive and unlabeled documents. Our results in the
previous section show that the two methods we propose are
both superior.
The assumption on which the results of this paper depend
is that the positive examples in the set P are selected completely at random. This assumption was ﬁrst made explicit
by and has been used in several papers since, including
 . Most algorithms based on this assumption need p(y = 1)
to be an additional input piece of information; a recent paper emphasizes the importance of p(y = 1) for learning from
positive and unlabeled data . In Section 3 above, we show
how to estimate p(y = 1) empirically.
The most similar previous work to ours is .
approach also makes the “selected completely at random”
assumption and also learns a classiﬁer directly from the positive and unlabeled sets, then transforms its output following a lemma that they prove. Two important diﬀerences are
that they assume the outputs of classiﬁers are binary as opposed to being estimated probabilities, and they do not suggest a method to estimate p(s = 1|y = 1) or p(y = 1). Hence
the algorithm they propose for practical use uses a validation
set to search for a weighting factor, like the weighting methods of , and like the biased SVM approach . Our
proposed methods are orders of magnitude faster because
correct weighting factors are computed directly.
The task of learning from positive and unlabeled examples
can also be addressed by ignoring the unlabeled examples,
and learning only from the labeled positive examples. Intuitively, this type of approach is inferior because it ignores
useful information that is present in the unlabeled examples. There are two main approaches of this type. The ﬁrst
approach is to do probability density estimation, but this is
well-known to be a very diﬃcult task for high-dimensional
data. The second approach is to use a so-called one-class
SVM . The aim of these methods is to model a region that contains most of the available positive examples.
Unfortunately, the outcome of these methods is sensitive to
the values chosen for tuning parameters, and no good way
is known to set these values . Moreover, the biased SVM
method has been reported to do better experimentally .
CONCLUSIONS
The central contribution of this paper is Lemma 1, which
shows that if positive training examples are labeled at random, then the conditional probabilities produced by a model
trained on the labeled and unlabeled examples diﬀer by only
a constant factor from the conditional probabilities produced
by a model trained on fully labeled positive and negative examples.
Following up on Lemma 1, we show how to use it in two
diﬀerent ways to learn a classiﬁer using only positive and
unlabeled training data. We apply both methods to an important biomedical classiﬁcation task whose purpose is to
ﬁnd new data instances that are relevant to a real-world
molecular biology database. Experimentally, both methods
lead to classiﬁers that are both hundreds of times faster and
more accurate than the current state-of-the-art SVM-based
method. These ﬁndings hold for four diﬀerent deﬁnitions of
accuracy: area under the ROC curve, F1 score or error rate
using natural thresholds for yes/no classiﬁcation, and recall
at a ﬁxed false positive rate that makes sense for a human
expert in the application domain.
ACKNOWLEDGMENTS
This research is funded by NIH grant GM077402. Tingfan
Wu provided valuable advice on using libSVM.