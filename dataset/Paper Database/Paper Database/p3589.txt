Gene Selection With
Guided Regularized Random Forest
Houtao Deng
Intuit, Mountain View, CA, USA
George Runger
Arizona State University, Tempe, AZ, USA
The regularized random forest (RRF) was recently proposed for feature selection by building only one ensemble. In RRF the features are evaluated
on a part of the training data at each tree node. We derive an upper bound
for the number of distinct Gini information gain values in a node, and show
that many features can share the same information gain at a node with a
small number of instances and a large number of features. Therefore, in a
node with a small number of instances, RRF is likely to select a feature not
strongly relevant.
Here an enhanced RRF, referred to as the guided RRF (GRRF), is proposed.
In GRRF, the importance scores from an ordinary random forest
(RF) are used to guide the feature selection process in RRF. Experiments on
10 gene data sets show that the accuracy performance of GRRF is, in general,
more robust than RRF when their parameters change. GRRF is computationally eﬃcient, can select compact feature subsets, and has competitive
accuracy performance, compared to RRF, varSelRF and LASSO logistic regression (with evaluations from an RF classiﬁer). Also, RF applied to the
features selected by RRF with the minimal regularization outperforms RF
applied to all the features for most of the data sets considered here. Therefore, if accuracy is considered more important than the size of the feature
∗This research was partially supported by ONR grant N00014-09-1-0656.
Email addresses: (Houtao Deng), 
(George Runger)
 
November 27, 2024
subset, RRF with the minimal regularization may be considered. We use the
accuracy performance of RF, a strong classiﬁer, to evaluate feature selection
methods, and illustrate that weak classiﬁers are less capable of capturing the
information contained in a feature subset. Both RRF and GRRF were implemented in the “RRF” R package available at CRAN, the oﬃcial R package
classiﬁcation; feature selection; random forest; variable
selection.
1. Introduction
Given a training data set consisting of N instances, P predictor variables/features Xi(i = 1, ..., P) and the class Y ∈{1, 2, ..., C}, the objective
of feature selection is to select a compact variable/feature subset without
loss of predictive information about Y . Note feature selection selects a subset of the original feature set, and, therefore, may be more interpretable than
feature extraction (e.g., principal component analysis and partial least
squares regression ) which creates new features based on transformations
or combinations of the original feature set . Feature selection has been
widely used in many applications such as gene selection as it can
moderate the curse of dimensionality, improve interpretability and avoid
the eﬀort to analyze irrelevant or redundant features.
Information-theoretic measures such as symmetrical uncertainty and mutual information can measure the degree of association between a pair of
variables and have been successfully used for feature selection, e.g., CFS
(correlation-based feature selection) and FCBF (fast correlation-based
ﬁlter) .
However, these measures are limited to two variables and do
not capture high-order interactions between variables well. For example, the
measures can not capture the exclusive OR relationship Y = XOR(X1, X2),
in which neither X1 nor X2 is predictive individually, but X1 and X2 together
can correctly determine Y .
LASSO logistic regression and recursive feature elimination with a
linear SVM (SVM-RFE) are well-known feature selection methods based
on classiﬁers. These methods assume a linear relationship between the log
odds of the class and the predictor variables (LASSO logistic regression) or
between the class and the predictor variables (linear SVM). Furthermore,
before using these methods, one often needs to preprocess the data such as
transforming categorical variables to binary variables or normalizing variables
of diﬀerent scales.
A random forest (RF) classiﬁer has been commonly used for measuring
feature importance . An RF naturally handles numerical and categorical
variables, diﬀerent scales, interactions and nonlinearities, etc. Although the
RF feature importance scores can be used to select K features with the
highest importance scores individually, there could be redundancy among
the K features. Consequently, the selected features subset can diﬀer from
the best combination of K-features, that is, the best feature subset. Similarly,
Boruta , a method based on RF, aims to select a set of relevant features,
which is diﬀerent from the objective of a relevant and also non-redundant
feature subset.
A feature selection method based on RF, varSelRF , has become popular. varSelRF consists of multiple iterations and eliminates the feature(s)
with the least importance score(s) at each iteration. Since eliminating one
feature at each iteration is computationally expensive, the authors considered
eliminating a fraction, e.g., 1/5, of the features at each iteration. However,
when there is a large number of features, many features are eliminated at
each iteration, and, thus, useful features, but with small importance scores,
can be eliminated.
The ACE algorithm is another ensembles-based feature selection
method. It was shown to be eﬀective, but it is more computationally demanding than the simpler approaches considered here. It requires multiple
forests to be constructed, along with multiple gradient boosted trees .
Recently, the regularized random forest (RRF) was proposed for feature
selection with one ensemble , instead of multiple ensembles . However, in RRF the features are evaluated on a part of the training data at each
tree node and the feature selection process may be greedy.
Here we analyze a feature evaluation issue that occurs in all the usual
splitting algorithms at tree nodes with a small number of training instances.
To solve this issue, we propose the guided RRF (GRRF) method, in which the
importance scores from an ordinary RF are used to guide the feature selection
process in RRF. Since the importance scores from an RF are aggregated from
all the trees based on all the training data, GRRF is expected to perform
better than RRF.
Section 2 presents previous work. Section 3 discusses the node sparsity
issue when evaluating features at tree nodes with a small number of training
instances. Section 4 describes the GRRF method. Section 5 presents and
discusses the experimental results. Section 6 concludes this work.
2. Background
2.1. Variable importance scores from Random Forest
A random forest (RF) is a supervised learner that consists of multiple
decision trees, each of which grown on a bootstrap sample from the original
training data. The Gini index at node v, Gini(v), is deﬁned as
c is the proportion of class-c observations at node v. The Gini information gain of Xi for splitting node v, Gain(Xi, v), is the diﬀerence between
the impurity at the node v and the weighted average of impurities at each
child node of v. That is,
Gain(Xi, v) =
Gini(Xi, v) −wLGini(Xi, vL) −wRGini(Xi, vR)
where vL and vR are the left and right child nodes of v, respectively, and wL
and wR are the proportions of instances assigned to the left and right child
nodes. At each node, a random set of mtry features out of P is evaluated,
and the feature with the maximum Gain(Xi, v) is used for splitting the node
The importance score for variable Xi can be calculated as
Gain(Xi, v)
where SXi is the set of nodes split by Xi in the RF with ntree trees. The
RF importance scores are commonly used to evaluate the contributions of
the features regarding predicting the classes.
2.2. Regularized Random Forest
The regularized random forest (RRF) applies the tree regularization
framework to RF and can select a compact feature subset. While RRF is
Algorithm 1: Feature selection at node v.
input : F and λ
1 gain∗←0, count ←0, f∗←−1, f1 ←{1, 2, 3, ..., P }, f2 ←∅
2 while f1 ̸= ∅do
m ←select(f1) //randomly select feature index m from f1
f2 ←{f2, m} //add m to f2
f1 ←f1 −m //remove m from f1
gainR(Xm, v) ←0
if Xm ∈F then
gainR(Xm, v) ←gain(Xm, v) //calculate gainR for all variables in F
if Xm /∈F and count < ⌈
gainR(Xm) ←λ · gain(Xm) //regularize the gain if the variable is not in F
count ←count + 1
if gainR(Xm, v) > gain∗then
gain∗←gainR(Xm, v), f∗←m
17 if f∗̸= −1 and f∗/∈F then
F ←{F, f∗}
20 return F
built in a way similar to RF, the main diﬀerence is that the regularized
information gain, GainR(Xi, v), is used in RRF
GainR(Xi, v) =
λ · Gain(Xi, v)
Gain(Xi, v)
where F is the set of indices of features used for splitting in previous nodes
and is an empty set at the root node in the ﬁrst tree. Here λ ∈(0, 1] is
called the penalty coeﬃcient. When i /∈F, the coeﬃcient penalizes the ith
feature for splitting node v. A smaller λ leads to a larger penalty. RRF uses
GainR(Xi, v) at each node, and adds the index of a new feature to F if the
feature adds enough predictive information to the selected features.
RRF with λ = 1, referred to as RRF(1), has the minimum regularization.
Still, a new feature has to be more informative at a node than the features
already selected to enter the feature subset. The feature subset selected by
RRF(1) is called the least regularized subset, indicating the minimal regularization from RRF.
Figure 1 illustrates the feature selection process. The nodes in the forest
are visited sequentially (from the left to the right and from the top to the
bottom). The indices of three distinct features used for splitting are added
to F in Tree 1, and the index of X5 is added to F in Tree 2. Algorithm 1
shows the feature selection process at each node.
F={1,3,4}+5
Figure 1: The feature selection procedure of RRF. The non-leaf nodes are marked with
the splitting variables. Three distinct features used for splitting are added to F in Tree 1,
and one feature X5 is added to F in Tree 2.
Similar to GainR(·), a penalized information gain was used to suppress
spurious interaction eﬀects in the rules extracted from tree models . The
objective of Friedman and Popescu was diﬀerent from the goal of a compact feature subset here.
Also, the regularization used in Friedman and
Popescu only reduces the redundancy in each path from the root node
to a leaf node, but the features extracted from such tree models can still be
redundant.
3. The Node Sparsity Issue
This section discusses an issue of evaluating features in a tree node with
a small number of instances, referred to as the node sparsity issue here. The
number of instances decreases as the instances are split recursively in a tree,
and, therefore, this issue can commonly occur in tree-based models.
In a tree node, the Gini information gain or other information-theoretic
measures are commonly used to evaluate and compare diﬀerent features.
However, the measures calculated from a node with a small number of instances may not be able to distinguish the features with diﬀerent predictive
information. In the following we establish an upper bound for the number
of distinct values of Gini information gain for binary classiﬁcation.
Let D(f) denote the number of distinct values of a function f (deﬁned
over a speciﬁed range), N denote the number of instances at a non-leaf node
v and N ≥2 (otherwise it cannot be split). For simplicity, in the following
we also assume N is even. The following procedure is similar when N is odd.
Let N1 and N2 be the number of instances of class 1 and class 2, respectively,
at the node. Let L denote the number of instances at the left child node,
and let L1 and L2 denote the number of instances for class 1 and class 2
at the left child node, respectively, with similar notations R, R1 and R2 for
the right child node. Note L ≥1 and R ≥1. The notation L1L2 denotes
the product function where L1 and L2 assume values in the feasible domain
0 ≤L1, L2 ≤L and L1 + L2 = L. Then we have the following lemmas and
Lemma 1. An upper bound of the number of distinct values of L1L2 is ⌈(L+
1)/2⌉. That is, D(L1L2) ≤⌈(L + 1)/2⌉, where ⌈·⌉denotes the ceiling.
Proof. L1 has at most L + 1 values (L1 ∈{0,1,...,L}). Now let l1l2 and m1m2
be two realizations of L1L2, and let l1l2 = m1m2, we have
= (L −m1)m1
1 −Lm1 + m2
⇔L(l1 −m1) + (m1 + l1)(m1 −l1) =
(l1 −m1)(L −l1 −m1)
l1 = m1 or l1 = L −m1
Therefore, we obtain an upper bound for the number of distinct values for
L1L2 as D(L1L2) <= ⌈(L + 1)/2⌉.
Lemma 2. D(L1L2/L) ≤N(N + 2)/4 −1.
Proof. For each L, L1L2/L has at most D(L1L2) distinct values. Because
1 ≤L ≤N −1, an upper bound for D(L1L2/L) is derived as follows
D(L1L2/L) ≤⌈2/2⌉+ ⌈3/2⌉+ ... + ⌈N/2⌉= N(N + 2)/4 −1
Lemma 3. D(L1L2/L + R1R2/R) ≤N(N + 2)/4 −1
Proof. Because L1L2/L and R1R2/R are symmetric, the upper bound is the
same as for one term. That is, D(L1L2/L + R1R2/R) ≤N(N + 2)/4 −1
Theorem 1. For binary classiﬁcation, an upper bound of the number of
distinct information gain values is N(N + 2)/4 −1.
Proof. The information gain for spliting node v is
Gain(v) = Gini(v) −wLGini(vL) −wRGini(vR)
Because Gini(v) is a constant at node v, we only need to consider D(wLGini(vL)+
wRGini(vR)). For two classes
wLGini(vL)
= (L/N)(L1/L · L2/L + L2/L · L1/L)
= 2(L/N)(L1L2)/L2
wLGini(vL) + wRGini(vR)
= 2(L/N)(L1L2)/L2 + 2(R/N)(R1R2)/R2
= (2/N)(L1L2/L) + (2/N)(R1R2/R)
Because N is a constant, we have D(wLGini(vL)+wRGini(vR)) = D(L1L2/L+
R1R2/R). According to Lemma 3, D(L1L2/L + R1R2/R) ≤N(N + 2)/4 −
Note similar conclusions may be applied to other information-theoretic
measures such as the regularized information gain in Equation 2.
Consequently, when N is small, the number of distinct Gini information
gain values is small. For a large number of variables, many could have the
same Gini information gain. For example, at a node with 10 instances there
are at most 29 distinct Gini information gain values for binary classiﬁcation
problems. For 1000 genes with two classes, there are at least 1000-29=971
genes having the same information gain as other genes.
The number of
instances can be even smaller than 10 in a node of an RF or RRF as each
tree is grown completely.
In RRF, a feature with the maximum regularized information gain is
added at a node based on only the instances at that node. When multiple
features have the maximum regularized information gain, one of these features is randomly selected. As discussed above, at a node with only a small
number of instances less relevant, or redundant features may be selected. An
additional metric is useful to help distinguish the features. In the following
section, we introduce the guided regularized random forest that leverages the
importance scores calculated from an RF based on all the training data.
4. Guided Regularized Random Forest
The guided RRF (GRRF) uses the importance scores from a preliminary
RF to guide the feature selection process of RRF. Because the importance
scores from an RF are aggregated from all the trees of the RF based on all the
training data, GRRF may be able to handle the node sparsity issue discussed
in the previous section.
A normalized importance score is deﬁned as
where Impi is the importance score from an RF (deﬁned in Equation 1).
Here 0 ≤Imp′
i ≤1. Instead of assigning the same penalty coeﬃcient to all
features in RRF, GRRF assigns a penalty coeﬃcient to each feature. That
GainR(Xi, v) =
λiGain(Xi, v)
Gain(Xi, v)
where λi ∈(0, 1] is the coeﬃcient for Xi (i ∈{1, ..., P}) and is calculated
based on the importance score of Xi from an ordinary RF. That is,
λi = (1 −γ)λ0 + γImp′
where λ0 ∈(0, 1] controls the degree of regularization and is called the base
coeﬃcient, and γ ∈ controls the weight of the normalized importance
score and is called the importance coeﬃcient. Note that RRF is a special case
of GRRF with γ = 0. Given λ0 and γ, a feature with a larger importance
score has a larger λi, and, therefore, is penalized less.
In our experiments we found the feature subset size can be eﬀectively
controlled by changing either λ0 or γ, but changing the latter often leads
to better performance in terms of classiﬁcation accuracy.
To reduce the
number of parameters of GRRF, we ﬁx λ0 to be 1 and consider γ as the only
parameter for GRRF. With λ0 = 1, we have
λi = (1 −γ) + γImp′
i = 1 −γ(1 −Imp′
For a feature Xi that does not have the maximum importance score (Imp′
1), a larger γ leads to a smaller λi and, thus, a larger penalty on Gain(Xi, v)
when Xi has not been used in the nodes prior to node v. Consequently, γ is
essentially the degree of regularization. Furthermore, GRRF with γ = 0 is
equivalent to RRF(1), with the minimal regularization.
5. Experiments
We implemented the RRF and GRRF algorithms in the “RRF” R package
based on the “randomForest” package . The “RRF” package is available
from CRAN ( the oﬃcial R package archive.
Work by showed that the accuracy performance of RF is largely
independent of the number of trees (between 1000 and 40000 trees), and
P is often a reasonable choice. Therefore, we used ntree = 1000
and mtry =
P for the classiﬁer RF and the feature selection methods
RRF, GRRF and varSelRF. Guided by initial experiments, randomly sampling 63% of the data instances (the same as the default setting in the “randomForest” package) without replacement was used in RRF and GRRF. We
also evaluated two well-known feature selection methods: varSelRF available
in the “varSelRF” R package and LASSO logistic regression available in the
“glmnet” R package. Unless otherwise speciﬁed, the default values in the
packages were used. Also note λ is the parameter of RRF, and as discussed,
γ is considered as the only parameter of GRRF here.
Table 1: The number of groups identiﬁed, and the number of irrelevant or redundant
features selected for diﬀerent algorithms.
# Irrelevant or Redundant Features
5.1. Simulated Data Sets
We start with a simulated data set generated by the following procedure. First generate 10 independent variables: X1,..., X10, each is uniformly
distributed in the interval . Variable Y is calculated by the formula
Y = 10 sin(πX1X2) + 20(X3 −0.5)2 + 10X4 + 5X5 + e
where e follows a standard normal distribution. Note the above data generation procedure was described in . We simulated 1000 instances and
then calculated the median of Y as y. We then labeled class 2 to the instances with Y > y, and class 1 otherwise, so that it becomes a classiﬁcation problem. Furthermore, we added ﬁve other variables with X11=X1,
X12=X2, X13=X3, X14=X4, X15=X5. Consequently, the feature selection
solution is {(X1|X11) & (X2|X12) & (X3|X13) & (X4|X14) & (X5|X15)},
Table 2: Summary of the data sets.
# Examples
# Features
adenocarcinoma
breast.2.class
breast.3.class
where & stands for “and” and (X1|X11) indicates that one and only one
from this group should be selected. For example, for the data set considered
here, both (X1, X2, X3, X4, X5) and (X11, X2, X13, X4, X5) are correct solutions. However, (X1, X3, X4, X5) misses the group (X2|X12). Furthermore,
(X1, X2, X3, X4, X5, X11) has a redundant variable because X11 is redundant
We simulated 20 replicates of the above data set, and then applied GRRF,
RRF and two well-known methods varSelRF and LASSO logistic regression
to the data sets. Here γ of GRRF was selected from {0.4,0.5,0.6}, λ of RRF
was selected from {0.6,0.7,0.8}, and the regularization parameter of LASSO
logistic regression was selected from {0.01,0.02,0.03,0.05,0.1,0.2}, all by 10fold CV.
The results are shown in Table 1. LASSO logistic regression identiﬁes the
least number of groups on average (4). The methods varSelRF, RRF and
GRRF identify almost all the groups, but varSelRF and RRF select 4.95,
on average, irrelevant or redundant variables, and GRRF only selects 0.75
irrelevant or redundant variables. This experiment shows GRRF’s potential
in selecting a relevant and non-redundant feature subset.
5.2. Gene Data Sets
The 10 gene expression data sets analyzed by Uriarte and de Andres 
are considered in this section. The data sets are summarized in Table 2. For
each data set, a feature selection algorithm was applied to 2/3 of the instances
(selected randomly) to select a feature subset. Then a classiﬁer was applied
to the feature subset. The error rate is obtained by applying the classiﬁer to
the other 1/3 of the instances. This procedure was conducted 100 times with
diﬀerent random seeds, and the average size of feature subsets and the average
error rate over 100 runs were calculated. In the experiments, we considered
Table 3: The number of features selected in the least regularized subset selected by RRF
(i.e., RRF(1)), the total number of original features (“All”), and the error rates of RF
applied to the least regularized subset and all the features. The win-lose-tie results of
“All” compared to RRF(1) are shown. Here “◦” or “•” represents a signiﬁcant diﬀerence
at the 0.05 level, according to the paired t-test. RRF(1) uses many fewer features than
the original features, and wins on 7 data sets. There are signiﬁcant diﬀerences for four
data sets.
Number of features
Average error rates
adenocarcinoma
breast.2.class
breast.3.class
win-lose-tie
Table 4: The total number of original features (“All”) and the average number of features
(from 100 replicates for each data set) selected by diﬀerent methods. All feature selection
methods are able to select a small number of features. Here GRRF(0.1) and RRF(0.9)
select a similar number of features, but it is shown later that GRRF(0.1) is more accurate
than RRF(0.9) (with an RF classiﬁer) for most data sets.
adenocarcinoma
breast.2.class
breast.3.class
RRF, GRRF, varSelRF and LASSO logistic regression as the feature selection
algorithms, and random forest (RF) and C4.5 as the classiﬁers.
5.2.1. Feature selection and classiﬁcation
First compare the accuracy of RF applied to all the features, denoted as
“All”, and the least regularized subset (i.e., features selected by RRF(1)).
The number of features and the error rates are shown in Table 3. The winlose-tie results of “All” compared to RRF(1) are also shown in the tables. To
investigate the statistical signiﬁcance of these results, we applied the paired
Table 5: The error rates of RF applied to the feature subsets selected by GRRF(0.1),
GRRF(0.2), RRF(0.9), varSelRF and LASSO logistic regression, respectively (to three
decimal places). The win-lose-tie results of each competitor compared to GRRF(0.1) with
RF are shown. Here “◦” or “•” represents a signiﬁcant diﬀerence between a method and
GRRF(0.1) with RF at the 0.05 level, according to the paired t-test. Here GRRF(0.1)
leads to competitive accuracy performance, compared to GRRF(0.2), RRF(0.9), LASSO
and varSelRF.
adenocarcinoma
breast.2.class
breast.3.class
win-lose-tie
Table 6: The error rates of C4.5 applied to the feature subsets selected by GRRF(0.1),
GRRF(0.2), RRF(0.9), varSelRF and LASSO logistic regression, respectively. The winlose-tie results of each competitor compared to GRRF(0.1) with C4.5 are calculated. Here
“◦” or “•” represents a signiﬁcant diﬀerence between a method and GRRF(0.1) with
C4.5 at the 0.05 level, according to the paired t-test. The GRRF methods and the other
methods have perform similarly in terms of the accuracy with C4.5. As expected, C4.5
has noticeably higher error rates than RF, shown in Table 5.
adenocarcinoma
breast.2.class
breast.3.class
win-lose-tie
t-test to the error rates of the two methods from 100 replicates for each data
set. The data sets with a signiﬁcant diﬀerence at the 0.05 level are marked
with “◦” or “•” in the table.
The least regularized subset not only has many fewer features than “All”,
but also leads to better accuracy performance on 7 data sets out of 10, and
the error rates are signiﬁcantly diﬀerent on 4 data sets. Therefore, RRF(1)
not only improves interpretability by reducing the number of features, but
also can improve the accuracy performance of classiﬁcation, even for RF, considered as a strong classiﬁer capable of handling irrelevant and relevant variables . It should also be noted that although the least regularized subset
is much smaller than the original feature set, the size may still be considered
large in some cases, e.g., more than 200 features for the breast.2.class data
set. GRRF and RRF with larger regularization, investigated in the following
experiments, are able to further reduce the number of features.
Next compare GRRF to RRF and two well-known methods: varSelRF
and LASSO logistic regression. The regularization parameter of LASSO logistic regression was selected from {0.01, 0.02, 0.03, 0.05, 0.1, 0.2} by 10-fold
CV. Here γ ∈{0.1, 0.2} was used for GRRF (i.e., GRRF(0.1) or GRRF(0.2)),
and λ = 0.9 was used for RRF (i.e., RRF(0.9)). We used a ﬁxed parameter
setting for GRRF or RRF, as the parameter sensitivity analysis in the following section shows a consistent trend that GRRF or RRF tends to select more
features and also tends to be more accurate, for a smaller γ or a larger λ. We
chose these parameters so that a reasonably small number of features can be
selected. One can also use cross-validation error to determine an appropriate
parameter value customized for each data set and potentially improve these
The total number of original features and the average number of features
selected by each feature selection method are shown in Table 4.
feature selection methods are able to select a small number of features.
The average error rates of RF applied to all the features (“All”) and the
subsets selected by diﬀerent feature selection methods are shown in Table
5. GRRF(0.1) with RF outperforms GRRF(0.2) with RF on 9 data sets out
of 10, 5 of which have signiﬁcant diﬀerences at the 0.05 level. Even though
GRRF(0.1) selects more features than GRRF(0.2), the sizes of the feature
subsets are reasonably small (all less than 80 features).
GRRF(0.1) and RRF(0.9) select a similar number of features. However,
GRRF(0.1) with RF outperforms RRF(0.9) with RF on 9 data sets, 5 of
which have signiﬁcant diﬀerences at the 0.05 level.
Consequently, GRRF
selects stronger features than RRF, which is consistent with the simulated
experiments. According to the discussion in Section 3, a feature with the
maximum Gini information gain in a node with a small number instances
may not be truly strong. Yet RRF adds this feature to the subset.
GRRF(0.1) with RF outperforms varSelRF with RF on 7 data sets, 6
of which have signiﬁcant diﬀerences. Therefore, GRRF(0.1) may be more
favorable than varSelRF for the data sets considered here. Also, as shown in
the following section, GRRF has a clear advantage over varSelRF in terms of
computational time. Furthermore, GRRF(0.1) with RF outperforms LASSO
logistic regression with RF on 7 data sets, 3 of which have signiﬁcant differences. The accuracy performance may be improved by applying a logistic
regression model to the features selected by LASSO logistic regression. However, tree models like GRRF have a few desirable properties compared to
LASSO logistic regression: they can naturally handle mixed categorical and
numerical features, and multiple classes, etc.
The average error rates of C4.5 applied to all the features (“All”) and the
subsets selected by diﬀerent feature selection methods are shown in Table 6.
It can be seen that the error rates of C4.5 are clearly higher than RF shown
in Table 5.
Indeed, RF has been considered as a stronger classiﬁer than
C4.5. Interestingly, the diﬀerences between the methods in terms of C4.5
are smaller than the RF results. As mentioned by , a relatively weaker
classiﬁer is less capable of capturing information from data than a stronger
classiﬁer. Consequently, a feature subset that includes strong features, but
misses the features of small contributions may not aﬀect the accuracy of
C4.5 much, but can aﬀect the accuracy of RF. A weak classiﬁer should only
be used for evaluating a feature subset if that classiﬁer is actually used for
classiﬁcation after feature selection. However, if a strong classiﬁer is used
for classiﬁcation after feature selection, or the objective is to evaluate the
information contained in the feature subset, a strong classiﬁer should be
considered .
5.2.2. Parameter Sensitivity and Computational Time
We investigated the performance of RRF and GRRF with diﬀerent parameter settings: λ ∈{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1} for RRF
and γ ∈{0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0} for GRRF. The parameter
values are arranged by the degree of regularization in a decreasing order for
both methods.
The sizes of the feature subsets, averaged over 100 replicates, for each
base coefficient of RRF
# features
adenocarcinoma
breast.2.class
breast.3.class
importance coefficient of GRRF
# features
adenocarcinoma
breast.2.class
breast.3.class
base coefficient of RRF
error rate
adenocarcinoma
breast.2.class
breast.3.class
importance coefficient of GRRF
error rate
adenocarcinoma
breast.2.class
breast.3.class
Figure 2: The performance of RRF for selected values of λ and GRRF for selected values
of γ. Figure 2(a) shows the number of features selected by RRF. A smaller λ leads to
fewer features. Figure 2(b) shows the number of features selected by GRRF. A larger γ
leads to fewer features. Figure 2(c) shows the error rates of RF applied to the feature
subsets selected by RRF for diﬀerent λ. The error rates tend to decrease as λ increases.
Figure 2(d) shows the error rates of RF applied to the feature subsets selected by GRRF
for diﬀerent γ. The error rates tend to decrease as γ decreases, but are reasonably robust.
Time(sec.)
adenocarcinoma
breast.2.class
breast.3.class
Figure 3: Computational time of RF, GRRF and varSelRF. GRRF is more computationally eﬃcient than varSelRF. GRRF takes about twice as much as RF as it builds two
ensembles.
parameter setting of RRF and GRRF, are shown in Figures 2(a) and 2(b),
respectively.
The number of features tends to increase as λ increases for
RRF, or as γ decreases for GRRF. The consistent trend illustrates that one
can control the size of the feature subset by adjusting the parameters.
The error rates, averaged over 100 replicates, for each parameter setting of
RRF and GRRF are shown in Figures 2(c) and 2(d), respectively. In general,
the error rates tend to decrease as λ increases for RRF, or as γ decreases for
GRRF. However, for most data sets, the error rates of GRRF seem to be
reasonably robust to the changes of γ. As mentioned, RRF(1) and GRRF(0)
are equivalent, and, therefore, they have similar number of features and error
rates for every data set (diﬀering only by random selections).
The computational time of the RF-based methods: RF, GRRF and varSelRF
is shown in Figure 3. As expected, RF was fast for training on these data
sets. GRRF builds two ensemble models, and, thus, the computational time
of GRRF is only about twice as much as RF. However, varSelRF needs to
build multiple ensemble models, and the computational advantage of GRRF
over varSelRF is clear.
6. Conclusions
We derive an upper bound for the number of distinct Gini information
gain values in a tree node for binary classiﬁcation problems.
bound indicates that the Gini information gain may not be able to distinguish
features at nodes with a small number of instances, which poses a challenge
for RRF that selects a feature only using the instances at a node. Motivated
by this node sparsity issue, we propose an enhanced method called the guided,
regularized random forest (GRRF), in which a preliminary random forest is
used to generate the initial variable importance scores to guide the regularized
feature selection process of RRF. The importance scores from the preliminary
RF help GRRF select better features when many features share the same
maximal Gini information gain at a node. For the experiments here, GRRF
is more favorable than RRF, computationally eﬃcient, selects a small set of
features, and has competitive accuracy performance.
Feature selection eliminates irrelevant or redundant features, but also may
eliminate features of small importance. This may not aﬀect the performance
of a weak classiﬁer which is less capable of capturing small information, but
may aﬀect the performance of a strong classiﬁer such as RF . Still, we
found that the least regularized subset selected by RRF with the minimal
regularization produces better accuracy performance than the complete feature set.
Finally we note that although RRF and GRRF can be used as classiﬁers,
they are designed for feature selection. The trees in RRF and GRRF are not
built independently as the features selected in previous trees have an impact
on the trees built later. Therefore, as a classiﬁer, RRF or GRRF may have
a higher variance than RF because the trees are correlated. Consequently,
in this work we applied RF on the feature subset selected by GRRF or RRF
for classiﬁcation.