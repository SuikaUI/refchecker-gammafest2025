Data Min Knowl Disc 15:107–144
DOI 10.1007/s10618-007-0064-z
Experiencing SAX: a novel symbolic representation
of time series
Jessica Lin · Eamonn Keogh · Li Wei ·
Stefano Lonardi
Received: 15 June 2006 / Accepted: 10 January 2007 / Published online: 3 April 2007
Springer Science+Business Media, LLC 2007
Many high level representations of time series have been proposed
for data mining, including Fourier transforms, wavelets, eigenwaves, piecewise
polynomial models, etc. Many researchers have also considered symbolic representations of time series, noting that such representations would potentiality
allow researchers to avail of the wealth of data structures and algorithms from
the text processing and bioinformatics communities. While many symbolic representations of time series have been introduced over the past decades, they all
suffer from two fatal ﬂaws. First, the dimensionality of the symbolic representation is the same as the original data, and virtually all data mining algorithms
scale poorly with dimensionality. Second, although distance measures can be
deﬁned on the symbolic approaches, these distance measures have little correlation with distance measures deﬁned on the original time series.
In this work we formulate a new symbolic representation of time series. Our
representation is unique in that it allows dimensionality/numerosity reduction,
Responsible editor: Johannes Gehrke.
J. Lin (B)
Information and Software Engineering Department, George Mason University, Fairfax,
VA 22030, USA
e-mail: 
E. Keogh · L. Wei · S. Lonardi
Computer Science & Engineering Department, University of California-Riverside, Riverside,
CA 92521, USA
e-mail: 
e-mail: 
S. Lonardi
e-mail: 
J. Lin et al.
and it also allows distance measures to be deﬁned on the symbolic approach that
lower bound corresponding distance measures deﬁned on the original series.
As we shall demonstrate, this latter feature is particularly exciting because it
allows one to run certain data mining algorithms on the efﬁciently manipulated
symbolic representation, while producing identical results to the algorithms
that operate on the original data. In particular, we will demonstrate the utility
of our representation on various data mining tasks of clustering, classiﬁcation,
query by content, anomaly detection, motif discovery, and visualization.
Time series · Data mining · Symbolic representation · Discretize
1 Introduction
Many high level representations of time series have been proposed for data mining. Figure 1 illustrates a hierarchy of all the various time series representations
in the literature .
One representation that the data mining community has not considered in detail
is the discretization of the original data into symbolic strings. At ﬁrst glance this
seems a surprising oversight. There is an enormous wealth of existing algorithms
and data structures that allow the efﬁcient manipulations of strings. Such algorithms have received decades of attention in the text retrieval community, and
more recent attention from the bioinformatics community . Some simple examples of “tools” that are
not deﬁned for real-valued sequences but are deﬁned for symbolic approaches
include hashing, Markov models, sufﬁx trees, decision trees, etc.
There is, however, a simple explanation for the data mining community’s
lack of interest in string manipulation as a supporting technique for mining
time series. If the data are transformed into virtually any of the other representations depicted in Fig. 1, then it is possible to measure the similarity of two
time series in that representation space, such that the distance is guaranteed to
lower bound the true distance between the time series in the original space.1
This simple fact is at the core of almost all algorithms in time series data mining
and indexing . However, in spite of the fact that there are
dozens of techniques for producing different variants of the symbolic representation ,
there is no known method for calculating the distance in the symbolic space,
while providing the lower bounding guarantee.
In addition to allowing the creation of lower bounding distance measures,
there is one other highly desirable property of any time series representation,
The exceptions are random mappings, which are only guaranteed to be within an epsilon of the
true distance with a certain probability, trees, interpolation and natural language.
Experiencing SAX: a novel symbolic representationof time series
Time Series Representations
Data Adaptive
Non Data Adaptive
Approximation
Polynomial
Decompos ition
Piecewise Linear
Approximation
Adaptive Piecewise
Approximation
Daubechies
Sorted Coefficients
Orthonormal
Bi-Orthonormal
Interpolatio n
Regression
Non- Lower
A hierarchy of all the various time series representations in the literature. The leaf nodes
refer to the actual representation, and the internal nodes refer to the classiﬁcation of the approach.
The contribution of this paper is to introduce a new representation, the lower bounding symbolic
including a symbolic one. Almost all time series datasets are very high dimensional. This is a challenging fact because all non-trivial data mining and indexing
algorithms degrade exponentially with dimensionality. For example, above 16–
20 dimensions, index structures degrade to sequential scanning . None of the symbolic representations that we are aware of allow
dimensionality reduction . There is some reduction in the storage space required,
since fewer bits are required for each value; however, the intrinsic dimensionality of the symbolic representation is the same as the original data.
There is no doubt that a new symbolic representation that remedies all
the problems mentioned above would be highly desirable. More speciﬁcally,
the symbolic representation should meet the following criteria: space efﬁciency, time efﬁciency (fast indexing), and correctness of answer sets (no false
dismissals).
In this work we formally formulate a novel symbolic representation and
show its utility on other time series tasks.2 Our representation is unique in
that it allows dimensionality/numerosity reduction, and it also allows distance
measures to be deﬁned on the symbolic representation that lower bound corresponding popular distance measures deﬁned on the original data. As we shall
demonstrate, the latter feature is particularly exciting because it allows one
to run certain data mining algorithms on the efﬁciently manipulated symbolic
representation, while producing identical results to the algorithms that operate
on the original data. In particular, we will demonstrate the utility of our representation on the classic data mining tasks of clustering ,
classiﬁcation , indexing , and anomaly detection .
The rest of this paper is organized as follows. Section 2 brieﬂy discusses
background material on time series data mining and related work. Section 3
introduces our novel symbolic approach, and discusses its dimensionality reduction, numerosity reduction and lower bounding abilities. Section 4 contains an
experimental evaluation of the symbolic approach on a variety of data mining
A preliminary version of this paper appears in Lin et al. .
J. Lin et al.
tasks. Impact of the symbolic approach is also discussed. Finally, Section 5 offers
some conclusions and suggestions for future work.
2 Background and Related Work
Time series data mining has attracted enormous attention in the last decade.
The review below is necessarily brief; we refer interested readers to for a more in depth review.
2.1 Time series data mining tasks
While making no pretence to be exhaustive, the following list summarizes the
areas that have seen the majority of research interest in time series data mining.
Indexing: Given a query time series Q, and some similarity/dissimilarity measure D(Q,C), ﬁnd the most similar time series in database DB .
Clustering: Find natural groupings of the time series in database DB under
some similarity/dissimilarity measure D(Q,C) .
Classiﬁcation: Given an unlabeled time series Q, assign it to one of two or
more predeﬁned classes .
Summarization: Given a time series Q containing n datapoints where n is
an extremely large number, create a (possibly graphic) approximation of Q
which retains its essential features but ﬁts on a single page, computer screen,
executive summary etc .
Anomaly detection: Given a time series Q, and some model of “normal”
“surprising/interesting/unexpected/novel” behavior .
Since the datasets encountered by data miners typically don’t ﬁt in main memory, and disk I/O tends to be the bottleneck for any data mining task, a simple
generic framework for time series data mining has emerged . The basic approach is outlined in Table 1.
It should be clear that the utility of this framework depends heavily on the
quality of the approximation created in Step 1. If the approximation is very
faithful to the original data, then the solution obtained in main memory is likely
to be the same as, or very close to, the solution we would have obtained on the
original data. The handful of disk accesses made in Step 3 to conﬁrm or slightly
modify the solution will be inconsequential compared to the number of disk
accesses required had we worked on the original data. With this in mind, there
has been great interest in approximate representations of time series, which we
consider below.
Experiencing SAX: a novel symbolic representationof time series
A generic time series
data mining approach
Create an approximation of the data, which
will ﬁt inmainmemory,yet retainsthe essential features of interest.
Approximately solve the task at hand in
main memory.
Make (hopefully very few) accesses to the
original data on disk to conﬁrm the solution
obtained in Step 2, or to modify the solution
so it agrees with the solution we would have
obtained on the original data.
Discrete Fourier
Piecewise Linear
Approximation
Haar Wavelet
Adaptive Piecewise
Constant Approximation
The most common representations for time series data mining. Each can be visualized as an
attempt to approximate the signal with a linear combination of basis functions
2.2 Time series representations
As with most problems in computer science, the suitable choice of representation greatly affects the ease and efﬁciency of time series data mining. With this
in mind, a great number of time series representations have been introduced,
including the Discrete Fourier Transform (DFT) , the
Discrete Wavelet Transform (DWT) , Piecewise Linear,
and Piecewise Constant models (PAA) , (APCA) , and Singular Value Decomposition (SVD) . Figure 2 illustrates the most commonly used representations.
Recent work suggests that there is little to choose between the above in
terms of indexing power ; however, the representations have other features that may act as strengths or weaknesses. As a simple
example, wavelets have the useful multiresolution property, but are only deﬁned
for time series that are an integer power of two in length .
One important feature of all the above representations is that they are real
valued. This limits the algorithms, data structures and deﬁnitions available for
them. For example, in anomaly detection we cannot meaningfully deﬁne the
probability of observing any particular set of wavelet coefﬁcients, since the
probability of observing any real number is zero . Such
limitations have lead researchers to consider using a symbolic representation
of time series.
J. Lin et al.
While there are literally hundreds of papers on discretizing (symbolizing,
tokenizing, quantizing) time series for an extensive survey), none of the techniques allows a distance measure that lower bounds a distance measure deﬁned
on the original time series. For this reason, the generic time series data mining
approach illustrated in Table 1 is of little utility, since the approximate solution
to problem created in main memory may be arbitrarily dissimilar to the true
solution that would have been obtained on the original data. If, however, one
had a symbolic approach that allowed lower bounding of the true distance, one
could take advantage of the generic time series data mining model, and of a
host of other algorithms, deﬁnitions and data structures which are only deﬁned
for discrete data, including hashing, Markov models, and sufﬁx trees. This is
exactly the contribution of this paper. We call our symbolic representation of
time series SAX (Symbolic Aggregate approXimation), and deﬁne it in the next
3 SAX: our symbolic approach
SAX allows a time series of arbitrary length n to be reduced to a string of arbitrary length w, (w < n, typically w ≪n). The alphabet size is also an arbitrary
integer a, where a > 2. Table 2 summarizes the major notation used in this and
subsequent sections.
Our discretization procedure is unique in that it uses an intermediate representation between the raw time series and the symbolic strings. We ﬁrst
transform the data into the Piecewise Aggregate Approximation (PAA) representation and then symbolize the PAA representation into a discrete string.
There are two important advantages to doing this:
Dimensionality reduction: We can use the well-deﬁned and well-documented
dimensionality reduction power of PAA , and the reduction is automatically carried over to the symbolic
representation.
Lower Bounding: Proving that a distance measure between two symbolic
strings lower bounds the true distance between the original time series is
non-trivial. The key observation that allowed us to prove lower bounds is to
concentrate on proving that the symbolic distance measure lower bounds the
PAA distance measure. Then we can prove the desired result by transitivity
A summarization of the notation used in this paper
A time series C = c1, . . ., cn
A Piecewise Aggregate Approximation of a time series ¯C = ¯c1, . . . , ¯cw
A symbol representation of a time series ˆC = ˆc1, . . . , ˆcw
The number of PAA segments representing time series C
Alphabet size (e.g., for the alphabet = {a, b, c}, a = 3)
Experiencing SAX: a novel symbolic representationof time series
by simply pointing to the existing proofs for the PAA representation itself
 .
We will brieﬂy review the PAA technique before considering the symbolic
extension.
3.1 Dimensionality reduction via PAA
A time series C of length n can be represented in a w−dimensional space by
a vector ¯C = ¯c1, . . . , ¯cw. The ith element of ¯C is calculated by the following
Simply stated, to reduce the time series from n dimensions to w dimensions, the data is divided into w equal sized “frames”. The mean value of the
data falling within a frame is calculated and a vector of these values becomes
the data-reduced representation. The representation can be visualized as an
attempt to approximate the original time series with a linear combination of
box basis functions as shown in Fig. 3. For simplicity and clarity, we assume that
n is divisible by w. We will relax this assumption in Sect. 3.5.
The PAA dimensionality reduction is intuitive and simple, yet has been
shown to rival more sophisticated dimensionality reduction techniques like
Fourier transforms and wavelets .
The PAA representation can be visualized as an attempt to model a time series with a
linear combination of box basis functions. In this case, a sequence of length 128 is reduced to eight
dimensions
J. Lin et al.
We normalize each time series to have mean of zero and a standard deviation
of one before converting it to the PAA representation, since it is well understood that it is meaningless to compare time series with different offsets and
amplitudes .
3.2 Discretization
Having transformed a time series database into the PAA we can apply a further transformation to obtain a discrete representation. It is desirable to have a
discretization technique that will produce symbols with equiprobability . This is easily achieved since normalized time series have a Gaussian distribution. To illustrate this, we extracted
subsequences of length 128 from 8 different time series and plotted normal
probability plots of the data as shown in Fig. 4. A normal probability plot is a
graphical technique that shows if the data is approximately normally distributed (NIST/SEMATECH): an approximately straight line indicates that the
data is approximately normally distributed. As the ﬁgure shows, the highly linear nature of the plots suggests that the data is approximately normal. For a
large family of the time series data in our disposal, we notice that the Gaussian
assumption is indeed true. For the small subset of data where the assumption
is not obeyed, the efﬁciency is slightly deteriorated; however, the correctness
of the algorithm is unaffected. The correctness of the algorithm is guaranteed
by the lower-bounding property of the distance measure in the symbolic space,
which we will explain in the next section.
Given that the normalized time series have highly Gaussian distribution, we
can simply determine the “breakpoints” that will produce a equal-sized areas
under Gaussian curve .
Deﬁnition 1 Breakpoints: breakpoints are a sorted list of numbers B = β1, . . . ,
βa−1 such that the area under a N(0, 1) Gaussian curve from βi to βi+1 = 1/a
(β0 and βa are deﬁned as −∞and ∞, respectively).
These breakpoints may be determined by looking them up in a statistical
table. For example, Table 3 gives the breakpoints for values of a from 3 to 10.
Once the breakpoints have been obtained we can discretize a time series in
the following manner. We ﬁrst obtain a PAA of the time series. All PAA coefﬁcients that are below the smallest breakpoint are mapped to the symbol “a”,
all coefﬁcients greater than or equal to the smallest breakpoint and less than
the second smallest breakpoint are mapped to the symbol “b”, etc. Figure 5
illustrates the idea.
Note that in this example the three symbols, “a”, “b” and “c” are approximately equiprobable as we desired. We call the concatenation of symbols that
represent a subsequence a word.
Deﬁnition 2 Word: A subsequence C of length n can be represented as a word
ˆC = ˆc1, . . . , ˆcw as follows. Let alpha i denote the ith element of the alphabet,
Experiencing SAX: a novel symbolic representationof time series
A normal probability plot of the distribution of values from subsequences of length 128
from eight different datasets. The highly linear nature of the plot strongly suggests that the data
came from a Gaussian distribution
A lookup table that contains the breakpoints that divide a Gaussian distribution in an
arbitrary number (from 3 to 10) of equiprobable regions
i.e., alpha1 = a and alpha2 = b. Then the mapping from a PAA approximation
¯C to a word ˆC is obtained as follows:
βj−1 ≤¯ci < βj
We have now deﬁned our symbolic representation (the PAA representation is
merely an intermediate step required to obtain the symbolic representation).
Recently, Bagnall and Janakec has empirically and theoretically shown
some very promising clustering results for “clipping”, that is to say, converting
J. Lin et al.
A time series is discretized by ﬁrst obtaining a PAA approximation and then using predetermined breakpoints to map the PAA coefﬁcients into SAX symbols. In the example above, with
n = 128, w = 8 and a = 3, the time series is mapped to the word baabccbc
the time series to a binary vector. They demonstrated that discretizing the time
series before clustering signiﬁcantly improves the accuracy in the presence of
outliers. We note that “clipping” is actually a special case of SAX, where a = 2.
3.3 Distance measures
Having introduced the new representation of time series, we can now deﬁne
a distance measure on it. By far the most common distance measure for time
series is the Euclidean distance .
Given two time series Q and C of the same length n, Eq. 3 deﬁnes their Euclidean distance, and Fig. 6A illustrates a visual intuition of the measure.
D (Q, C) ≡
If we transform the original subsequences into PAA representations, ¯Q and
¯C, using Eq. 1, we can then obtain a lower bounding approximation of the
Euclidean distance between the original subsequences by
DR( ¯Q, ¯C) ≡
i=1 (¯qi −¯ci)2
This measure is illustrated in Fig. 6B. If we further transform the data into
the symbolic representation, we can deﬁne a MINDIST function that returns
the minimum distance between the original time series of two words:
MINDIST( ˆQ, ˆC) ≡
dist(ˆqi , ˆci)
The function resembles Eq. 4 except for the fact that the distance between
the two PAA coefﬁcients has been replaced with the sub-function dist(). The
dist() function can be implemented using a table lookup as illustrated in Table 4.
Experiencing SAX: a novel symbolic representationof time series
A visual intuition of the three representations discussed in this work, and the distance
measures deﬁned on them. (A) The Euclidean distance between two time series can be visualized
as the square root of the sum of the squared differences of each pair of corresponding points. (B)
The distance measure deﬁned for the PAA approximation can be seen as the square root of the sum
of the squared differences between each pair of corresponding PAA coefﬁcients, multiplied by the
square root of the compression rate. (C) The distance between two SAX representations of a time
series requires looking up the distances between each pair of symbols, squaring them, summing
them, taking the square root and ﬁnally multiplying by the square root of the compression rate
A lookup table used by the MINDIST function
This table is for an alphabet of cardinality of 4, i.e. a = 4. The distance between two symbols
can be read off by examining the corresponding row and column. For example, dist(a,b) = 0 and
dist(a,c) = 0.67.
The value in cell (r, c) for any lookup table can be calculated by the following
expression.
if |r −c| ≤1
βmax(r,c)−1 −βmin(r,c), otherwise
For a given value of the alphabet size a, the table needs only be calculated
once, then stored for fast lookup. The MINDIST function can be visualized
in Fig. 6C.
J. Lin et al.
As mentioned, one of the most important characteristics of SAX is that it
provides a lower-bounding distance measure. Below, we show that MINDIST
lower-bounds the Euclidean distance in two steps. First, we will show that the
PAA distance lower-bounds the Euclidean distance. The proof has appeared
in by the current author; for completeness, we repeat the
proof here. Next, we will show that MINDIST lower-bounds the PAA distance,
which in turn, by transitivity, shows that MINDIST lower-bounds the Euclidean
Step 1: We need to show that the PAA distance lower-bounds the Euclidean
distance; that is, D(Q, C) ≥DR( ¯Q, ¯C). We will show the proof on the case
where there is a single PAA frame (i.e. mapping the time series into one single
PAA coefﬁcient). A more generalized proof for N frames can be obtained by
applying the single-frame proof on every frame.
Proof Using the same notations as Eqs. 3 and Eq. 4, we want to prove that
(qi −ci) 2 ≥
Let Q and C be the means of time series Q and C, respectively. Since we are
considering only the single-frame case, Ineq. (7) can be rewritten as:
(qi −ci) 2 ≥√n
Squaring both sides we get
(qi −ci) 2 ≥n(Q −C)2
Each point qi in Q can be represented in term of Q, i.e. qi = Q −qi. Same
applies to each point ci in C. Thus, Ineq. (9) can be rewritten as:
((Q −qi) −(C −ci)) 2 ≥n(Q −C)2
Re-arranging the left-hand side we get
((Q −C) −(qi −ci)) 2 ≥n(Q −C)2
Experiencing SAX: a novel symbolic representationof time series
We can expand and rewrite Ineq. (11) as:
((Q −C)2 −2(Q −C)(qi −ci) + (qi −ci)2) ≥n(Q −C)2
By distributive law we get:
2(Q −C)(qi −ci) +
(qi −ci)2 ≥n(Q −C)2
n(Q −C)2 −2(Q −C)
(qi −ci) +
(qi −ci)2 ≥n(Q −C)2
Recall that qi = Q −qi, which means that qi = Q −qi, and similarity,
ci = C −ci. Therefore, the summation part of the second term on the lefthand side of the inequality becomes:
(qi −ci) =
((Q −qi) −(C −ci))
Substituting 0 into the second term on the left-hand side, Ineq. (14) becomes:
n(Q −C)2 −0 +
(qi −ci)2 ≥n(Q −C)2
Cancelling out n(Q −C)2 on both sides of the inequality, we get
J. Lin et al.
(qi −ci)2 ≥0
which always holds true, hence completes the proof.
Step 2: Continuing from Step 1 and using the same methodology, we will now
show that MINDIST lower-bounds the PAA distance; that is, we will show that
n(Q −C)2 ≥n(dist( ˆQ, ˆC))2
Let a = 1, b = 2, and so forth, there are two possible scenarios:
Case 1: |( ˆQ −ˆC)|
≤1. In other words, the symbols representing the two
time series are either the same, or consecutive from the alphabet, e.g. ˆQ =
ˆC = ‘a’, or ˆQ = ‘a’ and ˆC = ‘b’. From Eq. 6, we know that the MINDIST is 0
in this case. Therefore, the right-hand side of Ineq. (17) becomes zero, which
makes the inequality always hold true.
Case 2: |( ˆQ −ˆC)| > 1. In other words, the symbols representing the two time
series are at least two alphabets apart, e.g. ˆQ = ‘c’ and ˆC =‘a’. For simplicity, assume ˆQ > ˆC; the case where ˆQ < ˆC can be proven in similar fashion.
According to Eq. 6, dist( ˆQ, ˆC) is
dist( ˆQ, ˆC) = β ˆQ−1 −β ˆC
For the example above, dist(‘c’,‘a’)= β2 −β1.
Recall that Eq. 2 states the following:
βj−1 ≤¯ci < βj
So we know that
β ˆQ−1 ≤Q < β ˆQ
β ˆC−1 ≤C < β ˆC
Substituting Eq. 18 into Ineq. (17) we get
n(Q −C)2 ≥n (β ˆQ−1 −β ˆC)2
which implies that
β ˆQ−1 −β ˆC
Note that from our assumptions earlier that |( ˆQ −ˆC)| > 1 and ˆQ > ˆC (i.e. Q is
at a “higher” region thanC), we can drop the absolute value notations on both
Experiencing SAX: a novel symbolic representationof time series
Q −C ≥β ˆQ−1 −β ˆC
Rearranging the terms we get:
Q −β ˆQ−1 ≥C −β ˆC
which we know always holds true since, from Ineq. (19), we know that
Q −β ˆQ−1 ≥0
C −β ˆC < 0
This completes the proof for ˆQ > ˆC. The case where ˆQ < ˆC can be proven
similarily, and is omitted for brevity.
There is one issue we must address if we are to use a symbolic representation
of time series. If we wish to approximate a massive dataset in main memory,
the parameters w and a have to be chosen in such a way that the approximation
makes the best use of the primary memory available. There is a clear tradeoff
between the parameter w controlling the number of approximating elements,
and the value a controlling the granularity of each approximating element.
It is unfeasible to determine the best tradeoff analytically, since it is highly
data dependent. We can however empirically determine the best values with a
simple experiment. Since we wish to achieve the tightest possible lower bounds,
we can simply estimate the lower bounds over all possible feasible parameters,
and choose the best settings.
Tightness of Lower Bound = MINDIST( ˆQ, ˆC)
We performed such a test with a concatenation of 50 time series datasets
taken from the UCR time series data mining archive. For every combination of
parameters we averaged the result of 100,000 experiments on subsequences of
length 256. Figure 7 shows the results.
The results suggest that using a low value for a results in weak bounds. While
it’s intuitive that larger alphabet sizes yield better results, there are diminishing
returns as a increases. If space is an issue, an alphabet size in the range 5–8
seems to be a good choice that offers a reasonable balance between space and
tightness of lower bound—each alphabet within this range can be represented
with just 3 bits. Increasing the alphabet size would require more bits to represent
each alphabet.
We end this section with a visual comparison between SAX and the four
most used representations in the literature (Fig. 8). We can see that SAX preserves the general shape of the original time series. Note that since SAX is a
symbolic representation, the alphabets can be stored as bits rather than doubles, which results in a considerable amount of space-saving. Therefore, SAX
J. Lin et al.
Word Size w
Alphabet size a
The empirically estimated tightness of lower bounds over the cross product of a = [3. . .11]
and w = [2. . .9]. The darker histogram bars illustrate combinations of parameters that require
approximately equal space to store every possible word
A visual comparison of SAX and the four most common time series data mining representations. A raw time series of length 128 is transformed into the word ffffffeeeddcbaabceedcbaaaaacddee. This is a fair comparison since the number of bits in each representation is the same
representation can afford to have higher dimensionality than the other real-valued approaches, while using less or the same amount of space.
3.4 Numerosity reduction
We have seen that, given a single time series, our approach can signiﬁcantly
reduce its dimensionality. In addition, our approach can reduce the numerosity
of the data for some applications.
Most applications assume that we have one very long time series T, and that
manageable subsequences of length n are extracted by use of a sliding window,
then stored in a matrix for further manipulation . Figure 9 illustrates the
When performing sliding windows subsequence extraction, with any of the
real-valued representations, we must store all |T|−n+1 extracted subsequences
Experiencing SAX: a novel symbolic representationof time series
An illustration of the notation introduced in this section: A time series T of length 128, the
subsequence C67, of length n = 16, and the ﬁrst eight subsequences extracted by a sliding window.
Note that the sliding windows are overlapping
(in dimensionality reduced form). However, imagine for a moment that we are
using our proposed approach. If the ﬁrst word extracted is aabbcc, and the
window is shifted to discover that the second word is also aabbcc, we can reasonably decide not to include the second occurrence of the word in sliding
windows matrix. If we ever need to retrieve all occurrences of aabbcc, we can
go to the location pointed to by the ﬁrst occurrences, and remember to slide to
the right, testing to see if the next window is also mapped to the same word. We
can stop testing as soon as the word changes. This simple idea is very similar to
the run-length-encoding data compression algorithm.
The utility of this optimization depends on the parameters used and the
data itself, but it typically yields a numerosity reduction factor of two or three.
However, many datasets are characterized by long periods of little or no movement, followed by bursts of activity (seismological data is an obvious example).
On these datasets the numerosity reduction factor can be huge. Consider the
example shown in Fig. 10.
There is only one special case we must consider. As we noted in Sect. 3.1,
we normalize each time series (including subsequences) to have a mean of zero
and a standard deviation of one. However, if the subsequence contains only
one value, the standard deviation is not deﬁned. More troublesome is the case
where the subsequence is almost constant, perhaps 31 zeros and a single 0.0001.
If we normalize this subsequence, the single differing element will have its value
exploded to 5.65. This situation occurs quite frequently. For example, the last
200 time units of the data in Fig. 10 appear to be constant, but actually contain
Space Shuttle STS-57 Telemetry
Sliding window extraction on Space Shuttle Telemetry data, with n = 32. At time point 61,
the extracted word is aabbcc, and the next 401 subsequences also map to this word. Only a pointer
to the ﬁrst occurrence must be recorded, thus producing a large reduction in numerosity
J. Lin et al.
Contributes to S1
with weight 1/3
Contributes to S2
with weight 2/3
Contributes to S3
with weight 1/3
Contributes to S2
with weight 2/3
(A) Ten data points are divided into ﬁve segments. (B) Ten data points are divided into
three segments. The data points marked with circles contribute to two adjacent segments at the
tiny amounts of noise. If we were to normalize subsequences extracted from this
area, the normalization would magnify the noise to large meaningless patterns.
We can easily deal with this problem, if the standard deviation of the sequence
before normalization is below an epsilon ε, we simply assign the entire word to
the middle-ranged alphabet (e.g. cccccc if a = 5).
3.5 Relaxation on the number of segments
So far we have described SAX with the assumption that the length of the time
series is divisible by the number of segments, i.e. n/w must be an integer. If n
is not dividable by w, there will be some points in the time series that we are
not sure which segment to put them. For example, in Fig. 11A, we are dividing
10 data points into ﬁve segments. And it is obvious that point 1, 2 should be in
segment 1; point 3, 4 should be in segment 2; so on and so forth. In in Fig. 11B,
we are dividing 10 data points into three segments. It’s not clear which segment
point 4 should go: segment 1 or segment 2. Same problem holds for point 7.
The assumption n must be dividable by w clearly limits our choices of w, and
is problematic if n is a prime number. Here we show that this needs not be the
case and provide a simple solution when n is not divisible by w.
Instead of putting the whole point into a segment, we can put part of it. For
example, in in Fig. 11B, point 4 contributes its 1/3 to segment 1 and its 2/3 to
segment 2, and point 7 contributes its 2/3 to segment 2 and its 1/3 to segment 3.
This makes each segment contains exactly 3 1/3 data points and solves the undividable problem. This generalization is implemented in the later version of
SAX, as well as some of the applications that utilize SAX.
4 Experimental validation of our symbolic approach
In this section, we perform various data mining tasks using our symbolic
approach and compare the results with other well-known existing approaches.
Experiencing SAX: a novel symbolic representationof time series
For clustering, classiﬁcation, and anomaly detection, we compare the results
with the classic Euclidean distance, and with other previously proposed symbolic approaches. Note that none of these other approaches use dimensionality
reduction. In the next paragraphs we summarize the strawmen representations
that we compare ours to. We choose these two approaches since they are typical
representatives of approaches in the literature.
André-Jönsson, and Badal proposed the SDA algorithm that computes the changes between values from one instance to the next, and divide
the range into user-predeﬁned sections. The disadvantages of this approach are
obvious: prior knowledge of the data distribution of the time series is required
in order to set the breakpoints; and the discretized time series does not conserve
the general shape or distribution of the data values.
Huang and Yu proposed the IMPACTS algorithm, which uses change ratio
between one time point to the next time point to discretize the time series
 . The range of change ratios are then divided into equalsized sections and mapped into symbols. The time series is converted to a
discretized collection of change ratios. As with SAX, the user needs to deﬁne
the cardinality of symbols.
4.1 Clustering
Clustering is one of the most common data mining tasks, being useful in its own
right as an exploratory tool, and also as a sub-routine in more complex algorithms . We consider
two clustering algorithms, one of hierarchical clustering, and one of partitional
clustering.
4.1.1 Hierarchical clustering
Comparing hierarchical clusterings is a very good way to compare and contrast
similarity measures, since a dendrogram of size N summarizes O(N2) distance
calculations . The evaluation is typically subjective,
we simply adjudge which distance measure appears to create the most natural
groupings of the data. However, if we know the data labels in advance we can
also make objective statements of the quality of the clustering. In Fig. 12 we
clustered nine time series from the Control Chart dataset, three each from the
decreasing trend, upward shift and normal classes.
In this case we can objectively state that SAX is superior, since it correctly
assigns each class to its own subtree. This is simply a side effect due to the
smoothing effect of dimensionality reduction. Therefore, it’s not surprising that
SAX can sometimes outperform the simple Euclidean distance, especially on
noisy data, or data with shifting on the time-axis. This fact is demonstrated in
the dendrogram produced by Euclidean distance: the “normal” class, which
contains a lot of noise, is not clustered correctly. More generally, we observed
that SAX closely mimics Euclidean distance on various datasets.
J. Lin et al.
IMPACTS (alphabet=8)
A comparison of the four distance measures’ ability to cluster members of the Control
Chart dataset. Complete linkage was used as the agglomeration technique
The reasons that SDA and IMPACTS perform poorly, we observe, are that
neither symbolic representation is very descriptive of the general shape of the
time series, and that the lack of dimensionality reduction can further distort the
results if the data is noisy. What SDA does is essentially differencing the time
series, and then discretizing the resulting series. While differencing has been
used historically in statistical time series analysis, its purposes to remove some
autocorrelation, and to make a time series stationary are not always applicable
in determination of similarity in data mining. In addition, although computing
the derivatives tells the type of change from one time point to the next time
point: sharp increase, slight increase, sharp decrease, etc., this approach doesn’t
appear very useful since time series data are typically noisy. More speciﬁcally,
in addition to the overall trends or shapes, there are noises that appear throughout the entire time series. Without any smoothing or dimensionality reduction,
these noises are likely to overshadow the actual characteristics of the time series.
To demonstrate why the “decreasing trend” and the “upward shift” classes
are indistinguishable by the clustering algorithm for SDA, let’s look at what the
differenced series look like. Figure 13 shows the original time series and their
corresponding series after differencing. It’s clear that the differenced series
from the same class are not any more similar than those from a different class.
As a matter of fact, as we compute the pairwise distances between all 6 differenced series, we realize that the distances are not indicative at all of the classes
these data belong. Table 5 and Table 6 show the inter- and the intra-distances
between the series (the series from the “decreasing trend” class are denoted as
Ai, and the series from the “upward shift” are denoted as Bi).
Experiencing SAX: a novel symbolic representationof time series
(A) Time series from the “decreasing trend” class and the resulting series after differencing.
(B) Time series from the “upward shift” class and the resulting series after differencing
Intra-class distances
between the differenced time
series from the “decreasing
trend” class
Inter-class distances
between the differenced time
series from the “decreasing
trend” and the “upward shift”
Interestingly, in Gavrilov et al. , the authors show that taking the ﬁrst
derivatives (i.e. differencing) actually worsens the results when compared to
using the raw data. Our experimental results validate their observations.
IMPACTS suffers from similar problems as SDA. In addition, it’s clear that
neither IMPACTS nor SDA can beat simple Euclidean distance, and the discussion above applies to all data mining tasks, since the problems lie in the nature
of the representations.
4.1.2 Partitional clustering
Although hierarchical clustering is a good sanity check for any proposed distance measure, it has limited utility for data mining because of its poor scalability. The most commonly used data mining clustering algorithm is k-means
 , so for completeness we will consider it here. We performed
k-means on both the original raw data, and our symbolic representation. Figure 14 shows a typical run of k-means on a space telemetry dataset. Both algorithms converge after 11 iterations. Since k-means algorithm seeks to optimize
the objective function, by minimizing the sum of squared intra-cluster error,
J. Lin et al.
we compare and plot the objective functions, after projecting the data back
to its original dimension (for fair comparison of objective functions), for each
iteration. The objective function for a given clustering is given by Eq. 26, where
xi is the time series, and cm is the cluster center of the cluster that xi belongs
to. The smaller the objective function, the more compact (thus better) the
The results here are quite unintuitive and surprising: working with an approximation of the data gives better results than working with the original data.
Fortunately, a recent paper offers a suggestion as to why this might be so. It has
been shown that initializing the clusters centers on a low dimension approximation of the data can improve the quality , this is what clustering
with SAX implicitly does.
In Section 4.4.3 we introduce another distance measure based on SAX. By
applying it on clustering, we show that it outperforms the Euclidean distance
4.2 Classiﬁcation
Classiﬁcation of time series has attracted much interest from the data mining
community. Although special- purpose algorithms have been proposed , we will consider only the two most common classiﬁcation
algorithms for brevity, clarity of presentations and to facilitate independent
conﬁrmation of our ﬁndings.
4.2.1 Nearest neighbor classiﬁcation
To compare different distance measures on 1-nearest-neighbor classiﬁcation,
we use leaving-one-out cross validation. Firstly, we compare SAX with Euclidean distance, IMPACTS, SDA, and LPinf. Two classic synthetic datasets are
used: the Cylinder-Bell-Funnel (CBF) dataset has 50 instances of time series
for each of the three clusters, and the Control Chart (CC) has 100 instances for
each of the six clusters .
Since SAX allows dimensionality and alphabet size as user input, and the
IMPACTS allows variable alphabet size, we ran the experiments on different combinations of dimensionality reduction and alphabet size. For the other
approaches we applied the simple dimensionality reduction technique of skipping data points at a ﬁxed interval. In Fig. 15, we show the result with a dimensionality reduction of 4 to 1.
Experiencing SAX: a novel symbolic representationof time series
Number of Iterations
Objective Function
A comparison of the k-means clustering algorithm using SAX and using the raw data. The
dataset was Space Shuttle telemetry, 1,000 subsequences of length 512. Surprisingly, working with
the symbolic approximation produces better results than working with the original data
Control Chart
Cylinder -Bell -Funnel
Error Rate
Alphabet Size
Alphabet Size
A comparison of ﬁve distance measures utility for nearest neighbor classiﬁcation. We
tested different alphabet sizes for SAX and IMPACTS, SDA’s alphabet size is ﬁxed at 5
Similar results were observed for other levels of dimensionality reduction.
Once again, SAX’s ability to beat Euclidean distance is probably due to the
smoothing effect of dimensionality reduction, nevertheless this experiment does
show the superiority of SAX over the others proposed in the literature.
Since both IMPACTS and SDA perform poorly compared to Euclidean distance and SAX, we will exclude them from the rest of the classiﬁcation experiments. To provide a closer look on how SAX compares to Euclidean distance,
J. Lin et al.
we ran an extensive experiment and compared the error rates on 22 datasets
(available online at Each
dataset is split to training and testing parts. We use the training part to search
the best value for SAX parameters w (number of SAX words) and a (size of
the alphabet):
For w, we search from 2 up to n/2 (n is the length of the time series). Each
time we double the value of w.
For a, we search each value between 3 and 10.
If there is a tie, we use the smaller values.
Thecompressionratio(last columnof next table) is calculatedas:w ×
/n × 32, because for SAX representation we only need
bits per word,
while for the original time series we need 4 bytes (32 bits) for each value.
Then we classify the testing set based on the training set using one nearest
neighbor classiﬁer and report the error rate. The results are shown in Table 7.
We also summarize the results by plotting the error rates for each dataset as a
2-dimensional point: (EU_error, SAX_error). If a point falls within the lower
triangle, then SAX is more accurate than Euclidean distance, and vice versa for
the upper triangle. The plot is shown in Fig. 16. From this experiment, we can
conclude that SAX is competitive with Euclidean distance, but requires far less
4.2.2 Decision tree classiﬁcation
Because of Nearest Neighbor’s poor scalability, it is unsuitable for most data
mining applications; instead decision trees are the most common choice of classiﬁer. While decision trees are deﬁned for real data, attempting to classify time
series using the raw data would clearly be a mistake, since the high dimensionality and noise levels would result in a deep, bushy tree with poor accuracy.
In an attempt to overcome this problem, Geurts suggests representing
the time series as a Regression Tree (RT) , see Fig. 2, and training the decision tree
directly on this representation. The technique shows great promise.
We compared SAX to the Regression Tree (RT) on two datasets; the results
are in Table 8.
Note that while our results are competitive with the RT approach, the RT representation is undoubtedly superior in terms of interpretability .
Once again, our point is simply that our “black box” approach can be competitive with specialized solutions.
4.3 Query by content (Indexing)
The majority of work on time series data mining appearing in the literature has
addressed the problem of indexing time series for fast retrieval . Indeed, it is in this context that most of the representations enumerated
Experiencing SAX: a novel symbolic representationof time series
1-NN comparison between Euclidean Distance and SAX
Size of training
Size of testing
Time series
Compression
Synthetic Control
Face (all)
Swedish leaf
Two patterns
Face (four)
Lightning-2
Lightning-7
J. Lin et al.
Error Rate of Euclidean Distance
In this region
SAX representation
is more accurate
In this region
Euclidean distance
is more accurate
Error rates for SAX and Euclidean distance on 22 datasets. Lower triangle is the region
where SAX is more accurate than Euclidean distance, and upper triangle is where Euclidean
distance is more accurate than SAX
in Fig. 1 were introduced . Dozens of papers have introduced techniques to
do indexing with a symbolic approach , but without exception, the answer set retrieved by these techniques can be very different to the answer set that would be retrieved by the
true Euclidean distance. It is only by using a lower bounding technique that one
can guarantee retrieving the full answer set, with no false dismissals .
To perform query by content, we build an index using SAX, and compare it
to an index built using the Haar wavelet approach . Since
the datasets we use are large and disk-resident, and the reduced dimensionality
could still be potentially high ),
we use Vector Approximation (VA) ﬁle as our indexing algorithm. We note,
however, that SAX could also be indexed by classic string indexing techniques
such as sufﬁx trees.
A comparison of SAX with the specialized Regression Tree approach for decision tree
classiﬁcation
Regression Tree
3.04 ± 1.64
2.78 ± 2.11
0.97 ± 1.41
1.14 ± 1.02
Our approach used an alphabet size of 6, both approaches used a dimensionality of 8
Experiencing SAX: a novel symbolic representationof time series
A comparison of indexing ability of wavelets versus SAX. The Y-axis is the percentage
of the data that must be retrieved from disk to answer a 1-NN query of length 256, when the
dimensionality reduction ratio is 32 to 1 for both approaches
To compare performance, we measure the percentage of disk I/Os required
in order to retrieve the one-nearest neighbor to a randomly extracted query,
relative to the number of disk I/Os required for sequential scan. Since it has
been forcibly shown that the choice of dataset can make a signiﬁcant difference
in the relative indexing ability of a representation, we tested on more than 50
datasets from the UCR Time Series Data Mining Archive. In Fig. 17 we show
four representative examples. The y-axis shows the index power in terms of the
percentage of the data retrieved from the disk, compared to sequential scan. In
almost all cases, SAX shows a superior reduction in the number of disk accesses.
In addition, SAX does not have the limitation faced by the Haar Wavelet that
the data length must be a power of two.
4.4 Taking advantage of the discrete nature of our representation
In the previous sections we showed examples of how our proposed representation can compete with real-valued representations and the original data. In
this section we illustrate examples of data mining algorithms that take explicit
advantage of the discrete nature of our representation.
4.4.1 Detecting Novel/Surprising/Anomalous Behavior
A simple idea for detecting anomalous behavior in time series is to examine
previously observed normal data and build a model of it. Data obtained in the
future can be compared to this model and any lack of conformity can signal
an anomaly . In order to achieve this, in Keogh
et al. we combined a statistically sound scheme with an efﬁcient combinatorial approach. The statistically scheme is based on Markov chains and
normalization. Markov chains are used to model the “normal” behavior, which
is inferred from the previously observed data. The time- and space-efﬁciency of
the algorithm comes from the use of sufﬁx tree as the main data structure. Each
node of the sufﬁx tree represents a pattern. The tree is annotated with a score
J. Lin et al.
A comparison of ﬁve anomaly detection algorithms on the same task. (I) The training
data, a slightly noisy sine wave of length 1,000. (II) The time series to be examined for anomalies
is a noisy sine wave that was created with the same parameters as the training sequence, then an
assortment of anomalies were introduced at time periods 250, 500 and 750. (III) and (IIII) The
Markov model technique using the IMPACTS and SDA representations did not clearly discover
the anomalies, and reported some false alarms. (V) The IMM anomaly detection algorithm appears
to have discovered the ﬁrst anomaly, but it also reported many false alarms. (VI) The TSA-Tree
approach is unable to detect the anomalies. (VII) The Markov model-based technique using SAX
clearly ﬁnds the anomalies, with no false alarms
obtained comparing the support of a pattern observed in the new data with the
support recorded in the Markov model. This apparently simple strategy turns
out to be very effective in discovering surprising patterns. In the original work
we use a simple symbolic approach, similar to IMPACTS ;
here we revisit the work using SAX.
For completeness, we will compare SAX to two highly referenced anomaly
detection algorithms that are deﬁned on real valued representations, the TSAtree Wavelet based approach of Shahabi et al. and the Immunology
(IMM) inspired work of Dasgupta and Forrest . We also include the Markov technique using IMPACTS and SDA in order to discover how much of the
difference can be attributed directly to the representation. Figure 18 contains
an experiment comparing all ﬁve techniques.
The results on this simple experiment are impressive. Since sufﬁx trees and
Markov models can be used only on discrete data, this offers a motivation
for our symbolic approach. While all the other approaches, including the Markov Models using IMPACTS and SDA representations, the Immunology-based
anomaly detection approach, and the TSA-Tree approach, did not clearly discover the anomalies and reported some false alarms, the SAX-based Markov
Model clearly ﬁnds the anomalies with no false alarms.
4.4.2 Motif discovery
It is well understood in bioinformatics that overrepresented DNA sequences
often have biological signiﬁcance 
Above, a motif discovered in a complex dataset by the modiﬁed Projection algorithm.
Below, the motif is best visualized by aligning the two subsequences and “zooming in”. The similarity
of the two subsequences is striking, and hints at unexpected regularity
Reinert et al. 2000). A substantial body of literature has been devoted to techniques to discover such patterns . In a previous work, we deﬁned the related concept of
“time series motif” . Time series motifs are close analogues of
their discrete cousins, although the deﬁnitions must be augmented to prevent
certain degenerate solutions. The naïve algorithm to discover the motifs is quadratic in the length of the time series. In Lin et al. , we demonstrated a
simple technique to mitigate the quadratic complexity by a large constant factor,
nevertheless this time complexity is clearly untenable for most real datasets.
The symbolic nature of SAX offers a unique opportunity to avail of the
wealth of bioinformatics research in this area. In particular, recent work by
Tompa and Buhler holds great promise . The authors
show that many previously unsolvable motif discovery problems can be solved
by hashing subsequences into buckets using a random subset of their features
as a key, then doing some post-processing search on the hash buckets.3 They
call their algorithm Projection.
We carefully reimplemented the random projection algorithm of Tompa and
Buhler, making minor changes in the post-processing step to allow for the fact
that although we are hashing random projections of our symbolic representation, we actually wish to discover motifs deﬁned on the original raw data . Figure 19 shows an example of a motif discovered in an industrial dataset using this technique. The patterns found are
extremely similar to one another.
Apart from the attractive scalability of the algorithm, there is another important advantage over other approaches. The Projection algorithm is able to
discover motifs even in the presence of noise. Our extension of the algorithm
3 Of course, this description greatly understates the contributions of this work. We urge the reader
to consult the original paper.
J. Lin et al.
inherits this robustness to noise. We direct interested readers to Chiu et al.
 for more detailed discussion of this algorithm.
4.4.3 Visualization
Data visualization techniques are very important for data analysis, since the
human eye has been frequently advocated as the ultimate data-mining tool.
However, despite their illustrative nature, which can provide users better understanding of the data and intuitive interpretation of the mining results, there has
been surprisingly little work on visualizing large time series datasets. One reason
for this lack of interest is that time series data are also usually very massive in
size. With limited pixel space and the typically enormous amount of data at
hand, it is infeasible to display all the data on the screen at once, much less ﬁnding any useful information from the data. How to efﬁciently organize the data
and present them in such a way that is intuitive and comprehensible to human
eyes thus remains a great challenge. Ideally, the visualization technique should
follow the Visual Information Seeking Mantras, as summarized by Dr. Ben
Shneiderman: “Overview, zoom & ﬁlter, details-on-demand.” In other words,
it should be able to provide users the overview or summary of the data, and
allows users to further investigate on the interesting patterns highlighted by the
tool. To this end, we developed VizTree , a time series pattern
discovery and visualization system based on augmenting sufﬁx trees. VizTree
visually summarizes both the global and local structures of time series data
at the same time. In addition, it provides novel interactive solutions to many
pattern discovery problems, including the discovery of frequently occurring
patterns (motif discovery), surprising patterns (anomaly detection), and query
by content. The user interactive paradigm allows users to visually explore the
time series, and perform real-time hypotheses testing. Since the use of sufﬁx
tree requires that the input data be discrete, SAX is the perfect candidate for
discretizing the time series data.
Compared to the existing time series visualization systems in the literature,
VizTree is unique in several respects. First, almost all other approaches assume
highly periodic time series, whereas VizTree makes no such assumption. Other
methods typically require space (both memory space, and pixel space) that
grows at least linearly with the length of the time series, making them untenable for mining massive datasets. Finally, VizTree allows us to visualize a much
richer set of features, including global summaries of the differences between
two time series, locally repeated patterns, anomalies, etc.
In VizTree, patterns are represented in a depth-limited tree structure, in
whichtheir frequencies of occurrenceareencodedinthethicknesses of branches.
The algorithm works by sliding a window across the time series and extracting
subsequences of user-deﬁned lengths. The subsequences are then discretized
into strings by SAX and inserted into an augmented sufﬁx tree. Each string is
regarded as a pattern, and the frequency of occurrence for each pattern is encoded by the thickness of the branch: the thicker the branch, the more frequent
the corresponding pattern. Motif discovery and anomaly detection can thus be
Experiencing SAX: a novel symbolic representationof time series
Anomaly detection on power consumption data. The anomaly shown here is a short week
during Christmas
easily achieved: those that occur frequently can be regarded as motifs, and those
that occur rarely can be regarded as anomaly. Figure 20 shows the screenshot of
VizTree for anomaly detection on the Dutch power demand dataset. Electricity
consumption is recorded every 15 min; therefore, for the year of 1997, there are
35,040 data points. The majority of the weeks follow the regular Monday-Friday, 5-working-day pattern, as shown by the thick branches. The thin branches
denote the anomalies (in the sense that the electricity consumption is abnormal given the day of the week). Note that in VizTree, we reverse the alphabet
ordering so the alphabets now read top-down rather than bottom-up (e.g. ‘a’ is
now in the topmost branch, rather than in the bottom-most branch). This way,
the string better describes the actual shape of the time series—‘a’ denotes the
top region, ‘b’ the middle region, ‘c’ the bottom region. The top right window
shows the subtree when we click on the 2nd child of the root node. Clicking
on any of the existing branches (in the main or the subtree window) will plot
the subsequences represented by them in the bottom right window. The highlighted, circled subsequence is retrieved by clicking on the branch “bab.” The
zoom-in shows why it is an anomaly: it’s the beginning of the three-day week
during Christmas (Thursday and Friday off). The other thin branches denote
other anomalies such as New Year’s Day, Good Friday, Queen’s Birthday, etc.
The evaluation for visualization techniques is usually subjective. Although
VizTree clearly demonstrates its capability in detecting non-trivial patterns, in
Lin et al. we also devise a measure that quantiﬁes the effectiveness of the
J. Lin et al.
Clustering result using the (dis)similarity coefﬁcient
algorithm. The measure, which we call the “dissimilarity coefﬁcient,” describes
how dissimilar two time series are, and ranges from 0 to 1. In essence, the coefﬁcient summarizes the difference in “behavior” of each pattern (represented by
a string) in two time series. More concretely, for each pattern, we count its
respective numbers of occurrences in both time series, and see how much the
frequencies differ. We call this measure support, which is then weighted by the
conﬁdence, or the degree of “interestingness” of the pattern. For example, a
pattern that occurs 120 times in time series A and 100 times in time series B is
probably less signiﬁcant than a pattern that occurs 20 times in A but zero times
in B, even though the support for both cases is 20.
Subtracting the dissimilarity coefﬁcient from 1 then gives us a novel similarity measure that describes how similar two time series are. More details on
the (dis)similarity measure can be found in Lin et al. . An important fact
about this similarity measure is that, unlike a distance measure that computes
point-to-point distances, it captures the global structure of the time series rather
than local differences. This time-invariant feature is useful if we are interested
in the overall structures of the time series. Figure 21 shows the dendrogram of
clustering result using the (dis)similarity coefﬁcient as the distance measure. It
clearly demonstrates that the coefﬁcient captures the dissimilarity very well and
that all clusters are separated perfectly. Note that it’s even able to distinguish
the four different sets of heartbeats (from top down, clusters 1, 4, 5, and 6)!
As a reference, we ran the same clustering algorithm using the widely-used
Euclidean distance. The result is shown in Fig. 22. Clearly, clustering using our
(dis)similarity measure returns superior results.
So far we discussed how the symbolic nature of SAX makes possible the
approaches that were not considered, at least not effectively, by the time series
Experiencing SAX: a novel symbolic representationof time series
Clustering result using Euclidean distance
data mining community before, since these approaches require the input be
discrete. It sheds some light in offering efﬁcient solutions on various problems
from a new direction. The problems listed in this section, however, are just a
small subset of examples that show the efﬁcacy of SAX. We have since then
proposed more algorithms on anomaly detection (HOT SAX) , contrast sets mining , visualization by time series
bitmaps , clustering , compression-based distance measures , etc., all of which based
on SAX. In the next section, we discuss the evident impact of SAX, illustrated
by great interests from other researchers/users from both the academic and the
industrial communities.
4.5 The impact of SAX
In the relatively short time since its initial introduction, SAX has had a large
impact in industry and academia. Below we summarize some of this work, without attempting to be exhaustive. We can broadly classify this work into those
who have simply used the original formulation of SAXto solve a particular problem, and those who have attempted to extend or augment SAX in some way.
4.5.1 Applications of SAX
In addition to the applications mentioned in this section, it has been used
worldwide in various domains. To name a few, in Androulakis the authors analyzed complex kinetic mechanisms using a method based on SAX.
J. Lin et al.
In Ferreira the authors consider the problem of analysis of protein
unfolding data. After noting that “a complexity/dimensionality reduction on the
data may be necessary and desirable” the authors consider various alternatives
before noting “We adopted a two step approach called SAX .” Dr. Amy McGovern of the University of Oklahoma is leading a project on dynamic relational
models for improved hazardous weather prediction. In attempting to use propositional models for this task, her group needs a discrete representation of the
real-valued metrological data, and they noted in a recent paper “We are currently using . In Duchene and Garbay , Duchene et al.
 , Silvent et al. , the authors use SAX and random projection to discover motifs in telemedicine time series. In Chen et al. the
authors convert palmprint to time series, then to SAX, then they do biometric
recognition. The authors in Tanaka and Uehara use SAX and random
projection to mine motion capture data. In Celly and Zordan SAX is
used to ﬁnd repeated patterns in motion capture data. Ohsaki et al. uses
SAX to ﬁnd rules in time series. In Tanaka and Uehara the authors use
SAX to ﬁnd motifs of unspeciﬁed length. In Murakami et al. SAX is
used to ﬁnd repeated patterns in robot sensors. In Bakalov et al. , the
authors use SAX to do spatiotemporal trajectory joins. In Pouget et al. 
the authors use SAX to “detect multi-headed stealthy attack tools”.
4.5.2 Extensions to SAX
As noted above, there have been dozens of applications of SAX to diverse
problems. However, there has been surprisingly little work to augment or extend the SAX representation itself. We attribute this to the generality of the
original framework; it simply works very well for most problems. Nevertheless,
recently there have been some SAX extensions, which we consider below.
In Lkhagva et al. the authors augment each SAX symbol by incorporating the minimum and maximum value in the range. Thus each SAX segment
contains a triplet of information, rather that a single symbol. Very preliminary
results are presented which suggest that this may be useful in some domains.
Hugueney has recently suggested that SAX could be improved
by allowing SAX symbols to adaptively represent different length sections of a
time series. Just as SAX may be seen as symbolic a symbolic version of the PAA
representation , Dr. Hugueneys approach may be seen as
symbolic a symbolic version of the APCA representation .
While the author shows that this does decrease the reconstruction error on some
datasets, it is not obvious that the new representation can be used with hashing
 ) or with sufﬁx trees ). In contrast
to Hugueneys idea of allowing segment lengths to be adaptive, recent work by
Mörchen and Ultsch has suggested that the breakpoints should be adaptive
 . The authors make a convincing case that in some
datasets this may be better than the Gaussian assumption (cf. Section 3.2).
For example they consider a time series of muscle activity and show that it
Experiencing SAX: a novel symbolic representationof time series
strongly violates the Gaussian assumption. Note that once the new breakpoints
are deﬁned, and Tables 3 and 4 are appropriately adjusted, the lower bounding
property still holds. Finally, in an interesting new development, Wei et al 
have shown techniques to adapt SAX to various problems in 2D shape matching, after modifying algorithms/representations to allow for rotation invariance,
which in the SAX representation corresponds to circular shifts.
The list of papers using SAX keeps growing at rapid rate. The wide acceptance of SAX by fellow researchers has shown its generality and utilities in
diverse domains. There is great potential for adapting and extending SAX on
an even broader class of data mining problems.
5 Conclusions and future directions
In this work we have formulated the ﬁrst dimensionality/numerosity reduction,
lower bounding symbolic approach in the literature. We have shown that our
representation is competitive with, or superior to, other representations on a
wide variety of classic data mining problems, and that its discrete nature allows
us to tackle emerging tasks such as anomaly detection, motif discovery and
visualization.
A host of future directions suggest themselves. There is an enormous wealth
of useful deﬁnitions, algorithms and data structures in the bioinformatics literature that can be exploited by our representation . It may be possible to create a
lower bounding approximation of Dynamic Time Warping , by slightly modifying the classic string edit distance. Finally, there may
be utility in extending our work to multidimensional and streaming time series
 .