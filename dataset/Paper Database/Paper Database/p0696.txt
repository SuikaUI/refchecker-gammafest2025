Adversarial Examples Are a Natural Consequence of Test Error in Noise
Nicolas Ford * 1 2 Justin Gilmer * 1 Nicholas Carlini 1 Ekin D. Cubuk 1
Over the last few years, the phenomenon of adversarial examples — maliciously constructed inputs that fool trained machine learning models —
has captured the attention of the research community, especially when the adversary is restricted
to small modiﬁcations of a correctly handled input. Less surprisingly, image classiﬁers also lack
human-level performance on randomly corrupted
images, such as images with additive Gaussian
noise. In this paper we provide both empirical and
theoretical evidence that these are two manifestations of the same underlying phenomenon, establishing close connections between the adversarial
robustness and corruption robustness research programs. This suggests that improving adversarial
robustness should go hand in hand with improving
performance in the presence of more general and
realistic image corruptions. Based on our results
we recommend that future adversarial defenses
consider evaluating the robustness of their methods to distributional shift with benchmarks such
as Imagenet-C.
1. Introduction
State-of-the-art computer vision models can achieve impressive performance on many image classiﬁcation tasks. Despite this, these same models still lack the robustness of the
human visual system to various forms of image corruptions.
For example, they are distinctly subhuman when classifying
images distorted with additive Gaussian noise , they lack robustness to different types of
blur, pixelation, and changes in brightness , lack robustness to random translations
of the input , and even make errors when foreign objects are inserted into the ﬁeld of view
 . At the same time, they are also
sensitive to small, worst-case perturbations of the input, socalled “adversarial examples” . This
*Equal contribution 1Google Brain 2This work was completed
as part of the Google AI Residency. Correspondence to: Nicolas
Ford < >, Justin Gilmer < >.
latter phenomenon has struck many in the machine learning
community as surprising and has attracted a great deal of
research interest, while the former has received considerably
less attention.
The machine learning community has researchers working
on each of these two types of errors: adversarial example researchers seek to measure and improve robustness to
small-worst case perturbations of the input while corruption
robustness researchers seek to measure and improve model
robustness to distributional shift. In this work we analyze
the connection between these two research directions, and
we see that adversarial robustness is closely related to robustness to certain kinds of distributional shift. In other
words, the existence of adversarial examples follows naturally from the fact that our models have nonzero test error
in certain corrupted image distributions.
We make this connection in several ways. First, in Section 4,
we provide a novel analysis of the error set of an image
classiﬁer. We see that, given the error rates we observe in
Gaussian noise, the small adversarial perturbations we observe in practice appear at roughly the distances we would
expect from a linear model, and that therefore there is no
need to invoke any strange properties of the decision boundary to explain them. This relationship was also explored in
Fawzi et al. .
In Section 5, we show that improving an alternate notion of
adversarial robustness requires that error rates under large
additive noise be reduced to essentially zero.
Finally, this connection suggests that methods which are
designed to increase the distance to the decision boundary
should also improve robustness to Gaussian noise, and vice
versa. In Section 6 we conﬁrm that this is true by examining both adversarially trained models and models trained
with additive Gaussian noise. We also show that measuring
corruption robustness can effectively distinguish successful
adversarial defense methods from ones that merely cause
vanishing gradients.
We hope that this work will encourage both the adversarial and corruption robustness communities to work more
closely together, since their goals seem to be so closely related. In particular, it is not common for adversarial defense
methods to measure corruption robustness. Given that sucarXiv:1901.10513v1 [cs.LG] 29 Jan 2019
Adversarial Examples Are a Natural Consequence of Test Error in Noise
cessful adversarial defense methods should also improve
some types of corruption robustness we recommend that
future researchers consider evaluating corruption robustness
in addition to adversarial robustness.
2. Related Work
The broader ﬁeld of adversarial machine learning studies
general ways in which an adversary may interact with an
ML system, and dates back to 2004 . Since the work of Szegedy et al. ,
a subﬁeld has focused speciﬁcally on the phenomenon of
small adversarial perturbations of the input, or “adversarial
examples.” Many algorithms have been developed to ﬁnd
the smallest perturbation in input space which fool a classi-
ﬁer . Defenses
have been proposed for increasing the robustness of classi-
ﬁers to small adversarial perturbations, however many have
later been shown ineffective . To
our knowledge the only method which has been conﬁrmed
by a third party to increase lp-robustness (for certain values
of ϵ) is adversarial training . However,
this method remains sensitive to slightly larger perturbations
 .
Several recent papers use concentation of measure to prove rigorous upper bounds on adversarial robustness for certain distributions in terms of test
error, suggesting non-zero test error may imply the existence
of adversarial perturbations. This may seem in contradiction
with empirical observations that increasing small perturbation robustness tends to reduce model accuracy . We note that these two conclusions are not
necessarily in contradiction to each other. It could be the
case that hard bounds on adversarial robustness in terms of
test error exist, but current classiﬁers have yet to approach
these hard bounds.
Because we establish a connection between adversarial robustness and model accuracy in corrupted image distributions, our results do not contradict reports that adversarial
training reduces accuracy in the clean distribution . In fact, we ﬁnd that improving adversarial
robustness also improves corruption robustness.
3. Adversarial and Corruption Robustness
Both adversarial robustness and corruption robustness can
be thought of as functions of the error set of a statistical
classiﬁer. This set, which we will denote E, is the set of
points in the input space on which the classiﬁer makes an
incorrect prediction. In this paper we will only consider
perturbed versions of training or test points, and we will
always assume the input is corrupted such that the “correct”
label for the corrupted point is the same as for the clean
point. This assumption is commonly made in works which
study model robustness to random corruptions of the input
 .
Because we are interested in how our models perform on
both clean images and corrupted ones, we introduce some
notation for both distributions. We will write p for the
natural image distribution, that is, the distribution from
which the training data was sampled. We will use q to denote
whichever corrupted image distribution we are working
with. A sample from q will always look like a sample from
p with a random corruption applied to it, like some amount
of Gaussian noise. Some examples of noisy images can be
found in Figure 10 in the appendix.
We will be interested in two quantities. The ﬁrst, corruption robustness under a given corrupted image distribution
q, is Px∼q[x /∈E], the probability that a random sample
from the q is not an error. The second is called adversarial
robustness. For a clean input x and a metric on the input
space d, let d(x, E) denote the distance from x to the nearest
point in E. The adversarial robustness of the model is then
Px∼p[d(x, E) > ϵ], the probability that a random sample
from p is not within distance ϵ of some point in the error set.
When we refer to “adversarial examples” in this paper, we
will always mean these nearby errors.
In this work we will investigate several different models trained on the CIFAR-10 and ImageNet datasets. For
CIFAR-10 we look at the naturally trained and adversarially
trained models which have been open-sourced by Madry
et al. . We also trained the same model on CIFAR-10
with Gaussian data augmentation. For ImageNet, we investigate an Inception v3 trained with
Gaussian data augmentation. In all cases, Gaussian data augmentation was performed by ﬁrst sampling a σ uniformly
between 0 and some speciﬁed upper bound and then adding
random Gaussian noise at that scale. Additional training details can be found in Appendix A. We were unable to study
the effects of adversarial training on ImageNet because no
robust open sourced model exists. only minimally improve robustness to
the white box PGD adversaries we consider here.)
4. Errors in Gaussian Noise Suggest
Adversarial Examples
We will start by examining the relationship between adversarial and corruption robustness in the case where q consists
of images with additive Gaussian noise.
The Linear Case. For linear models, the error rate in Gaussian noise exactly determines the distance to the decision
boundary. This observation was also made in Fawzi et al.
 .
Adversarial Examples Are a Natural Consequence of Test Error in Noise
Figure 1. When the input dimension, n, is large and the model
is linear, even a small error rate in additive noise implies the
existence of small adversarial perturbations. For a point x0 in
image space, most samples from N(x0; σ2I) (point B) lie close
to a sphere of radius σ√n around x0, drawn here as a circle. For
a linear model the error set E is a half-space, and the error rate
µ is approximately equal to the fraction of the sphere lying in
this half-space. The distance d(x0, E) from x0 to its nearest error
(point A) is also drawn. Note the relationship between σ, µ, and
d(x0, E) does not depend on the dimension. However, because
the typical distance to a sample from the Gaussian is σ√n the
ratio between the distance from x0 to A and the distance from x0
to B shrinks as the dimension increases.
It will be useful to keep the following intuitive picture in
mind. In high dimensions, most samples from the Gaussian
distribution N(x0; σ2I) lie close to the surface of a sphere
of radius σ centered at x0. The decision boundary of a
linear model is a plane, and since we are assuming that the
“correct” label for each noisy point is the same as the label
for x0, our error set is simply the half-space on the far side
of this plane.
The relationship between adversarial and corruption robustness corresponds to a simple geometric picture. If we slice a
sphere with a plane, as in Figure 1, the distance to the nearest error is equal to the distance from the plane to the center
of the sphere, and the corruption robustness is the fraction
of the surface area cut off by the plane. This relationship
changes drastically as the dimension increases: most of the
surface area of a high-dimensional sphere lies very close
to the equator, which means that cutting off even, say, 1%
of the surface area requires a plane which is very close to
the center. Thus, for a linear model, even a relatively small
error rate on Gaussian noise implies the existence of errors
very close to the clean image (i.e., an adversarial example).
To formalize this relationship, pick some clean image x0
and consider the Gaussian distribution N(x0; σ2I). For a
ﬁxed µ, let σ(x0, µ) be the σ for which the error rate is µ,
that is, for which
Ex∼N(x0;σ2I)[x ∈E] = µ.
Then, letting d denote l2 distance, we have
d(x0, E) = −σ(x0, µ)Φ−1(µ),
exp(−x2/2)dx
is the cdf of the univariate standard normal distribution.
(Note that Φ−1(µ) is negative when µ < 1
This expression depends only on the error rate µ and the
standard deviation σ of a single component, and not directly
on the dimension, but the dimension appears if we consider
the distance from x0 to a typical sample from N(x0; σ2I),
which is σ√n. When the dimension is large the distance to
the decision boundary will be signiﬁcantly smaller than the
distance to a noisy image.
For example, this formula says that a linear model with an
error rate of 0.01 in noise with σ = 0.1 will have an error at
distance about 0.23. In three dimensions, a typical sample
from this noise distribution will be at a distance of around
3 ≈0.17. However when n = 3072, the dimension of
the CIFAR-10 image space, these samples lie at a distance of
about 5.54. So, in the latter case, a 1% error rate on random
perturbations of size 5.54 implies an error at distance 0.23,
more than 20 times closer. Detailed curves showing this
relationship can be found in Appendix F.
Comparing Neural Networks to the Linear Case. The
decision boundary of a neural network is, of course, not
linear. However, by comparing the ratio between d(x0, E)
and σ(x0, µ) for neural networks to what it would be for a
linear model, we can investigate the relationship between adversarial and corruption robustness. We ran experiments on
several neural network image classiﬁers and found results
that closely resemble Equation 1. Adversarial examples
therefore are not “surprisingly” close to x0 given the performance of each model in Gaussian noise.
Concretely, we examine this relationship when µ = 0.01.
For each test point, we compare σ(x0, 0.01) to an estimate
of d(x0, E). Because it is not feasible to compute d(x0, E)
exactly, we instead search for an error using PGD and report the nearest error we can ﬁnd.
Figure 2 shows the results for several CIFAR-10 and ImageNet models, including ordinarily trained models, models
trained with Gaussian data augmentation with σ = 0.4, and
an adversarially trained CIFAR-10 model. We also included
a line representing how these quantities would be related for
a linear model, as in Equation 1. Because most test points
lie close to the predicted relationship for a linear model, we
see that the half-space model shown in Figure 1 accurately
predicts the existence of small perturbation adversarial examples.
Adversarial Examples Are a Natural Consequence of Test Error in Noise
Figure 2. (Top) Comparing the l2 distance to the decision boundary with the σ for which the error rate in Gaussian noise is 1%. Each
point represents 50 images from the test set, and the median values for each coordinate are shown. The error bars cover the 25th to
75th percentiles. The PGD attack was run with ϵ = 1, so the distances to the decision boundary reported here are cut off at 1. (Bottom)
Histograms of the x coordinates from the above plots. A misclassiﬁed point is assigned σ = 0.
It is interesting to observe how each training procedure
affected the two quantities we measured. First, adversarial
training and Gaussian data augmentation increased both
σ(x0, 0.01) and d(x0, E) on average. The adversarially
trained model deviates from the linear case the most, but it
does so in the direction of greater distances to the decision
boundary. While both augmentation methods do improve
both quantities, Gaussian data augmentation had a greater
effect on σ (as seen in the histograms) while adversarial
training had a greater effect on d. We explore this further in
Section 6.
Visual Conﬁrmation of the Half-space Model In Figure 3
we draw two-dimensional slices in image space through
three points. , and are called “church window plots.”)
This visualized decision boundary closely matches the halfspace model in Figure 1. We see that an error found in
Gaussian noise lies in the same connected component of the
error set as an error found using PGD, and that at this scale
that component visually resembles a half-space. This ﬁgure
also illustrates the connection between adversarial example
research and corruption robustness research. To measure
adversarial robustness is to ask whether or not there are any
errors in the l∞ball — the small diamond-shaped region
in the center of the image — and to measure corruption
robustness is to measure the volume of the error set in the
deﬁned noise distribution. At least in this slice, nothing
distinguishes the PGD error from any other point in the
error set apart from its proximity to the clean image.
We give many more church window plots in Appendix G.
5. Concentration of Measure for Noisy Images
There is an existing research program which proves
hard upper bounds on adversarial robustness in terms of
the error rate of a model. This phenomenon is sometimes
called concentration of measure. Because proving a theorem like this requires understanding the distribution in
question precisely, these results typically deal with simple
“toy” distributions rather than those corresponding to real
data. In this section we take a ﬁrst step toward bridging this
gap. By comparing our models to a classical concentration
of measure bound for the Gaussian distribution, we gain
another perspective on our motivating question.
The Gaussian Isoperimetric Inequality. As in Section 4,
let x0 be a correctly classiﬁed image and consider the distribution q = N(x0; σ2I). Note q is the distribution of
random Gaussian perturbations of x0. The previous section
discussed the distance from x0 to its nearest error. In this
Adversarial Examples Are a Natural Consequence of Test Error in Noise
Figure 3. Two-dimensional slices of image space together with the classes assigned by trained models. Each slice goes through three
points, a clean image from the test set (black), an error found by randomly perturbing the center image with Gaussian noise (blue), and an
error found using a targeted PGD attack (red). The black circles have radius σ√n, indicating the typical size of the Gaussian perturbation
used. The diamond-shaped region in the center of the right image shows the l∞ball of radius 8/255. In both slices, the decision boundary
resembles a half-space as predicted in Figure 1, demonstrating how non-zero error rate in noise predicts the existence of small adversarial
perturbations. The CIFAR-10 model on the left was evaluated with σ = 0.04 (black circle has radius 2.22), where 0.21% of Gaussian
perturbations are classiﬁed as “frog” (cyan region). The adversarial error was found at distance 0.159 while the half-space model predicts
errors at distance 0.081. The ImageNet model on the right was evaluated at σ = 0.08 (black circle has radius 31.4) where 0.1% of
Gaussian perturbations were misclassiﬁed as “miniture poodle” (cyan). The adversarial error has distance 0.189 while the half-space
model predicts errors at distance 0.246. For the panda picture on the right we also found closer errors than what is shown by using an
untargeted attack (an image was assigned class “indri” at distance 0.024). Slices showing more complicated behavior can be found in
Appendix G.
section we will instead discuss the distance from a typical
sample from q (e.g. point B in Figure 1) to its nearest error.
For random samples from q, there is a precise sense in which
small adversarial perturbations exist only because test error
is nonzero. That is, given the error rates we actually observe
on noisy images, most noisy images must be close to the
error set. This result holds completely independently of any
assumptions about the model and follows from a fundamental geometric property of the Gaussian distribution, which
we will now make precise.
q(E) be the median distance from one of these noisy
images to the nearest error. (In other words, it is the ϵ for
which Px∼q[d(x, E) ≤ϵ] = 1
2.) As before, let Px∼q[x ∈
E] be the probability that a random Gaussian perturbation of
x0 lies in E. It is possible to deduce a bound relating these
two quantities from the Gaussian isoperimetric inequality
 . The form we will use is:
Theorem (Gaussian Isoperimetric Inequality). Let q =
N(0; σ2I) be the Gaussian distribution on Rn with variance σ2I, and, for some set E ⊆Rn, let µ = Px∼q[x ∈E].
As before, write Φ for the cdf of the univariate standard
normal distribution. If µ ≥1
2, then ϵ∗
q(E) = 0. Otherwise,
q(E) ≤−σΦ−1(µ), with equality when E is a half space.
In particular, for any machine learning model for which
the error rate in the distribution q is at least µ, the median
distance to the nearest error is at most −σΦ−1(µ). Because
each coordinate of a multivariate normal is a univariate
normal, −σΦ−1(µ) is the distance to a half space for which
the error rate is µ. In other words, the right hand side of the
inequality is the same expression that appears in Equation 1.
So, among models with some ﬁxed error rate Px∼q[x ∈E],
the most robust are the ones whose error set is a half space
(as shown in Figure 1). In Appendix E we will give a more
common statement of the Gaussian isoperimetric inequality
along with a proof of the version presented here.
Comparing Neural Networks to the Isoperimetric
Bound. We evaluated these quantities for several models
on the CIFAR-10 and ImageNet test sets.
As in Section 4, we report an estimate of ϵ∗
q. For each
test image, we took 1,000 samples from the corresponding
Gaussian and estimated ϵ∗
q using PGD with 200 steps on
each sample and reported the median.
We ﬁnd that for the ﬁve models we considered, the relationship between our estimate of ϵ∗
q(E) and Px∼q[x ∈E]
is already close to optimal. This is visualized in Figure 4.
For CIFAR-10, adversarial training improves robustness to
small perturbations, but the gains are primarily because error rates in Gaussian noise were improved. In particular, it is
clear from the graph on the bottom left that adversarial training increases the σ at which the error rate is 1% on average.
This shows that improved adversarial robustness results in
Adversarial Examples Are a Natural Consequence of Test Error in Noise
Figure 4. These plots give two ways to visualize the relationship between the error rate in noise and the distance from noisy points to the
decision boundary (found using PGD). Each point on each plot represents one image from the test set. On the top row, we compare the
error rate of the model with Gaussian perturbations at σ = 0.1 to the distance from the median noisy point to its nearest error. On the
bottom row, we compare the σ at which the error rate is 0.01 to this same median distance. (These are therefore similar to the plots in
Figure 2.) The thick black line at the top of each plot is the upper bound provided by the Gaussian isoperimetric inequality. We include
data from a model trained on clean images, an adversarially trained model, and a model trained on Gaussian noise (σ = 0.4.)
improved robustness to large random perturbations, as the
isoperimetric inequality says it must.
6. Evaluating Corruption Robustness
The previous two sections show a relationship between adversarial robustness and one type of corruption robustness.
This suggests that methods designed to improve adversarial
robustness ought to also improve corruption robustness, and
vice versa. In this section we investigate this relationship.
We analyzed the performance of our models on the corruption robustness benchmark described in Hendrycks &
Dietterich . There are 15 different corruptions in this
benchmark, each of which is tested at ﬁve different levels
of severity. The results are summarized in Figure 6, where
we have aggregated the corruption types based on whether
the ordinarily trained model did better or worse than the
augmented models. We found a signiﬁcant difference in performance on this benchmark when the model is evaluated on
the compressed images provided with the benchmark rather
than applying the corruptions in memory. (In this section
we report performance on corruptions applied in-memory.)
Figure 5 shows an example for the Gaussian-5 corruption,
where performance degraded from 57% accuracy (in memory) to 10% accuracy (compressed images). Detailed results
on both versions of this benchmark are presented in Appendix B.
Gaussian data augmentation and adversarial training both
improve the overall benchmark1, which requires averaging
the performance across all corruptions, and the results were
quite close. Adversarial training helped more with blurring corruptions and Gaussian data augmentation helped
more with noise corruptions. Interestingly, both methods
performed much worse than the clean model on the fog and
contrast corruptions. For example, the adversarially trained
model was 55% accurate on the most severe contrast corruption compared to 85% for the clean model. Note that
Hendrycks & Dietterich also observed that adversarial training improves robustness on this benchmark on Tiny
The fact that adversarial training is so successful against the
noise corruptions further supports the connection we have
been describing. For other corruptions, the relationship is
more complicated, and it would be interesting to explore
this in future work.
1In reporting overall performance on this benchmark, we omit
the Gaussian noise corruption.
Adversarial Examples Are a Natural Consequence of Test Error in Noise
Figure 5. Performance on the Imagenet-C corruptions may vary dramatically depending on whether or not the model is evaluated on the
publicly released compressed images vs applying the corruptions directly in memory. For example, an InceptionV3 model trained with
Gaussian data augmentation was 57% accurate on the Gaussian-5 corruption when evaluated in memory (example image left). This same
model was only 10% accurate on the publicly released compressed images (example image right). The model prediction and conﬁdence
on each image is also shown. Note the image on the right was not modiﬁed adversarially, instead the drop in model performance is
due entirely to subtle compression artifacts. This severe degradation in model performance is particularly surprising because differences
between the compressed and uncompressed images are difﬁcult to spot for a human. This demonstrates the extreme brittleness of neural
networks to distributional shift.
We also evaluated these two augmentation methods on standard measures of lp robustness. We see a similar story there:
while adversarial training performs better, Gaussian data
augmentation does improve adversarial robustness as well.
Gaussian data augmenation has been proposed as an adversarial defense in prior work . Here
we evaluate this method not to propose it as a novel defense
but to provide further evidence of the connection between
adversarial and corruption robustness.
We also considered the MNIST adversarially trained model
from Madry et al. , and found it to be a special case
where robustness to small perturbations was increased while
generalization in noise was not improved (see Appendix D).
This is because this model violates the linearity assumption
discussed in Section 4.
Corruption Robustness as a Sanity Check for Defenses.
We also analyzed the performance several previously published adversarial defense strategies in Gaussian noise.
These methods have already been shown to result in vanishing gradients, which causes standard optimization procedures to fail to ﬁnd errors, rather than actually improving
adversarial robustness . We ﬁnd that
these methods also show no improvement in Gaussian noise.
The results are shown in Figure 7. Had these prior defenses
performed an analysis like this, they would have been able
to determine that their methods relied on vanishing gradients
and fail to improve robustness.
Obtaining Zero Test Error in Noise is Nontrivial. It is
important to note that applying Gaussian data augmentation does not reduce error rates in Gaussian noise to zero.
For example, we performed Gaussian data augmentation on
CIFAR-10 at σ = .15 and obtained 99.9% training accuracy
but 77.5% test accuracy in the same noise distribution. (For
comparison, the naturally trained obtains 95% clean test
accuracy.) Previous work has also
observed that obtaining perfect generalization in large Gaussian noise is nontrivial. This mirrors Schmidt et al. ,
which found that adversarial robustness did not generalize
to the test set, providing yet another similarity between adversarial and corruption robustness. This is perhaps not
surprising given that error rates on the clean test set are also
non-zero. Although the model is in some sense “superhuman” with respect to clean test accuracy, it still makes many
mistakes on the clean test set that a human would never
make. We collected some examples in Appendix I. More
detailed results on training and testing in noise can be found
in Appendices C and H.
Adversarial Examples Are a Natural Consequence of Test Error in Noise
Blurring Corruptions
Noise Corruptions
Fog and Contrast
All Corruptions
Top 1 Accuracy %
Corruption Robustness (ImageNet)
naturally trained
gaussian data augmentation (sigma=.4)
Figure 6. The performance of the models we considered on the corruption robustness benchmark, together with our measurements of
those models’ robustness to small lp perturbations. For all the robustness tests we used PGD with 100 steps and a step size of ϵ/25. The
adversarially trained CIFAR-10 model is the open sourced model from Madry et al. .
7. Conclusion
This paper investigates whether we should be surprised to
ﬁnd adversarial examples as close as we do, given the error
rates we observe in corrupted image distributions. After
running several experiments, we argue that the answer to
this question is no. Speciﬁcally:
1. The nearby errors we can ﬁnd show up at the same
distance scales we would expect from a linear model
with the same corruption robustness.
2. Concentration of measure shows that a non-zero error
rate in Gaussian noise logically implies the existence
of small adversarial perturbations of noisy images.
3. Finally, training procedures designed to improve adversarial robustness also improve many types of corruption robustness, and training on Gaussian noise moderately improves adversarial robustness.
In light of this, we believe it would be beneﬁcial for the
adversarial defense literature to start reporting generalization to distributional shift, such as the common corruption
benchmark introduced in Hendrycks & Dietterich ,
in addition to empirical estimates of adversarial robustness.
There are several reasons for this recommendation.
First, a varied suite of corruptions can expose failure modes
of a model that we might otherwise miss. For example, we
found that adversarial training signiﬁcantly degraded performance on the fog and contrast corruptions despite improving
small perturbation robustness. In particular, performance
on constrast-5 dropped to 55.3% accuracy vs 85.7% for the
vanilla model (see Appendix B for more details).
Second, measuring corruption robustness is signiﬁcantly
easier than measuring adversarial robustness — computing
adversarial robustness perfectly requires solving an NPhard problem for every point in the test set . Since Szegedy et al. , hundreds of adversarial
defense papers have been published. To our knowledge, only
one has reported robustness numbers
which were conﬁrmed by a third party. We believe the
difﬁculty of measuring robustness under the usual deﬁnition
has contributed to this unproductive situation.
Third, all of the failed defense strategies we examined also
Adversarial Examples Are a Natural Consequence of Test Error in Noise
Figure 7. (Left) The performance in Gaussian noise of the CIFAR models described in this paper. (Right) The performance in Gaussian
noise of several previously published defenses for ImageNet, along with an Imagenet model trained on Gaussian noise at σ = 0.4 for
comparison. For each point we ran ten trials; the error bars show one standard deviation. All of these defenses are now known not
to improve adversarial robustness . The defense strategies include bitdepth reduction , JPEG
compression , Pixel Deﬂection
 , total variance minimization , respresentation-guided denoising , and random
resizing and random padding of the input image .
failed to improve performance in Gaussian noise. For this
reason, we should be highly skeptical of defense strategies
that only claim improved lp robustness but are unable to
demonstrate robustness to distributional shift.
Finally, if the goal is improving the security of our models
in adversarial settings, errors on corrupted images already
imply that our models are not secure. Until our models are
perfectly robust in the presence of average-case corruptions,
they will not be robust in worst-case settings.
The communities of researchers studying adversarial and
corruption robustness seem to be attacking essentially the
same problem in two different ways. We believe that the
corruption robustness problem is also interesting independently of its connection to adversarial examples, and we
hope that the results presented here will encourage more
collaboration between these two communities.