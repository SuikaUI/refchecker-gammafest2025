NAS-FPN: Learning Scalable Feature Pyramid Architecture
for Object Detection
Golnaz Ghaisi
Tsung-Yi Lin
Ruoming Pang
Quoc V. Le
Google Brain
{golnazg,tsungyi,rpang,qvl}@google.com
Current state-of-the-art convolutional architectures for
object detection are manually designed. Here we aim to
learn a better architecture of feature pyramid network for
object detection. We adopt Neural Architecture Search and
discover a new feature pyramid architecture in a novel
scalable search space covering all cross-scale connections.
The discovered architecture, named NAS-FPN, consists of
a combination of top-down and bottom-up connections to
fuse features across scales. NAS-FPN, combined with various backbone models in the RetinaNet framework, achieves
better accuracy and latency tradeoff compared to state-ofthe-art object detection models. NAS-FPN improves mobile
detection accuracy by 2 AP compared to state-of-the-art SS-
DLite with MobileNetV2 model in and achieves 48.3
AP which surpasses Mask R-CNN detection accuracy
with less computation time.
1. Introduction
Learning visual feature representations is a fundamental
problem in computer vision. In the past few years, great
progress has been made on designing the model architecture of deep convolutional networks (ConvNets) for image
classiﬁcation and object detection . Unlike image classiﬁcation which predicts class probability for
an image, object detection has its own challenge to detect
and localize multiple objects across a wide range of scales
and locations. To address this issue, the pyramidal feature
representations, which represent an image with multiscale
feature layers, are commonly used by many modern object
detectors .
Feature Pyramid Network (FPN) is one of the representative model architectures to generate pyramidal feature representations for object detection. It adopts a backbone model, typically designed for image classiﬁcation, and
builds feature pyramid by sequentially combining two adjacent layers in feature hierarchy in backbone model with
Figure 1: Average Precision vs. inference time per image
across accurate models (top) and fast models (bottom) on
mobile device. The green curve highlights results of NAS-
FPN combined with RetinaNet. Please refer to Figure 9 for
top-down and lateral connections. The high-level features,
which are semantically strong but lower resolution, are upsampled and combined with higher resolution features to
generate feature representations that are both high resolution and semantically strong. Although FPN is simple
and effective, it may not be the optimal architecture design. Recently, PANet shows adding an extra bottomup pathway on FPN features improves feature represenarXiv:1904.07392v1 [cs.CV] 16 Apr 2019
tations for lower resolution features. Many recent works
 propose various crossscale connections or operations to combine features to generate pyramidal feature representations.
The challenge of designing feature pyramid architecture
is in its huge design space. The number of possible connections to combine features from different scales grow exponentially with the number of layers. Recently, Neural
Architecture Search algorithm demonstrates promising
results on efﬁciently discovering top-performing architectures for image classiﬁcation in a huge search space. To
achieve their results, Zoph et al. propose a modularized
architecture that can be repeated and stacked into a scalable
architecture. Inspired by , we propose the search space
of scalable architecture that generates pyramidal representations. The key contribution of our work is in designing
the search space that covers all possible cross-scale connections to generate multiscale feature representations. During
the search, we aims to discover an atomic architecture that
has identical input and output feature levels and can be applied repeatedly. The modular search space makes searching pyramidal architectures manageable. Another beneﬁt
of modular pyramidal architecture is the ability for anytime
object detection (or “early exit”). Although such early exit
approach has been attempted , manually designing such
architecture with this constraint in mind is quite difﬁcult.
The discovered architecture, named NAS-FPN, offers
great ﬂexibility in building object detection architecture.
NAS-FPN works well with various backbone model, such
as MobileNet , ResNet , and AmoebaNet . It
offers better tradeoff of speed and accuracy for both fast
mobile model and accurate model. Combined with MobileNetV2 backbone in RetinaNet framework, it outperforms state-of-the-art mobile detection model of SSDLite
with MobilenetV2 by 2 AP given the same inference
time. With strong AmoebaNet-D backbone model, NAS-
FPN achieves 48.3 AP single model accuracy with single
testing scale. The detection accuracy surpasses Mask R-
CNN reported in with even less inference time. A summary of our results is shown in Figure 1.
2. Related Works
2.1. Architecture for Pyramidal Representations
Feature pyramid representations are the basis of solutions for many computer vision applications required multiscale processing .
However, using Deep ConvNets
to generate pyramidal representations by featurizing image
pyramid imposes large computation burden. To address this
issue, recent works on human pose estimation, image segmentation, and object detection introduce cross-scale connections in ConvNets that connect internal feature layers in different scales. Such connections
effectively enhance feature representations such that they
are not only semantically strong but also contain high resolution information. Many works have studied how to improve mutliscale feature presentations. Liu et.al propose an additional bottom-up pathway based on FPN .
Recently, Zhao et al. extends the idea to build stronger
feature pyramid representations by employing multiple Ushape modules after a backbone model. Kong et al. 
ﬁrst combine features at all scales and generate features at
each scale by a global attention operation on the combined
features. Despite it is an active research area, most architecture designs of cross-scale connections remain shallow
compared to the backbone model. In addition to manually
design the cross-scale connections, propose to learn
the connections through gating mechanism for visual counting and dense label predictions.
In our work, instead of manually designing architectures
for pyramidal representations, we use a combination of scalable search space and Neural Architecture Search algorithm
to overcome the large search space of pyramidal architectures. We constrain the search to ﬁnd an architecture that
can be applied repeatedly. The architecture can therefore
be used for anytime object detection (or “early exit”). Such
early exit idea is related to , especially in image classiﬁcation .
2.2. Neural Architecture Search
Our work is closely related to the work on Neural Architecture Search . Most notably, Zoph et al. 
use a reinforcement learning with a controller RNN to design a cell (or a layer) to obtain a network, called NASNet
which achieves state-of-the-art accuracy on ImageNet. The
efﬁciency of the search process is further improved by 
to design a network called PNASNet, with similar accuracy
to NASNet. Similarly, an evolution method has also
been used to design AmoebaNets that improve upon NAS-
Net and PNASNet. Since reinforcement learning and evolution controllers perform similarly well, we only experiment
with a Reinforcement Learning controller in this paper. Our
method has two major differences compared to : (1)
the outputs of our method are multiscale features whereas
output of is single scale features for classiﬁcation; (2)
our method speciﬁcally searches cross-scale connections,
while only focuses on discovering connections within
the same feature resolution. Beyond image classiﬁcation,
Neural Architecture Search has also been used to improve
image segmentation networks . To the best of our knowledge, our work is the ﬁrst to report success of applying Neural Architecture Search for pyramidal architecture in object
detection. For a broader overview of related methods for
Neural Architecture Search, please see .
Our method is based on the RetinaNet framework 
because it is simple and efﬁcient. The RetinaNet framework
has two main components: a backbone network (often stateof-the-art image classiﬁcation network) and a feature pyramid network (FPN). The goal of the proposed algorithm is
to discover a better FPN architecture for RetinaNet. Figure 2 shows the RetinaNet architecture.
Figure 2: RetinaNet with NAS-FPN. In our proposal, feature pyramid network is to be searched by a neural architecture search algorithm. The backbone model and the subnets
for class and box predictions follow the original design in
RetinaNet . The architecture of FPN can be stacked N
times for better accuracy.
To discover a better FPN, we make use of the Neural Architecture Search framework proposed by . The Neural
Architecture Search trains a controller to select best model
architectures in a given search space using reinforcement
learning. The controller uses the accuracy of a child model
in the search space as the reward signal to update its parameters. Thus through trial and error the controller learns to
generate better architectures over time. As it has been identiﬁed by previous works , the search space plays
a crucial role in the success of architecture search.
In the next section, we design a search space for FPN to
generate feature pyramid representations. For scalability of
the FPN (i.e., so that an FPN architecture can be stacked repeatedly within RetinaNet), during the search, we also force
the the FPN to repeat itself N times and then concatenated
into a large architecture. We call our feature pyramid architecture NAS-FPN.
3.1. Architecture Search Space
In our search space, a feature pyramid network consists a
number of “merging cells” that combine a number of input
layers into representations for RetinaNet. In the following,
we will describe the inputs into the Feature Pyramid Network, and how each merging cell is constructed.
Feature Pyramid Network.
A feature pyramid network
takes multiscale feature layers as inputs and generate output feature layers in the identical scales as shown in Figure
2. We follow the design by RetinaNet which uses the
last layer in each group of feature layers as the inputs to the
ﬁrst pyramid network. The output of the ﬁrst pyramid network are the input to the next pyramid network. We use as
inputs features in 5 scales {C3, C4, C5, C6, C7} with corresponding feature stride of {8, 16, 32, 64, 128} pixels. The
C6 and C7 are created by simply applying stride 2 and stride
4 max pooling to C5. The input features are then passed to a
pyramid network consisting of a series of merging cells (see
below) that introduce cross-scale connections. The pyramid
network then outputs augmented multiscale feature representations {P3, P4, P5, P6, P7}. Since both inputs and outputs of a pyramid network are feature layers in the identical
scales, the architecture of the FPN can be stacked repeatedly
for better accuracy. In Section 4, we show controlling the
number of pyramid networks is one simple way to tradeoff
detection speed and accuracy.
Merging cell.
An important observation in previous
works in object detection is that it is necessary to “merge”
features at different scales. The cross-scale connections allow model to combine high-level features with strong semantics and low-level features with high resolution.
We propose merging cell, which is a fundamental building block of a FPN, to merge any two input feature layers into a output feature layer. In our implementation, each
merging cell takes two input feature layers (could be from
different scales), applies processing operations and then
combines them to produce one output feature layer of a desired scale. A FPN consists of N different merging cells,
where N is given during search. In a merging cell, all feature layers have the same number of ﬁlters. The process of
constructing a merging cell is shown in Figure 3.
feature layers
merging cell
Figure 3: Four prediction steps required in a merging cell.
Note the output feature layer is pushed back into the stack
of candidate feature layers and available for selection for
the next merging cell.
The decisions of how to construct the merging cell are
made by a controller RNN. The RNN controller selects any
two candidate feature layers and a binary operation to combine them into a new feature layer, where all feature layers
may have different resolution. Each merging cell has 4 prediction steps made by distinct softmax classiﬁers:
Step 1. Select a feature layer hi from candidates.
Select another feature layer hj from candidates
without replacement.
Step 3. Select the output feature resolution.
Step 4. Select a binary op to combine hi and hj selected
in Step 1 and Step 2 and generate a feature layer with the
resolution selected in Step 3.
In step 4, we design two binary operations, sum and
global pooling, in our search space as shown in Figure 4.
These two operations are chosen for their simplicity and ef-
ﬁciency. They do not add any extra trainable parameters.
The sum operation is commonly used for combining features . The design of global pooling operation is inspired by . We follow Pyramid Attention Networks 
except removing convolution layers in the original design.
The input feature layers are adjusted to the output resolution
by nearest neighbor upsampling or max pooling if needed
before applying the binary operation. The merged feature
layer is always followed by a ReLU, a 3x3 convolution, and
a batch normalization layer.
(b) Global pooling
Figure 4: Binary operations.
The input feature layers to a pyramid network form the
initial list of input candidates of a merging cell. In Step 5,
the newly-generated feature layer is appended to the list of
existing input candidates and becomes a new candidate for
the next merging cell. There can be multiple candidate features share the same resolution during architecture search.
To reduce computation in discovered architecture, we avoid
selecting stride 8 feature in Step 3 for intermediate merging cells. In the end, the last 5 merging cells are designed
to outputs feature pyramid {P3, P4, P5, P6, P7}. The order
of output feature levels is predicted by the controller. Each
output feature layer is then generated by repeating the step
1, 2, 4 until the output feature pyramid is fully generated.
Similar to , we take all feature layers that have not been
connected to any of output layer and sum them to the output
layer that has the corresponding resolution.
3.2. Deeply supervised Anytime Object Detection
One advantage of scaling NAS-FPN with stacked pyramid networks is that the feature pyramid representations
can be obtained at output of any given pyramid network.
This property enables anytime detection which can generate detection results with early exit. Inspired by ,
we can attach classiﬁer and box regression heads after all
intermediate pyramid networks and train it with deep supervision . During inference, the model does not need
to ﬁnish the forward pass for all pyramid networks. Instead,
it can stop at the output of any pyramid network and generate detection results. This can be a desirable property when
computation resource or latency is a concern and provides
a solution that can dynamically decide how much computation resource to allocate for generating detections. In Appendix A, we show NAS-FPN can be used for anytime detection.
4. Experiments
In this section, we ﬁrst describe our experiments of Neural Architecture Search to learn a RNN controller to discover the NAS-FPN architecture. Then we demonstrate the
discovered NAS-FPN works well with different backbone
models and image sizes. The capacity of NAS-FPN can be
easily adjusted by changing the number of stacking layers
and the feature dimension in pyramid network. We show
how to build accurate and fast architectures in the experiments.
4.1. Implementation Details
We use the open-source implementation of RetinaNet1
for experiments. The models are trained on TPUs with 64
images in a batch. During training, we apply multiscale
training with a random scale between [0.8, 1.2] to the output image size. The batch normalization layers are applied
after all convolution layers. We use α = 0.25 and γ = 1.5
for focal loss. We use a weight decay of 0.0001 and a momentum of 0.9. The model is trained using 50 epochs. The
initial learning rate 0.08 is applied for ﬁrst 30 epochs and
decayed 0.1 at 30 and 40 epochs. For experiments with
DropBlock , we use a longer training schedule of 150
epochs with ﬁrst decay at 120 and the second decay at 140
epochs. The step-wise learning rate schedule was not stable
for training our model with AmoebaNet backbone on image
size of 1280x1280 and for this case we use cosine learning
rate schedule. The model is trained on COCO train2017 and
evaluated on COCO val2017 for most experiments. In Table 1, we report test-dev accuracy to compare with existing
4.2. Architecture Search for NAS-FPN
Proxy task.
To speed up the training of the RNN controller we need a proxy task that has a short training
time and also correlates with the real task. The proxy task
can then be used during the search to identify a good FPN
architecture. We ﬁnd that we can simply shorten the training of target task and use it as the proxy task. We only
1 
number of sampled models
total unique architectures sampled
total architectures sampled
Figure 5: Left: Rewards over RL training. The reward is
computed as the AP of sampled architectures on the proxy
task. Right: The number of sampled unique architectures
to the total number of sampled architectures. As controller
converges, more identical architectures are sampled by the
controller.
train the proxy task for 10 epochs, instead of 50 epochs that
we use to train RetinaNet to converge. To further speed up
training proxy task, we use a small backbone architecture of
ResNet-10 with input 512 × 512 image size. With these reductions, the training time is 1hr for a proxy task on TPUs.
We repeat the pyramid networks 3 times in our proxy task.
The initial learning rate 0.08 is applied for ﬁrst 8 epochs
and decayed by the factor of 0.1 at epoch 8. We reserve a
randomly selected 7392 images from the COCO train2017
set as the validation set, which we use to obtain rewards.
Controller.
Similar to our controller is a recurrent
neural network (RNN) and it is trained using the Proximal
Policy Optimization (PPO) algorithm. The controller
samples child networks with different architectures. These
architectures are trained on a proxy task using a pool of
workers. The workqueue in our experiments consisted of
100 Tensor Processing Units (TPUs). The resulting detection accuracy in average precision (AP) on a held-out validation set is used as the reward to update the controller.
Figure 5-Left shows the AP of the sampled architectures
for different iterations of training. As it can be seen the
controller generated better architectures over time. Figure
5-Right shows total number of sampled architectures and
also the total number of unique architectures generated by
the RNN controller. The number of unique architectures
converged after about 8000 steps. We use the architecture
with the highest AP from all sampled architectures during
RL training in our experiments. This architecture is ﬁrst
sampled at 8000 step and sampled many times after that.
Figure 6 shows the details of this architecture.
Discovered feature pyramid architectures.
What makes
a good feature pyramid architecture? We hope to shed lights
on this question by visualizing the discovered architectures.
In Figure 7(b-f), we plot NAS-FPN architectures with progressively higher reward during RL training. We ﬁnd the
RNN controller can quickly pick up some important crossscale connections in the early learning stage. For example, it discovers the connection between high resolution input and output feature layers, which is critical to generate
high resolution features for detecting small objects. As the
controller converges, the controller discovers architectures
that have both top-down and bottom-up connections which
is different from vanilla FPN in Figure 7(a). We also ﬁnd
better feature reuse as the controller converges. Instead of
randomly picking any two input layers from the candidate
pool, the controller learns to build connections on newlygenerated layers to reuse previously computed feature representations.
4.3. Scalable Feature Pyramid Architecture
In this section, we show how to control the model capacity by adjusting (1) backbone model, (2) the number of
repeated pyramid networks, and (3) the number of dimension in pyramid network. We discuss how these adjustments
tradeoff computational time and speed. We deﬁne a simple
notation to indicate backbone model and NAS-FPN capacity. For example, R-50, 5 @ 256 indicate a model using
ResNet-50 backbone model, 5 stacked NAS-FPN pyramid
networks, and 256 feature dimension.
Stacking pyramid networks.
Our pyramid network has a
nice property that it can be scaled into a larger architecture
by stacking multiple repeated architectures. In Figure 8a,
we show that stacking the vanilla FPN architecture does not
always improve performance whereas stacking NAS-FPN
improves accuracy signiﬁcantly. This result highlights our
search algorithm can ﬁnd scalable architectures, which may
be hard to design manually. Interestingly, although we only
apply 3 pyramid networks for the proxy task during the architecture search phase, the performance still improves with
up to 7 pyramid networks applied.
Adopting different backbone architectures.
One common way to tradeoff accuracy and speed for object detection architectures is altering the backbone architecture. Although the pyramid network in NAS-FPN was discovered
by using a light-weight ResNet-10 backbone architecture,
we show that it can be transferred well across different
backbone architectures. Figure 8b shows the performance
of NAS-FPN on top of different backbones, from a lighter
weight architecture such as MobilenetV2 to a very high capacity architecture such as AmoebaNet-D . When we
apply NAS-FPN with MobilenetV2 on the image size of
640 × 640, we get 36.6 AP with 160B FLOPs. Using stateof-the-art image classiﬁcation architecture of AmoebaNet-
D as the backbone increases the FLOPs to 390B but
also adds about 5 AP. NAS-FPN with both light and heavy
backbone architectures beneﬁts from stacking more pyramid networks.
Figure 6: Architecture of the discovered 7-merging-cell pyramid network in NAS-FPN with 5 input layers (yellow) and 5
output feature layers (blue). GP and R-C-B are stands for Global Pooling and ReLU-Conv-BatchNorm, respectively.
(b) NAS-FPN / 7.5 AP
(c) NAS-FPN / 9.9 AP
(f) NAS-FPN / 16.8 AP
(e) NAS-FPN / 16.0 AP
(d) NAS-FPN / 15.0 AP
Figure 7: Architecture graph of NAS-FPN. Each dot represents a feature layer. Feature layers in the same row have identical
resolution. The resolution decreases in the bottom-up direction. The arrows indicate the connections between internal layers.
The graph is constructed such that an input layer is on the left side. The inputs to a pyramid network are marked with green
circles and outputs are marked with red circles. (a) The baseline FPN architecture. (b-f) The 7-cell NAS-FPN architectures
discovered by Neural Architecture Search over training of the RNN controller. The discovered architectures converged as the
reward (AP) of the proxy task progressively improves. (f) The ﬁnal NAS-FPN that we used in our experiments.
Adjusting feature dimension of feature pyramid networks.
Another way to increase the capacity of a model is
to increase the feature dimension of feature layers in NAS-
FPN. Figure 8c shows results of 128, 256, and 384 feature
dimension in NAS-FPN with a ResNet-50 backbone architecture. Not surprisingly, increasing the feature dimension
improves detection performance but it may not be an efﬁcient way to improve the performance. In Figure 8c, R-50
7 @ 256, with much less FLOPs, achieves similar AP compared to R-50 3 @ 384. Increasing feature dimension would
require model regularization technique. In Section 4.4, we
discuss using DropBlock to regularize the model.
Architectures for high detection accuracy.
scalable NAS-FPN architecture, we discuss how to build an
accurate model while remaining efﬁcient. In Figure 9a, we
ﬁrst show that NAS-FPN R-50 5 @256 model has comparable FLOPs to the R-101 FPN baseline but with 2.5 AP gain.
This shows using NA S-FPN is more effective than replacing the backbone with a higher capacity model. Going for
a higher accuracy model, one can use a heavier backbone
model or higher feature dimensions. Figure 9a shows that
NAS-FPN architectures are in the upper left part in the accuracy to inference time ﬁgure compared to existing methods.
The NAS-FPN is as accurate as to the state-of-the-art Mask
R-CNN model with less computation time.
FPN R-50 @256
NAS-FPN R-50 @256
(a) Number of pyramid networks
MobilenetV2 @ 256
R-101 @256
AmoebaNet-D @256
(b) Backbone architectures
(c) Feature dimension
Figure 8: The model capacity of NAS-FPN can be controlled with (a) stacking pyramid networks, (b) changing the backbone
architecture, and (c) increasing feature dimension in pyramid networks. All models are trained/tested on the image size of
640x640. Number above the marker indicates number of pyramid networks in NAS-FPN.
inference time (ms) on P100 GPU
@256,640 5 7
@384,1280+DB
FPN @256,640
FPN @256,1024
FPN @256,1280
backbone model
X-101-32x8d
X-101-64x4d
X-152-32x8d
@256,640 57
FPN @256,640
FPN @256,1024
FPN @256,1280
backbone model
Number of parameters (M)
FPN @256,640
FPN @256,1024
FPN @256,1280
backbone model
(a) Accurate models
inference time (ms) on Pixel 1 CPU
NAS-FPNLite @48 3
NAS-FPNLite @64
FPNLite @64
FPNLite @128
backbone model
MobilenetV2
MnasNet-92
NAS-FPNLite @48
NAS-FPNLite @64
FPNLite @128
FPNLite @64
backbone model
MobilenetV2
MnasNet-92
Number of parameters (M)
NAS-FPNLite @48
NAS-FPNLite @64
FPNLite @128
FPNLite @64
backbone model
MobilenetV2
MnasNet-92
(b) Fast models
Figure 9: Detection accuracy to inference time (left), FLOPs (middle), and parameters (right). (a) We compare to other high
accuracy models. The inference time of all models are computed on a machine with P100 GPU. The green curves highlights
results for NAS-FPN with different backbone architectures. The number above the marker indicates the number of repeats of
pyramid networks in NAS-FPN. The feature dimension of NAS-FPN/FPN and input image size are mentioned next to each
data point. (b) We compare to other fast models. The input image size of all models is 320x320 and the inference times are
computed on Pixel 1 CPU. Our model are trained with light-weight model of MobileNetV2.
Architectures for fast inference.
Designing object detector with low latency and limited computation budget is
an active research topic. Here, we introduce NAS-FPNLite
for mobile object detection. The major difference of NAS-
FPNLite and NAS-FPN is that we search a pyramid network that has outputs from P3 to P6. Also we follow SS-
DLite and replace convolution with depth-wise separable convolution in NAS-FPN. We discover a 15-cell architecture which yields good performance and use it in
our experiments.
We combine NAS-FPNLite and Momodel
image size
inference time (ms)
test-dev AP
YOLOv3 DarkNet-53 
22 (Titan X)
MobileNetV2 + SSDLite 
200 (Pixel 1 CPU)
MnasNet + SSDLite 
190 (Pixel 1 CPU)
MnasNet-92 + SSDLite 
227 (Pixel 1 CPU)
FPNLite MobileNetV2 @ 64
192 (Pixel 1 CPU)
FPNLite MobileNetV2 @ 128
264 (Pixel 1 CPU)
NAS-FPNLite MobileNetV2 (3 @ 48)
210 (Pixel 1 CPU)
NAS-FPNLite MobileNetV2 (7 @ 64)
285 (Pixel 1 CPU)
YOLOv3 DarkNet-53 
51 (Titan X)
CornerNet Hourglass 
244 (Titan X)
Mask R-CNN X-152-32x8d 
1280 × 800
325 (P100)
ReﬁneDet R-101 
90 (Titan X)
FPN R-50 @256 
37.5 (P100)
FPN R-101 @256 
51.1 (P100)
FPN R-50 @256 
1024 × 1024
73.0 (P100)
FPN R-101 @256 
1024 × 1024
83.7 (P100)
FPN AmoebaNet @256 
1280 × 1280
210.4 (P100)
NAS-FPN R-50 (7 @ 256)
56.1 (P100)
NAS-FPN R-50 (7 @ 256)
1024 × 1024
92.1 (P100)
NAS-FPN R-50 (7 @ 256)
1280 × 1280
131.9 (P100)
NAS-FPN R-50 (7 @ 384)
1280 × 1280
192.3 (P100)
NAS-FPN R-50 (7 @ 384) + DropBlock
1280 × 1280
192.3 (P100)
NAS-FPN AmoebaNet (7 @ 384)
1280 × 1280
278.9 (P100)
NAS-FPN AmoebaNet (7 @ 384) + DropBlock
1280 × 1280
278.9 (P100)
Table 1: Performance of RetinaNet with NAS-FPN and other state-of-the-art detectors on test-dev set of COCO.
bileNetV2 in RetinaNet framework. For a fair comparison, we create a FPNLite baseline, which follows the
original FPN structure and replaces all convolution layers
with depth-wise separable convolution. Following ,
we train NAS-FPNLite and FPNLite using an open-source
object detection API.2 In Figure 9b, we control the feature dimension of NAS-FPN to be 48 or 64 so that it has
similar FLOPs and CPU runtime on Pixel 1 as baseline
methods and show that NAS-FPNLite outperforms both SS-
DLite and FPNLite.
4.4. Further Improvements with DropBlock
Due to the increased number of new layers introduced
in NAS-FPN architecture, a proper model regularization is
needed to prevent overﬁtting. Following the technique in
 , we apply DropBlock with block size 3x3 after batch
normalization layers in the the NAS-FPN layers. Figure 10
shows DropBlock improves the performance of NAS-FPN.
Especially, it improves more for architecture that has more
newly introduced ﬁlters. Note that by default we do not
apply DropBlock in previous experiments for the fair comparison to existing works.
2 detection
Figure 10: Performance comparison of NAS-FPN with feature dimension of 256 or 384 when it is trained with and
without DropBlock (DB). Models are trained with backbone of ResNet-50 on image size of 1024x1024. Adding
DropBlock is more important when we increase feature dimension in pyramid networks.
5. Conclusion
In this paper, we proposed to use Neural Architecture
Search to further optimize the process of designing Feature Pyramid Networks for Object Detection. Our experiments on the COCO dataset showed that the discovered architecture, named NAS-FPN, is ﬂexible and performant for
building accurate detection model. On a wide range of accuracy and speed tradeoff, NAS-FPN produces signiﬁcant
improvements upon many backbone architectures.