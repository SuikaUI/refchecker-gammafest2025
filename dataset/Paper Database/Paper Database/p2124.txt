TOMMASI, CAPUTO: THE MORE YOU KNOW, THE LESS YOU LEARN
The more you know, the less you learn: from
knowledge transfer to one-shot learning of
object categories
Tatiana Tommasi
 
Idiap Research Institute
Martigny, CH
Barbara Caputo
 
Ecole Polytechnique Federale EPFL
Lausanne, CH
Learning a category from few examples is a challenging task for vision algorithms,
while psychological studies have shown that humans are able to generalise correctly
even from a single instance (one-shot learning). The most accredited hypothesis is that
humans are able to exploit prior knowledge when learning a new related category. This
paper presents an SVM-based model adaptation algorithm able to perform knowledge
transfer for a new category when very limited examples are available. Using a leaveone-out estimate of the weighted error-rate the algorithm automatically decides from
where to transfer (on which known category to rely), how much to transfer (the degree
of adaptation) and if it is worth transferring something at all. Moreover a weighted
least-squares loss function takes optimally care of data unbalance between negative and
positive examples. Experiments presented on two different object category databases
show that the proposed method is able to exploit previous knowledge avoiding negative
transfer. The overall classiﬁcation performance is increased compared to what would
be achieved by starting from scratch. Furthermore as the number of already learned
categories grows, the algorithm is able to learn a new category from one sample with
increasing precision, i.e. it is able to perform one-shot learning.
Introduction
A major goal in object categorisation is learning and recognising effectively thousands of
categories, as humans do . To this end, a very promising trend is to develop methods for
learning from small samples by exploiting prior experience via knowledge transfer. The basic intuition is that, if a system has already learned N categories, learning the N +1th should
be easier, even from one or few training samples, because the algorithm can take advantage of what was learned already . When considering knowledge transfer approaches to
object categorisation, it is worth keeping in mind the following issues: (a) when to transfer: while intuitively one might assume that prior knowledge is going to help in learning a
new category, this might not always be the case. Consider for instance a system that has
learned so far only different categories of animals (dogs, cats, ducks, dolphins etc). When
it starts to learn the new category “motorbike”, it is not obvious that the prior knowledge
is going to help much. The ideal knowledge transfer algorithm should be able to determine
c⃝2009. The copyright of this document resides with its authors.
It may be distributed unchanged freely in print or electronic forms.
TOMMASI, CAPUTO: THE MORE YOU KNOW, THE LESS YOU LEARN
automatically if it is worthwhile transferring knowledge or not; (b) from where to transfer:
we would expect that knowledge transfer will be more effective between similar categories.
For instance, when learning from few samples the category motorbike, it would help more
to transfer knowledge from models of other types of vehicles (cars, trucks, etc) rather than
from models of animals. This means having an algorithm able to measure quantitatively the
similarity between a new category and all the old ones stored in memory, and to use this
information for determining from where to transfer.
Several approaches have been proposed so far for transferring knowledge, spanning from
transferring model parameters , to samples , to general categorical properties , using also information coming from unlabelled data . While all
of these approaches proved to work reasonably well in some domain, how to transfer is still
an open research question. We argue that an ideal algorithm should transfer knowledge so to
boost learning when only one/few samples are available (the so called “one-shot learning”
phenomenon). The one-shot learning effect should become stronger as the number of known
categories grows, because in that case it is most likely that the system has already learned a
category very similar to the one to be learned.
This paper presents an algorithm that addresses these issues. We take a discriminative
approach, and we cast the object categorisation problem in a Least Square-Support Vector
Machine (LS-SVM, ) framework. We build on recent work on LS-SVM-based model
adaptation , where a crucial requirement is having available many samples of the new
class. Here we extend the model in order to enable it to learn a new category even from
only one image. The resulting algorithm determines automatically from where to transfer
and how much to rely on the transferred knowledge. Also, thorough experiments on two
different databases show that, when the number of known categories grows, the performance
obtained by using only one training image increases dramatically, clearly showing a one-shot
learning effect.
In the rest of the paper we review LS-SVM, describe the model adaptation method presented in and derive our knowledge transfer approach (Section 2). Experiments showing
the power of our algorithm are presented in Section 3. We conclude with an overall discussion and plans for future work.
The Knowledge Transfer Learning Approach
Let us suppose to have a category detection algorithm that has been trained so far to recognise
N categories. This concretely corresponds to deﬁne N functions f j(x) →{1,−1}, j = 1,...,N
such that the image x is assigned to the jth category if and only if f j(x) = 1. When beginning
to learn the N + 1th category, the algorithm will have initially only one/few samples for
learning fN+1(x). Our goal is to exploit, whenever possible, the existing prior knowledge to
boost the learning of fN+1(x). In the following we will brieﬂy review the LS-SVM theory
(Section 2.1) and how it can be used in a model adaptation framework (Section 2.2).
Starting from this, we will show how it is possible to derive a knowledge transfer algorithm
able to determine automatically when and where from to transfer, with a one-shot learning
behaviour in presence of a rich prior knowledge (Section 2.3).
Least Square-Support Vector Machine
Let us assume to have a binary problem and a set of l samples {xi,yi}l
i=1 where xi ∈X ⊂
Rd is an input vector describing the ith sample and yi ∈Y = {−1,1} is its label. The
TOMMASI, CAPUTO: THE MORE YOU KNOW, THE LESS YOU LEARN
goal of the SVM classiﬁer is to learn a linear model that assigns the correct label to an
unseen test sample . This can be thought as learning a linear function f(x) = w·φ(x)+b
where φ(x) maps the input samples to a high dimensional feature space, induced by a kernel
function K(x,x′) = φ(x)·φ(x′). In LS-SVM the model parameters (w,b) are found solving
the following constrained optimisation problem :
subject to
yi = w·φ(xi)−b+ξi
∀i ∈{1,··· ,l}.
The corresponding primal Lagrangian is :
αi{w·φ(xi)+b+ξi −yi},
where ααα = (α1,α2,...,αl) ∈Rl is a vector of Lagrange multipliers. The optimality conditions for the obtained problem deﬁne a system of linear equations that can be written
concisely in matrix form as :
where K is the kernel matrix. Let us call G the ﬁrst left-hand side matrix in (3). It turns out
that the least-square optimisation problem can be solved by simply inverting G.
The accuracy of the model on test data is critically dependent on the choice of good
learning parameters (e.g. the kernel parameters and the regularization parameter C). This
choice can be based on a preliminary cross validation evaluating the leave-one-out error,
which is known to be approximately an unbiased estimator of the classiﬁer generalisation
error . LS-SVM allows to write the leave-one-out error r(−i)
for the ith sample in closed
form . Let [α(−i);b(−i)] represent the dual parameters of the LS-SVM when the ith sample is omitted during the leave-one-out cross validation procedure. It is shown that :
[α(−i);b(−i)] = G−1
(−i)[y1,...,yi−1,yi+1,...,yl,0]T, where G(−i) is the matrix obtained when
the ith sample is omitted in G. Using the block matrix invertion lemma we have :
So without explicitly running cross validation experiments it is possible to deﬁne a criterion
error to maximise the LS-SVM model generalisation performance :
1+exp{−10∗z},
the best learning parameters are those minimising this error.
Learning a new object category from many samples
Let us assume that we want to learn a new category from a set of labelled training data
{xi}i=1,m, taking advantage of what learned so far. Orabona et al. proposes to start the
training with a known model and then reﬁne it through adaptation. Adaptation is deﬁned
TOMMASI, CAPUTO: THE MORE YOU KNOW, THE LESS YOU LEARN
constraining a new model to be close to one of a set of pre-trained models. The proposed
method is mathematically formulated in the LS-SVM classiﬁcation framework changing the
classical regularization term and deﬁning the following optimisation problem :
2∥w−βw′∥2 + C
subject to
yi = w·φ(xi)−b+ξi
∀i ∈{1,··· ,l}
where w′ is the parameter describing the old model and β is a scaling factor necessary to
control the degree to which the new model is close to the old one. The optimal solution :
is given by the sum of the pre-trained model scaled by the parameter β and a new model built
on the new data points. When β is 0 the obtained formula comes back to the original LS-
SVM formulation, that is without any adaptation to the previous data. To ﬁnd the optimal β,
the authors take advantage from the possibility of LS-SVM to write the leave-one-out error
in closed form. It turns out that it is still possible to do it for the modiﬁed formulation in (6)
obtaining:
(−i)[ˆy1,..., ˆyi−1, ˆyi+1,..., ˆyl,0]T and ˆyi = (w′ · φ(xi)), i.e. ˆyi is the prediction
of the old model on the ith sample. The obtained leave-one-out error depends on β, so for
each known model it is possible to ﬁnd the best β producing the lowest criterion error ERR
(5). Moreover, comparing all the criterion errors, the lowest one identiﬁes the best prior
knowledge model to use for adaptation.
We call this algorithm Adapt, it was proposed for learning adaptively grasping postures
for prosthetic hands and seems very promising also for learning new object categories
with knowledge transfer. The model from where to transfer is chosen as the one producing
the lowest criterion error, and knowledge is transferred in the form of its model parameter
w′. The scaling factor β determines how much to transfer, again depending on the criterion
error evaluation. Note that all of this is learned automatically by the algorithm. A major
drawback is that when learning from less than 150 samples, results are unstable, due to the
high variance of the leave-one-out error technique when considering few samples. In the
next section we will show that we overcome this point by introducing weighting factors that
“rebalance” the problem and that makes it possible to use effectively this method even when
learning from one single image.
Learning a new object category from few samples
Suppose to have a training set with 1 positive and 20 negative examples, on the basis of
which we want to estimate from where to transfer, using the leave-one-out error. Making a
wrong prediction on one of the examples contributes for 1/20 of the total error independently
respect to the sign of its label. This is not good: we would like to be more tolerant on negative
examples due to their higher number, and strict on the positive one which is alone. In such
cases, to use effectively the criterion error, it is necessary to reweight the leave-one-out
TOMMASI, CAPUTO: THE MORE YOU KNOW, THE LESS YOU LEARN
recognition of positive and negative examples. A way to do it is to modify the criterion error
to have a leave-one-out cross-validation estimate of the Weighted Error Rate (WERR) :
ζiΨ{yir(−i)
if yi = +1
if yi = −1.
Here the function Ψ is the same as in (5) and l+ and l−represent the number of positive
and negative examples respectively. Introducing the weighting factors ζi is asymptotically
equivalent to re-sampling the data so that object and non-object samples are balanced . If
we consider again a training set with 1 positive and 20 negative examples, the introduction
of the described weight makes the error on a negative example contribute for 1/40 of the total
while the error on the positive example contribute for 1/2. Let us identify with Adapt-W the
adaptation method described in the previous Section with ERR (5) substituted by WERR (9).
As already mentioned, the WERR helps in the selection of the best prior knowledge and
in deﬁning its relevance for the new task. This means that it gives a contribution just on the
“ﬁnal” part of the knowledge transfer method, but not while building the new adapted model.
To take care of the data unbalance also during this “initial” step, we propose to ﬁnd the model
parameters (w,b) via minimisation of a regularised weighted least-square loss function :
ζi[yi −w·φ(xi)−b]2.
It introduces just a small variation in the LS-SVM solution: the optimal dual model parameters (ααα,b) are deﬁned by a modiﬁed system of linear equations :
where W = diag{ζ −1
2 ,...,ζ −1
} and ζi are deﬁned as in (9). Let’s call the obtained
variant LS-SVM-W.
Hence the model adaptation method can be changed to its weighted formulation:
2∥w−βw′∥2 + C
subject to
yi = w·φ(xi)−b+ξi
∀i ∈{1,··· ,l}. (12)
In this way the weighting factors ζi take into account that the proportion of positive and
negative examples in the training data are known not to be representative of the operational
class frequencies. More in detail, the ξi term represents the misclassiﬁcation cost of the i-th
datum during training. Here, introducing a weight let the classiﬁcation model to be built
balancing the contribution of penalties coming from different labelled examples. In the case
of 1 positive and 20 negative examples, the misﬁt ξi term is multiplied by a factor 1/40
for a negative sample and 1/2 for the positive one. Let’s call Adapt-2W the strategy which
combines together the weighted model adaptation technique (12) and the WERR (9). In this
way we deﬁne a new knowledge transfer method which allows to learn new visual categories
from few examples as shown by our experimental results.
Experiments
We present here three set of experiments, designed for studying the behaviour of our algorithm when (a) it knows few categories, and none of them is very similar to the new one
TOMMASI, CAPUTO: THE MORE YOU KNOW, THE LESS YOU LEARN
(unrelated categories, Section 3.2); (b) it knows few categories that are very similar to the
new one (related categories, Section 3.3), and (c) the number of known categories increases,
with a speciﬁc focus on the one-shot performance (Section 3.4). The experiments were run
on two subsets of two different object category databases: the Caltech-256 and the IRMA
database used in the CLEF challenge 2008 . In the rest of this Section we ﬁrst describe
the experimental setup (Section 3.1), and then we report our ﬁndings for the three scenarios
described above.
Experimental setup
Our working assumption is to have N category detection models stored in memory, built
using standard SVM and looking for the optimal w′. We used the Gaussian kernel K(x,x′) =
exp(−γ∥x −x′∥2) for all experiments; the parameters C and γ were chosen by crossvalidation. When the new N +1 object category comes, the system starts learning. The new
data consists of m images from a background dataset and an increasing number of instances
of the new category, from 1 to m. Each experiment is repeated on ﬁve different ordering of the
data, chosen randomly. Moreover, to get a reliable estimate of the adaptation performance on
all the considered categories, we used a leave-one-out approach, using in turn each class for
adaptive learning and considering all the remaining categories as prior knowledge. At each
step the performance is evaluated on an equal number of unseen background images and
instances of the new category. The parameters C and γ for the adaptive LS-SVM were chosen
as described above for the known categories, and only the scaling factor β was selected
through the leave-one-out cross validation estimate of WERR (9).
In the following we will compare the performance of Adapt-W to that of Adapt-2W.
Moreover we consider the performance of LS-SVM and LS-SVM-W trained only on the new
incoming data, which correspond respectively to (6) and (12) where β = 0. We do not
directly compare against Adapt , because it does not work on small training samples. We
now describe the experimental setup speciﬁc to the two chosen databases.
Caltech 256 setup We considered eight object categories from the Caltech-256 database
 , namely bulldozer, car-side, ﬁretruck, motorbike, schoolbus, snowmobile, dog and duck.
From the original dataset, for each category, we selected images where the object was clearly
visible and where it always had the same orientation. This resulted in datasets with a minimum of 33 images (schoolbus) and a maximum of 83 images (snowmobile). We used the
whole category clutter (827 images), randomly selecting a background class for each category. As features we used the Pyramid Histograms of Oriented Gradients (PHOG) . We
computed descriptors with orientation in the range and we built a histogram with
K=8 bins. We considered L = 3 levels in forming the pyramid grid . The resulting feature
vector has 680 elements.
IRMA setup The IRMA database1 is a collection of radiographs presenting a large number of rich classes deﬁned according to a four-axis hierarchical code . We decided to
work on the 2008 IRMA database version , just considering the third axis of the code: it
describes the anatomy, namely which part of the body is depicted, independently to the used
acquisition technique or direction. 23 classes with more than 100 images were selected from
various sub-levels of the third axis, 3 of them were used to deﬁne the background class2. As
1Available from http:\\phobos.imib.rwth-aachen.de\irma\datasets_en.php.
2213-nose area (242 images), 230-neuro area (365 images), 310-cervical spine (508 images), 320-thoracic spine
(279 images), 330-lumbar spine (540 images), 411-hand ﬁnger (325 images), 414- left hand (541 images), 415-right
hand (176 images), 421-left carpal joint (124 images), 441-left elbow (114 images), 442-right elbow (105 images),
TOMMASI, CAPUTO: THE MORE YOU KNOW, THE LESS YOU LEARN
Figure 1: (a) classiﬁcation performance as a function of the number of object training images, when learning three unrelated categories. The results showed correspond to average
recognition rate over the three categories, considering each class-out experiment repeated 5
times. (b) average difference in classiﬁcation performance ± standard deviation, obtained
by Adapt-2W with respect to Adapt-W. (c) for each class-out experiment, the histogram bars
represent the known categories recall on the test set, indicating the prior knowledge capability in recognising the new object.
features we used the SIFT-based approach described in .
Experiments on unrelated categories
In the ﬁrst set of experiments we considered three visually different categories to understand
if the adaptation model is negatively affected by transferring from unrelated tasks. We chose
schoolbus, dog and duck from the described dataset and from each category we selected
randomly 36 images for training (18 object and 18 background instances) and 30 images
for testing (15 object and 15 background instances). Results are showed in Figure 1(a): we
see that the Adapt-W and LS-SVM curves are almost identical as well as Adapt-2W and LS-
SVM-W: if the WERR evaluation does not indicate any of the known classes as helpful, both
adaptation methods perform roughly as the corresponding non adaptative methods. Moreover we see that Adapt-2W performs better than Adapt-W: Figure 1(b) shows that Adapt-2W
has an improvement of up to 14% in recognition rate for less than 10 object images compared to Adapt-W. The two methods asymptotically coincide. Figure 1(c) shows, for each
category, the average recall of the known classes on the test set. These results can give an
intuition about the reliability of the known categories for the new task. It is clear that in each
case there is very few useful information stored in memory.
463-right humero-scapular joint (146 images), 610-right breast (144 images), 620-left breast (155 images), 914-left
foot(146 images), 915-right foot (139 images), 921-left ankle joint (192 images), 922-right ankle joint (229 images),
942-left knee (231 images), 943-right knee (222 images). Three classes used for background: 700-abdomen (219
images), 800-pelvis (263 images), 500-chest (4611 images).
TOMMASI, CAPUTO: THE MORE YOU KNOW, THE LESS YOU LEARN
Figure 2: (a) classiﬁcation performance as a function of the number of object training images
when learning three related categories. The results showed correspond to average recognition rate over the three categories, considering each class-out experiment repeated 5 times.
(b) average difference in classiﬁcation performance ± standard deviation obtained by the
Adapt-2W method with respect to Adapt-W. (c) for each class-out experiment, the histogram
bars represent the known categories recall on the test set, indicating the prior knowledge
capability in recognising the new object.
Experiments on related categories
In the second set of experiments we considered three visually related categories, all belonging to the Caltech-256 general class “motorized transportation” . We chose car, ﬁretruck
and motorbike from the described dataset and from each we selected randomly 36 images
for training (18 object and 18 background instances) and 30 images for testing (15 object
and 15 background instances). From Figure 2(a) we can see that adaptation produces clearly
better results than starting from scratch. Moreover the difference in recognition rate showed
in Figure 2(b) indicate that by using Adapt-2W we have an improvement in recognition rate
of up to 9% for less than 4 object images in the training set, compared to using Adapt-W. Finally, Figure 2(c) shows for each category the average recall of the prior knowledge classes.
This indicate that in each case there is at least one good known reliable category to use for
adaptation. The same set of experiments was repeated considering all the six visually related
categories in our dataset (bulldozer, car, ﬁretruck, motorbike, schoolbus and snowmobile)
from the Caltech-256 general class “motorized transportation” . The obtained results are
similar to what showed on three categories: using Adapt-2W we have better results (up to
5% in recognition rate) for less than 5 object images in the training set, compared to using
Adapt-W, while the two methods asymptotically coincide. Moreover it is possible to notice that the one-shot learning performance is improved respect to the three class case. For
Adapt-2W the recognition rate using only one object instance in the training set goes from
76% for three categories to 79% for six categories.
TOMMASI, CAPUTO: THE MORE YOU KNOW, THE LESS YOU LEARN
Figure 3: (a) one-shot learning performance of the Adapt-2W and corresponding LS-SVM-W,
varying the total number of categories. (b) classiﬁcation performance as a function of the
number of training images when learning on 20 object categories. The results correspond
to average recognition rate over the 20 categories, considering each class-out experiment
repeated 5 times. (c) average classiﬁcation performance difference obtained by the Adapt-
2W method with respect to LS-SVM-W. The error bars denote ± standard deviation with
respect to the average values.
Experiments on an increasing number of categories
All the experimental results showed till here asses the higher performance of the Adapt-2W
respect to the Adapt-W method. For this reason we decided to use just the ﬁrst approach
for the experiments on the IRMA database. Here we study how performance varies when
the number of known categories grows. We are especially interested in monitoring how the
method behaves when learning from one single image. We randomly selected from each
category 100 instances for training and 100 instances for testing (for both the sets, 50 object
and 50 background images). Five sets of experiments were run considering 3/5/7/10 and
15 classes plus a ﬁnal one with all the 20 categories. We started extracting three categories
through random selection and then we went on adding new ones till covering the whole 20
class dataset. Figure 3(a) shows the obtained recognition rate results for Adapt-2W and the
corresponding LS-SVM-W when only one object image is used for training. We expect that
the overall performance of the knowledge transfer method will increase along with the number of stored models, since there is a larger probability to ﬁnd a matching pre-trained model.
This intuition is conﬁrmed by the increasing trend in the one-shot learning recognition rate.
This trend is quite fast at the beginning passing from 3 (57% recognition rate) to 5 (72%
recognition rate) and 7 (85% recognition rate) categories and then becomes slower from 10
(86% recognition rate) to 20 categories (both for 15 and 20 classes the one-shot learning
rate is 87%). We show in Figure 3(b) the 20 categories results and in 3(c) the corresponding difference in performance when using the adaptation method with respect to learning
form scratch. As one can see, adaptation uniformly obtains a better performance showing an
asymptotic gain of about 2.5%.
Conclusions and Future work
We presented an SVM-based method for learning object categories from few examples using knowledge transfer. The algorithm decides automatically from where and how much to
transfer, adapting the known model to the incoming data. The reliability of prior knowledge
for the new task is evaluated by estimating its generalisation error so to weight properly pos-
TOMMASI, CAPUTO: THE MORE YOU KNOW, THE LESS YOU LEARN
itive and negative examples in the training set. Moreover the model adaptation is appropriately designed to balance the possible misﬁt of object and non-object instances. Experiments
show that the proposed method improves the learning performance when useful information
is stored in memory, while it never affects it negatively when the known categories are very
different from the new one. When the number of known categories increases, the performance of the model improves remarkably, showing a one-shot learning behaviour. In the
future we plan to run experiments to understand more deeply the algorithm capabilities and
to compare with the results presented in . Moreover, we would like to extend the method
to multiple cues, and to hierarchical categorisation, with the aim to reduce the computational
complexity of the algorithm for large number of known categories.
Acknowledgments
This work was supported by the EU project DIRAC (FP6-0027787) and by the EMMA
project thanks to the Hasler foundation (www.haslerstiftung.ch). We are thankful to Francesco
Orabona and to the anonymous reviewers for their many helpful comments and suggestions.