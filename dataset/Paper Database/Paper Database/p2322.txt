An Empirical Analysis of Feature Engineering for
Predictive Modeling
Jeff Heaton
McKelvey School of Engineering
Washington University in St. Louis
St. Louis, MO 63130
Email: 
Abstract—Machine learning models, such as neural networks,
decision trees, random forests, and gradient boosting machines,
accept a feature vector, and provide a prediction. These models
learn in a supervised fashion where we provide feature vectors
mapped to the expected output. It is common practice to engineer
new features from the provided feature set. Such engineered
features will either augment or replace portions of the existing
feature vector. These engineered features are essentially calculated ﬁelds based on the values of the other features.
Engineering such features is primarily a manual, timeconsuming task. Additionally, each type of model will respond
differently to different kinds of engineered features. This paper
reports empirical research to demonstrate what kinds of engineered features are best suited to various machine learning model
types. We provide this recommendation by generating several
datasets that we designed to beneﬁt from a particular type of
engineered feature. The experiment demonstrates to what degree
the machine learning model can synthesize the needed feature
on its own. If a model can synthesize a planned feature, it is not
necessary to provide that feature. The research demonstrated that
the studied models do indeed perform differently with various
types of engineered features.
I. INTRODUCTION
Feature engineering is an essential but labor-intensive component of machine learning applications . Most machinelearning performance is heavily dependent on the representation of the feature vector. As a result, data scientists spend
much of their effort designing preprocessing pipelines and data
transformations .
To utilize feature engineering, the model must preprocess
its input data by adding new features based on the other
features . These new features might be ratios, differences,
or other mathematical transformations of existing features.
This process is similar to the equations that human analysts
design. They construct new features such as body mass index
(BMI), wind chill, or Triglyceride/HDL cholesterol ratio to
help understand existing features’ interactions.
Kaggle and ACM’s KDD Cup have seen feature engineering play an essential part in several winning submissions.
Individuals applied feature engineering to the winning KDD
Cup 2010 competition entry . Additionally, researchers won
the Kaggle Algorithmic Trading Challenge with an ensemble
of models and feature engineering. These individuals created
these engineered features by hand.
Technologies such as deep learning can beneﬁt from
feature engineering. Most research into feature engineering
in the deep learning space has been in image and speech
recognition . Such techniques are successful in the highdimension space of image processing and often amount to
dimensionality reduction techniques such as PCA and
auto-encoders .
II. BACKGROUND AND PRIOR WORK
Feature engineering grew out of the desire to transform nonnormally distributed linear regression inputs. Such a transformation can be helpful for linear regression. The seminal work
by George Box and David Cox in 1964 introduced a method
for determining which of several power functions might be a
useful transformation for the outcome of linear regression .
This technique became known as the Box-Cox transformation.
The alternating conditional expectation (ACE) algorithm 
works similarly to the Box-Cox transformation. An individual
can apply a mathematical function to each component of
the feature vector outcome. However, unlike the Box-Cox
transformation, ACE can guarantee optimal transformations
for linear regression.
Linear regression is not the only machine-learning model
that can beneﬁt from feature engineering and other transformations. In 1999, researchers demonstrated that feature
engineering could enhance rules learning performance for
text classiﬁcation . Feature engineering was successfully
applied to the KDD Cup 2010 competition using a variety of
machine learning models.
III. EXPERIMENT DESIGN AND METHODOLOGY
Different machine learning model types have varying degrees of ability to synthesize various types of mathematical
expressions. If the model can learn to synthesize an engineered
feature on its own, there was no reason to engineer the
feature in the ﬁrst place. Demonstrating empirically a model’s
ability to synthesize a particular type of expression shows if
engineered features of this type might be useful to that model.
To explore these relations, we created ten datasets contain
the inputs and outputs that correspond to a particular type of
engineered feature. If the machine-learning model can learn
to reproduce that feature with a low error, it means that that
particular model could have learned that engineered feature
without assistance.
 
For this research, only considered regression machine learning models for this experiment. We chose the following four
machine learning models because of the relative popularity
and differences in approach.
• Deep Neural Networks (DANN)
• Gradient Boosted Machines (GBM)
• Random Forests
• Support Vector Machines for Regression (SVR)
To mitigate the stochastic nature of some of these machine
learning models, each experiment was run 5 times, and the
best run’s outcome was used for the comparison. These experiments were conducted in the Python programming language,
using the following third-party packages: Scikit-Learn 
and TensorFlow . Using this combination of packages,
model types of support vector machine (SVM) , deep
neural network , random forest , and gradient boosting
machine (GBM) were evaluated against the following
sixteen selected engineered features:
• Differences
• Distance Between Quadratic Roots
• Distance Formula
• Logarithms
• Max of Inputs
• Polynomials
• Power Ratio (such as BMI)
• Ratio of a Product
• Rational Differences
• Rational Polynomials
• Root Distance
• Root of a Ratio (such as Standard Deviation)
• Square Roots
The techniques used to create each of these datasets are
described in the following sections. The Python source code
for these experiments can be downloaded from the author’s
GitHub page or Kaggle .
The count engineered feature counts the number of elements
in the feature vector that satisﬁes a certain condition. For
example, the program might generate a count feature that
counts other features above a speciﬁed threshold, such as zero.
Equation 1 deﬁnes how a count feature might be engineered.
1 if xi > t else 0
The x-vector represents the input vector of length n. The
resulting y contains an integer equal to the number of x
values above the threshold (t). The resulting y-count was
uniformly sampled from integers in the range , and
the program creates the corresponding input vectors for the
program to generate a count dataset. Algorithm 1 demonstrates
this process.
Algorithm 1 Generate count test dataset
1: INPUT: The number of rows to generate r.
2: OUTPUT: A dataset where y contains random integers
sampled from , and x contains 50 columns randomly
chosen to sum to y.
3: METHOD:
4: x ←[...empty set...]
5: y ←[...empty set...]
6: for n ←1 TO r do
v ←zeros(50)
▷Vector of length 50
o ←uniform random int(0, 50)
▷Outcome(y)
▷remaining
while r ≥0 do:
i ←uniform random int(0, len(x) −1)
if x[i] = 0 then
x.append(x)
y.append(o)
return [x, y]
COUNTS TRANSFORMATION
Several example rows of the count input vector are shown
in Table I. The y1 value simply holds the count of the number
of features x1 through x50 that contain a value greater than 0.
B. Differences and Ratios
Differences and ratios are common choices for feature
engineering. To evaluate this feature type a dataset is generated
with x observations uniformly sampled in the real number
range , a single y prediction is also generated that
is various differences and ratios of the observations. When
sampling uniform real numbers for the denominator, the range
[0.1, 1] is used to avoid division by zero. The equations chosen
are simple difference (Equation 2), simple ratio (Equation 3),
power ratio (Equation 4), product power ratio (Equation 5)
and ratio of a polynomial (Equation 6).
y = x1 −x2
C. Distance Between Quadratic Roots
It is also useful to see how capable the four machine
learning models are at synthesizing ordinary mathematical
equations. We generate the ﬁnal synthesized feature from
a distance between the roots of a quadratic equation. The
distance between roots of a quadratic equation can easily be
calculated by taking the difference of the two outputs of the
quadratic formula, as given in Equation 7, in its unsimpliﬁed
The dataset for the transformation represented by Equation
7 is generated by uniformly sampling x values from the real
number range [−10, 10]. We discard any invalid results.
D. Distance Formula
The distance formula contains a ratio inside a radical,
and is shown in Equation 8. The input are for x values
uniformly sampled from the range , and the outcome
is the Euclidean distance between (x1, x2) and (x3, x4).
(x1 −x2)2 + (x3 −x4)2
E. Logarithms and Power Functions
Statisticians have long used logarithms and power functions
to transform the inputs to linear regression . Researchers
have shown the usefulness of these functions for transformation for other model types, such as neural networks . The
log and power transforms used in this paper are of the type
shown in Equations 9,10, and 11.
y = log(x)
This paper investigates using the natural log function, the
second power, and the square root. For both the log and root
transform, random x values were uniformly sampled in the real
number range . For the second power transformation,
the x values were uniformly sampled in the real number range
 . A single x1 observation is used to generate a single
y1 observation. The x1 values are simply random numbers
that produce the expected y1 values by applying the logarithm
F. Max of Inputs
Ten random inputs are generated for the observations (x1 −
x10). These random inputs are sampled uniformly in the range
 . The outcome is the maximum of the observations.
Equation 12 shows how this research calculates the max of
inputs feature.
y = max (x1...x10)
G. Polynomials
Engineered features might take the form of polynomials.
This paper investigated the machine learning models’ ability
to synthesize features that follow the polynomial given by
Equation 13.
y = 1 + 5x + 8x2
An equation such as this shows the models’ ability to
synthesize features that contain several multiplications and an
exponent. The data set was generated by uniformly sampling
x from real numbers in the range [0, 2). The y1 value is simply
calculated based on x1 as input to Equation 13.
H. Rational Differences and Polynomials
Useful features might also come from combinations of
rational equations of polynomials. Equations 14 & 15 show the
types of rational combinations of differences and polynomials
tested by this paper. We also examine a ratio power equation,
similar to the body mass index (BMI) calculation, shown in
Equations 16.
y = x1 −x2
To generate a dataset containing rational differences (Equation 14), four observations are uniformly sampled from real
numbers of the range . Generating a dataset of rational
polynomials, a single observation is uniformly sampled from
real numbers of the range .
IV. RESULTS ANALYSIS
To evaluate the effectiveness of the four model types over
the sixteen different datasets we must account for the differences in ranges of the y values. As Table II demonstrates,
the maximum, minimum, mean, and standard deviation of the
datasets varied considerably. Because an error metric, such
as root mean square error (RMSE) is in the same units as
its corresponding y values, some means of normalization is
needed. To allow comparison across datasets, and provide this
normalization, we made use of the normalized root-meansquare deviation (NRMSD) error metric shown in Equation 17.
We capped all NRMSD values at 1.5; we considered values
higher than 1.5 to have failed to synthesize the feature.
t=1(ˆyt −yt)2
The results obtained by the experiments performed in this
paper clearly indicate that some model types perform much
better with certain classes of engineered features than other
model types. The simple transformations that only involved
DATASET OBSERVATION (Y) STATISTICS AND RANGES
-24,976.28
MODEL SCORES FOR DATASETS
Score Neural
a single feature were all easily learned by all four models.
This included the log, polynomial, power, and root. However,
none of the models were able to successfully learn the ratio
difference feature. Table III provides the scores for each
equation type and model. The model speciﬁc results from this
experiment are summarized in the following sections.
A. Neural Network Results
For each engineered feature experiment, create an ADAM
 trained deep neural network. We made use of a learning
rate of 0.001, β1 of 0.9, β2 of 0.999, and ϵ of 1 × 10−7, the
default training hyperparameters for Keras ADAM.
The deep neural network contained the number of input
neurons equal to the number of inputs needed to test that
engineered feature type. Likewise, a single output neuron
provided the value generated by the speciﬁed engineered
feature. When viewed from the input to the output layer,
there are ﬁve hidden layers, containing 400, 200, 100, 50,
and 25 neurons, respectively. Each hidden layer makes use of
a rectiﬁer transfer function , making each hidden neuron
a rectiﬁed linear unit (ReLU). We provide the results of these
Fig. 1. Deep Neural Network Engineered Features
deep neural network engineered feature experiments in Figure
The deep neural network performed well on all equation
types except the ratio of differences. The neural network also
performed consistently better on the remaining equation types
than the other three models. An examination of the calculations
performed by a neural network will provide some insight into
this performance. A single-layer neural network is essentially
a weighted sum of the input vector transformed by a transfer
function, as shown in Equation 18.
f(x, w, b) = φ
(wixi) + b
The vector x represents the input vector, the vector w
represents the weights, and the scalar variable b represents
the bias. The symbol φ represents the transfer function. This
paper’s experiments used the rectiﬁer transfer function 
for hidden neurons and a simple identity linear function for
output neurons. The weights and biases are adjusted as the
neural network is trained. A deep neural network contains
many layers of these neurons, where each layer can form the
input (represented by x) into the next layer. This fact allows the
neural network to be adjusted to perform many mathematical
operations and explain some of the results shown in Figure 1.
The neural network can easily add, sum, and multiply. This
fact made the counts, diff, power, and rational polynomial
engineered features all relatively easy to synthesize by using
layers of Equation 18.
B. Support Vector Machine Results
The two primary hyper-parameters of an SVM are C and
γ. It is customary to perform a grid search to ﬁnd an optimal
combination of C and γ . We tried 3 C values of 0.001,
1, and 100, combined with the 3 γ values of 0.1, 1, and
10. This selection resulted in 9 different SVMs to evaluate.
The experiment results are from the best combination of C
and γ for each feature type. A third hyper-parameter speciﬁes
Fig. 2. SVM Engineered Features
the type of kernel that the SVM uses, which is a Gaussian
kernel. Because support vector machines beneﬁt from their
input feature vectors normalized to a speciﬁc range , we
normalized all SVM input to . This required normalization
step for the SVM does add additional calculations to the
feature investigated. Therefore, the SVM results are not as
pure of a feature engineering experiment as the other models.
We provide the results of the SVM engineered features in
The support vector machine found the max, quadratic, ratio
of differences, a polynomial ratio, and a ratio all difﬁcult
to synthesize. All other feature experiments were within a
low NRMSD level. Smola and Vapnik extended the original
support vector machine to include regression; we call the
resulting algorithm a support vector regression (SVR) .
A full discussion of how an SVR is ﬁtted and calculated is
beyond the scope of this paper. However, for this paper’s research, the primary concern is how an SVR calculates its ﬁnal
output. This calculation can help determine the transformations
that an SVR can synthesize. The ﬁnal output for an SVR is
given by the decision function, shown in Equation 19.
i )K(xi, x) + ρ
The vector x represents the input vector; the difference
between the two alphas is called the SVR’s coefﬁcient. The
weights of the neural network are somewhat analogous to
the coefﬁcients of an SVR. The function K represents a
kernel function that introduces non-linearity. This paper used
a radial basis function (RBF) kernel based on the Gaussian
function. The variable ρ represents the SVR intercept, which
is somewhat analogous to the bias of a neural network.
Like the neural network, the SVR can perform multiplications and summations. Though there are many differences
between a neural network and SVR, the ﬁnal calculations share
many similarities.
Fig. 3. Random Forest Engineered Features
C. Random Forest Results
Random forests are an ensemble model made up of decision
trees. We randomly sampled the training data to produce
a forest of trees that together will usually outperform the
individual trees. The random forests used in this paper all
use 100 classiﬁer trees. This tree count is a hyper-parameter
for the random forest algorithm. We show the result of the
random forest model’s attempt to synthesize the engineered
features in Figure 3.
The random forest model had the most difﬁculty with the
standard deviation, a ratio of differences, and sum.
D. Gradient Boosted Machine
The gradient boosted machine (GBM) model operates very
similarly to random forests. However, the GBM algorithm uses
the gradient of the training objective to produce optimal combinations of the trees. This additional optimization sometimes
gives GBM a performance advantage over random forests. The
gradient boosting machines used in this paper all used the
same hyper-parameters. The maximum depth was ten levels,
the number of estimators was 100, and the learning rate was
0.05. We provide the results of the GBM engineered features
in Figure 4.
Like the random forest model, the gradient boosted machine
had the most difﬁculty with the standard deviation, the ratio
of differences, and sum.
V. CONCLUSION & FURTHER RESEARCH
Figures 1-4 clearly illustrate that machine learning models
such as neural networks, support vector machines, random
forests, and gradient boosting machines beneﬁt from a different set of synthesized features. Neural networks and support
vector machines generally beneﬁt from the same types of
engineered features; similarly, random forests and gradient
boosting machines also typically beneﬁt from the same set
of engineered features. The results of this research allow us to
make recommendations for both the types of features to use
Fig. 4. Figure 4: GBM Engineered Features
for a particular machine learning model type and the types of
models that will work well with each other in an ensemble.
Based on the experiments performed in this research, the
type of machine learning model used has a great deal of
inﬂuence on the types of engineered features to consider.
Engineered features based on a ratio of differences were not
synthesized well by any of the models explored in this paper.
Because these ratios of difference might be useful to a wide
array of models, all models explored here might beneﬁt from
engineered features based on ratios with differences.
The research performed by this paper also empirically
demonstrates one of the reasons why ensembles of models typically perform better than individual models. Because neural
networks and support vector machines can synthesize different
features than random forests and gradient boosting machines,
ensembles made up of a model from each of these two groups
might perform very well. A neural network or support vector
machine might ensemble well with a random forest or gradient
boosting machine.
We did not spend signiﬁcant time tuning the models for
each of the datasets. Instead, we made reasonably generic
choices for the hyper-parameters chosen for the models. Results for individual models and datasets might have shown
some improvement for additional time spent tuning the hyperparameters.
Future research will focus on exploring other engineered
features with a wider set of machine learning models. Engineered features that are made up of multiple input features
seem a logical focus.
This paper examined 16 different engineered features for
four popular machine learning model types. Further research
is needed to understand what features might be useful for
other machine learning models. Such research could help
guide the creation of ensembles that use a variety of machine
learning model types. We might also examine additional types
of engineered features. It would be useful to see how more
complex classes of features affect machine learning models’
performance.