DOLLÁR, et al.: THE FASTEST PEDESTRIAN DETECTOR IN THE WEST
The Fastest Pedestrian Detector in the West
Piotr Dollár1
 
Serge Belongie2
 
Pietro Perona1
 
1 Dept. of Electrical Engineering
California Institute of Technology
Pasadena, CA, USA
2 Dept. of Computer Science and Eng.
University of California, San Diego
San Diego, CA, USA
We demonstrate a multiscale pedestrian detector operating in near real time (∼6 fps
on 640x480 images) with state-of-the-art detection performance. The computational bottleneck of many modern detectors is the construction of an image pyramid, typically
sampled at 8-16 scales per octave, and associated feature computations at each scale. We
propose a technique to avoid constructing such a ﬁnely sampled image pyramid without
sacriﬁcing performance: our key insight is that for a broad family of features, including gradient histograms, the feature responses computed at a single scale can be used to
approximate feature responses at nearby scales. The approximation is accurate within
an entire scale octave. This allows us to decouple the sampling of the image pyramid
from the sampling of detection scales. Overall, our approximation yields a speedup of
10-100 times over competing methods with only a minor loss in detection accuracy of
about 1-2% on the Caltech Pedestrian dataset across a wide range of evaluation settings.
The results are conﬁrmed on three additional datasets (INRIA, ETH, and TUD-Brussels)
where our method always scores within a few percent of the state-of-the-art while being
1-2 orders of magnitude faster. The approach is general and should be widely applicable.
Introduction
Signiﬁcant progress has been made in pedestrian detection in the last decade. While both detection and false alarm ﬁgures are still orders of magnitude away from human performance
and from the performance that is desirable for most applications, the rate of progress is excellent. False positive rates have gone down two orders of magnitude since the groundbreaking
work of Viola and Jones (VJ) . At 80% detection rate on the INRIA pedestrian
dataset , VJ outputs 10 false positives per image (fppi), HOG outputs 1 fppi, and the
most recent methods , through a combination of richer features and more sophisticated
learning techniques, output just .1 fppi (error rates as reported in ).
The increase in detection accuracy has been paid for with increased computational costs.
The VJ detector ran at roughly 15 frames per second (fps) on 384 × 288 video nearly a
decade ago, while the detectors recently evaluated on the Caltech Pedestrian dataset range
in time from 1-30 seconds per frame on 640×480 video on modern hardware . In many
applications of pedestrian detection, including automotive safety, surveillance, robotics, and
human machine interfaces, fast detection rates are of the essence.
c⃝2010. The copyright of this document resides with its authors.
It may be distributed unchanged freely in print or electronic forms.
BMVC 2010 doi:10.5244/C.24.68
DOLLÁR, et al.: THE FASTEST PEDESTRIAN DETECTOR IN THE WEST
Figure 1: In many applications, detection speed is as important as accuracy. (a) A standard pipeline
for performing modern multiscale detection is to create a densely sampled image pyramid, compute
features at each scale, and ﬁnally perform sliding window classiﬁcation (with a ﬁxed scale model).
Although effective; the creation of the feature pyramid can dominate the cost of detection, leading to
slow multiscale detection. (b) Viola and Jones utilized shift and scale invariant features, allowing
a trained detector to be placed at any location and scale without relying on an image pyramid. Constructing such a classiﬁer pyramid results in fast multiscale detection; unfortunately, most features are
not scale invariant, including gradient histograms, signiﬁcantly limiting the generality of this scheme.
(c) We propose a fast method for approximating features at multiple scales using a sparsely sampled
image pyramid with a step size of an entire octave and within each octave we use a classiﬁer pyramid.
The proposed approach achieves nearly the same accuracy as using densely sampled image pyramids,
with nearly the same speed as using a classiﬁer pyramid applied to an image at a single scale.
We present a method (Figure 1) for signiﬁcantly decreasing the run-time of multiscale
object detectors that utilize multiple feature types, including gradient histograms, with very
minor decreases to their detection accuracy. Speciﬁcally, we show an application to multiscale pedestrian detection that results in nearly real time rates on 640x480 images: about 6
fps for detecting pedestrians at least 100 pixels high and 3 fps for detecting pedestrians over
50 pixels. The resulting method achieves state-of-the-art results, being within 1-2% detection rate of the highest reported results across four datasets (Caltech Pedestrians , INRIA
 , ETH , and TUD-Brussels ).
We show that it is possible to create high ﬁdelity approximations of multiscale gradient
histograms using gradients computed at a single scale in §2, and develop a more general
theory applicable to various feature types in §3. In §4 we show how to effectively utilize
these concepts for fast multiscale detection, and in §5 we apply them to pedestrian detection,
resulting in speedups of 1-2 orders of magnitude with little loss in accuracy.
Related Work
One of the most successful approaches for object detection is the sliding window paradigm
 . Numerous other detection frameworks have been proposed , and although a full review is outside the scope of this work, the approximations we develop could
potentially be applicable to such approaches as well. For pedestrian detection , however, the top performing methods are all based on sliding windows , and each
of these methods utilizes some form of gradient histograms. Through numerous strategies,
including cascades , coarse-to-ﬁne search , distance transforms , branch and
bound search , and many others, the classiﬁcation stage can be made quite fast. Nevertheless, even when highly optimized, just constructing gradient histograms over a ﬁnely
sampled image pyramid takes a minimum of about one second per 640×480 image ;
thus this becomes a major bottleneck for all of the pedestrian detectors listed above.
DOLLÁR, et al.: THE FASTEST PEDESTRIAN DETECTOR IN THE WEST
One of the earliest attempts to create real time detectors that utilize gradient histograms
was the method of (based on integral histograms ). However, the proposed system
was real time for single scale detection only (recent methods achieve similar speeds
and with higher accuracy); in this work we are interested in real time multiscale detection.
Another recent approach used a fast coarse-to-ﬁne detection scheme but sacriﬁced detection
of small pedestrians (which are of crucial importance in many applications ). A
number of fast systems have been proposed ; however, a detailed overview is outside
of the scope of this work. Finally, a number of groups have recently ported HOG to a parallel
implementation using GPUs ; such efforts are complementary to our own.
Signiﬁcant research has also been devoted to scale space theory , including real time
variants . Although only loosely related, a key observation is that half octave pyramids
are often sufﬁcient for accurate approximations of various types. This suggests that ﬁne scale
sampling may also be unnecessary for object detection.
Approximating Multiscale Gradient Histograms
We seek to answer the following question: given gradients computed at one scale, is it
possible to approximate gradient histograms at a different scale? If so, then we can avoid
computing gradients over a ﬁnely sampled image pyramid. Intuitively, one would expect
this to be possible as signiﬁcant image structure is preserved when an image is resampled
(even if the gradients themselves change). We begin with an in depth look at a simple form
of gradient histograms below and develop a more general theory in §3.
A gradient histogram measures the distribution of the gradient angles within an image.
Let I(x,y) denote an m×n discrete signal, and ∂I/∂x and ∂I/∂y denote the discrete derivatives of I (typically 1D centered ﬁrst differences are used). Gradient magnitude and orientation are deﬁned by: M(i, j)2 = ∂I
∂x(i, j)2 + ∂I
∂y(i, j)2 and O(i, j) = arctan
∂y(i, j)/ ∂I
To compute the gradient histogram of an image, each pixel casts a vote, weighted by its gradient magnitude, for the bin corresponding to its gradient orientation. After the orientation
O is quantized into Q bins so that O(i, j) ∈{1,Q}, the qth bin of the histogram is deﬁned by:
hq = ∑i,j M(i, j)1[O(i, j) = q], where 1 is the indicator function. Local histograms with rectangular support are frequently used, these can be deﬁned identically except for the range of
the indices i and j. In the following everything that holds for global histograms also applies
to local histograms.
Gradient Histograms in Upsampled Images
Intuitively the information content of an upsampled image is the same as that of the original
image (upsampling does not create new image structure). Assume I is a continuous signal,
and let Ik denote an upscaled version of I by a factor of k: Ik(x,y) ≡I(x/k,y/k). Using the
deﬁnition of a derivative, one can show that ∂Ik
∂x (i, j) = 1
∂x(i/k, j/k), and likewise for ∂Ik
which simply states the intuitive fact that the rate of change in the upsampled image is k
times slower the rate of change in the original image. Under mild smoothness assumptions,
the above also holds (approximately) for discrete signals. Let Mk(i, j) ≈1
kM(⌈i/k⌉,⌈j/k⌉)
denote the gradient magnitude in the upsampled discrete image. Then:
Mk(i, j) ≈
k M(⌈i/k⌉,⌈j/k⌉) = k2
k M(i, j) = k
Thus, the sum of gradient magnitudes in the pair of images is related by a factor of k. Gradient angles are also preserved since ∂Ik
∂y (i, j) ≈∂I
∂x(i/k, j/k)
∂y(i/k, j/k). Therefore,
DOLLÁR, et al.: THE FASTEST PEDESTRIAN DETECTOR IN THE WEST
(a) upsampling (2x)
probability
pedestrians (µ=1.99, σ=.055)
probability
natural images (µ=1.99, σ=.106)
(b) downsampling (2x)
probability
pedestrians (µ=0.32, σ=.036)
probability
natural images (µ=0.32, σ=.064)
(c) downsampling
scaling factor (s)
pedestrians
natural images
best−fit: λ=1.099
scaling factor (s)
σ* (ratio)
pedestrians
natural images
Figure 2: Behavior of gradient histograms in resampled images, see text for details.
according to the deﬁnition of gradient histograms given above, the relationship between hq
(computed over I) and h′
q (computed over Ik) is simply: h′
q ≈khq. We can thus approximate
gradient histograms in an upsampled image using gradients computed at the original scale.
Experiments: We demonstrate that the quality of the approximation h′
q ≈khq in real
images, upsampled using a standard bilinear interpolation scheme, is quite high. We used
two sets of images for these experiments. First, we used the 1237 cropped pedestrian images from the INRIA pedestrians training dataset. Each image was 128×64 and contains a
pedestrian approximately 96 pixels tall. The second set of images contains 5000 128 × 64
windows cropped at random positions from the 1218 images in the INRIA negative training
set. We refer to the two sets of images as ‘pedestrian images’ and ‘natural images’, although
the latter set is biased toward windows that may (but do not) contain pedestrians.
In order to measure the ﬁdelity of this approximation, we deﬁne the ratio rq = h′
quantizing orientation into Q = 6 bins. Figure 2(a) shows the distribution of rq for one bin
on the 1237 pedestrian (top) and 5000 natural (bottom) images given an upsampling of k = 2
(results for other bins were similar). In both cases the mean is µ ≈2, as expected, and the
variance is fairly small, meaning the approximation is unbiased and reasonable.
Gradient Histograms in Downsampled Images
While the information content of an upsampled image is roughly the same as that of the
original image, information is typically lost during downsampling. Below, we demonstrate
the nontrivial ﬁnding that the information loss is relatively consistent, and furthermore show
that we can compensate for it to a large extent in a straightforward manner.
If I contains little high frequency energy, then the approximation h′
q ≈khq should apply.
In general, however, downsampling results in loss of high frequency content and its gradient
energy. Let Ik now denote I downsampled by a factor of k. We expect the relationship
between hq (computed over I) and h′
q (computed over Ik) to have the form h′
q ≤hq/k. The
question we seek to answer here is whether the information loss is consistent.
Experiments: As before, deﬁne rq = h′
q/hq. In Figure 2(b) we show the distribution of
rq for a single bin on the pedestrian (top) and natural (bottom) images given a downsampling
factor of k = 2. Observe that the information loss is consistent: rq is normally distributed
around µ = .32 < .5. This implies that h′
q ≈.32hq could serve as a reasonable approximation
for gradient histograms in images downsampled by k = 2. We seek to understand how this
relation arises and extend the above to all values of k.
Figure 3 shows the quality of the above approximations on example images.
DOLLÁR, et al.: THE FASTEST PEDESTRIAN DETECTOR IN THE WEST
Figure 3: Approximating gradient histograms in resampled images. For each image set, we take the original
image (cyan border) and generate an upsampled (blue) and downsampled (yellow) version. Shown at each scale
are the image (center), gradient magnitude (right), and gradient orientation (bottom). At each scale we compute a
gradient histogram with 8 bins, normalizing each bin by .5 and .32−1 in the upsampled and downsampled histogram,
respectively. Assuming the approximations developed in §2 hold, the three normalized gradient histograms should
be roughly equal. For the ﬁrst three cases, the approximations are fairly accurate. In the last case, showing a highly
structured Brodatz texture with signiﬁcant high frequency content, the downsampling approximation fails entirely.
Approximating Multiscale Features
In order to understand more generally how information behaves in resampled images, we turn
to the study of natural image statistics . While we analytically derived an expression
for predicting gradient histograms in upsampled images, there is no equivalent derivation for
downsampled images. Instead, an analysis of natural images statistics allows us to approximate gradient histograms and numerous additional features in resampled images.
We begin by deﬁning a broad family of features. Let Ωbe any shift-invariant function
that takes an image I(i, j) and creates a new channel image C = Ω(I), where C is a registered
map of the original image. Output pixels in C are typically computed from corresponding
patches of input pixels in I (thus preserving overall image layout). We deﬁne a feature as
the weighted sum of a channel C: f(I) = ∑i j wi jC(i, j). Numerous local and global features
can be written in this form including gradient histograms, linear ﬁlters, color statistics, and
countless others . For simplicity, we assume f(I) = ∑ijC(i, j) is the global sum of a
channel and refer to the result as the channel energy; in the following everything that holds
for the channel energy also holds for local weighted features. Finally, we write f(I,s) to
denote the channel energy computed over I after downsampling by a factor of 2s.
Exponential Scaling Law
Ruderman and Bialek showed that various statistics of natural images are independent of the scale at which the images were captured, or in other words, the statistics of an
image are independent of the scene area corresponding to a single pixel. In the context of
our work, we expect that on average, the difference in channel energy between an image and
a downsampled version of the image is independent of the scale of the original image and
depends only on the relative scale between the pair of images. In other words the expectation over natural images of f(I,s1)/ f(I,s2) should depend only on s1 −s2. We can formalize this by assuming there exists a function r(s) such that f(I,s1)/ f(I,s2) ≈r(s1 −s2) and
E[f(I,s1)/f(I,s2)] = E[f(I,s1)]/E[f(I,s2)] = r(s1 −s2) for all s1,s2. One can then show
that r(s1 +s2) = r(s1)r(s2); if r is also continuous and non-zero, then it must take the form
r(s) = e−λs for some constant λ . Therefore, E[f(I,s1)/ f(I,s2)] must have the form:
E[f(I,s+s0)/f(I,s0)] = e−λs
Each channel type has its own corresponding λ, determined empirically. In §3.2 we show
that (1) provides a remarkably good ﬁt to our data for multiple channel types and image sets.
DOLLÁR, et al.: THE FASTEST PEDESTRIAN DETECTOR IN THE WEST
(a) normalized gradient
scaling factor (s)
pedestrians
natural images
best−fit: λ=1.129
scaling factor (s)
σ* (ratio)
pedestrians
natural images
(b) local standard deviation
scaling factor (s)
pedestrians
natural images
best−fit: λ=1.133
scaling factor (s)
σ* (ratio)
pedestrians
natural images
scaling factor (s)
pedestrians
natural images
best−fit: λ=1.294
scaling factor (s)
σ* (ratio)
pedestrians
natural images
Figure 4: Behavior of channel energy in downsampled images. Top: E[r(I,s)] as a function of s; in each case
ae−λs provides an excellent ﬁt for both pedestrian and natural images. Bottom: standard deviation of r∗(I,s) =
r(I,s)/ae−λs increases slowly as a function of s. The channel types plotted are: (a) gradient histogram channels
computed over locally L1 normalized gradients M′(i, j) = M(i, j)/(E[M(i, j)]+.1) (with E computed over a 9×9
image neighborhood); (b) standard deviation of intensity values computed in each local 9×9 neighborhoodC(i, j) =
E[I(i, j)2]−E[I(i, j)]; and (c) HOG (each channel is a single gradient orientation at one of four normalizations).
Although (1) holds for an ensemble of images, the equivalent relation should also hold
for individual images. Given f(I,0), we propose to approximate f(I,s) by:
f(I,s) ≈f(I,0)e−λs
Experiments in §3.3 indicate that the quality of the approximation in (2) is very good, for
both natural and pedestrian images. Although the quality of the approximation degrades
with increasing value of s, it does so only gradually and proves effective in practice.
Equations (1) and (2) also hold for upsampled images (details omitted). However, λ for
upsampling and downsampling will typically be different even for the same channel type
(as in the case of gradient histograms, see §2). In practice, though, we want to predict
channel energy in higher resolution images (to which we may not have access) as opposed
to (smooth) upsampled images. For this one should use the same λ as for downsampling.
Estimating λ
We perform a series of experiments to verify (1) and estimate λ for four different channel types. Deﬁne r(I,s) ≡f(I,s)/ f(I,0). To estimate λ, we ﬁrst compute µs = E[r(I,s)]
8 . For the gradient histogram channels deﬁned previously as C(i, j) =
M(i, j)1[O(i, j) = q], the 24 resulting values µs for both pedestrian and natural images are
shown in Figure 2(c), top. Observe that µs does not start near 1 as expected: bilinear interpolation smooths an image somewhat even when using a single pixel downsampling rate
(s = ε), in which case E[r(I,ε)] ≈.88. We thus expect µs to have the form µs = ae−λs, with
a ≈.88 as an artifact of the interpolation. To estimate λ and a we use a least squares ﬁt of
λs = ln(a)−ln(µs) to the 24 means computed over natural images, obtaining λ = 1.099 and
a = .89. The agreement between the resulting best-ﬁt curve and the observed data points
is excellent: the average squared error is only 1.8 ×10−5. The best-ﬁt curve obtained from
natural images was also a very good ﬁt for pedestrian images, with average error 3.2×10−4.
We repeat the above experiment for the three additional channel types, results are shown
in Figure 4, top. For every channel type (1) is an excellent ﬁt to the observations µs for
both natural and pedestrian images (with a different λ for each channel). The derivation
of (1) depends on the distribution of image statistics being stationary with respect to image
scale; that this holds for pedestrian images in addition to natural images, and with nearly an
identical constant, implies the estimate of λ is robust and generally applicable.
DOLLÁR, et al.: THE FASTEST PEDESTRIAN DETECTOR IN THE WEST
Approximation Accuracy
We have shown that (1) is an excellent ﬁt for numerous channel types over an ensemble
of images; we now examine the quality of the approximation in (2) for individual images.
To do so, we deﬁne the quantity r∗(I,s) = r(I,s)/ae−λs, with a and λ set to the estimates
obtained previously. r∗(I,s) is normalized such that E[r∗(I,s)] ≈1; for an individual image,
r∗(I,s) ≈1 implies the approximation (2) is accurate. In Figures 2(c) and 4, bottom, we
plot the standard deviation σ∗of r∗(I,s). Low standard deviation implies low average error,
and for s ≤1 (downsampling by at most two), σ∗< .2 for all channel types studied. In
general, σ∗increases gradually and not too steeply as a function of s. The best evidence for
the validity of the approximation, however, is that its use does not signiﬁcantly degrade the
performance of pedestrian detection, as we will show in §5.
Fast Multiscale Detection
In numerous tasks, including sliding window detection, the same features are computed at
multiple locations and scales. In the context of the channel features discussed in §3, this can
be performed efﬁciently assuming C = Ω(I) is shift and scale invariant, respectively. Ωis
shift invariant if computing Ωon a translated version of an image I is the same as translating
the channel Ω(I); likewise Ωis scale invariant if computing Ωon a resampled version of I
is the same as resampling Ω(I). Shift invariance allows for fast single scale detection; scale
invariance allows for fast multiscale detection. Most Ωused in computer vision are shift
invariant but not scale invariant; nevertheless, the approximations developed in §3 can give
us nearly the same speed as true scale invariance.
Most modern detectors utilize features which are not scale invariant, such as gradient
histograms. This includes all top performing pedestrian detectors evaluated
on the Caltech Pedestrian dataset . Without scale invariance, the standard approach is to
explicitly construct an image pyramid and perform detection at each scale separately,
see Figure 1(a). To detect objects larger than the model scale the image is downsampled;
conversely, to detect smaller objects the image is upsampled. At each resulting scale features
are recomputed and single-scale detection applied. Typically, detectors are evaluated on 8-16
scales per octave , and even when optimized, just constructing gradient histograms over a
ﬁnely sampled image pyramid can take over one second per 640×480 image .
Given shift and scale invariance, fast multiscale detection is possible through the construction of a classiﬁer pyramid, which involves rescaling a single detector to multiple scales,
see Figure 1(b). The Viola and Jones detector (VJ) was built using this idea. Utilizing
integral images, Haar like features (differences of rectangular sums) at any position and
scale can be computed with a ﬁxed number of operations . To compute the detector at
different spatial locations, the support of the Haars simply needs to be shifted; to compute the
detector at different scales, the Haar features need to be shifted, their dimensions adjusted,
and a scaling factor introduced to account for their change in area, but otherwise no additional changes are needed. During detection the integral image needs to be computed only
once at the original scale, and since the classiﬁer is fast through the use of integral images
and cascades, so is the resulting overall multiscale detection framework.
Viola and Jones demonstrated a detection framework using scale invariant features that
achieves real time multiscale detection rates; however, scale invariant features tend to be
quite limited. On the other hand, using richer features, such as gradient histograms, leads
to large increases in accuracy but at the cost of much slower multiscale detection. Instead,
DOLLÁR, et al.: THE FASTEST PEDESTRIAN DETECTOR IN THE WEST
(a) scale step
detection rate
detection rate
(c) scales per octave
detection rate
Figure 5: Performance under various parameter choices in the construction of FPDW; evaluated using 25 trials
on the INRIA full image dataset using. The vertical gray lines denote default parameter values used in all other
experiments (scale step of 1, λ = 1.129 as predicted in §3.2, 8 scales per octave). (a) Detection rate decreases very
slowly as the scale step size for the image pyramid increases up to 2 octaves, demonstrating the applicability of
(2) over a wide scale range. (b) Sub-optimal values of λ for the normalized gradient channels causes a decrease in
performance. (c) At least 8 scales per octave are necessary in the classiﬁer pyramid for good performance.
we propose to construct a classiﬁer pyramid using features which are not scale invariant,
utilizing the approximation given in (2). As the quality of the approximation degrades with
increasing distance in scale, we utilize a hybrid approach: we construct a sparsely sampled
image pyramid with a step size of one octave and within each octave we use a classiﬁer
pyramid, see Figure 1(c). In essence, the proposed approach achieves the speed of a classiﬁer
pyramid with the accuracy of an image pyramid. Implementation details are given in §5.
Complexity Analysis
The computational savings of using the hybrid approach over a densely sampled image pyramid can be signiﬁcant. Assume the cost of computing features is linear in the number of
pixels in an n×n image (as is often the case). Typically the image pyramid is sampled using
a ﬁxed number of m scales per octave, with each successive image in the pyramid having
side length 21/m times that of the previous image. The cost of constructing the pyramid is:
n22−2k/m = n2
(4−1/m)k =
1−4−1/m ≈mn2
The second equality follows from the formula for a sum of a geometric series; the last approximation is valid for large m (and follows by a subtle application of l’Hôpital’s rule). In
the hybrid approach we use one scale per octave (m = 1). The total cost is 4
3n2, which is only
33% more than the cost of computing single scale features. Typical detectors are evaluated
on m = 8 to 16 scales per octave , thus according to (3) we expect an order of magnitude
savings by using the proposed hybrid approach (more if upsampled images are used).
Experiments
For the following experiments, we use the ChnFtrs detector described in . The detector
is relatively fast and achieves good performance across a number of pedestrian datasets and
scenarios, as described on the Caltech Pedestrian Dataset website . ChnFtrs spends
most of its computation constructing the feature pyramid, making it an excellent candidate
for our fast detection scheme. No re-training was necessary for this work; instead, we rescale
a pre-trained detector using the approximation in (2) (details below). We refer to our fast
multiscale variant of ChnFtrs as the ‘Fastest Pedestrian Detector in the West’ (FPDW).
The ChnFtrs detector is a generalization of VJ: instead of constructing an integral image
and extracting Haar-like features over just the original intensity image I, multiple channels
C = Ω(I) are used, including gradient magnitude, normalized gradient histogram and LUV
color channels. For additional details see . To resample the detector the Haars need
DOLLÁR, et al.: THE FASTEST PEDESTRIAN DETECTOR IN THE WEST
(a) Caltech Pedestrian Data (peds. ≥100 pixels)
time (seconds)
miss rate at 1 FPPI
Shapelet [0.05 fps]
MultiFtr [0.07 fps]
HikSvm [0.18 fps]
HOG [0.24 fps]
LatSvm−V1 [0.39 fps]
VJ [0.45 fps]
PoseInv [0.47 fps]
LatSvm−V2 [0.63 fps]
ChnFtrs [1.18 fps]
FPDW [6.49 fps]
(b) Caltech Pedestrian Data (peds. ≥50 pixels))
time (seconds)
miss rate at 1 FPPI
Shapelet [0.01 fps]
MultiFtr [0.02 fps]
HikSvm [0.04 fps]
HOG [0.05 fps]
VJ [0.09 fps]
LatSvm−V1 [0.10 fps]
PoseInv [0.10 fps]
LatSvm−V2 [0.16 fps]
ChnFtrs [0.28 fps]
FPDW [2.67 fps]
Figure 6: Time versus detection rate at 1 false positive per image on 640×480 images from the Caltech Pedestrian Dataset . Timing methodology and code may be obtained from . Run times of all algorithms are
normalized to the rate of a single modern machine, hence all times are directly comparable. (Note that the VJ
implementation used did not utilize scale invariance and hence its slow speed). FPDW obtains a speedup of about
10-100 compared to competing methods with a detection rate within a few percent of best reported performance.
to be repositioned, their dimensions adjusted accordingly, and ﬁnally, according to (2), the
output of each Haar must be multiplied by a channel speciﬁc scaling factor eλs (s > 0 for
downsampling, s < 0 for upsampling). For features computed over the gradient channels,
which were L1 normalized, λ = 1.129 is used (see Figure 4(a)). Color channels, like intensity
channels, are scale invariant and λ = ln(4) is used to compensate for the change in feature
area during resampling. Finally, as most Haars can’t be resized exactly (due to quantization
effects), a multiplicative factor can also used to compensate for changes in area.
As the approximation (2) degrades with increasing scale offsets, our hybrid approach is
to construct an image pyramid sampled once per octave and use the classiﬁer pyramid within
half an octave in each direction of the original detector. Details of how this and other choices
in the construction of FPDW affect performance are shown in Figure 5. We emphasize that
re-training was not necessary and all other parameters were unchanged from ChnFtrs.
Overall, FPDW is roughly 5-10 times faster than the ChnFtrs detector; detailed timing
results are reported in Figure 6. Computing the feature pyramid is no longer the bottleneck
of the detector; thus, if desired, additional speedups can now be achieved by sampling fewer
detections windows (although at some loss in accuracy). Finally, in Figure 7 we show fullimage results on three datasets . In all cases the detection rate of FPDW is within 1-
2% of the top performing algorithm, and always quite close to the original ChnFtrs classiﬁer,
all while being 1-2 orders of magnitude faster than competing methods.
false positives per image
Shapelet−orig (90.5%)
PoseInvSvm (68.6%)
VJ−OpenCv (53.0%)
PoseInv (51.4%)
Shapelet (50.4%)
VJ (47.5%)
FtrMine (34.0%)
HOG (23.1%)
HikSvm (21.9%)
LatSvm−V1 (17.5%)
MultiFtr (15.6%)
MultiFtr+CSS (10.9%)
LatSvm−V2 (9.3%)
FPDW (9.3%)
ChnFtrs (8.7%)
(a) INRIA Results
false positives per image
VJ (85.7%)
Shapelet (80.3%)
LatSvm−V1 (67.9%)
PoseInv (65.2%)
FtrMine (57.2%)
LatSvm−V2 (51.8%)
HikSvm (49.7%)
MultiFtr (48.9%)
HOG (48.1%)
MultiFtr+CSS (42.7%)
FPDW (38.7%)
ChnFtrs (36.9%)
(b) Caltech ‘Reasonable’ Results
false positives per image
Shapelet (88.2%)
VJ (86.8%)
LatSvm−V1 (76.3%)
PoseInv (70.4%)
HikSvm (57.5%)
HOG (56.6%)
MultiFtr (54.9%)
LatSvm−V2 (52.6%)
MultiFtr+CSS (40.5%)
FPDW (39.8%)
ChnFtrs (37.5%)
(c) TUD-Brussels Results
The ‘Fastest Pedestrian Detector in the West’ (FPDW) is obtained by rescaling ChnFtrs to multiple
target scales. Results on three datasets are shown (plot legends are ordered by miss rate at 1 false positive per image
– lower is better). In all cases the detection rate of FPDW is within a few percent of ChnFtrs while being 1-2
orders of magnitude faster than all competing methods. Evaluation scripts, detector descriptions, additional results
(including on the ETH dataset and under varying conditions) are all available online at .
DOLLÁR, et al.: THE FASTEST PEDESTRIAN DETECTOR IN THE WEST
Conclusion
The idea motivating our work is that we can approximate features, including gradient histograms, at nearby scales from features computed at a single scale. To take advantage of this,
we proposed a hybrid approach that uses a sparsely sampled image pyramid to approximate
features at intermediate scales. This increases the speed of a state-of-the-art pedestrian detector by an order of magnitude with little loss in accuracy. That such an approach is possible
is not entirely trivial and relies on the fractal structure of the visual world; nevertheless, the
mathematical foundations we developed should be readily applicable to other problems.
Acknowledgments: P. P. was supported by ONR MURI Grant #N00014-06-1-0734. S. B. was supported in part by NSF CAREER Grant #0448615 and ONR MURI Grant #N00014-08-1-0638.