Learning Temporal Regularity in Video Sequences
Mahmudul Hasan
Jonghyun Choi†
Jan Neumann†
Amit K. Roy-Chowdhury
Larry S. Davis‡
UC, Riverside
Comcast Labs, DC†
University of Maryland, College Park‡
{mhasa004@,amitrc@ee.}ucr.edu {jonghyun choi,jan neumann}@cable.comcast.com 
Perceiving meaningful activities in a long video sequence is a challenging problem due to ambiguous deﬁnition of ‘meaningfulness’ as well as clutters in the scene. We
approach this problem by learning a generative model for
regular motion patterns (termed as regularity) using multiple sources with very limited supervision. Speciﬁcally, we
propose two methods that are built upon the autoencoders
for their ability to work with little to no supervision. We
ﬁrst leverage the conventional handcrafted spatio-temporal
local features and learn a fully connected autoencoder on
them. Second, we build a fully convolutional feed-forward
autoencoder to learn both the local features and the classi-
ﬁers as an end-to-end learning framework. Our model can
capture the regularities from multiple datasets. We evaluate our methods in both qualitative and quantitative ways showing the learned regularity of videos in various aspects
and demonstrating competitive performance on anomaly
detection datasets as an application.
1. Introduction
The availability of large numbers of uncontrolled videos
gives rise to the problem of watching long hours of meaningless scenes . Automatic segmentation of ‘meaningful’ moments in such videos without supervision or with
very limited supervision is a fundamental problem for various computer vision applications such as video annotation , summarization , indexing or temporal segmentation , anomaly detection , and activity recognition . We address this problem by modeling temporal regularity of videos with limited supervision, rather than
modeling the sparse irregular or meaningful moments in a
supervised manner.
Learning temporal visual characteristics of meaningful
or salient moments is very challenging as the deﬁnition of
such moments is ill-deﬁned i.e., visually unbounded. On
This work is partially done during M. Hasan’s internship at Comcast
Regular Crowd
Frame 1060
Opposite Direction
Learned Regularity
Irregularity
Figure 1. Learned regularity of a video sequence. Y-axis refers
to regularity score and X-axis refers to frame number. When there
are irregular motions, the regularity score drops signiﬁcantly (from
CUHK-Avenue dataset ).
the other hand, learning temporal visual characteristics of
ordinary moments is relatively easier as they often exhibit
temporally regular dynamics such as periodic crowd motions. We focus on learning the characteristics of regular
temporal patterns with a very limited form of labeling - we
assume that all events in the training videos are part of the
regular patterns. Especially, we use multiple video sources,
e.g., different datasets, to learn the regular temporal appearance changing pattern of videos in a single model that can
then be used for multiple videos.
Given the training data of regular videos only, learning
the temporal dynamics of regular scenes is an unsupervised
learning problem. A state-of-the-art approach for such unsupervised modeling involves a combination of sparse coding and bag-of-words . However, bag-of-words does
not preserve spatio-temporal structure of the words and requires prior information about the number of words. Additionally, optimization involved in sparse coding for both
training and testing is computationally expensive, especially with large data such as videos.
We present an approach based on autoencoders.
objective function is computationally more efﬁcient than
sparse coding and it preserves spatio-temporal information
while encoding dynamics. The learned autoencoder reconstructs regular motion with low error but incurs higher reconstruction error for irregular motions. Reconstruction error has been widely used for abnormal event detection ,
since it is a function of frame visual statistics and abnormal-
 
ities manifest themselves as deviations from normal visual
patterns. Figure 1 shows an example of learned regularity, which is computed from the reconstruction error by a
learned model (Eq.3 and Eq.4).
We propose to learn an autoencoder for temporal regularity based on two types of features as follows. First, we
use state-of-the-art handcrafted motion features and learn a
neural network based deep autoencoder consisting of seven
fully connected layers. The state-of-the-art motion features,
however, may be suboptimal for learning temporal regularity as they are not designed or optimized for this problem.
Subsequently, we directly learn both the motion features
and the discriminative regular patterns using a fully convolutional neural network based autoencoder.
We train our models using multiple datasets including
CUHK Avenue , Subway (Enter and Exit) , and
UCSD Pedestrian datasets (Ped1 and Ped2) , without
compensating the dataset bias . Therefore, the learned
model is generalizable across the datasets. We show that our
methods discover temporally regular appearance-changing
patterns of videos with various applications - synthesizing
the most regular frame from a video, delineating objects
involved in irregular motions, and predicting the past and
the future regular motions from a single frame. Our model
also performs comparably to the state-of-the-art methods on
anomaly detection task evaluated on multiple datasets including recently released public ones.
Our contributions are summarized as follows:
• Showing that an autoencoder effectively learns the regular dynamics in long-duration videos and can be applied to identify irregularity in the videos.
• Learning the low level motion features for our proposed method using a fully convolutional autoencoder.
• Applying the model to various applications including
learning temporal regularity, detecting objects associated with irregular motions, past and future frame prediction, and abnormal event detection.
2. Related Work
Learning Motion Patterns Without Supervision. Learning motion patterns without supervision has received much
attention in recent years .
Goroshinet al. 
trained a regularized high capacity (i.e., deep) neural network based autoencoder using a temporal coherency prior
on adjacent frames. Ramanathanet al. trained a network to learn the motion signature of the same temporal
coherency prior and used it for event retrieval.
To analyze temporal information, recurrent neural networks (RNN) have been widely used for analyzing speech
and audio data . For video analysis, Donahueet al. take
advantage of long short term memory (LSTM) based RNN
for visual recognition with the large scale labeled data .
Duet al. built an RNN in a hierarchical way to recognize actions . The supervised action recognition setup requires
human supervision to train the models. Ranzatoet al. used
the RNN for motion prediction , while we model the
temporal regularity in a video sequence.
Anomaly Detection. One of the applications of our model
is abnormal or anomalous event detection. The survey paper contains a comprehensive review of this topic. Most
video based anomaly detection approaches involve a local feature extraction step followed by learning a model
on training video.
Any event that is an outlier with respect to the learned model is regarded as the anomaly.
These models include mixtures of probabilistic principal
components on optical ﬂow , sparse dictionary ,
Gaussian regression based probabilistic framework ,
spatio-temporal context , sparse autoencoder ,
codebook based spatio-temporal volumes analysis , and
shape . Xuet al. proposed a deep model for anomalous event detection that uses a stacked autoencoder for feature learning and a linear classiﬁer for event classiﬁcation.
In contrast, our model is an end-to-end trainable generative
one that is generalizable across multiple datasets.
Convolutional
Krizhevskyet al.’s work on image classiﬁcation ,
CNN has been widely applied to various computer vision
tasks such as feature extraction , image classiﬁcation , object detection , face veriﬁcation ,
semantic embedding , video analysis ,
Particularly in video, Karpathyet al. and Nget
al. recently proposed a supervised CNN to classify actions
in videos . Xuet al. trained a CNN to detect events
in videos .
Wanget al. learned a CNN to pool the
trajectory information for recognizing actions . These
methods, however, require human supervision as they are
supervised classiﬁcation tasks.
Convolutional Autoencoder. For an end-to-end learning
system for regularity in videos, we employ the convolutional autoencoder. Zhaoet al. proposed a uniﬁed loss function to train a convolutional autoencoder for classiﬁcation
purposes . Nohet al. used convolutional autoencoders for semantic segmentation.
3. Approach
We use an autoencoder to learn regularity in video sequences. The intuition is that the learned autoencoder will
reconstruct the motion signatures present in regular videos
with low error but will not accurately reconstruct motions in
irregular videos. In other words, the autoencoder can model
the complex distribution of the regular dynamics of appearance changes.
As an input to the autoencoder, initially, we use state-ofthe-art handcrafted motion features that consist of HOG and
HOF with improved trajectory features . Then we learn
the regular motion signatures by a (fully-connected) neural
network based autoencoder. However, even the state-of-the-
Crafted Feature based
Autoencoder
Regularity
Video Frames – Weakly Labeled
Learned Feature based
Autoencoder
Get Features
Raw Frames
Backward Pass
Crafted Feature based
Autoencoder
Reconstruction
Video Frames
Learned Feature based
Autoencoder
Get Features
Raw Frames
Irregular Object
Segmentation
Figure 2. Overview of our approach. It utilizes either state-of-the-art motion features or learned features combined with autoencoder to
reconstruct the scene. The reconstruction error is used to measure the regularity score that can be further analyzed for different applications.
art motion features may not be optimal for learning regularity as they are not speciﬁcally designed for this purpose.
Thus, we use the video as an input and learn both local motion features and the autoencoder by an end-to-end learning
model based on a fully convolutional neural network. We
illustrate the overview of our the approach in Fig. 2.
3.1. Learning Motions on Handcrafted Features
We ﬁrst extract handcrafted appearance and motion features from the video frames.
We then use the extracted
features as input to a fully connected neural network based
autoencoder to learn the temporal regularity in the videos,
similar to .
Low-Level Motion Information in a Small Temporal Cuboid.
We use Histograms of Oriented Gradients
(HOG) and Histograms of Optical Flows (HOF) 
in a temporal cuboid as a spatio-temporal appearance feature descriptor for their efﬁciency in encoding appearance
and motion information respectively.
Trajectory Encoding. In order to extract HOG and HOF
features along with the trajectory information, we use the
improved trajectory (IT) features from Wanget al. . It
is based on the trajectory of local features, which has shown
impressive performance in many human activity recognition benchmarks .
As a ﬁrst step of feature extraction, interest points are
densely sampled at dense grid locations of every ﬁve pixels. Eight spatial scales are used for scale invariance. Interest points located in the homogeneous texture areas are
excluded based on the eigenvalues of the auto-correlation
matrix. Then, the interest points in the current frame are
tracked to the next frame by median ﬁltering a dense optical ﬂow ﬁeld . This tracking is normally carried out up
to a ﬁxed number of frames (L) in order to avoid drifting.
Finally, trajectories with sudden displacement are removed
from the set .
Final Motion Feature. Local appearance and motion features around the trajectories are encoded with the HOG and
HOF descriptors. We ﬁnally concatenate them to form a
204 dimensional feature as an input to the autoencoder.
Model Architecture
Next, we learn a model for regular motion patterns on the
motion features in an unsupervised manner. We propose
to use a deep autoencoder with an architecture similar to
Hintonet al. as shown in Figure 3.
Our autoencoder takes the 204 dimensional HOG+HOF
feature as the input to an encoder and a decoder sequentially. The encoder has four hidden layers with 2,000, 1,000,
500, and 30 neurons respectively, whereas the decoder has
three hidden layers with 500, 1,000 and 2,000 neurons respectively. The small-sized middle layers are for learning
compact semantics as well as reducing noisy information.
Input HOG+HOF
Reconstructed HOG+HOF
Figure 3. Structure of our autoencoder taking the HOG+HOF feature as input.
Since both the input and the reconstructed signals of the
autoencoder are HOG+HOF histograms, their magnitude of
them should be bounded in the range from 0 to 1. Thus, we
use either sigmoid or hyperbolic tangent (tanh) as the activation function instead of the rectiﬁed linear unit (ReLU).
ReLU is not suitable for a network that has large receptive
ﬁelds for each neuron as the sum of the inputs to a neuron
can become very large.
In addition, we use the sparse weight initialization technique described in for the large receptive ﬁeld. In the
initialization step, each neuron is connected to k randomly
chosen units in the previous layer, whose weights are drawn
from a unit Gaussian with zero bias. As a result, the total
number of inputs to each neuron is a constant, which prevents the large input problem.
We deﬁne the objective function of the autoencoder by
an Euclidean loss of input feature (xi) and the reconstructed
feature (fW (xi)) with an L2 regularization term as shown
in Eq.1. Intuitively, we want to learn a non-linear classiﬁer
so that the overall reconstruction cost for the ith training
features xi is minimized.
ˆfW = arg min
∥xi −fW (xi)∥2
where N is the size of mini batch, γ is a hyper-parameter to
balance the loss and the regularization and fW (·) is a nonlinear classiﬁer such as a neural network associated with its
weights W.
3.2. Learning Features and Motions
Even though we use the state-of-the-art motion feature
descriptors, they may not be optimal for learning regular
patterns in videos. To learn highly tuned low level features
that best learn temporal regularity, we propose to learn a
fully convolutional autoencoder that takes short video clips
in a temporal sliding window as the input. We use fully
convolutional network because it does not contain fully connected layers. Fully connected layers loses spatial information . For our model, the spatial information needs to be
preserved for reconstructing the input frames. We present
the details of training data, network architecture, and the
loss function of our fully convolutional autoencoder model,
the training procedure, and parameters in the following subsections.
Model Architecture
Figure 4 illustrates the architecture of our fully convolutional autoencoder. The encoder consists of convolutional
layers and the decoder consists of deconvolutional layers that are the reverse of the encoder with padding removal
at the boundary of images.
We use three convolutional layers and two pooling layers
on the encoder side and three deconvolutional layers and
two unpooling layers on the decoder side by considering
the size of input cuboid and training data.
Convolutional Layers
Pooling Layers
Deconvolutional Layers
Unpooling Layers
Input Frames
Reconstructed Frames
10×227×227
10×227×227
Figure 4. Structure of our fully convolutional autoencoder.
The ﬁrst convolutional layer has 512 ﬁlters with a stride
of 4. It produces 512 feature maps with a resolution of
55 × 55 pixels. Both of the pooling layers have kernel of
size 2 × 2 pixels and perform max poling. The ﬁrst pooling layer produces 512 feature maps of size 27 × 27 pixels.
The second and third convolutional layers have 256 and 128
ﬁlters respectively. Finally, the encoder produces 128 feature maps of size 13 × 13 pixels. Then, the decoder reconstructs the input by deconvolving and unpooling the input
in reverse order of size. The output of ﬁnal deconvolutional
layer is the reconstructed version of the input.
Input Data Layer. Most of convolutional neural networks
are for classifying images and take an input of three channels (for R,G, and B color channel). Our input, however, is
a video, which consists of an arbitrary number of channels.
Recent works extract features for each video frame,
then use several feature fusion schemes to construct the input features to the network, similar to our ﬁrst approach described in Sec. 3.1.
We, however, construct the input by a temporal cuboid
using a sliding window technique without any feature transform. Speciﬁcally, we stack T frames together and use them
as the input to the autoencoder, where T is the length of the
sliding window. Our experiment shows that increasing T
results in a more discriminative regularity score as it incorporates longer motions or temporal information as shown in
Training Loss
Regularity Score
Figure 5. Effect of temporal length (T) of input video cuboid.
(Left) X-axis is the increasing number of iterations, Y-axis is the
training loss, and three plots correspond to three different values of
T. (Right) X-axis is the increasing number of video frames and Yaxis is the regularity score. As T increases, the training loss takes
more iterations to converge as it is more likely that the inputs with
more channels have more irregularity to hamper learning regularity. On the other hand, once the model is learned, the regularity
score is more distinguishable for higher values of T between regular and irregular regions (note that there are irregular motions in
the frame from 480 to 680, and 950 to 1250).
Data Augmentation In the Temporal Dimension. As the
number of parameters in the autoencoder is large, we need
large amounts of training data. The size of a given training
datasets, however, may not be large enough to train the network. Thus, we increase the size of the input data by generating more input cuboids with possible transformations to
the given data. To this end, we concatenate frames with
various skipping strides to construct T-sized input cuboid.
We sample three types of cuboids from the video sequences
- stride-1, stride-2, and stride-3. In stride-1 cuboids, all
T frames are consecutive, whereas in stride-2 and stride-
3 cuboids, we skip one and two frames, respectively. The
stride used for sampling cuboids is two frames.
We also performed experiments with precomputed optical ﬂows. Given the gradients and the magnitudes of optical ﬂows between two frames, we compute a single gray
scale frame by linearly combining the gradients and magnitudes. It increases the temporal dimension of the input
cuboid from T to 2T. Channels 1, . . . , T contain gray scale
video frames, whereas channels T +1, . . . , 2T contain gray
scale ﬂow information. This information fusion scheme was
used in . Our experiments reveal that the overall improvement is insigniﬁcant.
Convolutional and Deconvolutional Layer. A convolutional layers connects multiple input activations within the
ﬁxed receptive ﬁeld of a ﬁlter to a single activation output.
It abstracts the information of a ﬁlter cuboid into a scalar
On the other hand, deconvolution layers densify the
sparse signal by convolution-like operations with multiple
learned ﬁlters; thus they associate a single input activation
with patch outputs by an inverse operation of convolution.
Thus, the output of deconvolution is larger than the original
input due to the superposition of the ﬁlters multiplied by the
input activation at the boundaries. To keep the size of the
output mapping identical to the preceding layer, we crop out
the boundary of the output that is larger than the input.
The learned ﬁlters in the deconvolutional layers serve as
bases to reconstruct the shape of an input motion cuboid.
As we stack the convolutional layers at the beginning of the
network, we stack the deconvolutional layers to capture different levels of shape details for building an autoencoder.
The ﬁlters in early layers of convolutional and the later layers of deconvolutional layers tend to capture speciﬁc motion
signature of input video frames while high level motion abstractions are encoded in the ﬁlters in later layers.
Pooling and Unpooling Layer. Combined with a convolutional layer, the pooling layer further abstracts the activations for various purposes such as translation invariance
after the convolutional layer. Types of pooling operations
include ‘max’ and ‘average.’ We use ‘max’ for translation
invariance. It is known to help classifying images by making convolutional ﬁlter output to be spatially invariant .
By using ‘max’ pooling, however, spatial information
is lost, which is important for location speciﬁc regularity.
Thus, we employ the unpooling layers in the deconvolution
network, which perform the reverse operation of pooling
and reconstruct the original size of activations .
We implement the unpooling layer in the same way as
 which records the locations of maximum activations selected during a pooling operation in switch variables
and use them to place each activation back to the originally
pooled location.
Optimization Objective.
Similar to Eq.1, we use Euclidean loss with L2 regularization as an objective function
on the temporal cuboids:
ˆfW = arg min
∥Xi −fW (Xi)∥2
where Xi is ith cuboid, N is the size of mini batch, γ is a
hyper-parameter to balance the loss and the regularization
and fW (·) is a non-linear classiﬁer - a fully convolutionaldeconvolutional neural network with its weights W.
3.3. Optimization and Initialization
To optimize the autoencoders of Eq.1 and Eq.2, we
use a stochastic gradient descent with an adaptive subgradient method called AdaGrad . AdaGrad computes
a dimension-wise learning rate that adapts the rate of gradients by a function of all previous updates on each dimension. It is widely used for its strong theoretical guarantee
of convergence and empirical successes.
We also tested
Adam and RMSProp but empirically chose to use
We train the network using multiple datasets.
shows the learning curves trained with different datasets as
a function of iterations. We start with a learning rate of
0.001 and reduce it when the training loss stops decreasing.
For the autoencoder on the improved trajectory features, we
use mini-batches of size 1, 024 and weight decay of 0.0005.
For the fully convolutional autoencoder, we use mini batch
size of 32 and start training the network with learning rate
8000 10000 12000 14000 16000
Training Loss
All Datasets
Subway Enter
Subway Exit
Figure 6. Loss value of models trained on each dataset and all
datasets as a function of optimization iterations.
We initialized the weights using the Xavier algorithm
 since Gaussian initialization for various network structure has the following problems. First, if the weights in a
network are initialized with too small values, then the signal shrinks as it passes through each layer until it becomes
too small in value to be useful. Second, if the weights in a
network are initialized with too large values, then the signal
grows as it passes through each layer until it becomes too
large to be useful. The Xavier initialization automatically
determines the scale of initialization based on the number
of input and output neurons, keeping the signal in a reasonable range of values through many layers. We empirically
observed that the Xavier initialization is noticeably more
stable than Gaussian.
3.4. Regularity Score
Once we trained the model, we compute the reconstruction error of a pixel’s intensity value I at location (x, y) in
frame t of the video sequence as following:
e(x, y, t) = ∥I(x, y, t) −fW (I(x, y, t))∥2,
where fW is the learned model by the fully convolutional
autoencoder.
Given the reconstruction errors of the pixels of a frame t, we compute the reconstruction error of
a frame by summing up all the pixel-wise errors: e(t) =
(x,y) e(x, y, t). We compute the regularity score s(t) of a
frame t as follows:
s(t) = 1 −e(t) −mint e(t)
For the autoencoder on the improved trajectory feature, we
can simply replace I(x, y) with p(x, y) where p(·) is an improved trajectory feature descriptor of a patch that covers
the location of (x, y).
4. Experiments
We learn the model using multiple video datasets, totaling 1 hour 50 minutes, and evaluate our method both qualitatively and quantitatively. We modify1 and use Caffe 
for all of our experiments on NVIDIA Tesla K80 GPUs.
For qualitative analysis, we generate the most regular
image from a video and visualize the pixel-level irregularity. In addition, we show that the learned model based on
the convolutional autoencoder can be used to forecast future
frames and estimate past frames.
For quantitative analysis, we temporally segment the
anomalous events in video and compare performance
against the state of the arts. Note that our model is not
ﬁne-tuned to one dataset. It is general enough to capture
regularities across multiple datasets.
4.1. Datasets
We use three datasets to train and demonstrate our models. They are curated for anomaly or abnormal event detection and are referred to as Avenue , UCSD pedestrian , and Subway datasets. We describe the details of datasets in the supplementary material.
4.2. Learning a General Model Across Datasets
We compare the generalizability of the trained model using various training setups in terms of regularity scores ob-
1 
tained by each model in Fig. 7. Blue (conventional) represents the score obtained by a model trained on the speciﬁc target dataset. Red (generalized) represents the score
obtained by a model trained on all datasets, which is the
model we use for all other experiments.
Yellow (transfer) represents the score obtained by a model trained on all
datasets except that speciﬁc target dataset. Red shaded regions represent ground truth temporal segments of the abnormal events.
By comparing ‘conventional’ and ‘generalized’, we observe that the model is not degraded by other datasets. At
the same time, by comparing ‘transfer’ and either ‘generalized’ or ‘conventional’, we observe that the model is not
too much overﬁtted to the given dataset as it can generalize
to unseen videos in spite of potential dataset biases. Consequently, we believe that the proposed network structure is
well balanced between overﬁtting and underﬁtting.
CUHK Avenue-# 15
UCSD Ped1-# 32
UCSD Ped2-# 02
Subway Enter-#1
Subway-Exit-#1
Figure 7. Generalizability of Models by Obtained Regularity
Scores. ‘Conventional’ is by a model trained on the speciﬁc target dataset. ‘Generalized’ is by a model trained on all datasets.
‘Transfer’ is by a model trained on all datasets except that speciﬁc
target datasets. Best viewed in zoom.
4.3. Visualizing Temporal Regularity
The learned model measures the intensity of regularity
up to pixel precision. We synthesize the most regular frame
from the test video by collecting the pixels that have the
highest regularity score by our convolutional autoencoder
(conv-autoencoder) and autoencoder on improved trajectories (IT-autoencoder).
The ﬁrst column of Fig. 8 shows sample images that contain irregular motions. The second column shows the synthesized regular frame. Each pixel of the synthesized image corresponds to the pixel for which reconstruction cost
is minimum along the temporal dimension. The right most
column shows the corresponding regularity score. Blue represents high score, red represents low.
Fig. 9 shows the results using IT-autoencoder. The left
column shows the sample irregular frame of a video sequences, and the right column is the pixel-wise regularity
Avenue Dataset
UCSD Ped1 Dataset
UCSD Ped2 Dataset
Subway Exit Dataset
Figure 8. (Left) A sample irregular frame. (Middle) Synthesized
regular frame. (Right) Regularity Scores of the frame. Blue represents regular pixel. Red represents irregular pixel.
score for that video sequence. It captures irregularity to
patch precision; thus the spatial location is not pixel-precise
as obtained by conv-autoencoder.
Avenue Dataset
UCSD Ped2 Dataset
Figure 9. Learned regularity by improved trajectory features.
(Left) Frames with irregular motion. (Right) Learned regularity
on the entire video sequence. Blue represents regular region. Red
represents irregular region.
4.4. Predicting the Regular Past and the Future
Our convolutional autoencoder captures temporal appearance changes since it takes a short video clip as input.
Using a clip that is blank except for the center frame, we can
predict both near past and future frames of a regular video
clip for the given center frame.
Given a single irregular image, we construct a temporal
cube as the input to our network by padding other frames
with all zero values. Then we pass the cube through our
learned model to extrapolate the past and the future of that
center frame. Fig. 10 shows some examples of generated
videos. The objects in an irregular motion start appearing
from the past and gradually disappearing in the future.
Since the network is trained with regular videos, it learns
the regular motion patterns.
With this experiment, we
showed that the network can predict the regular motion of
(0.1 sec before)
(0.1 sec later)
(0.1 sec before)
(0.1 sec later)
Figure 10. Synthesizing a video of regular motion from a single seed image (at the center). Upper: CUHK-Avenue. Bottom:
Subway-Exit.
the objects in a given frame up to a few number of past and
future frames.
4.5. Anomalous Event Detection
As our model learns the temporal regularity, it can be
used for detecting anomalous events in a weakly supervised
manner. Fig. 11 shows the regularity scores as a function of
frame number. Table 1 compares the anomaly detection accuracies of our autoencoders against state-of-the-art methods. To the best of our knowledge, there are no correct detection or false alarm results reported for UCSD Ped1 and
Ped2 datasets in the literature. We provide the EER and
AUC measures from for reference. Additionally, the
state-of-the-art results for the avenue dataset from are
not directly comparable as it is reported on the old version
of the Avenue dataset that is smaller than the current version.
We ﬁnd the local minimas in the time series of regularity scores to detect abnormal events. However, these local
minima are very noisy and not all of them are meaningful
local minima. We use the persistence1D algorithm to
identify meaningful local minima and span the region with
a ﬁxed temporal window (50 frames) and group nearby expanded local minimal regions when they overlap to obtain
the ﬁnal abnormal temporal regions. Speciﬁcally, if two local minima are within ﬁfty frames of one another, they are
considered to be a part of same abnormal event. We consider a detected abnormal region as a correct detection if it
has at least ﬁfty percent overlap with the ground truth.
Our model outperforms or performs comparably to the
state-of-the-art abnormal event detection methods but with
a few more false alarms. It is because our method identi-
ﬁes any deviations from regularity, many of which have not
been annotated as abnormal events in those datasets while
competing approaches focused on the identiﬁcation of abnormal events. For example, in the top ﬁgure of Fig. 11, the
‘running’ event is detected as an irregularity due to its un-
Train Approaching
Wrong Direction
Wrong Direction
Walking Group
Wrong Direction
Figure 11. Regularity score (Eq.3) of each frame of three video sequences. (Top) Subway Exit, (Bottom-Left) Avenue, and (Bottom-Right)
Subway Enter datasets. Green and red colors represent regular and irregular frames respectively.
Regularity
Anomaly Detection
# Anomalous
Correct Detection / False Alarm
Correct Detect / FA
State of the art
State of the art
CUHK Avenue
11, 419/355
12/1 (Old Dataset) 
3, 135/310
92.7/16.0 
90.8/16.0 
Subway Entrance
112, 188/4, 154
Subway Exit
62, 871/1, 125
Table 1. Comparing abnormal event detection performance. AE refers to auto-encoder. IT refers to improved trajectory.
usual motion pattern by our model, but in the ground truth
it is a normal event and considered as a false alarm during
evaluation.
4.6. Filter Responses
We visualize some of the learned ﬁlter responses of our
model on Avenue datasets in Fig. 12. The ﬁrst row visualizes one channel of the input data and two ﬁlter responses
of the conv1 layer. These two ﬁlters show completely opposite responses to the irregular object - the bag in the top
of the frame. The ﬁrst ﬁlter provides very low response
(blue color) to it, whereas the second ﬁlter provides very
high response (red color). The ﬁrst ﬁlter can be described
as the ﬁlter that detects regularity, whereas the second ﬁlter
detects irregularity. All other ﬁlters show similar characteristics. The second row of Fig. 12 shows the responses of the
ﬁlters from conv2 and conv3 layers respectively.
Additional results can be found in the supplementary
material. Data, codes, and videos are available online2.
5. Conclusion
We present a method to learn regular patterns using autoencoders with limited supervision. We ﬁrst take advantage of the conventional spatio-temporal local features and
2 
(a) Input data frame
(b) Conv1 ﬁlter responses.
(c) Conv2 ﬁlter responses.
(d) Conv3 ﬁlter responses.
Figure 12. Filter responses of the convolutional autoencoder
trained on the Avenue dataset. Early layers (conv1) captures ﬁne
grained regular motion pattern whereas the deeper layers (conv3)
captures higher level information.
learn a fully connected autoencoder. Then, we build a fully
convolutional autoencoder to learn both the local features
and the classiﬁers in a single learning framework.
model is generalizable across multiple datasets even with
potential dataset biases. We analyze our learned models
in a number of ways such as visualizing the regularity in
frames and pixels and predicting a regular video of past and
future given only a single image. For quantitative analysis, we show that our method performs competitively to the
state-of-the-art anomaly detection methods.