Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4929–4952
November 7–11, 2021. ©2021 Association for Computational Linguistics
GeDi: Generative Discriminator Guided Sequence Generation
WARNING: This paper contains GPT-3 outputs which are offensive in nature.
Ben Krause∗, Akhilesh Deepak Gotmare∗, Bryan McCann†, Nitish Shirish Keskar
Shaﬁq Joty, Richard Socher†, Nazneen Fatema Rajani
Salesforce Research
{bkrause,akhilesh.gotmare}@salesforce.com
While large-scale language models (LMs) are
able to imitate the distribution of natural language well enough to generate realistic text,
it is difﬁcult to control which regions of the
distribution they generate. This is especially
problematic because datasets used for training large LMs usually contain signiﬁcant toxicity, hate, bias, and negativity. One promising
approach to address this is to use discriminators to guide decoding from LMs, but existing methods for this are too slow to be useful in practice for many applications.
present GeDi as a signiﬁcantly more efﬁcient
discriminator-based approach for guiding decoding. GeDi guides generation at each step
by computing classiﬁcation probabilities for
all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute,
or control code, and another conditioned on
the undesired attribute, or anti control code.
We ﬁnd that GeDi gives controllability on par
with or better than previous controllable generation methods. GeDi results in signiﬁcantly
faster generation speeds than the only previous method that achieved comparable controllability in our experiments. We also show that
GeDi can make GPT-2 and GPT-3 signiﬁcantly
less toxic while maintaining linguistic ﬂuency,
without sacriﬁcing signiﬁcantly on generation
speed. Lastly, we ﬁnd training GeDi on only
three topics allows us to controllably generate
new topics zero-shot from just a keyword.
Introduction
Natural language generation has seen great
progress with the advent of Transformers and large scale training . Large language models (LMs) like GPT-2 and GPT-3 are able
∗Equal Contribution
†Work performed while at Salesforce Research
to learn the distribution of their training set well
enough to generate realistic text. However, simply
imitating the distribution of the training data during generation has many drawbacks ; large-scale text training sets are crawled
from the web, which is imbued with toxicity, bias,
and misinformation. Methods for controlling generation are valuable for making LMs trained on
such data safer and more useful for downstream
applications.
Existing approaches to controlling LMs have
limitations. Class-conditional LMs (CC-LMs) such
as CTRL attempt to control
text generation by conditioning on a control code,
which is an attribute variable representing a data
source. However, using a speciﬁc control code can
reduce sample diversity across prompts, as samples will generally resemble the data source of the
control code.
Another approach for controlling LMs is to use
discriminators to guide decoding, but existing methods to do this are very computationally intensive.
Weighted decoding requires
feeding candidate next tokens into a discriminator,
and thus scales linearly in computation with the
number of tokens to be re-weighted. Plug and Play
LM applies up to
10 updates to the generating LM’s latent states per
time step using gradients from a discriminator, also
making it many times slower than generating from
the LM directly.
We present GeDi1,2 as a signiﬁcantly more efﬁcient algorithm for discriminator guided decoding.
Our proposed method uses class-conditional LMs
as generative discriminators (GeDis) to steer language generation towards desired attributes. We
use GeDis to compute classiﬁcation likelihoods
for all candidate next tokens during generation
using Bayes rule, saving many thousand-fold in
1pronounced “Jedi”
2Code available at 
computation as compared with using a standard
(non-generative) discriminator of the same size to
compute this for large vocabulary sizes. We then
show how these likelihoods can guide decoding
from large language models via weighted decoding
and ﬁltering.
Our experimental results verify the ability of
GeDi to control generation in a variety of settings
while maintaining linguistic quality on par with
strong language models. We apply GeDi (345M parameters) to guide decoding from larger language
models, and ﬁnd that:
• GeDi is very computationally efﬁcient for
both training and inference. GeDi guided decoding in our experiments is more than 30×
faster than applying PPLM with GPT2 using
default settings from Dathathri et al. .
Additionally, smaller GeDis ﬁne-tuned for
less than a day on a single GPU are effective
and computationally efﬁcient for controlling
larger language models.
• GeDi trained on sentiment of movie reviews
can generate book text with a positive or negative tone better than or equivalently to state
of the art baselines [Section 5.1]. Guiding towards positivity also has potential applications
towards making LMs friendlier.
• GeDi is able to signiﬁcantly reduce the toxicity of GPT-2 and GPT-3 generation [Section 5.2], without sacriﬁcing linguistic quality as compared with generating from GPT-2
and GPT-3 directly, suggesting applications
towards safer language modeling.
• GeDi trained on a dataset of only 3 topics
can generalize to new control codes zero-shot
[Section 5.3], allowing them to guide generation towards a wide variety of topics.
Background
Language modeling
Language models (LMs) rely on an auto-regressive
factorization to perform density estimation and generation of sequences. Auto-regressive sequence
models with parameters θ assign a probability to
a sequence x1:T = {x1, . . . , xT } by factorizing it
using the chain rule by applying
Pθ(x1:T ) =
Pθ(xt|x<t).
Models can assign probabilities to sequences by
iteratively predicting a distribution over the next
token given the previous tokens. Generating from
language models requires iteratively sampling from
Pθ(xt|x<t), and then feeding xt back into the
model as input for the next step.
Class-Conditional Language modeling
Class-conditional language models (CC-LMs) such
as CTRL are a way for language models to generate while conditioning on an
attribute variable. CC-LMs predict a probability
distribution Pθ(x1:T |c), where c is a class variable
or a “control code” that describes an attribute of
the text in x1:T , which could, for instance, describe
sentiment or topic. The auto-regressive factorization for a CC-LM is given by
Pθ(x1:T |c) =
Pθ(xt|x<t, c).
When training a CC-LM on a training set of
sequences {x(1)
1:T1, . . . , x(i)
1:Ti, . . . , x(N)
1:TN }, each sequence x(i)
1:T is paired with a control code c(i),
which is a label or category of the sequence. The
LM is trained to minimize the average negative
log-likelihood, L, given by
log Pθ(x(i)
<t, c(i)).
In addition to class-conditional generation, CC-
LMs can be used as generative classiﬁers by
applying Bayes rule to compute Pθ(c|x1:T ) ∝
P(c)Pθ(x1:T |c), as is done by Keskar et al. 
for source attribution.
An attribute discriminator can be used to guide
decoding from a language model. For instance,
given context x<t, and base language modeling
distribution PLM(xt|x<t), the discriminator could
compute Pθ(c|xt, x<t) for every possible next token xt. Generation could then be guided using a
weighted decoding heuristic via
Pw(xt|x<t, c) ∝PLM(xt|x<t)Pθ(c|xt, x<t)ω,
where ω > 1 to bias generation more strongly towards the desired class. The right hand side of
<positive>
<negative>
<positive>
desired attribute: positive
Figure 1: A toy example of how GeDi-guided decoding uses Bayes rule to efﬁciently compute classiﬁcation probabilities for possible next tokens at each generation timestep using only element-wise operations.
These classiﬁcation probabilities can then be used to
guide generation from a language model (e.g., GPT-2)
to achieve attribute control across domains. If a class
conditional language model was trained on movie reviews for sentiment control, its direct class-conditional
predictions will be biased towards predicting movie review words (illustrated by next word prediction of “cinematic”). However, the bias towards movie reviews can
be canceled out by contrasting the predictions of opposing control codes via Bayes rule.
Discriminator
Discriminator
Discriminator
Discriminator
Discriminator
was cinematic
Figure 2: A toy example of using a language model
with a discriminator head to guide next token generation. This requires feeding in each word in the vocabulary to compute the probability that the resulting generation would have positive sentiment, and using these
probabilities to guide the base language model (e.g.,
GPT-2) towards positive sentiment. This requires |V|
times the amount of computation to compute the ﬁnal
hidden states of the network as compared with using
GeDi if computing for the full vocabulary and using
the same neural architecture for both methods.
Equation (4) is normalized over all xt in the vocabulary to obtain Pw(xt|x<t, c). Applying this
to guide decoding is very inefﬁcient for standard
discriminators; using a language model with a
discriminator head such as GPT or BERT to compute
Pθ(c|xt, x<t) would require feeding in every possible input xt ∈V into the classiﬁer, and thus would
require |V| forward passes for a vocab set V to
compute the ﬁnal hidden states for the network.
The motivation of GeDi is to efﬁciently compute
Pθ(c|xt, x<t) with a generative discriminator without a separate forward pass for each candidate next
GeDi assumes we have a CC-LM with desired
control code c and an undesired or anti-control
code ¯c, and uses the contrast between Pθ(x1:t|c)
and Pθ(x1:t|¯c) to guide sampling from an LM that
gives PLM(x1:t). Speciﬁcally, when predicting
the next token during generation, GeDi uses this
contrast to compute the probability that every candidate next token xt belongs to the desired class,
given by Pθ(c|xt, x<t). This distribution can be
computed very efﬁciently when using CC-LMs as
GeDis via application of Bayes rule for partial sequences during generation via
Pθ(c|x1:t) =
j=1 Pθ(xj|x<j, c)
j=1 P(c′)Pθ(xj|x<j, c′).
When computing this online during sequence generation, the model will have already computed
Pθ(xj|x<j, c′) for any j < t from the previous time-steps, and it will only need to compute
Pθ(xt|x<t, c′). This can be computed in two parallel forward passes; one conditioning on c and
one conditioning on ¯c (both conditioning on the
same x<t) as illustrated in Figure 1. In contrast,
an LM with a binary discriminator head requires
computing |V| forward passes to compute attribute
probabilities for all candidate next tokens, as illustrated in Figure 2. While GeDi uses a larger
output layer than an LM with a discriminator head,
computing 2 forward passes through an LM with
a softmax head (in the case of GeDi) is still many
times more efﬁcient than computing |V| forward
passes through an LM with a binary discriminator
head, especially for modern Transformer architectures (or any architecture with many hidden layers)
where computing the ﬁnal hidden state is the bottleneck in the forward pass computation. While a
very small discriminator could also be used to ef-
ﬁciently guide generation, we ﬁnd experimentally
that this does not give strong attribute control.
In practice, applying Equation (5) to long sequences often results in poorly calibrated distributions later in the sequence that assign classiﬁcation
probabilities of 1 or 0 to all candidate next words,
which provides no useful signal. We addressed this
by normalizing probabilities by current sequence
length t. To compute Pθ(c|x1:t) for GeDi-guided
decoding, we use
Pθ(c|x1:t) =
(Pθ(x1:t|c))1/t
c′∈{c,¯c} Pθ(x1:t|c′)1/t ,
where class priors P(c) are omitted because we use
balanced classes for training. With the efﬁcient estimation of Pθ(c|xt, x<t), LM generation can be ef-
ﬁciently guided using Equation (4). This inherently
contrasts predictions conditioned on c and ¯c, causing attributes common to c and ¯c to be cancelled
out, more effectively allowing for the attribute described by c to be transferred across domains. For
instance, if Pθ(x1:t|c) captures a distribution over
positive movie reviews, and Pθ(x1:t|¯c) captures a
distribution over negative movie reviews, contrasting the two distributions will cancel out predictions
speciﬁc to movie reviews and better generalize the
concepts of positivity and negativity. In addition
to Equation (4), we also apply a ﬁltering heuristic
described in Appendix A that zeros out a portion of
the next token distribution with a lower Pθ(c|x1:t).
We summarize GeDi in Algorithm 1.
Multi-topic GeDi
To efﬁciently extend GeDi to the multi-class
setting, we propose reframing each classiﬁcation
task as binary classiﬁcation using control codes
and anti control codes for each class. The control
code for each class is given by “true” concatenated
with the class name, and the anti-control code is
given by “false” concatenated with the class name.
The CC-LM can then classify whether the class
name corresponds to the text. For instance, if the
CC-LM processed the following two sequences:
<true> <science>
T-rex achieved its massive
size due to an enormous growth spurt during its
adolescent years.
<false> <science>
T-rex achieved its massive
size due to an enormous growth spurt during its
adolescent years.
Algorithm 1 GeDi-guided decoding
Inputs: base LM PLM, CC-LM Pθ, vocabulary V,
posterior mixing weight ω, decoding scheme
1: P(x|c) ←1
2: P(x|¯c) ←1
3: for t = 1 . . . , N do
pLM ←[PLM(xt = v|x<t) for v in V]
px1:t|c ←[(P(x|c)Pθ(xt = v|x<t, c))1/t
for v in V]
px1:t|¯c ←[(P(x|¯c)Pθ(xt = v|x<t, ¯c))1/t
for v in V]
pc|x1:t ←px1:t|c ⊙
(px1:t|c+px1:t|¯c)
pw ←pLM ⊙(pc|x1:t)ω
vi ←Decode(pw)
P(x|c) ←P(x|c)Pθ(xt = vi|x<t, c)
P(x|¯c) ←P(x|¯c)Pθ(xt = vi|x<t, ¯c)
it could classify the text as true or false as to
whether the class (in this case “science”) matches
the category of the text by using Equation (6). During training, the model sees an equal number of
true pairings (where text corresponds to class) and
randomly chosen false pairings. After the model
has been trained, binary GeDi-guided decoding
can be applied, using c =<true> and ¯c =<false>,
and using the desired class name as the ﬁrst token
(x1) in the sequence. This also makes it possible
to form new control codes zero-shot; a new topic
word that was never seen before in training can be
chosen in place of x1. This works well when GeDi
is initialized as a pretrained language model, as
the model will have learned embeddings for many
topics during its pretraining that can be used as
zero-shot control codes.
Related Work
Methods for controlling text generation can be categorized broadly into two categories: training or
ﬁnetuning a model directly for controllable generation or using a
discriminator to guide decoding . Keskar et al. train a CC-LM with predeﬁned control codes placed at the start of every
sequence. GeDi also uses CC-LMs, but instead of
generating from them directly, GeDi uses them as
discriminators to guide decoding from another language model. This is much more computationally
efﬁcient than previous methods for discriminator
guided decoding. Holtzman et al. apply discriminators to re-weight a beam search, requiring
all candidate tokens to be passed through the discriminator, scaling linearly with the number of rescored tokens. PPLM trains
an attribute model on top of a language model’s
last hidden layer and backpropagates gradients to
update the hidden states of the model. This is computationally intensive because it requires multiple
forward and backward passes for each generation
step. For instance, applying PPLM with 10 update steps as done in Dathathri et al. would
require an additional factor of 20 fold computation (10 forward passes, 10 backward passes) as
compared to base LM generation at the ﬁrst decoding timestep. This factor also increases as the
sequence length increases, since PPLM updates the
previously stored keys and values. GeDi in comparison only adds constant overhead that is independent of the size of the base LM, and this constant
will be minimal if the GeDi is signiﬁcantly smaller
than the base LM. GeDi also relates to the rational speech acts framework for computational pragmatics where a “listener” model and a
“speaker” model interactively generate a sequence
such that the listener can recover the input. GeDi
most closely relates to distractor based pragmatics , where a single model processes a true input and a distractor input, and uses
Bayes rule to produce text that ﬁts the true input but
not the distractor input. GeDi differs from previous pragmatics based approaches in that it trains a
separate class-conditional language model (which
acts as the listener) on a single attribute, allowing
that attribute to be isolated, and uses it to guide
generation from a separate language model (which
acts as the speaker).
Other previous works seek to understand and
address toxicity and hate speech in language generation. RealToxictyPrompts 
gives an automatic evaluation of toxicity using generations from different language models using a
set of webtext prompts. 
also tests methods for mitigating toxicity, and ﬁnds
that applying PPLM was more effective than simpler decoding-based detoxiﬁcation methods such
as swear word ﬁlters. Xu et al. develop a
human in the loop method for adversarially probing
toxic responses in conversational agents, and train a
model to give preset responses when encountering
potentially unsafe probes. Other work has focused
on removing gender bias from language models
 . Related to the problem of
addressing toxicity in generation is toxicity detection, which can be performed using the Perspective
API or using a classiﬁer trained on a labelled toxicity dataset such as the Jigsaw Toxic Comment
Classiﬁcation Dataset . Toxicity detection is difﬁcult as toxicity labelling is
subjective and often has poor annotator agreement
 . Additionally,
existing toxicity classiﬁers are often biased in that
they overestimate the toxicity of text that mentions
sexual orientations or racial minorities .
Experiments
We experiment with GeDi-guided decoding for sentiment, detoxiﬁcation, and topic control. We ﬁnetune GPT2-medium (345M parameter) using the loss in Equation (3) with
control codes speciﬁc to each task to form a classconditional language model. We use these CC-LMs
as GeDis to guide generation from GPT2-XL (1.5B
parameter), and GPT-3 in our
detoxiﬁcation experiments. All experiments were
performed using adaptations of Huggingface Transformers .
We include experiments with greedy decoding
with a repetition penalty (conditioning on varying prompts to give diversity
across generations), which we found to give the
best quality generations, and top-p sampling . Our hyper-parameter settings for
GeDi-guided generation are given in Appendix C.1.
We also perform ablation studies in Appendix D,
and ﬁnd that combining both the weighted decoding and ﬁltering heuristics appears to be beneﬁcial
although is not critical to the success of the method,
Generation time
(sec/token)
GeDi-guided (w/ GPT2-XL)
PPLM (w/ GPT2-XL)
Table 1: Average generation time in seconds per token
for generating sequences of length 256 on a V100 GPU.
and that applying a very small LSTM discriminator that can
match the efﬁciency of GeDi is not as effective for
controlling generation.
Controlling sentiment of generations
from book prompts
We experiment with GeDi-guided decoding from
GPT-2 for sentiment control using CC-LMs ﬁnetuned on IMDb movie reviews. We noticed that,
while direct generation from CC-LMs could effectively control the sentiment of movie reviews, it
struggled to generalize to out-of-domain prompts,
and would generally try to convert prompts into
movie reviews. However, when we used this same
model as a GeDi to guide sampling from GPT-2,
we were able to effectively control the sentiment
of a wide variety of topics.
To experimentally verify that GeDi can generalize the concepts of “positivity” and “negativity”
beyond its training domain, we evaluate on a task
where models conditionally generate text from the
start of book chapters from Bookcorpus , and each prompt is at least 150 characters
and ends on the ﬁrst word break after the minimum length. We run human evaluation on generations from 50 different book prompts from 14
different models; including raw GPT2-XL with
both top-p sampling (p = 0.9) and greedy decoding (repetition penalty=1.2), and the following
models with both positive and negative sentiment:
1. GPT2-XL guided by GeDi, greedy decoding
(repetition penalty of 1.2). 2. GPT2-XL guided
by GeDi, top-p sampling with p = 0.9 (repetition
penalty of 1.05). 3. PPLM (w/GPT2-XL), greedy
decoding (repetition penalty of 1.2). 4. PPLM
(w/GPT2-XL), top-p sampling with p = 0.9.
5. CC-LM trained on movie reviews (same model
used as GeDi, but with direct CTRL-style generation), greedy decoding (repetition penalty of 1.2).
6. CTRL using control codes
for Amazon review sentiment, greedy decoding
(repetition penalty of 1.2).
CTRL was applied using the control codes corresponding to positive and negative Amazon reviews
used during training by Keskar et al. . The
PPLM discriminator was trained on SST-5 as in
Dathathri et al. , with the step size parameter retuned for GPT2-XL used GPT2-medium.). We found that it was
more than 30× faster to guide GPT2-XL with a
GeDi as compared with PPLM and in
our experiments), as shown in Table 1.
Amazon Mechanical Turk annotators rated the
generated text on sentiment, how book-like the text
was, ﬂuency, and whether or not the text resembled
an Amazon review or movie review (since CTRL
was trained on Amazon reviews and GeDi was
trained on movie reviews). Instructions given to
annotators are given in Appendix G. The results of
the experiment are given in Table 2. Using GeDi to
guide GPT2-XL was able to generate book-like and
linguistically ﬂuent text while giving strong control over the tone. In the greedy setting, GeDi was
also able to give roughly equivalent positive sentiment control and statistically signiﬁcantly stronger
negative sentiment control compared with PPLM
(p < 0.01 by two-tailed Wilcoxon signed rank test).
In the top-p setting, GeDi achieved statistically signiﬁcantly stronger sentiment control than PPLM
for both positive and negative sentiment (p = 0.01
and p = 0.005 for positive and negative sentiment
respectively). p-values for all signiﬁcance tests are
given in Appendix E. We include samples from all
greedy decoding models in Tables 11, 12, 13.
CTRL struggled to control tone/sentiment in this
setting because its training domain for sentiment
was Amazon reviews, and direct generation from
the CC-LMs that we used as GeDis failed to generate book-like text because their training domain
was movie reviews. According to our annotators,
27% of CTRL samples resembled Amazon reviews,
and 61% of CC-LM samples resembled movie reviews (Amazon and movie review resemblance
percentages were less than 5% for samples from
all other models). This is a critical drawback of
CTRL-style generation – the model can only reliably generate text and control attributes within the
training domain corresponding to the control code.
Samples that illustrate this are given in Table 14.
Discriminator-guided methods GeDi and PPLM
Positivity
Book-like ↑
Label ﬁdelity ↑
Perplexity score ↓
GeDi-guided-pos (greedy)
GeDi-guided-pos (top-p)
PPLM-pos (greedy)
PPLM-pos (top-p)
CC-LM-pos (greedy)
CTRL-pos (greedy)
GPT2-XL (greedy)
GPT2-XL (top-p)
CTRL-neg (greedy)
CC-LM-neg (greedy)
PPLM-neg (top-p)
PPLM-neg (greedy)
GeDi-guided-neg (top-p)
GeDi-guided-neg (greedy)
Table 2: Human and automatic evaluation for sentiment on book text generation (rated for positivity, book resemblance and ﬂuency all on a scale of 1-5). For human evaluation, we average three annotations on generations from
50 prompts for each model, where prompts are from the start of book chapters, and are a minimum of 150 char. For
automatic evaluation, we use a RoBERTa classiﬁer trained on SST-2 to measure label ﬁdelity
(how often the sample is classiﬁed as having the same label as the control code), and measure the perplexity of
generations under GPT-2 to compute perplexity scores. We compare using a CC-LM as a GeDi to guide GPT2-XL
(GeDi-guided), vs. direct class conditional generation (CC-LM). GeDi gives the strongest control over sentiment.
PPLM also gives strong sentiment control, but results in generation 30× slower.
result in text rated more book-like that very rarely
if ever reverts back to the domain that the discriminator was trained on. However, as compared with
PPLM, GeDi was able to generate 30× faster, and
sentiment control that was on par with or better
than PPLM in all settings.
Detoxifying GPT-2 and GPT-3
We test GeDi’s ability to detoxify language generation. We train a CC-LM on the Jigsaw Toxic Comment Classiﬁcation Dataset ,
which contains text samples labeled as “toxic” or
“non-toxic”. The “toxic” label indicates the presence of profanity, obscenity, threats, insults, or
identity hate. We train the model on an even split
of toxic and non-toxic examples, with “clean” and
“dirty” control codes to specify toxic and non-toxic
text. For evaluation, we use generations conditioned on RealToxicityPrompts . We consider two toxicity evaluations, one
based on automatic toxicity evaluations from a
large number of prompts following Gehman et al.
 , and one using human annotations on a
smaller number of trigger prompts that tend to lead
to especially toxic generations from LMs. We experiment with the same models as in the previous
section (expect for pretrained CTRL, which does
not have a detoxiﬁcation control code), but also add
results using 1. GPT3 using Open AI API, greedy
(repetition penalty of 1.2). 2. GPT3 using Open AI
API, guided by GeDi, greedy (repetition penalty
of 1.2). We add details of how we apply GeDi to
GPT-3 in Appendix B.
For our large-scale automatic evaluation, we select 5000 prompts from RealToxicityPrompts at
random and draw generations from each model.
Following Gehman et al. , we measure the
expected toxicity score and toxicity probability separately for generations from toxic and non-toxic
prompts using the Perspective API 3, which is a
toxicity classier that returns a probability between
0 and 1 that the submitted text is toxic. The expected toxicity is given by the average classiﬁcation
probability under Perspective’s toxicity classiﬁer
of continuations from a given model, whereas the
toxicity probability is the fraction of generations
that the Perspective API classiﬁes as having a toxicity probability greater than 0.5. For models that
use sampling, we draw 10 generations from each
prompt, and use the most toxic continuation as evaluated by the Perspective API to measure all statistics, following the expected max toxicity scores
and probabilities used by Gehman et al. .
The results are given in Table 3. GeDi was able to
reduce the toxicity of GPT-2 and GPT-3 and gave
a stronger detoxiﬁcation effect as compared with
PPLM (The reductions in expected toxicity of GeDi
vs. PPLM, GeDi vs. GPT-2, and GeDi vs. GPT-3
were strongly statistically signiﬁcant in all comparisons by a paired sample t-test). The advantage
3 
Expected toxicity ↓
Toxicity probability ↓
toxic prompt
non-toxic prompt
toxic prompt
non-toxic prompt
GPT2-XL (top-p, most toxic of 10 per prompt)
GeDi-guided GPT-2 (top-p, most toxic of 10 per prompt)
PPLM (top-p, most toxic of 10 per prompt)
GPT2-XL (greedy)
GeDi-guided GPT-2 (greedy)
PPLM (greedy)
CC-LM (greedy)
GPT-3 da-vinci (greedy)
GeDi-guided GPT-3 (greedy)
Table 3: RealToxicityPrompts automated toxicity evaluation. We measure the expected toxicity score (with standard deviation given in subscript) and toxicity probability from continuations from toxic (perspective toxicity score
> 0.5) and non-toxic (perspective toxicity score < 0.5) prompts for 9 models. Generations from 5000 prompts
were used ). For
models that use top-p sampling, we measure the expected toxicity and toxicity probability of the most toxic sample out of 10 generations per prompt. For generation with greedy models we simply average these metrics across
prompts. GeDi signiﬁcantly reduced the toxicity of GPT-2 and GPT-3 and resulted in a stronger detoxiﬁcation
effect as compared with PPLM.
of GeDi over PPLM was especially pronounced
in the case of top-p sampling, where PPLM generated at least one toxic sample (out of 10 samples
per prompt) from a non-toxic prompt more than 3
times as often, suggesting that GeDi is more robust
to worst case scenarios when applying sampling.
We also applied human evaluation to measure
toxicity using a smaller number of prompts that
probe LMs to generate toxic text. To identify strong
triggers, we selected a subset of prompts with Perspective API toxicity probabilities between 0.3 and
0.5, that also were classiﬁed as non-toxic by a
RoBERTa toxicity classiﬁer trained on the Jigsaw
dataset. We used GPT2-XL to draw 32 samples
from each prompt, and selected the 100 prompts
with the highest average toxicity probability over
their 32 completions according to the RoBERTa
toxicity classiﬁer. Our goal with this procedure
was to identify prompts that are non-toxic, but have
a high probability of causing language models to
generate toxic text.
We ran human evaluation to measure toxicity
and linguistic ﬂuency [1: very low ﬂuency, 5: very
high ﬂuency]. Results are given in Table 4 and
generations from evaluated models are given in
Table 15. GeDi was able to signiﬁcantly reduce
the toxicity in GPT-2 and GPT-3 (p < 0.001 by a
2 proportion z-test in all settings). GeDi resulted
in a similar toxicity as compared with PPLM for
greedy decoding and was signiﬁcantly less toxic
than PPLM for sampling (p = 0.02), while also
achieving 30× faster generation speeds.
Toxicity ↓
(human eval)
(human eval)
GPT2-XL (top-p)
GeDi-guided GPT-2 (top-p)
PPLM (top-p)
GPT2-XL (greedy)
GeDi-guided GPT-2 (greedy)
PPLM (greedy)
CC-LM (greedy)
GPT-3 da-vinci (greedy)
GeDi-guided GPT-3 (greedy)
Table 4: Human evaluation of toxicity on 100 trigger
prompts. We collect 3 annotations of toxicity labels
(where we classify each sample based on majority) and
linguistic ﬂuency scores (scale of 1-5) for each model.
We ﬁnd that GeDi is effective for detoxifying GPT-2
and GPT-3 while maintaining ﬂuency.
Extending GeDi to the multi-class setting
To experiment with multi-class GeDi, we use the
AG news topic classiﬁcation data set which has 4 topics (World, Sports, Business, and Science/Tech). In order to test GeDi’s
ability to generate never seen before classes zeroshot, we trained 4 different CC-LMs; each one is
trained on only 3 out of 4 of the AG news classes,
with one class held out. We then compare direct
(CTRL-style) generation from CC-LMs with GeDiguided decoding from GPT-2, on topics included
in training and held out (zero-shot) topics. To evaluate topic relevance, we use a RoBERTa classiﬁer
trained on all 4 AG news topics to estimate the
topic of generation. We obtain generations conditioning on short (minimum 30 characters, ending
on a space) prompts from the multi-news data-set
 , and report results in Table 5.
Trained on class
(Label ﬁdelity)
(Label ﬁdelity)
GeDi-guided
GeDi-guided
GeDi-guided
GeDi-guided
Table 5: Automatic label ﬁdelity on topics, measured
by how often a RoBERTa classiﬁer’s label matches
the control code used to generate the sample.
trained 4 different CC-LMs, each with 1 class held out
and we considered direct CTRL-style generation (CC-
LM), and GeDi-guided decoding from these models.
“trained on class” label ﬁdelity averages the label ﬁdelities from 3 models trained with the given class as one of
the training classes. The “zero-shot” label ﬁdelity for
each class uses generations from the model trained on
the other 3 classes, using a zero-shot control code for
the desired class. We include results from raw GPT-2-
XL to show how much GeDi and CC-LM are inﬂuencing generation. We ﬁnd that GeDi is able to inﬂuence
generation more effectively than CC-LM when conditioning on both training classes and held out classes.
GeDi was able to generate topics included in
training with a higher label ﬁdelity than CTRLstyle generation from a CC-LM. Unlike CC-LM,
GeDi was able to bias generation towards never
seen before zero-shot control codes that are held
out from training. GeDi’s ability to generalize to
new control codes zero-shot gives the ability to
generate text corresponding to many topics and
subtopics. This ability likely emerges because generative classiﬁers can classify unseen topics zeroshot from learned word embeddings , and GeDi uses a generative classiﬁer
to guide generation. While GPT-3 can also generate topics zero shot by conditioning prompts such
as “Write an article about sports:”, zero-shot generation with GeDi does not necessarily need to be
an article or have any other constraints that would
come about from the prompt. We provide examples
of zero-shot topic generation with GeDi in Table 6.
Conclusion
We present GeDi as an approach for controllable
generation that uses generative discriminators to
classify candidate next tokens on the ﬂy during in-
GeDi-guided generation
In a shocking finding NASA have
announced the discovery of
a mysterious object orbiting
our nearest neighbour, Proxima
Advertisement
Researchers have spent years
studying the strange object in
space as they are increasingly
becoming more convinced that it’s
In fact, some researchers
are starting to wonder if this
new discovery may prove to be
one of the greatest scientific
discoveries of recent years.
In a shocking finding police
believe two fire crews, including
a senior paramedic, were
deliberately set alight as part
of a revenge plot.
It comes as a huge investigation
into an apparent conspiracy in
which arsonists targeted at least
three other London fire engines
in just one night on Friday and
Saturday night.
In a shocking finding historians
believe to be "unprecedented"
British documents have been
unearthed which reveal the true
history of King Richard II and
show that he was not only the
son of Godfrey of Gloucester, but
also descended from King Henry
Richard, whose father was
executed for his crimes in
1483, became King in 1485 after
defeating John Balliol in a
battle at Bosworth.
Table 6: Controlling topic of generation (zero-shot)
with GeDi (greedy decoding). This topic GeDi was
trained on only three classes: science, sports and business. The topics of Space, Fire, and History were not
a part of the GeDi training set. Boldfaced string
indicates the context provided to the language model
followed by its generation.
ference, making it far more efﬁcient than previous
methods that use discriminators to guide decoding.
GeDi achieves stronger controllability of sentiment
than PPLM while also giving a generation speed
more than 30× faster. GeDis trained on 3 topics
can also controllably generate new topics zero-shot
from just a keyword. We also show that GeDi is
able to signiﬁcantly reduce the toxicity of GPT-2
and GPT-3 without sacriﬁcing noticeably on linguistic ﬂuency. GeDi moves towards unifying natural language generation with classiﬁcation, and
suggests that we may be able to efﬁciently generate
text that corresponds to any attribute that we can
accurately classify. This could have broad implications for improving text generation systems by
making them more controllable.