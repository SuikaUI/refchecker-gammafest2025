Validation, comparison, and combination of algorithms for automatic detection
of pulmonary nodules in computed tomography images: The LUNA16 challenge
Arnaud Arindra Adiyoso Setioa,1, Alberto Traversog,h,a,1, Thomas de Belo, Moira S.N. Berensn,
Cas van den Bogaardo, Piergiorgio Cerelloh, Hao Chenm, Qi Doum, Maria Evelina Fantaccik,i, Bram Geurtsb,
Robbert van der Gugtenn, Pheng Ann Hengm, Bart Jansene,f, Michael M.J. de Kasten, Valentin Kotovo,
Jack Yu-Hung Linj, Jeroen T.M.C. Mandersn, Alexander S´o˜nora-Menganad,e,1, Juan Carlos Garc´ıa-Naranjod,
Evgenia Papavasileioue, Mathias Prokopa, Marco Salettah, Cornelia M Schaefer-Prokopa,c, Ernst T. Scholtena,
Luuk Scholteno, Miranda M. Snoerenb, Ernesto Lopez Torresl, Jef Vandemeulebrouckee,f, Nicole Walaseko,
Guido C.A. Zuidhofn, Bram van Ginnekena,p, Colin Jacobsa
aDiagnostic Image Analysis Group, Radboud University Medical Center, Nijmegen, The Netherlands
bDepartment of Radiology and Nuclear Medicine, Radboud University Medical Center, Nijmegen, The Netherlands
cDepartment of Radiology, Meander Medisch Centrum, Amersfoort, The Netherlands
dCentro de Biof´ısica M´edica, Universidad de Oriente, Santiago de Cuba, Cuba
eDepartment of Electronics and Informatics, Vrije Universiteit Brussel, Brussels, Belgium
fimec, Leuven, Belgium
gDepartment of Applied Science and Technology, Polytechnic University of Turin, Turin, Italy
hTurin Section of Istituto Nazionale di Fisica Nucleare, Turin, Italy
iPisa Section of Istituto Nazionale di Fisica Nucleare, Pisa, Italy
jYan’an Xi Lu 129, 9th ﬂoor, Shanghai, China
kDepartment of Physics, University of Pisa, Pisa, Italy
lCenter of Applied Technologies and Nuclear Development, La Habana, Cuba
mDepartment of Computer Science and Engineering, The Chinese University of Hong Kong, China
nRadboud University, Nijmegen, The Netherlands
oInstitute for Computing and Information Sciences, Radboud University Nijmegen, Nijmegen, The Netherlands
pFraunhofer MEVIS, Bremen, Germany
Automatic detection of pulmonary nodules in thoracic computed tomography (CT) scans has been an active area of
research for the last two decades. However, there have only been few studies that provide a comparative performance
evaluation of diﬀerent systems on a common database. We have therefore set up the LUNA16 challenge, an objective evaluation framework for automatic nodule detection algorithms using the largest publicly available reference
database of chest CT scans, the LIDC-IDRI data set. In LUNA16, participants develop their algorithm and upload
their predictions on 888 CT scans in one of the two tracks: 1) the complete nodule detection track where a complete
CAD system should be developed, or 2) the false positive reduction track where a provided set of nodule candidates
should be classiﬁed. This paper describes the setup of LUNA16 and presents the results of the challenge so far.
Moreover, the impact of combining individual systems on the detection performance was also investigated. It was
observed that the leading solutions employed convolutional networks and used the provided set of nodule candidates.
The combination of these solutions achieved an excellent sensitivity of over 95% at fewer than 1.0 false positives
per scan. This highlights the potential of combining algorithms to improve the detection performance. Our observer
study with four expert readers has shown that the best system detects nodules that were missed by expert readers who
originally annotated the LIDC-IDRI data. We released this set of additional nodules for further development of CAD
Keywords: pulmonary nodules, computed tomography, computer-aided detection, medical image challenges, deep
learning, convolutional networks
 
1. Introduction
Lung cancer is the deadliest cancer worldwide,
accounting for approximately 27% of cancer-related
deaths in the United States ). The NLST trial showed that three annual
screening rounds of high-risk subjects using low-dose
computed tomography (CT) reduced lung cancer mortality after 7 years by 20% in comparison to screening
with chest radiography ). As a result of this trial and subsequent modeling studies, lung
cancer screening programs using low-dose CT are currently being implemented in the U.S. and other countries will likely follow soon. One of the major challenges arising from the implementation of these screening programs is the enormous amount of CT images that
must be analyzed by radiologists.
In the last two decades, researchers have been developing Computer-Aided-Detection (CAD) systems for
automatic detection of pulmonary nodules. CAD systems are intended to make the interpretation of CT images faster and more accurate, hereby improving the
cost-eﬀectiveness of the screening program. The typical setup of a CAD system consists of: 1) preprocessing, 2) nodule candidate detection, and 3) false positive
reduction. Preprocessing is typically used to standardize the data, restrict the search space for nodules to the
lungs, and reduce noise and image artifacts. The candidate detection stage aims to detect nodule candidates
at a very high sensitivity, which typically comes with
many false positives. Subsequently, the false positive
reduction stage reduces the number of false positives
among the candidates and generates the ﬁnal set of CAD
Although a large number of CAD systems have been
proposed ; Torres et al. ;
van Ginneken et al. ; Brown et al. ; Jacobs
et al. ; Choi and Choi ; Tan et al. ;
Teramoto and Fujita ; Cascio et al. ; Guo
and Li ; Camarlinghi et al. ; Tan et al.
 ; Riccardi et al. ; Messay et al. ;
Golosio et al. ; Murphy et al. ), there have
only been few studies providing an objective comparative evaluation framework using a common database.
The reported performances of published CAD systems
can vary substantially because diﬀerent data sets were
used for training and evaluation ;
Jacobs et al. ). Moreover, substantial variability among radiologists on what constitutes a nodule has
been reported ). Consequently, it
1These authors contributed equally to this work
is diﬃcult to directly and objectively compare diﬀerent
CAD systems. The evaluation of diﬀerent systems using
the same framework provides unique information that
can be leveraged to further improve the existing systems
and develop novel solutions.
ANODE09 was the ﬁrst comparative study aimed towards evaluating nodule detection algorithms ). This challenge has allowed groups
to evaluate their algorithms on a shared set of scans obtained from a lung cancer screening trial. However, this
study only included 50 scans from a single center, all acquired using one type of scanner and scan protocol. In
addition, the ANODE09 set contained a limited number
of larger nodules, which generally have a higher suspicion of malignancy. Evaluation on a larger and more
diverse image database is therefore needed.
In this paper, we introduce a novel evaluation framework for automatic detection of nodules in CT images.
A large data set, containing 888 CT scans with annotations from the publicly available LIDC-IDRI database
 ), is provided for both training and
testing. A web framework has been developed to eﬃciently evaluate algorithms and compare the result with
the other algorithms. The impact of combining multiple
candidate detection approaches and false positive reduction stages was also evaluated.
The key contributions of this paper are as follows.
(1) We describe and provide an objective web framework for evaluating nodule detection algorithms using
the largest publicly available data set; (2) We report the
performance of algorithms submitted to the framework
and investigate the impact of combining individual algorithms on the detection performance. We show that the
combination of classical candidate detectors and a combination of deep learning architectures processing these
candidates generates excellent results, better than any
individual system; (3) We update the LIDC-IDRI reference standard by identifying nodules that were missed
in the original LIDC-IDRI annotation process.
The data set was collected from the largest publicly available reference database for lung nodules: the
LIDC-IDRI ; Clark et al. ;
Armato III et al. ).
This database is available
from NCI’s Cancer Imaging Archive2 under a Creative Commons Attribution 3.0 Unsupported License.
2 
Public/LIDC-IDRI
The LIDC-IDRI database contains a total of 1018 CT
scans. CT images come with associated XML ﬁles with
annotations from four experienced radiologists.
database is very heterogeneous: It consists of clinical
dose and low-dose CT scans collected from seven diﬀerent participating academic institutions, and a wide range
of scanner models and acquisition parameters.
As recommended by Naidich et al. ; Manos
et al. and the American College of Radiology
 ), thin-slice CT scans should be
used for the management of pulmonary nodules. Therefore, we discarded scans with a slice thickness greater
than 3 mm. On top of that, scans with inconsistent slice
spacing or missing slices were also excluded. This led
to the ﬁnal list of 888 scans. These scans were provided
as MetaImage (.mhd) images that can be accessed and
downloaded from the LUNA16 website3. A more extensive data set description is provided in our previous
study ).
Each LIDC-IDRI scan was annotated by experienced
thoracic radiologists in a two-phase reading process. In
the initial blinded reading phase, four radiologists independently annotated scans and marked all suspicious
lesions as: nodule ≥3 mm; nodule < 3 mm; non-nodule
(any other pulmonary abnormality). For lesions annotated as nodule ≥3 mm, diameter measurements were
In a subsequent unblinded reading phase,
the anonymized blinded results of all other radiologists
were revealed to each radiologist, who then independently reviewed all marks. No consensus was forced.
In the 888 scans, a total of 36,378 annotations were
made by the radiologists. We only considered annotations categorized as nodules ≥3 mm as relevant lesions,
as nodules < 3 mm, and non-nodule lesions are not
considered relevant for lung cancer screening protocols
 ). Nodules could be annotated by
multiple radiologists; annotations from diﬀerent readers
that were located closer than the sum of their radii were
merged. In this case, position and diameters of these
merged annotations were averaged. This resulted in a
set of 2,290, 1,602, 1,186, and 777 nodules annotated
by at least 1, 2, 3, or 4 radiologists, respectively. We
considered the 1,186 nodules annotated by the majority
of the radiologists (at least 3 out of 4 radiologists) as
the positive examples in our reference standard. These
are the lesions that the algorithms should detect. Other
ﬁndings (1,104 nodules annotated by less than 3 out of 4
radiologists, 11,509 “nodule < 3 mm” annotations, and
19,004 “non-nodule” annotations) were considered “irrelevant ﬁndings” and marks on these locations were
3 
not counted as false positives nor as true positives in
the ﬁnal analysis; the same approach was used by ; Jacobs et al. ). Irrelevant ﬁndings were excluded in the evaluation because
they constitute pulmonary abnormalities that could be
important for diﬀerent clinical diagnosis ). As such, a CAD mark on such a lesion is not
a true false positive mark. It also alleviates the problem
of disagreement as to what constitutes a nodule ; van Ginneken et al. ).
3. LUNA16 challenge
The proposed evaluation framework was coined the
LUng Nodule Analysis 2016 (LUNA16) challenge.
LUNA16 invites participants to develop a CAD system that automatically detects pulmonary nodules in CT
scans. The challenge provides the data set and the reference annotations described in Section 2. This data set
can be used for training of the systems and the evaluation of the algorithms is performed on the same data set.
This makes LUNA16 a completely open challenge. To
prevent biased results as a result of training and testing
on the same data set, participants are instructed to perform cross-validation in the manner described in the following subsections. The LUNA16 website allows participants to submit the results. Submitted results are automatically evaluated and presented on the website.
3.1. Challenge tracks
The challenge consists of two separate tracks: (1)
complete nodule detection and (2) false positive reduction.
The complete nodule detection track requires the participants to develop a complete CAD system, meaning
that the only input into the system is a CT scan.
In the false positive reduction track, participants are
only required to classify a number of locations in each
scan as being a nodule or not. This is equivalent to
the so-called false positive reduction step in many published CAD systems.
For this track, a list of nodule candidates computed using existing nodule candidates detection algorithms is supplied to the participants (see Section 4.1). This can be seen as a typical
machine learning task, where a two class classiﬁcation
(nodule/not-nodule) has to be performed. We included
this track in the challenge to encourage the participation
of teams with experience in image classiﬁcation tasks
but no particular background on the analysis of medical
images. As further support, we included a tutorial on the
LUNA16 website on how to extract cubes and patches
around the nodule candidate locations in CT scans.
3.2. Cross-validation
Participants are required to perform 10-fold crossvalidation when they use the provided LIDC-IDRI data
both as training and as test data. The data set has been
randomly split into ten subsets of equal size on a patient
level. The subsets can be directly downloaded from the
LUNA16 website. The following steps describe how to
perform 10-fold cross-validation (for fold N):
1. Split the data set into a test set and a training set
(Subset N is used as test set and the remaining folds
are used as the training set).
2. For the ’false positive reduction’ track, test and
training candidates should be extracted on the corresponding test and training set.
3. Train the algorithm on the training set.
4. Test the trained algorithm on the test set and generate the result ﬁle.
5. After iterating this process over all folds, merge the
result ﬁles to get the result for all cases.
3.3. Evaluation
The results of the algorithms must be submitted online in the form of a comma separated value (csv) ﬁle.
The csv ﬁle contains all marks produced by the CAD
system. For each CAD mark, a position (image identi-
ﬁer, x, y, and z coordinates) and a score should be provided. The higher the score, the more likely the location
is a true nodule.
A CAD mark is considered a true positive if it is located within a distance r from the center of any nodule
included in the reference standard, where r is set to the
radius of the reference nodule. When a nodule is detected by multiple CAD marks, the CAD mark with the
highest score is selected. CAD marks that detect irrelevant ﬁndings are discarded from the analysis and are not
considered as either false positive or true positive. CAD
marks not falling into previous categories are marked as
false positives.
Results are evaluated using the Free-Response Receiver Operating Characteristic (FROC) analysis ). The sensitivity is deﬁned as the fraction of detected true positives (TPs) divided by the number of nodules in our reference standard. In the FROC
curve, sensitivity is plotted as a function of the average
number of false positives per scan (FPs/scan). For each
scan, we take a maximum of 100 CAD marks that were
given the highest scores. The 95% conﬁdence interval
of the FROC curve are computed using bootstrapping
with 1,000 bootstraps, as detailed in Efron and Tibshirani . In order to evaluate and compare diﬀerent
systems easily, we deﬁned one overall output score. The
overall score is deﬁned as the average of the sensitivity
at seven predeﬁned false positive rates: 1/8, 1/4, 1/2,
1, 2, 4, and 8 FPs per scan. The performance metric
was introduced in the ANODE09 challenge and is referred to as the Competition Performance Metric (CPM)
in Niemeijer et al. .
The evaluation script is publicly available on the
LUNA16 website and can thus be viewed and used by
all participants.
4. Methods
In this section we provide a brief description of the
algorithms applied in the LUNA16 challenge. As of 31
October 2016, seven systems have been applied to the
complete nodule detection track and ﬁve systems have
been applied to the false positive reduction track. The
candidate detection algorithms that were used to generate candidates for false positive reduction track are
presented in Section 4.1; the systems submitted to the
complete detection system track are described in Section 4.2; systems submitted to the false positive reduction track are detailed in Section 4.3.
4.1. Candidate detection
All candidate detection algorithms were developed as
part of published CAD systems ;
Jacobs et al. ; Setio et al. ; Tan et al.
 ; Torres et al. ), some of which are included
in the complete nodule detection track. As candidates
from multiple algorithms are likely to be complementary, we merged all candidates using the procedure described in Section 4.1.6. The list of merged candidates
can be downloaded from the LUNA16 website and can
be used by teams that want to participate in the false
positive reduction track.
4.1.1. ISICAD
The generic nodule candidate detection algorithm
was developed by Murphy et al. . First, the image
is downsampled from 512 × 512 to 256 × 256 with the
number of slices reduced to form isotropic resolution.
Thereafter, Shape Index (SI), and curvedness (CV) are
computed at every voxel in the lung volume as follows:
π arctan ). The
candidate detection algorithm by Jacobs et al. 
applies a double threshold density mask. The HU values commonly observed in subsolid nodules are used,
ranging between -750 HU and -300 HU. Since partial
volume eﬀects may occur at the boundaries of the lungs,
vessels, and airways, a morphological opening using
spherical structuring element (3 voxels diameter) is applied to remove these structures. Next, connected component analysis is performed. Components with a volume smaller than 34 mm3 are discarded from the list of
candidates as subsolid nodules with a diameter smaller
than 5 mm do not require follow-up CT. The centers of
the candidate regions are used as nodule candidate locations. The algorithm was developed using a data set
from a large European lung cancer screening trial.
4.1.3. LargeCAD
The algorithm has the function of detecting large nodules ). Large solid nodules (≥10
mm) have surface/shape index values or speciﬁc intensity range that is not captured by the two previously
described nodule detection algorithms.
An intensity
threshold of -300 HU (usually corresponding to solid
nodules) is applied in combination with multiple morphological operations. Thereafter, all connected voxels
are clustered using connected component analysis; clusters with an equivalent diameter outside the range 
mm are discarded. The algorithm was developed using
the data set used by LUNA16.
4.1.4. ETROCAD
The applied method uses the detector system proposed by Tan et al. . Isotropic re-sampling of
the image to a voxel dimension of 1 mm3 is applied in
the preprocessing step. The nodule candidate algorithm
consists of a nodule segmentation method based on nodule and vessel enhancement ﬁlters and a computed divergence feature to locate the centers of the nodule clusters. Three diﬀerent set of ﬁlters )
are applied to detect diﬀerent types of nodules: isolated,
juxtavascular, and juxtapleural nodules. To better estimate the location of the nodule centers and reduce the
FP rate, the maxima of the divergence of the normalized
gradient (DNG) of the image k = ∇(−→w) is used, where
||−→∆L|| and L is the image intensity. The enhancement ﬁlters and DNG are calculated at diﬀerent scales
in order to detect the seed points for diﬀerent sizes of
Thresholding on the ﬁltered image and DNG is applied to obtain the list of candidates. Diﬀerent thresholds on the ﬁltered image and the nodule-enhanced image are applied for isolated nodules, juxtavascular nodules, and juxtapleural nodules to get candidate locations. Finally, to ensure that a single nodule is represented by a single mark, cluster merging is performed.
The algorithm was developed using a set of scans from
LIDC-IDRI.
4.1.5. M5L
The candidate detection algorithm proposed by Torres et al. consists of two diﬀerent algorithms: LungCAM and Voxel-Based Neural Approach
LungCAM is inspired based on the life-cycle of ants
colonies ).
The lung internal
structures are segmented by iteratively deploying ant
colonies in voxels with intensity above a predeﬁned
thresholds. The ant colony moves to a speciﬁc destination and releases pheromones based on a set of rules
 ). Voxels visited by ant
colonies are removed and new ant colonies are deployed
in not-yet-visited voxels. Iterative thresholding of the
pheromone maps is applied to obtain a list of candidates. The probability Pi j that a candidate destination
is chosen is deﬁned as:
Pi j(vi →vj) =
n=1,26 W(σn)
where W(σj) depends on the amount of pheromone in
voxel vj. The algorithm ends when all the ants in the
colony have died.
VBNA uses two diﬀerent procedures to detect nodules inside the lung parenchyma ; Retico
et al. ) and nodules attached to the pleura ). The nodules inside the lung parenchyma
are detected using a dedicated dot-enhancement ﬁlter.
Since nodules can manifest with a diﬀerent size range, a
multi-scale approach is followed ). Nodule candidate locations are deﬁned as the local maxima
of the ﬁltered image. The pleural nodules are detected
by computing the surface normal at the lung wall. To
build the normal, a marching cube algorithm is used.
For each voxel inside the lung, the number of surface
normals passing through are accumulated. Pleural candidates are deﬁned as the local maxima of the accumulated scores. The algorithm was developed using a set
of scans from LIDC-IDRI, ANODE09, and ITALUNG-
4.1.6. Combining candidate detection algorithms
The combination of diﬀerent CAD systems has been
shown to improve the overall detection performance
for nodule detection in chest CT ; Niemeijer et al. ).
The previously described candidate detection algorithms used diﬀerent
approaches to detect nodules and are therefore likely to
detect diﬀerent sets of nodules. Consequently, the combination of multiple algorithms may improve the detection sensitivity of nodules and would therefore be a better baseline for the false positive reduction systems.
To combine the results of multiple candidate detection algorithms, we concatenated the lists of candidates,
where candidates located closer than 5 mm to each others were merged. The position of the merged candidates
were averaged. Candidates located outside the lung region were discarded, as they were irrelevant for nodule
detection. The lung region is determined based on the
lung segmentation algorithm proposed by van Rikxoort
et al. . As the algorithm may exclude nodules attached to the lung wall, a slack border of 10 mm was
4.2. Complete nodule detection system
The seven methods that were submitted to the complete nodule detection track are described in this section.
4.2.1. ZNET
ZNET uses ConvNets for both candidate detection
and false positive reduction. As a preprocessing step,
CT images are resampled to isotropic resolution of
0.5 mm. Candidate detection is extracted based on the
probability map given by U-Net ). U-net is applied on each axial slice. Before
applying U-Net, the resampled input slice is cropped
to 512 × 512. The candidates are extracted based on
the slice-based probability map output of the U-net. A
threholding is applied to obtain candidate masks. The
threshold was determined on the validation subset, maximizing the number of detected nodules. Thereafter, a
morphological erosion operation with a 4-neighborhood
kernel is used to remove partial volume eﬀects. The
candidates are then grouped by performing connected
component analysis. The center of mass of the components represent the coordinates of the candidates. The
false positive reduction is described in Section 4.3.4.
Both candidate detection and false positive reduction
stages were trained in a cross-validation using the provided folds from LUNA16.
4.2.2. Aidence
Aidence is a company developing computer assisted
diagnosis tools for radiologists based on deep learning
( The LUNAAidence algorithm uses end-to-end ConvNets trained on a subset of
studies from the National Lung Screening Trial (NLST)
with additional annotation provided by in-house radiologists. The LUNA16 data set was used for validation
purposes only and was not used as training data. A detailed description is not available because of commercial conﬁdentiality.
4.2.3. JianPeiCAD
JianPeiCAD is a system developed by Hangzhou
Jianpei Technology Co.
Ltd., a company based
on Hangzhou, China ( 
The algorithm follows the common two stage work-ﬂow
of nodule detection: Candidate detection and false positive reduction. A multi-scale rule-based screening is
applied to obtain nodule candidates.
The false positive reduction uses 3D ConvNets with wide channels,
which are trained using data augmentation to prevent
overﬁtting. The system was developed using in-house
resources (Chinese patient CT images and CT devices
from local-vendors) and the LUNA16 data set was used
as further validation for patients outside China. A detailed description is not available because of commercial conﬁdentiality.
4.2.4. MOT M5Lv1
The Multi Opening and Threshold CAD is a fully automatic CAD developed to be included into the M5L
system ). The lung volume is obtained using 3D region growing, with trachea exclusion
and lung separation procedures. The candidate detection algorithm was developed based on the method proposed by Messay et al. .
Multiple gray levelthresholding and morphological processing is used to
detect and segment nodule candidates. Several modi-
ﬁcations to the sequence of threshold and opening radius as well as the merging procedure are made. Subsequently, a dedicated nodule segmentation method ) is applied to separate nodules from
vascular structures during the segmentation step. The
false positive reduction computes 15 features, among
which geometrical (e.g. radius, sphericity, skewness of
distance from center) and intensity features (e.g. average, standard deviation, maximum, entropy). Classiﬁcation is performed using feed-forward neural networks
that consists of 1 input layer with 15 input units, 1 hidden layer with 31 units, and 1 output layer with 1 output
unit. The algorithm was developed using the LUNA16
4.2.5. VISIACTLung
This submission contains the results of the commercially available VisiaTM CT Lung CAD system, version
5.3 (MeVis Medical Solutions AG, Bremen, Germany).
This is an FDA approved CAD system designed to assist
radiologists in the detection of solid pulmonary nodules
during review of multidetector CT scans of the chest.
It is intended to be used as an adjunct, alerting the radiologist after his or her initial reading of the scan to
regions of interest (ROIs) that may have been initially
overlooked. A detailed description is not available because of commercial conﬁdentiality.
4.2.6. ETROCAD
ETROCAD is a CAD system adapted from Tan et al.
 . The candidate detection algorithm is described
in Section 4.1.4. The false positive reduction stage uses
a dedicated feature extraction and classiﬁcation algorithm. For each candidate, a set of features is computed,
including invariant features deﬁned on a 3D gauge coordinates system (?), shape features, and regional features.
The classiﬁcation is performed using a SVM classiﬁer
with a radial basis function. The algorithm was developed using a set of scans from LIDC-IDRI.
4.2.7. M5LCAD
M5LCAD is a CAD system developed by Torres et al.
 , which consists of two algorithms: LungCAM
and VBNA. This CAD system uses the candidate detector algorithms described in section 4.1.5. The false
positive reduction stage of LungCAM computes a set
of 13 features for nodule candidate analysis, including
spatial, intensity, and shape features. The set of features
is used to classify the candidates using a feed-forward
artiﬁcial neural network (FFNN). The FFNN architecture consists of 13 input neurons, 1 hidden layer with
25 neurons, and 1 neuron as output layer. The false positive reduction of VBNA performs the classiﬁcation using a standard three-layered FFNN using the raw voxels
as the feature vector ). The algorithm was developed using a set of scans from LIDC-
IDRI, ANODE09, and ITALUNG-CT.
4.3. False positive reduction systems
The ﬁve methods that were applied to the false positive reduction track are described in this section.
4.3.1. CUMedVis
CUMedVis uses multi-level contextual 3D ConvNets
developed by Dou et al. . To tackle challenges
coming from variations of nodule sizes, types, and geometry characteristics, a system that consists of three
diﬀerent 3D ConvNets architectures (Archi-a, Archi-b,
Archi-c) was presented. Each subsystem uses an input
image with diﬀerent receptive ﬁeld so that multiple levels of contextual information surrounding the suspicious
location could be incorporated.
Archi-a has a receptive ﬁeld of 20×20×6. Three convolutional layers are used with 64 kernels of 5 × 5 × 3,
5 × 5 × 3, 5 × 5 × 1, respectively. Thereafter, a fullyconnected layer with 150 output units and a softmax
layer are applied.
Archi-b has a receptive ﬁeld of
30 × 30 × 10. The ﬁrst convolutional layer with 64 kernels of 5 × 5 × 3 is used followed by a max-pooling
layer with kernel 2 × 2 × 1. Two convolutional layers
each with 64 kernels of 5 × 5 × 3 are then added, ﬁnalized by a fully-connected layer with 250 output units
and a softmax layer. Archi-c has the largest receptive
ﬁeld of 40 × 40 × 26. After the ﬁrst convolutional layer
with 64 kernels of 5 × 5 × 3, a max-pooling layer with
kernel 2 × 2 × 2 is used. Thereafter, two convolutional
layers each with 64 kernels of 5 × 5 × 3 are added. Finally, a fully-connected layer with 250 output units and
a softmax layer are established. The prediction probabilities from the three ConvNets architectures are fused
with weighted linear combination to produce the ﬁnal
prediction for a given candidate.
For pre-processing, voxel intensities are clipped into
the interval from -1000 to 400 HU and normalized into
the range of 0 to 1.
To deal with the class imbalance between the false positives and nodules, translation (one voxel along each axis) and rotation (900, 1800,
2700 within the transverse plane) augmentations are performed on the nodules. The weights are initialized using a Gaussian distribution and are optimized using the
standard back-propagation with momentum ). A dropout strategy )
was applied during training. The system was implemented using Theano ) and a GPU
of NVIDIA TITAN Z was used for acceleration. The algorithm was developed using the cerebral microbleeds
data set and was further optimized using LUNA16 data
4.3.2. JackFPR
The proposed method uses a similar multi-level contextual 3D ConvNet architecture as presented by Dou
et al. . It uses the three architectures (Archi-a,
Archi-b, Archi-c) described in Section 4.3.1 with several
modiﬁcations. Exponential activation units were used
as the activation functions. So instead of combining the
predictions of three ConvNets using linear combination,
the fully-connected layers from three architectures were
concatenated and connected to a fully-connected layer
with 128 output units. The last fully-connected layer
was then followed by a softmax layer to obtain the prediction.
The training was performed for 240 epochs with
1,024 iterations per epoch. Xavier initialization ) was used as the weight initialization and Nesterov accelerated Stochastic Gradient Descent (SGD) was used. Cross-entropy loss, L2 regularization loss, and center loss were used as the cost function. Center loss penalizes the diﬀerence between a running average of learned features for each class and sample class features seen during the particular batch ). The learning rate was set to 0.005 for the
ﬁrst 5 epochs as a warming up. Thereafter, the learning
rate was set to 0.01 and was reduced by 1/10 every 80
epochs. Data augmentation was performed and dropout
was applied to combat over-ﬁtting. The algorithm was
developed using LUNA16 data set.
4.3.3. DIAG CONVNET
This method uses multi-view ConvNets proposed by
Setio et al. . For each candidate, nine 65 × 65
patches of 50 × 50 mm from diﬀerent views are extracted. Each view corresponds to a diﬀerent plane of
symmetry in a cube and is processed using a stream of
2D ConvNets. The ConvNets stream consists of 3 consecutive convolutional layers and max-pooling layers:
The ﬁrst is formed by 24 kernels of 5 × 5; the second
by 32 kernels of 3 × 3; and the third by 48 kernels of
3 × 3. Weights are initialized randomly and updated
during training. The max-pooling layer is used to reduce the size of patches by half. The last layer is a
fully connected layer with 16 output units. Rectiﬁed
linear units (ReLU) are used as the activation functions.
The fusion of the diﬀerent ConvNets is performed using
the late fusion method ; Karpathy
et al. ). Fully-connected layers from all streams
are concatenated and are connected directly to a softmax layer. This approach allows the network to learn
3D characteristics by comparing outputs from multiple
ConvNets streams. In this approach, all the parameters
of the convolutional layers from diﬀerent streams are
Data augmentation was applied on candidates in the
training set to increase the variance of presentable candidates. For each candidate, random zooming [0.9, 1.1]
and random rotation [−20◦, +20◦] were performed. To
prevent over-ﬁtting during training, random positive and
negative candidates with equal distribution were sampled in a batch of 64 samples. Validation was performed
every 1,024 batches. Training was stopped when the
area under the curve of the receiver operating characteristic on the validation data set does not improve after 3 epochs.
Xavier initialization ) was used as the weight initialization. The
weights were optimized using RMSProp ), and evaluation was performed in a 10fold cross validation. Compared to the original work
 ), the submitted system uses an ensemble of three multi-view ConvNets trained using different random seed-points, averaging out biases from
training using random samples. The system was implemented using Theano ); a NVIDIA
TITAN X GPU was used for acceleration. Three diﬀerent architectures were evaluated by Setio et al. 
using the same LUNA16 data set and the best performing architecture was selected. The algorithm was further
optimized using the same LUNA16 data set.
4.3.4. ZNET
ZNET used the recently published wide residual networks ). For each
candidate, 64 × 64 patches from the axial, sagittal and
coronal views were extracted.
Each patch was processed separately by the wide residual networks. The
predicted output values of the network for these three
diﬀerent patches were averaged to obtain the ﬁnal prediction. The architecture used 4 sets of consecutive convolutional layers. The ﬁrst set consisted of 1 convolutional layer with 16 kernels of 3 × 3. Sets two to four
consisted of 10 convolutional layers with a stride of two,
each with 96 kernels of 3 × 3, 192 kernels of 3 × 3, and
384 kernels of 3 × 3, respectively. Each set also had a
1×1×N projection convolution in their skip connection,
where N is the number of kernels in the corresponding
set. The fourth layer was additionally connected to a
global average pooling layer, resulting in a 1 × 1 × 384
output image connected to the softmax layer.
Xavier initialization was used for weight initialization ) and ADAM was
used as the optimization method ).
Leaky Rectiﬁed Linear Units were used as nonlinearities throughout the network. Data augmentation (ﬂipping, rotation, zooming and translation) was applied not
only to the training data set but also the test data set in
order to improve the test set scores. The learning rate
was reduced over time: Learning rate is decreased by
90% after epochs 80 and 125. All convolutional networks were implemented using the Lasagne and Theano
libraries ; Bastien et al. ).
The training was performed on a computer cluster using
large range of CUDA enabled graphics cards including
the Tesla K40M, Titan X, GTX 980, GTX 970, GTX
760 and the GTX 950M. The algorithm was developed
using the LUNA16 data set.
4.3.5. CADIMI
This method used multi-slice ConvNets.
axial, sagittal, and coronal view, three patches are extracted at three locations: The plane in the exact candidate location and the planes 2 mm in both directions
of the remaining free axes (x, y, and z). The patches
were concatenated as three-dimensional arrays, which
resulted in patches of 52 × 52 × 3 mm centered around
the candidate location. The network consisted of 2D
ConvNets with three consecutive convolutional layers
and max-pooling: The ﬁrst convolutional layer used 24
kernels of 5 × 5; the second used 32 kernels of 3 × 3;
and the third 48 kernels of 3 × 3. The output of the last
max-pooling was connected to fully-connected layer of
512 output units. ReLU was used as the activation function, and the last fully-connected layer was connected
to a softmax layer.
Training was performed one time using patches from
all three views for 80 epochs. For each epoch, all positive patches and 20,000 random negative patches were
used. In order to tackle the problem of data imbalance,
data augmentation (vertical/horizontal ﬂip and random
cropping) was applied. During testing, 5 patches (1 center patch and 4 patches with [−4, +4] translation in two
axes) were extracted from each view. These patches
were processed using a single trained network and the
predictions were averaged.
Batch normalization was
applied after each max-pooling layer to reduce over-
ﬁtting. The weights were initialized using the uniform
initialization ). Nesterov accelerated
SGD with a learning rate of 0.01, a decay of 0.001, and
a momentum of 0.9 is used. The system was implemented using the Lasagne and Theano libraries ; Bastien et al. ). The algorithm
was developed using the LUNA16 data set.
4.4. Combining false positive reduction systems
The combination of multiple classiﬁcation methods,
also known as an ensemble method, has been used in
many machine-learning problems to improve the prediction performance ).
As systems
applied in the false positive reduction track use the same
set of candidates, the impact of combining multiple
methods could be evaluated. In this study, we combined
CAD results from the systems in the false positive reduction track. The combination is performed by simply
averaging the probabilities given by the systems. Such
is a common approach in optimizing the performance
of deep learning architectures ;
He et al. ).
4.5. Observer study
To evaluate the potential of CAD systems to detect
nodules missed by human readers, and to elucidate the
nature of the false positives detected by the CAD systems, an observer study was performed. In the observer
study, CAD marks from the combination of false positive reduction systems were assessed to identify if there
were additional nodules detected. The reading process
was performed by four expert readers independently.
We extracted all CAD marks at 0.25 FPs/scan that
were categorized as false positives to be further analyzed by expert readers. To reduce the readers’ workload, research scientists read and removed CAD marks
that were obvious false positives (e.g. vessels, ribs,
diaphragm) beforehand. Thereafter, CAD marks that
were close to annotated lesions in LIDC-IDRI but were
missed by our hit criteria (thus considered as false positives) were discarded. Most of these lesions were nonnodular and therefore were not well captured by the de-
ﬁned hit criteria (radius of the corresponding lesion).
This operation resulted in a set of 127 marks that were
potentially nodules. As a similar observer study was
performed in our previous study ),
marks which were already evaluated on this CT data
by radiologists were not read again and the scores of
the four radiologists from the previous study were used.
Last, we asked the expert readers to review and annotate the remaining marks as: nodule ≥3 mm, nodule < 3 mm, or false positives. Measurement tools were
made available to readers during the process in order to
enable size evaluation.
5. Results
In this section, we present the results achieved by all
individual systems described in Section 4. The results
of combining multiple algorithms are provided.
5.1. Candidate detection
Table 1 summarizes the performance of individual
candidate detection algorithms and their top performing
combinations. The best detection sensitivity of 92.9%
was achieved by ETROCAD. When multiple candidate detection algorithms were combined, the sensitivity substantially improved up to 98.3% (1,166/1,186
nodules), higher than the sensitivity of any individual
system. This illustrates the potential of combining multiple candidate detection algorithms to improve the sensitivity of CAD systems.
5.2. Complete nodule detection track
The FROC curves of the systems on the complete
nodule detection track are shown in Figure 1a. In this
track, the best score was achieved by ZNET with a CPM
of 0.811. Other systems show comparable performance.
It was observed that the relatively large diﬀerences in
terms of sensitivity at low FPs/scan substantially inﬂuences the overall scores of the systems.
5.3. False positive reduction track
The FROC curves of the systems on the false positive reduction track are shown in Figure 1b. The best
average score was achieved by CuMedVis, with a CPM
of 0.908. Table 2 shows all possible system combinations, where the sensitivities of the combined systems
were higher than the sensitivity achieved by the best system. Although all false positive reduction systems are
based on ConvNets, it is evident that combining ConvNets with diﬀerent conﬁgurations further improves the
overall sensitivity as shown in Table 2. The p-value was
deﬁned as the probability that a system’s CPM is higher
or lower than the reference system’s CPM. A p-value
below 0.002 is considered to be statistically signiﬁcant
after Bonferroni correction (m = 30).
5.4. Performance based on nodule type
To assess the performance of the algorithms on diﬀerent types of nodules (non-solid, part-solid, and solid),
additional analysis was performed.
The nodule type
was derived based on the morphological characteristic
scored by the LIDC-IDRI radiologists. The nodule was
labeled ”non-solid” if the majority of the radiologists
gave a texture score of 1, ”solid” for a majority score of
5, and ”part-solid” if the two previous criterion did not
hold. Using this labelling strategy, the LUNA16 data set
consisted of 64 non-solid nodules, 189 part-solid nodules, and 933 solid nodules. The performance of the
algorithms on diﬀerent set of nodules are tabulated in
5.5. Analysis of false positives: observer study
A summary of the observer study is shown in Table 4.
Among 127 CAD marks, 108, 91, 69, and 41 CAD
marks were accepted as nodules ≥3 mm by at least 1, 2,
3, or 4 readers, respectively; 6 out of 19 remaining CAD
marks were considered as nodule < 3 mm. Examples of
nodules found in this observer study are shown in Figure 2c. We shared the set of additional nodules on the
LUNA16 website to be used for further development of
CAD systems.
6. Discussion
In this study, we presented LUNA16: A novel evaluation framework for automatic nodule detection algorithms. The aim of the study was to supply the research
community a framework to test and compare algorithms
on a common large database with a standardized evaluation protocol. This allows the community to objectively
evaluate diﬀerent CAD systems and push forward the
development of state of the art nodule detection algorithms. The submitted systems were described and the
performance was evaluated. We showed that the combination of multiple false positive reduction algorithms
applied on a combined set of candidates outperformed
any individual system. This highlights the potential of
combining algorithms to improve the detection performance.
Candidate detection plays an important role of determining the maximum attainable detection sensitivity of
a CAD system. The algorithms should ideally detect all
nodules with an acceptable amount of false positives.
Table 1 shows that the individual candidate detection algorithms achieve a detection sensitivity between 31.8%
and 92.9%; combining diﬀerent candidate detection algorithms improved the sensitivity up to 98.3%. While
a smaller set of candidates (a combined set of only ISI-
CAD, SubsolidCAD, and LargeCAD candidates) were
also provided in the earlier phase of LUNA16, we here
only reported the results of the systems that use the latest set of candidates that has a much higher sensitivity.
The results of other systems that use the smaller set of
candidates, resulting in lower scores, are available on
the LUNA16 website. It is worth noting that the candidate detection systems used in this study do not employ
System name
Combination
Sensitivity
Best single
sensitivity
sensitivity
Total number
of candidates
Average number of
candidates / scan
SubsolidCAD
Table 1: The results of ﬁve candidate detection systems and all possible combinations. The ﬁlled squares indicate which systems were included in
the combination. CPM: Competition Performance Metric.
System name
Combination
Best single
DIAG CONVNET
Table 2: The results of ﬁve false positive reduction systems and all possible combinations. The ﬁlled squares indicate which systems were included
in the combination. CPM: Competition Performance Metric.
Average number of false positives per scan
Sensitivity
FROC performance
ZNET, CPM = 0.811
Aidence, CPM = 0.807
JianPeiCAD, CPM = 0.776
MOT_M5Lv1, CPM = 0.742
VisiaCTLung, CPM = 0.715
ETROCAD, CPM = 0.676
M5LCAD, CPM = 0.608
Average number of false positives per scan
Sensitivity
FROC performance
combined systems, CPM = 0.952
CUMedVis, CPM = 0.908
JackFPR, CPM = 0.872
DIAG_CONVNET, CPM = 0.854
CADIMI, CPM = 0.783
ZNET, CPM = 0.758
Figure 1: FROC curves of the systems in (a) the nodule detection track and (b) the false positive reduction track. Dashed curves represent the 95%
conﬁdence intervals estimated using bootstrapping.
Table 3: Performance benchmark of algorithms on diﬀerent sets of nodules. Nodules are categorized based on the nodule type, which was derived
based the morphological characteristic scored by the LIDC-IDRI radiologists (5-point scale: 1=non-solid, 3=part-solid, 5=solid). The nodule was
labeled ”non-solid” if the majority of the radiologists gave a texture score of 1, ”solid” for a majority score of 5, and ”part-solid” if the two previous
criterion did not hold. CPM score was used as the performance metric
System name
all (1,186)
non-solid (64)
part-solid (189)
solid (933)
Nodule detection track
JianPeiCAD
VisiaCTLung
False positive reduction track
DIAG CONVNET
Table 4: An overview of the observer study on 222 false positives at
0.25 FPs/scan. The table shows the number of false positives that were
accepted by the expert readers as nodule ≥3 mm at diﬀerent agreement levels. The number of false positives that were not accepted as
nodule ≥3 mm but were accepted as nodule<3 mm is also included.
nodule ≥3 mm - at least 1
nodule ≥3 mm - at least 2
nodule ≥3 mm - at least 3
nodule ≥3 mm - at least 4
nodule <3 mm
deep learning, while systems in the complete nodule detection track, e.g. ZNET, do employ ConvNets to detect
nodule candidates.
In the complete nodule detection track, a total of
seven systems were evaluated. Diverse methods were
applied and diﬀerent sets of data were used for training. When evaluated using the same data set, the detection sensitivity ranged between 69.1% and 91.5% at
1 and 8 FPs/scan, as shown in Figure 1a. Notably, the
top three systems make use of ConvNets for their detection algorithms. While the variability of the performance is determined by the underlying methods, it is
also aﬀected by the training data used to develop the
system (see also Table 5). This suggests the need of a
standardized training data set for appropriate comparison of algorithms.
In the false positive reduction track, diﬀerent systems
for false positive reduction were evaluated given a common set of candidates and training data. A total of ﬁve
systems were evaluated. ConvNets were used as the prediction model for all systems, which is in line with the
recent trend of adopting deep learning in the medical
image analysis domain. As shown in Figure 1b, all sys-
(a) true positives with highest probability
(b) random true positives at 1 FP/scan
(c) false positives accepted as nodules by radiologists
(d) random false positives at 1 FP/scan
(e) false negatives from the candidate detectors
(f) random false negatives at 1 FP/scan
Figure 2: Examples of true positives, false positives, and false negatives from the combined system. Each lesion is located at the center of the
50 × 50 mm patch in axial, coronal, and sagittal views.
tems achieve detection sensitivity between 79.3% and
98.3% at 1 and 8 FPs/scan. As the underlying method is
similar, one could hypothesize that there could be little
to no beneﬁt when these systems are combined. Nevertheless, combining multiple ConvNets systems did substantially improve the detection performance (black line
on Figure 1b); a detection sensitivity of over 95.0% was
achieved at fewer than 1 FP/scan. Despite all methods
being based on ConvNets, the diﬀerences in network
parameters, such as selected architectures, random initialization methods, and input patches, apparently make
these systems somewhat complementary for prediction,
and this is leveraged by the (simple averaging) combination.
To provide a broader context to the results reported in
this paper, we listed the performance of other published
CAD systems that use LIDC-IDRI data in Table 5. For
each CAD system, we listed the number of scans used
in the validation data set, nodule inclusion criteria, the
number of included nodules, and the reported CAD performance.
Note that diﬀerent subsets of LIDC-IDRI
database were used; LUNA16 aims to make the CAD
performance comparison more easy and more fair by
using exactly the same data and evaluation protocol for
each system. The CAD systems presented in Jacobs
et al. are not listed in this table as these CAD
systems also participated in the LUNA16 challenge and
hence are already described in this paper.
The observer study showed that some false positives
detected by the CAD systems are nodules that were
missed during the manual annotations of LIDC-IDRI.
The majority of these nodules were overlooked because
they were small or less visible (e.g. ground-glass/nonsolid nodules). Other nodules may be missed because
there were multiple nodules in the corresponding scans,
or because the nodules were part of a complex abnormality (e.g. an area of consolidation). While these nodules may be found during follow-up, detecting them
early could provide essential clinical information (e.g.
growth rate).
Examples of lesions detected or missed by the combined CAD system are shown in Figure 2. Nodules with
a wide range of morphological characteristics are detected at 1 FP/scan, showing that ConvNets are capable of capturing morphological variation of nodules in
the network. Larger nodules are unlikely to be missed,
which is just as well as there is a strong positive correlation between size and malignancy risk. Most false positives are large vessels, scar tissue, spinal abnormalities,
and other mediastinal structures. These false positives
are a challenge. In scans from subjects with interstitial lung disease, there are, even in mild cases, regions
with irregular opacities that can lead to a large number
of erroneous nodule CAD marks. Other false positives
are caused by motion artifacts and extreme noise. The
false negatives were small and/or had irregular shapes.
Improving the robustness of the candidate detection algorithms to detect small nodules should further improve
the performance.
This study has several limitations. As the LIDC-IDRI
is a web-accessible database for development and evaluation of CAD systems, all nodule annotations are publicly available. The usage of a completely open database
is not a common setup for challenges. Typically, an independent test set is provided, for which the reference
annotations are not made public. As a consequence, in
LUNA16 teams could tune the parameters of their algorithm to show good performance on this particular
data set, although the fact that LIDC-IDRI is a large set
of scans from many diﬀerent sources somewhat mitigates this risk. In order to allow performance comparison among diﬀerent systems, we instructed participants
that did not have their own training data to train their
system in a particular cross-validation approach. Although this prevents some of the positive bias, positive
bias may still remain if the design and architecture of a
system are selected or optimized based on the full challenge data set. Previously published algorithms have
also used LIDC-IDRI data to optimize the design of
their systems. This may have inﬂuenced the design of
algorithms used in LUNA16 as well. Moreover, a crossvalidation approach introduces some risks as people
may make a mistake that goes unnoticed while carrying out a cross-validation experiment. In fact, one team
that originally participated in the challenge and reported
excellent results had to withdraw because of a bug in the
re-initialization of the network weights when starting
training for the next fold in cross-validation. Unforeseen errors aside, allowing a cross-validation training
procedure means that the presented systems are evaluated on test data while having been trained with data
from the same sources (institutions, scanners, protocols). This may incur a positive bias in the reported
results. This potential for bias is however also present
in most, if not all, studies on the topic that have been
previously published. Ruling out any possible bias is
still an open problem for many machine learning competitions, even when the test data is not publicly available (?). On this note, it is important to further validate
the performance and the generalizability of the systems
using a completely independent validation data set.
Possible approaches for system improvement are as
follows. Most of the non-nodular abnormalities, especially in lungs with many irregularities, were still detected. Although they may be clinically relevant, the
algorithm should be able to diﬀerentiate these abnormalities. Classifying abnormalities into a subset of taxonomy, similar to ?, may be more useful for clinical usage. A more direct application would be to assign malignancy scores for all nodules detected by the system.
This would further optimize the lung cancer screening
workﬂow, for which less suspicious nodules would not
have to undergo extensive screening protocol.
A future challenge could incorporate an even larger
data set split into a training data set with annotations
and a dedicated test data set for evaluation in which the
reference standard is kept hidden. This still introduces a
risk that teams can visually inspect the test data and the
output of their system, notice false positives and false
negatives and use that information to improve their performance. This could be circumvented by letting teams
upload their algorithms, e.g. as machine executables or
software containers, and evaluate these on test data that
is not released publicly.
7. Conclusions
We have presented a web-based framework for a
fair and automated evaluation of nodule detection algorithms, using the largest publicly available data set
of chest CT scans in which nodules were annotated by
multiple exert human readers. We have shown that combining classical candidate detection algorithms and analyzing these candidates with convolutional networks
Table 5: The performance summary of published CAD systems evaluated using LIDC-IDRI data sets. Note that diﬀerent subsets of scans were
used by diﬀerent research groups.
CAD systems
sensitivity (%) / FPs/scan
Combined LUNA16
at least 3
98.2 / 4.0
96.9 / 1.0
Dou et al. 
at least 3
90.7 / 4.0
84.8 / 1.0
Setio et al. 
at least 3
90.1 / 4.0
85.4 / 1.0
Bergtholdt et al. 
at least 1
85.9 / 2.5
Torres et al. 
at least 2
80.0 / 8.0
van Ginneken et al. 
at least 3
76.0 / 4.0
73.0 / 1.0
Brown et al. 
at least 3
75.0 / 2.0
Choi and Choi 
at least 1
95.3 / 2.3
Tan et al. 
at least 4
83.0 / 4.0
Teramoto and Fujita 
at least 1
80.0 / 4.2
Cascio et al. 
at least 1
97.0 / 6.1
88.0 / 2.5
Guo and Li 
at least 3
80.0 / 7.4
75.0 / 2.8
Camarlinghi et al. 
at least 2
80.0 / 3.0
Riccardi et al. 
at least 4
71.0 / 6.5
60.0 / 2.5
Tan et al. 
at least 4
87.5 / 4.0
Messay et al. 
at least 1
82.7 / 3.0
yields excellent results.
Additionally, we have provided an update to the LIDC-IDRI reference standard
which includes additional nodules found by CAD. The
LUNA16 challenge will remain open for new submissions and can therefore be used as a benchmarking
framework for future CT nodule CAD development.
Acknowledgements
The authors acknowledge the National Cancer Institute and the Foundation for the National Institutes
of Health and their critical role in the creation of the
free publicly available LIDC-IDRI Database used in
this study.
This work was supported by a research
grant from the Netherlands Organization for Scientiﬁc
Research [project number 639.023.207].
A. S´o˜nora
was supported by the Belgian Development Cooperation through the VLIR-UOS program with Universidad
de Oriente in Santiago de Cuba. E. Papavasileiou was
supported by a PhD grant of the Research Foundation
Flanders (FWO).