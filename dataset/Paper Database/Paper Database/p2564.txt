Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 339–348,
Lisbon, Portugal, 17-21 September 2015. c⃝2015 Association for Computational Linguistics.
A Neural Network Model for Low-Resource
Universal Dependency Parsing
Long Duong,12 Trevor Cohn,1 Steven Bird,1 and Paul Cook3
1Department of Computing and Information Systems, University of Melbourne
2National ICT Australia, Victoria Research Laboratory
3Faculty of Computer Science, University of New Brunswick
 {t.cohn,sbird}@unimelb.edu.au 
Accurate dependency parsing requires
large treebanks, which are only available
for a few languages. We propose a method
that takes advantage of shared structure
across languages to build a mature parser
using less training data.
We propose
a model for learning a shared “universal” parser that operates over an interlingual continuous representation of language, along with language-speciﬁc mapping components. Compared with supervised learning, our methods give a consistent 8-10% improvement across several
treebanks in low-resource simulations.
Introduction
Dependency parsing is an important task for Natural Language Processing (NLP) with application
to text classiﬁcation ,
relation extraction ,
question answering , statistical
machine translation , and sentiment analysis . A mature
parser normally requires a large treebank for training, yet such resources are rarely available and
are costly to build. Ideally, we would be able to
construct a high quality parser with less training
data, thereby enabling accurate parsing for lowresource languages.
In this paper we formalize the dependency parsing task for a low-resource language as a domain
adaptation task, in which a target resource-poor
language treebank is treated as in-domain, while
a much larger treebank in a high-resource language forms the out-of-domain data. In this way,
we can apply well-understood domain adaptation
techniques to the dependency parsing task. However, a crucial requirement for domain adaptation
is that the in-domain and out-of-domain data have
compatible representations. In applying our approach to data from several languages, we must
learn such a cross-lingual representation.
we frame this representation learning as part of a
neural network training. The underlying hypothesis for the joint learning is that there are some
shared-structures across languages that we can exploit. This hypothesis is motivated by the excellent
results of the cross-lingual application of unlexicalised parsing , whereby
a delexicalized parser constructed on one language
is applied directly to another language.
Our approach works by jointly training a neural network dependency parser to model the syntax in both a source and target language. Many of
the parameters of the source and target language
parsers are shared, except for a small handful of
language-speciﬁc parameters. In this way, the information can ﬂow back and forth between languages, allowing for the learning of a compatible
cross-lingual syntactic representation, while also
allowing the parsers to mutually correct one another’s errors. We include some language-speciﬁc
components, in order to better model the lexicon
of each language and allow learning of the syntactic idiosyncrasies of each language. Our experiments show that this outperforms a purely supervised setting, on both small and large data conditions, with a gain as high as 10% for small training sets. Our proposed joint training method also
out-performs the conventional cascade approach
where the parameters between source and target
languages are related together through a regularization term .
Our model is ﬂexible, allowing easy incorporation of peripheral information. For example, assuming the presence of a small bilingual dictionary is beﬁtting of a low-resource setting, as this
is prototypically one of the ﬁrst artifacts generated by ﬁeld linguists.
We incorporate a bilingual dictionary as a set of soft constraints on the
model, such that it learns similar representations
for each word and its translation(s). For example,
the representation of house in English should be
close to haus in German. We empirically show
that adding a bilingual dictionary improves parser
performance, particularly when target data is limited.
The ﬁnal contribution of the paper concerns
the learned word embeddings.
We demonstrate
that these encode meaningful syntactic phenomena, both in terms of the observable clusters and
through a verb classiﬁcation task. The code for
this paper is published as an open source project.1
Related Work
This work is motivated by the idea of delexicalized parsing, in which a parser is built without
any lexical features and trained on a treebank for
a resource-rich source language . It is then applied directly to parse sentences
in the target resource-poor languages. Delexicalized parsing relies on the fact that identical part-ofspeech (POS) inventories are highly informative of
dependency relations, and that there exists shared
dependency structures across languages.
Building a dependency parser for a resourcepoor language usually starts with the delexicalized parser and then uses other resources to reﬁne
the model. McDonald et al. and Ma and
Xia exploited parallel data as the bridge
to transfer constraints from the source resourcerich language to the target resource-poor languages. T¨ackstr¨om et al. also used parallel data to induce cross-lingual word clusters
which added as features for their delexicalized
parser. Durrett et al. constructed the set of
language-independent features and used a bilingual dictionary as the bridge to transfer these features from source to target language. T¨ackstr¨om
et al. additionally used high-level linguistic features extracted from the World Atlas of Language Structures (WALS) . Accordingly, we adopt a different resource require-
1 
universal_dependency_parser
ment: a small treebank in the target low-resource
Domain adaptation or joint-training is a different branch of research, and falls outside the scope
of this paper. Nevertheless, we would like to contrast our work with Senna ,
a neural network framework to perform a variety of NLP tasks such as part-of-speech (POS)
tagging, named entity recognition (NER), chunking, and so forth. Both approaches exploit common linguistic properties of the data through joint
learning. However, Collobert et al’s goal is to ﬁnd
a single input representation that can work well
for many tasks. Our goal is different: we allow
the joint-training inputs to be different but constrain the parameter weights in the upper layer
to be identical.
Consequently, our method applies to the task where inputs are different, possibly from different languages or domains. Their
method applies for different tasks in the same language/domain where the inputs are fairly similar.
Supervised Neural Network Parser
This section describes the monolingual neural network dependency parser structure of Chen and
Manning . This parser achieves excellent
performance, and has a highly ﬂexible formulation allowing auxilliary inputs. The model is based
on a transition-based dependency parser formulated as a neural-network classiﬁer to
decide which transition to apply to each parsing
state conﬁguration.2 That is, for each conﬁguration, the selected list of words, POS tags and labels from the Stack, Queue and Arcs are extracted.
Each word, POS and label is mapped into a lowdimension vector representation using an embedding matrix, which is then fed into a two-layer
neural network classiﬁer to predict the next parsing action. The set of parameters for the model is
E = {Eword, Epos, Earc} for the embedding layer,
W1 for the fully connected cubic hidden layer and
W2 for the softmax output layer. The model prediction function is
P(Y |X = ⃗x, W1, W2, E) =
W2 × cube(W1 × Φ [⃗x, E])
2Our approach is focused on a technique for transfer
learning which can be more widely applied to other types
of dependency parser (and models, generally) regardless of
whether they are transition-based or graph-based.
where cube is a non-linear activation function, Φ is
the embedding function that returns a vector representation of parsing state x using an embedding
matrix E. We refer the reader to Chen and Manning for a more detailed description.
A Joint Interlingual Model
We assume a small treebank in a target resourcepoor language, as well as a larger treebank in the
source language. Our objective is to learn a model
of both languages, subject to the constraint that
both models are similar overall, while allowing for
some limited language variability. Instead of just
training two different parsers on source and then
on target, we train them jointly, in order to learn
an interlingual parser. This allows the method to
take maximum advantage of the limited treebank
data available, resulting in highly accurate predicted parses.
monolingual
optimizing
cross-entropy
objective,
i=1 log P(Y = ⃗y(i)|X = ⃗x(i)),
where P(Y |X) is given by equation 1 and
D = {⃗x(i), ⃗y(i)}n
i=1 is the training data.
training of a parser over the source and target
languages can be achieved by simply adding two
such cross-entropy objectives, i.e.,
Ljoint = −
log P(Ys = ⃗y(i)
s |Xs = ⃗x(i)
log P(Yt = ⃗y(i)
t |Xt = ⃗x(i)
where the training data, D = Ds ∪Dt, comprises
data in both the source and target language. However training the model according to equation 2
will result in two independent parsers. To enforce
similarity between the two parsers, we adopt parameter sharing: the neural network parameters,
W1 and W2, are identical in both parsers. Thereby
P(Yα|Xα = ⃗x) = P(Y |X = ⃗x, W1, W2, Eα) ,
where the subscript α ∈{s, t} denotes the source
or target language.
We allow the embedding
matrix Eα to differ in order to accommodate
language-speciﬁc features, in terms of the representations of lexical types, Eword
, part-of-speech,
and dependency arc labels Earc
s . This reﬂects
the fact that different languages have different lexicon, parts-of-speech often exhibit different roles,
and dependency edges serve different functions,
e.g. in Korean a static verb can serve as an adjective . During training, the languagespeciﬁc errors are back propagated through different branches according to the language, guiding learning towards an interlingual representation that informs parsing decisions in both languages. The set of parameters for the model is
W1, W2, Es, Et where Es, Et are the embedding
matrices for the source and target languages.
Generally speaking, we can understand the
model as building the universal dependency parser
that parses the universal language. Speciﬁcally,
the model is the combination of two parts: the
universal part (W1, W2) that is shared between the
languages, and the conversion part (Es, Et) that
maps a language-speciﬁc representation into the
universal language. Naturally, we could stack several non-linear layers in the conversion components such that the model can better transform the
input into the universal representation; we leave
this exploration for future work. Currently, our
cross-lingual word embeddings are meaningful for
a pair of source and target languages. However,
our model can easily be used for joint training over
k > 2 languages. We also leave this avenue of enquiry for future work
One concern from equation 2 is that when the
source language treebank Ds is much bigger than
the target language treebank Dt, it is likely to
dominate, and consequently, learning will mainly
focus on optimizing the source language parser.
We adjust for this disparity by balancing the two
datasets, Ds and Dt, during training. When selecting mini-batches for online gradient updates, we
select an equal number of classiﬁcation instances
from the source and target languages. Thus, for
each step |Ds| = |Dt|, effectively reweighting the
cross-entropy components in (2) to ensure parity
between the languages.
The other concern is over-ﬁtting, especially
when we only have a small treebank in the target language. As suggested by Chen and Manning , we apply drop-out, a form of regularization for both source and target language.
That is, we randomly drop some of the activation units from both hidden layer and input layer.
Following Srivastava et al. , we randomly
dropout 20% of the input layer and 50% of the hid-
den layer. Empirically, we observe a substantial
improvement applying dropout to the model over
MLE or l2 regularization.
Incorporating a Dictionary
Our model is ﬂexible, enabling us to freely add
additional components. In this section, we assume
the presence of a bilingual dictionary between the
source and target language. We seek to incorporate this dictionary as a part of model learning, to
encode the intuition that if two lexical items are
translations of one another, the parser should treat
them similarly.3
Recall that the mapping layer
is the combination of word, pos and arc embeddings, i.e., Eα = {Eword
easily add bilingual dictionary constraints to the
model in the form of regularization to minimize
the l2 distance between word representations, i.e.,
⟨i,j⟩∈D ∥Eword(i)
F , where D comprises translation pairs, word(i) and word(j).
When the languages share the same POS tagset
and arc set,4 we can also add further constraints
such as their language-speciﬁc embeddings be
close together. This results a regularised training
objective,
Ldict = Ljoint−λ
where λ ∈[0, ∞] controls to what degree we
bind these words or pos tags or arc labels together, with high λ tying the parameters and small
λ allowing independent learning. We expect the
best value of λ to fall somewhere between these
Finally, we use a mini-batch size of
1000 instance pairs and adaptive learning rate
trainer, adagrad to build our
two separate models corresponding to equations 2
Experiments
In this section, we compare our joint training approach with baseline methods of supervised learning in the target language, and cascaded learning
of source and target parsers.
3However, this is not always the case. For example, modal
or auxiliary verbs in English often have no translations in
different languages or map to words with different syntactic
functions.
4As was the case for our experiments.
We experiment with the Universal Dependency
Treebank (UDT) V1.0 , simulating low resource settings.5 This treebank has
many desirable properties for our model: the dependency types (arc labels set) and coarse POS
tagset are the same across languages.
This removes the need for mapping the source and target
language tagsets to a common tagset. Moreover,
the dependency types are also common across
languages allowing evaluation of the labelled attachment score (LAS). The treebank covers 10
languages,6 with some languages very highly
resourced—Czech, French and Spanish have 400k
tokens—and only modest amounts of data for
other languages—Hungarian and Irish have only
around 25k tokens. Cross-lingual models assume
English as the source language, for which we have
a large treebank, and only a small treebank of 3k
tokens exists in each target language, simulated by
subsampling the corpus.
Baseline Cascade Model
We compare our approach to a baseline interlingual model based on the same parsing algorithm as presented in section 2.1, but with cascaded training . This works
by ﬁrst learning the source language parser, and
then training the target language parser using a
regularization term to minimise the distance between the parameters of the target parser and the
source parser (which is ﬁxed). In this way, some
structural information from the source parser can
be used in the target parser, however it is likely
that the representation will be overly biased towards the source language and consequently may
not prove as useful for modelling the target.
Monolingual Word Embeddings
While the Epos and Earc are randomly initialized,
we initialize both the source and target language
word embeddings Eword
of our neural network models with pre-trained embeddings. This is
an advantage since we can incorporate the monolingual data which is often available, even for
5Evaluating on truly resource-poor languages would be
preferable to simulation. However for ease of training and
evaluation, which requires a small treebank in the target language, we simulate the low-resource setting using a small part
of the UDT.
6Czech (cs), English (en), Finnish (ﬁ), French (fr), German (de), Hungarian (hu), Irish (ga), Italian (it), Spanish (es),
Swedish (sv).
Figure 1: Sensitivity of regularization parameter λ
against the LAS measured on the Swedish development set trained on 1000 (tokens).
resource-poor languages. We collect monolingual
data for each language from the Machine Translation Workshop (WMT) data,7 Europarl and EU Bookshop Corpus . The size of monolingual data also varies
signiﬁcantly, with as much as 400 million tokens
for English and German, and as few as 4 million tokens for Irish. We use the skip-gram model
 to induce 50-dimensional
word embeddings.
Bilingual Dictionary
For the extended model as described in section 3.1,
we also need a bilingual dictionary. We extract
dictionaries from PanLex 
which currently covers around 1300 language varieties and about 12 million expressions.
dataset is growing and aims at covering all languages in the world and up to 350 million expressions. The translations in PanLex come from various sources such as glossaries, dictionaries, automatic inference from other languages, etc. Naturally, the bilingual dictionary size varies greatly
among resource-poor and resource-rich languages.
Regularization Parameter Tuning
Joint training with a dictionary (see equation 3)
includes a regularization sensitivity parameter λ.
This parameter controls to what extent we should
bind the source words and their target translation,
common POS tags and arcs together. In this section we measure the sensitivity of our approach
with respect to this parameter. In a real world sce-
7 
nario, getting development data to tune this parameter is difﬁcult. Thus, we want a parameter that
can work well cross-lingually. To simulate this,
we only tune the parameter on one language and
apply it directly to different languages. We trained
on a small Swedish treebank with 1k tokens, testing several different values of λ. We evaluated on
the Swedish development dataset. Figure 1 shows
the labelled attachment score (LAS) for different
λ. It’s clearly visible that λ = 0.0001 gives the
maximum LAS on the development set. Thus, we
use this value for all the experiments involving a
dictionary hereafter.
For our initial experiments we assume that we
have only a small target treebank with 3000 tokens (around 200 sentences).
Ideally the much
larger source language (English) treebank should
be able to improve parser performance versus simple supervised learning on such a small collection.
We apply the joint model (equation 2) and joint
model with the dictionary constraints (equation 3)
for each target language,
The results are reported in Table 1.
The supervised neural network dependency parser performed worst, as expected, and the baseline cascade model consistently outperformed the supervised model on all languages by an average margin of 5.6% (absolute).8
The joint model also
consistently out-performed both baselines giving
a further 1.9% average improvement over the cascade. This was despite the fact that the cascaded
model had the beneﬁt of tuning for the regularization parameters on a development corpus, while
the joint model had no parameter tuning. Note that
the improvement varies substantially across languages, and is largest for Czech but is only minor
for Swedish. The joint model with the bilingual
dictionary outperforms the joint model, however,
the improvement is modest (0.7%). Nevertheless,
this model gives substantial improvements compared with the cascaded and the supervised model
(2.6% and 8.2%).
Learning Curve
In section 4.6, we used a 3k token treebank in the
target language.
What if we have more or less
8We use absolute percentage comparisons herein.
Supervised
Baseline Cascaded
Joint + Dict
Table 1: Labelled attachment score (LAS) for each model type trained on 3000 tokens for each target
language (columns). All bar the supervised model also use a large English treebank.
Data Size (tokens)
Joint + Dict Model
Joint Model
Cascade Model
Supervised Model
Figure 2: Learning curve for Joint model, Joint
+ Dict model, Baseline cascaded and Supervised
model: the x-axis is the size of data (number of
tokens); the y-axis is the average LAS measured
on 9 languages (except English).
target language data? Figure 2 shows the learning curve with respect to various models on different data sizes averaged over all target languages.
For small datasets of 1k training tokens, the cascaded model, joint model and joint + dict model
performed similarly well, out-performing the supervised model by about 10% (absolute). With
more training data, we see interesting changes
to the relative performance of the different models. While the baseline cascade model still outperforms the supervised model, the improvement
is diminishing, and by 15k, the difference is only
2.9%. On the other hand, compared with the supervised model, the joint and joint + dict models
perform consistently well at all sizes, maintaining
an 8% lead at 15k. This shows the superiority of
joint training compared with single language training.
To understand this pattern of performance differences for the cascade versus the joint model,
one needs to consider the cascade model formulation. In this approach, the target language parameters are tied (softly) with the source language
parameters through regularization. This is a bene-
ﬁt for small datasets, providing a smoothing function to limit overtraining.
However, when we
have more training data, these constraints limit
the capacity of the model to describe the target
This is compounded by the problem that
the source representation may not be appropriate
for modelling the target language, and there is no
way to correct for this. In contrast the joint model
learns a mutually compatible representation automatically during joint training.
The performance results for the joint model
with and without the dictionary are similar overall. Only on small datasets (1k, 3k), is the difference notable.
From 5k tokens, the bilingual
dictionary doesn’t confer additional information,
presumably as there is sufﬁcient data for learning
syntactic word representations. Moreover, translation entries exist between syntactically related
word types as well as semantically related pairs,
with the latter potentially limiting the beneﬁcial
effect of the dictionary.
When training on all the target language data,
the supervised model does well, surpassing the
cascade model. Surprisingly, the joint models outperform slightly, yielding a 0.4% improvement.
This is an interesting observation suggesting that
our method has potential for use not only for low
resource problems, but also high resource settings.
Different Tagsets
In the above experiments, we used the universal
POS tagset for all the languages in the corpus.
However, for some languages,9 the UDT also provides language speciﬁc POS tags. We use this data
to test the relative performance of the model using
a universal tagset cf. language speciﬁc tagsets. In
this experiment, we applied the same joint model
(see §3) but with a language speciﬁc tagset instead
of UPOS for these languages. We expect the joint
9en, cs, ﬁ, ga, it and sv.
Data Size (tokens)
Figure 3: Learning curve for joint model using the
UPOS tagset or language speciﬁc POS tagset: the
x-axis is the size of data (number of tokens); the yaxis is the average LAS measured on 5 languages
(except English).
model to automatically learn to project the different tagsets into a common space, i.e., implicitly
learn a tagset mapping between languages. Figure 3 shows the learning curve comparing the joint
model with the two types of POS tagsets. For the
small dataset, it is clear that the data is insufﬁcient for the model to learn a good tagset mapping, especially for a morphologically rich language like Czech. However, with more data, the
model is better able to learn the tagset mapping as
part of joint training. Beyond 15k tokens, the joint
model using the language speciﬁc POS tagset outperforms UPOS. Clearly there is some information
lost in the UPOS tagset, although the UPOS mapping simultanously provides implicit linguistic supervision. This explains why the UPOS might be
useful in small data scenarios, but detrimental at
scale. Using all the target data (“All”) the language
speciﬁc POS provides a 1% (absolute) gain over
Universal Representation
As described in section 3, we can consider our
joint model as the combination of two parts: a universal parser and a language-speciﬁc embedding
Es or Et that converts the source and target language into the universal representation. We now
seek to analyse qualitatively this universal representation through visualization. For this purpose
we use a joint model of English and French, using
all the available French treebank (more than 350k
Figure 4: Universal Language visualization according to language and POS. (This should be
viewed in colour.)
tokens) as well as a bilingual dictionary.10 Figure 4 shows the t-SNE 
projection of the 50 dimensional word embeddings in both languages. We can see that English
and French are mixed nicely together. The colouring denotes the POS tag, showing clearly that the
words with similar POS tags are grouped together
regardless of languages. This is partially understandable since word embeddings for dependency
parsing need to convey the dependency context
rather than surrounding words, as in most distributional embedding models. Words having similar
dependency relation should be grouped together as
they are treated similarly by the parser.
cross-lingual
wordembeddings are shown in Table 2, which includes
the ﬁve nearest neighbours to selected English
words according to the monolingual word embedding (section 4.3) and our cross-lingual dependency word embeddings, trained using PanLex.
The monolingual sets appear to be strongly characterised by distributional similarity. The crosslingual embeddings display greater semantic similarity, while being more variable morphosyntactically. In many cases, the top ﬁve words of English
and French are translations of each other, but with
varying inﬂectional endings in the French forms.
For example, “buy” vs “vendez” or “invest” vs “in-
10We also visualized the cross-lingual word embeddings
without the dictionary, however the results were rather odd.
Although we saw coherent POS clusters, the two languages
were largely disjoint. We speculate that many components of
the embeddings are use for only one language, and these outnumber the shared components, and thus more careful projection is needed for meaningful visualisation.
Cross lingual embedding
magniﬁcent
originally
originally
r´eellement
previously
previously
r´ecemment
derni`erement
university
universitaire
universit´e
participant
ordinateurs
Table 2: Examples of 5 nearest neighbours with
the target English word using the original monolingual word embedding and our cross-lingual dependency based word embedding.
vestir”. This is a direct consequence of incorporating the bilingual lexicon. Moreover, the top ﬁve
closest words of both English and French mostly
have the same part of speech. This is consistent
with the ﬁnding in Figure 4.
Levin has shown that there is a strong
connection between a verb’s meaning and its syntactic behaviour.
We compare the English side
of our cross-lingual dependency based word embeddings with various other pre-trained monolingual English word embeddings and our monolingual embedding (section 4.3) on Verb-143
dataset .
This dataset contains 143 pairs of verbs that are manually given
score from 1 to 10 according to the meaning similarity.
Table 3 shows the Pearson correlation
Correlation
Senna 
Skip-gram 
RNN 
Our monolingual embedding
Our crosslingual embedding
Table 3: Compare the English side of our crosslingual embeddings with various other embeddings evaluated on Verb-143 dataset (Baker et al.,
We directly use the pre-trained models
from corresponding papers.
with human judgment for our embeddings and
other pre-trained embeddings. As expected, our
cross-lingual embeddings out-perform others embeddings on this dataset. This is partly because the
syntactic behaviour is well encoded in our word
embeddings through dependency relation.
Our embeddings encode not just cross-lingual
correspondences, but also capture dependency relations which we expect might be beneﬁcial for
other NLP tasks based on dependency parsing,
e.g., cross-lingual semantic role labelling where
long-distance relationship can be captured by
word embedding.
Conclusion
In this paper, we present a training method for
building a dependency parser for a resourcepoor language using a larger treebank in a highresource language. Our approach takes advantage
of the shared structure among languages to learn
a universal parser and language-speciﬁc mappings
to the lexicon, parts of speech and dependency
Compared with supervised learning, our
joint model gives a consistent 8-10% improvement
over several different datasets in simulation lowresource scenarios. Interestingly, some small but
consistent gains are still realised by joint crosslingual training even on large complete treebanks.
This suggests that our approach has utility not just
in low resource settings. Our joint model is ﬂexible, allowing the incorporation of a bilingual dictionary, which results in small improvements particularly for tiny training scenarios.
As the side-effect of training our joint model,
we obtain cross-lingual word embeddings specialized for dependency parsing. We expect these embeddings to be beneﬁcial to other syntatic and se-
mantic tasks. In future work, we plan to extend
joint training to several languages, and further explore the idea of learning and exploiting crosslingual embeddings.
Acknowledgments
This work was supported by the University of
Melbourne and National ICT Australia (NICTA).
Trevor Cohn is the recipient of an Australian Research Council Future Fellowship (project number
FT130101105).