Deep Visual-Semantic Alignments for Generating Image Descriptions
Andrej Karpathy
Li Fei-Fei
Department of Computer Science, Stanford University
{karpathy,feifeili}@cs.stanford.edu
We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to
learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a
novel combination of Convolutional Neural Networks over
image regions, bidirectional Recurrent Neural Networks
over sentences, and a structured objective that aligns the
two modalities through a multimodal embedding. We then
describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate
novel descriptions of image regions. We demonstrate that
our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO
datasets. We then show that the generated descriptions signiﬁcantly outperform retrieval baselines on both full images
and on a new dataset of region-level annotations.
1. Introduction
A quick glance at an image is sufﬁcient for a human to
point out and describe an immense amount of details about
the visual scene . However, this remarkable ability has
proven to be an elusive task for our visual recognition models. The majority of previous work in visual recognition
has focused on labeling images with a ﬁxed set of visual
categories and great progress has been achieved in these endeavors . However, while closed vocabularies of visual concepts constitute a convenient modeling assumption,
they are vastly restrictive when compared to the enormous
amount of rich descriptions that a human can compose.
Some pioneering approaches that address the challenge of
generating image descriptions have been developed . However, these models often rely on hard-coded visual
concepts and sentence templates, which imposes limits on
their variety. Moreover, the focus of these works has been
on reducing complex visual scenes into a single sentence,
which we consider to be an unnecessary restriction.
In this work, we strive to take a step towards the goal of
Figure 1. Motivation/Concept Figure: Our model treats language
as a rich label space and generates descriptions of image regions.
generating dense descriptions of images (Figure 1). The
primary challenge towards this goal is in the design of a
model that is rich enough to simultaneously reason about
contents of images and their representation in the domain
of natural language. Additionally, the model should be free
of assumptions about speciﬁc hard-coded templates, rules
or categories and instead rely on learning from the training
data. The second, practical challenge is that datasets of image captions are available in large quantities on the internet
 , but these descriptions multiplex mentions of
several entities whose locations in the images are unknown.
Our core insight is that we can leverage these large imagesentence datasets by treating the sentences as weak labels,
in which contiguous segments of words correspond to some
particular, but unknown location in the image.
Our approach is to infer these alignments and use them to learn
a generative model of descriptions. Concretely, our contributions are twofold:
• We develop a deep neural network model that infers the latent alignment between segments of sentences and the region of the image that they describe.
 
Our model associates the two modalities through a
common, multimodal embedding space and a structured objective. We validate the effectiveness of this
approach on image-sentence retrieval experiments in
which we surpass the state-of-the-art.
• We introduce a multimodal Recurrent Neural Network
architecture that takes an input image and generates
its description in text. Our experiments show that the
generated sentences signiﬁcantly outperform retrievalbased baselines, and produce sensible qualitative predictions. We then train the model on the inferred correspondences and evaluate its performance on a new
dataset of region-level annotations.
We make code, data and annotations publicly available. 1
2. Related Work
Dense image annotations. Our work shares the high-level
goal of densely annotating the contents of images with
many works before us. Barnard et al. and Socher et
al. studied the multimodal correspondence between
words and images to annotate segments of images. Several works studied the problem of holistic
scene understanding in which the scene type, objects and
their spatial support in the image is inferred. However, the
focus of these works is on correctly labeling scenes, objects
and regions with a ﬁxed set of categories, while our focus is
on richer and higher-level descriptions of regions.
Generating descriptions. The task of describing images
with sentences has also been explored. A number of approaches pose the task as a retrieval problem, where the
most compatible annotation in the training set is transferred
to a test image , or where training annotations are broken up and stitched together .
Several approaches generate image captions based on ﬁxed
templates that are ﬁlled based on the content of the image
 or generative grammars ,
but this approach limits the variety of possible outputs.
Most closely related to us, Kiros et al. developed a logbilinear model that can generate full sentence descriptions
for images, but their model uses a ﬁxed window context
while our Recurrent Neural Network (RNN) model conditions the probability distribution over the next word in a sentence on all previously generated words. Multiple closely
related preprints appeared on Arxiv during the submission
of this work, some of which also use RNNs to generate image descriptions . Our RNN is simpler
than most of these approaches but also suffers in performance. We quantify this comparison in our experiments.
Grounding natural language in images. A number of approaches have been developed for grounding text in the vi-
1cs.stanford.edu/people/karpathy/deepimagesent
sual domain . Our approach is inspired by
Frome et al. who associate words and images through
a semantic embedding. More closely related is the work of
Karpathy et al. , who decompose images and sentences
into fragments and infer their inter-modal alignment using a
ranking objective. In contrast to their model which is based
on grounding dependency tree relations, our model aligns
contiguous segments of sentences which are more meaningful, interpretable, and not ﬁxed in length.
Neural networks in visual and language domains. Multiple approaches have been developed for representing images and words in higher-level representations. On the image side, Convolutional Neural Networks (CNNs) 
have recently emerged as a powerful class of models for
image classiﬁcation and object detection . On the sentence side, our work takes advantage of pretrained word
vectors to obtain low-dimensional representations of words. Finally, Recurrent Neural Networks have
been previously used in language modeling , but we
additionally condition these models on images.
3. Our Model
Overview. The ultimate goal of our model is to generate
descriptions of image regions. During training, the input
to our model is a set of images and their corresponding
sentence descriptions (Figure 2). We ﬁrst present a model
that aligns sentence snippets to the visual regions that they
describe through a multimodal embedding. We then treat
these correspondences as training data for a second, multimodal Recurrent Neural Network model that learns to generate the snippets.
3.1. Learning to align visual and language data
Our alignment model assumes an input dataset of images
and their sentence descriptions. Our key insight is that sentences written by people make frequent references to some
particular, but unknown location in the image. For example, in Figure 2, the words “Tabby cat is leaning” refer to
the cat, the words “wooden table” refer to the table, etc.
We would like to infer these latent correspondences, with
the eventual goal of later learning to generate these snippets
from image regions. We build on the approach of Karpathy
et al. , who learn to ground dependency tree relations
to image regions with a ranking objective. Our contribution is in the use of bidirectional recurrent neural network
to compute word representations in the sentence, dispensing of the need to compute dependency trees and allowing
unbounded interactions of words and their context in the
sentence. We also substantially simplify their objective and
show that both modiﬁcations improve ranking performance.
We ﬁrst describe neural networks that map words and image
regions into a common, multimodal embedding. Then we
introduce our novel objective, which learns the embedding
Figure 2. Overview of our approach. A dataset of images and their sentence descriptions is the input to our model (left). Our model ﬁrst
infers the correspondences (middle, Section 3.1) and then learns to generate novel descriptions (right, Section 3.2).
representations so that semantically similar concepts across
the two modalities occupy nearby regions of the space.
Representing images
Following prior work , we observe that sentence descriptions make frequent references to objects and their attributes. Thus, we follow the method of Girshick et al. 
to detect objects in every image with a Region Convolutional Neural Network (RCNN). The CNN is pre-trained on
ImageNet and ﬁnetuned on the 200 classes of the ImageNet Detection Challenge . Following Karpathy et al.
 , we use the top 19 detected locations in addition to the
whole image and compute the representations based on the
pixels Ib inside each bounding box as follows:
v = Wm[CNNθc(Ib)] + bm,
where CNN(Ib) transforms the pixels inside bounding box
Ib into 4096-dimensional activations of the fully connected
layer immediately before the classiﬁer. The CNN parameters θc contain approximately 60 million parameters. The
matrix Wm has dimensions h × 4096, where h is the size
of the multimodal embedding space (h ranges from 1000-
1600 in our experiments). Every image is thus represented
as a set of h-dimensional vectors {vi | i = 1 . . . 20}.
Representing sentences
To establish the inter-modal relationships, we would like
to represent the words in the sentence in the same hdimensional embedding space that the image regions occupy. The simplest approach might be to project every individual word directly into this embedding. However, this
approach does not consider any ordering and word context
information in the sentence. An extension to this idea is
to use word bigrams, or dependency tree relations as previously proposed . However, this still imposes an arbitrary maximum size of the context window and requires
the use of Dependency Tree Parsers that might be trained on
unrelated text corpora.
To address these concerns, we propose to use a Bidirectional Recurrent Neural Network (BRNN) to compute
the word representations. The BRNN takes a sequence of
N words (encoded in a 1-of-k representation) and transforms each one into an h-dimensional vector. However, the
representation of each word is enriched by a variably-sized
context around that word. Using the index t = 1 . . . N to
denote the position of a word in a sentence, the precise form
of the BRNN is as follows:
et = f(Wext + be)
t = f(et + Wfhf
t = f(et + Wbhb
st = f(Wd(hf
Here, It is an indicator column vector that has a single one
at the index of the t-th word in a word vocabulary. The
weights Ww specify a word embedding matrix that we initialize with 300-dimensional word2vec weights and
keep ﬁxed due to overﬁtting concerns. However, in practice we ﬁnd little change in ﬁnal performance when these
vectors are trained, even from random initialization. Note
that the BRNN consists of two independent streams of processing, one moving left to right (hf
t ) and the other right to
t) (see Figure 3 for diagram). The ﬁnal h-dimensional
representation st for the t-th word is a function of both the
word at that location and also its surrounding context in the
sentence. Technically, every st is a function of all words in
the entire sentence, but our empirical ﬁnding is that the ﬁnal
word representations (st) align most strongly to the visual
concept of the word at that location (It).
We learn the parameters We, Wf, Wb, Wd and the respective biases be, bf, bb, bd. A typical size of the hidden representation in our experiments ranges between 300-600 dimensions. We set the activation function f to the rectiﬁed
linear unit (ReLU), which computes f : x 7→max(0, x).
Alignment objective
We have described the transformations that map every image and sentence into a set of vectors in a common hdimensional space. Since the supervision is at the level of
entire images and sentences, our strategy is to formulate an
image-sentence score as a function of the individual regionword scores. Intuitively, a sentence-image pair should have
a high matching score if its words have a conﬁdent support
in the image. The model of Karpathy et a. interprets the
dot product vT
i st between the i-th region and t-th word as a
measure of similarity and use it to deﬁne the score between
image k and sentence l as:
Here, gk is the set of image fragments in image k and gl
is the set of sentence fragments in sentence l. The indices
k, l range over the images and sentences in the training set.
Together with their additional Multiple Instance Learning
objective, this score carries the interpretation that a sentence
fragment aligns to a subset of the image regions whenever
the dot product is positive. We found that the following
reformulation simpliﬁes the model and alleviates the need
for additional objectives and their hyperparameters:
Here, every word st aligns to the single best image region.
As we show in the experiments, this simpliﬁed model also
leads to improvements in the ﬁnal ranking performance.
Assuming that k = l denotes a corresponding image and
sentence pair, the ﬁnal max-margin, structured loss remains:
max(0, Skl −Skk + 1)
rank images
max(0, Slk −Skk + 1)
rank sentences
This objective encourages aligned image-sentences pairs to
have a higher score than misaligned pairs, by a margin.
Decoding text segment alignments to images
Consider an image from the training set and its corresponding sentence. We can interpret the quantity vT
i st as the unnormalized log probability of the t-th word describing any
of the bounding boxes in the image. However, since we are
ultimately interested in generating snippets of text instead
of single words, we would like to align extended, contiguous sequences of words to a single bounding box. Note that
the na¨ıve solution that assigns each word independently to
the highest-scoring region is insufﬁcient because it leads to
words getting scattered inconsistently to different regions.
To address this issue, we treat the true alignments as latent
variables in a Markov Random Field (MRF) where the binary interactions between neighboring words encourage an
Figure 3. Diagram for evaluating the image-sentence score Skl.
Object regions are embedded with a CNN (left). Words (enriched
by their context) are embedded in the same multimodal space with
a BRNN (right). Pairwise similarities are computed with inner
products (magnitudes shown in grayscale) and ﬁnally reduced to
image-sentence score with Equation 8.
alignment to the same region. Concretely, given a sentence
with N words and an image with M bounding boxes, we
introduce the latent alignment variables aj ∈{1 . . . M} for
j = 1 . . . N and formulate an MRF in a chain structure
along the sentence as follows:
j (aj, aj+1)
j (aj = t) = vT
j (aj, aj+1) = β1[aj = aj+1].
Here, β is a hyperparameter that controls the afﬁnity towards longer word phrases. This parameter allows us to
interpolate between single-word alignments (β = 0) and
aligning the entire sentence to a single, maximally scoring
region when β is large. We minimize the energy to ﬁnd the
best alignments a using dynamic programming. The output
of this process is a set of image regions annotated with segments of text. We now describe an approach for generating
novel phrases based on these correspondences.
3.2. Multimodal Recurrent Neural Network for
generating descriptions
In this section we assume an input set of images and their
textual descriptions. These could be full images and their
sentence descriptions, or regions and text snippets, as inferred in the previous section. The key challenge is in the
design of a model that can predict a variable-sized sequence
of outputs given an image. In previously developed language models based on Recurrent Neural Networks (RNNs)
 , this is achieved by deﬁning a probability distribution of the next word in a sequence given the current word
and context from previous time steps. We explore a simple
but effective extension that additionally conditions the generative process on the content of an input image. More formally, during training our Multimodal RNN takes the image
pixels I and a sequence of input vectors (x1, . . . , xT ). It
then computes a sequence of hidden states (h1, . . . , ht) and
a sequence of outputs (y1, . . . , yt) by iterating the following
recurrence relation for t = 1 to T:
bv = Whi[CNNθc(I)]
ht = f(Whxxt + Whhht−1 + bh + 1(t = 1) ⊙bv) (14)
yt = softmax(Wohht + bo).
In the equations above, Whi, Whx, Whh, Woh, xi and bh, bo
are learnable parameters, and CNNθc(I) is the last layer of
a CNN. The output vector yt holds the (unnormalized) log
probabilities of words in the dictionary and one additional
dimension for a special END token. Note that we provide
the image context vector bv to the RNN only at the ﬁrst
iteration, which we found to work better than at each time
step. In practice we also found that it can help to also pass
both bv, (Whxxt) through the activation function. A typical
size of the hidden layer of the RNN is 512 neurons.
RNN training. The RNN is trained to combine a word (xt),
the previous context (ht−1) to predict the next word (yt).
We condition the RNN’s predictions on the image information (bv) via bias interactions on the ﬁrst step. The training
proceeds as follows (refer to Figure 4): We set h0 = ⃗0, x1 to
a special START vector, and the desired label y1 as the ﬁrst
word in the sequence. Analogously, we set x2 to the word
vector of the ﬁrst word and expect the network to predict
the second word, etc. Finally, on the last step when xT represents the last word, the target label is set to a special END
token. The cost function is to maximize the log probability
assigned to the target labels (i.e. Softmax classiﬁer).
RNN at test time. To predict a sentence, we compute the
image representation bv, set h0 = 0, x1 to the START vector and compute the distribution over the ﬁrst word y1. We
sample a word from the distribution (or pick the argmax),
set its embedding vector as x2, and repeat this process until
the END token is generated. In practice we found that beam
search (e.g. beam size 7) can improve results.
3.3. Optimization
We use SGD with mini-batches of 100 image-sentence pairs
and momentum of 0.9 to optimize the alignment model. We
cross-validate the learning rate and the weight decay. We
also use dropout regularization in all layers except in the
recurrent layers and clip gradients elementwise at 5
(important). The generative RNN is more difﬁcult to optimize, party due to the word frequency disparity between
rare words and common words (e.g. ”a” or the END token).
We achieved the best results using RMSprop , which is
an adaptive step size method that scales the update of each
weight by a running average of its gradient norm.
Figure 4. Diagram of our multimodal Recurrent Neural Network
generative model. The RNN takes a word, the context from previous time steps and deﬁnes a distribution over the next word in the
sentence. The RNN is conditioned on the image information at the
ﬁrst time step. START and END are special tokens.
4. Experiments
Datasets. We use the Flickr8K , Flickr30K and
MSCOCO datasets in our experiments. These datasets
contain 8,000, 31,000 and 123,000 images respectively
and each is annotated with 5 sentences using Amazon
Mechanical Turk.
For Flickr8K and Flickr30K, we use
1,000 images for validation, 1,000 for testing and the rest
for training (consistent with ). For MSCOCO we
use 5,000 images for both validation and testing.
Data Preprocessing. We convert all sentences to lowercase, discard non-alphanumeric characters. We ﬁlter words
to those that occur at least 5 times in the training set,
which results in 2538, 7414, and 8791 words for Flickr8k,
Flickr30K, and MSCOCO datasets respectively.
4.1. Image-Sentence Alignment Evaluation
We ﬁrst investigate the quality of the inferred text and image
alignments with ranking experiments. We consider a withheld set of images and sentences and retrieve items in one
modality given a query from the other by sorting based on
the image-sentence score Skl (Section 3.1.3). We report the
median rank of the closest ground truth result in the list and
Recall@K, which measures the fraction of times a correct
item was found among the top K results. The result of these
experiments can be found in Table 1, and example retrievals
in Figure 5. We now highlight some of the takeaways.
Our full model outperforms previous work. First, our
full model (“Our model: BRNN”) outperforms Socher et
al. who trained with a similar loss but used a single
image representation and a Recursive Neural Network over
the sentence. A similar loss was adopted by Kiros et al.
 , who use an LSTM to encode sentences. We list
their performance with a CNN that is equivalent in power
(AlexNet ) to the one used in this work, though similar to they outperform our model with a more powerful
CNN (VGGNet , GoogLeNet ). “DeFrag” are the
results reported by Karpathy et al. . Since we use different word vectors, dropout for regularization and different
cross-validation ranges and larger embedding sizes, we reimplemented their loss for a fair comparison (“Our imple-
Image Annotation
Image Search
SDT-RNN (Socher et al. )
Kiros et al. 
Mao et al. 
Donahue et al. 
DeFrag (Karpathy et al. )
Our implementation of DeFrag 
Our model: DepTree edges
Our model: BRNN
Vinyals et al. (more powerful CNN)
Our model: 1K test images
Our model: 5K test images
Table 1. Image-Sentence ranking experiment results. R@K is Recall@K (high is good). Med r is the median rank (low is good). In the
results for our models, we take the top 5 validation set models, evaluate each independently on the test set and then report the average
performance. The standard deviations on the recall values range from approximately 0.5 to 1.0.
Figure 5. Example alignments predicted by our model. For every test image above, we retrieve the most compatible test sentence and
visualize the highest-scoring region for each word (before MRF smoothing described in Section 3.1.4) and the associated scores (vT
We hide the alignments of low-scoring words to reduce clutter. We assign each region an arbitrary color.
mentation of DeFrag”). Compared to other work that uses
AlexNets, our full model shows consistent improvement.
Our simpler cost function improves performance. We
strive to better understand the source of our performance.
First, we removed the BRNN and used dependency tree relations exactly as described in Karpathy et al. (“Our
model: DepTree edges”). The only difference between this
model and “Our reimplementation of DeFrag” is the new,
simpler cost function introduced in Section 3.1.3. We see
that our formulation shows consistent improvements.
BRNN outperforms dependency tree relations. Furthermore, when we replace the dependency tree relations with
the BRNN we observe additional performance improvements. Since the dependency relations were shown to work
better than single words and bigrams , this suggests that
the BRNN is taking advantage of contexts longer than two
words. Furthermore, our method does not rely on extracting
a Dependency Tree and instead uses the raw words directly.
MSCOCO results for future comparisons. We are not
aware of other published ranking results on MSCOCO.
Therefore, we report results on a subset of 1,000 images
and the full set of 5,000 test images for future comparisons.
Note that the 5000 images numbers are lower since Recall@K is a function of test set size.
Qualitative. As can be seen from example groundings in
Figure 5, the model discovers interpretable visual-semantic
correspondences, even for small or relatively rare objects
such as an “accordion”. These would be likely missed by
models that only reason about full images.
Learned region and word vector magnitudes. An appealing feature of our model is that it learns to modulate
the magnitude of the region and word embeddings. Due
to their inner product interaction, we observe that representations of visually discriminative words such as “kayaking, pumpkins“ have embedding vectors with higher magnitudes, which in turn translates to a higher inﬂuence on
the image-sentence score. Conversely, stop words such as
“now, simply, actually, but” are mapped near the origin,
which reduces their inﬂuence. See more analysis in supplementary material.
MSCOCO 2014
Nearest Neighbor
Mao et al. 
Google NIC 
MS Research 
Chen and Zitnick 
Table 2. Evaluation of full image predictions on 1,000 test images. B-n is BLEU score that uses up to n-grams. High is good in all columns.
For future comparisons, our METEOR/CIDEr Flickr8K scores are 16.7/31.8 and the Flickr30K scores are 15.3/24.7.
Figure 6. Example sentences generated by the multimodal RNN for test images. We provide many more examples on our project page.
4.2. Generated Descriptions: Fulframe evaluation
We now evaluate the ability of our RNN model to describe
images and regions. We ﬁrst trained our Multimodal RNN
to generate sentences on full images with the goal of verifying that the model is rich enough to support the mapping
from image data to sequences of words. For these full image experiments we use the more powerful VGGNet image
features . We report the BLEU , METEOR and
CIDEr scores computed with the coco-caption
code 2. Each method evaluates a candidate sentence
by measuring how well it matches a set of ﬁve reference
sentences written by humans.
Qualitative. The model generates sensible descriptions of
images (see Figure 6), although we consider the last two
images failure cases. The ﬁrst prediction “man in black
shirt is playing a guitar” does not appear in the training set.
However, there are 20 occurrences of “man in black shirt”
and 60 occurrences of “is paying guitar”, which the model
may have composed to describe the ﬁrst image. In general,
we ﬁnd that a relatively large portion of generated sentences
(60% with beam size 7) can be found in the training data.
This fraction decreases with lower beam size; For instance,
with beam size 1 this falls to 25%, but the performance also
deteriorates (e.g. from 0.66 to 0.61 CIDEr).
Multimodal RNN outperforms retrieval baseline. Our
ﬁrst comparison is to a nearest neighbor retrieval baseline.
2 
Here, we annotate each test image with a sentence of the
most similar training set image as determined by L2 norm
over VGGNet fc7 features. Table 2 shows that the Multimodal RNN conﬁdently outperforms this retrieval method.
Hence, even with 113,000 train set images in MSCOCO
the retrieval approach is inadequate. Additionally, the RNN
takes only a fraction of a second to evaluate per image.
Comparison to other work. Several related models have
been proposed in Arxiv preprints since the original submission of this work. We also include these in Table 2 for comparison. Most similar to our model is Vinyals et al. .
Unlike this work where the image information is communicated through a bias term on the ﬁrst step, they incorporate it as a ﬁrst word, they use a more powerful but more
complex sequence learner (LSTM ), a different CNN
(GoogLeNet ), and report results of a model ensemble.
Donahue et al. use a 2-layer factored LSTM (similar
in structure to the RNN in Mao et al. ). Both models
appear to work worse than ours, but this is likely in large
part due to their use of the less powerful AlexNet features. Compared to these approaches, our model prioritizes
simplicity and speed at a slight cost in performance.
4.3. Generated Descriptions: Region evaluation
We now train the Multimodal RNN on the correspondences
between image regions and snippets of text, as inferred by
the alignment model. To support the evaluation, we used
Amazon Mechanical Turk (AMT) to collect a new dataset
Figure 7. Example region predictions. We use our region-level multimodal RNN to generate text (shown on the right of each image) for
some of the bounding boxes in each image. The lines are grounded to centers of bounding boxes and the colors are chosen arbitrarily.
of region-level annotations that we only use at test time. The
labeling interface displayed a single image and asked annotators (we used nine per image) to draw ﬁve bounding boxes
and annotate each with text. In total, we collected 9,000 text
snippets for 200 images in our MSCOCO test split (i.e. 45
snippets per image). The snippets have an average length of
2.3 words. Example annotations include “sports car”, “elderly couple sitting”, “construction site”, “three dogs on
leashes”, “chocolate cake”. We noticed that asking annotators for grounded text snippets induces language statistics
different from those in full image captions. Our region annotations are more comprehensive and feature elements of
scenes that would rarely be considered salient enough to be
included in a single sentence sentence about the full image,
such as “heating vent”, “belt buckle”, and “chimney”.
Qualitative. We show example region model predictions
in Figure 7. To reiterate the difﬁculty of the task, consider
for example the phrase “table with wine glasses” that is
generated on the image on the right in Figure 7. This phrase
only occurs in the training set 30 times. Each time it may
have a different appearance and each time it may occupy a
few (or none) of our object bounding boxes. To generate
this string for the region, the model had to ﬁrst correctly
learn to ground the string and then also learn to generate it.
Region model outperforms full frame model and ranking baseline. Similar to the full image description task, we
evaluate this data as a prediction task from a 2D array of
pixels (one image region) to a sequence of words and record
the BLEU score. The ranking baseline retrieves training
sentence substrings most compatible with each region as
judged by the BRNN model. Table 3 shows that the region
RNN model produces descriptions most consistent with our
collected data. Note that the fullframe model was trained
only on full images, so feeding it smaller image regions
deteriorates its performance.
However, its sentences are
also longer than the region model sentences, which likely
negatively impacts the BLEU score. The sentence length
is non-trivial to control for with an RNN, but we note that
the region model also outperforms the fullframe model on
all other metrics: CIDEr 61.6/20.3, METEOR 15.8/13.3,
ROUGE 35.1/21.0 for region/fullframe respectively.
Human agreement
Nearest Neighbor
RNN: Fullframe model
RNN: Region level model
Table 3. BLEU score evaluation of image region annotations.
4.4. Limitations
Although our results are encouraging, the Multimodal RNN
model is subject to multiple limitations. First, the model can
only generate a description of one input array of pixels at a
ﬁxed resolution. A more sensible approach might be to use
multiple saccades around the image to identify all entities,
their mutual interactions and wider context before generating a description. Additionally, the RNN receives the image
information only through additive bias interactions, which
are known to be less expressive than more complicated multiplicative interactions . Lastly, our approach consists of two separate models. Going directly from an imagesentence dataset to region-level annotations as part of a single model trained end-to-end remains an open problem.
5. Conclusions
We introduced a model that generates natural language descriptions of image regions based on weak labels in form of
a dataset of images and sentences, and with very few hardcoded assumptions. Our approach features a novel ranking
model that aligned parts of visual and language modalities
through a common, multimodal embedding. We showed
that this model provides state of the art performance on
image-sentence ranking experiments. Second, we described
a Multimodal Recurrent Neural Network architecture that
generates descriptions of visual data. We evaluated its performance on both fullframe and region-level experiments
and showed that in both cases the Multimodal RNN outperforms retrieval baselines.
Acknowledgements.
We thank Justin Johnson and Jon Krause for helpful comments and discussions. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs
used for this research. This research is partially supported
by an ONR MURI grant, and NSF ISS-1115313.