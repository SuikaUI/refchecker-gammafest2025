IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 42, NO. 5, MAY 1997
An Analysis of Temporal-Difference Learning
with Function Approximation
John N. Tsitsiklis, Member, IEEE, and Benjamin Van Roy
Abstract— We discuss the temporal-difference learning algorithm, as applied to approximating the cost-to-go function of
an inﬁnite-horizon discounted Markov chain. The algorithm we
analyze updates parameters of a linear function approximator online during a single endless trajectory of an irreducible aperiodic
Markov chain with a ﬁnite or inﬁnite state space. We present a
proof of convergence (with probability one), a characterization of
the limit of convergence, and a bound on the resulting approximation error. Furthermore, our analysis is based on a new line
of reasoning that provides new intuition about the dynamics of
temporal-difference learning.
In addition to proving new and stronger positive results than
those previously available, we identify the signiﬁcance of online updating and potential hazards associated with the use of
nonlinear function approximators. First, we prove that divergence may occur when updates are not based on trajectories
of the Markov chain. This fact reconciles positive and negative
results that have been discussed in the literature, regarding the
soundness of temporal-difference learning. Second, we present an
example illustrating the possibility of divergence when temporaldifference learning is used in the presence of a nonlinear function
approximator.
Index Terms— Dynamic programming, function approximation, Markov chains, neuro-dynamic programming, reinforcement learning, temporal-difference learning.
I. INTRODUCTION
HE PROBLEM of predicting the expected long-term
future cost (or reward) of a stochastic dynamic system
manifests itself in both time-series prediction and control. An
example in time-series prediction is that of estimating the net
present value of a corporation as a discounted sum of its future
cash ﬂows, based on the current state of its operations. In
control, the ability to predict long-term future cost as a function
of state enables the ranking of alternative states in order to
guide decision-making. Indeed, such predictions constitute the
cost-to-go function that is central to dynamic programming
and optimal control .
Temporal-difference learning, originally proposed by Sutton
 , is a method for approximating long-term future cost
as a function of current state. The algorithm is recursive,
efﬁcient, and simple to implement. A function approximator
is used to approximate the mapping from state to future cost.
Manuscript received March 20, 1996; revised November 11, 1996. Recommended by Associate Editor, E. K. P. Chong. This work was supported by
the NSF under Grant DMI-9625489 and the ARO under Grant DAAL-03-92-
The authors are with the Laboratory for Information and Decision Systems,
Massachusetts Institute of Technology, Cambridge, MA 02139 USA (e-mail:
 ).
Publisher Item Identiﬁer S 0018-9286(97)03437-5.
Parameters of the function approximator are updated upon
each observation of a state transition and the associated cost.
The objective is to improve approximations of long-term future
cost as more and more state transitions are observed. The
trajectory of states and costs can be generated either by a
physical system or a simulated model. In either case, we view
the system as a Markov chain. Adopting terminology from
dynamic programming, we will refer to the function mapping
states of the Markov chain to expected long-term cost as the
cost-to-go function.
Though temporal-difference learning is simple and elegant,
a rigorous analysis of its behavior requires signiﬁcant sophistication. Several previous papers have presented positive results
about the algorithm. These include – , all of which only
deal with cases where the number of tunable parameters is the
same as the cardinality of the state space. Such cases are not
practical when state spaces are large or inﬁnite. The more
general case, involving the use of function approximation,
is addressed by results in – . The latter three establish convergence with probability one. However, their results
only apply to a very limited class of function approximators
and involve variants of a constrained version of temporaldifference learning, known as TD(0). Dayan establishes
convergence in the mean for the general class of linear function
approximators, i.e., function approximators involving linear
combinations of ﬁxed basis functions, where the weights of
the basis functions are tunable parameters. However, this form
of convergence is rather weak, and the analysis used in the
paper does not directly lead to approximation error bounds
or interpretable characterizations of the limit of convergence.
Schapire and Warmuth carry out a (nonprobabilistic) worst
case analysis of an algorithm similar to temporal-difference
learning. Fewer assumptions are required by their analysis, but
the end results do not imply convergence and establish error
bounds that are weak relative to those that can be deduced in
the standard probabilistic framework.
In addition to the positive results, counterexamples to variants of the algorithm have been provided in several papers;
these include , , , and . As suggested by Sutton
 , the key feature that distinguishes these negative results
from their positive counterparts is that the variants of temporaldifference learning used do not employ on-line state sampling.
In particular, sampling is done by a mechanism that samples
states with frequencies independent from the dynamics of the
underlying system. Our results shed light on these counterexamples by showing that for linear function approximators,
convergence is guaranteed if states are sampled according
0018–9286/97$10.00 1997 IEEE
TSITSIKLIS AND VAN ROY: ANALYSIS OF TEMPORAL-DIFFERENCE LEARNING
to the steady-state probabilities, while divergence is possible
when states are sampled from distributions independent of
the dynamics of the Markov chain of interest. Given that the
steady-state probabilities are usually unknown, the only viable
approach to generating the required samples is to perform online sampling. By this we mean that the samples should consist
of an actual sequence of visited states obtained either through
simulation of a Markov chain or observation of a physical
In addition to the analysis of temporal-difference learning
in conjunction with linear function approximators, we provide
an example demonstrating that the algorithm may diverge
when a nonlinear function approximator is employed. This
example should be viewed as a warning rather than a ban on
all nonlinear function approximators. In particular, the function
approximator used in the example is somewhat contrived, and
it is not clear whether or not divergence can occur with speciﬁc
classes of nonlinear function approximators such as neural
In this paper, we focus on the application of temporaldifference learning to inﬁnite-horizon discounted Markov
chains with ﬁnite or inﬁnite state spaces. Though absorbing
(and typically ﬁnite state) Markov chains have been the
dominant setting for past analyses, we ﬁnd the inﬁnite-horizon
framework to be the most natural and elegant setting for
temporal-difference learning. Furthermore, the ideas used in
our analysis can be applied to the simpler context of absorbing
Markov chains. Though this extension is omitted from this
paper, it can be found in , which also contains a more
accessible version of the results in this paper for the case of
ﬁnite state spaces.
The contributions in this paper are as follows.
1) Convergence (with probability one) is established for
the case where approximations are generated by linear
combinations of (possibly unbounded) basis functions
over a (possibly inﬁnite) state space. This is the ﬁrst such
result that handles the case of “compact representations”
of the cost-to-go function, in which there are fewer
parameters than states. (In fact, convergence of on-line
TD( ) in the absence of an absorbing state had not
been established even for the case of a lookup table
representation.)
2) The limit of convergence is characterized as the solution
to a set of interpretable linear equations, and a bound is
placed on the resulting approximation error.
3) Our methodology leads to an interpretation of the limit
of convergence and hence new intuition on temporaldifference learning and the dynamics of weight updating.
4) We reconcile positive and negative results concerning
temporal-difference learning by proving a theorem that
identiﬁes the importance of on-line sampling.
5) We provide an example demonstrating the possibility of
divergence when temporal-difference learning is used in
conjunction with a nonlinear function approximator.
At about the same time that this paper was initially submitted, Gurvits independently established convergence with
probability one in the context of absorbing Markov chains.
Also, Pineda derived a stable differential equation for
the “mean ﬁeld” of temporal-difference learning, in the case
of ﬁnite-state absorbing Markov chains. He also suggested
a convergence proof based on a weighted maximum norm
contraction property, which, however, is not satisﬁed in the
presence of function approximation. (The proof was corrected
after the paper became available.)
This paper is organized as follows. In Section II, we provide
a precise deﬁnition of the algorithm that we will be studying.
Sections III–IX deal only with the use of linear function
approximators. In Section III, we recast temporal-difference
learning in a way that sheds light into its mathematical
structure. Section IV contains our main convergence result
together with our assumptions. We develop some mathematical machinery in Section V, which captures the fundamental
ideas involved in the analysis. Section VI presents a proof
of the convergence result, which consists primarily of the
technicalities required to integrate the machinery supplied
by Section V. Our analysis is valid for general state spaces,
subject to certain technical assumptions. In Section VII, we
show that these technical assumptions are automatically valid
for the case of irreducible aperiodic ﬁnite-state Markov chains.
In Section VIII, we argue that the class of inﬁnite-state Markov
chains that satisfy our assumptions is broad enough to be
of practical interest. Section IX contains our converse convergence result, which establishes the importance of on-line
sampling. Section X departs from the setting of linear function
approximators, presenting a divergent example involving a
nonlinear function approximator. Finally, Section XI contains
some concluding remarks.
II. DEFINITION OF TEMPORAL-DIFFERENCE LEARNING
In this section, we deﬁne precisely the nature of temporaldifference learning, as applied to approximation of the cost-togo function for an inﬁnite-horizon discounted Markov chain.
While the method as well as our subsequent results are
applicable to Markov chains with a fairly general state space,
we restrict our attention to the case where the state space
is countable. This allows us to work with relatively simple
notation; for example, the Markov chain can be deﬁned in
terms of an (inﬁnite) transition probability matrix as opposed
to a transition probability kernel. The extension to the case of
general state spaces requires the translation of the matrix notation into operator notation, but is otherwise straightforward.
We consider an irreducible aperiodic Markov chain whose
states lie in a ﬁnite or countably inﬁnite space
. By indexing
the states with positive integers, we can view the state space as
is possibly inﬁnite. Note that
the positive integers only serve as indexes here. In particular,
each state might actually correspond to some other entity such
as a vector of real numbers describing the state of a physical
system. In such a case, the actual state space would comprise
of a countable subset of a Euclidean space.
The sequence of states visited by the Markov chain is
denoted by
. The dynamics of the
Markov chain are described by a (ﬁnite or inﬁnite) transition
probability matrix
th entry, denoted by
IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 42, NO. 5, MAY 1997
the probability that
given that
. For any pair
, we are given a scalar
that represents the cost
of a transition from
. (Extensions to the case where the
one-stage costs are random is discussed in our conclusions
section.) Finally, we let
be a discount factor.
The cost-to-go function
associated with this
Markov chain is deﬁned by
assuming that this expectation is well-deﬁned. It is often
convenient to view
as a vector instead of a function (its
dimension is inﬁnite if
is inﬁnite).
We consider approximations of
using a function
, which we refer to as a function approximator.
To approximate the cost-to-go function one usually tries to
choose a parameter vector
so as to minimize some
error metric between the functions
Suppose that we observe a sequence of states
according to the transition probability matrix
and that at
the parameter vector
has been set to some value
. We deﬁne the temporal difference
corresponding to the
transition from
the temporal-difference learning
method updates
according to the formula
is initialized to some arbitrary vector,
is a sequence
of scalar step sizes,
is a parameter in
, and the gradient
is the vector of partial derivatives with respect to
the components of
. Since temporal-difference learning is
actually a continuum of algorithms, parameterized by
often referred to as TD( ).
In the special case of linear function approximators, the
takes the form
is the parameter vector and each
is a ﬁxed scalar function deﬁned on the state space
The functions
can be viewed as basis functions (or as
vectors of dimension
), while each
can be viewed
as the associated weight.
It is convenient to deﬁne a vector-valued function
by letting
. With this notation,
the approximation can also be written in the form
is viewed as an
matrix whose
is equal to
Note that the gradient vector here is given by
and we have
is the Jacobian matrix whose th column is equal
In the case of linear function approximators, a more convenient representation of TD( ) is obtained by deﬁning a
sequence of eligibility vectors
(of dimension
With this new notation, the TD( ) updates are given by
and the eligibility vectors can be updated according to
initialized with
In the next few sections, we focus on temporal-difference
learning as used with linear function approximators. Only
in Section X do we return to the more general context of
nonlinear function approximators.
III. UNDERSTANDING TEMPORAL-DIFFERENCE LEARNING
Temporal-difference learning originated in the ﬁeld of reinforcement learning. A view commonly adopted in the original
setting is that the algorithm involves “looking back in time
and correcting previous predictions.” In this context, the
eligibility vector keeps track of how the parameter vector
should be adjusted in order to appropriately modify prior
predictions when a temporal-difference is observed. In this
paper, we take a different view which involves examining the
“steady-state” behavior of the algorithm and arguing that this
characterizes the long-term evolution of the parameter vector.
In the remainder of this section, we introduce this view of
TD( ) and provide an overview of the analysis that it leads
to, in the context of linear function approximators. Our goal is
to convey some intuition about how the algorithm works, and
in this spirit we maintain the discussion at an informal level,
omitting technical assumptions and other details required to
TSITSIKLIS AND VAN ROY: ANALYSIS OF TEMPORAL-DIFFERENCE LEARNING
formally prove the statements we make. These technicalities
will be addressed in subsequent sections, where formal proofs
are presented.
A. Inner Product Space Concepts and Notation
We begin by introducing some notation that will make our
discussion here, as well as the analysis later in the paper,
more concise. Let
denote the steady-state
probabilities for the process
. We assume that
. We deﬁne an
diagonal matrix
with diagonal
. It is easy to see that
satisﬁes the requirements for an inner product. We denote
the norm on the associated inner product space by
and the set of vectors
. As we will later prove,
and it is in this inner product space that the approximations
evolve. Regarding notation, we will also keep
, without a subscript, to denote the Euclidean
norm on ﬁnite-dimensional vectors or the Euclidean-induced
norm on ﬁnite matrices. (That is, for any matrix
We will assume that each basis function
is an element
. For any pair
of functions
, we say that
-orthogonal
(denoted by
) if and only if
, there exists a unique element
minimizing
referred to as the projection of
with respect
. We deﬁne a “projection matrix” (more precisely,
projection operator)
that generates such a
when applied to
. Assuming that the basis functions
are linearly
independent, the projection matrix is given by
(Note that
matrix.) For any
we then have
Furthermore,
is the unique element of
. In other
words, the difference between
-orthogonal to the
space spanned by the basis functions.
The projection
is a natural approximation to
given the ﬁxed set of basis functions. In particular,
the solution to the weighted linear least-squares problem of
minimizing
with respect to . Note that the error associated with each state
is weighed by the frequency with which the state is visited.
(If the state space were continuous instead of countable, this
sum would be replaced by an integral.)
B. The TD( ) Operator
To streamline our analysis of TD( ) we introduce an operator that is useful in characterizing the algorithm’s dynamics.
This operator, which we will refer to as the TD( ) operator,
is indexed by a parameter
and is denoted by
. It is deﬁned by
some technical conditions). The fact that
will be established in a later section. To
interpret the TD( ) operator in a meaningful manner, note
that for each
, the term
is the expected cost to be incurred over
transitions plus an
approximation to the remaining cost to be incurred, based on
. This sum is sometimes called the “
-stage truncated costto-go.” Intuitively, if
is an approximation to the cost-to-go
function, the
-stage truncated cost-to-go can be viewed as an
improved approximation. Since
is a weighted average
-stage truncated cost-to-go values,
be viewed as an improved approximation to
. In fact, we
will prove later that
is a contraction on
ﬁxed point is
is always closer to
is, in the sense of the norm
C. Dynamics of the Algorithm
To clarify the fundamental structure of TD( ), we construct
. It is easy to see that
Markov process. In particular,
are deterministic
functions of
and the distribution of
only depends on
. Note that at each time , the random vector
, together
with the current parameter vector
, provides all necessary
information for computing
. By deﬁning a function
, we can rewrite the TD( ) algorithm as
As we will show later, for any ,
has a well-deﬁned
“steady-state” expectation, which we denote by
Intuitively, once
reaches steady state, the TD( ) algorithm,
in an “average” sense, behaves like the following deterministic
algorithm:
IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 42, NO. 5, MAY 1997
Under some technical assumptions, the convergence of this
deterministic algorithm implies convergence of TD( ), and
both algorithms share the same limit of convergence. Our study
centers on an analysis of this deterministic algorithm.
It turns out that
and thus the deterministic algorithm takes the form
As a side note, observe that the execution of this deterministic
algorithm would require knowledge of transition probabilities
and the transition costs between all pairs of states, and when
the state space is large or inﬁnite, this is not feasible. Indeed,
stochastic approximation algorithms like TD( ) are motivated by the need to alleviate such stringent information and
computational requirements. We introduce the deterministic
algorithm solely for conceptual purposes and not as a feasible
alternative for practical use.
To gain some additional insight about the evolution of
we rewrite the deterministic algorithm in the form
Note that in the case of
, this becomes
which is a steepest descent iteration for the problem of
minimizing
with respect to
. It is easy to show that if the step sizes are
appropriately chosen,
will converge to
In the case of
, we can think of each iteration of the
deterministic algorithm as that of a steepest descent method
for minimizing
with respect to , given a ﬁxed
. Note, however, that the error
function changes from one time step to the next, and therefore
it is not a true steepest descent method. Nevertheless, if we
as an approximation to
, the algorithm
makes some intuitive sense. However, some subtleties are
involved here.
To illustrate this, consider a probability distribution
over the state-space
that is different from the steady-state
distribution
. Deﬁne a diagonal matrix
with diagonal
. If we replace the matrix
deterministic variant of TD(1) with the matrix
, we obtain
which is a steepest descent method that minimizes
with respect to
. If step sizes are appropriately chosen,
will converge to
is the projection matrix
with respect to the inner product
. On the other hand, if
we replace
in the TD( ) algorithm for
algorithm might not converge at all! We will formally illustrate
this phenomenon in Section IX.
To get a better grasp on the issues involved here, let us
consider the following variant of the algorithm:
Note that by letting
, we recover the deterministic
variant of TD( ). Each iteration given by (3) can be thought
of as a steepest descent iteration on an error function given by
(The variable being optimized is
remains ﬁxed.)
Note that the minimum of this (time-varying) error function
is given by
. Hence, letting
we might think of
as a “target vector,” given a
current vector
. We can deﬁne an algorithm of the form
which moves directly to the target, given a current vector
Intuitively, the iteration of (3) can be thought of as an
incremental form of (4). Hence, one might expect the two
algorithms to have similar convergence properties. In fact,
they do. Concerning convergence of the algorithm given by
(4), note that if
is a contraction of the norm
the composition
is also a contraction of the norm
, since the projection
is a nonexpansion of that norm.
However, there is no reason to believe that the projection
will be a nonexpansion of the norm
In this case,
may not be a contraction and might
even be an expansion. Hence, convergence guarantees for the
algorithms of (3) and (4) rely on a relationship between
. This idea captures the issue that arises with variants
of TD( ) that sample states with frequencies independent of
the dynamics of the Markov process. In particular, the state
sampling frequencies are reﬂected in the matrix
, while the
dynamics of the Markov process make
a contraction
with respect to
. When states are sampled on-line, we
, while there is no such promise when states are
sampled by an independent mechanism.
For another perspective on TD( ), note that the deterministic variant, as given by (2), can be rewritten in the form
for some matrix
and vector
. As we will show later, the
contraction property of
and the fact that
is a projection
with respect to the same norm imply that the matrix
negative deﬁnite. From this fact, it is easy to see that the
iteration converges, given appropriate step-size constraints.
However, it is difﬁcult to draw an intuitive understanding
from the matrix
, as we did for the operators
Nevertheless, for simplicity of proof, we use the representation
in terms of
when we establish that TD( ) has the
TSITSIKLIS AND VAN ROY: ANALYSIS OF TEMPORAL-DIFFERENCE LEARNING
properties required for application of the available machinery
from stochastic approximation. This machinery is what allows
us to deduce convergence of the actual (stochastic) algorithm
from that of the deterministic counterpart.
IV. CONVERGENCE RESULT
In this section we present the main result of this paper,
which establishes convergence and characterizes the limit
of convergence of temporal-difference learning, when linear
function approximators are employed. We begin by stating the
required assumptions.
The ﬁrst assumption places constraints on the underlying
inﬁnite-horizon discounted Markov chain. Essentially, we assume that the Markov chain is irreducible and aperiodic and
that the steady-state variance of transition costs is ﬁnite. The
formal statement follows.
Assumption 1:
1) The Markov chain
is irreducible and aperiodic. Furthermore, there is a unique distribution
that satisﬁes
for all ; here,
is a ﬁnite or inﬁnite
vector, depending on the cardinality of
stand for expectation with respect to this distribution.
2) Transition costs
Our second assumption ensures that the basis functions used
for approximation are linearly independent and do not grow
Assumption 2:
1) The matrix
has full column rank; that is, the basis
are linearly independent.
2) For every
, the basis function
The next assumption essentially requires that the Markov
chain has a certain “degree of stability.” As will be shown
in Section VI, this assumption is always satisﬁed when the
state-space
is ﬁnite. It is also satisﬁed in many situations of
practical interest when the set
is inﬁnite. Further discussion
can be found in Section VII.
Assumption 3: There exists a function
range is the set of nonnegative reals) satisfying the following
requirements.
1) For all
, there exists a constant
such that for all
Implicit in the statement of this assumption is that certain
expectations are ﬁnite. It will be seen later that their ﬁniteness
is a consequence of earlier assumptions.
Our ﬁnal assumption places fairly standard constraints on
the sequence of step sizes.
Assumption 4: The step sizes
are positive, nonincreasing, and predetermined (chosen prior to execution of the
algorithm). Furthermore, they satisfy
The main result of this paper follows.
Theorem 1: Under Assumptions 1–4, the following hold.
1) The cost-to-go function
2) For any
, the TD( ) algorithm with linear function approximators, as deﬁned in Section II, converges
with probability one.
3) The limit of convergence
is the unique solution of
the equation
4) Furthermore,
In order to place Theorem 1 in perspective, let us discuss
its relation to available results. If one lets
be the th unit
vector for each , and if we assume that
is ﬁnite, we are
dealing with a lookup table representation of the cost-to-go
function. In that case, we recover a result similar to those in
 (actually, that paper dealt with the on-line TD( ) algorithm
only for Markov chains involving a termination state). With
a lookup table representation, the operator
shown to be a maximum norm contraction, the projection
is simply the identity matrix, and general results
on stochastic approximation methods based on maximum
norm contractions , become applicable. However, once
function approximation is introduced, the composition
need not be a maximum norm contraction, and this approach
does not extend.
Closer to our results is the work of Dayan who considered TD( ) for the case of linear function approximators
and established a weak form of convergence . Finally, the work of Dayan and Sejnowski 
contains a sketch of a proof of convergence with probability
one. However, it is restricted to the case where the vectors
are linearly independent, which is essentially equivalent
to having a lookup table representation. (A more formal proof,
for this restricted case, has been developed in .) Some of the
ideas in our method of proof originate in the work of Sutton
 and Dayan . Our analysis also leads to an interpretation
of the limit of convergence. In particular, Theorem 1 offers an
illuminating ﬁxed-point equation, as well as a graceful bound
on the approximation error. Previous works lack interpretable
results of this kind.
V. PRELIMINARIES
In this section we present a series of lemmas that provide
the essential ideas behind Theorem 1. Lemma 1 states a
general property of Markov chains that is central to the
analysis of TD( ). Lemma 2 ensures that our assumptions
are sufﬁcient to have a well-deﬁned cost-to-go function
. Lemmas 3–6 deal with properties of the TD( )
and the composition
, as well as their ﬁxed
points. Lemma 7 characterizes the steady-state expectations
of various variables involved in the dynamics of TD( ), and
these results are used in the proof of Lemma 8, which deals
with the steady-state dynamics. Lemma 9 establishes that these
dynamics lead to convergence of the deterministic version
of the algorithm. Finally, we state a theorem concerning
stochastic approximation that will be used in Section VI, along
with the lemmas, to establish convergence of the stochastic
algorithm.
We begin with the fundamental lemma on Markov chains.
Lemma 1: Under Assumption 1-1), for any
Proof: The
inequality,
Tonelli–Fubini theorem, and the property
Our ﬁrst use of Lemma 1 will be in showing that
. In particular, we have the following result, where
we use the notation
to denote the vector of dimension
whose th component is equal to
Lemma 2: Under Assumptions 1-1) and 2),
deﬁned and ﬁnite for every
. Furthermore,
Proof: If the Markov chain starts in steady state, it
remains in steady state, and therefore
where we are using the Tonelli–Fubini theorem to interchange
the expectation and the summation, as well as Assumption
1-2). Since
, it follows that
for all , the expectation deﬁning
deﬁned and ﬁnite.
Using the Tonelli–Fubini theorem to switch the order of
expectation and summation in the deﬁnition of
, we obtain
and it follows that
To show that
where the second inequality follows from Lemma 1. Note that
by Assumption 1-2). It follows that
TSITSIKLIS AND VAN ROY: ANALYSIS OF TEMPORAL-DIFFERENCE LEARNING
The next lemma states that the operator
into itself and provides a formula for evaluating
Lemma 3: Under Assumption 1, for any
Proof: We have
and the formula in the statement of the lemma follows.
We have shown in Lemma 2 that
. Thus, for
, we can use Lemma 1 to obtain
, by Lemma 1. This completes the proof.
Lemma 1 can also be used to show that
is a contraction
. This fact, which is captured by the next lemma,
will be useful for establishing error bounds.
Lemma 4: Under
Assumption
Proof: The case of
is trivial. For
, the result
follows from Lemmas 1 and 3. In particular, we have
The next lemma states that
is the unique ﬁxed point of
Lemma 5: Under Assumption 1, for any
cost-to-go function
uniquely solves the system of equations
Proof: For the case of
, the result follows directly
from the deﬁnition of
, the fact that
ﬁxed point follows from Lemmas 2 and 3, the Tonelli–Fubini
theorem, and some simple algebra:
The contraction property (Lemma 4) implies that the ﬁxed
point is unique.
The next lemma characterizes the ﬁxed point of the composition
. This ﬁxed point must lie in the range of
, which is the space
[note that this is a
subspace of
, because of Assumption 2-2)]. The
lemma establishes existence and uniqueness of this ﬁxed point,
which we will denote by
. Note that in the special case
the lemma implies that
, in agreement
with the deﬁnition of
Lemma 6: Under Assumptions 1 and 2,
contraction and has a unique ﬁxed point which is of the form
for a unique choice of
. Furthermore,
satisﬁes the
following bound:
Proof: Lemma 4 ensures that
is a contraction from
into itself, and from
is its ﬁxed point by
Lemma 5. Note that for
, by the Babylonian–Pythagorean theorem we have
. It follows that
is nonexpansive,
and thus the composition
is a contraction. Hence,
has a unique ﬁxed point of the form
, for some
. Because the functions
are assumed to be linearly
independent, it follows that the choice of
is unique.
Using the fact that
(Lemma 2) and is
the ﬁxed point of
(Lemma 5), we establish the desired
IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 42, NO. 5, MAY 1997
bound. In particular, we have
and it follows that
We next set out to characterize the expected behavior of
the steps taken by the TD( ) algorithm in “steady state.”
In particular, we will get a handle on
. While this expression can be viewed as a limit of
goes to inﬁnity, it is simpler to view it as
an expectation referring to a process that is already in steady
state. We therefore make a short digression to construct a
stationary process
We proceed as follows. Let
be a Markov chain that
evolves according to the transition probability matrix
is in steady state, in the sense that
and all . Given any sample path of this Markov chain,
is constructed by taking the stationary process
, whose variance is ﬁnite (Assumption 2), and passing it
through an exponentially stable linear time invariant system.
It is then well known that the output
of this ﬁlter is ﬁnite
with probability one and has also ﬁnite variance. With
constructed, we let
and note that this is
a Markov process with the same transition probabilities as
the Markov process
that was constructed in the middle
of Section III (the evolution equation is the same). The only
difference is that the process
of Section III was initialized
, whereas here we have a stationary process
We can now identify
with the expectation with respect
to this invariant distribution.
Prior to studying
, let us establish a few preliminary relations in the next lemma.
Lemma 7: Under Assumptions 1 and 2, the following relations hold.
Furthermore, each of the above expressions is well deﬁned
and ﬁnite.
Proof: We ﬁrst observe that for any
(Note that
, by Lemma 1, and using the
Cauchy–Schwartz inequality,
is ﬁnite.) By specializing to the case where we are dealing with vectors of the
(these vectors are in
a consequence of Assumption 2), we obtain
Since the vectors
are arbitrary, it follows that
We place a bound on the Euclidean-induced matrix norm
as follows. We have
which is a ﬁnite constant
, by Assumption 2-2). We have
used here the notation
to indicate the
th column of
the matrix
, with entries
. Note that the
second inequality above follows from the Cauchy–Schwartz
inequality.
We have so far veriﬁed parts 1) and 2) of the lemma. We
now begin with the analysis for part 3). Note that
is the same for all , and it sufﬁces to prove the result for the
where the interchange of summation and expectation is justi-
ﬁed by the dominated convergence theorem. The desired result
follows by using the result of part 1).
The results of parts 4) and 5) are proved by entirely similar
arguments, which we omit.
With the previous lemma at hand, we are ready to characterize
. This is done in the following lemma.
Lemma 8: Under Assumptions 1 and 2, we have
which is well deﬁned and ﬁnite for any ﬁnite
TSITSIKLIS AND VAN ROY: ANALYSIS OF TEMPORAL-DIFFERENCE LEARNING
Proof: By applying Lemma 7, we have
, it follows that
Note that for
Hence, for
by Lemma 3. Each expression is ﬁnite and well deﬁned by
The next lemma shows that the steps taken by TD( ) tend
Lemma 9: Under Assumptions 1 and 2, we have
Proof: We have
where the last equality follows because
(1)]. As shown in the beginning of the proof of Lemma
is a contraction with ﬁxed point
contraction factor is no larger than
and using the Cauchy–Schwartz inequality, we obtain
, the result follows.
We now state without proof a result concerning stochastic
approximation which will be used in the proof of Theorem 1.
This is a special case of a very general result on stochastic
approximation algorithms [19, Th. 17, p. 239]. It is straightforward to check that all of the assumptions in the result of
 follow from the assumptions imposed in the result below.
We do not show here the assumptions of because the list
is long and would require a lot in terms of new notation.
However, we note that in our setting here, the potential
that would be required to satisfy the assumptions
of the theorem from is given by
Theorem 2: Consider an iterative algorithm of the form
1) the (predetermined) step-size sequence
is positive,
nonincreasing,
is a Markov process with a unique invariant distribution, and there exists a mapping
from the states of
the Markov process to the positive reals, satisfying the
remaining conditions. Let
stand for expectation
with respect to this invariant distribution;
are matrix and vector valued functions,
respectively, for which
are well deﬁned and ﬁnite;
4) the matrix
is negative deﬁnite;
5) there exist constants
such that for all
6) for any
there exists a constant
such that for
converges to
, with probability one, where
the unique vector that satisﬁes
VI. PROOF OF THEOREM 1
involved in the update of
takes the form
By Lemma 7,
well deﬁned and ﬁnite.
By Lemma 6, we have
. From (1), we
IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 42, NO. 5, MAY 1997
We now compare with the formula for
, as given
by Lemma 8, and conclude that
It follows from Lemma 9 that
, and thus
is negative deﬁnite.
We will use Theorem 2 to show that
converges. Our
analysis thus far ensures validity of all conditions except for
5) and 6). We now show that Assumption 3 is sufﬁcient to
ensure validity of these two conditions.
We begin by bounding the summations involved in 5).
, recall that
Let us concentrate on the term
. Using the formula for
Using the triangle inequality, we obtain
We will individually bound the magnitude of each summation
in the right-hand side.
First we have
where the second inequality follows from the fact that
. Assumption 3-1) implies that
for some constants
. It follows that
for some constants
Next, we deal with the second summation. Letting
be deﬁned by
for some constant
, where the inequality follows from
Assumption 3-1).
Finally, recalling that
, for some
absolute constant
(Lemma 7), we have
Given these bounds, it follows that there exist positive
In other words, the summation above is bounded by a polynomial function of
. An identical argument
can be carried out for the terms
which we omit to avoid repetition. Using these arguments, we
can place bounds that are polynomial in
on the summations in Condition 5) of Theorem 2. We can thus
satisfy the condition with a function
that is polynomial in
. The fact that such
a function
would satisfy Condition 6) then follows from
Assumption 3-2).
We now have all the conditions needed to apply Theorem 2.
It follows that
converges to
, which solves
, Lemma 8 implies that
By Lemma 6 along with the fact that
has full row
rank [by virtue of Assumption 2-1)],
uniquely satisﬁes this
TSITSIKLIS AND VAN ROY: ANALYSIS OF TEMPORAL-DIFFERENCE LEARNING
equation and is the unique ﬁxed point of
. Lemma 6 also
provides the desired error bound. This completes the proof to
Theorem 1.
VII. THE CASE OF A FINITE STATE SPACE
In this section, we show that Assumptions 1-2), 2-2), and
3 are automatically true whenever we are dealing with an
irreducible aperiodic Markov chain with a ﬁnite state space.
This tremendously simpliﬁes the conditions required to apply
Theorem 1, reducing them to a requirement that the basis
functions be linearly independent [Assumption 2a)]. Actually,
even this assumption can be relaxed if Theorem 1 is stated
in a more general way. This assumption was adopted for the
sake of simplicity in the proof.
Let us now assume that
is an irreducible aperiodic ﬁnitestate Markov chain [Assumption 1-1)]. Assumptions 1-2) and
2-2) are trivially satisﬁed when the state space is ﬁnite. We
therefore only need to prove that Assumption 3 is satisﬁed.
It is well known that for any irreducible aperiodic ﬁnite-state
Markov chain, there exist scalars
. We deﬁne a sequence of
diagonal matrices
with the th diagonal element equal to
It is then easy to show that
the proof being essentially the same as in Lemma 7-1). We
Note that all entries of
are bounded by one, and therefore
there exists a constant
The ﬁrst part of Assumption 3-1) is thus satisﬁed by a function
that is equal to a constant for all
. An analogous
argument, which we omit, can be used to establish that
the same is true for the second part of Assumption 3-1).
Assumption 3-2) follows from the fact that
is constant.
VIII. INFINITE STATE SPACES
The purpose of this section is to shed some light on the
nature of our assumptions and to suggest that our results
apply to inﬁnite-state Markov chains of practical interest. For
concreteness, let us assume that the state space is a countable
. Each state
is associated with an integer
and denoted by
Let us ﬁrst assume that the state space is a bounded subset of
and that the mappings deﬁned by
are continuous functions on
. Then, Assumptions 1-2) and 2-2) are automatically valid
because continuous functions are bounded on bounded sets.
Assumption 3-1) basically refers to the speed with which the
Markov chain reaches steady state. Let
be a diagonal
matrix whose th entry is
. Then Assumption 3-
3) is satisﬁed by a function
if we impose a condition
of the form
for some ﬁnite constant
. In other words, we want the -step
transition probabilities to converge fast enough to the steadystate probabilities (for example,
could drop at the
). In addition, we need this convergence to be
uniform in the initial state.
As a special case, suppose that the Markov chain has a
distinguished state, say state zero, and that for some
converges to
exponentially fast, and uniformly
, and Assumption 3-1) is satisﬁed with
. Validity
of Assumption 3-2) easily follows.
Let us now consider the case where the state space is an
unbounded subset of
. For many stochastic processes of
practical interest (e.g., those that satisfy a large deviations
principle), the tails of the probability distribution
exhibit exponential decay; let us assume that this is the case.
For the purposes of Assumption 3, it is natural in this context
to employ a function
, for some
. Assumption 3-2) is essentially a stability condition; given
our deﬁnition of
, it states that
is not expected to
grow too rapidly, and this is satisﬁed by most stable Markov
chains of practical interest. Note that by taking the steady-state
limit we obtain
, which in essence
says that the tails of the steady-state distribution
faster than any polynomial (e.g., exponentially).
Assumption 3-1) is the most complex one. Recall that it
deals with the speed of convergence of certain functions of
the Markov chain to steady state. Whether it is satisﬁed has
to do with the interplay between the speed of convergence of
and the growth rate of the functions
. Note that the assumption allows the rate of convergence
to get worse as
increases; this is captured by the term
in the right-hand side.
We close with a concrete illustration, related to queueing
theory. Let
be a Markov chain that takes values in the
IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 42, NO. 5, MAY 1997
nonnegative integers, and let its dynamics be
are independent, identically distributed nonnegative integer random variables with a “nice” distribution; e.g.,
assume that the tail of the distribution of
asymptotically
decays at an exponential rate. (This Markov chain corresponds
to an M/G/1 queue which is observed at service completion
times, with
being the number of new arrivals while serving
a customer.) Assuming that
, this chain has a
“downward drift,” is “stable,” and has a unique invariant
distribution . Furthermore, there exists some
sufﬁciently large. Let
that the cost function basically counts the number of customers
in queue. Let us introduce the basis functions
. Then, Assumptions 1 and 2 are satisﬁed.
Assumption 3-2) can be shown to be true for functions of the
by exploiting the downward drift
property (in this example, it is natural to simply let
Let us now discuss Assumption 3-1). The key is again
the speed of convergence of
. Starting from
large, the Markov chain has a negative drift
and requires
steps to enter (with high probability) the
vicinity of state zero , . Once the vicinity of state zero
is reached, it quickly reaches steady state. Thus, if we concentrate on
, the difference
is of the order of
steps and afterwards decays at a fast rate. This suggests
that Assumption 3-1) is satisﬁed by a function
that grows
polynomially with
Our discussion in the preceding example was far from
rigorous. Our objective was not so much to prove that our
assumptions are satisﬁed by speciﬁc examples, but rather
to demonstrate that their content is plausible. Furthermore,
while the M/G/1 queue is too simple an example, we expect that stable queueing networks that have a downward
drifting Lyapunov function should also generically satisfy our
assumptions.
IX. THE IMPORTANCE OF ON-LINE SAMPLING
In the introduction, we claimed that on-line sampling plays
an instrumental role in ensuring convergence of TD( ). In particular, when working with a simulation model, it is possible
to deﬁne variants of TD( ) that do not sample states with the
frequencies natural to the Markov chain and, as a result, do not
generally converge. Many papers, including , , ,
and , present such examples as counterexamples to TD( ).
In this section, we provide some insight into this issue by
exploring the behavior of a variant of TD(0). More generally,
variants of TD( ) can be deﬁned in a similar manner, and the
same issues arise in that context. We limit our discussion to
TD(0) for ease of exposition.
We consider a variant of TD(0) where states
are sampled
independently from a distribution
, and successor
are generated by sampling according to
. Each iteration of the algorithm takes the form
Let us refer to this algorithm as
-sampled TD(0). Note that
this algorithm is closely related to the original TD(0) algorithm
as deﬁned in Section II. In particular, if
is generated by the
Markov chain and
, we are back to the original
algorithm. It is easy to show, using a subset of the arguments
required to prove Theorem 1, that this algorithm converges
for all , and Assumptions 1, 2, and 4 are
satisﬁed. However, results can be very different when
arbitrary. This is captured by the following Theorem.
Theorem 3: Let
be a probability distribution over a
countable set
with at least two elements. Let the discount
be constrained to the open interval
the sequence
satisfy Assumption 4. Then, there exists a
stochastic matrix
, a transition cost function
, such that Assumptions 1 and 2 are satisﬁed, and
execution of the
algorithm leads to
for some unique vector
Proof: Without loss of generality, we will assume
throughout this proof that
We deﬁne a probability distribution
satisfying
for all . The fact that
ensures that such a probability distribution exists. We deﬁne
the transition probability matrix
with each row equal to
. In other words, we have
Finally, we deﬁne the transition cost function to be
and . Assumption 1 is trivially satisﬁed by our choice
, and the invariant distribution of the Markov
. Note that
, since no transition incurs any
matrix, deﬁned by a single scalar function
otherwise.
Note that, implicit from our deﬁnition of
is scalar, and
Assumption 2 is trivially satisﬁed. We let
In general, we can express
in terms of a recurrence
of the form
is the diagonal matrix with diagonal elements
TSITSIKLIS AND VAN ROY: ANALYSIS OF TEMPORAL-DIFFERENCE LEARNING
Specializing to our choice of parameters, the recurrence
For shorthand notation, let
be deﬁned by
, there exists some
It follows that
X. DIVERGENCE WITH A NONLINEAR APPROXIMATOR
Our analysis of temporal-difference learning up until now
has focused on linear function approximators. In many situations, it may be natural to employ classes of nonlinear function
approximators. Neural networks present one popular example.
One might hope that the analysis we have provided for the
linear case generalizes to nonlinear parameterizations, perhaps
under some simple regularity conditions. Unfortunately, this
does not seem to be the case. To illustrate potential difﬁculties,
we present an example for which TD(0) diverges due to the
structure of a nonlinear function approximator. (By divergence
here, we mean divergence of both the approximate cost-togo function and the parameters.) For the sake of brevity, we
limit our study to a characterization of steady-state dynamics,
rather than presenting a rigorous proof, which would require
arguments formally relating the steady-state dynamics to the
actual stochastic algorithm.
We consider a Markov chain with three states (
), all transition costs equal to zero, and a discount
. The cost-to-go function
is therefore
. Let the function approximator
be parameterized by a single scalar
. Let the form of
deﬁned by letting
be some nonzero vector satisfying
Example of divergence with a nonlinear function approximator. The
plot is of points in the plane fJ 2 <3je0J = 0g.
, requiring that
unique solution to the linear differential equation
identity matrix,
is a small positive
constant, and
is given by
Given our deﬁnition of
, it is easy to show that all functions
representable by
lie on the plane
Furthermore, the set of functions
forms a spiral
that diverges as
grows to inﬁnity (see Fig. 1).
We let the transition probability matrix of the Markov chain
Since all transition costs are zero, the TD(0) operator is given
. It turns out that there is
an acute angle
and a scalar
such that for any
is equal to the vector
and rotated by
degrees in the plane
. The points labeled
in Fig. 1 illustrate the nature of this transformation.
Before discussing divergence of TD(0), let us motivate the
underlying intuition by observing the qualitative behavior of
a simpler algorithm. In particular, suppose we generated a
sequence of approximations
, where each
IEEE TRANSACTIONS ON AUTOMATIC CONTROL, VOL. 42, NO. 5, MAY 1997
(Note that the steady-state distribution is uniform so that the
Euclidean norm is the appropriate one for this context.) In
Fig. 1, the point on the spiral closest to
is further from
the origin than
, even though
is closer to the origin
(the origin is located at the center of the circle in the
diagram). Therefore, if
Furthermore, since each application of
induces the same
degree of rotation and scaling, we might expect that each
subsequent iteration takes the approximation further from the
origin in a completely analogous way. Hence, the underlying
dynamics suggest that divergence is conceivable.
Let us now more concretely identify divergent behavior
in the steady-state dynamics of TD(0). The TD(0) algorithm
applies the update equation
is the state visited by the trajectory at time . Since
the steady-state distribution resulting from
is uniform, the
steady-state expectation of the update direction, within a factor
of three, is given by
This is the inner product of the vector
components
, which is the vector
As the step size becomes extremely small, we can think of
the deterministic version of the algorithm as an approximation
to a differential equation. Given the average direction of
motion of the parameter , the appropriate differential equation
for our example is
. Note that
which is easily veriﬁed to be positive deﬁnite. Hence, there
exists a positive constant
By a continuity argument, this inequality remains true (possibly with a smaller positive constant
is positive but
sufﬁciently small. The combination of this inequality and the
implies that both
diverge to inﬁnity.
XI. CONCLUSIONS
We have established the convergence of on-line temporaldifference learning with linear function approximators when
applied to irreducible aperiodic Markov chains. We note
that this result is new even for the case of lookup table
representations (i.e., when there is no function approximation),
but its scope is much greater. Furthermore, in addition to
covering the case where the underlying Markov chain is
ﬁnite, the result also applies to Markov chains over a general
(inﬁnite) state space, as long as certain technical conditions
are satisﬁed.
The key to our development was the introduction of the
and the property
. Furthermore, our
development indicates that the progress of the algorithm can
be monitored in two different ways: 1) we can keep track of
the magnitude of the approximation error
; the natural
norm for doing so is
, or 2) we can keep track of the
parameter error
; the natural norm here is the Euclidean
norm, as made clear by our convergence proof.
To reinforce the central ideas in the proof, let us revisit
the TD(0) method, for the case where the costs per stage
are identically zero. In this case,
deterministic counterpart of the algorithm, as introduced in
Section III, takes the form
For any vector
This shows that the matrix
is negative deﬁnite,
is also negative deﬁnite and convergence
of this deterministic iteration follows.
Besides convergence, we have also provided bounds on the
distance of the limiting function
from the true cost-togo function
. These bounds involve the expression
, which is natural because no approximation could have
error smaller than this expression (when the error is measured
in terms of
). What is interesting is the factor of
This expression is one when
. For every
is larger than one, and the bound actually deteriorates as
decreases. The worst bound, namely
is obtained when
. Although this is only a bound, it
strongly suggests that higher values of
are likely to produce
more accurate approximations of
. This is consistent with
the examples that have been constructed by Bertsekas .
TSITSIKLIS AND VAN ROY: ANALYSIS OF TEMPORAL-DIFFERENCE LEARNING
The sensitivity of the error bound to
raises the question
of whether or not it ever makes sense to set
to values less
than one. Experimental results , , and suggest that
to values less than one can often lead to signiﬁcant
gains in the rate of convergence. Such acceleration may be
critical when computation time and/or data (in the event
that the trajectories are generated by a physical system) are
limited. A full understanding of how
inﬂuences the rate
of convergence is yet to be found. Furthermore, it might
be desirable to tune
as the algorithm progresses, possibly
initially starting with
and approaching
the opposite has also been advocated). These are interesting
directions for future research.
In many applications of temporal-difference learning, one
deals with a controlled Markov chain and at each stage
a decision is “greedily” chosen, by minimizing the righthand side of Bellman’s equation and using the available
approximation
in place of
. Our analysis does not apply
to such cases involving changing policies. Of course, if the
policy eventually settles into a limiting policy, we are back to
the case studied in this paper and convergence is obtained.
However, there exist examples for which the policy does
not converge . It remains an open problem to analyze
the limiting behavior of the parameters
and the resulting
approximations
for the case where the policy does not
On the technical side, we mention a few straightforward
extensions of our results. First, the linear independence of the
basis functions
is not essential. In the linearly dependent
case, some components of
become linear combinations of the other components and can be simply eliminated,
which takes us back to the linearly independent case. A second
extension is to allow the cost per stage
noisy, as opposed to being a deterministic function of
. In particular, we can replace the Markov process
that was constructed for the purposes of
our analysis with a process
is the cost associated with the transition from
as long as the distribution of the noise only depends on the
current state and its moments are such that the assumptions of
Theorem 2 are still satisﬁed, our proof can easily be modiﬁed
to accommodate this situation. Finally, the assumption that
the Markov chain was aperiodic can be alleviated. No part of
our convergence proof truly required this assumption—it was
introduced merely to simplify the exposition.
Our results in Section IX have elucidated the importance
of sampling states according to the steady-state distribution of
the Markov chain under consideration. In particular, variants
of TD( ) that sample states otherwise can lead to divergence
when function approximators are employed. As a parting
note, we point out that a related issue arises when one
“plays” with the evolution equation for the eligibility vector
. (For example Singh and Sutton have suggested an
alternative evolution equation for
known as the “replace
trace.”) A very general class of such mechanisms can be
shown to lead to convergent algorithms for the case of lookup
table representations . However, different mechanisms for
adjusting the coefﬁcients
lead to a change in the steadystate average value of
, affect the matrix
negative deﬁniteness property can be easily lost.
Finally, the example of Section X identiﬁes the possibility of divergence when TD( ) is used in conjunction with
nonlinear function approximators. However, the example is
somewhat contrived, and it is unclear whether divergence can
occur with special classes of function approximators, such
as neural networks. This presents an interesting question for
future research.
ACKNOWLEDGMENT
The authors would like to thank R. S. Sutton for starting
them on the path that led to this work by pointing out that the
counterexample in would no longer be a counterexample
if on-line state sampling was used. They also thank him for
suggesting an algebraic simpliﬁcation to the original expression for the error bound in Theorem 1, which resulted in its
current form. The authors would like to thank the reviewers
for their feedback, especially the one who provided them with
four pages of detailed corrections and useful comments.