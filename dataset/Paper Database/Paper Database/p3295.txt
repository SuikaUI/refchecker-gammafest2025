Published as a conference paper at ICLR 2017
DEEP PREDICTIVE CODING NETWORKS FOR VIDEO
PREDICTION AND UNSUPERVISED LEARNING
William Lotter, Gabriel Kreiman & David Cox
Harvard University
Cambridge, MA 02215, USA
{lotter,davidcox}@fas.harvard.edu
 
While great strides have been made in using deep learning algorithms to solve
supervised learning tasks, the problem of unsupervised learning — leveraging unlabeled examples to learn about the structure of a domain — remains a difﬁcult
unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the
visual world. We describe a predictive neural network (“PredNet”) architecture
that is inspired by the concept of “predictive coding” from the neuroscience literature. These networks learn to predict future frames in a video sequence, with
each layer in the network making local predictions and only forwarding deviations
from those predictions to subsequent network layers. We show that these networks
are able to robustly learn to predict the movement of synthetic (rendered) objects,
and that in doing so, the networks learn internal representations that are useful
for decoding latent object parameters (e.g. pose) that support object recognition
with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects
of both egocentric movement and the movement of objects in the visual scene,
and the representation learned in this setting is useful for estimating the steering angle. Altogether, these results suggest that prediction represents a powerful
framework for unsupervised learning, allowing for implicit learning of object and
scene structure.
INTRODUCTION
Many of the most successful current deep learning architectures for vision rely on supervised learning from large sets of labeled training images. While the performance of these networks is undoubtedly impressive, reliance on such large numbers of training examples limits the utility of deep
learning in many domains where such datasets are not available. Furthermore, the need for large
numbers of labeled examples stands at odds with human visual learning, where one or a few views
of an object is often all that is needed to enable robust recognition of that object across a wide range
of different views, lightings and contexts. The development of a representation that facilitates such
abilities, especially in an unsupervised way, is a largely unsolved problem.
In addition, while computer vision models are typically trained using static images, in the real world,
visual objects are rarely experienced as disjoint snapshots. Instead, the visual world is alive with
movement, driven both by self-motion of the viewer and the movement of objects within the scene.
Many have suggested that temporal experience with objects as they move and undergo transformations can serve as an important signal for learning about the structure of objects . For instance, Wiskott and Sejnowski
proposed “slow feature analysis” as a framework for exploiting temporal structure in video streams
 . Their approach attempts to build feature representations that extract
Code and video examples can be found at: 
 
Published as a conference paper at ICLR 2017
slowly-varying parameters, such as object identity, from parameters that produce fast changes in the
image, such as movement of the object. While approaches that rely on temporal coherence have
arguably not yet yielded representations as powerful as those learned by supervised methods, they
nonetheless point to the potential of learning useful representations from video .
Here, we explore another potential principle for exploiting video for unsupervised learning: prediction of future image frames . A key insight here is that in order to be able to predict how the visual world
will change over time, an agent must have at least some implicit model of object structure and the
possible transformations objects can undergo. To this end, we have designed a neural network architecture, which we informally call a “PredNet,” that attempts to continually predict the appearance
of future video frames, using a deep, recurrent convolutional network with both bottom-up and topdown connections. Our work here builds on previous work in next-frame video prediction , but we take particular inspiration from the concept of “predictive coding”
from the neuroscience literature . Predictive
coding posits that the brain is continually making predictions of incoming sensory stimuli . Top-down (and perhaps lateral) connections convey these predictions,
which are compared against actual observations to generate an error signal. The error signal is then
propagated back up the hierarchy, eventually leading to an update of the predictions.
We demonstrate the effectiveness of our model for both synthetic sequences, where we have access
to the underlying generative model and can investigate what the model learns, as well as natural
videos. Consistent with the idea that prediction requires knowledge of object structure, we ﬁnd
that these networks successfully learn internal representations that are well-suited to subsequent
recognition and decoding of latent object parameters (e.g. identity, view, rotation speed, etc.). We
also ﬁnd that our architecture can scale effectively to natural image sequences, by training using
car-mounted camera videos. The network is able to successfully learn to predict both the movement
of the camera and the movement of objects in the camera’s view. Again supporting the notion
of prediction as an unsupervised learning rule, the model’s learned representation in this setting
supports decoding of the current steering angle.
Representation
Prediction
Figure 1: Predictive Coding Network (PredNet). Left: Illustration of information ﬂow within two
layers. Each layer consists of representation neurons (Rl), which output a layer-speciﬁc prediction at
each time step ( ˆAl), which is compared against a target (Al) to produce an error term
(El), which is then propagated laterally and vertically in the network. Right: Module operations for
case of video sequences.
Published as a conference paper at ICLR 2017
THE PREDNET MODEL
The PredNet architecture is diagrammed in Figure 1. The network consists of a series of repeating
stacked modules that attempt to make local predictions of the input to the module, which is then
subtracted from the actual input and passed along to the next layer. Brieﬂy, each module of the
network consists of four basic parts: an input convolutional layer (Al), a recurrent representation
layer (Rl), a prediction layer ( ˆAl), and an error representation (El). The representation layer, Rl, is
a recurrent convolutional network that generates a prediction, ˆAl, of what the layer input, Al, will
be on the next frame. The network takes the difference between Al and ˆAl and outputs an error
representation, El, which is split into separate rectiﬁed positive and negative error populations. The
error, El, is then passed forward through a convolutional layer to become the input to the next layer
(Al+1). The recurrent prediction layer Rl receives a copy of the error signal El, along with top-down
input from the representation layer of the next level of the network (Rl+1). The organization of the
network is such that on the ﬁrst time step of operation, the “right” side of the network (Al’s and El’s)
is equivalent to a standard deep convolutional network. Meanwhile, the “left” side of the network
(the Rl’s) is equivalent to a generative deconvolutional network with local recurrence at each stage.
The architecture described here is inspired by that originally proposed by , but
is formulated in a modern deep learning framework and trained end-to-end using gradient descent,
with a loss function implicitly embedded in the network as the ﬁring rates of the error neurons. Our
work also shares motivation with the Deep Predictive Coding Networks of Chalasani & Principe
 ; however, their framework is based upon sparse coding and a linear dynamical system with
greedy layer-wise training, whereas ours is rooted in convolutional and recurrent neural networks
trained with backprop.
While the architecture is general with respect to the kinds of data it models, here we focus on image
sequence (video) data. Consider a sequence of images, xt. The target for the lowest layer is set
to the the actual sequence itself, i.e. At
0 = xt ∀t. The targets for higher layers, At
l for l > 0, are
computed by a convolution over the error units from the layer below, Et
l−1, followed by rectiﬁed
linear unit (ReLU) activation and max-pooling. For the representation neurons, we speciﬁcally
use convolutional LSTM units . In our setting,
l hidden state is updated according to Rt−1
, as well as Rt
l+1, which is ﬁrst spatially
upsampled (nearest-neighbor), due to the pooling present in the feedforward path. The predictions,
l are made through a convolution of the Rt
l stack followed by a ReLU non-linearity. For the
lowest layer, ˆAt
l is also passed through a saturating non-linearity set at the maximum pixel value:
SatLU(x; pmax) := min(pmax, x). Finally, the error response, Et
l , is calculated from the difference
between ˆAt
l and is split into ReLU-activated positive and negative prediction errors, which
are concatenated along the feature dimension. As discussed in , although not
explicit in their model, the separate error populations are analogous to the existence of on-center,
off-surround and off-center, on-surround neurons early in the visual system.
The full set of update rules are listed in Equations (1) to (4). The model is trained to minimize
the weighted sum of the activity of the error units. Explicitly, the training loss is formalized in
Equation 5 with weighting factors by time, λt, and layer, λl, and where nl is the number of units in
the lth layer. With error units consisting of subtraction followed by ReLU activation, the loss at each
layer is equivalent to an L1 error. Although not explored here, other error unit implementations,
potentially even probabilistic or adversarial , could also be used.
MAXPOOL(RELU(CONV(Et
l = RELU(CONV(Rt
l = [RELU(At
l); RELU ; RELU( ˆAt
if l < L then
l+1 = MAXPOOL(CONV(El
The order in which each unit in the model is updated must also be speciﬁed, and our implementation is described in Algorithm 1. Updating of states occurs through two passes: a top-down pass
where the Rt
l states are computed, and then a forward pass to calculate the predictions, errors, and
higher level targets. A last detail of note is that Rl and El are initialized to zero, which, due to the
convolutional nature of the network, means that the initial prediction is spatially uniform.
EXPERIMENTS
RENDERED IMAGE SEQUENCES
To gain an understanding of the representations learned in the proposed framework, we ﬁrst trained
PredNet models using synthetic images, for which we have access to the underlying generative
stimulus model and all latent parameters. We created sequences of rendered faces rotating with two
degrees of freedom, along the “pan” (out-of-plane) and “roll” (in-plane) axes. The faces start at a
random orientation and rotate at a random constant velocity for a total of 10 frames. A different face
was sampled for each sequence. The images were processed to be grayscale, with values normalized
between 0 and 1, and 64x64 pixels in size. We used 16K sequences for training and 800 for both
validation and testing.
Predictions generated by a PredNet model are shown in Figure 2. The model is able to accumulate
information over time to make accurate predictions of future frames. Since the representation neurons are initialized to zero, the prediction at the ﬁrst time step is uniform. On the second time step,
with no motion information yet, the prediction is a blurry reconstruction of the ﬁrst time step. After
further iterations, the model adapts to the underlying dynamics to generate predictions that closely
match the incoming frame.
For choosing the hyperparameters of the model, we performed a random search and chose the model
that had the lowest L1 error in frame prediction averaged over time steps 2-10 on a validation set.
Given this selection criteria, the best performing models tended to have a loss solely concentrated at
the lowest layer (i.e. λ0 = 1, λl>0 = 0), which is the case for the model shown. Using an equal loss
at each layer considerably degraded predictions, but enforcing a moderate loss on upper layers that
was one magnitude smaller than the lowest layer (i.e. λ0 = 1, λl>0 = 0.1) led to only slightly worse
predictions, as illustrated in Figure 9 in the Appendix. In all cases, the time loss weight, λt, was set to
zero for the ﬁrst time step and then one for all time steps after. As for the remaining hyperparameters,
the model shown has 5 layers with 3x3 ﬁlter sizes for all convolutions, max-pooling of stride 2, and
number of channels per layer, for both Al and Rl units, of (1, 32, 64, 128, 256). Model weights were
optimized using the Adam algorithm .
Published as a conference paper at ICLR 2017
Figure 2: PredNet next-frame predictions for sequences of rendered faces rotating with two degrees
of freedom. Faces shown were not seen during training.
Table 1: Evaluation of next-frame predictions
on Rotating Faces Dataset (test set).
PredNet L0
PredNet Lall
CNN-LSTM Enc.-Dec.
Copy Last Frame
Quantitative evaluation of generative models is a
difﬁcult, unsolved problem , but
here we report prediction error in terms of meansquared error (MSE) and the Structural Similarity
Index Measure (SSIM) . SSIM
is designed to be more correlated with perceptual
judgments, and ranges from −1 and 1, with a larger
score indicating greater similarity. We compare the
PredNet to the trivial solution of copying the last
frame, as well as a control model that shares the overall architecture and training scheme of the
PredNet, but that sends forward the layer-wise activations (Al) rather than the errors (El). This
model thus takes the form of a more traditional encoder-decoder pair, with a CNN encoder that has
lateral skip connections to a convolutional LSTM decoder. The performance of all models on the
rotating faces dataset is summarized in Table 1, where the scores were calculated as an average over
all predictions after the ﬁrst frame. We report results for the PredNet model trained with loss only
on the lowest layer, denoted as PredNet L0, as well as the model trained with an 0.1 weight on
upper layers, denoted as PredNet Lall. Both PredNet models outperformed the baselines on both
measures, with the L0 model slightly outperforming Lall, as expected for evaluating the pixel-level
predictions.
Synthetic sequences were chosen as the initial training set in order to better understand what is
learned in different layers of the model, speciﬁcally with respect to the underlying generative model
 . The rotating faces were generated using the FaceGen software package (Singular Inversions, Inc.), which internally generates 3D face meshes by a principal component analysis
in “face space”, derived from a corpus of 3D face scans. Thus, the latent parameters of the image
sequences used here consist of the initial pan and roll angles, the pan and roll velocities, and the principal component (PC) values, which control the “identity” of the face. To understand the information
contained in the trained models, we decoded the latent parameters from the representation neurons
(Rl) in different layers, using a ridge regression. The Rl states were taken at the earliest possible
informative time steps, which, in the our notation, are the second and third steps, respectively, for
the static and dynamic parameters. The regression was trained using 4K sequences with 500 for
validation and 1K for testing. For a baseline comparison of the information implicitly embedded
in the network architecture, we compare to the decoding accuracies of an untrained network with
random initial weights. Note that in this randomly initialized case, we still expect above-chance decoding performance, given past theoretical and empirical work with random networks .
Published as a conference paper at ICLR 2017
Latent variable decoding accuracies of the pan and roll velocities, pan initial angle, and ﬁrst PC are
shown in the left panel of Figure 3. There are several interesting patterns. First, the trained models
learn a representation that generally permits a better linear decoding of the underlying latent factors
than the randomly initialized model, with the most striking difference in terms of the the pan rotation
speed (αpan). Second, the most notable difference between the Lall and L0 versions occurs with
the ﬁrst principle component, where the model trained with loss on all layers has a higher decoding
accuracy than the model trained with loss only on the lowest layer.
Figure 3: Information contained in PredNet representation for rotating faces sequences. Left: Decoding of latent variables using a ridge regression (αpan: pan (out-of-frame) angular velocity, θpan:
pan angle, PC-1: ﬁrst principal component of face, αroll: roll (in-frame) angular velocity). Right:
Orientation-invariant classiﬁcation of static faces.
The latent variable decoding analysis suggests that the model learns a representation that may generalize well to other tasks for which it was not explicitly trained. To investigate this further, we
assessed the models in a classiﬁcation task from single, static images. We created a dataset of 25
previously unseen FaceGen faces at 7 pan angles, equally spaced between [−π
2 ], and 8 roll angles,
equally spaced between [0, 2π). There were therefore 7 · 8 = 56 orientations per identity, which
were tested in a cross-validated fashion. A linear SVM to decode face identity was ﬁt on a model’s
representation of a random subset of orientations and then tested on the remaining angles. For each
size of the SVM training set, ranging from 1-40 orientations per face, 50 different random splits
were generated, with results averaged over the splits.
For the static face classiﬁcation task, we compare the PredNets to a standard autoencoder and a
variant of the Ladder Network . Both models were constructed
to have the same number of layers and channel sizes as the PredNets, as well as a similar alternating convolution/max-pooling, then upsampling/convolution scheme. As both networks are autoencoders, they were trained with a reconstruction loss, with a dataset consisting of all of the individual
frames from the sequences used to train the PredNets. For the Ladder Network, which is a denoising autoencoder with lateral skip connections, one must also choose a noise parameter, as well as
the relative weights of each layer in the total cost. We tested noise levels ranging from 0 to 0.5
in increments of 0.1, with loss weights either evenly distributed across layers, solely concentrated
at the pixel layer, or 1 at the bottom layer and 0.1 at upper layers (analogous to the PredNet Lall
model). Shown is the model that performed best for classiﬁcation, which consisted of 0.4 noise and
only pixel weighting. Lastly, as in our architecture, the Ladder Network has lateral and top-down
streams that are combined by a combinator function. Inspired by , where a
learnable MLP improved results, and to be consistent in comparing to the PredNet, we used a purely
convolutional combinator. Given the distributed representation in both networks, we decoded from
a concatenation of the feature representations at all layers, except the pixel layer. For the PredNets,
the representation units were used and features were extracted after processing one input frame.
Published as a conference paper at ICLR 2017
Face classiﬁcation accuracies using the representations learned by the L0 and Lall PredNets, a standard autoencoder, and a Ladder Network variant are shown in the right panel of Figure 3. Both
PredNets compare favorably to the other models at all sizes of the training set, suggesting they learn
a representation that is relatively tolerant to object transformations. Similar to the decoding accuracy of the ﬁrst principle component, the PredNet Lall model actually outperformed the L0 variant.
Altogether, these results suggest that predictive training with the PredNet can be a viable alternative
to other models trained with a more traditional reconstructive or denoising loss, and that the relative
layer loss weightings (λl’s) may be important for the particular task at hand.
NATURAL IMAGE SEQUENCES
We next sought to test the PredNet architecture on complex, real-world sequences. As a testbed, we
chose car-mounted camera videos, since these videos span across a wide range of settings and are
characterized by rich temporal dynamics, including both self-motion of the vehicle and the motion
of other objects in the scene . Models were trained using the raw videos from
the KITTI dataset , which were captured by a roof-mounted camera on a car
driving around an urban environment in Germany. Sequences of 10 frames were sampled from the
“City”, “Residential”, and “Road” categories, with 57 recording sessions used for training and 4
used for validation. Frames were center-cropped and downsampled to 128x160 pixels. In total, the
training set consisted of roughly 41K frames.
A random hyperparameter search, with model selection based on the validation set, resulted in a 4
layer model with 3x3 convolutions and layer channel sizes of (3, 48, 96, 192). Models were again
trained with Adam using a loss either solely computed on the lowest layer
(L0) or with a weight of 1 on the lowest layer and 0.1 on the upper layers (Lall). Adam parameters
were initially set to their default values (α = 0.001, β1 = 0.9, β2 = 0.999) with the learning rate, α,
decreasing by a factor of 10 halfway through training. To assess that the network had indeed learned
a robust representation, we tested on the CalTech Pedestrian dataset , which
consists of videos from a dashboard-mounted camera on a vehicle driving around Los Angeles.
Testing sequences were made to match the frame rate of the KITTI dataset and again cropped to
128x160 pixels. Quantitative evaluation was performed on the entire CalTech test partition, split
into sequences of 10 frames.
Sample PredNet predictions (for the L0 model) on the CalTech Pedestrian dataset are shown in
Figure 4, and example videos can be found at The
model is able to make fairly accurate predictions in a wide range of scenarios. In the top sequence
of Fig. 4, a car is passing in the opposite direction, and the model, while not perfect, is able to predict
its trajectory, as well as ﬁll in the ground it leaves behind. Similarly in Sequence 3, the model is
able to predict the motion of a vehicle completing a left turn. Sequences 2 and 5 illustrate that the
PredNet can judge its own movement, as it predicts the appearance of shadows and a stationary
vehicle as they approach. The model makes reasonable predictions even in difﬁcult scenarios, such
as when the camera-mounted vehicle is turning. In Sequence 4, the model predicts the position of a
tree, as the vehicle turns onto a road. The turning sequences also further illustrate the model’s ability
to “ﬁll-in”, as it is able to extrapolate sky and tree textures as unseen regions come into view. As an
additional control, we show a sequence at the bottom of Fig. 4, where the input has been temporally
scrambled. In this case, the model generates blurry frames, which mostly just resemble the previous
frame. Finally, although the PredNet shown here was trained to predict one frame ahead, it is also
possible to predict multiple frames into the future, by feeding back predictions as the inputs and
recursively iterating. We explore this in Appendix 5.3.
Table 2: Evaluation of Next-Frame Predictions on
CalTech Pedestrian Dataset.
PredNet L0
3.13 × 10−3
PredNet Lall
3.33 × 10−3
CNN-LSTM Enc.-Dec.
3.67 × 10−3
Copy Last Frame
7.95 × 10−3
Quantitatively, the PredNet models again
outperformed the CNN-LSTM Encoder-
Decoder. To ensure that the difference in
performance was not simply because of the
choice of hyperparameters, we trained models with four other sets of hyperparameters,
which were sampled from the initial random search over the number of layers, ﬁlter sizes, and number of ﬁlters per layer. For each of the four additional sets, the PredNet L0 had
the best performance, with an average error reduction of 14.7% and 14.9% for MSE and SSIM,
Published as a conference paper at ICLR 2017
Figure 4: PredNet predictions for car-cam videos. The ﬁrst rows contain ground truth and the second
rows contain predictions. The sequence below the red line was temporally scrambled. The model
was trained on the KITTI dataset and sequences shown are from the CalTech Pedestrian dataset.
respectively, compared to the CNN-LSTM Encoder-Decoder. More details, as well as a thorough
investigation of systematically simpliﬁed models on the continuum between the PredNet and the
CNN-LSTM Encoder-Decoder can be found in Appendix 5.1. Brieﬂy, the elementwise subtraction
operation in the PredNet seems to be beneﬁcial, and the nonlinearity of positive/negative splitting
also adds modest improvements. Finally, while these experiments measure the beneﬁts of each component of our model, we also directly compare against recent work in a similar car-cam setting, by
reporting results on a 64x64 pixel, grayscale car-cam dataset released by Brabandere et al. .
Our PredNet model outperforms the model by Brabandere et al. by 29%. Details can be
found in Appendix 5.2. Also in Appendix 5.2, we present results for the Human3.6M dataset, as reported by Finn et al. . Without re-optimizing hyperparameters, our
Published as a conference paper at ICLR 2017
model underperforms the concurrently developed DNA model by Finn et al. , but outperforms
the model by Mathieu et al. .
To test the implicit encoding of latent parameters in the car-cam setting, we used the internal representation in the PredNet to estimate the car’s steering angle . We used a dataset released by Comma.ai consisting of 11 videos totaling about 7 hours of mostly highway driving. We ﬁrst trained networks for next-frame prediction
and then ﬁt a linear fully-connected layer on the learned representation to estimate the steering angle, using a MSE loss. We again concatenate the Rl representation at all layers, but ﬁrst spatially
average pool lower layers to match the spatial size of the upper layer, in order to reduce dimensionality. Steering angle estimation results, using the representation on the 10th time step, are shown
in Figure 5. Given just 1K labeled training examples, a simple linear readout on the PredNet L0
representation explains 74% of the variance in the steering angle and outperforms the CNN-LSTM
Enc.-Dec. by 35%. With 25K labeled training examples, the PredNet L0 has a MSE (in degrees2)
of 2.14. As a point of reference, a CNN model designed to predict the steering angle , albeit from a single frame instead of multiple frames, achieve a MSE of ~4 when
trained end-to-end using 396K labeled training examples. Details of this analysis can be found in
Appendix 8. Interestingly, in this task, the PredNet Lall model actually underperformed the L0
model and slightly underperformed the CNN-LSTM Enc.-Dec, again suggesting that the λl parameter can affect the representation learned, and different values may be preferable in different end
tasks. Nonetheless, the readout from the Lall model still explained a substantial proportion of the
steering angle variance and strongly outperformed the random initial weights. Overall, this analysis again demonstrates that a representation learned through prediction, and particularly with the
PredNet model with appropriate hyperparameters, can contain useful information about underlying
latent parameters.
Figure 5: Steering angle estimation accuracy on the Comma.ai dataset . Left:
Example steering angle curve with model estimations for a segment in the test set. Decoding was
performed using a fully-connected readout on the PredNet representation trained with 25K labeled
training examples. PredNet representation was trained for next-frame prediction on Comma.ai training set. Right: Mean-squared error of steering angle estimation.
DISCUSSION
Above, we have demonstrated a predictive coding inspired architecture that is able to predict future
frames in both synthetic and natural image sequences. Importantly, we have shown that learning to
predict how an object or scene will move in a future frame confers advantages in decoding latent
parameters (such as viewing angle) that give rise to an object’s appearance, and can improve recognition performance. More generally, we argue that prediction can serve as a powerful unsupervised
learning signal, since accurately predicting future frames requires at least an implicit model of the
objects that make up the scene and how they are allowed to move. Developing a deeper understanding of the nature of the representations learned by the networks, and extending the architecture, by,
for instance, allowing sampling, are important future directions.
Published as a conference paper at ICLR 2017
ACKNOWLEDGMENTS
We would like to thank Rasmus Berg Palm for fruitful discussions and early brainstorming. We
would also like to thank the developers of Keras . This work was supported by IARPA
(contract D16PC00002), the National Science Foundation (NSF IIS 1409097), and the Center for
Brains, Minds and Machines (CBMM, NSF STC award CCF-1231216).