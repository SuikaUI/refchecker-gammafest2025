RON: Reverse Connection with Objectness Prior Networks
for Object Detection
Tao Kong1∗
Fuchun Sun1
Anbang Yao2
Huaping Liu1
Ming Lu3 Yurong Chen2
1State Key Lab. of Intelligent Technology and Systems, Tsinghua National Laboratory for
Information Science and Technology (TNList), Department of Computer Science and Technology,
Tsinghua University 2Intel Labs China 3Department of Electronic Engineering, Tsinghua University
1{kt14@mails,fcsun@,hpliu@}.tsinghua.edu.cn
2{anbang.yao, yurong.chen}@.intel.com
 
We present RON, an efﬁcient and effective framework for
generic object detection. Our motivation is to smartly associate the best of the region-based (e.g., Faster R-CNN)
and region-free (e.g., SSD) methodologies. Under fully convolutional architecture, RON mainly focuses on two fundamental problems: (a) multi-scale object localization and (b)
negative sample mining. To address (a), we design the reverse connection, which enables the network to detect objects on multi-levels of CNNs. To deal with (b), we propose the objectness prior to signiﬁcantly reduce the searching space of objects. We optimize the reverse connection,
objectness prior and object detector jointly by a multi-task
loss function, thus RON can directly predict ﬁnal detection
results from all locations of various feature maps.
Extensive experiments on the challenging PASCAL VOC
2007, PASCAL VOC 2012 and MS COCO benchmarks
demonstrate the competitive performance of RON. Speciﬁcally, with VGG-16 and low resolution 384×384 input size,
the network gets 81.3% mAP on PASCAL VOC 2007, 80.7%
mAP on PASCAL VOC 2012 datasets. Its superiority increases when datasets become larger and more difﬁcult,
as demonstrated by the results on the MS COCO dataset.
With 1.5G GPU memory at test phase, the speed of the network is 15 FPS, 3× faster than the Faster R-CNN counterpart. Code will be available at 
taokong/RON.
1. Introduction
We are witnessing signiﬁcant advances in object detection area, mainly thanks to the deep networks. Current top
deep-networks-based object detection frameworks could be
*This work was done when Tao Kong was an intern at Intel Labs China
supervised by Anbang Yao who is responsible for correspondence.
Figure 1. Objectness prior generated from a speciﬁc image. In this
example, sofa is responded at scales (a) and (b), the brown dog
is responded at scale (c) and the white spotted dog is responded
at scale (d). The network will generate detection results with the
guidance of objectness prior .
grouped into two main streams: the region-based methods
 and the region-free methods .
The region-based methods divide the object detection
task into two sub-problems:
At the ﬁrst stage, a dedicated region proposal generation network is grafted on
deep convolutional neural networks (CNNs) which could
generate high quality candidate boxes.
Then at the second stage, a region-wise subnetwork is designed to classify and reﬁne these candidate boxes.
Using very deep
CNNs , the Fast R-CNN pipeline has recently shown high accuracy on mainstream object detection
benchmarks . The region proposal stage could
reject most of the background samples, thus the searching
space for object detection is largely reduced . Multistage training process is usually developed for joint optimization of region proposal generation and post detection
(e.g., ). In Fast R-CNN , the region-wise
subnetwork repeatedly evaluates thousands of region proposals to generate detection scores.
Under Fast R-CNN
pipeline, Faster R-CNN shares full-image convolutional
features with the detection network to enable nearly costfree region proposals. Recently, R-FCN tries to make
the unshared per-RoI computation of Faster R-CNN to be
sharable by adding position-sensitive score maps. Never-
 
theless, R-FCN still needs region proposals generated from
region proposal networks . To ensure detection accuracy, all methods resize the image to a large enough size
(usually with the shortest side of 600 pixels). It is somewhat
resource/time consuming when feeding the image into deep
networks, both in training and inference time. For example,
predicting with Faster R-CNN usually takes 0.2 s per image
using about 5GB GPU memory for VGG-16 networks .
Another solution family is the region-free methods
 . These methods treat object detection as a single
shot problem, straight from image pixels to bounding box
coordinates by fully convolutional networks (FCNs). The
main advantage of these detectors is high efﬁciency. Originated from YOLO , SSD tries to deal object detection problem with multiple layers of deep CNNs. With low
resolution input, the SSD detector could get state-of-the-art
detection results. However, the detection accuracy of these
methods still has room for improvement: (a) Without region
proposals, the detectors have to suppress all of the negative
candidate boxes only at the detection module. It will increase the difﬁculties on training the detection module. (b)
YOLO detects objects with the top-most CNN layer, without deeply exploring the detection capacities of different
layers. SSD tries to improve the detection performance by
adding former layers’ results. However, SSD still struggles
with small instances, mainly because of the limited information of middle layers. These two main bottlenecks affect
the detection accuracy of the methods.
Driven by the success of the two solution families, a
critical question arises: is it possible to develop an elegant framework which can smartly associate the best of
both methodologies and eliminate their major demerits?
We answer this question by trying to bridge the gap between the region-based and region-free methodologies. To
achieve this goal, we focus on two fundamental problems:
(a) Multi-scale object localization. Objects of various scales
could appear at any position of an image, so tens of thousands of regions with different positions/scales/aspect ratios should be considered. Prior works show that
multi-scale representation will signiﬁcantly improve object
detection of various scales. However, these methods always
detect various scales of objects at one layer of a network
 . With the proposed reverse connection, objects
are detected on their corresponding network scales, which
is more elegant and easier to optimize. (b) Negative space
mining. The ratio between object and non-object samples
is seriously imbalanced. So an object detector should have
effective negative mining strategies . In order to reduce
the searching space of the objects, we create an objectness
prior (Figure 1) on convolutional feature maps and jointly
optimize it with the detector at training phase.
As a result, we propose RON (Reverse connection
with Objectness prior Networks) object detection framework, which could associate the merits of region-based and
region-free approaches. Furthermore, recent complementary advances such as hard example mining , bounding
box regression and multi-layer representation 
could be naturally employed.
Contributions. We make the following contributions:
1. We propose RON, a fully convolutional framework for
end-to-end object detection. Firstly, the reverse connection assists the former layers of CNNs with more
semantic information.
Second, the objectness prior
gives an explicit guide to the searching of objects. Finally, the multi-task loss function enables us to optimize the whole network end-to-end on detection performance.
2. In order to achieve high detection accuracy, effective
training strategies like negative example mining and
data augmentation have been employed. With low resolution 384×384 input size, RON achieves state-ofthe-art results on PASCAL VOC 2007, with a 81.3%
mAP, VOC 2012, with a 80.7% mAP, and MS COCO,
with a 27.4% mAP.
3. RON is time and resource efﬁcient. With 1.5G GPU
memory, the total feed-forward speed is 15 FPS, 3×
faster than the seminal Faster R-CNN. Moreover, we
conduct extensive design choices like the layers combination, with/without objectness prior, and other variations.
2. Related Work
fundamental
heavilyresearched task in computer vision area. It aims to localize and recognize every object instance with a bounding
box . Before the success of deep CNNs , the
widely used detection systems are based on the combination of independent components (HOG , SIFT et
al.). The DPM and its variants have been the dominant methods for years. These methods use object component descriptors as features and sweep through the entire
image to ﬁnd regions with a class-speciﬁc maximum response. With the great success of the deep learning on large
scale object recognition, several works based on CNNs have
been proposed . R-CNN and its variants usually combine region proposals (generated from Selective
Search , Edgeboxes , MCG , et al.) and ConvNet
based post-classiﬁcation. These methods have brought dramatic improvement on detection accuracy . After the
original R-CNN, researches are improving it in a variety of
ways. The SPP-Net and Fast R-CNN speed up
the R-CNN approach with RoI-Pooling (Spatial-Pyramid-
Pooling) that allows the classiﬁcation layers to reuse features computed over CNN feature maps. Under Fast Rconv4
connection
objectness
objectness
objectness
objectness
connection
connection
Figure 2. RON object detection overview. Given an input image,
the network ﬁrstly computes features of the backbone network.
Then at each detection scale: (a) adds reverse connection; (b)
generates objectness prior; (c) detects object on its corresponding
CNN scales and locations. Finally, all detection results are fused
and selected with non-maximum suppression.
CNN pipeline, several works try to improve the detection
speed and accuracy, with more effective region proposals
 , multi-layer fusion , context information 
and more effective training strategy . R-FCN tries
to reduce the computation time with position-sensitive score
maps under ResNet architecture . Thanks to the FCNs
 which could give position-wise information of input
images, several works are trying to solve the object detection problem by FCNs.
The methods skip the region
proposal generation step and predict bounding boxes and
detection conﬁdences of multiple categories directly .
YOLO uses the top most CNN feature maps to predict both conﬁdences and locations for multiple categories.
Originated from YOLO, SSD tries to predict detection
results at multiple CNN layers. With carefully designed
training strategies, SSD could get competitive detection results. The main advantage of these methods is high timeefﬁciency. For example, the feed-forward speed of YOLO
is 45 FPS, 9× faster than Faster R-CNN.
3. Network Architecture
This section describes RON object detection framework
(Figure 2). We ﬁrst introduce the reverse connection on traditional CNNs in Section 3.1, such that different network
scales have effective detection capabilities. Then in Section
3.2, we explain how to generate candidate boxes on different network scales. Next, we present the objectness prior
to guide the search of objects in Section 3.3 and Secction
3.4. Finally, we combine the objectness prior and object detection into a uniﬁed network for joint training and testing
(Section 3.5).
Network preparation We use VGG-16 as the test
rf-map n+1
Figure 3. A reverse connection block.
case reference model, which is pre-trained with ImageNet
dataset . Recall that VGG-16 has 13 convolutional layers and 3 fully-connected layers. We convert FC6 (14th
layer) and FC7 (15th layer) to convolutional layers , and
use 2×2 convolutional kernels with stride 2 to reduce the
resolution of FC7 by half1. By now, the feature map sizes
used for object detection are 1/8 (conv 4 3), 1/16 (conv
5 3), 1/32 (conv 6) and 1/64 (conv 7) of the input size,
both in width and height (see Figure 2 top).
3.1. Reverse Connection
Combining ﬁne-grained details with highly-abstracted
information helps object detection with different scales
 . The region-based networks usually fuse multiple CNN layers into a single feature map . Then object detection is performed on the fused maps with regionwise subnetworks . As all objects need to be detected
based on the ﬁxed features, the optimization becomes much
complex. SSD detects objects on multiple CNN layers. However, the semantic information of former layers
is limited, which affects the detection performance of these
layers. This is the reason why SSD has much worse performance on smaller objects than bigger objects .
Inspired from the success of residual connection 
which eases the training of much deeper networks, we propose the reverse connection on traditional CNN architectures. The reverse connection enables former features to
have more semantic information. One reverse connection
block is shown in Figure 3. Firstly, a deconvolutional layer
is applied to the reverse fusion map (annotated as rf-map)
n+1, and a convolutional layer is grafted on backbone layer
n to guarantee the inputs have the same dimension. Then
the two corresponding maps are merged by element-wise
addition. The reverse fusion map 7 is the convolutional output (with 512 channels by 3×3 kernels) of the backbone
layer 7. After this layer has been generated, each reverse
connection block will be generated in the same way, as
shown in Figure 2. In total, there are four reverse fusion
maps with different scales.
Compared with methods using single layer for object
detection , multi-scale representation is more effective on locating all scales of objects (as shown in ex-
1The last FC layer (16th layer) is not used in this paper.
periments). More importantly, as the reverse connection is
learnable, the semantic information of former layers can be
signiﬁcantly enriched. This characteristic makes RON more
effective in detecting all scales of objects compared with
3.2. Reference Boxes
In this section, we describe how to generate bounding
boxes on feature maps produced from Section 3.1. Feature
maps from different levels within a network are known to
have different receptive ﬁeld sizes . As the reverse connection can generate multiple feature maps with different
scales. We can design the distribution of boxes so that speciﬁc feature map locations can be learned to be responsive
to particular scales of objects. Denoting the minimum scale
with smin, the scales Sk of boxes at each feature map k are
Sk = {(2k −1) · smin, 2k · smin}, k ∈{1, 2, 3, 4}.
We also impose different aspect ratios { 1
2, 1, 2, 3} for the
default boxes. The width and height of each box are computed with respect to the aspect ratio . In total, there
are 2 scales and 5 aspect ratios at each feature map location. The smin is
10 of the input size (e.g., 32 pixels for
320×320 model). By combining predictions for all default
boxes with different scales and aspect ratios, we have a diverse set of predictions, covering various object sizes and
3.3. Objectness Prior
As shown in Section 3.2, we consider default boxes
with different scales and aspect ratios from many feature
maps. However, only a tiny fraction of boxes covers objects.
In other words, the ratio between object and non-object
samples is seriously imbalanced. The region-based methods overcome this problem by region proposal networks
 . However, the region proposals will bring translation variance compared with the default boxes. So the
Fast R-CNN pipeline usually uses region-wise networks for
post detection, which brings repeated computation . In
contrast, we add an objectness prior for guiding the search
of objects, without generating new region proposals. Concretely, we add a 3×3×2 convolutional layer followed by
a Softmax function to indicate the existence of an object in
each box. The channel number of objectness prior maps is
10, as there are 10 default boxes at each location.
Figure 1 shows the multi-scale objectness prior generated from a speciﬁc image. For visualization, the objectness prior maps are averaged along the channel dimension.
We see that the objectness prior maps could explicitly re-
ﬂect the existence of an object. Thus the searching space of
the objects could be dramatically reduced. Objects of various scales will respond at their corresponding feature maps,
and we enable this by appropriate matching and end-to-end
former layer
latter layer
Figure 4. One inception block.
training. More results could be seen in the experiment section.
3.4. Detection and Bounding Box Regression
Different from objectness prior, the detection module
needs to classify regions into K+1 categories (K= 20 for
PASCAL VOC dataset and K= 80 for MS COCO dataset,
plus 1 for background). We employ the inception module
 on the feature maps generated in Section 3.1 to perform detection. Concretely, we add two inception blocks
(one block is shown in Figure 4) on feature maps, and classify the ﬁnal inception outputs. There are many inception
choices as shown in . In this paper, we just use the most
simple structure.
With Softmax, the sub-network outputs the per-class
score that indicates the presence of a class-speciﬁc instance.
For bounding box regression, we predict the offsets relative
to the default boxes in the cell (see Figure 5).
3×3, (k+1)×A
Figure 5. Object detection and bounding box regression modules.
Top: bounding box regression; Bottom: object classiﬁcation.
3.5. Combining Objectness Prior with Detection
In this section, we explain how RON combines objectness prior with object detection. We assist object detection
with objectness prior both in training and testing phases.
For training the network, we ﬁrstly assign a binary class
label to each candidate region generated from Section 3.2.
Then if the region covers object, we also assign a classspeciﬁc label to it. For each ground truth box, we (i) match
it with the candidate region with most jaccard overlap; (ii)
match candidate regions to any ground truth with jaccard
overlap higher than 0.5. This matching strategy guarantees
that each ground truth box has at least one region box assigned to it. We assign negative labels to the boxes with
jaccard overlap lower than 0.3.
By now, each box has its objectness label and classspeciﬁc label. The network will update the class-speciﬁc
labels dynamically for assisting object detection with objectness prior at training phase. For each mini-batch at feedforward time, the network runs both the objectness prior and
class-speciﬁc detection. But at the back-propagation phase,
the network ﬁrstly generates the objectness prior, then for
detection, samples whose objectness scores are high than
threshold op are selected (Figure 6). The extra computation only comes from the selection of samples for backpropagation. With suitable op (we use op = 0.03 for all models), only a small number of samples is selected for updating
the detection branch, thus the complexity of backward pass
has been signiﬁcantly reduced.
Figure 6. Mapping the objectness prior with object detection. We
ﬁrst binarize the objectness prior maps according to op, then
project the binary masks to the detection domain of the last convolutional feature maps. The locations within the masks are collected
for detecting objects.
4. Training and Testing
In this section, we ﬁrstly introduce the multi-task loss
function for optimizing the networks. Then we explain how
to optimize the network jointly and perform inference directly.
4.1. Loss Function
For each location, our network has three sibling output branches. The ﬁrst outputs the objectness conﬁdence
score pobj = {pobj
1 }, computed by a Softmax over
the 2×A outputs of objectness prior (A = 10 in this paper, as there are 10 types of default boxes).
the objectness loss with Lobj. The second branch outputs
the bounding-box regression loss, denoted by Lloc. It targets at minimizing the smoothed L1 loss between the
predicted location offsets t = (tx, ty, tw, th) and the target
offsets t∗= (t∗
h). Different from Fast R-CNN
 that regresses the offsets for each of K classes, we just
regress the location one time with no class-speciﬁc information. The third branch outputs the classiﬁcation loss Lcls|obj
for each box, over K+1 categories.
Given the objectness conﬁdence score pobj, the branch ﬁrst excludes regions
whose scores are lower than the threshold op. Then like
Lobj, Lcls|obj is computed by a Softmax over K+1 outputs
for each location pcls|obj = {pcls|obj
, pcls|obj
, . . . , pcls|obj
We use a multi-task loss L to jointly train the networks endto-end for objectness prior, classiﬁcation and bounding-box
regression:
Lloc+(1−α−β)
The hyper-parameters α and β in Equation 2 control the
balance between the three losses. We normalize each loss
term with its input number. Under this normalization, α =
3 works well and is used in all experiments.
4.2. Joint Training and Testing
We optimize the models described above jointly with
end-to-end training. After the matching step, most of the
default boxes are negatives, especially when the number of
default boxes is large. Here, we introduce a dynamic training strategy to scan the negative space. At training phase,
each SGD mini-batch is constructed from N images chosen uniformly from the dataset. At each mini-batch, (a) for
objectness prior, all of the positive samples are selected for
training. Negative samples are randomly selected from regions with negative labels such that the ratio between positive and negative samples is 1:3; (b) for detection, we ﬁrst
reduce the sample number according to objectness prior
scores generated at this mini-batch (as discribed in Section
3.5). Then all of the positive samples are selected. We randomly select negative samples such that the ratio between
positive and negative samples is 1:3. We note that recent
works such as Faster R-CNN and R-FCN usually
use multi-stage training for joint optimization. In contrast,
the loss function 2 is trained end-to-end in our implementation by back-propagation and SGD, which is more efﬁcient at training phase. At the beginning of training, the
objectness prior maps are in chaos. However, along with
the training progress, the objectness prior maps are more
concentrated on areas covering objects.
Data augmentation To make the model more robust to
various object scales, each training image is randomly sampled by one of the following options: (i) Using the original/ﬂipped input image; (ii) Randomly sampling a patch
whose edge length is { 4
10} of the original image and making sure that at least one object’s center
is within this patch. We note that the data augmentation
strategy described above will increase the number of large
objects, but the optimization beneﬁt for small objects is limited. We overcome this problem by adding a small scale
bottle bus
horse mbike person plant
sheep sofa
Fast R-CNN 
Faster R-CNN 
SSD300 
SSD500 
Table 1. Detection results on PASCAL VOC 2007 test set. The entries with the best APs for each object category are bold-faced.
Figure 7. Visualization of performance for RON384 on animals, vehicles, and furniture from VOC2007 test. The Figures show the
cumulative fraction of detections that are correct (Cor) or false positive due to poor localization (Loc), confusion with similar categories
(Sim), with others (Oth), or with background (BG). The solid red line reﬂects the change of recall with the ‘strong’ criteria (0.5 jaccard
overlap) as the number of detections increases. The dashed red line uses the ‘weak’ criteria (0.1 jaccard overlap).
for training. A large object at one scale will be smaller at
smaller scales. This training strategy could effectively avoid
over-ﬁtting of objects with speciﬁc sizes.
Inference At inference phase, we multiply the classconditional probability and the individual box conﬁdence
predictions. The class-speciﬁc conﬁdence score for each
box is deﬁned as Equation 3:
pcls = pobj · pcls|obj.
The scores encode both the probability of the class appearing in the box and how well the predicted box ﬁts the object.
After the ﬁnal score of each box is generated, we adjust the
boxes according to the bounding box regression outputs. Finally, non-maximum suppression is applied to get the ﬁnal
detection results.
5. Results
We train and evaluate our models on three major
datasets: PASCAL VOC 2007, PASCAL VOC 2012, and
MS COCO. For fair comparison, all experiments are based
on the VGG-16 networks. We train all our models on a
single Nvidia TitanX GPU, and demonstrate state-of-the-art
results on all three datasets.
5.1. PASCAL VOC 2007
On this dataset, we compare RON against seminal Fast
R-CNN , Faster R-CNN , and the most recently
proposed SSD . All methods are trained on VOC2007
trainval and VOC2012 trainval, and tested on VOC2007 test
dataset. During training phase, we initialize the parameters for all the newly added layers by drawing weights from
a zero-mean Gaussian distribution with standard deviation
0.01. All other layers are initialized by standard VGG-16
model . We use the 10−3 learning rate for the ﬁrst 90k
iterations, then we decay it to 10−4 and continue training
for next 30k iterations. The batch size is 18 for 320×320
model according to the GPU capacity. We use a momentum
of 0.9 and a weight decay of 0.0005.
Table 1 shows the result comparisons of the methods2.
With 320×320 input size, RON is already better than Faster
R-CNN. By increasing the input size to 384×384, RON
gets 75.4% mAP, outperforming Faster R-CNN by a margin of 2.2%. RON384 is also better than SSD with input
size 500×500. Finally, RON could achieve high mAPs of
76.6% (RON320++) and 77.6% (RON384++) with multiscale testing, bounding box voting and ﬂipping .
Small objects are challenging for detectors. As shown
in Table 1, all methods have inferior performance on ‘boat’
and ‘bottle’. However, RON improves performance of these
categories by signiﬁcant margins: 4.0 points improvement
for ‘boat’ and 7.1 points improvement for ‘bottle’. In sum-
2We note that the latest SSD uses new training tricks (color distortion,
random expansion and online hard example mining), which makes the results much better. We expect these tricks will also improve our results,
which is beyond the focus of this paper.
bottle bus
horse mbike person plant
sheep sofa
Fast R-CNN 
Faster R-CNN 
HyperNet 
SSD300 
SSD500 
Table 2. Results on PASCAL VOC 2012 test set. All methods are based on the pre-trained VGG-16 networks.
mary, performance of 17 out of 20 categories has been improved by RON.
To understand the performance of RON in more detail,
we use the detection analysis tool from . Figure 7 shows
that our model can detect various object categories with
high quality. The recall is higher than 85%, and is much
higher with the ‘weak’ (0.1 jaccard overlap) criteria.
5.2. PASCAL VOC 2012
We compare RON against top methods on the comp4
(outside data) track from the public leaderboard on PAS-
CAL VOC 2012. The training data is the union set of all
VOC 2007, VOC 2012 train and validation datasets, following . We see the same performance trend as
we observed on VOC 2007 test. The results, as shown in
Table 2, demonstrate that our model performs the best on
this dataset. Compared with Faster R-CNN and other variants , the proposed network is signiﬁcantly better,
mainly due to the reverse connection and the use of boxes
from multiple feature maps.
5.3. MS COCO
To further validate the proposed framework on a larger
and more challenging dataset, we conduct experiments on
MS COCO and report results from test-dev2015 evaluation server. The evaluation metric of MS COCO dataset is
different from PASCAL VOC. The average mAP over different IoU thresholds, from 0.5 to 0.95 (written as 0.5:0.95)
is the overall performance of methods. This places a signiﬁcantly larger emphasis on localization compared to the
PASCAL VOC metric which only requires IoU of 0.5. We
use the 80k training images and 40k validation images 
to train our model, and validate the performance on the testdev2015 dataset which contains 20k images. We use the
5×10−4 learning rate for 400k iterations, then we decay
it to 5×10−5 and continue training for another 150k iterations. As instances in MS COCO dataset are smaller and
denser than those in PASCAL VOC dataset, the minimum
scale smin of the referenced box size is 24 for 320×320
model, and 32 for 384×384 model. Other settings are the
same as PASCAL VOC dataset.
Train Data
Average Precision
Fast R-CNN 
OHEM++ 
Faster R-CNN 
SSD300 
trainval35k
SSD500 
trainval35k
Table 3. MS COCO test-dev2015 detection results.
With the standard COCO evaluation metric, Faster R-
CNN scores 21.9% AP, and RON improves it to 27.4% AP.
Using the VOC overlap metric of IoU ≥0.5, RON384++
gives a 5.8 points boost compared with SSD500. It is also
interesting to note that with 320×320 input size, RON gets
26.2% AP, improving the SSD with 500×500 input size by
1.8 points on the strict COCO AP evaluation metric.
We also compare our method against Fast R-CNN with
online hard example mining (OHEM) , which gives
a considerable improvement on Fast R-CNN. The OHEM
method also adopts recent bells and whistles to further improve the detection performance. The best result of OHEM
is 25.5% AP (OHEM++).
RON gets 27.4% AP, which
demonstrates that the proposed network is more competitive on large dataset.
5.4. From MS COCO to PASCAL VOC
Large-scale dataset is important for improving deep neural networks. In this experiment, we investigate how the
MS COCO dataset can help with the detection performance
of PASCAL VOC. As the categories on MS COCO are a
superset of these on PASCAL VOC dataset, the ﬁne-tuning
process becomes easier compared with the ImageNet pretrained model. Starting from MS COCO pre-trained model,
RON leads to 81.3% mAP on PASCAL VOC 2007 and
80.7% mAP on PASCAL VOC 2012.
The extra data from the MS COCO dataset increases the
mAP by 3.7% and 5.3%. Table 4 shows that the model
trained on COCO+VOC has the best mAP on PASCAL
Faster R-CNN 
OHEM++ 
SSD512 
Table 4. The performance on PASCAL VOC datasets. All models
are pre-trained on MS COCO, and ﬁne-tuned on PASCAL VOC.
VOC 2007 and PASCAL VOC 2012. When submitting, our
model with 384×384 input size has been ranked as the top 1
on the VOC 2012 leaderboard among VGG-16 based models. We note that other public methods with better results
are all based on much deeper networks .
6. Ablation Analysis
6.1. Do Multiple Layers Help?
As described in Section 3, our networks generate detection boxes from multiple layers and combine the results. In
this experiment, we compare how layer combinations affect
the ﬁnal performance. For all of the following experiments
as shown in Table 5, we use exactly the same settings and
input size (320×320), except for the layers for object detection.
detection from layer
Table 5. Combining features from different layers.
From Table 5, we see that it is necessary to use all of the
layer 4, 5, 6 and 7 such that the detector could get the best
performance.
6.2. Objectness Prior
As introduced in Section 3.3, the network generates objectness prior for post detection. The objectness prior maps
involve not only the strength of the responses, but also their
spatial positions. As shown in Figure 8, objects with various scales will respond at the corresponding maps. The
maps can guide the search of different scales of objects, thus
signiﬁcantly reducing the searching space.
We also design an experiment to verify the effect of objectness prior. In this experiment, we remove the objectness
prior module and predict the detection results only from the
detection module. Other settings are exactly the same as the
baseline. Removing objectness prior maps leads to 69.6%
mAP on VOC 2007 test dataset, resulting 4.6 points drop
from the 74.2% mAP baseline.
Figure 8. Objectness prior maps generated from images.
6.3. Generating Region Proposals
After removing the detection module, our network could
get region proposals.
We compare the proposal performance against Faster R-CNN and evaluate recalls with
different numbers of proposals on PASCAL VOC 2007 test
set, as shown in Figure 9. The N proposals are the top-
N ranked ones based on the conﬁdence generated by these
  ✶     ✵✁
Figure 9. Recall versus number of proposals on the PASCAL VOC
2007 test set (with IoU = 0.5).
Both Faster R-CNN and RON achieve promising region
proposals when the region number is larger than 100. However, with fewer region proposals, the recall of RON boosts
Faster R-CNN by a large margin. Speciﬁcally, with top 10
region proposals, our 320 model gets 80.7% recall, outperforming Faster R-CNN by 20 points. This validates that
our model is more effective in applications with less region
proposals.
7. Conclusion
We have presented RON, an efﬁcient and effective object detection framework. We design the reverse connection
to enable the network to detect objects on multi-levels of
CNNs. And the objectness prior is also proposed to guide
the search of objects. We optimize the whole networks by a
multi-task loss function, thus the networks can directly predict ﬁnal detection results. On standard benchmarks, RON
achieves state-of-the-art object detection performance.
Figure 10. Selected examples of object detection results on the PASCAL VOC 2007 test set using the RON320 model. The training data is
07+12 trainval .