Convolutional Networks with Adaptive Inference Graphs
Andreas Veit · Serge Belongie
Abstract Do convolutional networks really need a ﬁxed
feed-forward structure? What if, after identifying the
high-level concept of an image, a network could move
directly to a layer that can distinguish ﬁne-grained differences? Currently, a network would ﬁrst need to execute sometimes hundreds of intermediate layers that
specialize in unrelated aspects. Ideally, the more a network already knows about an image, the better it should
be at deciding which layer to compute next. In this
work, we propose convolutional networks with adaptive inference graphs (ConvNet-AIG) that adaptively
deﬁne their network topology conditioned on the input image. Following a high-level structure similar to
residual networks (ResNets), ConvNet-AIG decides for
each input image on the ﬂy which layers are needed. In
experiments on ImageNet we show that ConvNet-AIG
learns distinct inference graphs for diﬀerent categories.
Both ConvNet-AIG with 50 and 101 layers outperform
their ResNet counterpart, while using 20% and 38%
less computations respectively. By grouping parameters into layers for related classes and only executing
relevant layers, ConvNet-AIG improves both eﬃciency
and overall classiﬁcation quality. Lastly, we also study
the eﬀect of adaptive inference graphs on the susceptibility towards adversarial examples. We observe that
ConvNet-AIG shows a higher robustness than ResNets,
complementing other known defense mechanisms.
Keywords Convolutional Networks
Google Research, New York City
E-mail: 
S. Belongie
Department of Computer Science & Cornell Tech
Cornell University, New York
E-mail: 
1 Introduction
Often, convolutional networks (ConvNets) are already
conﬁdent about the high-level concept of an image after only a few layers. This raises the question of what
happens in the remainder of the network that often
comprises hundreds of layers for many state-of-the-art
models. To shed light on this, it is important to note
that due to their success, ConvNets are used to classify increasingly large sets of visually diverse categories.
Thus, most parameters model high-level features that,
in contrast to low-level and many mid-level concepts,
cannot be broadly shared across categories. As a result,
the networks become larger and slower as the number
of categories rises. Moreover, for any given input image
the number of computed features focusing on unrelated
concepts increases.
What if, after identifying that an image contains a
bird, a ConvNet could move directly to a layer that can
distinguish diﬀerent bird species, without executing intermediate layers that specialize in unrelated aspects?
Intuitively, the more the network already knows about
an image, the better it could be at deciding which layer
to compute next. This shares resemblance with decision
trees that employ information theoretic approaches to
select the most informative features to evaluate. Such a
network could decouple inference time from the number
of learned concepts. A recent study provides a key
insight towards the realization of this scenario. The authors study residual networks (ResNets) and show
that almost any individual layer can be removed from
a trained ResNet without interfering with other layers.
This leads us to the following research question: Do we
really need ﬁxed structures for convolutional networks,
or could we assemble network graphs on the ﬂy, conditioned on the input?
 
Andreas Veit, Serge Belongie
In this work, we propose ConvNet-AIG, a convolutional network that adaptively deﬁnes its inference
graph conditioned on the input image. Speciﬁcally, ConvNet-AIG learns a set of convolutional layers and decides for each input image which layers are needed. By
learning both general layers useful to all images and
expert layers specializing on subsets of categories, it
allows to only compute features relevant to the input
image. It is worthy to note that ConvNet-AIG does not
require special supervision about label hierarchies and
relationships to guide layers to specialize.
Figure 1 gives an overview of our approach. ConvNet-
AIG (bottom) follows a structure similar to a ResNet
(center). The key diﬀerence is that for each residual
layer, a gate determines whether the layer is needed
for the current input image. The main technical challenge is that the gates need to make discrete decisions,
which are diﬃcult to integrate into convolutional networks that we would like to train using gradient descent. To incorporate the discrete decisions, we build
upon recent work that introduces diﬀerentiable approximations for discrete stochastic nodes in
neural networks. In particular, we model the gates as
discrete random variables over two states: to execute
the respective layer or to skip it. Further, we model the
gates conditional on the output of the previous layer.
This allows to construct inference graphs adaptively
based on the input and to train both the convolutional
weights and the discrete gates jointly end-to-end.
In experiments on ImageNet , we demonstrate
that ConvNet-AIG eﬀectively learns to generate inference graphs such that for each input only relevant features are computed. In terms of accuracy both ConvNet-
AIG 50 and ConvNet-AIG 101 outperform their ResNet
counterpart, while at the same time using 20% and 38%
less computations. We further show that, without speciﬁc supervision, ConvNet-AIG discovers parts of the
class hierarchy and learns specialized layers focusing on
subsets of categories such as animals and man-made objects. It even learns distinct inference graphs for some
mid-level categories such as birds, dogs and reptiles.
By grouping parameters for related classes and only
executing relevant layers, ConvNet-AIG both improves
eﬃciency and overall classiﬁcation quality. Lastly, we
also study the eﬀect of adaptive inference graphs on
susceptibility towards adversarial examples. We show
that ConvNet-AIG is consistently more robust than
ResNets, independent of adversary strength and that
the additional robustness persists even when applying
additional defense mechanisms.
Tradiঞonal feed-forward convoluঞonal network:
Residual network:
Convoluঞonal network with adapঞve inference graph (AIG):
Fig. 1 ConvNet-AIG (bottom) follows a high level structure similar to ResNets (center) by introducing identity skipconnections that bypass each layer. The key diﬀerence is that
for each layer, a gate determines whether to execute or skip
the layer. This enables individual inference graphs conditioned on the input.
2 Related Work
Our study is related to work in multiple ﬁelds. Several
works have focused on neural network composition
for visual question answering (VQA) and zeroshot learning . While these approaches include convolutional networks, they focus on constructing a ﬁxed
computational graph up front to solve tasks such as
VQA. In contrast, the focus of our work is to construct
a convolutional network conditioned on the input image
on the ﬂy during execution.
Our approach can be seen as an example of adaptive computation for neural networks. Cascaded classiﬁers have a long tradition for computer vision by
quickly rejecting “easy” negatives. Recently, similar approaches have been proposed for neural networks . In an alternative direction, propose to adjust
the amount of computation in fully-connected neural
networks. To adapt computation time in convolutional
networks, propose architectures that add classiﬁcation branches to intermediate layers. This allows
stopping a computation early once a satisfying level of
conﬁdence is reached. Most closely related to our approach is the work on spatially adaptive computation
time for residual networks . In that paper, a ResNet
adaptively determines after which layer to stop computation. Our work diﬀers from this approach in that we
do not perform early stopping, but instead determine
which subset of layers to execute. This is key as it allows the grouping of parameters that are relevant for
similar categories and thus enables distinct inference
graphs for diﬀerent categories.
Our work is further related to network regularization with stochastic noise. By randomly dropping
Convolutional Networks with Adaptive Inference Graphs
Fig. 2 Left: During training, the output of a gate is multiplied with the output of its respective layer. Right: During
inference, a layer does not need to be executed if its gate
decides to skip the layer.
neurons during training, Dropout oﬀers an eﬀective way to prevent neural networks from over-ﬁtting.
Closely related is the work on stochastic depth ,
where entire layers of a ResNet are randomly removed
during each training iteration. Our work resembles this
approach in that it also includes stochastic nodes that
decide whether to execute layers. However, in contrast
to our work, layer removal in stochastic depth is independent from the input and aims to increase redundancy among layers. In our work, we construct the inference graph conditioned on the input image to reduce
redundancy and allow the network to learn layers specialized on subsets of the data.
Lastly, our work can also be seen as an example of
an attention mechanism in that we select speciﬁc
layers of importance for each input image to assemble
the inference graph. This is related to approaches such
as highway networks and squeeze-and-excitation
networks where the output of a residual layer is
rescaled according to the layer’s importance. This allows these approaches to emphasize some layers and
pay less attention to others. In contrast to our work,
these are soft attention mechanisms and still require
the execution of every single layer. Our work is a hard
attention mechanism and thus enables decoupling computation time from the number of categories.
3 Adaptive Inference Graphs
Traditional feed-forward ConvNets can be considered
as a set of N layers which are sequentially applied to
an input image. Figure 1 (top) provides an exemplary
illustration. Formally, let fl(·), l ∈{1, ..., N} denote the
function computed by the lth layer. With x0 as input
image and xl as output of the lth layer, such a network
can be recursively deﬁned as
xl = fl(xl−1)
ResNets , shown in the center of Figure 1, change
this deﬁnition by introducing identity skip-connections
that bypass each layer, i.e., the input to each layer
is also added to its output. This has been shown to
greatly ease optimization during training. As gradients
can propagate directly through the skip-connection, early
layers still receive suﬃcient learning signal even in very
deep networks. A ResNet can be deﬁned as
xl = xl−1 + fl (xl−1)
In a follow-up study on the eﬀects of the skipconnection, it has been shown that, although all layers
are trained jointly, they exhibit a high degree of independence. Further, almost any individual layer can be
removed from a trained ResNet without harming performance and interfering with other layers.
3.1 Gated Computation
Inspired by the observations in , we design ConvNet-
AIG, a network that can deﬁne its topology on the ﬂy.
The architecture follows the basic structure of a ResNet
with the key diﬀerence that instead of executing all layers, the network determines for each input image which
subset of layers to execute. In particular, with layers
focusing on diﬀerent subgroups of categories, it can select only those layers necessary for the speciﬁc input.
A ConvNet-AIG can be deﬁned as
xl = xl−1 + gl(xl−1) · fl (xl−1)
where gl(xl−1) ∈{0, 1}
where gl(xl−1) is a gate that, conditioned on the input
to the layer, decides whether to execute the next layer.
The gate chooses between two discrete states: 0 for ‘oﬀ’
and 1 for ‘on’, which can be seen as a hard attention
mechanism.
Since during training gradients are required with respect to all gates’ parameters, the computational graphs
diﬀer between training and inference time. Figure 2 illustrates the key diﬀerence between the two settings.
During training, each gate and layer is executed and the
output of each gate is multiplied with the output of its
associated layer according to Equation 3. Since gl(xl−1)
is binary, activations are only propagated through the
network, where gates decide to execute their layers.
During inference, no gradients are needed. As a consequence, computation can be saved as a layer does not
need to be executed if its respective gate decides to
skip the layer. The computational graph at inference
time can thus be deﬁned as follows
if gl(xl−1) = 0
xl−1 + fl (xl−1)
if gl(xl−1) = 1
For the gate to be eﬀective, it needs to address a
few key challenges. First, to estimate the relevance of
Andreas Veit, Serge Belongie
if argmax is 1
execute layer
Convoluঞonal
feature map
i.i.d. Gumbel samples
esঞmaঞng relevance
straight-through Gumbel sampling
Fig. 3 Overview of gating unit. Each gate comprises two parts. The ﬁrst part estimates the relevance of the layer to be
executed. The second part decides whether to execute the layer given the estimated relevance. In particular, the Gumbel-Max
trick and its softmax relaxation are used to allow for the propagation of gradients through the discrete decision.
its layer, the gate needs to understand its input features. To prevent mode collapse into trivial solutions
that are independent of the input features, such as always or never executing a layer, we found it to be of
key importance for the gate to be stochastic. We achieve
this by adding noise to the estimated relevance. Second,
the gate needs to make a discrete decision, while still
providing gradients for the relevance estimation. We
achieve this with the Gumbel-Max trick and its softmax relaxation. Third, the gate needs to operate with
low computational cost. Figure 3 provides and overview
of the two key components of the proposed gate. The
ﬁrst one eﬃciently estimates the relevance of the respective layer for the current image. The second component
makes a discrete decision by sampling using Gumbel-
Softmax .
3.2 Estimating Layer Relevance
The goal of the gate’s ﬁrst component is to estimate the
associated layer’s relevance given the input features.
The input to the gate is the output of the previous
layer xl−1 ∈RW ×H×C. Since operating on the full feature map is computationally expensive, we build upon
recent studies which show that much of the
information in convolutional features is captured by the
statistics of the diﬀerent channels and their interdependencies. In particular, we only consider channel-wise
means gathered by global average pooling. This compresses the input features into a 1 × 1 × C channel descriptor.
To capture the dependencies between channels, we
add a simple non-linear function of two fully-connected
layers connected with BatchNorm and a ReLU 
activation function. The output of this operation is the
relevance score for the layer. Speciﬁcally, it is a vector
β containing two unnormalized scores for the actions
of (a) computing and (b) skipping the following layer,
respectively. Generally, a layer is considered relevant for
a given input if the score for execution β1 is larger than
the score for skipping the layer, i.e., β0. The scores are
computed as follows
β = W2σ(W1z)
where σ refers to the ReLU, W1 ∈Rd×C, W2 ∈R2×d
and d is the dimension of the hidden layer. The lightweight
design of the gating function leads to minimal computational overhead. For a ConvNet-AIG based on Resnet 101
for ImageNet, the gating function adds only a computational overhead of 0.04%, but allows to skip 38% of
its layers on average.
3.3 Greedy Gumbel Sampling
The goal of the second component is to make a discrete
decision based on the relevance scores. For this, we build
upon recent work that propose approaches for propagating gradients through stochastic neurons . In
particular, we utilize the Gumbel-Max trick and its
recent continuous relaxation .
A na¨ıve attempt would be to choose the maximum
of the two relevance score to decide whether to execute or skip the layer. However, this approach leads
to rapid mode collapse as it does not account for the
gate’s uncertainty. Further, this approach is not diﬀerentiable. Ideally, we would like to choose among the
options proportional to their relevance scores. A standard way to introduce such stochasticity is to add noise
to the scores.
We choose the Gumbel distribution for the noise, because of its key property that is known as the Gumbel-
Max trick . A random variable G follows a Gumbel
Convolutional Networks with Adaptive Inference Graphs
distribution if G = µ−log(−log(U)), where µ is a realvalued location parameter and U a sample from the
uniform distribution U ∼Unif . Then, the Gumbel-
Max trick states that if we samples from K Gumbel
distributions with location parameters {µk′}K
outcome of the kth Gumbel is the largest exactly with
the softmax probability of its location parameter
P(k is largest|{µk′}K
With this we can parameterize discrete distributions
in terms of Gumbel random variables. In particular,
let X be a discrete random variable with probabilities
P(X = k) ∝αk and let {Gk}k∈{1,...,K} be a sequence
of i.i.d. Gumbel random variables with location µ = 0.
Then, we can sample from the discrete variable X by
sampling from the Gumbel random variables
X = arg max
k∈{1,...,K}
(log αk + Gk)
A drawback of this approach is that the argmax
operation is not continuous. To address this, a continuous relaxation of the Gumbel-Max trick has been
proposed , replacing the argmax with a softmax.
Note that a discrete random variable can be expressed
as a one-hot vector, where the realization of the variable
is the index of the non-zero entry. With this notation,
a sample from the Gumbel-Softmax relaxation can be
expressed by the vector ˆX as follows:
ˆXk = softmax ((log αk + Gk) /τ)
where ˆXk is the kth element in ˆX and τ is the temperature of the softmax. With τ →0, the softmax
function approaches the argmax function and Equation 9 becomes equivalent to the discrete sampler. For
τ →∞it becomes a uniform distribution. Since softmax is diﬀerentiable and Gk is independent noise, we
can propagate gradients to the probabilities αk. To generate samples with the gating function, we set the log
probabilities for the Gumbel-Max trick to the estimated
relevance scores, log α = β.
One option to employ the Gumbel-softmax estimator is to use the continuous version from Equation 9
during training and obtain discrete samples with Equation 8 during testing. An alternative is the straightthrough version of the Gumbel-softmax estimator.
There, during training, for the forward pass we get discrete samples from Equation 8, but during the backwards pass we compute the gradient of the softmax
relaxation in Equation 9. Note that the estimator is
biased due to the mismatch between forward and backward pass. However, we observe that empirically the
straight-through estimator performs better and leads
to inference graphs that are more category-speciﬁc. We
illustrate the two diﬀerent paths during the forward and
backward pass in Figure 3.
3.4 Training Loss
For the network to learn when to use which layer, we
constrain how often each layer is allowed to be used.
Speciﬁcally, we use soft constraints by introducing an
additional loss term that encourages each layer to be executed at a certain target rate. This target rate could
be the same for each layer or layer speciﬁc. We approximate the execution rates for each layer over each
mini-batch and penalize deviations from the target rate.
Speciﬁcally, let L be the set of layers in the network.
Each layer has a target rate t which lies within the interval ti ∈ . Further, with a mini-batch B of training
instances i ∈B and the output of the gate for the lth
layer and ith training instance as gl,i, the target rate
loss is deﬁned as
Ltarget = 1
The target rate loss allows the optimization to reach
solutions in which parameters that are relevant only to
subsets of related categories are grouped together in
separate layers, which minimizes the amount of unnecessary features to be computed. The target rate provides an easy instrument to adjust computation time.
ConvNet-AIG is robust to a wide range of target rates.
We study the eﬀect of the target rate on classiﬁcation
accuracy and inference time in the experimental section. With the standard multi-class logistic loss, LMC,
the overall training loss is
LAIG = LMC + λLtarget
where λ balances the two losses. In our experiments
we use λ = 2 We optimize this joint loss with minibatch stochastic gradient descent.
3.5 Adaptive Gating During Inference
Once the network is trained, there are diﬀerent alternatives for how to perform adaptive gating during inference. The ﬁrst alternative is to follow the same procedure as used during training and use stochastic inference by sampling according to Equation 8. A second
alternative is to compute the gates in a deterministic
fashion. For deterministic inference, we do not sample from the relevance scores by adding Gumbel noise,
Andreas Veit, Serge Belongie
but directly compute the softmax over them and use a
threshold to decide whether to execute the layer
gl(xl−1) =
if softmax(β)k′ ≤T
if softmax(β)k′ > T
where k′ is the element in the relevance score vector β
that corresponds to executing the layer and T ∈ 
is a threshold. With a threshold of T = 0.5, this performs an argmax over the relevance scores and executes
the layer whenever the score for executing is higher
that for skipping. Thus, varying the threshold, provides a tool that allows to control computation time
even after a model has already been trained. Our empirical evaluation indicates that deterministic inference
slightly outperforms the stochastic alternative. Further
varying thresholds allows for minor trade-oﬀs between
inference time and classiﬁcation quality.
4 Experiments
We perform a series experiments to evaluate the performance of ConvNet-AIG and whether it learns specialized layers and category-speciﬁc inference graphs.
We compare the diﬀerent proposed training and inference modes as well as ablation studies for varying target
rates and thresholds. Lastly, we study its robustness by
analyzing the eﬀect of adaptive inference graphs on the
susceptibility towards adversarial attacks.
4.1 Results on CIFAR
We ﬁrst perform a set of experiments on CIFAR-10 
to validate the proposed gating mechanism and its effectiveness to distribute computation among layers.
4.1.1 Model conﬁgurations and training details
We build a ConvNet-AIG based on the original ResNet
110 . Besides the added gates, ConvNet-AIG follows
the same architecture as ResNet 110. For the gates, we
choose a hidden state of size d = 16. The additional
gate per residual block, adds a ﬁxed overhead of 0.01%
more ﬂoating point operations and 4.8% more parameters compared to the standard ResNet-110. We follow a
similar training scheme as with momentum 0.9 and
weight decay 5 × 10−4. All models are trained for 350
epochs with a mini-batch size of 256. We use a stepwise learning rate starting at 0.1 and decaying by 10−1
after 150 and 250 epochs. We adopt a standard dataaugmentation scheme, where images are padded with 4
pixels on each side, randomly cropped to 32 × 32 and
with probability 0.5 horizontally ﬂipped.
Test error on CIFAR 10 in %. ConvNet-
AIG 110 clearly outperforms ResNet 110 while only using
a subset of 82% of the layers. When executing all layers
(ConvNet-AIG 110∗), it also outperforms stochastic depth.
Params (106)
ResNet 110 
Pre-ResNet 110 
Stoch. Depth 110
ConvNet-AIG 110
ConvNet-AIG 110∗
4.1.2 Results
Table 1 shows test error on CIFAR 10 for ResNet ,
pre-activation ResNet , stochastic depth and
their ConvNet-AIG counterpart. The table also shows
the number of model parameters and ﬂoating point operations (multiply-adds). We compare two variants: For
standard ConvNet-AIG, we only execute layers with
open gates using the stochastic inference setup. As a
second variant, which we indicate by “ ∗”, we execute
all layers and analogous to Dropout and stochastic depth the output of each layer is scaled by its
expected execution rate.
From the results, we observe that ConvNet-AIG outperforms its ResNet counterparts clearly, even when using only a subset of the layers. In particular, ConvNet-
AIG 110 with a target-rate of 0.7 uses only 82% of the
layers in expectation. Since ResNet 110 might be overparameterized for CIFAR-10, the regularization induced
by dropping layers could be a key factor to performance. We observe that ConvNet-AIG 110∗outperforms stochastic depth, implying beneﬁts of adaptive
inference graphs beyond regularization. In fact, ConvNet-
AIG learns to identify layers of key importance such
as downsampling layers and learns to always execute
them, although they incur computation cost. We do
not observe any downward outliers, i.e. layers that are
dropped every time.
4.2 Results on ImageNet
In experiments on ImageNet , we study whether the
proposed ConvNet-AIG learns to group parameters such
that for each image only relevant features are computed. ImageNet is well suited for this study, as it contains a large variety of categories including man-made
objects, food, and many diﬀerent animals.
4.2.1 Model conﬁgurations and training details
We build ConvNet-AIGs based on ResNet 50 and ResNet
101 . Again, we follow the same architectures as the
Convolutional Networks with Adaptive Inference Graphs
Floating point operations in 109 (GFLOPs)
ImageNet top-1 error
ConvNet-AIG 50
ConvNet-AIG 101
ResNet 101
Model architecture
ConvNet-AIG (stochastic)
ConvNet-AIG (deterministic)
Fig. 4 Top-1 accuracy vs. computational cost on ImageNet. ConvNet-AIG 50 outperforms ResNet 50, while skipping 20% of its layers in expectation. Similarly, ConvNet-
AIG 101 outperforms ResNet 101 while requiring 38% less
computations. Deterministic inference outperforms stochastic
inference, particularly for the larger models with 101 layers.
original ResNets, with the sole exception of the added
gates. The size of the hidden state is again d = 16,
adding a ﬁxed overhead of 3.9% more parameters and
0.04% more ﬂoating point operations.
We compare ConvNet-AIG with diﬀerent target-rate
schedules. First, we evaluate a ConvNet-AIG 50, where
all 16 residual layers have the same target rate. As discussed in detail below, in this case some layers are too
early in the network to yet eﬀectively distinguish between diﬀerent categories, and some layers are needed
for all inputs. Thus we also evaluate custom target-rate
schedules. In particular, for our quantitative results we
use a target rate of 1 for all layers up to the second
downsampling layer and for ResNet 50 we further set
the target rate to 1 for the third downsampling layer
and the last layer. This slightly improves quantitative
performance, and also improves convergence speed.
We follow the standard ResNet training procedure,
with mini-batch size of 256, momentum of 0.9 and weight
decay of 10−4. All models are trained for 100 epochs
with step-wise learning rate starting at 0.1 and decaying
by 10−1 every 30 epochs. We use the data-augmentation
procedure as in and at test time ﬁrst rescale images to 256 × 256 followed by a 224 × 224 center crop.
The gates are initialized to open at a rate of 85% at the
beginning of training.
4.2.2 Quantitative comparison
Figure 4 shows top-1 error on ImageNet and computational cost in terms of GFLOPs for ConvNet-AIG with
Floating point operations in 109 (GFLOPs)
ImageNet Top1 error
threshold=0.1
threshold=0.5
threshold=0.9
Model architecture
ConvNet-AIG 101 t=0.3
ConvNet-AIG 101 t=0.5
ResNet 101
Fig. 5 Impact of inference thresholds on top-1 accuracy and computational cost on ImageNet. Varying
thresholds allow for minor trade-oﬀs between inference time
and classiﬁcation quality after a model is already trained. For
larger adjustments in computation time it is more eﬀective to
train a model with a diﬀerent target-rate.
50 and 101 layers and the respective ResNets of varying depth. We further show the impact of diﬀerent target rates on performance and eﬃciency. We compare
models with target rates for the layers after the second downsampling layer from 0.4 to 0.7 for ConvNet-
AIG 50 and 0.3 to 0.5 for ConvNet-AIG 101. For each
variant of ConvNet-AIG we show both the performance
of stochastic inference (green) as well as deterministic
inference (red). The threshold for each model is set to
0.5. The impact of varying thresholds is shown for both
ConvNet-AIG 101 models in Figure 5. Details about
the models’ complexities and further baselines are presented in Table 2.
From the results we make the following key observations. Both ConvNet-AIG 50 and ConvNet-AIG 101
outperform their ResNet counterpart, while also using
only a subset of the layers. In particular, ConvNet-
AIG 50 saves about 20% of computation. Similarly,
ConvNet-AIG 101 outperforms its respective Resnet
while using 38% less computations. Further, we observe
that deterministic inference consistently outperforms
stochastic inference. The diﬀerence is most noticeable
for the large model with 101 layers. This is likely due
to the larger proportion of ‘specialization layers’ in the
larger model that are focusing on speciﬁc subsets of the
data, which is highlighted in Figure 6.
These results indicate that convolutional networks
do not need a ﬁxed feed-forward structure and that ConvNet-AIG is an eﬀective means to enable adaptive inference graphs that are conditioned on the input image.
Figure 4 also visualizes the eﬀect of the target rate.
As expected, decreasing the target rate reduces compu-
Andreas Veit, Serge Belongie
Test error on ImageNet in % for ConvNet-AIG 50, ConvNet-AIG 101 and the respective ResNets of varying
depth. ConvNet-AIGs using the Straight-Through Gumbel training paradigm outperform the standard Gumbel-Softmax,
which is indicated with ‘soft gates’. Further, models with deterministic inference outperform their stochastic counterparts. All
numbers shown are based on a threshold of 0.5. Overall, all ConvNet-AIG variants outperform their ResNet counterpart, while
at the same time using only a subset of the layers. This demonstrates that ConvNet-AIG is more eﬃcient and also improves
overall classiﬁcation quality.
#Params (106)
FLOPs (109)
ResNet 34 
ResNet 50 
ResNet 50 (our)
ResNet 101 
ResNet 101 (our)
Stochastic Depth ResNet 50
Stochastic Depth ResNet 101
ConvNet-AIG 50 soft gates [t=0.5]
ConvNet-AIG 50 soft gates [t=0.7]
stochastic
ConvNet-AIG 50 [t=0.4]
ConvNet-AIG 50 [t=0.5]
ConvNet-AIG 50 [t=0.6]
ConvNet-AIG 50 [t=0.7]
ConvNet-AIG 101 [t=0.3]
ConvNet-AIG 101 [t=0.5]
deterministic
ConvNet-AIG 50 [t=0.4]
ConvNet-AIG 50 [t=0.5]
ConvNet-AIG 50 [t=0.6]
ConvNet-AIG 50 [t=0.7]
ConvNet-AIG 101 [t=0.3]
ConvNet-AIG 101 [t=0.5]
tation time. Interestingly, penalizing computation ﬁrst
improves accuracy, before lowering the target rate further decreases accuracy. This demonstrates that ConvNet-AIG both improves eﬃciency and overall classiﬁcation quality. Further, it appears often more eﬀective
to decrease the target rate compared to reducing layers
in standard ResNets.
In Table 2, we also compare the two diﬀerent training regimes, of (a) standard Gumbel-Softmax, where
softmax is applied during both forward and backward
pass and (b) Straight-Through Gumbel, where argmax
is performed during forward and softmax during backward pass. Speciﬁcally, we compare performance for
ConvNet-AIG 50 with target-rates of 0.5 and 0.7. The
results show that the straight-through variant consistently outperforms the standard Gumbel-Softmax. One
reason for the observed diﬀerence could be that, when
using softmax during the forward pass, although scaled
down, activations are always propagated through the
network, even if a gate decides that a layer is not relevant for a given input image.
Lastly, due to surface resemblance, we also compare
our model to stochastic depth . We observe that
for smaller ResNet models stochastic depth does not
provide competitive results. Only very large models see
beneﬁts from stochastic depth regularization. The paper on stochastic depth reports that even for the
very large ResNet 152 performance remains below a basic ResNet. This highlights the opposite goals of ConvNet-AIG and stochastic depth. Stochastic depth aims
to create redundant features by enforcing each subset
of layers to model the whole dataset . ConvNet-AIG
aims to separate parameters that are relevant to diﬀerent subsets of the dataset into diﬀerent layers.
4.2.3 Evaluating gating thresholds during inference
During training, the gates are optimized for a threshold
of 0.5, i.e., argmax over the relevance scores. However,
there is some ﬂexibility at test time to vary the thresholds so as to adjust the trade-oﬀbetween inference
time and classiﬁcation quality. Figure 5 shows top-1 error on ImageNet and computational cost for ConvNet-
AIG 101 with target-rates 0.3 and 0.5 for thresholds
ranging from 0.1 to 0.9. The results show that varying the thresholds within the range of 0.3 to 0.7 allows
to slightly adjust inference time of an already trained
model without a large decrease in accuracy. However,
for larger adjustments to computation time it is more
eﬀective to train a model with a diﬀerent target rate.
Convolutional Networks with Adaptive Inference Graphs
ConvNet-AIG 50:
ConvNet-AIG 101:
downsampling layer
Residual layers
Residual layers
other animals
man-made objects
ImageNet categories
other mammals
other objects
structures
consumer goods
target rate = 0.6
target rate = 1
target rate = 0.6
Fig. 6 Learned inference graphs on ImageNet. The histograms show for ConvNet-AIG 50 (left) and ConvNet-AIG 101
(right) how often each residual layer (x-axis) is executed for each of the 1000 classes in ImageNet (y-axis). We observe a clear
diﬀerence between layers used for man-made objects and for animals and even for some mid-level categories such as birds,
mammals and reptiles. Without speciﬁc supervision, the network discovers parts of the class hierarchy. Further, downsampling
layers and the last layers appear of key importance and are executed for all images. Lastly, the left histogram shows that early
layers are mostly agnostic to the diﬀerent classes. Thus, we set early layers in ConvNet-AIG 101 to be always executed. The
remaining layers are suﬃcient to provide diﬀerent inference graphs for the various categories.
4.2.4 Analysis of learned inference graphs
To analyze the learned inference graphs, we study the
rates at which diﬀerent layers are executed for images
of diﬀerent categories. Figure 6 shows the execution
rates of each layer for ConvNet-AIG 50 on the left and
ConvNet-AIG 101 on the right. The x-axis indicates
the residual layers and the y-axis breaks down the execution rates by the 1000 classes in ImageNet. Further,
the ﬁgure shows high-level and mid-level categories that
contain large numbers of classes. The color in each cell
indicates the percentage of validation images from a
given category that the respective layer is executed.
From the ﬁgure, we see a clear diﬀerence between
man-made objects and animals. Moreover, we even observe distinctions between mid-level animal categories
such as birds, mammals and reptiles. This reveals that
the network discovers part of the label hierarchy and
groups parameters accordingly. Generally, we observe
similar structures in ConvNet-AIG 50 and ConvNet-
AIG 101. However, the grouping of the mid-level categories is more distinct in ConvNet-AIG 101 due to the
larger number of layers that can capture high-level features. This result demonstrates that ConvNet-AIG successfully learns layers that focus on speciﬁc subsets of
categories. It is worthy to note that the training objective does not include an incentive to learn category speciﬁc layers. The specialization appears to emerge naturally when the computational budget gets constrained.
Further, we observe that downsampling layers and
the last layers deviate signiﬁcantly from the target rate
and are executed for all images. This demonstrates their
key role in the network (as similarly observed in )
Andreas Veit, Serge Belongie
fewest layers
musical instruments
most layers
Fig. 7 Validation images from ImageNet that use the fewest layers (top) and the most layers (bottom) within the
categories of birds, dogs and musical instruments. The examples illustrate how instance diﬃculty translates into layer usage.
other layers
downsampling
last layer
Training epochs
Frequency of execution
second last
Fig. 8 Execution rates per layer over ﬁrst 30 epochs
of training. Layers are quickly separated into key and less critical layers. Downsampling layers and the last layer increase
execution rate, while the remaining layers slowly approach
the target rate.
consumer goods
all classes
Number of layers
Fig. 9 Distribution over the number of executed layers. For ConvNet-AIG 50 on ImageNet with target rate 0.4,
in average 10.8 out of 16 residual layers are executed. Images
of animals tend to use fewer layers than man-made objects.
and shows how ConvNet-AIG learns to eﬀectively tradeoﬀcomputational cost for accuracy.
Lastly, the ﬁgure shows that for ConvNet-AIG 50,
inter-class variation is mostly present in the later layers
of the network after the second downsampling layer.
One reason for this could be that features from early
layers are useful for all categories. Further, early layers
might not yet capture suﬃcient semantic information
to discriminate between categories. Thus, we set the
target rate for the early layers of ConvNet-AIG 101 to 1
so that they are always executed. The remaining layers
still provide suﬃcient ﬂexibility for diﬀerent inference
paths for the various categories.
Figure 8 shows a typical trajectory of the execution
rates during training for ConvNet-AIG 50 with a target
rate 0.6 for all layers. The layers are initialized to execute a rate of 85% at the start of training. The ﬁgure
shows the ﬁrst 30 training epochs and highlights how
the layers are quickly separated into key layers and less
critical layers. Important layers such as downsampling
and the last layers increase their execution rate, while
the remaining layers slowly approach the target rate.
4.2.5 Variable inference time
Due to the adaptive inference graphs, computation time
varies across images. Figure 9 shows the distribution
over how many of the 16 residual layers in ConvNet-
AIG 50 are executed over all ImageNet validation images. On average 10.81 layers are executed with a standard deviation of 1.11. The ﬁgure also highlights the
mid-level categories of birds and consumer goods. It
appears that in expectation, images of birds use one
layer less than images of consumer goods. From Figure 6 we further know that the two groups also use
diﬀerent sets of layers. To get a better understanding
for what aspects impact inference time, Figure 7 shows
the validation images that use the fewest and the most
layers within the categories of birds, dogs and musical instruments. The examples highlight that easy instances with iconic views require only a few layers. On
the other hand, diﬃcult instances that are small or occluded need more computation.
Convolutional Networks with Adaptive Inference Graphs
Adversary strength (epsilon)
ImageNet top-1 accuracy
ConvNet-AIG 50
ConvNet-AIG 50
+ JPEG compression
+ JPEG compression
(no attack)
(FGSM attack,
epsilon 0.047)
Residual layers
Frequency of execution
Fig. 10 Adversarial attack using Fast Gradient Sign Method. Left: ConvNet-AIG is consistently more robust than
the plain Resnet, independent of adversary strength. The additional robustness persists even when applying additional defense
mechanisms. Right: Average execution rates per layer for images of birds before and after the attack. The execution rates
remain mostly unaﬀected by the attack.
4.3 Robustness to adversarial attacks
In a third set of experiments we aim to understand
the eﬀect of adaptive inference graphs on the susceptibility towards adversarial attacks. Speciﬁcally, we are
interested in the ConvNet-AIG variant with stochastic
inference. While exhibiting slightly lower performance
compared to the deterministic counterpart, the stochasticity of the inference graph might improve robustness
towards adversarial attacks.
We perform a Fast Gradient Sign Attack on ConvNet-AIG 50 and ResNet 50, both trained on ImageNet.
The results are presented in Figure 10. In the graph
on the left, the x-axis shows the strength of the adversary measured in the amount each pixel can to be
changed. The y-axis shows top-1 accuracy on ImageNet.
We observe that ConvNet-AIG is consistently more robust, independent of adversary strength. To investigate
whether this additional robustness complements other
defenses , we perform JPEG compression on the adversarial examples. We follow and use a JPEG quality setting of 75%. While both networks greatly beneﬁt
from the defense, ConvNet-AIG remains more robust,
indicating that the additional robustness can complement other defenses. We repeat the experiment for deterministic inference and observe performance very similar to the basic ResNet.
To understand the eﬀect of the attack on the gates
and the robustness of stochastic inference, we look at
the execution rates before and after the attack. On the
right side, Figure 10 shows the average execution rates
per layer over all bird categories for ConvNet-AIG 50
before and after a FGSM attack with epsilon 0.047. Although the accuracy of the network drops from 74.62%
to 11%, execution rates remain similar. Since the increased robustness only appears during stochastic inference, it seems that the reason for the gates’ resilience
is the added Gumbel noise which outweighs the noise
introduced by the attack.
5 Conclusion
In this work, we have shown that convolutional networks do not need ﬁxed feed-forward structures. With
ConvNet-AIG, we introduced a ConvNet that adaptively assembles its inference graph on the ﬂy based
on the input image. Speciﬁcally, we presented both a
stochastic as well as a deterministic mode to construct
the inference graph. Experiments on ImageNet show
that ConvNet-AIG groups parameters for related classes
into specialized layers and learns to only execute those
layers relevant to the input. This allows decoupling inference time from the number of learned concepts and
improves both eﬃciency as well as overall classiﬁcation
quality. In particular, this translates into 38% less computations for ResNet 101 while achieving the same classiﬁcation quality.
This work opens up numerous paths for future work.
With respect to network architecture, it would be intriguing to extend this work beyond ResNets to other
structures such as densely-connected or inceptionbased networks. From a practitioner’s point of view,
it might be exciting to extend this work into a framework where the set of executed layers is adaptive, but
their number is ﬁxed so as to achieve constant inference
times. Further, we have seen that the gates are largely
unaﬀected by basic adversarial attacks. For an adversary, it could be interesting to investigate attacks that
speciﬁcally target the gating functions.
Andreas Veit, Serge Belongie
Acknowledgements We would like to thank Ilya Kostrikov,
Daniel D. Lee, Kimberly Wilber, Antonio Marcedone, Yiqing
Hua and Charles Herrmann for insightful discussions and