Real-Time Bidding by Reinforcement Learning
in Display Advertising
†Han Cai, †Kan Ren, †Weinan Zhang,∗
‡Kleanthis Malialis, ‡♮Jun Wang, †Yong Yu, ♯Defeng Guo
†Shanghai Jiao Tong University, ‡University College London, ♮MediaGamma Ltd, ♯Vlion Inc.
{hcai,kren,wnzhang}@apex.sjtu.edu.cn, 
The majority of online display ads are served through realtime bidding (RTB) — each ad display impression is auctioned oﬀin real-time when it is just being generated from a
user visit. To place an ad automatically and optimally, it is
critical for advertisers to devise a learning algorithm to cleverly bid an ad impression in real-time. Most previous works
consider the bid decision as a static optimization problem of
either treating the value of each impression independently
or setting a bid price to each segment of ad volume. However, the bidding for a given ad campaign would repeatedly
happen during its life span before the budget runs out. As
such, each bid is strategically correlated by the constrained
budget and the overall eﬀectiveness of the campaign (e.g.,
the rewards from generated clicks), which is only observed
after the campaign has completed. Thus, it is of great interest to devise an optimal bidding strategy sequentially so that
the campaign budget can be dynamically allocated across all
the available impressions on the basis of both the immediate and future rewards. In this paper, we formulate the bid
decision process as a reinforcement learning problem, where
the state space is represented by the auction information
and the campaign’s real-time parameters, while an action is
the bid price to set. By modeling the state transition via
auction competition, we build a Markov Decision Process
framework for learning the optimal bidding policy to optimize the advertising performance in the dynamic real-time
bidding environment. Furthermore, the scalability problem
from the large real-world auction volume and campaign budget is well handled by state value approximation using neural
networks. The empirical study on two large-scale real-world
datasets and the live A/B testing on a commercial platform
have demonstrated the superior performance and high eﬃciency compared to state-of-the-art methods.
Bid Optimization, Reinforcement Learning, Display Ads
∗Weinan Zhang is the corresponding author of this paper.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from .
WSDM 2017, February 06-10, 2017, Cambridge, United Kingdom
c⃝2017 ACM. ISBN 978-1-4503-4675-7/17/02...$15.00
DOI: 
INTRODUCTION
The increased availability of big data and the improved
computational power have advanced machine learning and
artiﬁcial intelligence for various prediction and decision making tasks. In particular, the successful application of reinforcement learning in certain settings such as gaming control
 has demonstrated that machines not only can predict,
but also have a potential of achieving comparable humanlevel control and decision making. In this paper, we study
machine bidding in the context of display advertising. Auctions, particularly real-time bidding (RTB), have been a
major trading mechanism for online display advertising . Unlike the keyword-level bid decision in sponsored
search , the advertiser needs to make the impression-level
bid decision in RTB, i.e., bidding for every single ad impression in real time when it is just being generated by a
user visit . Machine based bid decision, i.e., to calculate the strategic amount that the advertiser would like
to pay for an ad opportunity, constitutes a core component
that drives the campaigns’ ROI . By calculating an
optimal bid price for each ad auction (also considering the
remaining budget and the future availability of relevant ad
impressions in the ad exchange) and then observing the auction result and user feedback, the advertiser would be able
to reﬁne their bidding strategy and better allocate the campaign budget across the online page view volume.
A straightforward bidding strategy in RTB display advertising is mainly based on the truthfulness of second-price
auctions , which means the bid price for each ad impression should be equal to its true value, i.e., the action value
(e.g., click value) multiplied by the action rate (e.g., clickthrough rate) .
However, for budgeted bidding in repeated auctions, the optimal bidding strategy may not be
truthful but depends on the market competition, auction
volume and campaign budget . In , researchers
have proposed to seek the optimal bidding function that
directly maximizes the campaign’s key performance indicator (KPI), e.g., total clicks or revenue, based on the static
distribution of input data and market competition models.
Nevertheless, such static bid optimization frameworks may
still not work well in practice because the RTB market competition is highly dynamic and it is fairly common that the
true data distribution heavily deviates from the assumed one
during the model training , which requires additional control step such as budget pacing to constrain the budget
In this paper, we solve the issue by considering bidding
as a sequential decision, and formulate it as a reinforcement
 
learning to bid (RLB) problem. From an advertiser’s perspective, the whole ad market and Internet users are regarded as the environment. At each timestep, the advertiser’s bidding agent observes a state, which consists of the
campaign’s current parameters, such as the remaining lifetime and budget, and the bid request for a speciﬁc ad impression (containing the information about the underlying
user and their context) .
With such state (and context),
the bidding agent makes a bid action for its ad. After the
ad auction, the winning results with the cost and the corresponding user feedback will be sent back to the bidding
agent, which forms the reward signal of the bidding action.
Thus, the bid decision aims to derive an optimal bidding
policy for each given bid request.
With the above settings, we build a Markov Decision Process (MDP) framework for learning the optimal bidding policy to optimize the advertising performance. The value of
each state will be calculated by performing dynamic programming. Furthermore, to handle the scalability problem
for the real-world auction volume and campaign budget, we
propose to leverage a neural network model to approximate
the value function. Besides directly generalizing the neural
network value function, we also propose a novel coarse-to-
ﬁne episode segmentation model and state mapping models
to overcome the large-scale state generalization problem.
In our empirical study, the proposed solution has achieved
16.7% and 7.4% performance gains against the state-of-theart methods on two large-scale real-world datasets. In addition, our proposed system has been deployed into a commercial RTB platform. We have performed an online A/B
testing, where a 44.7% improvement in click performance
was observed against a most widely used method in the industry.
BACKGROUND AND RELATED WORK
Reinforcement Learning. An MDP provides a mathematical framework which is widely used for modelling the
dynamics of an environment under diﬀerent actions, and is
useful for solving reinforcement learning problems . An
MDP is deﬁned by the tuple ⟨S, A, P, R⟩.
The set of all
states and actions are represented by S and A respectively.
The reward and transition probability functions are given
by R and P. Dynamic programming is used in cases where
the environment’s dynamics, i.e., the reward function and
transition probabilities are known in advance. Two popular
dynamic programming algorithms are policy iteration and
value iteration. For large-scale situations, it is diﬃcult to
experience the whole state space, which leads to the use of
function approximation that constructs an approximator of
the entire function . In this work, we use value iteration for small-scale situations, and further build a neural
network approximator to solve the scalability problem.
RTB Strategy. In the RTB process , the advertiser receives the bid request of an ad impression with its real-time
information and the very ﬁrst thing to do is to estimate the
utility, i.e., the user’s response on the ad if winning the auction. The distribution of the cost, i.e., the market price , which is the highest bid price from other competitors,
is also forecasted by the bid landscape forecasting component. Utility estimation and bid landscape forecasting are
described below. Given the estimated utility and cost factors, the bidding strategy decides the ﬁnal bid price with
accessing the information of the remaining budget and auction volume. Thus it is crucial to optimize the ﬁnal bidding
strategy considering the market and bid request information
with budget constraints. A recent comprehensive study on
the data science of RTB display advertising is posted in .
Utility Estimation. For advertisers, the utility is usually
deﬁned based on the user response, i.e., click or conversion,
and can be modeled as a probability estimation task .
Much work has been proposed for user response prediction,
e.g., click-through rate (CTR) , conversion rate (CVR)
 and post-click conversion delay patterns . For modeling, linear models such as logistic regression and nonlinear models such as boosted trees and factorization
machines are widely used in practice. There are also
online learning models that immediately perform updating
when observing each data instance, such as Bayesian probit
regression , FTRL learning in logistic regression . In
our paper, we follow and adopt the most widely used
logistic regression for utility estimation to model the reward
on agent actions.
Bid Landscape Forecasting. Bid landscape forecasting
refers to modeling the market price distribution for auctions
of speciﬁc ad inventory, and its c.d.f. is the winning probability given each speciﬁc bid price . The authors in presented some hypothetical winning functions and
learned the parameters. For example, a log-normal market
price distribution with the parameters estimated by gradient
boosting decision trees was proposed in . Since advertisers only observe the winning impressions, the problem of
censored data is critical. Authors in proposed
to leverage censored linear regression to jointly model the
likelihood of observed market prices in winning cases and
censored ones with losing bids.
Recently, the authors in
 proposed to combine survival analysis and decision tree
models, where each tree leaf maintains a non-parametric survival model to ﬁt the censored market prices. In this paper,
we follow and use a non-parametric method to model
the market price distribution.
Bid Optimization. As has been discussed above, bidding
strategy optimization is the key component within the decision process for the advertisers . The auction theory 
proved that truthful bidding is the optimal strategy under
the second-price auction.
However, truthful bidding may
perform poorly when considering the multiple auctions and
the budget constraint . In real-world applications, the
linear bidding function is widely used. The authors in
 empirically showed that there existed non-linear bidding
functions better than the linear ones under variant budget
constraints. When the data changes, however, the heuristic
model or hypothetical bidding functions cannot depict well the real data distribution. The authors in
 proposed the model-based MDPs to derive the optimal policy for bidding in sponsored search or ad selection in
contextual advertising, where the decision is made on keyword level. In our work, we investigate the most challenging
impression-level bid decision problem in RTB display advertising that is totally diﬀerent from . We also tackle
the scalability problem, which remains unsolved in , and
demonstrate the eﬃciency and eﬀectiveness of our method
in a variety of experiments.
Bidding Agent
Environment
[s] remaining volume t
[s] bid request x
[r] auction win, cost į
[r] user click r
[s] remaining budget b
[a] action
[r] reward
Figure 1: Diagram of reinforcement learning to bid.
PROBLEM AND FORMULATION
In a RTB ad auction, each bidding agent acts on behalf of
its advertiser and generates bids to achieve the campaign’s
speciﬁc target. Our main goal is to derive the optimal bidding policy in a reinforcement learning fashion. For most
performance-driven campaigns, the optimization target is
to maximize the user responses on the displayed ads if the
bid leads to auction winning. Without loss of generality, we
consider clicks as our targeted user response objective, while
other KPIs can be adopted similarly. The diagram of interactions between the bidding agent and the environment is
shown in Figure 1.
Problem Deﬁnition
Mathematically, we consider bidding in display advertising as an episodic process ; each episode comprises T auctions which are sequentially sent to the bidding agent. Each
auction is represented by a high dimensional feature vector
x, which is indexed via one-hot binary encoding. Each entry of x corresponds to a category in a ﬁeld, such as the
category London in the ﬁeld City, and the category Friday
in the ﬁeld Weekday. The ﬁelds consist of the campaign’s
ad information (e.g., ad creative ID and campaign ID) and
the auctioned impression contextual information (e.g., user
cookie ID, location, time, publisher domain and URL).
At the beginning, the agent is initialized with a budget B,
and the advertising target is set to acquire as many clicks as
possible during the following T auctions. Three main pieces
of information are considered by the agent (i) the remaining auction number t ∈{0, · · · , T}; (ii) the unspent budget
b ∈{0, · · · , B} and (iii) the feature vector x. During the
episode, each auction will be sent to the agent sequentially
and for each of them the agent needs to decide the bid price
according to the current information t, b and x.
The agent maintains the remaining number of auctions t
and the remaining budget b. At each timestep, the agent
receives an auction x ∈X (the feature vector space), and
determines its bid price a. We denote the market price p.d.f.
given x as m(δ, x), where δ is the market price and m is its
probability. If the agent bids at price a ≥δ, then it wins
the auction and pays δ, and the remaining budget changes
to b −δ. In this case, the agent can observe the user response and the market price later. Alternatively, if losing,
the agent gets nothing from the auction. We take predicted
CTR (pCTR) θ(x) as the expected reward, to model the
action utility. After each auction, the remaining number of
auctions changes to t −1. When the auction ﬂow of this
episode runs out, the current episode ends and both the remaining auction number and budget are reset.
MDP Formulation of RTB
A Markov Decision Process (MDP) provides a framework
that is widely used for modeling agent-environment interactions . Our notations are listed in Table 1. An MDP can
Table 1: A summary of our notations.
Description
The feature vector that represents a bid request.
The whole feature vector space.
The probability density function of x.
The predicted CTR (pCTR) if winning the auction of x.
The p.d.f. of market price δ given x.
The p.d.f. of market price δ.
V (t, b, x)
The expected total reward with starting state (t, b, x),
taking the optimal policy.
The expected total reward with starting state (t, b),
taking the optimal policy.
a(t, b, x)
The optimal action in state (t, b, x).
be represented by a tuple (S, {As}, {P a
ss′}), where
S and As are two sets of all states and all possible actions
in state s ∈S, respectively. P a
ss′ represents the state transition probability from state s ∈S to another state s′ ∈S
when taking action a ∈As, which is denoted by µ(a, s, s′).
Similarly, Ra
ss′ is the reward function denoted by r(a, s, s′)
that represents the reward received after taking action a in
state s and then transiting to state s′.
We consider (t, b, xt) as a state s1 and assume the feature vector xt is drawn i.i.d. from the probability density
function px(x). The full state space is S = {0, · · · , T} ×
{0, · · · , B} × X. And if t = 0, the state is regarded as a
terminal state which means the end of the episode.
set of all actions available in state (t, b, xt) is A(t,b,xt) =
{0, · · · , b}, corresponding to the bid price. Furthermore, in
state (t, b, xt) where t > 0, the agent, when bidding a, can
transit to (t−1, b−δ, xt−1) with probability px(xt−1)m(δ, xt)
where δ ∈{0, · · · , a} and xt−1 ∈X. That is the case of
winning the auction and receiving a reward θ(xt).
the agent may lose the auction whereafter transit to (t −
1, b, xt−1) with probability px(xt−1) P∞
δ=a+1 m(δ, xt), where
xt−1 ∈X. All other transitions are impossible because of
the auction process.
In summary, transition probabilities
and reward function can be written as:
a, (t, b, xt), (t −1, b −δ, xt−1)
= px(xt−1)m(δ, xt),
a, (t, b, xt), (t −1, b, xt−1)
= px(xt−1)
a, (t, b, xt), (t −1, b −δ, xt−1)
a, (t, b, xt), (t −1, b, xt−1)
where δ ∈{0, · · · , a}, xt−1 ∈X and t > 0. Speciﬁcally, the
ﬁrst equation is the transition when giving a bid price a ≥δ,
while the second equation is the transition when losing the
A deterministic policy π is a mapping from each state s ∈
S to action a ∈As, i.e., a = π(s), which corresponds to the
bidding strategy in RTB display advertising. According to
the policy π, we have the value function V π(s): the expected
sum of rewards upon starting in state s and obeying policy
This satisﬁes the Bellman equation with the discount
factor γ = 1 since in our scenario the total click number is
the optimization target, regardless of the click time.
µ(π(s), s, s′)
r(π(s), s, s′) + V π(s′)
The optimal value function is deﬁned as V ∗(s) = maxπ V π(s).
1For simplicity, we slightly abuse the notation by including
t in the state.
We also have the optimal policy as:
π∗(s) = argmax
µ(a, s, s′)
r(a, s, s′) + V ∗(s′)
which gives the optimal action at each state s and V ∗(s) =
The optimal policy π∗(s) is exactly the optimal
bidding strategy we want to ﬁnd. For notation simplicity,
in later sections, we use V (s) to represent the optimal value
function V ∗(s).
One may consider the possibility of model-free approaches
 to directly learn the bidding policy from experience.
However, such model-free approaches may suﬀer from the
problems of transition dynamics of the enormous state space,
the sparsity of the reward signals and the highly stochastic
environment. Since there are many previous works on modeling the utility (reward) and the market price distribution
(transition probability) as discussed in Section 2, we take
advantage of them and propose our model-based solution
for this problem.
DYNAMIC PROGRAMMING SOLUTION
In a small scale, Eq. (3) can be solved using a dynamic
programming approach.
As deﬁned, we have the optimal
value function V (t, b, x), where (t, b, x) represents the state
s. Meanwhile, we consider the situations where we do not
observe the feature vector x; so another optimal value function is V (t, b): the expected total reward upon starting in
(t, b) without observing the feature vector x when the agent
takes the optimal policy.
It satisﬁes V (t, b) =
V (t, b, x) dx. Also that, we have the optimal policy π∗and
express it as the optimal action a(t, b, x).
From the deﬁnition, we have V (0, b, x) = V (0, b) = 0 as
the agent gets nothing when there are no remaining auctions.
Combined with the transition probability and reward function described in Eq. (1), the deﬁnition of V (t, b), V (t, b, x)
can be expressed with V (t −1, ·) as
V (t, b, x) = max
m(δ, x)px(xt−1) ·
θ(x) + V (t −1, b −δ, xt−1)
m(δ, x)px(xt−1)V (t −1, b, xt−1) dxt−1
θ(x) + V (t −1, b −δ)
m(δ, x)V (t −1, b)
where the ﬁrst summation2 is for the situation when winning
the auction and the second summation is that when losing.
Similarly, the optimal action in state (t, b, x) is
a(t, b, x) = argmax
θ(x) + V (t −1, b −δ)
m(δ, x)V (t −1, b)
2In practice, the bid prices in various RTB ad auctions are
required to be integer.
where the optimal bid action a(t, b, x) involves three terms:
m(δ, x), θ(x) and V (t −1, ·). V (t, b) is derived by marginalizing out x:
V (t, b) =
θ(x) + V (t −1, b −δ)
m(δ, x)V (t −1, b)
px(x)m(δ, x)θ(x) dx +
V (t −1, b −δ) ·
px(x)m(δ, x) dx + V (t −1, b)
px(x)m(δ, x) dx
px(x)m(δ, x)θ(x) dx +
m(δ)V (t −1, b −δ) + V (t −1, b)
To settle the integration over x in Eq. (6), we consider an
approximation m(δ, x) ≈m(δ) by following the dependency
assumption x →θ →a →w(winning rate) in . Thus
px(x)m(δ, x)θ(x) dx ≈m(δ)
px(x)θ(x) dx
= m(δ)θavg ,
where θavg is the expectation of the pCTR θ, which can
be easily calculated with historical data.
Taking Eq. (7)
into Eq. (6), we get an approximation of the optimal value
function V (t, b):
V (t, b) ≈max
m(δ)θavg +
m(δ)V (t −1, b −δ) +
m(δ)V (t −1, b)
Noticing that P∞
δ=0 m(δ, x) = 1, Eq. (5) is rewritten as
a(t, b, x) = argmax
θ(x) + V (t −1, b −δ)
m(δ, x)V (t −1, b)
θ(x) + V (t −1, b −δ) −V (t −1, b)
m(δ, x)g(δ)
where we denote g(δ) = θ(x) + V (t −1, b −δ) −V (t −1, b).
From the deﬁnition, we know V (t −1, b) monotonically increases w.r.t. b, i.e., V (t −1, b) ≥V (t −1, b′) where b ≥
b′. As such, V (t −1, b −δ) monotonically decreases w.r.t.
δ. Thus g(δ) monotonically decreases w.r.t. δ. Moreover,
g(0) = θ(x) ≥0 and m(δ, x) ≥0. Here, we care about the
value of g(b). (i) If g(b) ≥0, then g(b′) ≥g(b) ≥0 where
0 ≤b′ ≤b, so m(δ, x)g(δ) ≥0 where 0 ≤δ ≤b.
result, in this case, we have a(t, b, x) = b. (ii) If g(b) < 0,
then there must exist an integer A such that 0 ≤A < b and
g(A) ≥0, g(A+1) < 0. So m(δ, x)g(δ) ≥0 when δ ≤A and
g(δ) (×10−3)
g(δ) on campaign 2821
under t = 2500 and b = 10000
2θavg, a = 15
θ(x) = θavg, a = 30
θ(x) = 2θavg, a = 61
g(δ) (×10−3)
g(δ) on campaign 2821
under t = 5000 and b = 20000
2θavg, a = 15
θ(x) = θavg, a = 30
θ(x) = 2θavg, a = 61
Figure 2: g(δ) on campaign 2821 as an example.
Algorithm 1 Reinforcement Learning to Bid
Input: p.d.f. of market price m(δ), average CTR θavg, episode
length T, budget B
Output: value function V (t, b)
1: initialize V (0, b) = 0
2: for t = 1, 2, · · · , T −1 do
for b = 0, 1, · · · , B do
enumerate at,b from 0 to min(δmax, b) and set V (t, b) via
6: end for
Input: CTR estimator θ(x), value function V (t, b), current state
(tc, bc, xc)
Output: optimal bid price ac in current state
1: calculate the pCTR for the current bid request: θc = θ(xc)
2: for δ = 0, 1, · · · , min(δmax, bc) do
if θc + V (tc −1, bc −δ) −V (tc −1, bc) ≥0 then
6: end for
m(δ, x)g(δ) < 0 when δ > A. Consequently, in this case, we
have a(t, b, x) = A. In conclusion, we have
a(t, b, x) =
if g(b) ≥0
g(A) ≥0 and g(A + 1) < 0
if g(b) < 0 . (10)
Figure 2 shows examples of g(δ) on campaign 2821 from
iPinYou real-world dataset. Additionally, we usually have
a maximum market price δmax, which is also the maximum
bid price. The corresponding RLB algorithm is shown in
Algorithm 1.
Discussion on Derived Policy. Contrary to the linear
bidding strategies which bids linearly w.r.t. the pCTR with
a static parameter , such as Mcpc and Lin discussed in
Section 5.3, our derived policy (denoted as RLB) adjusts its
bidding function according to current t and b. As shown in
Figure 3, RLB also introduces a linear form bidding function
when b is large, but decreases the slope w.r.t. decreasing b
and increasing t. When b is small (such as b < 300), RLB
introduces a non-linear concave form bidding function.
Discussion on the Approximation of V (t, b).
(7), we take the approximation m(δ, x) ≈m(δ) by following
the dependency assumption x →θ →a →w(winning rate)
in and consequently get an approximation of the optimal value function V (t, b) in Eq.
Here, we consider
a more general case where such assumption does not hold
in the whole feature vector space X, but holds within each
individual subset. Suppose X can be explicitly divided into
several segments, i.e., X = ⊔iXi. The segmentation can
be built by publisher, user demographics etc. For each segment Xi, we take the approximation m(δ, x) ≈mi(δ) where
a(100, b, x)
a(t, 1000, x)
Figure 3: Example derived bidding functions.
x ∈Xi. As such, we have
px(x)m(δ, x)θ(x) dx =
px(x)m(δ, x)θ(x) dx
px(x)θ(x) dx =
mi(δ)(θavg)iP(x ∈Xi).
Handling Large-Scale Issues
Algorithm 1 gives a solution to the optimal policy. However, when it comes to the real-world scale, we should also
consider the complexity of the algorithm. Algorithm 1 consists of two stages. The ﬁrst one is about updating the value
function V (t, b), while the second stage is about taking the
optimal action for current state based on V (t, b). We can
see that the main complexity is on the ﬁrst stage. Thus we
focus on the ﬁrst stage in this section. Two nested loops
in the ﬁrst stage lead the time complexity to O(TB). As
for the space complexity, we need to use a two-dimensional
table to store V (t, b), which will later be used when taking
action. Thus the space complexity is O(TB).
In consideration of the space complexity and the time
complexity, Algorithm 1 can only be applied to small-scale
situations. When we confront the situation where T and B
are very large, which is a common case in real world, there
will probably be not enough resource to get the exact value
of V (t, b) for every (t, b) ∈{0, · · · , T} × {0, · · · , B}.
With restricted computational resources, one may not be
able to go through the whole value function update. Thus
we propose some parameterized models to ﬁt the value function on small data scale, i.e., {0, · · · , T0} × {0, · · · , B0}, and
generalize to the large data scale {0, · · · , T} × {0, · · · , B}.
Good parameterized models are supposed to have low
deviation to the exact value of V (t, b) for every (t, b) ∈
{0, · · · , T} × {0, · · · , B}. That means low root mean square
error (RMSE) in the training data and good generalization
Basically, we expect the prediction error of θ(x) + V (t −
1, b −δ) −V (t −1, b) from Eq. (9) in the training data to be
low in comparison to the average CTR θavg. For most (t, b),
V (t, b) is much larger than θavg. For example, if the budget
b is large enough, V (t, b) is with the same scale of t × θavg.
Therefore, if we take V (t, b) as our target to approximate,
it is diﬃcult to give a low deviation in comparison to θavg.
Actually, when calculating a(t, b, x) in Eq. (9), we care about
the value of V (t−1, b−δ)−V (t−1, b) rather than V (t−1, b−
δ) or V (t −1, b). Thus here we introduce a new function of
value diﬀerential D(t, b) = V (t, b+1)−V (t, b) to replace the
D(t, b) (×10−5)
Figure 4: D(t, b) and V (t, b) on campaign 3427.
0 2 4 6 8 1012
D(1000, b) (×10−5)
Dt(b) (×10−5)
D(1000, b)
D(4500, b)
Db(t) (×10−6)
D(t, 15000)
D(t, 20000)
D(t, 30000)
Figure 5: Analysis of D(t, b) on campaign 3386.
role of V (t, b) by
V (t −1, b −δ) −V (t −1, b) = −
D(t −1, b −δ′).
Figure 4 illustrates the value of D(t, b) and V (t, b) on the
data of an example campaign. In Figure 5, we use the campaign 3386 from iPinYou real-world dataset as an example and show some interesting observations of D(t, b) (other
campaigns are similar). At ﬁrst, for a given t, we consider
D(t, b) as a function of b and denote it as Dt(b). Dt(b) ﬂuctuates heavily when b is very small, and later keeps decreasing
to 0. Similarly, for a given b, we have Db(t) as a function of
t and it keeps increasing. Moreover, both Dt(b) and Db(t)
are obviously nonlinear. Consequently, we apply the neural
networks to approximate them for large-scale b and t.
As a widely used solution , here we take a fully connected neural network with several hidden layers as a nonlinear approximator. The input layer has two nodes for t and
b. The output layer has one node for D(t, b) without activation function. As such, the neural network corresponds to a
non-linear function of t and b, denoted as NN(t, b).
Coarse-to-ﬁne Episode Segmentation Model.
the neural networks do not guarantee good generalization
ability and may suﬀer from overﬁtting, and also to avoid
directly modeling D(t, b) or V (t, b), we explore the feasibility of mapping unseen states (t > T0 and b > B0) to
acquainted states (t ≤T0 and b ≤B0) rather than giving a
global parameterized representation. Similar to budget pacing, we have the ﬁrst simple implicit mapping method where
we can divide the large episode into several small episodes
with length T0 and within each large episode we allocate
the remaining budget to the remaining small episodes. If
the agent does not spend the budget allocated for the small
episode, it will have more allocated money for the rest of the
small episodes in the large episode.
Dev /θavg (×10−4)
Dev /θavg (×10−4)
Figure 6: Dev(t, T0, b) on campaign 3427 as an example.
State Mapping Models. Also we consider explicit mapping methods. At ﬁrst, because Dt(b) keeps decreasing and
Db(t) keeps increasing, then for D(t, b) where t and b are
large, there should be some points {(t′, b′)} where t′ ≤T0
and b′ ≤B0 such that D(t′, b′) = D(t, b) as is shown in
Figure 4, which conﬁrms the existence of the mapping for
D(t, b). Similarly, a(t, b, x) decreases w.r.t. t and increases
b, which can be seen in Figure 3 and is consistent
with intuitions.
Thus the mapping for a(t, b, x) also exists. From the view of practical bidding, when the remaining number of auctions are large and the budget situation
is similar, given the same bid request, the agent should
give a similar bid price (see Figure 2). We consider a simple case that b/t represents the budget condition.
here we have two linear mapping forms: (i) map a(t, b, x)
where t > T0 to a(T0, b
t × T0, x).
(ii) map D(t, b) where
t > T0 to D(T0, b
t × T0). Denote |D(t, b) −D(T0, b
as Dev(t, T0, b). Figure 6 shows that the deviations of the
simple linear mapping method are low enough (< 10−3θavg).
EXPERIMENTAL SETUP
Two real-world datasets are used in our experimental study,
namely iPinYou and YOYI.
iPinYou is one of the mainstream RTB ad companies in
The whole dataset comprises 19.5M impressions, 14.79K clicks and 16.0K CNY expense on 9 different campaigns over 10 days in 2013. We follow 
for splitting the train/test sets and feature engineering.
YOYI is a leading RTB company focusing on multi-device
display advertising in China. YOYI dataset comprises
441.7M impressions, 416.9K clicks and 319.5K CNY
expense during 8 days in Jan. 2016. The ﬁrst 7 days
are set as the training data while the last day is set as
the test data.
For experiment reproducibility we publicize our code3. In
the paper we mainly report results on iPinYou dataset, and
further verify our algorithms over the YOYI dataset as supplementary.
3The experiment code is available at 
han-cai/rlb-dp and iPinYou dataset is available at http://
data.computational-advertising.org.
Evaluation Methods
The evaluation is from the perspective of an advertiser’s
campaign with a predeﬁned budget and lifetime (episode
Evaluation metrics. The main goal of the bidding agent
is to optimise the campaign’s KPI (e.g., clicks, conversions,
revenue, etc.) given the campaign budget. In our work, we
consider the number of acquired clicks as the KPI, which is
set as the primary evaluation measure in our experiments.
We also analyze other statistics such as win rate, cost per
mille impressions (CPM) and eﬀective cost per click (eCPC).
Evaluation ﬂow. We mostly follow when building the
evaluation ﬂow, except that we divide the test data into
episodes. Speciﬁcally, the test data is a list of records, each
of which consists of the bid request feature vector, the market price and the user response (click) label. We divide the
test data into episodes, each of which contains T records
and is allocated with a budget B. Given the CTR estimator
and the bid landscape forecasting, the bidding strategy goes
through the test data episode by episode. Speciﬁcally, the
bidding strategy generates a price for each bid request (the
bid price cannot exceed current budget). If the bid price is
higher than or equal to the market price of the bid request,
the advertiser wins the auction and then receives the market
price as cost and the user click as reward and then updates
the remaining auction number and budget.
Budget constraints. Obviously, if the allocated budget
B is too high, the bidding strategy can simply give a very
high bid price each time to win all clicks in the test data.
Therefore, in evaluation, budget constraints should not be
higher than the historic total cost of the test data.
determine the budget B in this way: B = CPMtrain×10−3×
T × c0, where CPMtrain is the cost per mille impressions
in the training data and c0 acts as the budget constraints
parameter.
Following previous work , we run the
evaluation with c0 = 1/32, 1/16, 1/8, 1/4, 1/2.
Episode length. The episode auction number T inﬂuences
the complexity of our algorithms. When T is high, the original Algorithm 1 is not capable of working with limited resources, which further leads to our large-scale algorithms.
For the large-scale evaluation, we set T as 100,000, which
corresponds to a real-world 10-minute auction volume of a
medium-scale RTB ad campaign. And for the small-scale
evaluation, we set the episode length as 1,000. In addition,
we run a set of evaluations with c0 = 0.2 and the episode
length T = 200, 400, 600, 800, 1000 to give a more comprehensive performance analysis.
Compared Methods
The following bidding policies are compared with the same
CTR estimation component which is a logistic regression
model and the same bid landscape forecasting component
which is a non-parametric method, as described in Section
SS-MDP is based on , considering the bid landscape but
ignoring the feature vector of bid request when giving
the bid price. Although we regard this model as the
state-of-the-art, it is proposed to work on keywordlevel bidding in sponsored search, which makes it not
ﬁne-grained enough to compare with RTB display advertising strategies.
Overall performance on iPinYou under
T = 103 and diﬀerent budget conditions.
Mcpc gives its bidding strategy as aMcpc(t, b, x) = CPC ×
θ(x), which matches some advertisers’ requirement of
maximum CPC (cost per click).
Lin is a linear bidding strategy w.r.t. the pCTR: aLin(t, b, x)
θavg , where b0 is the basic bid price and is tuned
using the training data . This is the most widely
used model in industry.
RLB is our proposed model for the small-scale problem as
shown in Algorithm 1.
RLB-NN is our proposed model for the large-scale problem, which uses the neural network NN(t, b) to approximate D(t, b).
RLB-NN-Seg combines the neural network with episode
segmentation.
For each small episode, the allocated
budget is Bs = Br/Nr where Br is the remaining
budget of the current large episode and Nr is the remaining number of small episodes in the current large
episode. Then RLB-NN is run for the small episode. It
corresponds to the coarse-to-ﬁne episode segmentation
model discussed in Section 4.1.
RLB-NN-MapD combines the neural network with the
mapping of D(t, b).
That is: (i) D(t, b) = NN(t, b)
where t ≤T0.
(ii) D(t, b) = NN(T0, b
t × T0) where
RLB-NN-MapA combines the neural network with the
mapping of a(t, b, x). That is: a(t, b, x) = a(T0, b
T0, x) where t > T0. The last two models correspond
to the state mapping models discussed in Section 4.1.
EXPERIMENTAL RESULTS
In this section we present the experimental results on
small- and large-scale data settings respectively.
Click improvement of RLB over Lin for
each campaign under T = 103 and diﬀerent budget
conditions.
Small-Scale Evaluation
The performance comparison on iPinYou dataset under
T = 1000 and diﬀerent budget conditions are reported in
Figure 7. In the comparison on total clicks (upper left plot),
we ﬁnd that (i) our proposed model RLB performs the best
under every budget condition, verifying the eﬀectiveness of
the derived algorithm for optimizing attained clicks. (ii) Lin
has the second best performance, which is a widely used
bidding strategy in industry . (iii) Compared to RLB
and Lin, Mcpc does not adjust its strategy when the budget
condition changes. Thus it performs quite well when c0 ≥
1/4 but performs poorly on very limited budget conditions,
which is consistent with the discussion in Section 2.
SS-MDP gives the worst performance, since it is unaware
of the feature information of each bid request, which shows
the advantages of RTB display advertising.
As for the comparison on win rate, CPM and eCPC, we
observe that (i) under every budget condition, SS-MDP
keeps the highest win rate.
The reason is that SS-MDP
considers each bid request equally, thus its optimization target is equivalent to the number of impressions. Therefore,
its win rate should be the highest. (ii) Lin and RLB are
very close in comparison on CPM and eCPC. RLB can generate a higher number of clicks with comparable CPM and
eCPC against Lin because RLB eﬀectively spends the budget according to the market situation, which is unaware of
Table 2 provides a detailed performance on clicks of RLB
over Lin under various campaigns and budget conditions.
Among all 50 settings, RLB wins Lin in 46 (92%), ties in
1 (2%) and loses in 3 (6%) settings. It shows that RLB is
robust and signiﬁcantly outperforms Lin in the vast majority of the cases. Speciﬁcally, for 1/8 budget, RLB outperforms Lin by 16.7% on iPinYou data and 7.4% on YOYI
data. Moreover, Figure 8 shows the performance comparison under the same budget condition (c0 = 0.2) and diﬀerent episode lengths. The ﬁndings are similar to the above
results. Compared to Lin, RLB can attain more clicks with
similar eCPC. Note that, in oﬄine evaluations the total auction number is stationary, larger episode length also means
smaller episode number.
Thus the total click numbers in
Figure 8 do not increase largely w.r.t. T.
SS-MDP is the only model that ignores the feature information of the bid request, thus providing a poor overall
performance. Table 3 reports in detail the clicks along with
the AUC of the CTR estimator for each campaign. We ﬁnd
performance
comparison
iPinYou under c0 = 0.2 and diﬀerent T’s.
Table 3: Detailed AUC and clicks (T = 103 and c0 =
AUC of θ(x)
that when the performance of the CTR estimator is relatively low (AUC < 70%), e.g., campaign 2259, 2261, 2821,
2997, the performance of SS-MDP on clicks is quite good
in comparison to Mcpc and Lin.
By contrast, when the
performance of the CTR estimator gets better, other methods which utilize the CTR estimator can attain much more
clicks than SS-MDP.
Large-Scale Evaluation
In this section, we ﬁrst run the value function update in
Algorithm 1 under T0 = 10, 000 and B0 = CPMtrain×10−3×
T0 ×1/2, then train a neural network with the attained data
(t, b, D(t, b)) (where (t, b) ∈{0, · · · , T0}×{0, · · · , B0}). Here
we use a fully connected neural network with two hidden
layers which use tanh activation function. The ﬁrst hidden
layer has 30 hidden nodes and the second one has 15 hidden
nodes. Next, we apply the neural network to run bidding
under T = 100, 000 and B = CPMtrain × 10−3 × T × c0. In
addition, SS-MDP is not tested in this experiment because
it suﬀers from scalability issues and will have a similarly low
performance as in the small-scale evaluation.
Table 4 shows the performance of the neural network on
iPinYou and YOYI. We can see that the RMSE is relatively low in comparison to θavg, which means that the neural network can provide a good approximation to the exact
algorithm when the agent comes to a state (t, b, x) where
(t, b) ∈{0, · · · , T0} × {0, · · · , B0}.
Figure 9 shows the performance comparison on iPinYou
under T = 100, 000 and diﬀerent budget conditions.
observe that (i) Mcpc has a similar performance to that observed in small-scale situations. (ii) For total clicks, RLB-
NN performs better than Lin under c0 = 1/32, 1/16, 1/8
and performs worse than Lin under c0 = 1/2, which shows
Table 4: Approximation performance of the neural
RMSE (×10−6)
RMSE / θavg (×10−4)
Overall performance on iPinYou under
T = 105 and diﬀerent budget conditions.
that the generalization ability of the neural network is satisfactory only in small scales.
For relatively large scales,
the generalization of RLB-NN is not reliable.
(iii) Compared to RLB-NN, the 3 sophisticated algorithms RLB-
NN-Seg, RLB-NN-MapD and RLB-NN-MapA are more
robust and outperform Lin under every budget condition.
They do not rely on the generalization ability of the approximation model, therefore their performance is more stable.
The results clearly demonstrate that they are eﬀective solutions for the large-scale problem. (iv) As for eCPC, all
models except from Mcpc are very close, thus making the
proposed RLB algorithms practically eﬀective.
ONLINE DEPLOYMENT AND A/B TEST
Our proposed RLB model is deployed and tested in a live
environment provided by Vlion DSP. The deployment environment is based on HP ProLiant DL360p Gen8 servers. A
5-node cluster is utilized for the bidding agent, where each
node is in CentOS release 6.3, with 6 core Intel Xeon CPU
E5-2620 (2.10GHz) and 64GB RAM. The model is implemented in Lua with Nginx.
The compared bidding strategy is Lin as discussed in Section 5.3. The optimization target is click. The two compared
methods are given the same budget, which is further allocated to episodes. Unlike oﬄine evaluations, the online evaluation ﬂow stops only when the budget is exhausted. Within
an episode, a maximum bid number T is set for each strategy
to prevent overspending too much. Speciﬁcally, T is mostly
determined by the allocated budget for the episode B, previous CPM and win rate: T = B/CPM/win rate × 103.
The possible available auction number during the episode is
also considered when determining T. The agent keeps the
remaining bid number and budget, which we consider as t
and b respectively.
Note that the remaining budget may
have some deviation due to latency. The latency is typically
less than 100ms, which is negligible. We test over 5 campaigns during 25-28th of July, 2016. All the methods share
the same previous 7-day training data, and the same CTR
estimator which is a logistic regression model trained with
FTRL. The bid requests of each user are randomly sent to
either method. The overall results are presented in Figure
Bids (×106)
4.0 Impressions (×105)
Total Clicks
eCPC (CNY)
Figure 10: Online A/B testing results.
Total Clicks
Cost (CNY)
Figure 11:
Total clicks and cost increase over
10, while the click and cost performances w.r.t. time are
shown in Figure 11.
From the comparison, we observe the following: (i) with
the same cost, RLB achieves lower eCPC than Lin, and
thus more total clicks, which shows the cost eﬀectiveness
of RLB. (ii) RLB provides better planning than Lin: the
acquired clicks and spent budget increase evenly across the
time. (iii) With better planning, RLB obtains lower CPM
than Lin, yielding more bids and more winning impressions.
(iv) With lower CPM on cheap cases, RLB achieves a close
CTR compared to Lin, which leads to superior performance.
In summary, the online evaluation demonstrates the eﬀectiveness of our proposed RLB model for optimizing attained
clicks with a good pacing.
CONCLUSIONS
In this paper, we proposed a model-based reinforcement
learning model (RLB) for learning the bidding strategy in
RTB display advertising. The bidding strategy is naturally
deﬁned as the policy of making a bidding action given the
state of the campaign’s parameters and the input bid request
information. With an MDP formulation, the state transition
and reward function are captured via modeling the auction
competition and user click, respectively. The optimal bidding policy is then derived using dynamic programming.
Furthermore, to deal with the large-scale auction volume
and campaign budget, we proposed neural network models
to ﬁt the diﬀerential of the values between two consecutive
states. Experimental results on two real-world large-scale
datasets and online A/B test demonstrated the superiority
of our RLB solutions over several strong baselines and stateof-the-art methods, as well as their high eﬃciency to handle
large-scale data.
For future work, we will investigate model-free approaches
such as Q-learning and policy gradient methods to unify
utility estimation, bid landscape forecasting and bid optimization into a single optimization framework and handle
the highly dynamic environment. Also, since RLB naturally
tackles the problem of budget over- or under-spending across
the campaign lifetime, we will compare our RLB solutions
with the explicit budget pacing techniques .
ACKNOWLEDGMENTS
We sincerely thank the engineers from YOYI DSP to provide
us the oﬄine experiment dataset and the engineers from
Vlion DSP to help us conduct online A/B tests.