KPConv: Flexible and Deformable Convolution for Point Clouds
Hugues Thomas1
Charles R. Qi2
Jean-Emmanuel Deschaud1
Beatriz Marcotegui1
Franc¸ois Goulette1
Leonidas J. Guibas2,3
1Mines ParisTech
2Facebook AI Research
3Stanford University
We present Kernel Point Convolution1 (KPConv), a new
design of point convolution, i.e.
that operates on point
clouds without any intermediate representation. The convolution weights of KPConv are located in Euclidean space by
kernel points, and applied to the input points close to them.
Its capacity to use any number of kernel points gives KP-
Conv more ﬂexibility than ﬁxed grid convolutions. Furthermore, these locations are continuous in space and can be
learned by the network. Therefore, KPConv can be extended
to deformable convolutions that learn to adapt kernel points
to local geometry. Thanks to a regular subsampling strategy, KPConv is also efﬁcient and robust to varying densities.
Whether they use deformable KPConv for complex tasks, or
rigid KPconv for simpler tasks, our networks outperform
state-of-the-art classiﬁcation and segmentation approaches
on several datasets.
We also offer ablation studies and
visualizations to provide understanding of what has been
learned by KPConv and to validate the descriptive power
of deformable KPConv.
1. Introduction
The dawn of deep learning has boosted modern computer
vision with discrete convolution as its fundamental building
block. This operation combines the data of local neighborhoods on a 2D grid. Thanks to this regular structure, it can
be computed with high efﬁciency on modern hardware, but
when deprived of this regular structure, the convolution operation has yet to be deﬁned properly, with the same efﬁciency as on 2D grids.
Many applications relying on such irregular data have
grown with the rise of 3D scanning technologies. For example, 3D point cloud segmentation or 3D simultaneous
localization and mapping rely on non-grid structured data:
point clouds. A point cloud is a set of points in 3D (or
higher-dimensional) space. In many applications, the points
1Project page: https:// github.com/ HuguesTHOMAS/ KPConv
are coupled with corresponding features like colors. In this
work, we will always consider a point cloud as those two elements: the points P ∈RN×3 and the features F ∈RN×D.
Such a point cloud is a sparse structure that has the property
to be unordered, which makes it very different from a grid.
However, it shares a common property with a grid which
is essential to the deﬁnition of convolutions: it is spatially
localized. In a grid, the features are localized by their index in a matrix, while in a point cloud, they are localized by
their corresponding point coordinates. Thus, the points are
to be considered as structural elements, and the features as
the real data.
Various approaches have been proposed to handle such
data, and can be grouped into different categories that we
will develop in the related work section.
Several methods fall into the grid-based category, whose principle is to
project the sparse 3D data on a regular structure where a
convolution operation can be deﬁned more easily . Other approaches use multilayer perceptrons (MLP) to
process point clouds directly, following the idea proposed
by .
More recently, some attempts have been made to design
a convolution that operates directly on points . These methods use the spatial localization property of
Figure 1. KPConv illustrated on 2D points. Input points with a
constant scalar feature (in grey) are convolved through a KPConv
that is deﬁned by a set of kernel points (in black) with ﬁlter weights
on each point.
 
a point cloud to deﬁne point convolutions with spatial kernels. They share the idea that a convolution should deﬁne a
set of customizable spatial ﬁlters applied locally in the point
This paper introduces a new point convolution operator
named Kernel Point Convolution (KPConv). KPConv also
consists of a set of local 3D ﬁlters, but overcomes previous
point convolution limitations as shown in related work. KP-
Conv is inspired by image-based convolution, but in place
of kernel pixels, we use a set of kernel points to deﬁne
the area where each kernel weight is applied, like shown
in Figure 1. The kernel weights are thus carried by points,
like the input features, and their area of inﬂuence is deﬁned
by a correlation function. The number of kernel points is
not constrained, making our design very ﬂexible. Despite
the resemblance of vocabulary, our work differs from ,
which is inspired from point cloud registration techniques,
and uses kernel points without any weights to learns local
geometric patterns.
Furthermore, we propose a deformable version of our
convolution , which consists of learning local shifts applied to the kernel points (see Figure 3). Our network generates different shifts at each convolution location, meaning
that it can adapt the shape of its kernels for different regions of the input cloud. Our deformable convolution is
not designed the same way as its image counterpart. Due
to the different nature of the data, it needs a regularization
to help the deformed kernels ﬁt the point cloud geometry
and avoid empty space. We use Effective Receptive Field
(ERF) and ablation studies to compare rigid KPConv
with deformable KPConv.
As opposed to , we favor radius neighborhoods instead of k-nearest-neighbors (KNN). As shown by
 , KNN is not robust in non-uniform sampling settings.
The robustness of our convolution to varying densities is
ensured by the combination of radius neighborhoods and
regular subsampling of the input cloud . Compared to
normalization strategies , our approach also alleviates the computational cost of our convolution.
In our experiments section, we show that KPConv can
be used to build very deep architectures for classiﬁcation
and segmentation, while keeping fast training and inference times. Overall, rigid and deformable KPConv both
perform very well, topping competing algorithms on several datasets. We ﬁnd that rigid KPConv achieves better
performances on simpler tasks, like object classiﬁcation, or
small segmentation datasets. Deformable KPConv thrives
on more difﬁcult tasks, like large segmentation datasets offering many object instances and greater diversity. We also
show that deformable KPConv is more robust to a lower
number of kernel points, which implies a greater descriptive power. Last but not least, a qualitative study of KPConv
ERF shows that deformable kernels improve the network
ability to adapt to the geometry of the scene objects.
2. Related Work
In this section, we brieﬂy review previous deep learning
methods to analyze point clouds, paying particular attention
to the methods closer to our deﬁnition of point convolutions.
Projection networks. Several methods project points to an
intermediate grid structure. Image-based networks are often multi-view, using a set of 2D images rendered from the
point cloud at different viewpoints . For scene
segmentation, these methods suffer from occluded surfaces
and density variations. Instead of choosing a global projection viewpoint, proposed projecting local neighborhoods to local tangent planes and processing them with 2D
convolutions. However, this method relies heavily on tangent estimation.
In the case of voxel-based methods, the points are projected on 3D grids in Euclidean space . Using
sparse structures like octrees or hash-maps allows larger
grids and enhanced performances , but these networks still lack ﬂexibility as their kernels are constrained
to use 33 = 27 or 53 = 125 voxels. Using a permutohedral
lattice instead of an Euclidean grid reduces the kernel to
15 lattices , but this number is still constrained, while
KPConv allows any number of kernel points. Moreover,
avoiding intermediate structures should make the design of
more complex architectures like instance mask detector or
generative models more straightforward in future works.
Graph convolution networks. The deﬁnition of a convolution operator on a graph has been addressed in different
ways. A convolution on a graph can be computed as a multiplication on its spectral representation , or it can focus on the surface represented by the graph .
Despite the similarity between point convolutions and the
most recent graph convolutions , the latter learn ﬁlters on edge relationships instead of points relative positions. In other words, a graph convolution combines features on local surface patches, while being invariant to the
deformations of those patches in Euclidean space. In contrast, KPConv combines features locally according to the
3D geometry, thus capturing the deformations of the surfaces.
Pointwise MLP networks.
PointNet is considered
a milestone in point cloud deep learning.
This network
uses a shared MLP on every point individually followed
by a global max-pooling. The shared MLP acts as a set of
learned spatial encodings and the global signature of the input point cloud is computed as the maximal response among
all the points for each of these encodings. The network’s
performances are limited because it does not consider local
spatial relationships in the data. Following PointNet, some
hierarchical architectures have been developed to aggregate
local neighborhood information with MLPs .
Figure 2. Comparison between an image convolution (left) and a KPConv (right) on 2D points for a simpler illustration. In the image, each
pixel feature vector is multiplied by a weight matrix (Wk)k<K assigned by the alignment of the kernel with the image. In KPConv, input
points are not aligned with kernel points, and their number can vary. Therefore, each point feature fi is multiplied by all the kernel weight
matrices, with a correlation coefﬁcient hik depending on its relative position to kernel points.
As shown by , the kernel of a point convolution can be implemented with a MLP, because of its ability
to approximate any continuous function. However, using
such a representation makes the convolution operator more
complex and the convergence of the network harder. In our
case, we deﬁne an explicit convolution kernel, like image
convolutions, whose weights are directly learned, without
the intermediate representation of a MLP. Our design also
offers a straightforward deformable version, as offsets can
directly be applied to kernel points.
Point convolution networks. Some very recent works also
deﬁned explicit convolution kernels for points, but KPConv
stands out with unique design choices.
Pointwise CNN locates the kernel weights with
voxel bins, and thus lacks ﬂexibility like grid networks.
Furthermore, their normalization strategy burdens their network with unnecessary computations, while KPConv subsampling strategy alleviates both varying densities and computational cost.
SpiderCNN deﬁnes its kernel as a family of polynomial functions applied with a different weight for each
neighbor. The weight applied to a neighbor depends on the
neighbor’s distance-wise order, making the ﬁlters spatially
inconsistent. By contrast, KPConv weights are located in
space and its result is invariant to point order.
Flex-convolution uses linear functions to model its
kernel, which could limit its representative power. It also
uses KNN, which is not robust to varying densities as discussed above.
PCNN design is the closest to KPConv. Its deﬁnition
also uses points to carry kernel weights, and a correlation
function. However, this design is not scalable because it
does not use any form of neighborhood, making the convolution computations quadratic on the number of points. In
addition, it uses a Gaussian correlation where KPConv uses
a simpler linear correlation, which helps gradient backpropagation when learning deformations .
We show that KPConv networks outperform all comparable networks in the experiments section. Furthermore, to
the best of our knowledge, none of the previous works experimented a spatially deformable point convolution.
3. Kernel Point Convolution
3.1. A Kernel Function Deﬁned by Points
Like previous works, KPConv can be formulated with
the general deﬁnition of a point convolution (Eq. 1), inspired by image convolutions. For the sake of clarity, we
call xi and fi the points from P ∈RN×3 and their corresponding features from F ∈RN×D. The general point
convolution of F by a kernel g at a point x ∈R3 is deﬁned
(F ∗g)(x) =
g(xi −x)fi
We stand with advising radius neighborhoods to
ensure robustness to varying densities, therefore, Nx =
with r ∈R being the chosen
radius. In addition, showed that hand-crafted 3D point
features offer a better representation when computed with
radius neighborhoods than with KNN. We believe that having a consistent spherical domain for the function g helps
the network to learn meaningful representations.
The crucial part in Eq. 1 is the deﬁnition of the kernel function g, which is where KPConv singularity lies. g
takes the neighbors positions centered on x as input. We
call them yi = xi −x in the following. As our neighborhoods are deﬁned by a radius r, the domain of deﬁnition
of g is the ball B3
y ∈R3 | ∥y∥⩽r
. Like image
convolution kernels (see Figure 2 for a detailed comparison between image convolution and KPConv), we want g
to apply different weights to different areas inside this domain. There are many ways to deﬁne areas in 3D space,
and points are the most intuitive as features are also localized by them. Let {exk | k < K} ⊂B3
r be the kernel points
and {Wk | k < K} ⊂RDin×Dout be the associated weight
matrices that map features from dimension Din to Dout.
We deﬁne the kernel function g for any point yi ∈B3
h (yi, exk) Wk
where h is the correlation between exk and yi, that should
be higher when exk is closer to yi. Inspired by the bilinear
interpolation in , we use the linear correlation:
h (yi, exk) = max
0, 1 −∥yi −exk∥
where σ is the inﬂuence distance of the kernel points, and
will be chosen according to the input density (see Section
3.3). Compared to a gaussian correlation, which is used by
 , linear correlation is a simpler representation. We advocate this simpler correlation to ease gradient backpropagation when learning kernel deformations. A parallel can be
drawn with rectiﬁed linear unit, which is the most popular
activation function for deep neural networks, thanks to its
efﬁciency for gradient backpropagation.
3.2. Rigid or Deformable Kernel
Kernel point positions are critical to the convolution operator. Our rigid kernels in particular need to be arranged
regularly to be efﬁcient. As we claimed that one of the KP-
Conv strengths is its ﬂexibility, we need to ﬁnd a regular
disposition for any K. We chose to place the kernel points
by solving an optimization problem where each point applies a repulsive force on the others. The points are constrained to stay in the sphere with an attractive force, and
one of them is constrained to be at the center. We detail
this process and show some regular dispositions in the supplementary material. Eventually, the surrounding points are
rescaled to an average radius of 1.5σ, ensuring a small overlap between each kernel point area of inﬂuence and a good
space coverage.
With properly initialized kernels, the rigid version of KP-
Conv is extremely efﬁcient, in particular when given a large
enough K to cover the spherical domain of g. However it
is possible to increase its capacity by learning the kernel
point positions. The kernel function g is indeed differentiable with respect to exk, which means they are learnable
parameters. We could consider learning one global set of
{exk} for each convolution layer, but it would not bring more
descriptive power than a ﬁxed regular disposition. Instead
the network generates a set of K shifts ∆(x) for every convolution location x ∈R3 like and deﬁne deformable
KPConv as:
(F ∗g)(x) =
gdeform(x −xi, ∆(x))fi
Figure 3. Deformable KPConv illustrated on 2D points.
gdeform(yi, ∆(x)) =
h (yi, exk + ∆k(x)) Wk
We deﬁne the offsets ∆k(x) as the output of a rigid KP-
Conv mapping Din input features to 3K values, as shown
in Figure 3. During training, the network learns the rigid
kernel generating the shifts and the deformable kernel generating the output features simultaneously, but the learning
rate of the ﬁrst one is set to 0.1 times the global network
learning rate.
Unfortunately, this straightforward adaptation of image
deformable convolutions does not ﬁt point clouds. In practice, the kernel points end up being pulled away from the
input points. These kernel points are lost by the network,
because the gradients of their shifts ∆k(x) are null when
no neighbors are in their inﬂuence range. More details on
these “lost” kernel points are given in the supplementary. To
tackle this behaviour, we propose a “ﬁtting” regularization
loss which penalizes the distance between a kernel point and
its closest neighbor among the input neighbors. In addition,
we also add a “repulsive” regularization loss between all
pair off kernel points when their inﬂuence area overlap, so
that they do not collapse together. As a whole our regularization loss for all convolution locations x ∈R3 is:
Lﬁt(x) + Lrep(x)
∥yi −(exk + ∆k(x))∥
h (exk + ∆k(x), exl + ∆l(x))2
With this loss, the network generates shifts that ﬁt the
local geometry of the input point cloud. We show this effect
in the supplementary material.
3.3. Kernel Point Network Layers
This section elucidates how we effectively put the KP-
Conv theory into practice. For further details, we have released our code using Tensorﬂow library.
Subsampling to deal with varying densities. As explained
in the introduction, we use a subsampling strategy to control
the density of input points at each layer. To ensure a spatial
consistency of the point sampling locations, we favor grid
subsampling. Thus, the support points of each layer, carrying the features locations, are chosen as barycenters of the
original input points contained in all non-empty grid cells.
Pooling layer. To create architectures with multiple layer
scales, we need to reduce the number of points progressively. As we already have a grid subsampling, we double the cell size at every pooling layer, along with the other
related parameters, incrementally increasing the receptive
ﬁeld of KPConv. The features pooled at each new location
can either be obtained by a max-pooling or a KPConv. We
use the latter in our architectures and call it “strided KP-
Conv”, by analogy to the image strided convolution.
KPConv layer. Our convolution layer takes as input the
∈RN×3, their corresponding features F
RN×Din, and the matrix of neighborhood indices N ∈
[[1, N]]N′×nmax. N ′ is the number of locations where the
neighborhoods are computed, which can be different from
N (in the case of “strided” KPConv). The neighborhood
matrix is forced to have the size of the biggest neighborhood nmax. Because most of the neighborhoods comprise
less than nmax neighbors, the matrix N thus contains unused elements. We call them shadow neighbors, and they
are ignored during the convolution computations.
Network parameters. Each layer j has a cell size dlj from
which we infer other parameters. The kernel points inﬂuence distance is set as equal to σj = Σ × dlj. For rigid KP-
Conv, the convolution radius is automatically set to 2.5 σj
given that the average kernel point radius is 1.5 σj. For deformable KPConv, the convolution radius can be chosen as
rj = ρ × dlj. Σ and ρ are proportional coefﬁcients set for
the whole network. Unless stated otherwise, we will use the
following set of parameters, chosen by cross validation, for
all experiments: K = 15, Σ = 1.0 and ρ = 5.0. The ﬁrst
subsampling cell size dl0 will depend on the dataset and, as
stated above, dlj+1 = 2 ∗dlj.
3.4. Kernel Point Network Architectures
Combining analogy with successful image networks and
empirical studies, we designed two network architectures
for the classiﬁcation and the segmentation tasks. Diagrams
detailing both architectures are available in the supplementary material.
KP-CNN is a 5-layer classiﬁcation convolutional network.
Each layer contains two convolutional blocks, the ﬁrst one
being strided except for the ﬁrst layer. Our convolutional
blocks are designed like bottleneck ResNet blocks with
a KPConv replacing the image convolution, batch normalization and leaky ReLu activation. After the last layer, the
features are aggregated by a global average pooling and processed by the fully connected and softmax layers like in an
image CNN. For the results with deformable KPConv, we
only use deformable kernels in the last 5 KPConv blocks
(see architecture details in the supplementary material).
KP-FCNN is a fully convolutional network for segmentation. The encoder part is the same as in KP-CNN, and the
decoder part uses nearest upsampling to get the ﬁnal pointwise features. Skip links are used to pass the features between intermediate layers of the encoder and the decoder.
Those features are concatenated to the upsampled ones and
processed by a unary convolution, which is the equivalent of
a 1×1 convolution in image or a shared MLP in PointNet. It
is possible to replace the nearest upsampling operation by a
KPConv, in the same way as the strided KPConv, but it does
not lead to a signiﬁcant improvement of the performances.
4. Experiments
4.1. 3D Shape Classiﬁcation and Segmentation
First, we evaluate our networks on two common
model datasets. We use ModelNet40 for classiﬁcation
and ShapenetPart for part segmentation. ModelNet40
contains 12,311 meshed CAD models from 40 categories.
ShapenetPart is a collection of 16,681 point clouds from
16 categories, each with 2-6 part labels. For benchmarking purpose, we use data provided by . In both cases,
we follow standard train/test splits and rescale objects to ﬁt
them into a unit sphere (and consider units to be meters for
the rest of this experiment). We ignore normals because
they are only available for artiﬁcial data.
Classiﬁcation task. We set the ﬁrst subsampling grid size
to dl0 = 2cm. We do not add any feature as input; each
input point is assigned a constant feature equal to 1, as opposed to empty space which can be considered as 0. This
constant feature encodes the geometry of the input points.
Like , our augmentation procedure consists of scaling,
ﬂipping and perturbing the points. In this setup, we are able
to process 2.9 batches of 16 clouds per second on an Nvidia
Titan Xp. Because of our subsampling strategy, the input
point clouds do not all have the same number of points,
which is not a problem as our networks accept variable input
point cloud size. On average, a ModelNet40 object point
cloud comprises 6,800 points in our framework. The other
training parameters are detailed in the supplementary material, along with the architecture details. We also include the
number of parameters and the training/inference speeds for
both rigid and deformable KPConv.
As shown on Table 1, our networks outperform other
state-of-the-art methods using only points (we do not take
into account methods using normals as additional input).
We also notice that rigid KPConv performances are slightly
better. We suspect that it can be explained by the task simplicity. If deformable kernels add more descriptive power,
ModelNet40
ShapeNetPart
SPLATNet 
3DmFV-Net 
SynSpecCNN 
RSNet 
SpecGCN 
PointNet++ 
SO-Net 
PCNN by Ext 
SpiderCNN 
MCConv 
FlexConv 
PointCNN 
DGCNN 
SubSparseCNN 
KPConv rigid
KPConv deform
Table 1. 3D Shape Classiﬁcation and Segmentation results. For
generalizability to real data, we only consider scores obtained
without shape normals on ModelNet40 dataset. The metrics are
overall accuracy (OA) for Modelnet40, class average IoU (mcIoU)
and instance average IoU (mIoU) for ShapeNetPart.
they also increase the overall network complexity, which
can disturb the convergence or lead to overﬁtting on simpler tasks like this shape classiﬁcation.
Segmentation task. For this task, we use KP-FCNN architecture with the same parameters as in the classiﬁcation
task, adding the positions (x, y, z) as additional features to
the constant 1, and using the same augmentation procedure.
We train a single network with multiple heads to segment
the parts of each object class. The clouds are smaller (2,300
points on average), and we can process 4.1 batches of 16
shapes per second. Table 1 shows the instance average, and
the class average mIoU. We detail each class mIoU in the
supplementary material. KP-FCNN outperforms all other
algorithms, including those using additional inputs like images or normals. Shape segmentation is a more difﬁcult task
than shape classiﬁcation, and we see that KPConv has better
performances with deformable kernels.
4.2. 3D Scene Segmentation
Data. Our second experiment shows how our segmentation architecture generalizes to real indoor and outdoor data.
To this end, we chose to test our network on 4 datasets of
different natures. Scannet , for indoor cluttered scenes,
S3DIS , for indoor large spaces, Semantic3D , for
outdoor ﬁxed scans, and Paris-Lille-3D , for outdoor
mobile scans. Scannet contains 1,513 small training scenes
and 100 test scenes for online benchmarking, all annotated
with 20 semantic classes. S3DIS covers six large-scale indoor areas from three different buildings for a total of 273
million points annotated with 13 classes. Like , we advocate the use of Area-5 as test scene to better measure the
generalization ability of our method. Semantic3D is an online benchmark comprising several ﬁxed lidar scans of different outdoor scenes. More than 4 billion points are annotated with 8 classes in this dataset, but they mostly cover
ground, building or vegetation and there are fewer object
instances than in the other datasets. We favor the reduced-8
challenge because it is less biased by the objects close to the
scanner. Paris-Lille-3D contains more than 2km of streets
in 4 different cities and is also an online benchmark. The
160 million points of this dataset are annotated with 10 semantic classes.
Pipeline for real scene segmentation. The 3D scenes in
these datasets are too big to be segmented as a whole. Our
KP-FCNN architecture is used to segment small subclouds
contained in spheres. At training, the spheres are picked
randomly in the scenes. At testing, we pick spheres regularly in the point clouds but ensure each point is tested
multiple times by different sphere locations. As in a voting scheme on model datasets, the predicted probabilities
for each point are averaged. When datasets are colorized,
we use the three color channels as features. We still keep
the constant 1 feature to ensure black/dark points are not ignored. To our convolution, a point with all features equal
to zero is equivalent to empty space. The input sphere radius is chosen as 50 × dl0 (in accordance to Modelnet40
experiment).
Results. Because outdoor objects are larger than indoor objects, we use dl0 = 6cm on Semantic3D and Paris-Lille-
3D, and dl0 = 4cm on Scannet and S3DIS. As shown in
Table 2, our architecture ranks second on Scannet and outperforms all other segmentation architectures on the other
datasets. Compared to other point convolution architectures
 , KPConv performances exceed previous scores
by 19 mIoU points on Scannet and 9 mIoU points on S3DIS.
SubSparseCNN score on Scannet was not reported in their
original paper , so it is hard to compare without knowing
their experimental setup. We can notice that, in the same experimental setup on ShapeNetPart segmentation, KPConv
outperformed SubSparseCNN by nearly 2 mIoU points.
Among these 4 datasets, KPConv deformable kernels
improved the results on Paris-Lille-3D and S3DIS while the
rigid version was better on Scannet and Semantic3D. If we
follow our assumption, we can explain the lower scores on
Semantic3D by the lack of diversity in this dataset. Indeed,
despite comprising 15 scenes and 4 billion points, it contains a majority of ground, building and vegetation points
and a few real objects like car or pedestrians. Although
this is not the case of Scannet, which comprises more than
1,500 scenes with various objects and shapes, our validation
Scannet Sem3D
Pointnet 
Pointnet++ 
SnapNet 
SPLATNet 
SegCloud 
RF MSSF 
Eff3DConv 
TangentConv 
MSDVN 
RSNet 
PointCNN 
SPGraph 
ParamConv 
SubSparseCNN 
KPConv rigid
KPConv deform
Table 2. 3D scene segmentation scores (mIoU). Scannet, Semantic3D and Paris-Lille-3D (PL3D) scores are taken from their
respective online benchmarks (reduced-8 challenge for Semantic3D). S3DIS scores are given for Area-5 (see supplementary material for k-fold).
studies are not reﬂected by the test scores on this benchmark. We found that the deformable KPConv outperformed
its rigid counterpart on several different validation sets (see
Section 4.3). As a conclusion, these results show that the
descriptive power of deformable KPConv is useful to the
network on large and diverse datasets.
We believe KP-
Conv could thrive on larger datasets because its kernel combines a strong descriptive power (compared to other simpler
representations, like the linear kernels of ), and great
learnability (the weights of MLP convolutions like 
are more complex to learn). An illustration of segmented
scenes on Semantic3D and S3DIS is shown in Figure 4.
More results visualizations are provided in the supplementary material.
4.3. Ablation Study
We conduct an ablation study to support our claim that
deformable KPConv has a stronger descriptive power than
rigid KPConv. The idea is to impede the capabilities of the
network, in order to reveal the real potential of deformable
kernels. We use Scannet dataset (same parameters as before) and use the ofﬁcial validation set, because the test set
cannot be used for such evaluations. As depicted in Figure
5, the deformable KPConv only loses 1.5% mIoU when restricted to 4 kernel points. In the same conﬁguration, the
rigid KPConv loses 3.5% mIoU.
As stated in Section 4.2, we can also see that deformable
Figure 4. Outdoor and Indoor scenes, respectively from Semantic3D and S3DIS, classiﬁed by KP-FCNN with deformable kernels.
KPConv performs better than rigid KPConv with 15 kernel
points. Although it is not the case on the test set, we tried
different validation sets that conﬁrmed the superior performances of deformable KPConv. This is not surprising as we
obtained the same results on S3DIS. Deformable KPConv
seem to thrive on indoor datasets, which offer more diversity than outdoor datasets. To understand why, we need to
go beyond numbers and see what is effectively learned by
the two versions of KPConv.
4.4. Learned Features and Effective Receptive Field
To achieve a deeper understanding of KPConv, we offer
two insights of the learning mechanisms.
Learned features. Our ﬁrst idea was to visualize the features learned by our network. In this experiment, we trained
KP-CNN on ModelNet40 with rigid KPConv. We added
random rotation augmentations around vertical axis to increase the input shape diversity. Then we visualize each
learned feature by coloring the points according to their
level of activation for this features. In Figure 6, we chose
input point clouds maximizing the activation for different
features at the ﬁrst and third layer. For a cleaner display, we
Figure 5. Ablation study on Scannet validation set. Evolution of
the mIoU when reducing the number of kernel points.
Figure 6. Low and high level features learned in KP-CNN. Each
feature is displayed on 2 input point clouds taken from Model-
Net40. High activations are in red and low activations in blue.
projected the activations from the layer subsampled points
to the original input points. We observe that, in its ﬁrst layer,
the network is able to learn low-level features like vertical/horizontal planes (a/b), linear structures (c), or corners
(d). In the later layers, the network detects more complex
shapes like small buttresses (e), balls (f), cones (g), or stairs
(h). However, it is difﬁcult to see a difference between rigid
and deformable KPConv. This tool is very useful to understand what KPConv can learn in general, but we need
another one to compare the two versions.
Effective Receptive Field.
To apprehend the differences between the representations learned by rigid and deformable KPConv, we can compute its Effective Receptive
Field (ERF) at different locations. The ERF is a measure of the inﬂuence that each input point has on the result
of a KPConv layer at a particular location. It is computed
as the gradient of KPConv responses at this particular location with respect to the input point features. As we can
see in Figure 7, the ERF varies depending on the object it
is centered on. We see that rigid KPConv ERF has a relatively consistent range on every type of object, whereas
deformable KPConv ERF seems to adapt to the object size.
Indeed, it covers the whole bed, and concentrates more on
the chair that on the surrounding ground. When centered on
a ﬂat surface, it also seems to ignore most of it and reach for
Figure 7. KPConv ERF at layer 4 of KP-FCNN, trained on Scannet. The green dots represent the ERF centers. ERF values are
merged with scene colors as red intensity. The more red a point is,
the more inﬂuence it has on the green point features.
further details in the scene. This adaptive behavior shows
that deformable KPConv improves the network ability to
adapt to the geometry of the scene objects, and explains the
better performances on indoor datasets.
5. Conclusion
In this work, we propose KPConv, a convolution that operates on point clouds. KPConv takes radius neighborhoods
as input and processes them with weights spatially located
by a small set of kernel points. We deﬁne a deformable
version of this convolution operator that learns local shifts
effectively deforming the convolution kernels to make them
ﬁt the point cloud geometry. Depending on the diversity
of the datasets, or the chosen network conﬁguration, deformable and rigid KPConv are both valuable, and our networks brought new state-of-the-art performances for nearly
every tested dataset. We release our source code, hoping
to help further research on point cloud convolutional architectures. Beyond the proposed classiﬁcation and segmentation networks, KPConv can be used in any other application
addressed by CNNs. We believe that deformable convolutions can thrive in larger datasets or challenging tasks such
as object detection, lidar ﬂow computation, or point cloud
completion.
Acknowledgement. The authors gratefully acknowledge
the support of ONR MURI grant N00014-13-1-0341, NSF
grant IIS-1763268, a Vannevar Bush Faculty Fellowship,
and a gift from the Adobe and Autodesk corporations.