Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894–6910
November 7–11, 2021. c⃝2021 Association for Computational Linguistics
SimCSE: Simple Contrastive Learning of Sentence Embeddings
Tianyu Gao†∗
Xingcheng Yao‡∗
Danqi Chen†
†Department of Computer Science, Princeton University
‡Institute for Interdisciplinary Information Sciences, Tsinghua University
{tianyug,danqic}@cs.princeton.edu
 
This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We ﬁrst describe an unsupervised approach, which takes an input sentence and
predicts itself in a contrastive objective, with
only standard dropout used as noise.
simple method works surprisingly well, performing on par with previous supervised counterparts.
We ﬁnd that dropout acts as minimal data augmentation and removing it leads
to a representation collapse.
Then, we propose a supervised approach, which incorporates annotated pairs from natural language
inference datasets into our contrastive learning framework, by using “entailment” pairs
as positives and “contradiction” pairs as hard
negatives. We evaluate SimCSE on standard
semantic textual similarity (STS) tasks, and
our unsupervised and supervised models using
BERTbase achieve an average of 76.3% and
81.6% Spearman’s correlation respectively, a
4.2% and 2.2% improvement compared to
previous best results.
We also show—both
theoretically and empirically—that contrastive
learning objective regularizes pre-trained embeddings’ anisotropic space to be more uniform, and it better aligns positive pairs when
supervised signals are available.1
Introduction
Learning universal sentence embeddings is a fundamental problem in natural language processing and has been studied extensively in the literature .
In this work, we advance state-of-the-art sentence
*The ﬁrst two authors contributed equally (listed in alphabetical order). This work was done when Xingcheng visited
the Princeton NLP group remotely.
1Our code and pre-trained models are publicly available at
 
embedding methods and demonstrate that a contrastive objective can be extremely effective when
coupled with pre-trained language models such as
BERT or RoBERTa . We present SimCSE, a simple contrastive
sentence embedding framework, which can produce superior sentence embeddings, from either
unlabeled or labeled data.
Our unsupervised SimCSE simply predicts the
input sentence itself with only dropout used as noise (Figure 1(a)). In other
words, we pass the same sentence to the pre-trained
encoder twice: by applying the standard dropout
twice, we can obtain two different embeddings as
“positive pairs”. Then we take other sentences in the
same mini-batch as “negatives”, and the model predicts the positive one among negatives. Although it
may appear strikingly simple, this approach outperforms training objectives such as predicting next
sentences and discrete
data augmentation (e.g., word deletion and replacement) by a large margin, and even matches previous
supervised methods. Through careful analysis, we
ﬁnd that dropout acts as minimal “data augmentation” of hidden representations, while removing it
leads to a representation collapse.
Our supervised SimCSE builds upon the recent
success of using natural language inference (NLI)
datasets for sentence embeddings and incorporates annotated sentence pairs in contrastive learning (Figure 1(b)). Unlike previous work that casts
it as a 3-way classiﬁcation task (entailment, neutral and contradiction), we leverage the fact that
entailment pairs can be naturally used as positive
instances. We also ﬁnd that adding corresponding contradiction pairs as hard negatives further
improves performance. This simple use of NLI
datasets achieves a substantial improvement compared to prior methods using the same datasets.
We also compare to other labeled sentence-pair
The pets are sitting on a couch.
Different hidden dropout masks
in two forward passes
There are animals outdoors.
There is a man.
The man wears a business suit.
A kid is skateboarding.
A kit is inside the house.
Two dogs are running.
A man surfing on the sea.
A kid is on a skateboard.
are running.
A man surfing
on the sea.
A kid is on a
skateboard.
(a) Unsupervised SimCSE
(b) Supervised SimCSE
label=entailment
label=contradiction
label=contradiction
label=entailment
label=contradiction
label=entailment
Positive instance
Negative instance
The pets are sitting on a couch.
Figure 1: (a) Unsupervised SimCSE predicts the input sentence itself from in-batch negatives, with different hidden
dropout masks applied. (b) Supervised SimCSE leverages the NLI datasets and takes the entailment (premisehypothesis) pairs as positives, and contradiction pairs as well as other in-batch instances as negatives.
datasets and ﬁnd that NLI datasets are especially
effective for learning sentence embeddings.
To better understand the strong performance of
SimCSE, we borrow the analysis tool from Wang
and Isola , which takes alignment between
semantically-related positive pairs and uniformity
of the whole representation space to measure the
quality of learned embeddings. Through empirical analysis, we ﬁnd that our unsupervised Sim-
CSE essentially improves uniformity while avoiding degenerated alignment via dropout noise, thus
improving the expressiveness of the representations. The same analysis shows that the NLI training signal can further improve alignment between
positive pairs and produce better sentence embeddings. We also draw a connection to the recent ﬁndings that pre-trained word embeddings suffer from
anisotropy and
prove that—through a spectrum perspective—the
contrastive learning objective “ﬂattens” the singular value distribution of the sentence embedding
space, hence improving uniformity.
We conduct a comprehensive evaluation of Sim-
CSE on seven standard semantic textual similarity
(STS) tasks and
seven transfer tasks .
On the STS tasks, our unsupervised and supervised
models achieve a 76.3% and 81.6% averaged Spearman’s correlation respectively using BERTbase, a
4.2% and 2.2% improvement compared to previous
best results. We also achieve competitive performance on the transfer tasks. Finally, we identify
an incoherent evaluation issue in the literature and
consolidate results of different settings for future
work in evaluation of sentence embeddings.
Background: Contrastive Learning
Contrastive learning aims to learn effective representation by pulling semantically close neighbors
together and pushing apart non-neighbors . It assumes a set of paired examples
D = { and take a cross-entropy objective with in-batch negatives : let hi and h+
i denote the
representations of xi and x+
i , the training objective
for (xi, x+
i ) with a mini-batch of N pairs is:
esim(hi,h+
j=1 esim(hi,h+
where τ is a temperature hyperparameter and
sim(h1, h2) is the cosine similarity
∥h1∥·∥h2∥. In
this work, we encode input sentences using a
pre-trained language model such as BERT or RoBERTa :
h = fθ(x), and then ﬁne-tune all the parameters
using the contrastive learning objective (Eq. 1).
Positive instances. One critical question in contrastive learning is how to construct (xi, x+
i ) pairs.
In visual representations, an effective solution is to
take two random transformations of the same image
(e.g., cropping, ﬂipping, distortion and rotation) as
i . A similar
approach has been recently adopted in language
representations 
by applying augmentation techniques such as word
deletion, reordering, and substitution. However,
data augmentation in NLP is inherently difﬁcult
because of its discrete nature. As we will see in §3,
simply using standard dropout on intermediate representations outperforms these discrete operators.
In NLP, a similar contrastive learning objective
has been explored in different contexts . In these cases, (xi, x+
i ) are collected from
supervised datasets such as question-passage pairs.
Because of the distinct nature of xi and x+
approaches always use a dual-encoder framework,
i.e., using two independent encoders fθ1 and fθ2 for
i . For sentence embeddings, Logeswaran
and Lee also use contrastive learning with
a dual-encoder approach, by forming current sentence and next sentence as identify two key properties related to
contrastive learning—alignment and uniformity—
and propose to use them to measure the quality of
representations. Given a distribution of positive
pairs ppos, alignment calculates expected distance
between embeddings of the paired instances (assuming representations are already normalized):
(x,x+)∼ppos
∥f(x) −f(x+)∥2.
On the other hand, uniformity measures how well
the embeddings are uniformly distributed:
ℓuniform ≜log
e−2∥f(x)−f(y)∥2,
where pdata denotes the data distribution. These
two metrics are well aligned with the objective
of contrastive learning: positive instances should
stay close and embeddings for random instances
should scatter on the hypersphere. In the following
sections, we will also use the two metrics to justify
the inner workings of our approaches.
Unsupervised SimCSE
The idea of unsupervised SimCSE is extremely
simple: we take a collection of sentences {xi}m
and use x+
i = xi. The key ingredient to get this to
work with identical positive pairs is through the use
of independently sampled dropout masks for xi and
i . In standard training of Transformers , there are dropout masks placed on
fully-connected layers as well as attention probabilities (default p = 0.1). We denote hz
i = fθ(xi, z)
where z is a random mask for dropout. We simply
feed the same input to the encoder twice and get
Data augmentation
None (unsup. SimCSE)
Word deletion
Delete one word
w/o dropout
Synonym replacement
Table 1: Comparison of data augmentations on STS-B
development set (Spearman’s correlation). Crop k%:
keep 100-k% of the length; word deletion k%: delete
k% words; Synonym replacement: use nlpaug to randomly replace one word with its synonym;
MLM k%: use BERTbase to replace k% of words.
Training objective
(fθ1, fθ2)
Next sentence
Next 3 sentences
Delete one word
Unsupervised SimCSE
Table 2: Comparison of different unsupervised objectives (STS-B development set, Spearman’s correlation).
The two columns denote whether we use one encoder
or two independent encoders. Next 3 sentences: randomly sample one from the next 3 sentences. Delete
one word: delete one word randomly (see Table 1).
two embeddings with different dropout masks z, z′,
and the training objective of SimCSE becomes:
j=1 esim 2. Table 1 compares our approach to common
data augmentation techniques such as crop, word
deletion and replacement, which can be viewed as
2We randomly sample 106 sentences from English
Wikipedia and ﬁne-tune BERTbase with learning rate = 3e-5,
N = 64. In all our experiments, no STS training sets are used.
Table 3: Effects of different dropout probabilities p
on the STS-B development set (Spearman’s correlation,
BERTbase). Fixed 0.1: default 0.1 dropout rate but apply the same dropout mask on both xi and x+
h = fθ(g(x), z) and g is a (random) discrete operator on x. We note that even deleting one word
would hurt performance and none of the discrete
augmentations outperforms dropout noise.
We also compare this self-prediction training
objective to the next-sentence objective used in Logeswaran and Lee , taking either one encoder
or two independent encoders. As shown in Table 2,
we ﬁnd that SimCSE performs much better than
the next-sentence objectives (82.5 vs 67.4 on STS-
B) and using one encoder instead of two makes a
signiﬁcant difference in our approach.
Why does it work? To further understand the
role of dropout noise in unsupervised SimCSE, we
try out different dropout rates in Table 3 and observe that all the variants underperform the default
dropout probability p = 0.1 from Transformers.
We ﬁnd two extreme cases particularly interesting:
“no dropout” (p = 0) and “ﬁxed 0.1” (using default
dropout p = 0.1 but the same dropout masks for
the pair). In both cases, the resulting embeddings
for the pair are exactly the same, and it leads to
a dramatic performance degradation. We take the
checkpoints of these models every 10 steps during
training and visualize the alignment and uniformity
metrics3 in Figure 2, along with a simple data augmentation model “delete one word”. As clearly
shown, starting from pre-trained checkpoints, all
models greatly improve uniformity. However, the
alignment of the two special variants also degrades
drastically, while our unsupervised SimCSE keeps
a steady alignment, thanks to the use of dropout
noise. It also demonstrates that starting from a pretrained checkpoint is crucial, for it provides good
initial alignment. At last, “delete one word” improves the alignment yet achieves a smaller gain
on the uniformity metric, and eventually underperforms unsupervised SimCSE.
3We take STS-B pairs with a score higher than 4 as ppos
and all STS-B sentences as pdata.
8nifoUmity
1o dUoSout
Delete one woUd
8nsuS. 6imC6E
No dropout
Delete one word
Unsup. SimCSE
Training direction
Figure 2: ℓalign-ℓuniform plot for unsupervised SimCSE,
“no dropout”, “ﬁxed 0.1”, and “delete one word”. We
visualize checkpoints every 10 training steps and the
arrows indicate the training direction. For both ℓalign
and ℓuniform, lower numbers are better.
Supervised SimCSE
We have demonstrated that adding dropout noise
is able to keep a good alignment for positive pairs
(x, x+) ∼ppos. In this section, we study whether
we can leverage supervised datasets to provide
better training signals for improving alignment of
our approach. Prior work has demonstrated
that supervised natural language inference (NLI)
datasets are effective for learning sentence embeddings, by predicting whether the relationship between two sentences is entailment, neutral or contradiction. In our contrastive learning framework,
we instead directly take (xi, x+
i ) pairs from supervised datasets and use them to optimize Eq. 1.
Choices of labeled data. We ﬁrst explore which
supervised datasets are especially suitable for constructing positive pairs (xi, x+
i ). We experiment
with a number of datasets with sentence-pair examples, including 1) QQP4: Quora question pairs;
2) Flickr30k : each image is
annotated with 5 human-written captions and we
consider any two captions of the same image as a
positive pair; 3) ParaNMT : a large-scale back-translation paraphrase
dataset5; and ﬁnally 4) NLI datasets: SNLI and MNLI .
We train the contrastive learning model (Eq. 1)
with different datasets and compare the results in
4 
5ParaNMT is automatically constructed by machine translation systems. Strictly speaking, we should not call it “supervised”. It underperforms our unsupervised SimCSE though.
Table 4. For a fair comparison, we also run experiments with the same # of training pairs. Among
all the options, using entailment pairs from the
NLI (SNLI + MNLI) datasets performs the best.
We think this is reasonable, as the NLI datasets
consist of high-quality and crowd-sourced pairs.
Also, human annotators are expected to write the
hypotheses manually based on the premises and
two sentences tend to have less lexical overlap.
For instance, we ﬁnd that the lexical overlap (F1
measured between two bags of words) for the entailment pairs (SNLI + MNLI) is 39%, while they
are 60% and 55% for QQP and ParaNMT.
Contradiction as hard negatives. Finally, we further take the advantage of the NLI datasets by using its contradiction pairs as hard negatives6. In
NLI datasets, given one premise, annotators are required to manually write one sentence that is absolutely true (entailment), one that might be true (neutral), and one that is deﬁnitely false (contradiction).
Therefore, for each premise and its entailment hypothesis, there is an accompanying contradiction
hypothesis7 (see Figure 1 for an example).
Formally, we extend (xi, x+
i ) to (xi, x+
where xi is the premise, x+
i are entailment
and contradiction hypotheses. The training objective ℓi is then deﬁned by (N is mini-batch size):
esim(hi,h+
esim(hi,h+
j )/τ + esim(hi,h−
As shown in Table 4, adding hard negatives can
further improve performance (84.9 →86.2) and
this is our ﬁnal supervised SimCSE. We also tried
to add the ANLI dataset or combine it with our unsupervised SimCSE approach,
but didn’t ﬁnd a meaningful improvement. We also
considered a dual encoder framework in supervised
SimCSE and it hurt performance (86.2 →84.2).
Connection to Anisotropy
Recent work identiﬁes an anisotropy problem in
language representations , i.e., the learned embeddings occupy a
narrow cone in the vector space, which severely
limits their expressiveness.
Gao et al. 
6We also experimented with adding neutral hypotheses as
hard negatives. See Section 6.3 for more discussion.
7In fact, one premise can have multiple contradiction hypotheses. In our implementation, we only sample one as the
hard negative and we did not ﬁnd a difference by using more.
Unsup. SimCSE (1m)
QQP (134k)
Flickr30k (318k)
ParaNMT (5m)
entailment (314k)
neutral (314k)8
contradiction (314k)
all (942k)
entailment + hard neg.
+ ANLI (52k)
Table 4: Comparisons of different supervised datasets
as positive pairs. Results are Spearman’s correlations
on the STS-B development set using BERTbase (we
use the same hyperparameters as the ﬁnal SimCSE
Numbers in brackets denote the # of pairs.
Sample: subsampling 134k positive pairs for a fair comparison among datasets; full: using the full dataset. In
the last block, we use entailment pairs as positives and
contradiction pairs as hard negatives (our ﬁnal model).
demonstrate that language models trained with tied
input/output embeddings lead to anisotropic word
embeddings, and this is further observed by Ethayarajh in pre-trained contextual representations. Wang et al. show that singular values
of the word embedding matrix in a language model
decay drastically: except for a few dominating singular values, all others are close to zero.
A simple way to alleviate the problem is postprocessing, either to eliminate the dominant principal components , or to map embeddings to an
isotropic distribution . Another common solution is to add regularization during training . In this work, we show that—both
theoretically and empirically—the contrastive objective can also alleviate the anisotropy problem.
The anisotropy problem is naturally connected to
uniformity , both highlighting that embeddings should be evenly distributed
in the space. Intuitively, optimizing the contrastive
learning objective can improve uniformity (or ease
the anisotropy problem), as the objective pushes
negative instances apart. Here, we take a singular
spectrum perspective—which is a common practice
8Though our ﬁnal model only takes entailment pairs as
positive instances, here we also try taking neutral and contradiction pairs from the NLI datasets as positive pairs.
in analyzing word embeddings , and
show that the contrastive objective can “ﬂatten” the
singular value distribution of sentence embeddings
and make the representations more isotropic.
Following Wang and Isola , the asymptotics of the contrastive learning objective (Eq. 1)
can be expressed by the following equation when
the number of negative instances approaches inﬁnity (assuming f(x) is normalized):
(x,x+)∼ppos
f(x)⊤f(x+)
ef(x)⊤f(x−)/τi
where the ﬁrst term keeps positive instances similar
and the second pushes negative pairs apart. When
pdata is uniform over ﬁnite samples {xi}m
hi = f(xi), we can derive the following formula
from the second term with Jensen’s inequality:
ef(x)⊤f(x−)/τi
Let W be the sentence embedding matrix corresponding to {xi}m
i=1, i.e., the i-th row of W is
hi. Optimizing the second term in Eq. 6 essentially minimizes an upper bound of the summation
of all elements in WW⊤, i.e., Sum(WW⊤) =
Since we normalize hi, all elements on the diagonal of WW⊤are 1 and then tr(WW⊤) (the
sum of all eigenvalues) is a constant. According
to Merikoski , if all elements in WW⊤are
positive, which is the case in most times according to Figure G.1, then Sum(WW⊤) is an upper
bound for the largest eigenvalue of WW⊤. When
minimizing the second term in Eq. 6, we reduce
the top eigenvalue of WW⊤and inherently “ﬂatten” the singular spectrum of the embedding space.
Therefore, contrastive learning is expected to alleviate the representation degeneration problem and
improve uniformity of sentence embeddings.
Compared to post-processing methods in Li et al.
 ; Su et al. , which only aim to encourage isotropic representations, contrastive learning
also optimizes for aligning positive pairs by the
ﬁrst term in Eq. 6, which is the key to the success
of SimCSE. A quantitative analysis is given in §7.
Experiment
Evaluation Setup
We conduct our experiments on 7 semantic textual
similarity (STS) tasks. Note that all our STS experiments are fully unsupervised and no STS training
sets are used. Even for supervised SimCSE, we
simply mean that we take extra labeled datasets
for training, following previous work . We also evaluate 7 transfer learning
tasks and provide detailed results in Appendix E.
We share a similar sentiment with Reimers and
Gurevych that the main goal of sentence
embeddings is to cluster semantically similar sentences and hence take STS as the main result.
Semantic textual similarity tasks.
We evaluate on 7 STS tasks:
STS 2012–2016 , STS
Relatedness . When comparing to previous work, we identify invalid comparison patterns in published papers in the evaluation
settings, including (a) whether to use an additional
regressor, (b) Spearman’s vs Pearson’s correlation,
and (c) how the results are aggregated (Table B.1).
We discuss the detailed differences in Appendix B
and choose to follow the setting of Reimers and
Gurevych in our evaluation (no additional
regressor, Spearman’s correlation, and “all” aggregation). We also report our replicated study of
previous work as well as our results evaluated in
a different setting in Table B.2 and Table B.3. We
call for unifying the setting in evaluating sentence
embeddings for future research.
Training details. We start from pre-trained checkpoints of BERT (uncased)
or RoBERTa (cased) and take
the [CLS] representation as the sentence embedding9 (see §6.3 for comparison between different
pooling methods). We train unsupervised SimCSE
on 106 randomly sampled sentences from English
Wikipedia, and train supervised SimCSE on the
combination of MNLI and SNLI datasets (314k).
More training details can be found in Appendix A.
9There is an MLP layer over [CLS] in BERT’s original
implementation and we keep it with random initialization.
Unsupervised models
GloVe embeddings (avg.)♣
BERTbase (ﬁrst-last avg.)
BERTbase-ﬂow
BERTbase-whitening
IS-BERTbase
CT-BERTbase
∗SimCSE-BERTbase
RoBERTabase (ﬁrst-last avg.)
RoBERTabase-whitening
DeCLUTR-RoBERTabase
∗SimCSE-RoBERTabase
∗SimCSE-RoBERTalarge
Supervised models
InferSent-GloVe♣
Universal Sentence Encoder♣
SBERTbase-ﬂow
SBERTbase-whitening
CT-SBERTbase
∗SimCSE-BERTbase
SRoBERTabase
SRoBERTabase-whitening
∗SimCSE-RoBERTabase
∗SimCSE-RoBERTalarge
Table 5: Sentence embedding performance on STS tasks (Spearman’s correlation, “all” setting). We highlight the
highest numbers among models with the same pre-trained encoder. ♣: results from Reimers and Gurevych ;
♥: results from Zhang et al. ; all other results are reproduced or reevaluated by ourselves. For BERT-ﬂow and whitening , we only report the “NLI” setting (see Table C.1).
Main Results
We compare unsupervised and supervised Sim-
CSE to previous state-of-the-art sentence embedding methods on STS tasks. Unsupervised baselines include average GloVe embeddings , average BERT or RoBERTa
embeddings10, and post-processing methods such
as BERT-ﬂow and BERTwhitening . We also compare to several recent methods using a contrastive objective,
including 1) IS-BERT , which
maximizes the agreement between global and local features; 2) DeCLUTR ,
which takes different spans from the same document as positive pairs; 3) CT ,
which aligns embeddings of the same sentence
from two different encoders.11 Other supervised
10Following Su et al. , we take the average of the ﬁrst
and the last layers, which is better than only taking the last.
11We do not compare to CLEAR , because
they use their own version of pre-trained models, and the
numbers appear to be much lower. Also note that CT is a
concurrent work to ours.
methods include InferSent ,
Universal Sentence Encoder , and
SBERT/SRoBERTa 
with post-processing methods (BERT-ﬂow, whitening, and CT). We provide more details of these
baselines in Appendix C.
Table 5 shows the evaluation results on 7 STS
tasks. SimCSE can substantially improve results
on all the datasets with or without extra NLI supervision, greatly outperforming the previous stateof-the-art models. Speciﬁcally, our unsupervised
SimCSE-BERTbase improves the previous best
averaged Spearman’s correlation from 72.05% to
76.25%, even comparable to supervised baselines.
When using NLI datasets, SimCSE-BERTbase further pushes the state-of-the-art results to 81.57%.
The gains are more pronounced on RoBERTa
encoders, and our supervised SimCSE achieves
83.76% with RoBERTalarge.
In Appendix E, we show that SimCSE also
achieves on par or better transfer task performance
compared to existing work, and an auxiliary MLM
objective can further boost performance.
w/ MLP (train)
First-last avg.
Table 6: Ablation studies of different pooling methods
in unsupervised and supervised SimCSE. [CLS] w/
MLP (train): using MLP on [CLS] during training but
removing it during testing. The results are based on the
development set of STS-B using BERTbase.
Contradiction
Table 7: STS-B development results with different hard
negative policies. “N/A”: no hard negative.
Ablation Studies
We investigate the impact of different pooling methods and hard negatives. All reported results in this
section are based on the STS-B development set.
We provide more ablation studies (normalization,
temperature, and MLM objectives) in Appendix D.
Pooling methods. Reimers and Gurevych ;
Li et al. show that taking the average embeddings of pre-trained models (especially from
both the ﬁrst and last layers) leads to better performance than [CLS]. Table 6 shows the comparison
between different pooling methods in both unsupervised and supervised SimCSE. For [CLS] representation, the original BERT implementation takes
an extra MLP layer on top of it. Here, we consider
three different settings for [CLS]: 1) keeping the
MLP layer; 2) no MLP layer; 3) keeping MLP during training but removing it at testing time. We ﬁnd
that for unsupervised SimCSE, taking [CLS] representation with MLP only during training works
the best; for supervised SimCSE, different pooling
methods do not matter much. By default, we take
[CLS]with MLP (train) for unsupervised SimCSE
and [CLS]with MLP for supervised SimCSE.
Hard negatives. Intuitively, it may be beneﬁcial
to differentiate hard negatives (contradiction examples) from other in-batch negatives. Therefore, we
extend our training objective deﬁned in Eq. 5 to
incorporate weighting of different negatives:
esim(hi,h+
esim(hi,h+
j )/τ + α1j
i esim(hi,h−
j )/τ, (8)
i ∈{0, 1} is an indicator that equals 1 if
and only if i = j. We train SimCSE with different
values of α and evaluate the trained models on
the development set of STS-B. We also consider
taking neutral hypotheses as hard negatives. As
shown in Table 7, α = 1 performs the best, and
neutral hypotheses do not bring further gains.
In this section, we conduct further analyses to understand the inner workings of SimCSE.
Uniformity and alignment. Figure 3 shows uniformity and alignment of different sentence embedding models along with their averaged STS results.
In general, models which have both better alignment and uniformity achieve better performance,
conﬁrming the ﬁndings in Wang and Isola .
We also observe that (1) though pre-trained embeddings have good alignment, their uniformity is
poor (i.e., the embeddings are highly anisotropic);
(2) post-processing methods like BERT-ﬂow and
BERT-whitening greatly improve uniformity but
also suffer a degeneration in alignment; (3) unsupervised SimCSE effectively improves uniformity
of pre-trained embeddings whereas keeping a good
alignment; (4) incorporating supervised data in
SimCSE further amends alignment. In Appendix F,
we further show that SimCSE can effectively ﬂatten singular value distribution of pre-trained embeddings. In Appendix G, we demonstrate that
SimCSE provides more distinguishable cosine similarities between different sentence pairs.
Qualitative comparison.
We conduct a smallscale retrieval experiment using SBERTbase and
SimCSE-BERTbase. We use 150k captions from
Flickr30k dataset and take any random sentence as
query to retrieve similar sentences (based on cosine
similarity). As several examples shown in Table 8,
the retrieved sentences by SimCSE have a higher
quality compared to those retrieved by SBERT.
Related Work
Early work in sentence embeddings builds upon the
distributional hypothesis by predicting surrounding
sentences of a given one .
8nifoUmity
Avg. BERT (56.7)
Next3Sent (63.1)
SBERT (74.9)
SimCSE (81.6)
Unsup. SimCSE (76.3)
SBERT-flow (76.6)
SBERT-whitening (77.0)
BERT-flow (66.6)
BERT-whitening (66.3)
ℓalign-ℓuniform plot of models based on
BERTbase. Color of points and numbers in brackets
represent average STS performance (Spearman’s correlation). Next3Sent: “next 3 sentences” from Table 2.
et al., 2016; Logeswaran and Lee, 2018). Pagliardini et al. show that simply augmenting
the idea of word2vec with
n-gram embeddings leads to strong results. Several recent (and concurrent) approaches adopt contrastive objectives by
taking different views—from data augmentation or
different copies of models—of the same sentence
or document. Compared to these work, SimCSE
uses the simplest idea by taking different outputs
of the same sentence from standard dropout, and
performs the best on STS tasks.
Supervised sentence embeddings are promised
to have stronger performance compared to unsupervised counterparts. Conneau et al. propose to ﬁne-tune a Siamese model on NLI datasets,
which is further extended to other encoders or
pre-trained models . Furthermore, Wieting and Gimpel ; Wieting et al. demonstrate that
bilingual and back-translation corpora provide useful supervision for learning semantic similarity. Another line of work focuses on regularizing embeddings to alleviate the representation degeneration
problem (as discussed in §5), and yields substantial
improvement over pre-trained language models.
Conclusion
In this work, we propose SimCSE, a simple contrastive learning framework, which greatly improves state-of-the-art sentence embeddings on semantic textual similarity tasks. We present an unsupervised approach which predicts input sentence
itself with dropout noise and a supervised approach
utilizing NLI datasets. We further justify the inner
workings of our approach by analyzing alignment
and uniformity of SimCSE along with other baseline models. We believe that our contrastive objective, especially the unsupervised one, may have a
broader application in NLP. It provides a new perspective on data augmentation with text input, and
can be extended to other continuous representations
and integrated in language model pre-training.
Acknowledgements
We thank Tao Lei, Jason Lee, Zhengyan Zhang,
Jinhyuk Lee, Alexander Wettig, Zexuan Zhong,
and the members of the Princeton NLP group for
helpful discussion and valuable feedback. This
research is supported by a Graduate Fellowship at
Princeton University and a gift award from Apple.