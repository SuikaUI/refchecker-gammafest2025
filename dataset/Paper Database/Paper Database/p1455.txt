Soft Comput 19:3387–3400
DOI 10.1007/s00500-014-1492-5
One-class classiﬁers with incremental learning and forgetting
for data streams with concept drift
Bartosz Krawczyk · Michał Wo´zniak
Published online: 21 October 2014
© The Author(s) 2014. This article is published with open access at Springerlink.com
One of the most important challenges for
machine learning community is to develop efﬁcient classi-
ﬁers which are able to cope with data streams, especially
with the presence of the so-called concept drift. This phenomenon is responsible for the change of classiﬁcation task
characteristics, and poses a challenge for the learning model
to adapt itself to the current state of the environment. So there
is a strong belief that one-class classiﬁcation is a promising
research direction for data stream analysis—it can be used for
binary classiﬁcation without an access to counterexamples,
decomposing a multi-class data stream, outlier detection or
novel class recognition. This paper reports a novel modiﬁcation of weighted one-class support vector machine, adapted
to the non-stationary streaming data analysis. Our proposition can deal with the gradual concept drift, as the introduced
one-class classiﬁer model can adapt its decision boundary to
new, incoming data and additionally employs a forgetting
mechanism which boosts the ability of the classiﬁer to follow the model changes. In this work, we propose several
different strategies for incremental learning and forgetting,
and additionally we evaluate them on the basis of several real
data streams. Obtained results conﬁrmed the usability of proposed classiﬁer to the problem of data stream classiﬁcation
withthepresenceofconceptdrift.Additionally,implemented
Communicated by E. Lughofer.
This work was supported by the Polish National Science Center under
the Grant No. DEC-2013/09/B/ST6/02264.
B. Krawczyk (B) · M. Wo´zniak
Department of Systems and Computer Networks, Wrocław University
of Technology, Wybrze˙ze Wyspia´nskiego 27, 50-370 Wrocław, Poland
e-mail: 
M. Wo´zniak
e-mail: 
forgetting mechanism assures the limited memory consumption, because only quite new and valuable examples should
be memorized.
Pattern classiﬁcation · One-class classiﬁcation ·
Data stream classiﬁcation · Concept drift · Incremental
learning · Forgetting
1 Introduction
Contemporary computer systems manage and store enormous amount of data. It is predicted that the volume of stored
information will be doubling every two years. People send 14
billion e-mails and more than 350 million tweets per day. The
huge chains of discount department stores (as Walmarkt Inc.)
register more than 1 million transactions per hour. Therefore, the marked leading companies desire to develop smart
analytic tools based on machine learning approach, which
can analyze such enormous amount of data. Additionally,
designing such analytical tools should take into a consideration that most of data arrives continuously in the form of
so-called data stream . Furthermore, the relation within the data, i.e., statistical dependencies characterizing a given phenomenon (such as client behavior), may
change . This observation requires a special
analytical model which can cope with such non-stationary
characteristics. In the beginning, the data streams originated
in the ﬁnancial markets. Today, data streams can be found
everywhere—in the Internet, monitoring systems, sensor networks and other domains . Data streams
differ from the traditional static data, because they can be
viewed as an inﬁnite amount of data that arrives continuously,
where memory and computational complexity play the crucial roles. Due to this mining data stream poses many new
B. Krawczyk, M. Wo´zniak
challenges to the contemporary machine learning systems
 .
In this paper, we will mainly focus on the classiﬁcation
task, which is a widely used analytical approach . Basically, it aims at assigning a given observation to
one of the predeﬁned categories. Such situation can be found,
e.g., in spam ﬁltering, biometrics, medical decision support,
or fraud detection to enumerate only a few. The concept drift
in the classiﬁcation model mean that the statistical dependenciesbetweenattributesdescribinganobjectanditspredeﬁned
label could change over time.
To explain the possible types of the changes, let us
shortly introduce a statistical classiﬁcation model. This theory assumes that both the attributes
describing an object x ∈X ⊆Rd and its correct classi-
ﬁcation (class label) j ∈M = {1, 2, . . . , M} are observed
values of a pair of random variables (X, J). The probability
distribution of them is given by the prior class probabilities
p j = P(J = j),
and class-conditional probability density function of X
f j(x) = f (x| j),
x ∈X, j ∈M.
The classiﬁcation algorithm  maps the feature space X
to the set of deﬁned class labels M
If the probability characteristics given by Eqs. (1) and (2)
are known, then the optimal classiﬁer ∗, minimizing the
misclassiﬁcation probability, makes decisions according to
the following rule:1
if pi(x) = max
k∈M pk(x),
where p j(x) stands for the posterior probability
pi(x) = pi fi(x)
and f (x) denotes a probability density function
Reverting to the notion of a concept drift, we can distinguish two types of such an event, according to its inﬂuence
on the probability density function Eq. (6) or on the posterior
probability Eq. (5) :
1 We assume so-called “zero-one” lost function, i.e., if the cost of the
misclassiﬁcation error is the same and equals 1 and cost of correct
classiﬁcation is 0. Otherwise the classiﬁcation rule points on the class
for which the expectation value of the lost function is the smallest .
– virtual concept drift means that changes do not impact the
posterior probabilities, but affect the conditional probability density functions .
– real concept drift means that changes affect the posterior
probabilities and may impact unconditional probability
density function .
From the classiﬁcation point of view, the real concept drift is
important because it can strongly affect the shape of the decision boundary. The virtual drift does not affect the decision
rule, especially taking into consideration the Bayes decision
rule Eq. (4). Another drift taxonomy depends on the drift
impetuosity and here we can distinguish:
– slow changes, i.e., gradual or incremental drift.
– abrupt changes, i.e., sudden drift.
The presence of a concept drift can lead to serious deterioration of classiﬁer’s accuracy .
This is depicted in Fig. 1, where two types of concept drift
are shown.
Additionally, we can consider a reoccurring concept drift.
It may occur in cases of, e.g., seasonal phenomena as weather
prediction or client preferences of clothes or sport stores
 . Therefore, developing efﬁcient
methods which are able to deal with this type of change in
data stream is nowadays the focus of intense research.
The main aim of this paper is to introduce an efﬁcient
method of incremental data stream classiﬁcation, i.e., we will
consider the task where the concept drift is rather smooth and
the classiﬁer model will try to follow the model’s changes.
As implementation of the classiﬁer, we propose a novel modiﬁcation of the weighted one-class classiﬁer which tunes
the shape of its decision boundary on the basis of weights
assigned to the training objects. The main contribution of this
work is a novel one-class classiﬁer applied to this task with
thebuilt-inadaptationandforgettingmechanisms.Theevaluation of the proposed method is carried out on the basis of the
computer experiments on real and semi-synthetic data sets.
The outline of the work is as follows. First, the related works
on data stream classiﬁcation and one-class classiﬁers will
be presented. Then, the original algorithm will be described.
The following section is focusing on the results of the experimental research. The last part concludes the paper.
2 Related works
In this section, a short overview on the related ﬁelds will be
One-class classiﬁers with incremental learning
Fig. 1 Exemplary classiﬁer accuracy deterioration—sudden drift versus incremental one
2.1 Data stream classiﬁcation in the presence of concept
In general, the following approaches can be considered to
cope with drifting data stream.
– Training new classiﬁer every time when new data are
available. Such an approach is impractical and very
expensive, especially if drift occurs rapidly.
– Detecting concept drift in new data, and if these changes
are signiﬁcant enough then train the classiﬁer on the basis
of new data gathered after a drift occurred.
– Adopting an incremental learning algorithm for the classiﬁcation model to smoothly adapt to changing nature of
incoming objects.
The ﬁrst algorithms designed to deal with a drifting data
were STAGGER , IB3 , and the suite of FLORA algorithms . Now, there are plethora of methods proposed to
cope with the concept drift phenomenon. Basically, we can
organize them into the following groups:
1. Drift detection algorithms.
2. Online learners.
3. Sliding window-based solutions.
4. Ensemble approaches , but
this mechanism is not used by all classiﬁers dedicated to
data stream. Some evolving systems continuously adjust the
model to incoming data , what is called
implicit drift detection as opposed to
explicit drift detection methods that raise a signal to indicate
change. The detector can be based on changes in the probability distribution of the instances or classiﬁcation accuracy . Many detection algorithms are based on a knowledge of object labels after the
classiﬁcation to detect the presence of a concept drift, however as pointed out in Zliobaite , such approach is not
useful from a practical point of view.
Online learners relate to classiﬁcation algorithms that continuously update their classiﬁer parameters while processing the incoming data. According to Domingos and Hulten
 , such methods should meet some basic requirements:
– Each object must be processed only once in the course
of training.
– The memory and computing time are limited.
– The classiﬁer training could be interrupted several times
and its quality should not be lower than the classiﬁer
trained using batch mode.
Such classiﬁers work fast and are able to adapt their model
in a very ﬂexible manner. Among the others, the following
are the most popular online learners: Naïve Bayes, Neural
Networks , and Nearest Neighbor . It is worth noting the Concept-Adapting Very
Fast Decision Tree (CVFDT) algorithm ,
which is an extended version of the ultra fast decision tree,
which ensures consistency with incoming data by maintaining alternative subtrees.
The last groups of algorithms based on sliding windows
incorporatetheforgettingmechanism.Thisapproachisbased
on the assumption that the recently arrived data have higher
relevancy, because they contain characteristics of the current
context. Usually three strategies are used:
– selecting the instances by means of a sliding window that
cuts off older instances ;
– weighting the data according to their relevance;
B. Krawczyk, M. Wo´zniak
– applying bagging and boosting algorithms that focus on
misclassiﬁed instances .
When dealing with the sliding window, the main question is
how to adjust the window size. On the one hand, a shorter
window allows focusing on the emerging context, though
data may not be representative for a longer lasting context.
On the other hand, a wider window may result in mixing the
instances representing different contexts.
Therefore, certain advanced algorithms adjust the window size dynamically depending on the detected state [e.g.,
FLORA2 and ADWIN2 ] or multiple windows may even be used
 .
The last group consists of algorithms that incorporate a
set of classiﬁers . It has been shown that a collective decision can
increase classiﬁcation accuracy because the knowledge that
is distributed among the classiﬁers may be more comprehensive. This premise is true if the set consists of diverse
members .
In static environments, diversity may refer to the classiﬁer
model, the feature set, or the instances used in training.
In a changing environment, diversity can also refer to
the context. This makes ensemble systems interesting for
researchers dealing with concept drift. We can distinguish
three main approaches related to classiﬁer ensembles for data
1. Dynamic combiners, where individual classiﬁers are
trained in advance and their relevance to the current context is evaluated dynamically while processing subsequent data .
3. Dynamic changes of the line-up of ensemble, namely,
individual classiﬁers are evaluated dynamically and the
worst one is replaced by a new individual trained on the
most recent data or the Accuracy Weighted Ensemble (AWE) keeps a ﬁxed-size set of classiﬁers. Data are
collected in data chunks, which are used to train new classiﬁers. The SEA uses a majority voting, whereas the AWE
makes decision on the basis of weighted voting. Dynamic
Weighted Majority (DWM) algorithm reduces the weight when the classiﬁer makes an incorrect decision. Eventually, the classiﬁer is removed from the
ensemble when its weight falls below a given threshold. Independently, a new classiﬁer is added to the ensemble when the
committee makes a wrong decision. Jackowski proposes a
classiﬁer ensemble training methods dedicated to a so-called
recurring context . To select an ensemble
for the current model, the ensemble pruning method based on
evolutionary programming is employed. Wo´zniak et al. propose the dynamic ensemble model called Weighted Aging
Ensemble (WAE) . It can modify the
line-up of a classiﬁer ensemble on the basis of three factors: diversity measure, overall ensemble accuracy, and time
that given classiﬁer has spent as a member of the ensemble
In this paper, we present a novel adaptive weighted oneclass classiﬁer that is able to change itself according to the
nature of received data streams. We propose to use principles
of incremental learning and forgetting to adjust shape of the
decision boundary according to the concept changes in new
data chunks. The learning and forgetting in data streams is
realized by modifying weights assigned to objects—we propose how to calculate weights for new incoming objects to
use their information to change the classiﬁer and how to forget the old objects to prevent the overﬁtting of the classiﬁer
and uncontrolled increase in the volume of the stored data.
Thus, let us shortly present the idea of one-class classiﬁer.
2.2 One-class classiﬁcation
One-class classiﬁcation (OCC) is one of the most challenging areas of machine learning. It is assumed that during the
classiﬁer training stage we have at our disposal only objects
coming from a single class distribution—the target concept
 . During the exploitation phase of such a
classiﬁer, there may appear new objects originating from different distributions than the target class. They are known as
outliers and should be rejected by the trained model. Therefore, one-class classiﬁcation aims at deriving a classiﬁcation
boundary that may separate the known target objects from
possible outliers that may appear. No assumptions about the
nature of outliers should be made.
OCC is an attractive approach for data stream classiﬁcation, because it can be used for binary classiﬁcation without
an access to counterexamples, decomposing a multi-class
data stream, outlier detection or novel class recognition. Most
of data stream methods focus on supervised learning, which
require a fully labeled data set for training. However, labeling
an entire chunk of data stream can be expensive or hard to
obtain, which limits the real-life applications of such methods . Additionally, let us consider the intrusion detection for computer system (IDS/IPS)
One-class classiﬁers with incremental learning
 . The target class covering the normal and
safe messages is unchanged but the malicious messages as
intrusion are still changing, because the malicious users are
trying to lead security systems on. Therefore, they are inventing a variety of attacks. If we concentrate on normal messages only, as target class, then we can believe that we are
able to train classiﬁers which are capable of distinguishing
normal messages from malicious messages without knowledge about the outlier class. It could protect our security system against so-called “zero day attack” as well. Furthermore,
OCCs require only a small number of positively labeled
examples for initial training, which can be a valuable property in case of non-stationary classiﬁcation.
Most of the existing works on OCC have not explicitly
dealt with the changing nature of the input data . They are based on an underlying assumption
that the training data set does not contain any uncertainty
information and properly represents the examined concept.
However, in many real-world applications, data change its
nature over time—which is a vital problem for data stream
analysis . For example, in environmental monitoring applications, data may change according to the examined conditions and what once was considered an outlier may in near future become a representative of the target concept. This kind of dynamic information
(typically ignored in most of the existing one-class learning methods) is critically important for capturing the full
picture of the examined phenomenon. Therefore, there is a
need for introducing novel, adaptive techniques for dealing
with non-stationary data sets .
On the other hand, OCC can be very useful for a variety of
real-life data stream applications. Such an practical example would include a process of monitoring the nuclear power
plant . Here, the data arrive as
a stream of sensor outputs and can be subject to changes
due to the different energy demand ratios. It is easy to gather
labeledexamplesofproperbehaviorofsuchaplant,butcounterexamples are obviously dangerous to collect. This is an
example of real-life situation in which an incremental OCC
model, that can adapt to changes in data, can be of a high
So far the OCC problem for data streams, especially in
the presence of concept drift, has not been investigated thoroughly. One should mention several proposal of on-line One-
Class Support Vector Machines , an oneclass modiﬁcation of very fast decision trees (OcVFDT) and uncertain one-class classiﬁer for summarizing concepts in the presence of uncertainty applying a
generated bound score into a one-class SVM-based learning phase . Few ensemble techniques, based
on chunks of data and standard one-class classiﬁers, have
also been recently introduced 
 achieves this goal by computing a closed boundary in a form of a hypersphere enclosing all the objects from iT . During the exploitation phase, a
decision made about the new object is based upon checking
whether it falls inside the hypersphere. If so, the new object
is labeled as one belonging to iT . Otherwise it belongs to
iO. The center a and a radius R are the two parameters that
are sufﬁcient for describing such a decision hypersphere. To
have a low acceptance of the possible outliers, the volume
of this d-dimensional hypersphere, which is proportional to
Rd, should be minimized in such a way that tightly encompasses all available objects from iT . The minimization of Rd
implies minimization with respect to R2. Following this, the
minimization functional may be formulated as follows:
(a, R) = R2,
with respect to the constraint:
∀i∈{1,...,N}
∥xi −a∥2 ≤R2,
where xi are objects from iT , and N stands for the quantity of
training objects. Additionally, to allow the fact that there may
have been some outliers in the training set and to increase
the robustness of the trained classiﬁer, some objects with
distance to a greater than R are allowed in the training set,
but associated with an additional penalty factor. This is done
identically as in a standard SVM by the introduction of slack
variables ξi.
This concept can be further extended to a weighted
one-class support vector machine (WOCSVM) by the introduction of weights wi that
allows for an association of an importance measure to each of
the training objects. This forces slack variables ξi, to be additionally controlled by wi. If with object xi there is associated
a small weight wi, then the corresponding slack variable ξi
indicates a small penalty. In effect, the corresponding slack
variable will be larger, allowing xi to lie further from the
center a of the hypersphere. This reduces an impact of xi on
the shape of a decision boundary of WOCSVM.
Using the above-mentioned ideas, we can modify the minimization functional:
(a, R) = R2 + C
B. Krawczyk, M. Wo´zniak
with the modiﬁed constraints that almost all objects are
within the hypersphere:
∀i∈{1,...,N}
∥xi −a∥2 ≤R2 + ξi,
where ξi ≥0, 0 ≤wi ≤1. Here, C stands for a parameter
that controls the optimization process—the larger the C, the
less the outliers are allowed with the increase of the volume
of the hypersphere.
For establishing weights, we may use techniques dedicated to a weighted multi-class support vector machines
 . In this paper, we propose to use a following
wi = |xi −xmean|
where δ > 0 is used to prevent the case of wi = 0. The value
of xmean is computed with the usage of all available learning samples. In case of data streams, this would mean that
samples from the incoming chunk are used together with the
samples from previous chunks that have not been discarded
in the forgetting process.
3 Adaptive WOCSVM for incremental data stream
We assume that the classiﬁed data stream is given in a form
of data chunks. At the beginning, we have at our disposal
an initial data set DS0 that allows to train the ﬁrst version
of the classiﬁer. Then, for each i-th iteration, we receive an
additional chunk of data labeled as DSi. We assume that
there is a possibility of concept drift presence in the incoming
chunks of data. Therefore, it would be valuable to adjust the
one-class classiﬁer to changes in the nature of data. The idea
is depicted in Fig. 2. Later, we will train the classiﬁer on the
incoming chunk, but it will be employed (evaluated) on the
following one.
In case when we are using a WOCSVM trained on DS0
for all new incoming data, we notice a signiﬁcant drop in
performance—and after few new chunks of data, it is possible
that this model will not be able to handle the new objects, as
objects in the stream have shifted signiﬁcantly from their initial distributions. To prevent this from happening, we propose
to adapt an one-class classiﬁer incrementally with the new
incoming data to deal with the presence of concept drift and
allow for a more efﬁcient novelty detection in data streams.
We propose to apply the classiﬁer adaptation in a changing
environment via modiﬁcation of weights assigned to objects
from the data set. We introduce an incremental learning procedure, meaning that the data set DS will consist of all available chunks of data at the given i-th moment. Additionally,
we augment our model with a forgetting mechanism to discard objects that are no longer relevant to the current state of
the analyzed concept. The proposed algorithm is summarized
in a pseudo-code manner in Algorithm 1.
Algorithm 1 Adaptive WOCSVM with forgetting
Require: input data stream,
data chunk size,
classiﬁer training procedure()
treshold ε
collect new data chunk DSi
DS = DS ∪DSi
calculate the weights of the examples according to Eq. (11)
correct the weights of the data from DSi (according to Eq. (12) if
applicable)
correct the weights of the data to simulate forgetting (according
to Eq. (13),(14), or (15))
remove from DS all objects with weights smaller than ε
classiﬁer ←classiﬁer training procedure(DS, weights assigned
to objects in DS)
11: until end of the input data stream
Fig. 2 Algorithm training and testing phases on the successive data chunks
One-class classiﬁers with incremental learning
Let us present the details of the crucial algorithm’s phases.
3.1 Incremental learning
We propose to extend the WOCSVM concept by adding an
incremental learning principle to it . We use
the passive incremental learning. In this method, new data
are added without considering its importance. Incremental
learning should allow to change the shape of the previously
learned decision boundary. Here, we propose two strategies
for updating our classiﬁer with new, incoming data by:
– assigning weights to objects from DSk according to
Eq. (11). This is motivated by the fact that in the incoming
data chunk not all objects should have the same impact
on the shape of the new decision boundary,
– assigning the highest weights to objects coming from the
new data chunks:
∀xi∈DSk wi = 1.
This is motivated by the fact that in the presence of the
concept drift objects from a new chunk, therefore, should
have a top priority in forming the new decision boundary.
3.2 Incremental forgetting
If we apply only the incremental learning principle to the
WOCSVM, the decision boundary will become more and
more complex with each additional chunk of data. This
enlarges our data set and the memory requirements, what
is impractical and leads to a poor generalization ability. This
can be avoided by forgetting unnecessary, outdated examples . It seems natural that the degree of
importance of data reduces as the time passes.
The simplest way is a removal of objects coming from
the previous (or oldest iteration). Yet in this way, we discard
all the information they carried—while they still may have
some valuable inﬂuence on the classiﬁcation boundary (e.g.,
in case of a gradual concept drift where the changes of the
data distribution are not rapid). A method that allows for a
gradual decrease of the object inﬂuence over time seems a
far more attractive idea.
Identically as in incremental learning, we modify the
weights assigned to objects to change the inﬂuence of the
data on the shape of the decision boundary. In this case, we
propose to reduce weights of objects from previous chunks
of data in each iteration.
We propose three methods for calculating new weights for
objects delivered in previous iterations:
– gradual decrease of weights with respect to their initial
importance—here, we introduce a denomination factor τ
that is a user-speciﬁed value used to decrease the weights
in each iteration:
This is motivated by the fact that if an object had initially
assigned a higher weight, it had a bigger importance for
the classiﬁer. As such, these objects can be valuable for a
longer period of time than objects with initial low weights.
In this approach, their weights will sooner approach the 0
value and they will be removed in a fewer iterations than
objects with high initial weights.
– aligned decrease of weights without considering their initial importance—here, we introduce a time factor κ that is
a user-speciﬁed value standing for a number of iterations
after which the object should be removed:
stands for the initial value of the weight
assigned to ith object. As we can see, the weights of
objects are reduced with each iteration till they are equal
to 0 (and removed from DS)—the main difference is that
this method does not consider the initial importance of
data. This means that all the objects from the k-th data
chunk will be removed in the same moment, after κ iterations. This is motivated by the fact that changes in the
dynamic environment can be unpredictable and quickly
move from the original distribution—therefore, data from
previous steps may quickly loose its importance.
– decrease of weights according to the following sigmoidal
i 2 exp(−β(k + 1))(1 + exp(−β(k + 1))
where β is responsible for the forgetting rapidity. Its value
should be determined experimentally. This method allows
for a smooth forgetting of previous data with the rapidity
of changes controlled by user.
4 Experimental investigations
Our experimental aims were as follow:
• toestablish,ifapplyingprinciplesofincrementallearning
and forgetting in one-class classiﬁcation will allow to
handle data streams with the presence of concept drift
efﬁciently;
• to examine the effectiveness of proposed incremental
learning and forgetting schemes that were introduced in
this paper.
B. Krawczyk, M. Wo´zniak
Table 1 Details of data stream benchmarks used in the experiments
Drift type
4.1 Data sets
There is an abundance of benchmarks for comparing machine
learning algorithms working in static environments. However, for non-stationary data streams, there is still just a few
publicly available data sets to work with.2 Most of them are
artiﬁcially generated ones, with only some real-life examples. Following the standard approaches found in the literature, we decided to use both artiﬁcial and real-life data sets.
Additionally, to the best knowledge of authors , there are
no data stream benchmarks for one-class classiﬁcation problems. Therefore, we need to apply a transformation changing
existing multi-class streams into one-class data. To do this,
we need to select a single class to serve as the target concept and use remaining class(es) as outliers. Details of used
benchmarks can be found in Table 1.
Below, we provide a short description of each data set:
– RBF: the radial basis function generator outputs a userdeﬁned number of drifting centroids. Each of such centroids is described by a class label, position, weight, and
standard deviation. The created data set is deﬁned as a
two-class problem with 1,000,000 objects in each class.
Four gradual recurring concept drifts were deﬁned. To
change this data set into a one-class problem, we use
positive class as the target class and negative class as
outlier class.
– LED: this is an artiﬁcial data stream generator, described
by 24 features that describe an output of seven-segment
LED display. We generate a data set consisting of
1,000,000 objects and with the presence of gradual concept drift. We use digit 0 as the target class and remaining
digits as outliers.
– COV: forest cover type is a real-life data set that deals
with different cover types in four wilderness areas. Each
example is described by 53 cartographic features that
allow to categorize cover type into one out of seven
classes. There are 581,012 examples in this data set. We
use Lodgepole Pine (cover type 2) class as target class
2 
(as it is the most numerous one, with 283,301 examples)
and remaining six classes as outliers.
– ELEC: electricity is a real-life data set describing ﬂuctuations in energy prices from the electricity market. This
benchmark consists of 45,312 objects, each described by
seven features. We use the positive class as the target
concept and negative class as outliers.
– AIR: airlines in a real-life data set dealing with the problem of predicting ﬂight delays. This data stream consists
of 539,383 objects, each described by seven features. We
use the positive class (ﬂight on time) as the target concept
and negative class (ﬂight delay) as outliers.
4.2 Set-up
For the purposes of experimental analysis, we use as a base
model WOCSVM with RBF kernel, σ = 0.1 and C = 10.
We use seven different models of the proposed oneclass classiﬁers in our experiments—each with applied different combination of incremental learning and forgetting
mechanisms. The details and abbreviations of these algorithms are given in Table 2. As reference methods, we
apply an Incremental and On-line One-Class Support Vector Machine (IOCSVM, with RBF kernel, σ = 0.1 and
C = 10) and One-Class Very Fast Decision Tree (OcVFDT) .
The data block size used for creating data chunks was
d = 2,500 for all the data sets. For all experiments, we set
the threshold for discarding old objects ε = 0.05
As we deal with one-class task, for training purposes we
utilize only objects belonging to the target class in given
chunks. For testing, we use all of objects from the incoming
chunk (both target class and outliers).
An important aspect of designing an experiment for nonstationary data was choosing an appropriate metric for evaluating examined methods. We decided to measure classiﬁcation accuracy and time efﬁciency. We use the data chunkbased evaluation method, described earlier in this paper.
4.3 Results and discussion
Theaverageaccuraciesachievedbytestedalgorithmsarepresented in Table 3, their average training times on data chunk
are presented in Table 4, and their average memory usage is
presented in Table 5. Additionally, we present detailed accuracy behavior of the proposed incremental learning and forgetting schemes on each of analyzed data sets in Figs. 3, 4, 5,
6 and 7 to allow for a visual inspection of their performance.
The experimental analysis allows us to draw several interesting conclusions about the performance of the proposed
schemes for one-class classiﬁcation in non-stationary environments. First of all, analysis of performance of a standard
One-class classiﬁers with incremental learning
Table 2 Details of used versions of the proposed one-class classiﬁcation model with different incremental learning and forgetting procedures
Description
Standard WOCSVM without any adaptation mechanism
Trained on a single incoming chunk, without any previous data
Incremental learning: assigning weights to new objects according to Eq. (11)
Forgetting: gradual decrease of weights according to Eq. (13) with τ = 0.15
Incremental learning: assigning highest weights to new objects
Forgetting: gradual decrease of weights according to Eq. (13) with τ = 0.15
Incremental learning: assigning weights to new objects according to Eq. (11)
Forgetting: aligned decrease of weights without considering their initial importance according to Eq. (14) with κ = 0.1
Incremental learning: assigning highest weights to new objects
Forgetting: aligned decrease of weights without considering their initial importance according to Eq. (14) with κ = 0.1
Incremental learning: assigning weights to new objects according to Eq. (11)
Forgetting: decrease of weights according to proposed function Eq. (15) with β = 9
Incremental learning: assigning highest weights to new objects
Forgetting: decrease of weights according to proposed function Eq. (15) with β = 9
Table 3 Average classiﬁcation
accuracies (%)
Bold values indicate the most
accurate methods for a given
Table 4 Average chunk training
time in seconds [s]
Table 5 Average memory
consumption in megabytes
WOCSVM shows us that canonical one-class classiﬁers display a very poor adaptation abilities in the presence of changing concepts. We tested the performance of this classiﬁer
without any incremental learning or forgetting scheme, so
it was trained each time from a scratch on incoming data
chunks. This limited its adaptation, as it did not have any
access to previous objects—which can be very important
in case of gradual drifts. Without any mechanism to incorporate new observations, while storing the previous ones,
standard WOCSVM could not adapt sufﬁciently its deci-
B. Krawczyk, M. Wo´zniak
Fig. 3 Accuracies of proposed incremental learning and forgetting methods for RBF data set
Fig. 4 Accuracies of proposed incremental learning and forgetting methods for LED data set
sion boundary, which resulted in an extended acceptance of
When comparing our approaches to the reference methods (IOCSVM and OcVFDT), one may see that they outperform these state-of-the-art classiﬁers for non-stationary
single class streams. This is because most used one-class
methods for data streams (such as the reference ones) do
not have embedded methodologies for dealing with shifting
concept—they just work in an incremental way. Our method
is able to efﬁciently adapt to the changing context by forgetting the no longer relevant samples, and thus increasing its
overall accuracy. One should note that OcVFDT has much
lower training time than our methods, but this is at the cost
of a much lower overall accuracy.
One-class classiﬁers with incremental learning
Fig. 5 Accuracies of proposed incremental learning and forgetting methods for COV data set
Fig. 6 Accuracies of proposed incremental learning and forgetting methods for ELEC data set
When examining the proposed schemes for incremental
learning, one may see two major trends. First, calculating
new weights based on the distance returns in most cases an
inferior performance to approach based on assigning highest
weights for incoming objects. This can be explained by the
fact that weight calculation in the ﬁrst scheme is based on the
distance between the new objects and the center of the oneclass classiﬁer hypersphere. As the center of the classiﬁer is
moving with each new training data chunk (as it is adapting to changes in data), weights quickly lose their meaning
with the change of inner-class data distribution. Additionally, data from incoming chunk represent the current state of
the classiﬁed target concept and its inﬂuence on the shape
of the decision boundary should be boosted. That is why the
B. Krawczyk, M. Wo´zniak
Fig. 7 Accuracies of proposed incremental learning and forgetting methods for AIR data set
simpler method based on assigning highest weights to new
object returns more accurate classiﬁers. Furthermore, this
approach signiﬁcantly reduces the computational time and
memory consumption of proposed methods, as can be seen
in Tables 4 and 5. This is due to the fact that calculating distances and new weights for incoming examples requires additionalprocessingtime,whilesimplyassigninghighestweight
alleviates the computational load connected with processing
new chunks.
As for the forgetting mechanism, we can see their important inﬂuence on the proposed method. They allow to store
useful information for some time, but allow to gradually
reduce their level of inﬂuence on the shape of the decision boundary. This is of high importance for constructing
efﬁcient models for cases with gradual or recurring drifts.
From the three proposed methods, we can see that gradual
decrease of weights with respect to their initial importance
and decrease of weights according to proposed sigmoidal
function returns the best performance. This can be explained
by the fact that these methods take into consideration the initial weight of examples and remove them according to their
relevance. We can assume that if a given object was important
in previous chunk, it can still be of some use in few incoming
partitionsofdatastream(especiallyincaseofslowchangesin
the environment). Out of these two methods, the one based
on our sigmoidal function returns superior results in three
out of ﬁve data sets. For the remaining two benchmarks, all
proposed forgetting mechanisms output similar accuracies.
The memory and time consumptions of these methods are
quite similar, but one may see that sigmoidal-based forgetting tends to return slightly less complex models.
Taking the mentioned factors into consideration, one may
conclude that the best one-class classiﬁcation model for data
streams with the presence of concept drift would be returned
by combining incremental learning by assigning highest
weights to objects coming from the new data chunk with
forgetting by sigmoidal-based function. Experimental results
show us that such a combination returns superior results to
all other algorithms presented in this paper. Additionally, it
has the lowest memory consumption and one of the lowest
time complexities. This allows us to draw a conclusion that
this model is a good choice for tackling one-class classiﬁcation for non-stationary data in changing environments both in
terms of overall accuracy for dichotomizing between target
concept and outliers, and in terms of overall computational
complexity and ability to work in real-time pattern classiﬁcation systems.
5 Conclusions
In this work, we introduced a modiﬁed Weighted One-
Class Support Vector Machine augmented with the principles of incremental learning and forgetting. These techniques
allowed to adapt the decision boundary of the classiﬁer to
changes in the incoming data.
One-class classiﬁers with incremental learning
Our proposition focused on the modiﬁcations of weights
used by WOSVM. We employed the incremental training of
the weights which was additionally supported by forgetting
mechanism. The forgetting boosts classiﬁer’s ability to generalization and strongly reduces the amount of stored data,
because outdated examples are removed from the memory.
We conclude with suggestion that the efﬁcient one-class
classiﬁcation model for data streams with the presence of
concept drift would be returned by combining incremental
learning by assigning highest weights to objects coming from
the new data chunk with forgetting by sigmoidal-based function. Experimental results show us that such a combination
returns superior results to all other algorithms presented in
this paper. Additionally, it has the lowest memory consumption and one of the lowest time complexities.
As the achieved results are very promising, then we
decided to continue our work with them in the future. The
following research direction will be explored:
– implementing and testing new forgetting mechanisms,
– testing our approach on different types of concept drift,
especially on sudden shift which requires drift detector
and shorter restoration time of the adaptation strategies,
– implementing active learning mechanisms which do not
require labels of each examples in a chunk,
– implementing classiﬁer ensemble based on the proposed
incremental WOCSVM.
These topics can lead to a new propositions of efﬁcient
classiﬁers for one-class classiﬁcation in data streams with
changing concept.
Open Access
This article is distributed under the terms of the Creative
Commons Attribution License which permits any use, distribution, and
reproduction in any medium, provided the original author(s) and the
source are credited.