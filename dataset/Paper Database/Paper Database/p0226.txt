Active Learning for Probability Estimation Using
Jensen-Shannon Divergence
Prem Melville1, Stewart M. Yang2, Maytal Saar-Tsechansky4, and Raymond Mooney3
1 Dept. of Computer Sciences, Univ. of Texas at Austin
 
2 
3 
4 Red McCombs School of Business, Univ. of Texas at Austin
 
Abstract. Active selection of good training examples is an important approach
to reducing data-collection costs in machine learning; however, most existing
methods focus on maximizing classiﬁcation accuracy. In many applications, such
as those with unequal misclassiﬁcation costs, producing good class probability
estimates (CPEs) is more important than optimizing classiﬁcation accuracy. We
introduce novel approaches to active learning based on the algorithms Bootstrap-
LV and ACTIVEDECORATE, by using Jensen-Shannon divergence (a similarity measure for probability distributions) to improve sample selection for optimizing CPEs. Comprehensive experimental results demonstrate the beneﬁts of
our approaches.
Introduction
Many supervised learning applications require more than a simple classiﬁcation of instances. Often, also having accurate Class Probability Estimates (CPEs) is critical for
the task. Class probability estimation is a fundamental concept used in a variety of applications including marketing, fraud detection and credit ranking. For example, in direct marketing the probability that each customer would purchase an item is employed
in order to optimize marketing budget expenditure. Similarly, in credit scoring, class
probabilities are used to estimate the utility of various courses of actions, such as the
proﬁtability of denying or approving a credit application. While prediction accuracy of
CPE improves with the availability of more labeled examples, acquiring labeled data
is sometimes costly. For example, customers’ preferences may be induced from customers’ responses to offerings; but solicitations made to acquire customer responses
(labels) may be costly, because unwanted solicitations can result in negative customer
attitudes. It is therefore critical to reduce the number of label acquisitions necessary to
obtain a desired prediction accuracy.
The active learning literature offers several algorithms for cost-effective label
acquisitions. Active learners acquire training data incrementally, using the model induced from the available labeled examples to identify helpful additional training examples for labeling. Different active learning approaches employ different utility scores to
estimate how informative each unlabeled example is, if it is labeled and added to the
J. Gama et al. (Eds.): ECML 2005, LNAI 3720, pp. 268–279, 2005.
c⃝Springer-Verlag Berlin Heidelberg 2005
Active Learning for Probability Estimation
training data. When successful, active learning methods reduce the number of instances
that must be labeled to achieve a particular level of accuracy. Almost all work in active
learning has focused on acquisition policies for inducing accurate classiﬁcation models and thus are aimed at improving classiﬁcation accuracy. Although active learning
algorithms for classiﬁcation can be applied for learning accurate CPEs, they may not
be optimal. Active learning algorithms for classiﬁcation may (and indeed should) avoid
acquisitions that can improve CPEs but are not likely to impact classiﬁcation. Accurate
classiﬁcation only requires that the model accurately assigns the highest CPE to the
correct class, even if the CPEs across classes may be inaccurate. Therefore, to perform
well, active learning methods for classiﬁcation ought to acquire labels of examples that
are likely to change the rank-order of the most likely class. To improve CPEs, however,
it is necessary to identify potential acquisitions that would improve the CPE accuracy,
regardless of the implications for classiﬁcation accuracy. Bootstrap-LV is an active
learning approach designed speciﬁcally to improve CPEs for binary class problems. The
method acquires labels for examples for which the current model exhibits high variance
for its CPEs. BOOTSTRAP-LV was shown to signiﬁcantly reduce the number of label
acquisitions required to achieve a given CPE accuracy compared to random acquisitions
and existing active learning approaches for classiﬁcation.
In this paper, we propose two new active learning approaches. In contrast to
BOOTSTRAP-LV, the methods we propose can be applied to acquire labels to improve the CPEs of an arbitrary number of classes. The two methods differ by
the measures each employs to identify informative examples: the ﬁrst approach,
BOOTSTRAP-JS, employs the Jensen-Shannon divergence measure (JSD) . The
second approach, BOOTSTRAP-LV-EXT, uses a measure of variance inspired by the
local variance proposed in BOOTSTRAP-LV. We demonstrate that for binary class
problems,BOOTSTRAP-JS is at least comparable and often superior to BOOTSTRAP-
LV. In addition, we establish that for multi-class problems, BOOTSTRAP-JS and
BOOTSTRAP-LV-EXT identify particularly informative examples that signiﬁcantly improve the CPEs compared to a strategy in which a representative set of examples are acquired uniformly at random. This paper also extends the work of Melville and Mooney
 , which introduced a method, ACTIVEDECORATE, for active learning for classiﬁcation. They compared two measures for evaluating the utility of examples - label margins
and JSD. The results showed that both measures are effective for improving classi-
ﬁcation accuracy, though JSD is less effective than margins. It was conjectured that
JSD would be a particularly useful measure when the objective is improving CPEs. We
demonstrate here that, for the task of active learning for CPE, ACTIVEDECORATE using
JSD indeed performs signiﬁcantly better than using margins.
Jensen-Shannon Divergence
Jensen-Shannon divergence (JSD) is a measure of the “distance” between two probability distributions which can also be generalized to measure the distance (similarity) between a ﬁnite number of distributions . JSD is a natural extension of the
Kullback-Leibler divergence (KLD) to a set of distributions. KLD is deﬁned between
two distributions, and the JSD of a set of distributions is the average KLD of each
P. Melville et al.
distribution to the mean of the set. Unlike KLD, JSD is a true metric and is bounded.
If a classiﬁer can provide a distribution of class membership probabilities for a given
example, then we can use JSD to compute a measure of similarity between the distributions produced by a set (ensemble) of such classiﬁers. If Pi(x) is the class probability distribution given by the i-th classiﬁer for the example x (which we will abbreviate as Pi) we can then compute the JSD of a set of size n as JS(P1, P2, ..., Pn) =
i=1 wiPi) −n
i=1 wiH(Pi); where wi is the vote weight of the i-th classiﬁer in
the set;1 and H(P) is the Shannon entropy of the distribution P = {pj : j = 1, ..., K},
deﬁned as H(P) = −K
j=1 pj log pj. Higher values for JSD indicate a greater spread
in the CPE distributions, and it is zero if and only if the distributions are identical. JSD
has been successfully used to measure the utility of examples in active learning for improving classiﬁcation accuracy . A similar measure was also used for active learning
for text classiﬁcation by McCallum and Nigam .
Bootstrap-LV and JSD
To the best of our knowledge, Bootstrap-LV is the only active learning algorithm
designed for learning CPEs. It was shown to require signiﬁcantly fewer training examples to achieve a given CPE accuracy compared to random sampling and uncertainty
sampling, which is an active learning method focused on classiﬁcation accuracy .
Bootstrap-LV reduces CPE error by acquiring examples for which the current model
exhibits relatively high local variance (LV), i.e., the variance in CPE for a particular example. A high LV for an unlabeled example indicates that the model’s estimation of its
class membership probabilities is likely to be erroneous, and the example is therefore
more desirable to be selected for learning.
Bootstrap-LV, as deﬁned in is only applicable to binary class problems. We
ﬁrst provide the details of this method, and then describe how we extended it to solve
multi-class problems. Bootstrap-LV is an iterative algorithm that can be applied to any
base learner. At each iteration, we generate a set of n bootstrap samples from
the training set, and apply the given learner L to each sample to generate n classiﬁers Ci : i = 1, ..., n. For each example in the unlabeled set U, we compute a
score which determines its probability of being selected, and which is proportional
to the variance of the CPEs. More speciﬁcally, the score for example xj is computed
i=1 (pi(xj) −pj)2)/pj,min; where pi(xj) denotes the estimated probability the
classiﬁer Ci assigns to the event that example xj belongs to class 0 (the choice of performing the calculation for class 0 is arbitrary, since the variance for both classes is
identical), pj is the average estimate for class 0 across classiﬁers Ci, and pj,min is the
average probability estimate assigned to the minority class by the different classiﬁers.
Saar-Tsechansky and Provost attempt to compensate for the under-representation of
the minority class by introducing the term pj,min in the utility score. The scores produced for the set of unlabeled examples are normalized to produce a distribution, and
then a subset of unlabeled examples are selected based on this distribution. The labels
for these examples are acquired and the process is repeated.
1 Our experiments use uniform vote weights, normalized to sum to one.
Active Learning for Probability Estimation
The model’s CPE variance allows the identiﬁcation of examples that can improve
CPE accuracy. However as noted above, the local variance estimated by Bootstrap-LV
captures the CPE variance of a single class and thus is not applicable to multi class
problems. Since we have a set of probability distributions for each example, we can
instead, use an information theoretic measure, such as JSD to measure the utility of
an example. The advantage to using JSD is that it is a theoretically well-motivated
distance measure for probability distributions that can be therefore used to capture the uncertainty of the class distribution estimation; and furthermore, it naturally
extends to distributions over multiple classes. We propose a variation of BOOTSTRAP-
LV, where the utility score for each example is computed as the JSD of the CPEs
produced by the set of classiﬁers Ci. This approach, BOOTSTRAP-JS, is presented in
Algorithm 1.
Our second approach, BOOTSTRAP-LV-EXT, is inspired by the Local Variance concept proposed in BOOTSTRAP-LV. For each example and for each class, the variance
in the prediction of the class probability across classiﬁers Ci, i = 1, ..., n is computed,
capturing the uncertainty of the CPE for this class. Subsequently, the utility score for
each potential acquisition is calculated as the mean variance across classes, reﬂecting the average uncertainty in the estimations of all classes. Unlike BOOTSTRAP-LV,
BOOTSTRAP-LV-EXT does not incorporate the factor of pj,min in the score for multiclass problems, as this is inappropriate in this scenario.
Algorithm 1. Bootstrap-JS
Given: set of training examples T, set of unlabeled training examples U, base learning algorithm L, number of bootstrap samples n, size of each sample m
1. Repeat until stopping criterion is met
Generate n bootstrap samples Bi, i = 1, ..., n from T
Apply learner L to each sample Bi to produce classiﬁer Ci
For each xj ∈U
∀Ci generate CPE distribution Pi(xj)
scorej = JS(P1, P2, ..., Pn)
∀xj ∈U, D(xj) = scorej/
Sample a subset S of m examples from U based on the distribution D
Remove examples in S from U and add to T
10. Return C = L(T)
ActiveDecorate and JSD
ACTIVEDECORATE is an active learning method that selects examples to be labeled so
as to improve classiﬁcation accuracy . It is built on the Query by Committee (QBC)
framework for selective sampling ; and has been shown to outperform other QBC
approaches, Query by Bagging and Query by Boosting. ACTIVEDECORATE is based
on DECORATE , which is a recently introduced ensemble meta-learner that directly constructs diverse committees of classiﬁers by employing specially-constructed
artiﬁcial training examples.
P. Melville et al.
Given a pool of unlabeled examples, ACTIVEDECORATE iteratively selects examples to be labeled for training. In each iteration, it generates a committee of classiﬁers
by applying DECORATE to the currently labeled examples. Then it evaluates the potential utility of each example in the unlabeled set, and selects a subset of examples with
the highest expected utility. The labels for these examples are acquired and they are
transfered to the training set. The utility of an example is determined by some measure
of disagreement in the committee about its predicted label. Melville and Mooney 
compare two measures of utility for ACTIVEDECORATE— margins and JSD. Given
the CPEs predicted by the committee for an example,2 the margin is deﬁned as the difference between the highest and second highest predicted probabilities. It was shown
that ACTIVEDECORATE using either measure of utility produces substantial error reductions in classiﬁcation compared to random sampling. However, in general, using
margins produces greater improvements. Using JSD tends to select examples that reduce the uncertainty in CPE, which indirectly helps to improve classiﬁcation accuracy.
On the other hand, ACTIVEDECORATE using margins focuses more directly on determining the decision boundary. This may account for its better classiﬁcation performance. It was conjectured that if the objective is improving CPEs, then JSD may be a
better measure.
In this paper, we validate this conjecture. In addition to using JSD, we made two
more changes to the original algorithm, each of which independently improved its performance. First, each example in the unlabeled set is assigned a probability of being
sampled, which is proportional to the measure of utility for the example. Instead of selecting the examples with the m highest utilities, we sample the unlabeled set based on
the assigned probabilities (as in BOOTSTRAP-LV). This sampling has been shown to
improve the selection mechanism as it reduces the probability of adding outliers to the
training data and avoids selecting many similar or identical examples .
The second change we made is in the DECORATE algorithm. DECORATE ensembles
are created iteratively; where in each iteration a new classiﬁer is trained. If adding
this new classiﬁer to the current ensemble increases the ensemble training error, then
this classiﬁer is rejected, else it is added to the current ensemble. In previous work,
training error was evaluated using the 0/1 loss function; however, DECORATE can use
any loss (error) function. Since we are interested in improving CPE we experimented
with two alternate error functions — Mean Squared Error (MSE) and Area Under the
Lift Chart (AULC) (deﬁned in the next section). Using MSE performed better on the
two metrics used, so we present these results in the rest of the paper. Our approach,
ACTIVEDECORATE-JS, is shown in Algorithm 2.
Experimental Evaluation
Methodology
To evaluate the performance of the different active CPE methods, we ran experiments
on 24 representative data sets from the UCI repository . 12 of these datasets were
2 The CPEs for a committee are computed as the simple average of the CPEs produced by its
constituent classiﬁers.
Active Learning for Probability Estimation
Algorithm 2. ActiveDecorate-JS
Given: set of training examples T, set of unlabeled training examples U, base learning algorithm L, number of bootstrap samples n, size of each sample m
1. Repeat until stopping criterion is met
Generate an ensemble of classiﬁers, C∗= Decorate(L, T, n)
For each xj ∈U
∀Ci ∈C∗generate CPE distribution Pi(xj)
scorej = JS(P1, P2, ..., Pn)
∀xj ∈U, D(xj) = scorej/
Sample a subset S of m examples from U based on the distribution D
Remove examples in S from U and add to T
9. Return Decorate(L, T, n)
two-class problems, the rest being multi-class. For three datasets (kr-vs-kp, sick, and
optdigits), we used a random sample of 1000 instances to reduce experimentation time.
All the active learning methods we discuss in this paper are meta-learners, i.e., they
can be applied to any base learner. For our experiments, as a base classiﬁer we use a
Probability Estimation Tree (PET) , which is an unpruned J483 decision tree for
which Laplace correction is applied at the leaves. Saar-Tsechansky and Provost 
showed that using Bagged-PETs for prediction produced better probability estimates
than single PETs for BOOTSTRAP-LV; so we used Bagged-PETs for both
BOOTSTRAP-LV and BOOTSTRAP-JS. The number of bootstrap samples and the size
of ensembles in ACTIVEDECORATE was set to 15.
The performance of each algorithm was averaged over 10 runs of 10-fold crossvalidation. In each fold of cross-validation,we generated learning curves as follows. The
set of available training examples was treated as an unlabeled pool of examples, and at
each iteration the active learner selected a sample of points to be labeled and added to the
training set. Each method was allowed to select a total of 33 batches of training examples,
measuring performance after each batch in order to generate a learning curve. To reduce
computation costs, and because of diminishing variance in performance for different selected examples along the learning curve, we incrementally selected larger batches at
each acquisition phase. The resulting curves evaluate how well an active learner orders
the set of available examples in terms of utility for learning CPEs. As a baseline, we used
random sampling, where the examples in each iteration were selected randomly.
To the best of our knowledge, there are no publicly-available datasets that provide
true class probabilities for instances; hence there is no direct measure for the accuracy of
CPEs. Instead, we use two indirect metrics proposed in other studies for CPEs . The
ﬁrst metric is squared error, which is deﬁned for an instance xj, as 
y(Ptrue(y|xj) −
P(y|xj))2; where P(y|xj) is the predicted probability that xj belongs to class y, and
Ptrue(y|xj) is the true probability that xj belongs to y. We compute the Mean Squared
Error (MSE) as the mean of this squared error for each example in the test set. Since
we only know the true class labels and not the probabilities, we deﬁne Ptrue(y|xj)
to be 1 when the class of xj is y and 0 otherwise. Given that we are comparing with
3 J48 is the Weka implementation of C4.5
P. Melville et al.
this extreme distribution, squared error tends to favor classiﬁers that produce accurate
classiﬁcation, but with extreme probability estimates. Hence, we do not recommend
using this metric by itself.
The second measure we employ is the area under the lift chart (AULC) , which
is computed as follows. First, for each class k, we take the α% of instances with the
highest probability estimates for class k. rα is deﬁned to be the proportion of these
instances actually belonging to class k; and r100 is the proportion of all test instances
that are from class k. The lift l(α), is then computed as
r100 . The AULCk is calculated
by numeric integration of l(α) from 0 to 100 with a step-size of 5. The overall AULC is
computed as the weighted-average of AULCk for each k; where AULCk is weighted by
the prior class probability of k according to the training set. AULC is a measure of how
good the probability estimates are for ranking examples correctly, but not how accurate
the estimates are. However, in the absence of a direct measure, an examination of MSE
and AULC in tandem provides a good indication of CPE accuracy. We also measured
log-loss or cross-entropy, but these results were highly correlated with MSE, so we do
not report them here.
To effectively summarize the comparison of two algorithms, we compute the percentage reduction in MSE of one over the other, averaged along the points of the learning curve. We consider the reduction in error to be signiﬁcant if the difference in the
errors of the two systems, averaged across the points on the learning curve, is determined to be statistically signiﬁcant according to paired t-tests (p < 0.05). Similarly, we
report the percentage increase in AULC.4
The results of all our comparisons are presented in Tables 1-3. In each table we present
two active learning methods compared to random sampling as well as to each other.
We present the statistics % MSE reduction and % AULC increase averaged across the
learning curves. All statistically signiﬁcant results are presented in bold font. The bottom of each table presents the win/draw/loss (w/d/l) record; where a win or loss is only
counted if the improved performance is determined to be signiﬁcant as deﬁned above.
Bootstrap-JS, Bootstrap-LV and Bootstrap-LV-EXT
We ﬁrst examine the performance of BOOTSTRAP-JS for binary-class problems and
compared it with that of BOOTSTRAP-LV and of random sampling. As shown in
Table 1, BOOTSTRAP-JS often exhibits signiﬁcant improvements over BOOTSTRAP-
LV, or is otherwise comparable to BOOTSTRAP-LV. For all data sets, BOOTSTRAP-JS
shows substantial improvements with respect to examples selected uniformly at random
on both MSE and AULC. The effectiveness of BOOTSTRAP-JS can be clearly seen in
Figure 1. (The plot shows the part of learning curve where the two active learners diverge in performance.)
In the absence of an active class probability estimation approach that can be applied
to multi-class problems, we compare BOOTSTRAP-JS and BOOTSTRAP-LV-EXT with
4 A larger AULC usually implies better probability estimates.
Active Learning for Probability Estimation
Table 1. BOOTSTRAP-JS versus BOOTSTRAP-LV on binary datasets
%MSE Reduction
%AULC Increase
Data set LV vs.
JS vs. LV vs.
Random Random
Random Random
breast-w 14.92
kr-vs-kp 38.97
Number of Examples Labeled
Bootstrap-LV
Bootstrap-JS
Fig. 1. Comparing different algorithms on kr-vs-kp
acquisitions of a representative set of examples selected uniformly at random. Table 2
presents results on multi-class datasets for BOOTSTRAP-JS and BOOTSTRAP-LV-EXT.
Both active methods acquire particularly informative examples, such that for a given
number of acquisitions, both methods produce signiﬁcant reductions in error over random sampling. The two active methods perform comparably to each other for most data
sets, and JSD performs slightly better in some domains. Because JSD successfully measures the uncertainty of the distribution estimation over all classes, we would recommend
using BOOTSTRAP-JS for actively learning CPE models in multi-class domains.
ActiveDecorate: JSD Versus Margins
Table 3 shows the results of using JSD versus margins for ACTIVEDECORATE. In previous work, it was shown that ACTIVEDECORATE, with both these measures, performs
very well on the task of active learning for classiﬁcation. Our results here conﬁrm that
P. Melville et al.
Table 2. BOOTSTRAP-JS versus BOOTSTRAP-LV-EXT on multi-class datasets
% MSE Reduction
% AULC Increase
JS vs. JS vs.
JS vs. JS vs.
vs. Rand. Rand. LV-Ext vs. Rand. Rand. LV-Ext
11/1/0 4/5/3
12/0/0 4/6/2
both measures are also effective for active learning for CPE. ACTIVEDECORATE using
margins focuses on picking examples that reduce the uncertainty of the classiﬁcation
boundary. Since having better probability estimates usually improves accuracy, it is not
surprising that a method focused on improving classiﬁcation accuracy selects examples that may also improve CPE. However, using JSD directly focuses on reducing the
uncertainty in probability estimates and hence performs much better on this task than
margins. On the AULC metric both measures seem to perform comparably; however, on
MSE, JSD shows clear and signiﬁcant advantages over using margins. As noted above,
one needs to analyze a combination of these metrics to effectively evaluate any active
CPE method. Figure 2 presents the comparison of ACTIVEDECORATE with JSD versus
margins on the AULC metric on glass. The two methods appear to be comparable, with
JSD performing better earlier in the curve and margins performing better later. However, when the two methods are compared on the same dataset, using the MSE metric
(Figure 3), we note that JSD outperforms margins throughout the learning curve. Based
on the combination of these results, we may conclude that using JSD is more likely to
produce accurate CPEs for this dataset. This example reinforces the need for examining
multiple metrics.
ActiveDecorate-JS vs Bootstrap-JS
In addition to demonstrating the effectiveness of JSD, we also compare the two active
CPE methods that use JSD. The comparison is made in two scenarios. In the full dataset
scenario, the setting is the same as in previous experiments. In the early stages scenario,
each algorithm is allowed to select 1 example at each iteration starting from 5 examples
and going up to 20 examples. This characterizes the performance at the beginning of
the learning curve. In the interest of space, we only present the win/draw/loss statistics
(Table 4). For the full dataset, on the AULC metric, the methods perform comparably,
but BOOTSTRAP-JS outperforms ACTIVEDECORATE-JS on MSE. However, for most
datasets, ACTIVEDECORATE-JS shows signiﬁcant advantages over BOOTSTRAP-JS in
Active Learning for Probability Estimation
Table 3. ACTIVEDECORATE-JS versus Margins
% MSE Reduction
% AULC Increase
JS vs. JS vs.
JS vs. JS vs.
vs. Rand. Rand. Margin vs. Rand. Rand. Margin
23/1/0 23/1/0
22/2/0 10/3/11
Number of Examples Labeled
ActiveDecorate-Margins
ActiveDecorate-JS
Fig. 2. Comparing AULC of different algorithms on glass
the early stages. These results could be explained by the fact that DECORATE (used
byACTIVEDECORATE-JS) has a clear advantage over Bagging (used by BOOTSTRAP-
JS) when training sets are small, as explained in .
P. Melville et al.
Table 4. BOOTSTRAP-JS vs. ACTIVEDECORATE-JS: Win/Draw/Loss records
% MSE Reduction % AULC Increase
Full dataset
Early stages
Number of Examples Labeled
ActiveDecorate-Margins
ActiveDecorate-JS
Fig. 3. Comparing MSE of different algorithms on glass
For DECORATE, we only specify the desired ensemble size; the ensembles formed
could be smaller depending on the maximum number of classiﬁers it is permitted to explore. In our experiments, the desired size was set to 15 and a maximum of 50 classiﬁers
were explored. On average DECORATE ensembles formed by ACTIVEDECORATE-JS
are much smaller than those formed by Bagging in BOOTSTRAP-JS. Having larger
ensembles generally increases classiﬁcation accuracy and may improve CPE. This
may account for the weaker overall performance of ACTIVEDECORATE-JS to
BOOTSTRAP-JS; and may be signiﬁcantly improved by increasing the ensemble size.
Conclusions and Future Work
In this paper, we propose the use of Jensen-Shannon divergence as a measure of the utility of acquiring labeled examples for learning accurate class probability estimates. Extensive experiments have demonstrated that JSD effectively captures the uncertainty of
class probability estimation and allows us to identify particularly informative examples
that signiﬁcantly improve the model’s class distribution estimation. In particular, we
show that, for binary-class problems, BOOTSTRAP-JS which employs JSD to acquire
training examples is either comparable or signiﬁcantly superior to BOOTSTRAP-LV, an
existing active CPE learner for binary class problems. BOOTSTRAP-JS maintains its
effectiveness for multi-class domains as well: it acquires informative examples which
result in signiﬁcantly more accurate models as compared to models induced from examples selected uniformly at random. We have also demonstrated that when JSD is used
with ACTIVEDECORATE, an active learner for classiﬁcation, it produces substantial improvements over using margins, which focuses on classiﬁcation accuracy. Furthermore,
our results indicate that, in general, BOOTSTRAP-JS with Bagged-PETs is a preferable
Active Learning for Probability Estimation
method for active CPE compared to ACTIVEDECORATE-JS. However, if one is concerned primarily with the early stages of learning, then ACTIVEDECORATE-JS has a
signiﬁcant advantage.
Our study uses standard metrics for evaluating CPE employed in existing research.
However, we have shown that JSD is a good measure for selecting examples for improving CPE; and therefore it should also be a good measure for evaluating CPE. When
the true class probabilities are known, we propose to also evaluate CPEs by computing
the JSD between the estimated and the true class distributions.
Acknowledgments
This research was supported by DARPA grant HR0011-04-1-007.