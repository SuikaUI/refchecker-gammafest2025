Estimating the Parameters of a Nonhomogeneous
Poisson Process with Linear Rate
William A. Massey,1 Geraldine A. Parker2 and Ward Whitt3
AT&T Bell Laboratories
Murray Hill, NJ 07974-0636
November 16, 1993
Revision: January 30, 1995
1 AT&T Bell Laboratories, Room 2C-120, Murray Hill, NJ 07974-0636; email: 
2 Educational Testing service, Princeton, NJ 08554
(This work was done at AT&T Bell Laboratories as part of an AT&T Summer Research Program fellowship.)
3 AT&T Bell Laboratories, Room 2C-178, Murray Hill, NJ 07974-0636; email: 
Motivated by telecommunication applications, we investigate ways to estimate the parameters
of a nonhomogeneous Poisson process with linear rate over a ﬁnite interval, based on the number
of counts in measurement subintervals. Such a linear arrival-rate function can serve as a
component of a piecewise-linear approximation to a general arrival-rate function. We consider
ordinary least squares (OLS), iterative weighted least squares (IWLS) and maximum likelihood
(ML), all constrained to yield a nonnegative rate function. We prove that ML coincides with
IWLS. As a reference point, we also consider the theoretically optimal weighted least squares
(TWLS), which is least squares with weights inversely proportional to the variances (which
would not be known with data). Overall, ML performs almost as well as TWLS. We describe
computer simulations conducted to evaluate these estimation procedures. None of the procedures
differ greatly when the rate function is not near 0 at either end, but when the rate function is near
0 at one end, TWLS and ML are signiﬁcantly more effective than OLS. The number of
measurement subintervals (with ﬁxed total interval) makes surprisingly little difference when the
rate function is not near 0 at either end. The variances are higher with only two or three
subintervals, but there usually is little beneﬁt from going above ten. In contrast, more
measurement intervals help TWLS and ML when the rate function is near 0 at one end. We
derive explicit formulas for the OLS variances and the asymptotic TWLS variances (as the
number of measurement intervals increases), assuming the nonnegativity constraints are not
violated. These formulas reveal the statistical precision of the estimators and the inﬂuence of the
parameters and the method. Knowing how the variance depends on the interval length can help
determine how to approximate general arrival-rate functions by piecewise-linear ones. We also
develop statistical tests to determine whether the linear Poisson model is appropriate.
1. Introduction and Summary
Trafﬁc measurements and trafﬁc models have long played an important role in
telecommunications systems, e.g., see Cole and Rahko . However, the emergence of new
services and new technologies has led to new kinds of trafﬁc and new trafﬁc models, e.g., see
Frost and Melamed , Leland, Taqqu, Willinger and Wilson ], Meier-Hellstern, Wirth, Yan
and Hoeﬂin and Part I of Roberts . Just as with overﬂow trafﬁc associated with
alternative routing, in many new situations the classical Poisson process trafﬁc model is not
nearly appropriate. On the other hand, in other situations it evidently is still appropriate.
We want to be able to determine if a Poisson process trafﬁc model is appropriate and, when it
is, we want to be able to estimate its parameters from measurements. These issues are relatively
well understood in the familiar setting of homogeneous arrival processes, i.e., when the arrival
rate is constant. However, in reality the arrival process is typically nonhomogeneous; i.e., the
arrival rate typically varies signiﬁcantly in time, e.g., see pp. 258-260 and Chapter 6 of Hall .
It is much easier to detect departures from a homogeneous Poisson process than from a
nonhomogeneous Poisson process. Indeed, we may actually have a Poisson process when we
think we do not, if we do not properly account for the nonhomogeneity; i.e., predictions of higher
variability associated with non-Poisson homogeneous processes can often be explained by
ﬂuctuations in the deterministic arrival-rate function of a nonhomogeneous Poisson process; e.g.,
see Holtzman and Jagerman . This led us in to consider non-Poisson homogeneous
processes as approximations for non-homogeneous Poisson processes.
A major difﬁculty with the nonhomogeneous Poisson process model is that it has inﬁnitely
many parameters. In particular, it is parameterized by its arrival-rate function λ(t). A natural
ﬁrst step is to restrict attention to special parametric families of arrival-rate functions. In many
cases it is reasonable to regard the arrival-rate function as linear over appropriate subintervals,
- 2 i.e., as piecewise linear. For example, data show that the arrival-rate function for telephone calls
over a day is very nonlinear, but it often is reasonable to approximate it by a linear function
within single hours, especially when the available data are summaries over subintervals (e.g., ﬁve
minutes). The linearity is a helpful simpliﬁcation because it reduces the number of parameters.
A speciﬁc motivating application for us is the AT&T long distance network. Summaries of
telephone calls by origin and destination (for the more than 100 major switches) are currently
collected every ﬁve minutes. Figure 1 displays these summaries for three origin-destination pairs
in one hour. Since the time is between 8:00 am and 9:00 am, it is not surprising that there is
signiﬁcant increase over the hour. This example shows that the rates might not nearly be
constant, but that they might be approximately linear over one hour.
Hence, in this paper we consider the case of a linear rate over a subinterval. In particular, we
assume that we have a nonhomogeneous Poisson process over the interval [ 0 ,T] with arrival-rate
λ(t) = a + bt , 0 ≤t ≤T ,
and we investigate how to estimate the two parameters a and b in (1) and the arrival rates λ(t)
based on arrival process data. We assume that the overall time interval ( 0 ,T] is divided into N
measurement subintervals ( (k −1 ) T / N, kT / N], 1 ≤k ≤N, and we observe the number of points
in each. In addition to ﬁnding estimators aˆ and bˆ for the parameters a and b in (1), we want to
provide insight into the choice of the interval length T and the number N of measurement
subintervals. Having a larger T will yield better estimates if the arrival-rate function is indeed
nearly linear, but it can yield worse estimates if the arrival-rate function is only nearly linear over
short intervals. Having a larger N may be more costly, but may improve the quality of the
We primarily focus on the variance of the estimators as a function of the parameters and the
method. Explicit expressions for the variances enable us to see how the parameters a,b,T and N
affect the estimation. We do not speciﬁcally discuss how to choose subintervals to approximate a
general arrival-rate function by a piecewise-linear arrival-rate function, but our variance formulas
can help make the choice.
Our estimation might be based on a single realization of an arrival process or multiple
independent samples. For example, we might have data from the same hour on the same day of
the week for n different weeks. If we consider the N sample means obtained from the Nn
observations, then the variances of all the estimators we consider are inversely proportional to n.
Since the effect of n is predictable, we henceforth only consider the case n = 1.
It is important to emphasize that we are interested in this linear Poisson process model from
two points of view. First, we obviously want to understand how different estimation procedures
perform when the model is approximately valid. Second, we want to develop ways to determine
whether or not the linear Poisson model is appropriate. The model is not valid if the arrival
process is not Poisson or if the arrival-rate function over the designated subinterval is nonlinear.
We develop ways to test whether the linear Poisson model is appropriate. Such a test might be
based on a single realization of the process or upon multiple realizations.
There is a substantial history of statistical inference for nonhomogeneous Poisson processes,
e.g., see Basawa , Brown and Snyder and Miller , but we are unaware of any work
closely related to what we do here. In the physical sciences it is more natural to consider Poisson
processes with nonlinear rates; see and .
We are ultimately interested in characterizing arrival processes so that we can analyze the
performance of telecommunications and other service systems with these arrival processes. There
is a growing literature on ways to analyze service system models with nonhomogeneous Poisson
- 4 arrival processes; e.g., see Choudhury, Lucantoni and Whitt , Davis, Massey and Whitt ,
Hall , Massey and Whitt , Taaffe and Ong and references cited in these sources. In
this paper we investigate ways to select nonhomogeneous Poisson arrival process models given
arrival process data. Less attention has been given to ﬁtting arrival process models to data; see
Basawa and Hall .
In our study we consider three different estimation procedures: (1) ordinary least squares
(OLS), (2) iterative weighted least squares (IWLS) and (3) maximum likelihood (ML). These
estimation procedures are described in detail in Sections 2–5. OLS is the natural simple
estimator; the other two are attempts to do better. The ML approach is discussed by Basawa 
and Snyder and Miller . We are led to consider weighted least squares, because the
observations have unequal variances. To achieve the best linear unbiased estimator, we would
want weights inversely proportional to the variances (see Section 2). Our IWLS procedure
estimates the desired weights from estimates of the variances, which in turn depend on estimates
of the parameters a and b, successively improving the estimates for both (a,b) and the weights by
iteration. When our estimates of (ab) produce weights, which in turn reproduce these same
estimates of (a,b), the iteration has converged.
We prove a fundamental result linking these estimators; the proof appears in Section 8. This
is reminiscent of the classical result for linear models with i.i.d. normally distributed residuals;
then ML coincides with OLS. Indeed, it can be deduced from the theory of generalized linear
models; see p. 31 of McCullagh and Nelder .
Theorem 1. The ML estimators coincide with the solution of IWLS.
We also study these estimators by computer simulation. Given the parameters a,b,T and N,
we randomly generate the resulting Poisson random variables and study how the estimation
procedures perform. Since we know the parameters with the simulation, we also can consider the
- 5 theoretical optimal weighted least squares (TWLS), obtained by using weights inversely
proportional to the known variances (depending on a and b).
From our numerical experience, we ﬁnd that the three estimation procedures (OLS, ML and
TWLS) do not differ much from each other when the rate function is not nearly 0 at either end;
i.e., they all tend to be unbiased and they all have nearly the same variances. The estimators fail
to be precisely unbiased because of adjustments to account for nonnegativity constraints. Even
though ML and IWLS coincide by Theorem 1, differences can occur due to numerical accuracy of
the algorithms. We found that the ML procedure developed here was more robust for very large
We will show that OLS tends to be as good as TWLS when the slope b is relatively small,
while TWLS tend to be signiﬁcantly better than OLS when b is large and the rate function is near
0 at one endpoint. Indeed, we show that when the rate function is 0 at one endpoint, as occurs
when the nonnegativity constraint is violated, then ML and TWLS coincide and that these are
signiﬁcantly more effective than OLS, primarily because they predict the intercept (a when a = 0
and a + bT when a + bT = 0) much better; i.e., the bias created by adjustments to the
nonnegativity constraints is substantially less with ML and TWLS. The IWLS estimator usually
converges very quickly (e.g., 2–5 iterations) to an estimator very close to the TWLS estimator.
As a basis for understanding the performance of these estimation procedures, we develop
analytical formulas for the variances of the OLS and TWLS estimators, assuming that the
nonnegativity conditions are not violated. Since the numerical results show that ML is very close
to TWLS, these formulas give a clear quantitative description of the statistical precision of both
the OLS and ML procedures and the advantage of ML over OLS, provided that the rate function
is indeed not near 0 at either end.
As should be anticipated, we ﬁnd that the total length of the interval, T, is very important, so
- 6 we should strongly prefer long intervals if linearity is not sacriﬁced. However, we ﬁnd that the
number N of measurement subintervals makes surprisingly little difference. When the rate
function is not near 0 at either end, the variances decrease as N → ∞, but rapidly converge to
limits. The variance values are noticeably larger for N = 2 , 3 or 4, but hardly at all by N = 10.
The insigniﬁcance of N might be anticipated because the total number of Poisson counts in [ 0 ,T]
is independent of N. The insigniﬁcance of N is established by extensive simulation results and
explicit formulas for OLS. For OLS we have simple formulas for the variances for all N, but for
TWLS we only have simple formulas for the asymptotic variances as N → ∞; we develop an
algorithm to compute the TWLS variances for all ﬁnite N. Our numerical experience indicates
that the dependence on N depicted by the OLS formulas applies approximately (but not exactly)
to the dependence on N for the other estimators.
The role of N is different, however, when the rate function is near 0 at one end. Then
increasing the number of measurement subintervals still does not help OLS, but it improves the
performance of the ML and TWLS methods, with there being greater improvement for TWLS.
For example, when a = 0, both the bias and the sample standard deviation are decreasing in N
for TWLS. For a∼∼0 and large N, ML does not perform as well as TWLS, but ML is signiﬁcantly
better than OLS in this setting.
Here is how the rest of the paper is organized. In Section 2 we describe the weighted least
squares procedures. In Section 3 we present the formulas for the OLS variances. In section 4 we
develop the asymptotic formulas for the TWLS variances. In Section 5 we develop the ML
estimation procedure. In Section 6 we describe a simulation experiment to evaluate the
estimation procedures and present some of the numerical results. In Section 7 we develop
statistical tests to see if the linear Poisson model is appropriate. In Section 8 we prove
Theorem 1.
2. The Least Squares Estimators
There are four parameters: a,b,T and N. We assume that we have a nonhomogeneous
Poisson process with rate function λ(t) = a + bt over [ 0 ,T] as in (1). We count the number of
points in the N subintervals ( (k −1 ) T / N, kT / N], 1 ≤k ≤N. This sampling procedure from a
single realization of the nonhomogeneous Poisson process over [ 0 ,T] produces N mutually
independent Poisson random variables Y k with means
T_ _ (a + bx k ) ,
x k = (k −2
T_ _ , 1 ≤k ≤N .
If we form the linear model Y = α + βx + ε, i.e., if we assume that Y k = α + βx k + ε k
for each k, then we can estimate the parameters α and β by weighted least squares; i.e., using
positive weights w k, 1 ≤k ≤N, with
w k = N, we can choose estimators αˆ and βˆ to
minimize the weighted sum of the squared errors, i.e., to ﬁnd
w k (Y k −[α + βx k ] )2 .
Applying calculus with (4) in the usual way, we obtain
w k (x k −x_)2
w k (x k −x_) (Y k −Y
_ __________________ =
w k (x k −x_)2
w k (x k −x_) Y k
_ ______________
- 8 x_ = N −1
From (2), we see that α = aT / N and β = bT / N. Hence, associated estimators for a and b are
for βˆ in (5) and αˆ in (6). The resulting estimators aˆ and bˆ are linear functions of the observations
Y k and are unbiased; i.e., Eaˆ = a and Ebˆ = b.
However, it is natural to impose the constraint that the estimated rate function aˆ + bˆt be
nonnegative throughout [ 0 ,T]. This constraint reduces to
bˆ ≥ −aˆ/ T .
Because of the sum of squares in (4) is a convex function of (α,β), it is easy to determine the
constrained minimum. If the constraints in (9) are satisﬁed by the solution in (5)–(8), then we are
done. If not, then either (i) aˆ < 0 or (ii) aˆ ≥0 and bˆ < −aˆ/ T. In the ﬁrst case, the minimum is
attained with aˆ = 0 and bˆ = Nβˆ / T where βˆ is the solution to
w k (Y k − βx k )2 ,
w k x k Y k
_ __________ .
In the second case the minimum is attained with aˆ = −bˆT and bˆ = Nβˆ / T, where βˆ is the
solution to
w k (Y k − β(T −x k ) )2 ,
w k (T −x k )2
w k (T −x k ) Y k
_______________ .
In this framework, the simplest procedure is ordinary least squares (OLS) in which all the
weights are the same, i.e., w k = 1, 1 ≤k ≤N. However, we are motivated to consider unequal
weights because, if b ≠0, then the variances of the successive Y k variables are unequal. Since
the variable Y k has a Poisson distribution, its variance equals its mean. Thus, from (2), we see
that the variances are
Var Y k = λ k ≡N
T_ _ (a + bx k ) , 1 ≤k ≤N .
Of course, since we do not know a and b in advance, we do not know these variances in advance.
However, if we did know the variances in advance, then we might want to use the weights
, 1 ≤k ≤N ,
because these weights produce the minimum variance estimator among linear functions of the
observations Y k that are unbiased (assuming that we do not consider the constraints in (9)). This
can be veriﬁed by a direct calculation using Lagrange multipliers as for the Gauss-Markov
theorem, as on p. 341 of Mood and Graybill , or by transforming the problem to the equalvariance case; see p. 78 of Draper and Smith or p. 81 of Weisberg . For the
transformation, we replace Y k = α + βx k + ε k with σk
−1 Y = α σk
−1 x k + σk
−1 ε k = (σk
−1 Y k − α σk
−1 x k ) are independent random variables with mean 0 and
variance 1. Hence, the standard linear regression theory can indeed be applied to the transformed
Since we do not actually know a and b in advance with data, we cannot use the weights in
(15). However, in our simulation studies evaluating different estimators we can use the weights
(15). We call this case theoretically optimal weighted least squares (TWLS).
We can approach TWLS above with data by using iterative weighted least squares (IWLS).
We ﬁrst perform ordinary least squares to obtain the estimates aˆ and bˆ. We then form the
associated estimate λˆ k = (T / N) (aˆ + bˆx k ) and use this in (15) to obtain weight estimates wˆ k.
We then successively obtain new estimates of (aˆ,bˆ) and wˆ k until insigniﬁcant change results.
Our numerical experience indicates that IWLS consistently converges very quickly (e.g., 2–5
iterations) and that the resulting estimates are very close to the TWLS estimates. See Carroll ,
McCullagh and Nelder and p. 87 of Weisberg for further discussion about IWLS.
It is easy to see how the estimators are related when we make adjustments to satisfy the
nonnegativity constraints in (9). Note that (11) becomes bˆ = N
x k Y k / T
2 for OLS, but
(11) becomes bˆ = N
Y k / T 2 for both TWLS and IWLS, and similarly for
(13). Hence, OLS coincides with TWLS when b = 0, while IWLS coincides with TWLS
whenever the constraints (9) are violated.
We conclude this section by noting that, while it is natural to apply the adjustments to satisfy
the nonnegativity constraints when we have a single data set { (x k , Y k ) : 1 ≤k ≤N}, it is
natural to combine the data before making adjustments, when we have multiple samples. It is
easy to see that with a single data set, the adjustment minimizes mean square error.
3. Analytical Formulas for Ordinary Least Squares
If we ignore the nonlinear adjustment in the OLS estimators in order to satisfy the constraints
in (9), then the estimators aˆ and bˆ in (5)–(8) are linear functions of the random variables Y k, so
that it is not difﬁcult to calculate their means and variances. Since the random variables Y k are
- 11 independent and since a Poisson distribution approaches a normal distribution as the mean
increases, it is natural to regard the joint distribution of the estimators as approximately
multivariate normal as well, but we do not exploit this property. (Either large N or large means
would imply normality, the ﬁrst by the central limit theorem and the second by a direct normal
approximation for the Poisson distribution.)
If we ignore the constraints in (9), then the estimators are all unbiased, i.e., Eaˆ = a and
Ebˆ = b for any weights. However, the adjustment to meet the constraint introduces some bias.
For example, if b is positive but not too large, then Eaˆ > a while Ebˆ < b. Henceforth in this
section we ignore the constraints in (9).
We can also calculate the variances for OLS. We use a superscript O to indicate OLS and a
subscript N to indicate the dependence on N, but we omit them from x k and Y k. From (2), (3), (5)
(x k −x_)2
(x k −x_)2 
(a + bx k )
_ ________________________ =
_ _______
6 ( 2a + bT)
_ __________ .
(Part of the calculation in (16) is based on
(x k −x_)2 = T 2 (N 2 −1 )/12N.) The dependence
upon a,b,T and N is quite clear from (16). To see the impact of N in the standard deviation, we
can expand the N-term in inverse powers of N, i.e.,
_ _______ = 1 +
_ _____ + O
From (16) and (17), we see that the standard deviation SDV(bˆ) for N = 10 exceeds the standard
deviation for N = ∞by only about 0.5%.
Next it is easy to focus on m ≡a + bT /2. The OLS estimator for m is
a + b(k −2
_ ______ .
Since m = Z / T where Z ≡
Y k is Poisson, because it is the sum of n independent Poisson
random variables, (18) is elementary. Note that (18) is independent of N.
Next, a = m −bT /2, so that the variance of aˆN
0 = Var mˆ N
O −TCov (mˆ N
−T Cov (mˆ N
O ) = −T Cov
(x k −x_) Y k /
(x k −x_)2
T(N 2 −1 )
_ ________
(x k −x_) Var Y k = −b .
From (16), (18)–(20),
6 ( 2a + bT)
_ _________ +
_ ______ −b →b +
Next, for any t, let λˆ N
0 (t) = aˆN
0 t be the estimator of the arrival rate at time t. Then
0 (t) = VaraˆN
0 + t 2 VarbˆN
0 + 2t Cov (aˆN
0 ) = Cov (mˆ N
T_ _ Var (bˆN
Finally, we give approximate formulas for the mean and the standard deviation of the
intercept estimator a˜N
0 assuming that the adjustment to satisfy the nonnegativity condition (9) is
made. This shows the bias caused by the nonnegativity constraint in (9). We assume that the
estimator aˆN
0 before adjustment is normally distributed with mean 0 and variance Var aN
0 in (21).
0 = 0 when aˆN
0 ≤0, while a˜N
0 when aˆN
0 > 0. Hence, using basic properties of the
positive normal distribution, we see that the mean is
∼∼0. 4 SD(aˆN
0 ) →0. 4√ b
and the standard deviation is
0 ∼∼0. 584 SD aˆN
0 →0. 584√ b
4. Asymptotic Formulas for TWLS
In this section we develop asymptotic formulas for the sample variances associated with
TWLS, i.e., using the weights in (15). We ﬁrst develop formulas for the limits as N → ∞. Then
we consider the asymptotic form of these limiting formulas as ε ≡bT / a gets small. The
parameter ε is a relevant measure of departure from homogeneity; bTis the difference between
the left and right end points of λ(t) over [ 0 ,T], whereas a is the left end point. At the outset, we
assume that a ≠0 and b ≠0; we consider the cases a = 0 and b = 0 at the end of this section.
We use a superscript T and a subscript N for the estimators to indicate the dependence on TWLS
and N, but we omit them in λ k, x k and w k in (2), (3) and (15).
As N → ∞, the sums approach integrals which can be directly integrated. First,
_ _____ /∫0
_ _____ = ln ( 1 + [bT / a] )
_ ____________ −b
where ln (x) is the natural logarithm. By the same reasoning,
w k (x k −x_
2 (x k −x_
_ _________________
_ _______ dt
_ _________
T →ln ( 1 + [bT / a] )
_ ____________
→0 as N → ∞,
T )2 Var bˆN
→ln ( 1 + [bT / a] )
_ ____________ +
_ __________
Indeed, our numerical experience indicates that Cov (bˆN
T ) = 0 for all N. For
T (t) = aˆN
T (t) = VaraˆN
T + t 2 VarbˆN
T + 2tCov (aˆN
T ) = Cov (Y
T ) = Cov (Y
The formulas for x_
T , Var bˆ∞
T and Var aˆ∞
T in (25), (26) and (29) do not look much like their
counterparts for OLS. However, a connection can be seen when we develop expressions for the
TWLS asymptotic formulas as ε ≡bT / a gets small. We will show that the asymptotic relative
difference between OLS and TWLS is of order O(ε2 ) as ε →0.
First, applying the familiar asymptotic expansion for the logarithm, ln ( 1 + z) = z −z 2 /2 + . . .,
to (25), we obtain
_ ___ ε2 + O(ε4 ) )
Next, from (25), (26) and (32),
T 3 ( 1 −2
_ __ ε2 + O(ε3 ) )
_ _______________________
_ ___ ( 1 + 2
_ __ + O(ε3 ) ) as ε →0 .
Thus, the asymptotic relative difference between the asymptotic variances Var bˆ ∞of the OLS and
TWLS methods is
O −Var bˆ∞
_ ______________ = 15
_ __ + O(ε3 ) as ε →0 .
Next, to treat Var aˆ∞
T , note that
T )2 Var bˆ∞
_ __ ( 1 + 6
_ ___ ε2 + O(ε3 ) ) as ε →0
- 16 ln ( 1 + ε)
_ _______ = T
a_ _ ( 1 + 2
_ __ + O(ε2 ) ) as ε →0 ,
_ __ + b −5T
_ ___ + O(ε3 ) as ε →0
0 −Var aˆ∞
_ ______________ = 20
_ __ + O(ε3 ) as ε →0 .
So far in this section, we have excluded the cases a = 0 and b = 0. The case b = 0 is
obtained as the limit as b →0 as described above; then TWLS coincides with OLS.
The cases a = 0 without adjustments is more complicated, so we do not treat it analytically.
To see the complications, note that the limiting integral ∫0
Tt −1 dt diverges. We now analyze the
case in which we assume that a = 0, as we do if we make adjustments after ﬁnding aˆ < 0. For
a = 0 and ﬁnite N, the TWLS slope estimator is
The associated variance is
which is independent of N. By (16) and (38), when a = 0,
_ ______ =
_ _____ →3 as N → ∞.
Since ML coincides with TWLS when a = 0 (see Section 5 below), formula (39) roughly
indicates how much more efﬁcient is the ML slope estimator than the OLS slope estimator when
- 17 a or a + bT is very small and an adjustment is made to satisfy the constraints in (9). As shown
in Section 6, the overall relative efﬁciency of ML compared to OLS for the slope is actually only
about 1.5.
We also obtain (38) asymptotically from (26) as a →0. Note that x_
∞→0 as a →0 for x_
in (25). Thus Var bˆ∞
T →2b / T 2 and Var aˆ∞
T →0 as a →0. In contrast, by (23),
b_ _ →b as N → ∞.
In Section 6 we will see that TWLS is indeed signiﬁcantly better than OLS for estimating the
intercept in this region. However, ML does not share the advantage of large N.
5. Maximum Likelihood
Let p(n;a,b) be the probability that the count vector Y ≡(Y 1 , . . . , Y N ) for the N
subintervals is n ≡(n 1 , . . . , n N ) when the parameter pair is (a,b). The ML estimator is the pair
(aˆ,bˆ) that maximizes p(Y;a,b) or, equivalently
lnp(Y;a,b) = −
Y k ln λ k −
ln (Y k ! )
Y k ln ( (a + bx k ) (T / N) ) + C
for a constant C. Note that ln p(Y; a,b) is a strictly concave function of (a,b) provided that
Y k > 0 for some k, and a + bx k > 0 for all k, which we assume is the case. (If Y k = 0 for all
k, then (41) is obviously maximized by a = b = 0.)
By differentiating with respect to a and b in (41), we ﬁnd that the pair (aˆ,bˆ) is the solution to
the two equations
_ ______ = T
_ ______ =
provided that the constraints in (9) are satisﬁed.
If aˆ < 0 by (42) and (43), then we set aˆN
M = 0 and minimize ln p(Y, 0 ,b), obtaining
Y k / T 2 ,
just as in (37). If we knew that a = 0, then we would also use (44) and obtain
M = 2b / T 2 ,
just as in (38). Similarly, if aˆ + bˆT < 0, then we set aˆN
M = −bT and maximize ln p(Y, −bT,b)
Y k / T 2 .
We now consider how to solve (42) and (43). Multiplying equations (42) and (43) by a and b,
respectively, and adding, we obtain the linear equation
Y k = aT +
(We include the last expression to emphasize that aT + bT 2 /2 is the expected total number of
points in the interval [ 0 ,T].)
By virtue of (47), we can substitute 2 (S −aT)/ T 2 for b in (42), we obtain the single equation
_ ______________ = 2 .
First note that a = S / T and b = 0 always solves (47) and (48), but there may be other solutions.
Next note that g(a) in (48) has singularities for each x k > x_ for which Y k > 0. In particular, the
singularities are at
for k > N /2 ,
all of which are greater than S / T.
By looking at the second derivative of g, we see g is inﬁnitely differentiable and convex in the
open interval ( 0 ,S / T). Hence, there is at most one root to equation (48) in the interval ( 0 , S / T).
If g′ (S / T) < 0 and g( 0 ) > 2, then there is such a root. Hence, we compute these quantities:
g( 0 ) = S
g′ (S / T) = −
Y k (x_ −x k ) .
If indeed g′ (S / T) < 0 and g( 0 ) > 2, then we ﬁnd the unique root of (48) in ( 0 , S / T) by a
convenient method. Since derivatives are readily available, it is natural to use a Newton method
(starting at 0 since we do not want the root at S / T).
We always allow for a root to (48) with a ≤0. However, using the constraint (9), we
consider aˆ = 0 and bˆN
M in (44) as a candidate optimum.
Next, we consider the possibility of a > S / T. However, by (47), a > S / T if and only if
b < 0. We treat the case b < 0 by repeating the procedure above on the reverse-time problem,
∗= Y N −k + 1, b = −b ∗, a + bT = a ∗. Let g ∗be g in (48) for the reverse-time
problem. Note that for the reverse-time problem
g ∗′ (S / T) = −g′ (S / T)
- 20 for g′ (S / T) in (50). Hence, there is at most one root in ( 0 , S / T) for the two problems.
Finally, we evaluate the likelihood for each of the candidate solutions:
root of g(a) = 2 in ( 0 , S / T) ,
b = 2 (S −aT)/ T 2
a = a ∗+ b ∗T,
g ∗(a) = 2 in ( 0 , S / T) and
b ∗= 2 (S −a ∗T)/ T 2,
a = S / T , b = 0
a = 0 , b = 2S / T 2
a = −bT , b = −2S / T 2
with the understanding that we consider (ii) only if there is no solution to (i) and (iii)-(v) only if
there is no solution to (i) and (ii). By (41) and (47), to evaluate (iii)-(v) it sufﬁces to calculate
Y k ln (a + bx k )
for each possibility and choose the one yielding the maximum. (Note that a + bx k > 0 for all k
in each case.) In this way we obtain our estimates (aˆN
6. Simulation Results
We wrote a simulation program (in C) to randomly generate the mutually independent
Poisson random variables Y k with means λ k in (2) and carry out the estimation procedures. In
Table 1 we give the sample standard deviations of the estimators aˆ and bˆ for each of the methods
(OLS, ML and TWLS) in a representative set of eight cases in which the constraints in (9) are
very rarely violated, each obtained from 1000 replications. The cases are speciﬁed by the
parameter four-tuple (N,T,a,b). The constraints in (9) were in fact never violated in this sample.
Hence, in this sample the associated sample means of aˆ and bˆ were consistently close to the true
values a and b.
From Table 1 we see that the standard deviations for ML and TWLS are almost identical, and
are consistently less than for OLS. (The only minor exception is the last case in which the actual
slope is zero.) However, all three sample standard deviations are quite close. Moreover, the
observed OLS and TWLS standard deviations, are quite accurately predicted by the formulas in
Sections 3 and 4 (as is easily veriﬁed).
In Table 2 we give the l 2 distances between the estimates aˆ and bˆ for different pairs of
methods, based on the same 1000 replications. For example, the l 2 distance for a by OLS and
Table 2 shows that ML tends to produce nearly the same estimates as TWLS.
The information in Table 2 can also be seen from plots of the estimator pairs, e.g.,
M ) : 1 ≤i ≤1000 }. We illustrate by contrasting OLS-ML and ML-TWLS for the case
( 12 , 12 , 100 , 25 ) in Figure 2. This ﬁgure conﬁrms that in this case
TWLS −ML< OLS −ML.
In Tables 3 and 4 we present the sample means and standard deviations of the estimators aˆ
and bˆ when T = 12, a = 0 and b = 10 for several different values of N, again based on 1000
replications. In Tables 5 and 6 we present the same results for the case T = 6, a = 0 and
b = 100. Since a = 0, the nonnegativity conditions (9) are violated about half the time in these
examples. Figure 3 plots estimator pairs for the case in Tables 5 and 6.
From Tables 3 and 5, we see that there is a signiﬁcant bias in the intercept estimate due to the
adjustment to satisfy (9). From Tables 3 and 5, we see that the OLS mean and standard deviation
agree with formulas (23) and (24). We also see that ML and TWLS are signiﬁcantly better than
OLS in this case; there is both a smaller bias and a smaller sample standard deviation.
From Tables 4 and 6, we see that there is relatively little bias in the slope. Moreover, the
three methods do not differ greatly for the slope, although TWLS and ML actually have
signiﬁcantly less bias than OLS. However, this bias differential is small compared to the sample
standard deviation.
Figure 3 tells a similar story pictorially. From Figure 3, we see that the ML and TWLS
estimators are close for the intercept, but the ML and OLS estimators are not. The same is true
for the slope, but the difference between ML and OLS is not nearly so great.
Figure 4 shows the estimator pairs for the same case as Figure 3, except the number of
measurement intervals is increased from N = 12 to N = 120. From Figure 4 we see that the
agreement between ML and TWLS decreases. Moreover, increasing N helps ML and TWLS, but
not OLS. However, the ML bias does not steadily decline as N increases the way the TWLS bias
In conclusion, the major advantage of ML over OLS is in estimating the intercept when a ∼∼0
(or a + bT when a + bT ∼∼0).
7. Model Tests
In this section we discuss how we can test whether the nonhomogeneous Poisson process with
a linear rate λ(t) = a + bt is a reasonable model for data over the interval [ 0 ,T]. The possible
alternative hypotheses include: (i) a Poisson process with nonlinear rate, (ii) a non-Poisson
process with linear rate, and (iii) a non-Poisson process with nonlinear rate. We do not directly
examine any of these alternatives here. However, following standard statistical practice, we can
detect a poor ﬁt of our linear Poisson model from statistics that fall outside the main region of
their distributions under the linear Poisson model.
A simple direct procedure is to compare the observed sample standard deviations of the
estimators aˆ and bˆ based on actual data with the distributions of those same estimators from the
linear Poisson model. The linear Poisson case can be described by the histograms from
1000 replications in the examples here.
We can also exploit more structure. We consider the two cases: multiple replications and a
single replication. For n independent replications, we note that the distributions of the estimators
aˆ and bˆ by any of our methods should be approximately normal by the central limit theorem.
Indeed, the normality was demonstrated by looking at histograms of the 1000 observations in the
examples of Table 1. However, when the nonnegativity conditions (9) are frequently violated, as
in the examples in Table 2, the y-intercept aˆ tends to have approximately a truncated normal
distribution instead of a normal distribution, with parameters as in (23) and (24).
Henceforth, suppose that there are n replications and that the normal approximation is
appropriate. Let a_ = n −1
aˆ i and b
bˆ i; let σa
2 be the actual variance of aˆ
and bˆ. The normality implies that
(aˆ i −a_)2 /σa
should both have chi-square distributions with n −1 degrees of freedom. For OLS, we
approximate σa
2 by the exact OLS variances associated with the estimated parameter pair
) determined in Section 3. For IWLS and ML, we approximate σa
2 by the exact
TWLS asymptotic variances associated with the estimated parameter pair (a_, b
). Both of these
exact variances are relevant only under the assumption that the constraints in (9) are not violated
too frequently, which we check in our calculations of the estimate.
For a single replication, we assume that the means EY k are sufﬁciently large that we can
regard the Poisson variables Y k as being approximately normally distributed. Then we use the
- 24 transformation to convert the unequal variance problem to the equal variance problem, which was
used to justify the optimality of TWLS. Since we do not know the real parameters a and b, we
use our estimates. The normalized sum of squares
Y k −(aˆ + bˆx k ) N
2 = (aˆ + bˆx k ) N
should thus be approximately chi-square with N −2 degrees of freedom.
These chi-square distributional properties should enable us to roughly gauge model
consistency. The test in (56) is able to capture signiﬁcant departures from the linear Poisson
model as occur in the telephone call data in hours immediately after a rate change.
8. Proof of Theorem 1
First note that b = 0, a = S / T solves (42) and (43) if and only if
x k Y k = 2
and note the same is true for (5) and (6); then the weights in (15) are constant, so that x_ = T /2.
Henceforth, we consider solutions with b ≠0.
for j = 0 , 1 , 2 and note that
- 25 aA 0 + bA 1 = N
aA 1 + bA 2 =
so that A 1 = (N −aA 0 )/ b,
_ __ A 0 ,
The rest of the proof is contained in the following two lemmas.
Lemma 1. Equation (6) holds, i.e., aˆ = (NY
/ T) −bˆx_ for weights (15) with λ k = aˆ + bˆx k and
bˆ ≠0 if and only if (aˆ,bˆ) satisﬁes (42).
Proof. By (57) and (60), we can write (6) as
_ _______ −b
which we see is equivalent to (42).
Lemma 2. Assuming that (42) holds, equation (5) is equivalent to (43).
Proof. First note that, by (57)-(60),
(x k −x_)2
_ _______ =
2 −2x_x k + x_2
_ _____________
a_ _ −bA 0
Hence, using (42), we can write (5) as
a_ _ −bA 0
_ ______ −x_ T
_ ___________________
or, equivalently,
_ __ −bA 0
_ _______ + bA 0
which we see is equivalent to (43).
Acknowledgment. We thank our colleague Stephen G. Eick for assistance in the early phase of
this research, Kingshuk Choudhury for pointing out that Theorem 1 can be regarded as a
consequence of the theory of generalized linear models as in McCullagh and Nelder , and the
reviewers for their helpful comments.
Appendix. An Alternative Three-Parameter Model
In this section we consider an approach to capture additional stochastic variability in the
arrival process. Since the Poisson distribution is approximately normal for large means, we use a
normal distribution framework. In particular, we assume that the random counts Y k are still
independent but now normally distributed with means λ k and variances cλ k for λ k in (2), where
Using the ML approach with this normal distribution, we get
aˆ + bˆx k
(aˆ + bˆx k ) N
_ ____________________
aˆ + bˆx k
_ _______ −N
Y k = aˆT +
as in (47), and
(aˆ + bˆx k )2
_ __________ = cˆ
aˆ + bˆx k
_ _______ + T .
As in §5, a = S / T and b = 0 is a solution to (61) and (62). We can ﬁnd another solution to (61)
and (62) as in §5.
Since the normal model must itself be an approximation for point processes, we suggest using
(47) and (48) to ﬁnd aˆ and bˆ, and then (60) to get cˆ. If aˆ ∼∼a and bˆ ∼∼b in (60), then
E cˆ ∼∼1 and Var cˆ ∼∼T 2
For the variance approximation in (63) we use the fact that the fourth central moment of a normal
random variable is 3σ4.
We conclude this section by considering three different Poisson process examples. First, we
consider a Poisson process with linear rate λ(t) = 100 + 25t for 0 ≤t ≤12. Second, we
consider a Poisson process with nonlinear rate
6 < t ≤12 .
Third, we consider a Poisson process with nonlinear rate
9 < t ≤12 .
All three rate functions have the same average rate λ
= 250. Moreover, all three have the same
average rates over the two subintervals and ( 6 , 12 ], namely, 175 and 325, respectively.
Indeed, all three have the same best linear ﬁt according to the least squares distance
 λ − λ lim 2 = (∫0
T (λ(t) −(a + bt) )2 dt)1/2
The respective distances according to this criterion are 0, 150 and 312.2. The ﬁrst case is of
course exactly linear, while the third case evidently departs from linearity more than the second.
For all three examples we let N = 12. Then, for all three methods, EY k ≥41 for all k, so that
the normal approximation for Y k is very reasonable.
For these three Poisson processes we test for linear rate using the multiple-replication
statistics V a and V b in (55) and the single-replication normalized sum of squares U in (56).