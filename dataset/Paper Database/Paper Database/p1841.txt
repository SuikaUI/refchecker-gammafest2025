Modeling global scene factors in attention
Antonio Torralba
Artiﬁcial Intelligence Laboratory, Massachusetts Institute of Technology, 400 Technology Square,
Cambridge, Massachusetts 02115
Received October 1, 2002; revised manuscript received February 12, 2003; accepted March 12, 2003
Models of visual attention have focused predominantly on bottom-up approaches that ignored structured contextual and scene information.
I propose a model of contextual cueing for attention guidance based on the
global scene conﬁguration.
It is shown that the statistics of low-level features across the whole image can be
used to prime the presence or absence of objects in the scene and to predict their location, scale, and appearance before exploring the image.
In this scheme, visual context information can become available early in the
visual processing chain, which allows modulation of the saliency of image regions and provides an efﬁcient
shortcut for object detection and recognition.
© 2003 Optical Society of America
OCIS codes: 330.0330, 330.4060, 100.5010.
1. INTRODUCTION
As illustrated in Fig. 1(a), contextual information allows
one to unambiguously recognize an object, such as a pedestrian, even when the intrinsic local information is not
sufﬁcient for reliable recognition.
Note that blurring of
the image has reduced the available local information.
Figure 1(b) illustrates that even when there is enough local information for object recognition, contextual information plays a major role in object search.
The target object
can be localized easily when it is clearly differentiated
from the background as well as when the background context constrains the possible locations of the target.
A popular strategy for ﬁnding an object embedded in a
complex background is to look for local features that differ
the most from the rest of the image.
Accordingly, most
computational models of attention are based on low-level
saliency maps that ignore contextual information provided by the correlation between objects and the scene.1–5
A second class of models of attention include information
appearance
process.5–7
This approach ignores image regions that
have features incompatible with the target and enhances
the saliency of regions that have features compatible with
the target.
But again, no contextual information is taken
into account.
A number of studies have shown the importance of
scene factors in object search and recognition.
showed that the task changes the way observers look at a
Studies by Biederman et al.9 and Palmer10 highlight the importance of contextual information for object
search and recognition.
Rensink and co-workers11,12
showed that changes in real-world scenes are noticed
most quickly for objects or regions of interest, thus suggesting a preferential deployment of attention to these
parts of a scene.
Henderson and Hollingworth13 reported
results suggesting that the choice of these regions is governed not merely by their low-level saliency but also by
scene semantics.14
Chun and Jiang15 showed that visual
search is facilitated when there is correlation across different trials between the contextual conﬁguration of the
display and the target location.
Oliva et al.16 showed
also that familiar scenes automatically activate visual
search strategies that were successful in past experiences, without volitional control by the subject.
results are in agreement with the idea that scene information can be processed fast and without relying on
single objects.17
Schyns and Oliva17 showed that a
coarse representation of the scene initiates semantic recognition before the identity of objects is processed.
Several other studies support this idea that scene semantics
can be available early in the chain of information
processing18–21 and suggest that scene recognition may
not require object recognition as a ﬁrst step.22–24
Notwithstanding the accumulating evidence for contextual effects on visual exploration, few models of visual
search and attention proposed so far include the use of
context.25–30
In this paper a statistical framework for incorporating contextual information in the search task is
2. COMPUTATIONAL MODELS OF
In this section, ﬁrst, saliency-based models of attention
are introduced.
Then a probabilistic framework is introduced in which a model that incorporates both low-level
factors (saliency) and higher-level factors (scene/context)
for directing the focus of attention is proposed.
Saliency-Based Models of Attention
In the feature-integration theory,3 attention is driven by
low-level features, and the search for objects is believed to
require slow serial scanning; low-level features are integrated into single objects when attention focuses on them.
Computational models of visual attention (saliency maps)
have been inspired by this approach, as it allows a simple
implementation
attentional
mechanisms.1,28,31,32
Saliency maps provide a measure of the saliency of
each location in the image based on low-level features
such as contrast, color, orientation, texture, and motion
Antonio Torralba
Vol. 20, No. 7/July 2003/J. Opt. Soc. Am. A
1084-7529/2003/071407-12$15.00
© 2003 Optical Society of America
(see Ref. 33 for a review).
In the saliency-map model, regions with properties different from those of their neighborhoods are considered more informative and therefore
attract attention.
When one is looking for an object, a
possible strategy for exploring the image is analyzing ﬁrst
the salient regions in the scene.
This approach is very
efﬁcient when the target object is distinct from the background in terms of simple low-level image features.
The image features most commonly used for describing
local image structure (orientation, scale, and texture) are
multiscale
Gabor-like ﬁlters have interesting properties for the encoding of natural images.34,35
These features have also
been shown to be relevant for the tasks of object
detection6,36 and scene recognition.24,37–39
These image
features are obtained as the convolution
i~x8!gk~x 2 x8!,
where i(x) is the input image, uk(x) is the output image,
and gk(x) is an oriented bandpass Gabor ﬁlter deﬁned by
gk(x) 5 exp(2ixi2/sk
2)exp(2p j^fk ,x&).
imaginary parts of gk are Gabor ﬁlters in quadrature.
The variable k indexes ﬁlters tuned to different orientations and scales.
For each feature a saliency map is computed by using a
hardwire scheme1:
the output of each ﬁlter is processed
by center–surround mechanisms to enhance regions with
outputs that differ from the neighborhood.
The results
are then amplitude normalized and combined in order to
produce a unique saliency map that also combines information coming from other image features such as contrast and color.1
The scanning of attention is then modeled as exploring the image in succession following
regions of decreasing saliency.
Also, the set of low-level
features can be enhanced to account for other image properties that also attract attention such as edge length and
curvature.4
Most of the computations for the saliency map rely only
on local measurements, and global information is considered only in the normalization step.
Saliency maps are
easy to compute and can be hard-wired into the design of
a system, thus minimizing the need for learning.
However, their reliance on local measures forces bottom-up
models to treat the background as a collection of distracting elements that complicate rather than facilitate the
search for speciﬁc objects.
Most of the time, the target
Examples of scene/context inﬂuences in object recognition and search.
(a) Examples with increasing contextual information but
where the local target information remains constant.
Observer recognition improves drastically as background information is added.
(b) Scene information affects the efﬁciency of search for a target (a person).
The context acts in two competing ways:
(1) by introducing
distractors but also (2) by offering more constraints on target location.
Most previous models of attention focus on modeling masking
effects (saliency maps).
J. Opt. Soc. Am. A/Vol. 20, No. 7/July 2003
Antonio Torralba
object is camouﬂaged in a cluttered background.
situations, saliency alone may not provide an efﬁcient
way of ﬁnding an object.
Low-level saliency maps compute the saliency of a local
image region by looking at the distribution of features in
the neighborhood (context) of each location.
This use of
context treats the background as a collection of distractors instead of exploiting the correlations that exist between target properties and background properties.
the next sections a term will be introduced into the saliency of a target that depends on the correlation between
object and scene properties.
In the model presented in
this paper, contextual features refer to image features
correlated with a higher-level scene description.
Model for Contextual Modulation of Target Saliency
In contrast to traditional models of search for which background scene information is more of a hindrance than a
help, the statistical correlations that exist between global
scene structure and object properties will be used to facilitate search in complex scenes.
This approach is motivated by the observation that the structure of many realworld scenes is governed by strong conﬁgurational rules
These statistical regularities can provide estimates of the likelihood of ﬁnding an object in a given
scene and can also indicate the most likely position and
scales at which an object might appear.
Next to be described is how these intuitions have been formalized into
an operational scheme for object search in complex
A statistical framework will be used for the
model, as it provides a simple way of accounting for different factors in evaluating the target saliency.
The object in a scene is described here by means of a set
of parameters O 5 $o, x, t%, where o denotes the category of the object, x 5 (x, y) is its spatial location, and t
are object appearance parameters such as the object’s
scale in the image and its pose.
In a statistical framework, object search requires the evaluation of the probability density function7,36 (PDF), P(Ouv).
This function
is the probability of the presence of an object O in a scene
given a set of image features v.
Here v represents all the
features obtained from the input image.
Therefore v is
very high dimensional, making the evaluation of the function P(Ouv) impractical.
Furthermore, writing the object
probability as P(Ouv) does not reveal how scene or object
features might inﬂuence the search, because it does not
differentiate
contextual
Therefore we consider two sets of image features:
(1) local features, vL(x), which are the set of features obtained
in a neighborhood of the location x, and (2) contextual features, vC , which encode structural properties of the
scene/background.
Here it is proposed that object detection requires the evaluation of the probability function
(target saliency function), P(OuvL , vC), which provides
the probability of the presence of the object O given a set
of local and contextual measurements.29,30,40
The object probability function can be decomposed by
applying Bayes’s rule as
P~OuvL , vC! 5
P~vLuvC! P~vLuO, vC!P~OuvC!.
Those three factors provide a simpliﬁed framework for
representing three levels of attention guidance.
The normalization factor, 1/P(vLuvC), does not depend on
constraints
bottom-up factor.
It provides a measure of how unlikely
it is to ﬁnd a set of local measurements vL within the context
5 1/P(vL(x)uvC).
Saliency is large for unlikely features
in a scene.
This formulation follows the hypothesis that frequent
image features are more likely to belong to the background, whereas rare image features are more likely to be
key features40 for the detection of (interesting) objects.
Target-Driven Control of Attention
The second factor, P(vLuO, vC), gives the likelihood of the
local measurements vL when the object O is present in a
particular context.
This factor represents the top-down
knowledge of the target appearance and how it contributes to the search.
Regions of the image with features
unlikely to belong to the target object are vetoed, and regions with attended features are enhanced.33,41
that when the object properties O fully constrain the object appearance, then it is possible to approximate
P(vLuO, vC) . P(vLuO).
This is a good approximation,
because O does not just include the deﬁnition of the object
category (e.g., a car) but also speciﬁes information about
the appearance of the target (location, scale, pose, etc.).
This approximation allows dissociation of the contribution of local image features and contextual image features.
Images that are similar in terms of global spatial properties have a tendency to be composed of similar objects with
similar spatial arrangement.17,24
Since scene semantics may be
available early in the visual processing, these regularities suggest that an efﬁcient procedure for object search in a new scene is
to see how objects were organized in similar environments.
Antonio Torralba
Vol. 20, No. 7/July 2003/J. Opt. Soc. Am. A
Contextual Priors
The third factor, the PDF P(OuvC), provides contextbased priors on object class, location, and scale.29,30
of critical importance for ensuring reliable inferences in
situations where the local image measurements vL produce ambiguous interpretations.40
This factor does not
measurements
Therefore the term P(OuvC) modulates the saliency of local image properties in the search for an object of the
With an object in a scene deﬁned as O 5 $o, x, t%, contextual inﬂuences become more evident if we apply
Bayes’s rule successively in order to split the PDF
P(OuvC) into three factors that model three kinds of context priming on object search:
P~OuvC! 5 P~tux, vC , o!P~xuvC , o!P~ouvC! .
According to this decomposition of the PDF, the contextual modulation of target saliency is a function of three
main factors:
Object-class priming:
This PDF provides the probability of presence of the object class o in
the scene.
If P(ouvC) is very small, then object search
need not be initiated (e.g., we do not need to look for cars
in a living room).
Contextual
attention:
P(xuo, vC).
This PDF gives the most likely locations for
the presence of object o given context information, and it
allocates computational resources into relevant scene regions.
Contextual selection of local target appearance:
P(tux, vC , o).
This PDF gives the likely (prototypical)
shapes (point of views, size, aspect ratio, object aspect) of
5 $ s, p,...%, s being scale and p being aspect ratio.
Other parameters describing the appearance of an object
in an image can be added.
Computational models of object recognition have focused on modeling the probability function P(OuvL), ignoring contextual priors.7,36,42–46
The role of the contextual priors in modulating attention is to provide information about past search experience in similar environments and the strategies that were
successful in ﬁnding the target.
In this model we assume
that the contextual features vC already carry all the information needed to identify the scene.
The scene is
movements.18,19,22
Eye movements are required for a detailed analysis of regions of the image that are relevant
for a task (e.g., to ﬁnd somebody).
Scene factors in attentional deployment are effective
Scheme that incorporates contextual information to select candidate target locations.
The scheme consists of two parallel pathways:
The ﬁrst processes local image information, and the second encodes globally the pattern of activation of the feature maps.
Contextual information is obtained by projecting the feature maps into the (holistic) principal components.
In the task of looking for a
person in the image, the saliency map, which is task independent, will select image regions that are salient in terms of local orientations
and spatial frequencies.
However, the contextual priming (task dependent) will drive attention to the image regions that can contain
the target object (sidewalks for pedestrian).
Combining context and saliency gives better candidates for the location of the target.
J. Opt. Soc. Am. A/Vol. 20, No. 7/July 2003
Antonio Torralba
only after the system has accumulated enough experience.
That is when the system knows about the regularities of the visual world:
how objects and scenes are related and which visual search strategies are successful in
ﬁnding objects of interest, given a visual context.
Therefore the likelihood function P(OuvC) contains the information about how the scene features vC were related to the
target properties O (image location, scale, position) during previous experience.
As shown in Eq. (3), the contextual priming can be decomposed into three factors:
object-class priming, contextual guidance of focus of attention, and contextual selection of object appearance.
In the next sections it will
be shown how to compute global scene features24,47 and
how these features can be used in the object search task.
Global Image Features and Context Representation
As discussed in Subsection 2.A, the image features most
commonly used for describing local image structure are
multiscale
Therefore the local image representation at the spatial location (x) is given by the vector vL(x) 5 $vk(x)%k51,N ,
where vk(x) is given by Eq. (1) and N is the number of
Gabor ﬁlters.
In such a representation, v(x, k) is the
output magnitude at the location x of a complex Gabor ﬁlter tuned to the spatial frequency fk .
The variable k indexes ﬁlters tuned to different spatial frequencies and orientations.
Contextual features have to describe the structure of
the whole image.
It has been shown that a holistic lowdimensional encoding of the image features conveys relevant information for a semantic categorization of the
scene/context24,47 and can be used for contextual priming
in object-recognition tasks.29,30
This deﬁnition of context
does not require the identiﬁcation of other objects in the
Such a representation can be achieved by decomposing
the image features into the basis functions provided by
principal components analysis:
uv~x, k!u cn~x, k!.
decomposition
coefﬁcients
5 $an%n51,N as context features.
The functions cn are
the eigenfunctions of the covariance matrix deﬁned by the
image features v(x, k).
Figure 3 illustrates how the features an are obtained from the magnitude output of the
Gabor bank.
Each feature ai is obtained as a linear combination of the magnitude output of all the Gabor ﬁlters
used in the image decomposition.24
By using only a reduced set of components (N 5 60 for the rest of the paper;
we use a ﬁlter bank with six orientations and four scales),
the coefﬁcients $an%n51,N encode the main spectral characteristics of the scene with a coarse description of their
spatial arrangement.
In essence, $an%n51,N is a holistic
representation, as all the regions of the image contribute
to all the coefﬁcients, and objects are not encoded individually.
In the next sections we discuss each of these three factors and show results using an annotated database of
real-world images (see Appendix A).
3. RESULTS
Object-Class Priming
Before attention is deployed across the different parts of
the scene, the global conﬁguration may be a strong indicator of the presence or absence of an object.
If the scene
has a layout in which, given previous experience, the target was rarely present, then the system can rapidly decide not to initiate the search.
If we assume that the feature vector vC conveys enough information about the
identity of the context, then there should exist strong priors on object identities, at least at the superordinate level
(people, furniture, vehicles, vegetation, etc.).
For instance, contextual object priming should capture the fact
that while we do not expect to ﬁnd cars in a room, we do
expect a high probability of ﬁnding furniture.
These intuitions are formalized by means of the PDF
P(ouvC) that gives the probability of presence of the object
class o given contextual information vC .
For instance, if
for a scene we obtain P(ouvC) ; 1, then we can be almost
certain about the presence of the object class o in the
scene even before exploring the image in detail.
other hand, if P(ouvC) ; 0, then we can decide that the
object is absent and forego initiating search.
The number of scenes in which the system may be able to make
high-conﬁdence decisions will depend on various factors
such as the strength of the relationship between the target object and its context and the ability of the features vC
to characterize the context efﬁciently.
The function
P(ouvC) is learned by using an annotated image database
(see Appendix A, Subsection B).
Figure 4 shows some typical results from the priming
model for four categories of objects (people, furniture, vegetation,
vehicles).
highconﬁdence predictions were made in at least 50% of the
tested scenes, and presence or absence was correctly predicted by the model on 95% of those images.
model was forced to make binary decisions in all the images (by selecting an acceptance threshold of 0.5), the
presence or absence of the objects was correctly predicted
by the model on 81% of the scenes of the test set.
in the test set were selected such that a random guess
about the presence or absence of an object gives 50% correct predictions.
The results reveal the ability of the contextual features
to distinguish between different environments.
priming provides an efﬁcient technique for reducing the
set of possible objects that are likely to be found within
the scene and for determining whether search needs to be
initiated.
Contextual Guidance of Focus of Attention
The PDF P(xuo, vC) indicates the most likely locations
for the presence of the object class o given context information.
This PDF can be thought of as the input to an
attentional system that directs computational resources
(focus of attention) toward regions more likely to contain
an object of interest.
It also provides criteria for rejecting possible false detections that fall outside the primed
When the target is small (a few pixels), the problem of detection using only object intrinsic (local) features
is ill-posed.
As illustrated in Fig. 1(a), in the absence of
Antonio Torralba
Vol. 20, No. 7/July 2003/J. Opt. Soc. Am. A
contextual information, local information might not be
enough for reliable recognition, because when only local
information is used, similar arrangements of pixel intensities can be found in other regions of the image.
For instance, some of the pedestrians in Fig. 5 are so small as to
be mere scratches on the image.
Similar scratches can
be found in other locations of the picture, but given the
context information they are not considered potential targets because they fall outside the likely ‘‘pedestrian region.’’ During the learning stage (see Appendix A, Subsec-
Contextual priming of superordinate object categories (1, people; 2, furniture; 3, vegetation; 4, vehicles).
The heights of the bars
show the model’s predictions of the likelihood P(ouvC) of ﬁnding members of these four categories in each scene.
Model results on context-driven focus of attention in the task of looking for faces (left) and vegetation (right).
Examples of
real-world scenes and the image regions with the largest likelihood P(x, ouvC) 5 P(x, ouvC)P(ouvC).
The two foci of attention for each
image show how the task (o 5 faces or o 5 trees) changes the way attention is deployed in the image in considering scene/context information.
The factor P(ouvC) is included here to illustrate how attention is not driven to any image region when the target object o is
inconsistent with the context (e.g., trees in an indoor scene or pedestrians on a highway).
J. Opt. Soc. Am. A/Vol. 20, No. 7/July 2003
Antonio Torralba
tion B for a description of the training database and the
learning procedure), the system associates the locations of
objects with the features of the context in which they are
P(xuo, vC) can be used to predict the most likely locations
of the target.
Figure 5 shows some results of the focus-of-attention
system when the task is to look for heads and vegetation
in real-world scenes.
For each image, we show the PDF
P(xuo, vC) superimposed on the original image to better
show the selected regions.
The dark regions indicate low
values of the PDF and therefore image locations with low
probability of containing the target object.
Starting the
search in the regions that are selected by the contextual
priming mechanism greatly reduces the need for exhaustive search.
Note also that task constraints (looking for
faces or for vegetation) changes the way attention is deployed in the image when one is integrating contextual information.
Note that in some of the examples of Fig. 5,
attention is directed to the region that is most likely to
contain the target even when the target object is not
present in the scene.
This illustrates the point that, at
this stage, attention is driven only by global contextual
features and not by the presence of local features that
may belong to the target object.
In the examples shown
in Fig. 5, no target model is included in the selection procedure.
The selected locations are chosen only as a function of task and context.
It is worth contrasting these results to those from
bottom-up models in which focus of attention is mediated
by low-level feature-saliency maps (see Subsection 3.D).
Common to saliency models is the use of features in a
local-type framework, ignoring high-level context information that is available in a global-type framework.
model’s use of the PDF P(xuo, vC) provides information
that is both task-driven (looking for object o) and contextdriven (given holistic context features).
Figure 3 is an
example of region selection using both saliency maps and
context-driven focus of attention.
Contextual Selection of Object Appearance
One major problem encountered in computational approaches to object detection is the large variability in object appearance.
The classical solution is to explore the
space of possible shapes, looking for the best match.
main sources of variability in object appearance are size,
pose (point of view), intraclass shape variability (deforma-
Scale priming from a familiar context.
(a) Examples of scenes and the model’s estimate of the size of a face at the center of focus
of attention.
(b) Scale estimation results plotted against ground truth.
Selection of prototypical object appearances based on
contextual cues.
Antonio Torralba
Vol. 20, No. 7/July 2003/J. Opt. Soc. Am. A
tions, style, etc.), and illumination effects.
Including contextual information can reduce the possible appearances
of the target object that are compatible with the rest of
the scene.
For instance, the expected size of people in an
image differs greatly between an indoor environment and
a perspective view of a street.
The two environments
produce different patterns of contextual features.24
Automatic scale selection is a fundamental problem in
computational vision.
If scale information could be estimated by an efﬁcient preprocessing stage, then subsequent stages of object detection and recognition would be
greatly simpliﬁed by focusing the processing onto only the
relevant scales.
As in the problem of focus of attention,
existing approaches in computational vision for automatic
scale selection use a low-level approach that does not rely
on contextual information.2
Here it is shown that prior
P( suo, vC) provides a strong constraint on scale selection
for object detection.47
See Appendix A for a description of
the training database and the learning procedure of the
Figure 6 shows several results of preattentive scale selection obtained by using the contextual priming model
when it is instructed to look for heads.
For each scene
the mean scale of a head within the scene was estimated
5 *sP( suo, vC)ds.
For 84% of the images tested, the
@hm/2, hm2#, with hm being the actual mean scale of the
heads in the picture, and for 41% of the images the estimated
@hm/1.25, hm1.25#.
The same procedure can be used to estimate other object parameters.
For instance, context introduces strong
constraints on the three-dimensional orientation (pose) of
Once these two aspect parameters (pose and scale)
have been estimated, we can propose a prototypical model
of the target object for a given context.
In the case of a
view-based object representation, the model of the object
will consist of a collection of templates that correspond to
the possible aspects of the target.
As illustrated in Fig.
7, the model provides samples of the expected appearance
of the object when it is embedded in a scene with similar
contextual features.
These views correspond to the distribution of local image features (here vL correspond to
pixel intensities), P(vLuO, vC).
Contextual Modulation of Local Image Saliency
In this subsection we illustrate how the model for contextual priming based on global scene features can be used to
modulate local image saliency.
In the framework presented in Subsection 2.B (Eq. 2), saliency is deﬁned as
S(x) 5 P(vLuvC)21.
That is, saliency is large for local
features that are unusual in the image.
When the target object is indeed salient in the image,
then saliency maps provide an efﬁcient shortcut for object
detection.
However, in general, the object of interest will
not be the most salient object in the image.
The inclusion of contextual priming provides an efﬁcient mechanism for concentrating ﬁxation points only in the image
region that is relevant for the task.
This is very important when the target object is not the most salient element of the scene.
Context provides a way of shadowing
salient image regions that are not relevant for the task.
As described in Eqs. (2) and (3), contextual information
modulates local image saliency as
Sc~x! 5 S~x!P~xuo, vC!P~ouvC!,
where Sc(x) is the local image saliency modulated by context and task demands.
Note that if the object is inconsistent with the scene (P(ouvC) . 0), then the system
does not need to search for the object.
Figure 8 shows an image (a) and the local saliency (b).
Figure 8(c) shows the image region that is relevant for the
task of looking for pedestrians.
In the saliency model,
the image is explored according to the most salient features.
When task and context information are included,
(a) Input image (color is not taken into account).
The task is to look for pedestrians.
(b) Bottom-up saliency map, S(x).
Context-driven focus of attention, Sc(x).
The image region in the shadow is not relevant for the task, and saliency is suppressed.
Points that correspond to the largest salience S(x).
(e) Image regions with the largest salience, including contextual priming, Sc(x).
J. Opt. Soc. Am. A/Vol. 20, No. 7/July 2003
Antonio Torralba
saliency is shadowed outside the selected image region.
Figures 8(d) and 8(e) show the selected image regions
that correspond to maximum points of image saliency.
Including contextual information allows the concentration of attention in the image regions that are relevant for
the task (ﬁnding pedestrians).
4. DISCUSSION
The dominant framework in computational models of object recognition consists in modeling the probability of
presence of an object using only P(OuvL) and ignoring
contextual priors.7,36,42–46
In object-detection approaches
based on local features, the background is a distracting element.
The best performance in detection is obtained
when the object is against an unstructured background.48
However, when background is structured and there is correlation between object and context, a system that makes
use of context could have higher performance and be more
robust to image degradation (see Fig. 1).
In this paper the saliency of a target in an image is
given by the more complete object probability function29,40
P(OuvL , vC).
This allows for use of the correlations that
exist between the background and the objects within the
There are a few studies that propose computational
contextual
recognition.26,27
Common to these models is the use of
object-centered representations in which the context is
described as a collection of objects and a model of the joint
distribution of objects in a reduced world.
This approach
requires object-centered mechanisms that provide candidate objects that are transformed into recognizable objects through analysis of the consistency with the other
candidate objects in the scene.
There is no attempt at
recognizing the scene/context as a ﬁrst stage.
In this paper we have focused on encoding global scene conﬁguration in order to introduce contextual information to drive
the search for objects.
This dispenses with the need to
identify individual objects or regions within a scene.22,23
We use the global output distribution of a Gabor ﬁlter
bank encoded at low resolution, in both the spatial and
the spectral domains, as a representation of the scene/
But there are many other global features that
are relevant for encoding scene properties without encoding
histograms,36
histograms,49
high-order
statistics,47
arrangements.23
The strength of contextual features in providing priors
for objects depends on two main factors:
ﬁrst, how well
the contextual features differentiate between different
scenes and second, how strong the relationship is between
the object of interest and the scene.
Some objects in reduced environments may be very poorly constrained by
The strength of the contextual priors is a function of both the object property that is relevant for the
task (e.g., the location, the scale) and the conﬁgurational
regularities of the scene.
For instance, in the case of face
detection, the orientation of faces is very poorly related to
the scene.
However, other object properties such as scale
or location have a strong relationship with the scene.
Figure 9 illustrates how the strength of contextual priors
for one object (e.g., cars) changes as a function of the
(a) scale, pose, and location are constrained; (b)
pose and location are constrained but scale is unconstrained; (c) only location is constrained; (d) both scale
and pose are constrained but location is unconstrained.
The results provided in this paper have focused on contextual priors P(OuvC) and on saliency P(vLuvC)21.
of these factors are interesting, because they provide information about the location of the target without using
The strength of contextual features in providing priors for objects depends on two factors:
(1) how well the contextual features
differentiate between different scenes and (2) how strong the relationship is between the object of interest and the scene.
Antonio Torralba
Vol. 20, No. 7/July 2003/J. Opt. Soc. Am. A
any information about the expected appearance of the target (e.g., cars have dominant horizontal structure, and pedestrians are vertical structures).
We have ignored the
factor P(vLuO, vC) from Eq. (2), which is also known to
have a large inﬂuence on object search.6,33
5. CONCLUSION
I have shown how simple holistic contextual features provide strong constraints for the identity, locations, and
scales of objects in a scene.
The proposed architecture
for attention guidance consists of three parallel modules
extracting different information:
(1) bottom-up saliency,
(2) object-centered features, and (3) contextual modulation of attention.
The focus has been on showing how to
introduce global scene factors in order to model the contextual modulation of local saliency.
The proposed system learns the relationship between global scene features
and local object properties (identity, location, and image
The inclusion of scene factors in models of attention
and also in computational approaches for object detection
and recognition is essential for building reliable and efﬁcient systems.
Context information offers a way of cutting down the need for exhaustive search, thus providing
a shortcut to object detection.
APPENDIX A: PROBABILITY DENSITY
FUNCTION MODELING AND LEARNING
Image Saliency
In this model, saliency is computed from the distribution
of features within the image.50
We model the PDF
P(vLuvC) by using a mixture of Gaussians with N clusters,
P~vLuvC! 5 (
biG~vL ; mi , Xi!,
G~vL ; m, X! 5
exp@21/2~vL 2 m!TX21~vL 2 m!#
~2p!N/2uXu1/2
simplicity,
approximate
distribution
P(vLuvC) as the distribution of local features vL in the input image (this approximation assumes that images with
similar contextual features have similar distributions of
local features).
The parameters of the mixture of Gaussians (bi , mi, and X) are obtained by using the EM
algorithm.51–53
Given a set of Nt training samples (in
this case these samples correspond to the set of all local
feature vectors computed from one image), the EM algorithm is an iterative procedure:
kG~vt ; mi
kG~vt ; mi
k~t!~vt 2 mi
k11!~vt 2 mi
Once the learning is completed (usually there is no improvement after ten iterations), we can evaluate Eq. (A1)
at each location.
Contextual Priors
For the experiments presented in this paper, a database
of 2700 annotated images was used.
Pictures were 256
3 256 pixels in size.
Images were transformed into
gray scale, as color was not included in this study.
image was annotated by indicating the categories of the
objects present in the scene and their locations and sizes
in pixels.
The contextual prior model requires the learning of the
P(ouvC), P(xuo, vC), and P(tux, vC , o).
mixture of Gaussians is used to model each PDF.
the database was used for the learning stage and the
other half for the test.
The learning of the P(ouvC) 5 P(vCuo)P(o)/P(vC) with
P(vC) 5 P(vCuo)P(o) 1 P(vCu¬o)P(¬o), where ¬o denotes object absent, is done by approximating the in-class
and out-of-class PDFs by a mixture of Gaussians.
the in-class PDF we use
P~vCuo! 5 (
biG~vC ; ai , Ai!,
where G(vC ; ai , Ai) is a multivariate Gaussian function
of vC with center ai and covariance matrix Ai ; N is the
number of Gaussians used for the approximation.
parameters
algorithm.30,53
The same scheme holds for P(vCu¬o).
The probability of the object presence P(o) is approximated by the frequency presence of the object class.
our database we use P(o) 5 0.5 for evaluating model performances.
In our experiments we found that the learning requires the use of a few Gaussian clusters for modeling the PDFs (the results summarized in Subsection 3.A
were obtained with N 5 2).
The learning is performed
by using the half of the database; the remaining half is
used for the testing stage.
The learning of the PDF P(xuo, vC) provides the relationship between the context and the more typical locations of the objects belonging to one class.
The images
used for the training of the PDF P(x, vCuo) are a random
J. Opt. Soc. Am. A/Vol. 20, No. 7/July 2003
Antonio Torralba
selection of pictures among the ones that contain the object o.
For each image we know the location of the object
of interest, and we also compute the contextual features
The PDF learns the joint distribution between contextual features and the location of the target.
For modeling the PDF we use a mixture of Gaussians:
P~x, vCuo! 5 (
biG~x; xi , Xi!G~vC ; vi , Vi!.
The joint PDF is modeled as a sum of N Gaussian clusters.
Each cluster is decomposed into the product of two
Gaussians.
The ﬁrst Gaussian models the distribution of
object locations, and the second Gaussian models the distribution of contextual features for each cluster.
The center of the Gaussian distribution of object locations is written as having a linear dependency with respect to the
contextual features for each cluster52:
xi 5 ai 1 Ai(vC
algorithm.52
The performances shown in Subsection 3.B
(Fig. 5) were obtained with N 5 4 clusters.
Learning for
scale and pose priming follows a similar strategy.
ACKNOWLEDGMENTS
The author thanks Aude Oliva, Whitman Richards, and
William T. Freeman for fruitful discussions and Pawan
Sinha and Gadi Geiger for comments.
The author’s e-mail address is .