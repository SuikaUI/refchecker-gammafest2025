Using Maximum Entropy for Automatic Image
Annotation
Jiwoon Jeon and R. Manmatha
Center for Intelligent Information Retrieval
Computer Science Department
University of Massachusetts Amherst
Amherst, MA-01003.
{jeon, manmatha}@cs.umass.edu,
Abstract. In this paper, we propose the use of the Maximum Entropy
approach for the task of automatic image annotation. Given labeled
training data, Maximum Entropy is a statistical technique which allows
one to predict the probability of a label given test data. The techniques
allow for relationships between features to be eﬀectively captured. and
has been successfully applied to a number of language tasks including
machine translation. In our case, we view the image annotation task as
one where a training data set of images labeled with keywords is provided and we need to automatically label the test images with keywords.
To do this, we ﬁrst represent the image using a language of visterms and
then predict the probability of seeing an English word given the set of
visterms forming the image. Maximum Entropy allows us to compute the
probability and in addition allows for the relationships between visterms
to be incorporated. The experimental results show that Maximum Entropy outperforms one of the classical translation models that has been
applied to this task and the Cross Media Relevance Model. Since the
Maximum Entropy model allows for the use of a large number of predicates to possibly increase performance even further, Maximum Entropy
model is a promising model for the task of automatic image annotation.
Introduction
The importance of automatic image annotation has been increasing with the
growth of the worldwide web. Finding relevant digital images from the web and
other large size databases is not a trivial task because many of these images do
not have annotations. Systems using non-textual queries like color and texture
have been proposed but many users ﬁnd it hard to represent their information
needs using abstract image features. Many users prefer textual queries and automatic annotation is a way of solving this problem.
Recently, a number of researchers have applied various statistical
techniques to relate words and images. Duygulu et al. proposed that the image
annotation task can be thought of as similar to the machine translation problem
and applied one of the classical IBM translation models to this problem.
Jeon et al. showed that relevance models (ﬁrst proposed for information
retrieval and cross-lingual information retrieval ) could be used for image
annotation and they reported much better results than . Berger et al. 
showed how Maximum Entropy could be used for the machine translation tasks
and demonstrated that it outperformed the classical (IBM) machine translation
models for the English-French translation task. The Maximum Entropy approach
has also been applied successfully to a number of other language tasks .
Here, we apply Maximum Entropy to the same dataset used in and show
that it outperforms both those models. We ﬁrst compute an image dictionary
of visterms which is obtained by ﬁrst partitioning each image into rectangular
regions and then clustering image regions across the training set. Given a training
set of images and keywords, we then deﬁne unigram predicates which pair image
regions and labels. We automatically learn using the training set how to weight
the diﬀerent terms so that we can predict the probability of a label (word) given
a region from a test image. To allow for relationships between regions we deﬁne
bigram predicates. In principle this could be extended to arbitrary n-grams but
for computational reasons we restrict ourselves to unigram and bigram predicates
in this paper.
Maximum Entropy maximizes entropy i.e. it prefers a uniform distribution when no information is available. Additionally, the approach automatically
weights features (predicates). The relationship between neighboring regions is
very important in images and Maximum Entropy can account for this in a natural way.
The remainder of the paper is organized as follows. Related work is discussed
in section 2. Sections 3 provides a brief description of the features and image
vocabulary used while the Maximum Entropy model and its application to image
annotation are brieﬂy discussed in 4 Experiments and results are discussed in 5
while Section 6 concludes the paper.
Related Work
In image annotation one seeks to annotate an image with its contents. Unlike
more traditional object recognition techniques we are not interested
in specifying the exact position of each object in the image. Thus, in image
annotation, one would attach the label “car” to the image without explicitly
specifying its location in the picture. For most retrieval tasks, it is suﬃcient to do
annotation. Object detection systems usually seek to ﬁnd a speciﬁc foreground
object, for example, a car or a face. This is usually done by making separate
training and test runs for each object. During training positive and negative
examples of the particular object in question are presented. However, in the
annotation scheme here background objects are also important and we have
to handle at least a few hundred diﬀerent object types at the same time. The
model presented here learns all the annotation words at the same time. Object
recognition and image annotation are both very challenging tasks.
Recently, a number of models have been proposed for image annotation . Duygulu et al described images using a vocabulary of blobs. First,
regions are created using a segmentation algorithm like normalized cuts. For
each region, features are computed and then blobs are generated by clustering
the image features for these regions across images. Each image is generated by
using a certain number of these blobs. Their Translation Model applies one of the
classical statistical machine translation models to translate from the set of blobs
forming an image to the set of keywords of an image. Jeon et al. instead
assumed that this could be viewed as analogous to the cross-lingual retrieval
problem and used a Cross Media Relevance Model (CMRM) to perform both
image annotation and ranked retrieval. They showed that the performance of the
model on the same dataset was considerably better than the models proposed
by Duygulu et al. and Mori et al. .
The above models use a discrete image vocabulary. A couple of other models
use the actual (continuous) features computed over each image region. This
tends to give improved results. Correlation LDA proposed by Blei and Jordan
 extends the Latent Dirichlet Allocation (LDA) Model to words and images.
This model assumes that a Dirichlet distribution can be used to generate a
mixture of latent factors. This mixture of latent factors is then used to generate
words and regions. Expectation-Maximization is used to estimate this model.
Lavrenko et al. proposed the Continuous Relevance Model (CRM) to extend the
Cross Media Relevance Model (CMRM) to directly use continuous valued
image features. This approach avoids the clustering stage in in CMRM. They
showed that the performance of the model on the same dataset was a lot better
than other models proposed.
In this paper, we create a discrete image vocabulary similar to that used
in Duygulu et al and Jeon et al. . The main diﬀerence is that the initial
regions we use are rectangular and generated by partitioning the image into a
grid rather than using a segmentation algorithm. We ﬁnd that this improves
performance (see also ). Features are computed over these rectangular regions
and then the regions are clustered across images. We call these clusters visterms
(visual terms) to acknowledge that they are similar to terms in language.
Berger et al. proposed the use of Maximum Entropy approaches for various Natural Language Processing tasks in the mid 1990’s and after that many
researchers have applied this successfully to a number of other tasks. The Maximum Entropy approach has not been much used in computer vision or imaging
applications. In particular, we believe this is the ﬁrst application of the Maximum Entropy approach to image annotation
Visual Representation
An important question is how can one create visterms. In other words, how does
one represent every image in the collection using a subset of items from a ﬁnite
set of items. An intuitive answer to this question is to segment the image into
regions, cluster similar regions and then use the clusters as a vocabulary. The
hope is that this will produce semantic regions and hence a good vocabulary. In
general, image segmentation is a very fragile and erroneous process and so the
results are usually not very good.
Barnard and Forsyth and Duygulu et al. used general purpose segmentation algorithms like Normalized-cuts to extract regions. In this paper, we
use a partition of the image into rectangular grids rather than a segmentation
of the image. The annotation algorithm works better when the image is partitioned into a regular grid. than if a segmentation algorithm is used to break up
the image into regions (see also ). This means that the current state of the
art segmentation algorithms are still not good enough to extract regions corresponding to semantic entities. The Maximum Entropy algorithm cannot undo
the hard decisions made by the segmentation algorithm. These segmentation
algorithms make decisions based on a single image. By using a ﬁner grid, the
Maximum Entropy algorithm automatically makes the appropriate associations.
For each segmented region, we compute a feature vector that contains visual
information of the region such as color, texture, position and shape. We used
K-means to quantize these feature vectors and generate visterms. Each visterm
represent a cluster of feature vectors. As in Duygulu et al we arbitrarily
choose the value of k. In the future, we need a systematic way of choosing the
After the quantization, each image in the training set can now be represented
as a set of visterms. Given a new test image , it can be segmented into regions and
region features can be computed. For each region, the visterm which is closest
to it in cluster space is assigned.
Maximum Entropy For Image Annotation
We assume that there is a random process that given an image as an observation generates a label y, an element of a ﬁnite set Y . Our goal is to create a
stochastic model for this random process. We construct a training dataset by
observing the behavior of the random process. The training dataset consists of
pairs (x1, y1), (x2, y2), ..., (xN, yN) where xi represents an image and yi is a label. If an image has multiple labels, xi may be part of multiple pairings with
other labels in the training dataset. Each image xi is represented by a vector of
visterms. Since we are using rectangular grids, for each position of the cell there
is a corresponding visterm.
Predicate Functions and Constraints
We can extract statistics from the training samples and these observations should
match the output of our stochastic model. In Maximum Entropy, any statistic is
represented by the expected value of a feature function. To avoid confusion with
image features, from now on, we will refer to the feature functions as predicates.
Two diﬀerent types of predicates are used.
– Unigram Predicate
This type of predicate captures the co-occurrence statistics of a visual term
and a label. The following is an example unigram predicate that checks the
co-occurrence of the label ‘tiger’ and the visterm v1 in image x.
fv1,tiger(x, y) =
1 if y = tiger and v1 ∈x
0 otherwise
If image x contains visual term v1 and has ‘tiger’ as a label, then the value of
the predicate is 1, otherwise 0. We have unigram predicates for every label
and visterm pair that occurs in the training data. Since, we have 125 visual
terms and 374 labels, the total number of possible unigram predicates is
– Bigram Predicate
The bigram predicate captures the co-occurrence statistic of two visterms
and a label. This predicate attempts to capture the conﬁguration of the
image and the positional relationship between the two visterms is important.
Two neighboring visterms are horizontally connected if they are next to each
other and their row coordinates are the same. They are vertically connected
if they are next to each other and their column coordinates are the same.
The following example of a bigram predicate models the co-occurrence of the
label ‘tiger’ and the two horizontally connected visterms v1 and v2 in image
fhorizontal v1v2,tiger(x, y) =
1 if y = tiger and x contains
horizontally connected v1, v2
0 otherwise
If x contains horizontally connected visterms v1 and v2 and ‘tiger’ is a label
of x, then the value of the predicate is 1. We also use predicates that captures
the occurrence of two vertically connected visterms. In the same way, we can
design predicates that use 3 or more visterms or more complicated positional
relationships. However, moving to trigrams or even n-grams leads to a large
increase in the number of predicates and the number of parameters that must
be computed and this requires substantially more computational resources.
The expected value of a predicate with respect to the training data is deﬁned
as follow,
˜p(x, y)f(x, y)
where ˜p(x, y) is a empirical probability distribution that can be easily calculated
from the training data. The expected value of the predicate with respect to the
output of the stochastic model should be equal to the expected value measured
from the training data.
˜p(x, y)f(x, y) =
˜p(x)p(y|x)f(x, y)
where p(y|x) is the stochastic model that we want to construct. We call equation
4 a constraint. We have to choose a model that satisﬁes this constraint for all
predicates.
Parameter Estimation and Image Annotation
In theory, there are an inﬁnite number of models that satisfy the constraints
explained in section 4.1. In Maximum Entropy, we choose the model that has
maximum conditional entropy
˜p(x)p(y|x) log p(y|x)
The constrained optimization problem is to ﬁnd the p which maximizes H(p)
given the constraints in equation 4. Following Berger et al we can do this using
Lagrange multipliers. For each predicate, fi, we introduce a Lagrange multiplier
λi. We then need to maximize
Λ(p, λ) = H(p) +
λi(p(fi) −˜p(fi))
Given ﬁxed λ, the solution may be obtained by maximizing p. This leads to
the following equations :
λifi(x, y)
˜p(x)logZ(x) +
where Z(x) is a normalization factor which is obtained by setting P
y p(y|x) = 1.
The solution to this problem is obtained by iteratively solving both these
equations. A few algorithms have been proposed in the literature including Generalized Iterative Scaling and Improved Iterative Scaling . We use Limited
Memory Variable Metric method which is very eﬀective for Maximum Entropy
parameter estimation . We use Zhang Le’s maximum entropy toolkit for
the experiments in this paper.
Experiment
We use the dataset in Duygulu et al. to compare the models. We partition
images into 5 × 5 rectangular grids and for each region, extract a feature vector.
The feature vector consists of average LAB color and the output of the Gabor
ﬁlters. By clustering the feature vectors across the training images, we get 125
The dataset has 5,000 images from 50 Corel Stock Photo cds. Each cd includes
100 images on the same topic. 4,500 images are used for training and 500 are
used for test. Each image is assigned from 1 to 5 keywords. Overall there are
374 words (see ).
We automatically annotate each test image using the top 5 words and then
simulate image retrieval tasks using all possible one word queries. We calculate
the mean of the precisions and recalls for each query and also the F-measure by
combining the two measures using F = 1/(λ 1
P + (1 −λ) 1
R) where P is the mean
precision, R is the mean recall. We set the λ as 0.5.
In this paper, we used the results of the Translation Model and the
CMRM as our baseline since they also use similar features. The experiment
shows that Maximum Entropy using unigram predicates has performance comparable to the CMRM model (both have F-measures of 0.1). While one has
better recall, the other has better precision. Both models outperform the classical translation model used by . Using unigram and bigram predicates improves
the performance of the Maximum Entropy model. Our belief is that by using
predicates which provide even more conﬁguration information, the model’s performance can be further improved.
Models which use continuous features (for example ) perform even better.
Maximum Entropy models can also be used to model such continuous features
and future work will involve using such features. The results show that Maximum
Entropy models have great potential and also enable us to incorporate arbitrary
conﬁguration information in a natural way.
Table 1. Experimental results
Experiment
recall precision F-measure
Translation
Binary Unigram
Binary Unigram + Binary Bigram 0.12
Conclusions and Future Work
In this paper we show that Maximum Entropy may be used for the image annotation task and the experimental results show the potential of the approach.
Since, we can easily add new types of predicates ( this is the one of the nice properties in Maximum Entropy ), there is great potential for further improvements
in performance. More work on continuous valued predicates, image segmentation techniques and feature extraction methods will also lead to performance
improvements.
Fig. 1. Examples: Images in the ﬁrst row are the top 4 images retrieved by query
‘swimmer’. Images in the second row are the top 4 images retrieved by query ‘ocean’.
Acknowledgments
We thank Kobus Barnard for making their dataset available. We used Zhang
Le’s publicly available Maximum Entropy Modeling Toolkit . This work was
supported in part by the Center for Intelligent Information Retrieval and in part
by the National Science Foundation under grant NSF IIS-9909073. Any opinions,
ﬁndings and conclusions or recommendations expressed in this material are the
author(s) and do not necessarily reﬂect those of the sponsors.