Feature-level Deeper Self-Attention Network for Sequential Recommendation
Tingting Zhang1,2 , Pengpeng Zhao1,2∗, Yanchi Liu3 , Victor S. Sheng4 ,
Jiajie Xu1 , Deqing Wang5 , Guanfeng Liu6 and Xiaofang Zhou7,2
1Institute of AI, School of Computer Science and Technology, Soochow University, China
2Zhejiang Lab, China
3Rutgers University, New Jersey, USA
4University of Central Arkansas, Conway, USA
5School of Computer, Beihang University, Beijing, China
6Department of Computing, Macquarie University, Sydney, Australia
7The University of Queensland, Brisbane, Australia
Sequential recommendation, which aims to recommend next item that the user will likely interact in
a near future, has become essential in various Internet applications. Existing methods usually consider
the transition patterns between items, but ignore the
transition patterns between features of items. We
argue that only the item-level sequences cannot reveal the full sequential patterns, while explicit and
implicit feature-level sequences can help extract the
full sequential patterns. In this paper, we propose
a novel method named Feature-level Deeper Self-
Attention Network (FDSA) for sequential recommendation. Speciﬁcally, FDSA ﬁrst integrates various heterogeneous features of items into feature sequences with different weights through a vanilla attention mechanism. After that, FDSA applies separated self-attention blocks on item-level sequences
and feature-level sequences, respectively, to model
item transition patterns and feature transition patterns. Then, we integrate the outputs of these two
blocks to a fully-connected layer for next item recommendation. Finally, comprehensive experimental results demonstrate that considering the transition relationships between features can signiﬁcantly improve the performance of sequential recommendation.
Introduction
With the quick development of the Internet, sequential recommendation has become essential in various applications, such
as ad click prediction, purchase recommendation and web
page recommendation. In such applications, each user behavior can be modeled as a sequence of activities in chronological order, with his/her following activity inﬂuenced by the
previous activities. And sequential recommendation aims to
recommend the next item that a user will likely interact by
∗Pengpeng Zhao is the corresponding author and his email is 
capturing useful sequential patterns from user historical behaviors.
Increasing research interests have been put in sequential
recommendation with various models proposed. For modeling sequential patterns, the classic Factorizing Personalized
Markov Chain (FPMC) model has been introduced to factorize the user-speciﬁc transition matrix by considering the
Markov Chains [Rendle et al., 2010]. However, the Markov
assumption has difﬁculty in constructing a more effective
relationship among factors [Huang et al., 2018]. With the
success of deep learning, Recurrent Neural Network (RNN)
methods have been widely adopted in sequential recommendation [Hidasi et al., 2016; Zhao et al., 2019]. These RN-
N methods usually employ the last hidden state of RNN as
the user representation, which is used to predict the next action. Despite the success, these RNN models are difﬁcult to
preserve long-range dependencies even using the advanced
memory cell structures like Long Short-Term Memory (LST-
M) and Gated Recurrent Units (GRU) [Chung et al., 2014].
Besides, RNN-based methods need to learn to pass relevant information forward step by step, which makes RNN hard
to parallelize [Al-Rfou et al., 2019]. Recently, self-attention
networks (SANs) have shown promising empirical results in
various NLP tasks, such as machine translation [Vaswani et
al., 2017], natural language inference [Shen et al., 2018],
and question answering [Li et al., 2019]. One strong point
of self-attention networks is the strength of capturing longrange dependencies by calculating attention weights between
each pair of items in a sequence. Inspired by self-attention
networks, Kang et al. [Kang and McAuley, 2018] proposed
Self-Attentive Sequential Recommendation model (SASRec)
that applied a self-attention mechanism to replace traditional
RNNs for sequential recommendation and achieved the stateof-the-art performance. However, it only considers the sequential patterns between items, ignoring the sequential patterns between features that are beneﬁcial for capturing the
user’s ﬁne-grained preferences.
Actually, our daily activities usually present transition patterns at the item feature level, i.e., explicit features like category or other implicit features. For example, a user is more
likely to buy shoes after buying clothes, indicating that the
Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)
next product’s category is highly related to the category of
the current product.
Here we refer to the user’s evolving
appetite for structured attributes (e.g., categories) as explicit feature transition.
Moreover, an item may also contain
some other unstructured attributes, like description texts or
image, which present more details of the item. Therefore, we
want to mine the user’s potential feature-level patterns from
these unstructured attributes, which we call implicit feature
transition. However, explicit and implicit feature transitions
among item features are often overlooked by existing methods. We argue that only the item-level sequences cannot reveal the full sequential patterns, while the feature-level sequences can help achieve this goal better. To this end, in this
work, we propose a novel feature-level deeper self-attention
network for sequential recommendation. For capturing explicit feature-level transition patterns, instead of using the
combined representation of item and its features, we apply
separated self-attention blocks on item sequences and feature
sequences, respectively, to capture the item-item and featurefeature relationships. Then, we combine the contexts at the
item-level and the feature-level to make a recommendation.
Moreover, we further investigate how to capture meaningful
implicit feature-level transition patterns from heterogeneous
attributes of items. We additionally utilize vanilla attention
to assist feature-based self-attention block to adaptively select essential features from the various types of attributes
of items and further learn potential implicit feature transition
patterns. Then, we combine item transition patterns with implicit feature transition patterns to a fully-connected layer for
the recommendation. Finally, we conduct extensive experiments on two real-world datasets of a famous E-commerce
platform. Experimental results demonstrate that considering
feature-level transition patterns can signiﬁcantly improve the
performance of recommendation.
The main contributions of this paper are summarized as
• We propose a novel framework, Feature-level Deeper
Self-Attention Network (FDSA), for sequential recommendation. FDSA applies self-attention networks to integrate item-level transitions with feature-level transitions for modeling user’s sequential intents.
• Explicit and implicit feature transitions are modeled
by applying different self-attention blocks on item sequences and feature sequences, respectively.
For obtaining implicit feature transitions, a vanilla attention
mechanism is added to assist feature-based self-attention
block to adaptively select important features from various item attributes.
• We conduct extensive experiments on two real-world
datasets to demonstrate the effectiveness of our proposed
Related Work
In this section, we review closely related work from two perspectives, which are sequential recommendation and attention mechanisms.
Sequential Recommendation
Many sequential recommendation methods strove to capture
meaningful sequence patterns more efﬁciently. Most existing sequential approaches focused on Markov Chain based
methods and Neural network-based methods. Markov Chain
based methods estimated an item-item transition probability matrix and used it to predict the next item given the last
interaction of a user. FPMC fused matrix factorization and
ﬁrst-order Markov Chains to capture long-term preferences
and short-term item-item transitions respectively [Rendle et
al., 2010]. All these Markov Chain based methods have the
same deﬁciency that these models only model the local sequential pattern between every two adjacent items. With the
success of neural network, recurrent neural network (RNN)
methods are widely adopted in sequence modeling. [Hidasi et
al., 2016] proposed GRU4Rec approach to model item transition patterns using Gated Recurrent Unit (GRU). Though
RNN is an efﬁcient way to model sequential patterns, it still
suffers from several difﬁculties, such as hard to parallelize,
time-consuming, and hard to preserve long-term dependencies even using the advanced memory cell structures like L-
STM and GRU.
Attention Mechanisms
Attention mechanisms are popular in many tasks, such as image/video caption [Chen et al., 2017], machine translation
[Chen et al., 2018] and recommendation [He et al., 2018].
Recently, self-attention networks have achieved promising
empirical results in machine translation task [Vaswani et al.,
Inspired by Transformer, [Zhou et al., 2018] proposed an attention-based user behavior modeling framework
ATRank, which projected user behavior representation into
multiple latent spaces and then used the self-attention network to model the inﬂuences brought by other behaviors.
[Huang et al., 2018] proposed a uniﬁed framework CSAN
that modeled multiple types of behaviors and various modal
items into a common latent space and then applied the selfattention mechanism to extract different aspects of user’s behavior sequence.
[Zhou et al., 2018; Huang et al., 2018]
focused on modeling multiple types of actions, but collecting multiple behaviors in many applications is difﬁcult, so
here we only consider modeling single-type behavior. [Kang
and McAuley, 2018] applied self-attention network to model sequential recommendation, conﬁrming that self-attention
based methods have achieved better performance than RNN.
Different from the above approaches in that they only model the item-level sequences, but we employ separated selfattention blocks on the item-level sequences and the feature
sequences, respectively, to learn item transition patterns and
feature transition patterns and the experimental results show
the signiﬁcant effects of our model.
Feature-level Deeper Self-Attention
Network for Sequential Recommendation
In this section, we ﬁrst describe the problem statement in our
work, and then present the architecture of our feature-level
deeper self-attention network (FDSA) for next item recommendation.
Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)
Problem Statement
Before going into the details of our proposed model, we ﬁrst
introduce notations used in this paper and deﬁne the sequential recommendation problem. We denote a set of users as U
= {u1, u2,..., uN} and a set of items as I = {i1, i2,..., iM},
where N and M are the numbers of users and items, respectively. We use S = {s1, s2,..., s|S|} to denote a sequence of
items in chronological order that a user has interacted with
before, where si ∈I. Each item i has some attributes, such
as category, brand, and description text. Here we take category as an example, the category of item i is denoted as ci ∈C,
where C is the set of categories. The goal of sequential recommendation is to recommend the next item that the user may
act on, given the user historical activities on items.
The Network Architecture of Feature-level
Deeper Self-Attention (FDSA)
As we mentioned before, daily human activities usually
present feature-level (e.g., category-level) transition patterns.
In this paper, we propose a novel feature-level deeper selfattention network for sequential recommendation (FDSA).
FDSA utilizes not only the item-based self-attention block
to learn item-level sequence patterns but a feature-based selfattention block to search for feature-level transition patterns.
As shown in Figure 1 FDSA consists of ﬁve components, i.e.,
Embedding layer, Vanilla Attention layer, Item-based selfattention block, Feature-based self-attention block, and Fullyconnected layer. Speciﬁcally, we ﬁrst project the sparse representation of items and discrete attributes of items (i.e., onehot representation) into low-dimensional dense vectors. For
text attributes of items, we employ a topic model to extract the
topical keywords of these texts, and then apply Word2vector
to gain the word vector representation of these keywords. Due
to the features (attributes) of item are often heterogeneous and
come in different domains and data types. Hence, we utilize
a vanilla attention mechanism to assist the self-attention network in selecting important features from the various features
of items adaptively. After that, a user’s sequence patterns
are learned through two self-attention networks, in which the
item-based self-attention block is applied to learn item-level
sequence patterns, and the feature-based self-attention block
is used to capture feature-level transition patterns. Finally, we
integrate the outputs of these two blocks to a fully-connected
layer for getting the ﬁnal prediction. Next, we will introduce
the details of each component of FDSA.
Embedding layer.
Due to the number of user’s action sequence is not ﬁxed, we take a ﬁxed-length sequence s =
(s1, s2, ..., sn) from user’s history sequence to calculate user’s historical preferences, where n denotes the maximum
length that our model handles. If a user’s action sequence
is less than n, we add zero-padding to the left side of the sequence to convert the user’ action sequence to a ﬁxed-length.
If a user’s sequence length is greater than n, we take the
most recent n behaviors. Similarly, we process the feature sequence in the same way. Let us use the category information
as an example. Since each item corresponds to a category,
we get a ﬁxed-length category sequence c = (c1, c2, ..., cn).
Then, we apply a lookup layer to transform the one-hot vectors of action sequence s and its corresponding category sequence c into dense vector representations. For other categorical features (such as brand, seller), the same way is applied.
For the textual features (i.e., description text, title), we ﬁrst
utilize the widely-used topic model to extract the topical keywords of texts, then apply Word2vector model to learn textual
semantic representations. In this paper, we extract ﬁve topical keywords from the description text and title of each item,
and then apply the Mean Pooling method to fuse ﬁve topical
keyword vectors into a vector representation.
Vanilla attention layer.
Since the characteristics of items
are often heterogeneous, it is difﬁcult to know which features
will determine a user’s choice. Therefore, we employ vanilla attention to assist the feature-based self-attention block in
capturing the user’s varying appetite toward attributes (e.g.,
categories, brands). Given an item i, its attributes can be
embedded as Ai = {vec(ci), vec(bi), vec(itemtext
vec(ci) and vec(bi) represent the dense vector representation of category and brand of item i, respectively.
vec(itemtext
) denotes the textual feature representation of
item i. Formally, the attention network is deﬁned as follows.
αi = softmax(WfAi + bf),
where Wf is d × d matrice and bf is d-dimensional vector.
Finally, we compute the feature representation of item i as a
sum of the item i’s attribute vector representations weighted
by the attention scores as follows.
fi = αiAi.
It is worth noting that if item i only considers one feature
(e.g., category), then the feature representation of item i is
Feature-based self-attention block.
Since the item-based
self-attention block and the feature-based self-attention block
only differ in their inputs, we focus on illustrating the process of the feature-based self-attention block in detail. From
the above attention layer, we can get a feature representation fi for item i.
Thus, given a user, we get the feature
sequence f = {f1, f2, ..., fn}. To model category-level transition patterns, we utilize the self-attention network proposed
by [Vaswani et al., 2017], which can keep the sequential contextual information and capture the relationships between categories in the category sequence, regardless of their distance.
Though the self-attention network can ensure computational
efﬁciency and derive long-term dependencies, it ignores the
positional information of the sequential input [Gehring et al.,
2017]. Hence, we inject a positional matrix P ∈Rn×d into
the input embedding. Namely, the input matrix of the featurebased self-attention block is
The scaled dot-product attention (SDPA) proposed by
[Vaswani et al., 2017] is deﬁned as below:
SDPA(Q, K, V) = softmax(QKT
Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)
categroy 1
Vanilla Attention Layer
Embedding Layer
Fully-connected
Fullyconnected
Fullyconnected
Item-based self-attention block
Feature-based self-attention block
decription
categroy 2
decription
categroy 3
decription
Figure 1: The Network Architecture of FDSA.
where Q, K, V represent query, key, and value, respectively,
d denotes feature dimension of each feature. Here, query, key
and value in the feature-based self-attention block equal to F,
we ﬁrst convert it to three matrices through linear transformation, and then feed them into the SDPA as follows.
Hf = SDPA(FWQ, FWK, FWV ),
where WQ, WK, WV ∈Rd×d are the projection matrices.
In order to enable the model to jointly attend to information
from different representation subspaces at different positions
[Vaswani et al., 2017], the self-attention adopts multi-head attention (MH). The multi-head attention is deﬁned as follows.
Mf = MH(F) = Concat(h1, h2, ..., hlf )WO,
hi = SDPA(FWQ
where WO, WQ
i are parameters to be learned and
lf is the number of heads in the feature-based self-attention
Also, the self-attention network employs a residual connection, a layer normalization and two-layer fullyconnected layer with a ReLU activation function to strengthen the performance of the self-attention network. Finally, the
output of the feature-based self-attention block is deﬁned as
Mf = LayerNorm(Mf + F),
Of = ReLU((MfW1 + b1)W2 + b2),
Of = LayerNorm(Of + Mf),
where W∗, b∗are model parameters. For the sake of simplicity, we deﬁne the entire self-attention block as follows.
Of = SAB(F).
After the ﬁrst self-attention block, Of essentially aggregates
all previous features’ embedding. However, it may need to
capture more complex feature transitions via another selfattention block based on Of. Thus, we stack the self-attention
block and the q-th (q > 1) block is deﬁned as follows.
= SAB(O(q−1)
where O(0)
Item-based self-attention block.
The goal of the itembased self-attention block is to learn meaningful item-level
transition patterns. Given a user, we can get an item action
sequence s whose corresponding matrix is S. Thus, the output of the stack item-based self-attention block is constructed
as follows.
= SAB(O(q−1)
where O(0)
Fully-connected layer.
To capture the transition patterns of
items and categories simultaneously, we concatenate the output of item-based self-attention block O(q)
and the output of
feature-based self-attention block O(q)
together and project
them into a fully-connected layer.
Wsf + bsf,
where Wsf ∈R2d×d, bsf ∈Rd. Finally, we calculate the
user’s preference for items through a dot product operation.
t,i = OsftNT
where Osft denotes the t-th line of Osf, N
is an item embedding matrix, yt,i is the relevance of
item i being the next item given the previous t items
(i.e., s1, s2, ..., st). It is worth noting that the model inputs a
sequence (i1, i2, ..., in−1) and its expected output is a ‘shifted’ version of the same sequence: (i2, i3, ..., in) during training process. In the test process, we take the last row of matrix
Osf to predict the next item.
The Loss Function for Optimization
In this subsection, to effectively learn from the training process, we adopt the binary cross-entropy loss as the optimization objective function of our FDSA model, which is deﬁned
t∈[1,2,...,n]
[log(σ(yt,i)) +
log(1 −σ(yt,j))].
Moreover, for each target item i in each action sequence, we
randomly sample a negative item j.
Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)
Experiments
In this section, we conduct experiments to evaluate the performance of our proposed method FDSA on two real-world
datasets. We ﬁrst brieﬂy introduce the datasets and baseline
methods, then we compare FDSA with these baseline methods. Finally, we analyze our experimental results.
Toys and Games
# avg. actions/user
Table 1: Datasets statistics
We perform experiments on two publicly available datasets, i.e., Amazon 1 [Zhou et al., 2018] and Tmall 2 [Tang
and Wang, 2018]. Amazon is an E-commerce platform and
is widely used for product recommendation evaluation. We
adopt a sub-category: Toys and Games. For Toys and Games
dataset, we ﬁlter users who rated less than 5 items and items that are rated by less than 10 users [Kang and McAuley,
2018]. The feature set of each item contains category, brand,
description text on Toys and Games dataset.
Tmall, the
largest B2C platform in China, is a user-purchase data obtained from IJCAI 2015 competition. We remove items that
are observed by less than 30 users and eliminate users who
rated less than 15 items [Kang and McAuley, 2018]. The
characteristics of each item are category, brand, and seller on
Tmall dataset. The statistics of two datasets are summarized
in Table 1.
Evaluation Metrics and Implementation
To evaluate the performance of each model for sequential recommendation, we apply two widely used evaluation metrics,
i.e., hit ratio (Hit) and normalized discounted cumulative gain
(NDCG). Hit ratio measures the accuracy of the recommendation, and NDCG is a position-aware metric which assigns
larger weights on higher positions[Yuan et al., 2019]. In our
experiments, we choose K = {5, 10} to illustrate different
results of Hit@K and NDCG@K. Without a special mention
in this text, we ﬁx the embedding size of all models to 100
and the batch size to 10. Also, the maximum sequence length
n is set to 50 on the two datasets.
Baseline Methods
We will compare our model FDSA with following baseline
methods, which are brieﬂy described as follows.
• PopRec ranks items according to their popularity. The
most popular items are recommended to users.
• BPR [Rendle et al., 2009] is a classic method for building recommendation from implicit feedback data, which
1 
2 
proposes a pair-wise loss function to model the relative
preferences of users.
• FPMC [Rendle et al., 2010] fuses matrix factorization
and ﬁrst-order Markov Chains to capture long-term preferences and short-term item-item transitions, respectively, for next item recommendation.
• TransRec [He et al., 2017] regards users as a relational
vector acting as the junction between items.
• GRU4Rec [Hidasi et al., 2016] applies GRU to model
user click sequences for session-based recommendation.
• CSAN [Huang et al., 2018] can model multi-type
actions and multi-modal contents based on the selfattention network. Here we only consider content and
behavior in datasets.
• SASRec [Kang and McAuley, 2018] is a self-attentionbased sequential model, and it can consider consumed
items for next item recommendation.
• SASRec+ is our extension to the SASRec method,
which concatenates item vector representations and category vector representations together as the input of the
item-level self-attention network.
• SASRec++ is our extension of SASRec method, which
splices item representations and various heterogeneous
features of items together as the input of the item-level
self-attention mechanism.
• CFSA is a simpliﬁed version of our proposed method,
which only considers a category feature. It applies separated self-attention blocks on the item-level sequences
and the category-level sequences, respectively.
Performance Comparison
We compare the performance of FDSA with ten baselines regarding Hit and NDCG with cutoffs at 5 and 10. Table 2
reports their overall experimental performances on the two
datasets. We summarize the experimental analysis as follows.
Firstly, both BPR and GRU4Rec outperform PopRec on
the two datasets. This suggests the effectiveness of personalized recommendation methods. Among the baseline methods, the sequential model (e.g., FPMC and TransRec) usually
perform better than the non-sequential model (i.e., BPR) on
the two datasets. This demonstrates the importance of considering sequential information in next item recommendation.
Secondly, compared with FPMC and TransRec, SASRec
performs better performance in terms of the two metrics. This
conﬁrms the advantages of using a self-attention mechanism to model a sequence pattern. Although CSAN splices the
heterogeneous features of the item in the item representation
to help the self-attention mechanism learn the sequential patterns, the self-attention mechanism may only be able to better
model temporal order information. However, SASRec employs not only self-attention mechanism to capture long-term
preferences but also considers short-term preferences (i.e.,
last action) through a residual connection.
Thirdly, SASRec+ and SASRec++ achieve a better result
than SASRec on the Toys and Games dataset and perform
worse than SASRec on the Tmall dataset. This phenomenon
Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)
Toys and Games
Table 2: Experimental results of FDSA and baselines. The best performance of each column (the larger is the better) is in bold.
can be explained that the sequential patterns may not be stably
modeled by concatenating items’ representations and items’
feature representations together as input vectors of the selfattention mechanism. Moreover, the performance of CFSA is
better than SASRec+, and FDSA surpasses SASRec++. This
demonstrates that applying separated self-attention blocks on
item-level sequences and feature-level sequences, respectively, to capture item transition patterns and feature transition
patterns (i.e., CFSA and FDSA) is more effective than splicing item representations and its feature representations as the
input to a self-attention mechanism (i.e., SASRec+ and SAS-
Rec++). The above experiments demonstrate that modeling
item and feature transition patterns through two separate independent item-level and feature-level sequences is valuable
and meaningful for sequential recommendation.
Finally, regardless of the datasets and the evaluation metrics, our proposed FDSA achieves the best performance. Our
degenerated model CFSA consistently beats most baseline
methods. This shows the effectiveness of modeling independent category-level sequences by the self-attention network.
FDSA performs better than CFSA, indicating the effectiveness of modeling more features in feature-level sequences.
Toys and Games
Table 3: The Performance of FDSA and CFSA with varying ls and
lf in terms of NDCG@10 on two datasets.
Inﬂuence of Hyper-parameters
We investigate the inﬂuence of hyper-parameters, such as the
embedding size d, the number of heads in item-based selfattention block ls and the number of heads in feature-based
self-attention block lf. Due to space limitation, we only show
the experimental results of NDCG@10. We have obtained
similar experimental results on the Hit@10 metric.
Toys and Games
Figure 2: The performance of FDSA and CFSA under difference
choices of d.
Inﬂuence of embedding size d.
Figure 2 shows the performance of our model with different embedding sizes d on the
two datasets. As we can see from Figure 2, high dimensions
can model more information for items, but when the dimension exceeds 100, the performance of FDSA and CFSA degrade. This demonstrates that over-ﬁtting may occur when
the implicit factor dimension of the model is too high.
Inﬂuence of the number of heads ls and lf.
We conduct
experiments to study the performance of our model with varying ls and lf on the two datasets. Table 3 demonstrates the
experimental result in term of NDCG@10. We can observe
that CFSA and FDSA achieve the best performance with the
setting ls = 4, lf = 2 on the Tmall dataset, while they get
the best result with the setting ls = 2, lf = 4 on the Toys and
Games dataset. This may be because our model needs more
heads to capture the transition relationships between features
due to each item contains a descriptive text and a title in the
Toys and Games dataset, while the single data type of the
features of these items on Tmall dataset may not require too
complicated structures to model the relationships between the
Conclusion
In this paper, a novel method named Feature-level Deeper Self-Attention Network (FDSA) is proposed for sequential recommendation. FDSA modeled the transition patterns between items through an item-based self-attention block,
and it also learned the transition patterns between features
by a feature-based self-attention block. Then, the outputs of
these two blocks are integrated into a fully-connected layer
for next item prediction. Extensive experimental results have
shown that our model outperformed the state-of-the-art baseline methods.
Acknowledgments
This research was partially supported by NSFC (No.
61876117, 61876217, 61872258, 61728205), Major Project
of Zhejiang Lab (No. 2019DH0ZX01), Open Program of Key
Lab of IIP of CAS (No. IIP2019-1) and PAPD.
Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)