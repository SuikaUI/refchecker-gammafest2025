Quasi-Monte Carlo Feature Maps for Shift-Invariant Kernels ∗
Haim Avron†‡
Vikas Sindhwani†§
Jiyan Yang†¶
Michael W. Mahoney ∥
We consider the problem of improving the efﬁciency of randomized Fourier feature maps to accelerate training and testing speed of kernel methods on large datasets. These approximate feature maps arise
as Monte Carlo approximations to integral representations of shift-invariant kernel functions (e.g., Gaussian kernel). In this paper, we propose to use Quasi-Monte Carlo (QMC) approximations instead, where
the relevant integrands are evaluated on a low-discrepancy sequence of points as opposed to random
point sets as in the Monte Carlo approach. We derive a new discrepancy measure called box discrepancy
based on theoretical characterizations of the integration error with respect to a given sequence. We then
propose to learn QMC sequences adapted to our setting based on explicit box discrepancy minimization.
Our theoretical analyses are complemented with empirical results that demonstrate the effectiveness of
classical and adaptive QMC techniques for this problem.
Introduction
Kernel methods [Sch¨olkopf and Smola, 2002, Wahba, 1990, Cucker and Smale, 2001] offer a comprehensive suite of mathematically well-founded non-parametric modeling techniques for a wide range of problems
in machine learning. These include nonlinear classiﬁcation, regression, clustering, semi-supervised learning [Belkin et al., 2006], time-series analysis [Parzen, 1970], sequence modeling [Song et al., 2010], dynamical systems [Boots et al., 2013], hypothesis testing [Harchaoui et al., 2013], causal modeling [Zhang et al.,
2011] and many more.
The central object of kernel methods is a kernel function k : X × X →R deﬁned on an input domain
X ⊂Rd 1. The kernel k is (non-uniquely) associated with an embedding of the input space into a highdimensional Hilbert space H (with inner product ⟨·, ·⟩H) via a feature map, Ψ : X →H, such that
k(x, z) = ⟨Ψ(x), Ψ(z)⟩H .
Standard regularized linear statistical models in H then provide non-linear inference with respect to the
original input representation. The algorithmic basis of such constructions are classical Representer Theorems [Wahba, 1990, Sch¨olkopf and Smola, 2002] that guarantee ﬁnite-dimensional solutions of associated
optimization problems, even if H is inﬁnite-dimensional.
∗A short version of this paper has been presented in ICML 2014.
†Equal contributors
‡Mathematical Sciences & Analytics, IBM T.J. Watson Research Center, Yorktown Heights, NY 10598, USA. Email:
 
§Google Research, New York, NY 10011, USA. Email: 
¶Institute for Computational and Mathematical Engineering, Stanford University, Stanford, CA 94305, USA. Email:
 
∥International Computer Science Institute and Department of Statistics, University of California at Berkeley, Berkeley, CA
94720, USA. Email: 
1In fact, X can be a rather general set. However, in this paper it is restricted to being a subset of Rd.
However, there is a steep price of these elegant generalizations in terms of scalability. Consider, for example, least squares regression given n data points {(xi, yi)}n
i=1 and assume that n ≫d. The complexity of
linear regression training using standard least squares solvers is O(nd2), with O(nd) memory requirements,
and O(d) prediction speed on a test point. Its kernel-based nonlinear counterpart, however, requires solving
a linear system involving the Gram matrix of the kernel function (deﬁned by Kij = k(xi, xj)). In general,
this incurs O(n3 + n2d) complexity for training, O(n2) memory requirements, and O(nd) prediction time
for a single test point – none of which are particularly appealing in “big data” settings. Similar conclusions
apply to other algorithms such as Kernel PCA.
This is rather unfortunate, since non-parametric models, such as the ones produced by kernel methods,
are particularly appealing in a big data settings as they can adapt to the full complexity of the underlying domain, as uncovered by increasing dataset sizes. It is well-known that imposing strong structural constraints
upfront for the purpose of allowing an efﬁcient solution (in the above example: a linear hypothesis space)
often limits, both theoretically and empirically, the potential to deliver value on large amounts of data. Thus,
as big data becomes pervasive across a number of application domains, it has become necessary to be able
to develop highly scalable algorithms for kernel methods.
Recent years have seen intensive research on improving the scalability of kernel methods; we review
some recent progress in the next section. In this paper, we revisit one of the most successful techniques,
namely the randomized construction of a family of low-dimensional approximate feature maps proposed
by Rahimi and Recht . These randomized feature maps, ˆΨ : X →Cs, provide low-distortion approximations for (complex-valued) kernel functions k : X × X →C:
k(x, z) ≈⟨ˆΨ(x), ˆΨ(z)⟩Cs
where Cs denotes the space of s-dimensional complex numbers with the inner product, ⟨α, β⟩Cs = Ps
with z∗denoting the conjugate of the complex number z (though Rahimi and Recht also deﬁne realvalued feature maps for real-valued kernels, our technical exposition is simpliﬁed by adopting the generality
of complex-valued features). The mapping ˆΨ(·) is now applied to each of the data points, to obtain a randomized feature representation of the data. We then apply a simple linear method to these random features.
That is, if our data is {(xi, yi)}n
i=1 we learn on {(zi, yi)}n
i=1 where zi = ˆΨ(xi). As long as s is sufﬁciently
smaller than n, this leads to more scalable solutions, e.g., for regression we get back to O(ns2) training
and O(sd) prediction time, with O(ns) memory requirements. This technique is immensely successful, and
has been used in recent years to obtain state-of-the-art accuracies for some classical datasets [Huang et al.,
2014, Dai et al., 2014, Sindhwani and Avron, 2014, Lu et al., 2014].
The starting point of Rahimi and Recht is a celebrated result that characterizes the class of positive deﬁnite functions:
Deﬁnition 1. A function g : Rd 7→C is a positive deﬁnite function if for any set of m points, x1 . . . xm ∈Rd,
the m × m matrix A deﬁned by Aij = g(xi −xj) is positive semi-deﬁnite.
Theorem 2 (Bochner ). A complex-valued function g : Rd 7→C is positive deﬁnite if and only if it is
the Fourier Transform of a ﬁnite non-negative Borel measure µ on Rd, i.e.,
g(x) = ˆµ(x) =
Rd e−ixT wdµ(w),
Without loss of generality, we assume henceforth that µ(·) is a probability measure with associated
probability density function p(·).
A kernel function k : Rd × Rd 7→C on Rd is called shift-invariant if k(x, z) = g(x −z), for some
positive deﬁnite function g : Rd 7→C. Bochner’s theorem implies that a scaled shift-invariant kernel can
therefore be put into one-to-one correspondence with a density p(·) such that,
k(x, z) = g(x −z) =
Rd e−i(x−z)T wp(w)dw .
For the most notable member of the shift-invariant family of kernels – the Gaussian kernel:
k(x, z) = e−
the associated density is again Gaussian N(0, σ−2Id).
The integral representation of the kernel (2) may be approximated as follows:
Rd e−i(x−z)T wp(w)dw
e−i(x−z)T ws
⟨ˆΨS(x), ˆΨS(z)⟩Cs ,
through the feature map,
ˆΨS(x) = 1
e−ixT w1 . . . e−ixT wsi
The subscript S denotes dependence of the feature map on the sequence S = {w1, . . . , ws}.
The goal of this work is to improve the convergence behavior of this approximation, so that a smaller
s can be used to get the same quality of approximation to the kernel function. This is motivated by recent
work that showed that in order to obtain state-of-the-art accuracy on some important datasets, a very large
number of random features is needed [Huang et al., 2014, Sindhwani and Avron, 2014].
Our point of departure from the work of Rahimi and Recht is the simple observation that when
w1, . . . , ws are are drawn from the distribution deﬁned by the density function p(·), the approximation
in (3) may be viewed as a standard Monte Carlo (MC) approximation to the integral representation of the
kernel. Instead of using plain MC approximation, we propose to use the low-discrepancy properties of
Quasi-Monte Carlo (QMC) sequences to reduce the integration error in approximations of the form (3).
A self-contained overview of Quasi-Monte Carlo techniques for high-dimensional integration problems is
provided in Section 2. In Section 3, we describe how QMC techniques apply to our setting.
We then proceed to apply an average case theoretical analysis of the integration error for any given sequence S (Section 4). This bound motivates an optimization problem over the sequence S whose minimizer
provides adaptive QMC sequences ﬁne tuned to our kernels (Section 5).
Finally, empirical results (Section 6) clearly demonstrate the superiority of QMC techniques over the
MC feature maps [Rahimi and Recht, 2008], the correctness of our theoretical analysis and the potential
value of adaptive QMC techniques for large-scale kernel methods.
Preliminaries
We use i both for subscript and for denoting √−1, relying on the context to distinguish between the two. We
use y, z, . . . to denote scalars. We use w, t, x . . . to denote vectors, and use wi to denote the i-th coordinate
of vectors w. Furthermore, in a sequence of vectors, we use wi to denote the i-th element of the sequence
and use wij to denote the j-th coordinate of vector wi. Given x1, . . . , xn, the Gram matrix is deﬁned
as K ∈Rn×n where Kij = k(xi, xj) for i, j = 1, . . . , n. We denote the error function by erf(·), i.e.,
0 e−z2dz for z ∈C; see Weideman and Mori for more details.
In “MC sequence” we mean points drawn randomly either from the unit cube or certain distribution that
will be clear from the text. For “QMC sequence” we mean a deterministic sequence designed to reduce the
integration error. Typically, it will be a low-discrepancy sequence on the unit cube.
It is also useful to recall the deﬁnition of Reproducing Kernel Hilbert Space (RKHS).
Deﬁnition 3 . A reproducing kernel Hilbert space (RKHS) is a Hilbert Space H : X →C that possesses a reproducing kernel, i.e., a function
h : X × X →C for which the following hold for all x ∈X and f ∈H:
• h(x, ·) ∈H
• ⟨f, h(x, ·)⟩H = f(x) (Reproducing Property)
Equivalently, RKHSs are Hilbert spaces with bounded, continuous evaluation functionals. Informally,
they are Hilbert spaces with the nice property that if two functions f, g ∈H are close in the sense of the distance derived from the norm in H (i.e., ∥f −g∥H is small), then their values f(x), g(x) are also close for all
x ∈X; in other words, the norm controls the pointwise behavior of functions in H [Berlinet and Thomas-Agnan,
Related Work
In this section we discuss related work on scalable kernel methods. Relevant work on QMC methods is
discussed in the next subsection.
Scalability has long been identiﬁed as a key challenge associated with deploying kernel methods in
practice. One dominant line of work constructs low-rank approximations of the Gram matrix, either using
data-oblivious randomized feature maps to approximate the kernel function, or using sampling techniques
such as the classical Nystr¨om method [Williams and Seeger, 2001]. In its vanilla version, the latter approach - Nystr¨om method - samples points from the dataset, computes the columns of the Gram matrix
that corresponds to the sampled data points, and uses this partial computation of the Gram matrix to construct an approximation to the entire Gram matrix. More elaborate techniques exist, both randomized and
deterministic; see Gittens and Mahoney for a thorough treatment.
More relevant to our work is the randomized feature mapping approach. Pioneered by the seminal
paper of Rahimi and Recht , the core idea is to construct, for a given kernel on a data domain X, a
transformation ˆΨ : X →Cs such that k(x, z) ≈⟨ˆΨ(x), ˆΨ(z)⟩Cs. Invoking Bochner’s theorem, a classical
result in harmonic analysis, Rahimi and Recht show how to construct a randomized feature map for shiftinvariant kernels, i.e., kernels that can be written k(x, z) = g(x −y) for some positive deﬁnite function
Subsequently, there has been considerable effort given to extending this technique to other classes of
kernels. Li et al. use Bochner’s theorem to provide random features to the wider class of groupinvariant kernels. Maji and Berg suggested random features for the intersection kernel k(x, z) =
i=1 min(xi, zi). Vedaldi and Zisserman developed feature maps for γ-homogeneous kernels. Sreekanth et al.
 developed feature maps for generalized RBF kernels k(x, z) = g(D(x, z)2) where g(·) is a positive deﬁnite function, and D(·, ·) is a distance metric. Kar and Karnick suggested feature maps for
dot-product kernels. The feature maps are based on the Maclaurin expansion, which is guaranteed to be nonnegative due to a classical result of Schoenberg . Pham and Pagh suggested feature maps for
the polynomial kernels. Their construction leverages known techniques from sketching theory. It can also
be shown that their feature map is an oblivious subspace embedding, and this observation provides stronger
theoretical guarantees than point-wise error bounds prevalent in the feature map literature [Avron et al.,
2014]. By invoking a variant of Bochner’s theorem that replaces the Fourier transform with the Laplace
transform, Yang et al. obtained randomized feature maps for semigroup kernels on histograms. We
note that while the original feature maps suggested by Rahimi and Recht were randomized, some of the
aforementioned maps are deterministic.
Our work is more in-line with recent efforts on scaling up the random features, so that learning and
prediction can be done faster. Le et al. return to the original construction of Rahimi and Recht ,
and devise a clever distribution of random samples w1, w2, . . . , ws that is structured so that the generation
of random features can be done much faster. They showed that only a very limited concession in term of
convergence rate is made. Hamid et al. , working on the polynomial kernel, suggest ﬁrst generating
a very large amount of random features, and then applying them a low-distortion embedding based the Fast
Johnson-Lindenstruass Transform, so the make the ﬁnal size of the mapped vector rather small. In contrast,
our work tries to design w1, . . . , ws so that less features will be necessary to get the same quality of kernel
approximation.
Several other scalable approaches for large-scale kernel methods have been suggested over the years,
starting from approaches such as chunking and decomposition methods proposed in the early days of SVM
optimization literature. Raykar and Duraiswami use an improved fast Gauss transform for large scale
Gaussian Process regression. There are also approaches that are more speciﬁc to the objective function at
hand, e.g., Keerthi et al. builds a kernel expansion greedily to optimize the SVM objective function.
Another well known approach is the Core Vector Machines [Tsang et al., 2005] which draws on approximation algorithms from computational geometry to scale up a class of kernel methods that can be reformulated
in terms of the minimum enclosing ball problem.
For a broader discussion of these methods, and others, see Bottou et al. .
Quasi-Monte Carlo Techniques: an Overview
In this section we provide a self-contained overview of Quasi-Monte Carlo (QMC) techniques. For brevity,
we restrict our discussion to background that is necessary for understanding subsequent sections. We refer the interested reader to the excellent reviews by Caﬂisch and Dick et al. , and the recent
book Leobacher and Pillichschammer for a much more detailed exposition.
Consider the task of computing an approximation of the following integral
 d f(x)dx .
One can observe that if x is a random vector uniformly distributed over d then Id[f] = E [f(x)].
An empirical approximation to the expected value can be computed by drawing a random point set S =
{w1, . . . , ws} independently from d, and computing:
This is the Monte Carlo (MC) method.
Deﬁne the integration error with respect to the point set S as,
ǫS[f] = |Id(f) −IS(f)| .
When S is drawn randomly, the Central Limit Theorem asserts that if s = |S| is large enough then ǫS[f] ≈
σ[f]s−1/2ν where ν is a standard normal random variable, and σ[f] is the square-root of the variance of f;
Figure 1: Comparison of MC and QMC sequences.
 d (f(x) −Id(f))2 dx .
In other words, the root mean square error of the Monte Carlo method is,
ǫS[f]21/2 ≈σ[f]s−1/2.
Therefore, the Monte Carlo method converges at a rate of O(s−1/2).
The aim of QMC methods is to improve the convergence rate by using a deterministic low-discrepancy
sequence to construct S, instead of randomly sampling points. The underlying intuition is illustrated in
Figure 1, where we plot a set of 1000 two-dimensional random points (left graph), and a set of 1000 twodimensional points from a quasi-random sequence (Halton sequence; right graph). In the random sequence
we see that there is an undesired clustering of points, and as a consequence empty spaces. Clusters add
little to the approximation of the integral in those regions, while in the empty spaces the integrand is not
sampled. This lack of uniformity is due to the fact that Monte Carlo samples are independent of each other.
By carefully designing a sequence of correlated points to avoid such clustering effects, QMC attempts to
avoid this phenomena, and thus provide faster convergence to the integral.
The theoretical apparatus for designing such sequences are inequalities of the form
ǫS(f) ≤D(S)V (f) ,
in which V (f) is a measure of the variation or difﬁculty of integrating f(·) and D(S) is a sequencedependent term that typically measures the discrepancy, or degree of deviation from uniformity, of the
sequence S. For example, the expected Monte Carlo integration error decouples into a variance term, and
s−1/2 as in (5).
A prototypical inequality of this sort is the following remarkable and classical result:
Theorem 4 (Koksma-Hlawka inequality). For any function f with bounded variation, and sequence S =
{w1, . . . , ws}, the integration error is bounded above as follows,
ǫS[f] ≤D⋆(S)VHK[f] ,
where VHK is the Hardy-Krause variation of f (see Niederreiter ), which is deﬁned in terms of the
following partial derivatives,
I⊂[d],I̸=∅
uj=1,j /∈I
and D⋆is the star discrepancy deﬁned by
x∈ d|disrS(x)| ,
where disrS is the local discrepancy function
disrS(x) = Vol(Jx) −|{i : wi ∈Jx}|
with Jx = [0, x1) × [0, x2) × · · · × [0, xd) with Vol(Jx) = Qd
Given x, the second term in disrS(x) is an estimate of the volume of Jx, which will be accurate if the
points in S are uniform enough. D⋆(S) measures the maximum difference between the actual volume of
the subregion Jx and its estimate for all x in d.
An inﬁnite sequence w1, w2, . . . is deﬁned to be a low-discrepancy sequence if, as a function of s,
D⋆({w1, . . . , ws}) = O((log s)d/s). Several constructions are know to be low-discrepancy sequences.
One notable example is the Halton sequences, which are deﬁned as follows. Let p1, . . . , pd be the ﬁrst d
prime numbers. The Halton sequence w1, w2, . . . of dimension d is deﬁned by
wi = (φp1(i), . . . , φpd(i))
where for integers i ≥0 and b ≥2 we have
in which i0, i1, · · · ∈{0, 1, . . . , b −1} is given by the unique decomposition
It is outside the scope of this paper to describe all these constructions in detail. However we mention
that in addition to the Halton sequences, other notable members are Sobol’ sequences, Faure sequences,
Niederreiter sequences, and more (see Dick et al. , Section 2). We also mention that it is conjectured
that the O((log s)d/s) rate for star discrepancy decay is optimal.
The classical QMC theory, which is based on the Koksma-Hlawka inequality and low discrepancy
sequences, thus achieves a convergence rate of O((log s)d/s). While this is asymptotically superior to
O(s−1/2) for a ﬁxed d, it requires s to be exponential in d for the improvement to manifest. As such, in the
past QMC methods were dismissed as unsuitable for very high-dimensional integration.
However, several authors noticed that QMC methods perform better than MC even for very highdimensional integration [Sloan and Wozniakowski, 1998, Dick et al., 2013].2 Contemporary QMC literature
explains and expands on these empirical observations, by leveraging the structure of the space in which the
2Also see: “On the unreasonable effectiveness of QMC”, I.H. Sloan 
Algorithm 1 Quasi-Random Fourier Features
Input: Shift-invariant kernel k, size s.
Output: Feature map ˆΨ(x) : Rd 7→Cs.
1: Find p, the inverse Fourier transform of k.
2: Generate a low discrepancy sequence t1, . . . , ts.
3: Transform the sequence: wi = Φ−1(ti) by (7).
4: Set ˆΨ(x) =
e−ixT w1, . . . , e−ixT ws
integrand function lives, to derive more reﬁned bounds and discrepancy measures, even when classical measures of variation such as (6) are unbounded. This literature has evolved along at least two directions: one,
where worst-case analysis is provided under the assumption that the integrands live in a Reproducing Kernel
Hilbert Space (RKHS) of sufﬁciently smooth and well-behaved functions (see Dick et al. , Section 3)
and second, where the analysis is done in terms of average-case error, under an assumed probability distribution over the integrands, instead of worst-case error [Wozniakowski, 1991, Traub and Wozniakowski,
1994]. We refrain from more details, as these are essentially the paths that the analysis in Section 4 follows
for our speciﬁc setting.
QMC Feature Maps: Our Algorithm
We assume that the density function in (2) can be written as p(x) = Qd
j=1 pj(xj), where pj(·) is a univariate
density function. The density functions associated to many shift-invariant kernels, e.g., Gaussian, Laplacian
and Cauchy, admits such a form.
The QMC method is generally applicable to integrals over a unit cube. So typically integrals of the
form (2) are handled by ﬁrst generating a low discrepancy sequence t1, . . . , ts ∈ d, and transforming it
into a sequence w1, . . . , ws in Rd, instead of drawing the elements of the sequence from p(·) as in the MC
To convert (2) to an integral over the unit cube, a simple change of variables sufﬁces. For t ∈Rd, deﬁne
1 (t1), . . . , Φ−1
where Φj(·) is the cumulative distribution function (CDF) of pj(·), for j = 1, . . . , d. By setting w =
Φ−1(t), then (2) can be equivalently written as
Rd e−i(x−z)T wp(w)dw =
 d e−i(x−z)T Φ−1(t)dt .
Thus, a low discrepancy sequence t1, . . . , ts ∈ d can be transformed using wi = Φ−1(ti), which is
then plugged into (3) to yield the QMC feature map. This simple procedure is summarized in Algorithm 1.
QMC feature maps are analyzed in the next section.
Theoretical Analysis and Average Case Error Bounds
The proofs for assertions made in this section and the next can be found in the Appendix.
The goal of this section is to develop a framework for analyzing the approximation quality of the QMC
feature maps described in the previous section (Algorithm 1). We need to develop such a framework since
the classical Koksma-Hlawka inequality cannot be applied to our setting, as the following proposition shows:
Proposition 5. For any p(x) = Qd
j=1 pj(xj), where pj(·) is a univariate density function, let
1 (t1), . . . , Φ−1
For a ﬁxed u ∈Rd, consider fu(t) = e−iuT Φ−1(t), t ∈ d. The Hardy-Krause variation of fu(·) is
unbounded. That is, one of the integrals in the sum (6) is unbounded.
Our framework is based on a new discrepancy measure, box discrepancy, that characterizes integration
error for the set of integrals deﬁned with respect to the underlying data domain. Throughout this section we
use the convention that S = {w1, . . . , ws}, and the notation ¯
X = {x −z | x, z ∈X}.
Given a probability density function p(·) and S, we deﬁne the integration error ǫS,p[f] of a function f(·)
with respect to p(·) and the s samples as,
Rd f(x)p(x)dx −1
We are interested in characterizing the behavior of ǫS,p[f] on f ∈F ¯
fu(x) = e−iuT x, u ∈¯
As is common in modern QMC analysis [Leobacher and Pillichschammer, 2014, Dick et al., 2013], our
analysis is based on setting up a Reproducing Kernel Hilbert Space of “nice” functions that is related to
integrands that we are interested in, and using properties of the RKHS to derive bounds on the integration
error. In particular, the integration error of integrands in an RKHS can be bounded using the following
proposition.
Proposition 6 (Integration Error in an RKHS). Let H be an RKHS with kernel h(·, ·). Assume that κ =
supx∈Rd h(x, x) < ∞. Then, for all f ∈H we have,
ǫS,p[f] ≤∥f∥HDh,p(S) ,
Rd h(ω, ·)p(ω)dω −1
Rd h(ω, φ)p(ω)p(φ)dωdφ −2
Rd h(wl, ω)p(ω)dω
h(wl, wj) .
Remark 7. In the theory of RKHS embeddings of probability distributions [Smola et al., 2007, Sriperumbudur et al.,
2010], the function
Rd h(ω, x)p(ω)dω
is known as the kernel mean embedding of p(·). The function
ˆµh,p,S(x) = 1
is then the empirical mean map.
The RKHS we use is as follows. For a vector b ∈Rd, let us deﬁne □b = {u ∈Rd | |uj| ≤bj}. Let
fu(x) = e−iuT x, u ∈□b
and consider the space of functions that admit an integral representation over F□b of the form
ˆf(u)e−iuT xdu where ˆf(u) ∈L2(□b) .
This space is associated with bandlimited functions, i.e., functions with compactly-supported inverse Fourier
transforms, which are of fundamental importance in the Shannon-Nyquist sampling theory. Under a natural
choice of inner product, these spaces are called Paley-Wiener spaces and they constitute an RKHS.
Proposition 8 . By PWb, denote the space of functions which admit the representation in (10), with the inner
product ⟨f, g⟩P Wb = (2π)2d⟨ˆf, ˆg⟩L2(□b). PWb is an RKHS with kernel function,
sincb(u, v) = π−d
sin (bj(uj −vj))
For notational convenience, in the above we deﬁne sin(b · 0)/0 to be b.
Furthermore, ⟨f, g⟩P Wb =
⟨f, g⟩L2(□b).
If bj = supu∈¯
X |uj| then ¯
X ⊂□b, so F ¯
X ⊂F□b. Since we wish to bound the integration error
on functions in F ¯
X , it sufﬁces to bound the integration error on F□b. Unfortunately, while F□b deﬁnes
PWb, the functions in it, being not square integrable, are not members of PWb, so analyzing the integration error in PWb do not directly apply to them. However, damped approximations of fu(·) of the form
˜fu(x) = e−iuT x sinc(Tx) are members of PWb with ∥˜f∥P Wb =
T . Hence, we expect the analysis of the
integration error in PWb to provide provide a discrepancy measure for integrating functions in F□b.
For PWb the discrepancy measure Dh,S in Proposition 6 can be written explicitly.
Theorem 9 (Discrepancy in PWb). Suppose that p(·) is a probability density function, and that we can
write p(x) = Qd
j=1 pj(xj) where each pj(·) is a univariate probability density function as well. Let ϕj(·)
be the characteristic function associated with pj(·). Then,
Dsincb,p(S)2
|ϕj(β)|2dβ −
ϕj(β)eiwljβdβ +
sincb(wl, wj) .
This naturally leads to the deﬁnition of the box discrepancy, analogous to the star discrepancy described
in Theorem 4.
Deﬁnition 10 (Box Discrepancy). The box discrepancy of a sequence S with respect to p(·) is deﬁned as,
p (S) = Dsincb,p(S) .
For notational convenience, we generally omit the b from D□b
p (S) as long as it is clear from the context.
The worse-case integration error bound for Paley-Wiener spaces is stated in the following as a corollary
of Proposition 6. As explained earlier, this result not yet apply to functions in F□b because these functions
are not part of PWb. Nevertheless, we state it here for completeness.
Corollary 11 (Integration Error in PWb). For f ∈PWb we have
ǫS,p[f] ≤∥f∥P WbD□
Our main result shows that the expected square error of an integrand drawn from a uniform distribution
over F□b is proportional to the square discrepancy measure D□
p (S). This result is in the spirit of similar
average case analysis in the QMC literature [Wozniakowski, 1991, Traub and Wozniakowski, 1994].
Theorem 12 (Average Case Error). Let U(F□b) denote the uniform distribution on F□b. That is, f ∼
U(F□b) denotes f = fu where fu(x) = e−iuT x and u is randomly drawn from a uniform distribution on
□b. We have,
We now give an explicit formula for D□
p (S) for the case that p(·) is the density function of the multivariate Gaussian distribution with zero mean and independent components. This is an important special case
since this is the density function that is relevant for the Gaussian kernel.
Corollary 13 (Discrepancy for Gaussian Distribution). Let p(·) be the d-dimensional multivariate Gaussian
density function with zero mean and covariance matrix equal to diag(σ−2
1 , . . . , σ−2
d ). We have,
sincb(wl, wj) −
Intuitively, the box discrepancy of the Gaussian kernel can be interpreted as follows. The function
sinc(x) = sin(x)/x achieves its maximum at x = 0 and minimizes at discrete values of x decaying to 0 as
|x| goes to ∞. Hence the ﬁrst term in (12) tends to be minimized when the pairwise distance between wj
are sufﬁciently separated. Due to the shape of cumulative distribution function of Gaussian distribution, the
values of tj = Φ(wj) (j = 1, . . . , s) are driven to be close to the boundary of the unit cube. As for second
term, the original expression is -2
Rd h(wl, ω)p(ω)dω. This term encourages the sequence {wl} to
mimic samples from p(ω). Since p(ω) concentrates its mass around ω = 0, the wj also concentrates around
0 to maximize the integral and therefore the values of tj = Φ(wj) (j = 1, . . . , s) are driven closer to the
center of the unit cube. Sequences with low box discrepancy therefore optimize a tradeoff between these
competing terms.
Two other shift-invariant kernel that have been mentioned in the machine learning literature is the Laplacian kernel [Rahimi and Recht, 2008] and Matern kernel [Le et al., 2013]. The distribution associated with
the Laplacian kernel can be written as a product p(x) = Qd
j=1 pj(xj), where pj(·) is density associated
with the Cauchy distribution. The characteristic function is simple (φj(β) = e−|β|/σj) so analytic formulas
like (12) can be derived. The distribution associated with the Matern kernel, on the other hand, is the multivariate t-distribution, which cannot be written as a product p(x) = Qd
j=1 pj(xj), so the presented theory
does not apply to it.
Discrepancy of Monte-Carlo Sequences.
We now derive an expression for the expected discrepancy of Monte-Carlo sequences, and show that it
decays as O(s−1/2). This is useful since via an averaging argument we are guaranteed that there exists sets
for which the discrepancy behaves O(s−1/2).
Corollary 14. Suppose t1, . . . , ts are chosen uniformly from d. Let wi = Φ−1(ti), for i = 1, . . . , s.
Assume that κ = supx∈Rd h(x, x) < ∞. Then
Rd h(ω, ω)p(ω)dω −1
Rd h(ω, φ)p(ω)p(φ)dωdφ .
Again, we can derive speciﬁc formulas for the Gaussian density. The following is straightforward from
Corollary 14. We omit the proof.
Corollary 15. Let p(·) be the d-dimensional multivariate Gaussian density function with zero mean and
covariance matrix equal to diag(σ−2
1 , . . . , σ−2
d ). Suppose t1, . . . , ts are chosen uniformly from d. Let
wi = Φ−1(ti), for i = 1, . . . , s. Then,
Learning Adaptive QMC Sequences
For simplicity, in this section we assume that p(·) is the density function of Gaussian distribution with zero
mean. We also omit the subscript p from D□
p . Similar analysis and equations can be derived for other density
functions.
Error characterization via discrepancy measures like (12) is typically used in the QMC literature to prescribe sequences whose discrepancy behaves favorably. It is clear that for the box discrepancy, a meticulous
design is needed for a high quality sequence and we leave this to future work. Instead, in this work, we use
the fact that unlike the star discrepancy (4), the box discrepancy is a smooth function of the sequence with a
closed-form formula. This allows us to both evaluate various candidate sequences, and select the one with
the lowest discrepancy, as well as to adaptively learn a QMC sequence that is specialized for our problem
setting via numerical optimization. The basis is the following proposition, which gives an expression for the
gradient of D□(S).
Proposition 16 (Gradient of Box Discrepancy). Deﬁne the following scalar functions and variables,
π sinc′(bz) ;
, j = 1, . . . , d ;
j sin(bjx) .
In the above ,we deﬁne sinc′(0) to be 0. Then, the elements of the gradient vector of D□are given by,
bj(wlj, wmj)
sincbq(wlq, wmq)
We explore two possible approaches for ﬁnding sequences based on optimizing the box discrepancy,
namely global optimization and greedy optimization.
The latter is closely connected to herding algorithms [Welling, 2009].
Global Adaptive Sequences.
The task is posed in terms minimization of the box discrepancy function (12) over the space of sequences
of s vectors in Rd:
S∗= arg minS=(w1...ws)∈Rds D□(S) .
The gradient can be plugged into any ﬁrst order numerical solver for non-convex optimization. We use
non-linear conjugate gradient in our experiments (Section 6.2).
The above learning mechanism can be extended in various directions. For example, QMC sequences
for n-point rank-one Lattice Rules [Dick et al., 2013] are integral fractions of a lattice deﬁned by a single
generating vector v. This generating vector may be learnt via local minimization of the box discrepancy.
Greedy Adaptive Sequences.
Starting with S0 = ∅, for t ≥1, let St = {w1, . . . , wt}. At step t+1, we solve the following optimization
wt+1 = arg minw∈Rd D□(St ∪{w}) .
Set St+1 = St ∪{wt+1} and repeat the above procedure. The gradient of the above objective is also given
in (14). Again, we use non-linear conjugate gradient in our experiments (Section 6.2).
The greedy adaptive procedure is closely related to the herding algorithm, recently presented by Welling
 . Applying the herding algorithm to PWb and p(·), and using our notation, the points w1, w2, . . .
are generated using the following iteration
w∈Rd⟨zt(·), h(w, ·)⟩P Wb
zt(x) + µh,p(x) −h(w, x) .
In the above, z0, z1, . . . is a series of functions in PWb. The literature is not speciﬁc on the initial value of
z0, with both z0 = 0 and z0 = µh,p suggested. Either way, it is always the case that zt = z0 + t(µh,p −
ˆµh,p,St) where St = {w1, . . . , wt}.
Chen et al. showed that under some additional assumptions, the herding algorithm, when applied
to a RKHS H, greedily minimizes ∥µh,p −ˆµh,p,St∥2
H, which, recall, is equal to Dh,p(St). Thus, under
certain assumptions, herding and (15) are equivalent. Chen et al. also showed that under certain
restrictions on the RKHS, herding will reduce the discrepancy in a ratio of O(1/t). However, it is unclear
whether those restrictions hold for PWb and p(·). Indeed, Bach et al. recently shown that these
restrictions never hold for inﬁnite-dimensional RKHS, as long as the domain is compact. This result does
not immediately apply to our case since Rd is not compact.
Weighted Sequences.
Classically, Monte-Carlo and Quasi-Monte Carlo approximations of integrals are unweighted, or more
precisely, have a uniform weights. However, it is quite plausible to weight the approximations, i.e. approximate Id[f] =
 d f(x)dx using
where Ξ = {ξ1, . . . , ξs} ⊂R is a set of weights. This lead to the feature map
ξ1e−ixT w1 . . .
ξse−ixT wsi
This construction requires ξi ≥0 for i = 1, . . . , s, although we note that (16) itself does not preclude
negative weights. We do not require the weights to be normalized, that is it is possible that Ps
i=1 ξi ̸= 1.
One can easily generalize the result of the previous section to derive the following discrepancy measure
that takes into consideration the weights
|ϕj(β)|2dβ −
ϕj(β)eiwljβdβ +
ξlξj sincb(wl, wj) .
Using this discrepancy measure, global adaptive and greedy adaptive sequences of points and weights can
However, we note that if we ﬁx the points, then optimizing just the weights is a simple convex optimization problem. The box discrepancy can be written as
p (S, Ξ)2 = π−d
|ϕj(β)|2dβ −2vT ξ + ξT Hξ ,
where ξ ∈Rs has entry i equal to ξi, v ∈Rs and H ∈Rs×s are deﬁned by
Hij = sincb(wl, wj)
vi = (2π)−d
ϕj(β)eiwljβdβ .
The ξ that minimizes D□b
p (S, Ξ)2 is equal to H−1v, but there is no guarantee that ξi ≥0 for all i. We
need to explicitly impose these conditions. Thus, the optimal weights can be found by solving the following
convex optimization problem
Ξ∗= arg minξ∈Rs ξT Hξ −2vT ξ
s.t. ξ ≥0 .
Selecting the weights in such a way is closely connected to the so-called Bayesian Monte Carlo (BMC)
method, originally suggested by Ghahramani and Rasmussen . In BMC, a Bayesian approach is
utilized in which the function is a assumed to be random with a prior that is a Gaussian Process. Combining with the observations, a posterior is obtained, which naturally leads to the selection of weights.
Husz´ar and Duvenaud subsequently pointed out the connection between this approach and the herding algorithm discussed earlier.
We remark that as long as all the weights are positive, the hypothesis space of functions induced by the
feature map (that is, {gw(x) = ˆΨT
S(x)w, w ∈Rs}) will not change in terms of the set of functions in it.
However, the norms will be affected (that is, the norm of a function in that set also depends on the weights),
which in turn affects the regularization.
Experiments
In this section we report experiments with both classical QMC sequences and adaptive sequences learnt
from box discrepancy minimization.
Experiments With Classical QMC Sequences
We examine the behavior of classical low-discrepancy sequences when compared to random Fourier features
(i.e., MC). We consider four sequences: Halton, Sobol’, Lattice Rules, and Digital Nets. For Halton and
Sobol’, we use the implementation available in MATLAB.3 For Lattice Rules and Digital Nets, we use publicly available implementations.4 For all four low-discrepancy sequences, we use scrambling and shifting
techniques recommended in the QMC literature (see Dick et al. for details). For Sobol’, Lattice Rules
and Digital Nets, scrambling introduces randomization and hence variance. For Halton sequence, scrambling is deterministic, and there is no variance. The generation of these sequences is extremely fast, and
quite negligible when compared to the time for any reasonable downstream use. For example, for census
dataset with size 18,000 by 119, if we choose the number of random features s = 2000, the running time
for performing kernel ridge regression model is more than 2 minutes, while the time of generating the QMC
sequences is only around 0.2 seconds (Digital Nets sequence takes longer, but not much longer) and that of
MC sequence is around 0.01 seconds. Therefore, we do not report running times as these are essentially the
same across methods.
In all experiments, we work with a Gaussian kernel. For learning, we use regularized least square
classiﬁcation on the feature mapped dataset, which can be thought of as a form of approximate kernel ridge
regression. For each dataset, we performed 5-fold cross-validation when using random Fourier features (MC
sequence) to set the bandwidth σ, and then used the same σ for all other sequences.
3 
4 dirk.nuyens/qmc-generators/
Number of features
Relative error on ||K||
Euclidean norm
Digital Net
Number of features
Frobenius norm
Digital Net
(a) USPST, n = 1506
Number of features
Relative error on ||K||
Euclidean norm
Digital Net
Number of features
Frobenius norm
Digital Net
(b) cpu, n = 6554
Number of features
Relative error on ||K||
Euclidean norm
Digital Net
Number of features
Frobenius norm
Digital Net
(c) census, n = 5000
Number of features
Relative error on ||K||
Euclidean norm
Digital Net
Number of features
Frobenius norm
Digital Net
(d) MNIST, n = 5000
Figure 2: Relative error on approximating the Gram matrix measured in Euclidean norm and Frobenius
norm, i.e., ∥K −˜K∥2/∥K∥2 and ∥K −˜K∥F /∥K∥F , for various s. For each kind of random feature and s,
10 independent trials are executed, and the mean and standard deviation are plotted.
Quality of Kernel Approximation
In our setting, the most natural and fundamental metric for comparison is the quality of approximation of
the Gram matrix. We examine how close ˜K (deﬁned by ˜Kij = ˜k(xi, xj) where ˜k(·, ·) = ⟨ˆΨS(·), ˆΨS(·)⟩is
the kernel approximation) is to the Gram matrix K of the exact kernel.
We examine four datasets: cpu (6554 examples, 21 dimensions), census (a subset chosen randomly
with 5,000 examples, 119 dimensions), USPST (1,506 examples, 250 dimensions after PCA) and MNIST (a
subset chosen randomly with 5,000 examples, 250 dimensions after PCA). The reason we do subsampling
on large datasets is to be able to compute the full exact Gram matrix for comparison purposes. The reason
we use dimensionality reduction on MNIST is that the maximum dimension supported by the Lattice Rules
implementation we use is 250.
To measure the quality of approximation we use both ∥K −˜K∥2/∥K∥2 and ∥K −˜K∥F /∥K∥F . The
plots are shown in Figure 2.
We can clearly see that except Sobol’ sequences classical low-discrepancy sequences consistently produce better approximations to the Gram matrix than the approximations produced using MC sequences.
Among the four classical QMC sequences, the Digital Nets, Lattice Rules and Halton sequences yield much
Number of features
Number of features
Number of features
Digital Net
(c) CENSUS
Number of features
Digital Net
Figure 3: Ratio between relative errors on approximating the Gram matrix using both the scrambled and
non-scambled version of the same QMC sequence for various s. The lower the ratio value is, the more
accurate the scrambled QMC approximation is. For each kind of QMC sequences and s, 10 independent
trails are executed, and the mean and standard deviation are plotted.
lower error. Similar results were observed for other datasets (not reported here). Although using scrambled
variants of QMC sequences may incur some variance, the variance is quite small compared to that of the
MC random features.
Scrambled (whether deterministic or randomized) QMC sequences tend to yield higher accuracies than
non-scambled QMC sequences. In Figure 3, we show the ratio between the relative errors achieved by using
both scrambled and non-scrambled QMC sequences. As can be seen, scrambled QMC sequences provide
more accurate approximations in most cases as the ratio value tends to be less than one. In particular,
scrambled Lattice sequence outperforms the non-scrambled one across all the cases for larger values of s.
Therefore, in the rest of the experiments we use scrambled sequences.
Does better Gram matrix approximation translate to lower generalization errors?
We consider two regression datasets, cpu and census, and use (approximate) kernel ridge regression
to build a regression model. The ridge parameter is set by the optimal value we obtain via 5-fold crossvalidation on the training set by using the MC sequence. Table 1 summarizes the results.
As we see, for cpu, all the sequences behave similarly, with the Halton sequence yielding the lowest
test error. For census, the advantage of using Halton sequence is signiﬁcant (almost 20% reduction in
generalization error) followed by Digital Nets and Sobol’. In addition, MC sequence tends to generate
higher variance across all the sampling size. Overall, QMC sequences, especially Halton, outperform MC
sequences on these datasets.
When performed on classiﬁcation datasets by using the same learning model, with a moderate range
of s, e.g., less than 2000, the QMC sequences do not yield accuracy improvements over the MC sequence
with the same consistency as in the regression case. The connection between kernel approximation and the
performance in downstream applications is outside the scope of the current paper. Worth mentioning in this
regard, is the recent work by Bach , which analyses the connection between Nystr¨om approximations
of the Gram matrix, and the regression error, and the work of El Alaoui and Mahoney on kernel
methods with statistical guarantees.
Table 1: Regression error, i.e., ∥ˆy −y∥2/∥y∥2 where ˆy is the predicted value and y is the ground truth. For
each kind of random feature and s, 10 independent trials are executed, and the mean and standard deviation
are listed.
Behavior of Box Discrepancy
Next, we examine if D□is predictive of the quality of approximation. We compute the normalized square
box discrepancy values (i.e., πd(Qd
j=1 bj)−1D□(S)2) as well as Gram matrix approximation error for the
different sequences with different sample sizes s. The expected normalized square box discrepancy values
for MC are computed using (13).
Our experiments revealed that using the full □b does not yield box discrepancy values that are very
useful. Either the values were not predictive of the kernel approximation, or they tended to stay constant.
Recall, that while the bounding box □b is set based on observed ranges of feature values in the dataset,
the actual distribution of points ¯
X encountered inside that box might be far from uniform. This lead us to
consider the discrepancy measure when measured on the central part of the bounding box (i.e., □b/2 instead
of □b), which is equal to the integration error averaged over that part of the bounding box. Presumably,
points from ¯
X concentrate in that region, and they may be more relevant for downstream predictive task.
The results are shown in Figure 4. In the top graphs we can see, as expected, increasing number of
features in the sequence leads to a lower box discrepancy value. In the bottom graphs, which compare
∥K −˜K∥F /∥K∥F to D□b/2, we can see a strong correlation between the quality of approximation and the
discrepancy value.
Experiments With Adaptive QMC
The goal of this subsection is to provide a proof-of-concept for learning adaptive QMC sequences, using the
three schemes described in Section 5. We demonstrate that QMC sequences can be improved to produce
better approximation to the Gram matrix, and that can sometimes lead to improved generalization error.
Note that the running time of learning the adaptive sequences is less relevant in our experimental setting
for the following reasons. Given the values of s, d, b and σ the optimization of a sequence needs only to
be done once. There is some ﬂexibility in these parameters: d can be adjusted by adding zero features or
by doing PCA on the input; one can use longer or shorter sequences; and the data can be forced to a ﬁt a
particular bounding box using (possibly non-equal) scaling of the features (this, in turn, affects the choice
of the σ) . Since designing adaptive QMC sequences is data-independent with applicability to a variety of
downstream applications of kernel methods, it is quite conceivable to generate many point sets in advance
Number of samples
Normalized box discrepancy
Digital Net
(a) cpu, D□b/2 versus s
Number of samples
Normalized box discrepancy
Digital Net
(b) census, D□b/2 versus s
Normalized box discrepancy
||K −˜K||F/||K||F
Digital Net
(c) cpu, relative error versus D□b/2
Normalized box discrepancy
||K −˜K||F/||K||F
Digital Net
(d) census, relative error versus D□b/2
Figure 4: Discrepancy values (D□b/2) for the different sequences on cpu and census. We measure the
discrepancy on the central part of the bounding box (we use □b/2 instead of □b as the domain in the box
discrepancy).
Err=0.0004
Adaptive QMC
(b/2): Err=0.0052, Box=0.0002
MC: Error=0.075750
Halton: Error=0.008535
Figure 5: Integration error.
and to use them for many learning tasks. Furthermore, the total size of the sequences (s × d) is independent
of the number of examples n, which is the dominant term in large scale learning settings.
We name the three sequences as Global Adaptive, Greedy Adaptive and Weighted respectively. For
Global Adaptive, the Halton sequence is used as the initial setting of the optimization variables S. For
Greedy Adaptive, when optimizing for wt, the t-th point in the Halton sequence is used as the initial point.
In both cases, we use non-linear conjugate gradient to perform numerical optimization. For Weighted,
the initial features are generated using the Halton sequence and we optimize for the weights. We used
CVX [Grant and Boyd, 2014, 2008] to compute the sequence (solve (17)).
Integral Approximation
We begin by examining the integration error over the unit square by using three different sequences,
namely, MC, Halton and global adaptive QMC sequences. The integral is of the form
 2 e−iuT tdt where
u spans the unit square. The error is plotted in Figure 5. We see that MC sequences concentrate most of the
error reduction near the origin. The Halton sequence gives signiﬁcant improvement expanding the region
of low integration error. Global adaptive QMC sequences give another order of magnitude improvement
in integration error which is now diffused over the entire unit square; the estimation of such sequences is
“aware” of the full integration region. In fact, by controlling the box size (see plot labeled b/2), adaptive
sequences can be made to focus in a speciﬁed sub-box which can help with generalization if the actual data
distribution is better represented by this sub-box.
Quality of Kernel Approximation
In Figure 6 and Figure 7 we examine how various metrics (discrepancy, maximum squared error, mean
squared error, norm of the error) on the Gram matrix approximation evolve during the optimization process
for both adaptive sequences. Since learning the adaptive sequences on dataset with low dimensional features
CPU dataset, s=100
Normalized Db (S)2
Maximum Squared Error
Mean Squared Error
∥˜K −K∥2/∥K∥2
HOUSING dataset, s=100
Normalized Db (S)2
Maximum Squared Error
Mean Squared Error
∥˜K −K∥2/∥K∥2
HOUSING dataset, s=100, optimizing Db/2(S)2
Normalized Db/2 (S)2
Maximum Squared Error
Mean Squared Error
∥˜K −K∥2/∥K∥2
Figure 6: Examining the behavior of learning Global Adaptive sequences. Various metrics on the Gram
matrix approximation are plotted.
CPU dataset
Number of Points
Normalized Db (S)2
Maximum Squared Error
Mean Squared Error
∥˜K −K∥2/∥K∥2
HOUSING dataset
Number of Points
Normalized Db (S)2
Maximum Squared Error
Mean Squared Error
∥˜K −K∥2/∥K∥2
HOUSING dataset, optimizing Db/2(S)2
Number of Points
Figure 7: Examining the behavior of learning Greedy Adaptive sequences. Various metrics on the Gram
matrix approximation are plotted.
is more affordable, the experiment is performed on two such datasets, namely, cpu and housing.
For Global Adaptive, we ﬁxed s = 100 and examine how the performance evolves as the number of
iterations grows. In Figure 6 (a) we examine the behavior on cpu. We see that all metrics go down as the
iteration progresses. This supports our hypothesis that by optimizing the box discrepancy we can improve
the approximation of the Gram matrix. Figure 6 (b), which examines the same metrics on the scaled version
of the housing dataset, has some interesting behaviors. Initially all metrics go down, but eventually all the
metrics except the box-discrepancy start to go up; the box-discrepancy continues to go down. One plausible
explanation is that the integrands are not uniformly distributed in the bounding box, and that by optimizing
the expectation over the entire box we start to overﬁt it, thereby increasing the error in those regions of the
box where integrands actually concentrate. One possible way to handle this is to optimize closer to the center
of the box (e.g., on □b/2), under the assumption that integrands concentrate there. In Figure 6 (c) we try
this on the housing dataset. We see that now the mean error and the norm error are much improved, which
supports the interpretation above. But the maximum error eventually goes up. This is quite reasonable as
the outer parts of the bounding box are harder to approximate, so the maximum error is likely to originate
from there. Subsequently, we stop the adaptive learning of the QMC sequences early, to avoid the actual
error from going up due to averaging.
For Greedy Adaptive, we examine its behavior as the number of points increases. In Figure 7 (a) and (b),
as expected, as the number of points in the sequence increases, the box discrepancy goes down. This is also
translated to non-monotonic decrease in the other metrics of Gram matrix approximation. However, unlike
the global case, we see in Figure 7 (c), when the points are generated by optimizing on a smaller box □b/2,
the resulting metrics become higher for a ﬁxed number of points. Although the Greedy Adaptive sequence
can be computed faster than the adaptive sequence, potentially it might need a large number of points to
achieve certain low magnitude of discrepancy. Hence, as shown in the plots, when the number of points is
below 500, the quality of the optimization is not good enough to provide a good approximation the Gram
matrix. For example, one can check when the number of points is 100, the discrepancy value of the Greedy
Adaptive sequence is higher than that of the Global Adaptive sequence with more than 10 iterations.
WEIGHTEDb/4
Table 2: Discrepancy values, measured on the full bounding box and its central part, i.e., D□b and D□b/4.
Table 2 also shows the discrepancy values of various sequences on cpu and census. Using adaptive
sequences improves the discrepancy values by orders-of-magnitude. We note that a signiﬁcant reduction in
terms of discrepancy values can be achieved using only weights, sometimes yielding discrepancy values that
are better than the hard-to-compute global or greedy sequences.
Generalization Error
We use the three algorithms for learning adaptive sequences as described in the previous subsections,
and use them for doing approximate kernel ridge regression. The ridge parameter is set by the value which
is near-optimal for both sequences in 5-fold cross-validation on the training set. Table 3 summarizes the
WEIGHTEDb/4
Table 3: Regression error, i.e., ∥ˆy −y∥2/∥y∥2 where ˆy is the predicted value and y is the ground truth.
For both cpu and census, at least one of the adaptive sequences sequences can yield lower test error
for each sampling size (since the test error is already low, around 3% or 5%, such improvement in accuracy
is not trivial). For cpu, greedy approach seems to give slightly better results. When s = 500 or even larger
(not reported here), the performance of the sequences are very close. For census, the weighted sequence
yields the lowest generalization error when s = 400, 800. Afterwards we can see global adaptive sequence
outperforms the rest of the sequences, even though it has better discrepancy values. In some cases, adaptive
sequences sometimes produce errors that are bigger than the unoptimized sequences.
In most cases, the adaptive sequence on the central part of the bounding box outperforms the adaptive
sequence on the entire box. This is likely due to the non-uniformity phenomena discussed earlier.
Conclusion and Future Work
Recent work on applying kernel methods to very large datasets, has shown their ability to achieve stateof-the-art accuracies that sometimes match those attained by Deep Neural Networks (DNN) [Huang et al.,
2014]. Key to these results is the ability to apply kernel method to such datasets. The random features
approach, originally due to Rahimi and Recht , as emerged as a key technology for scaling up kernel
methods [Sindhwani and Avron, 2014].
Close examination of those empirical results reveals that to achieve state-of-the-art accuracies, a very
large number of random features was needed. For example, on TIMIT, a classical speech recognition
dataset, over 200,000 random features were used in order to match DNN performance [Huang et al., 2014].
It is clear that improving the efﬁciency of random features can have a signiﬁcant impact on our ability to
scale up kernel methods, and potentially get even higher accuracies.
This paper is the ﬁrst to exploit high-dimensional approximate integration techniques from the QMC
literature in this context, with promising empirical results backed by rigorous theoretical analyses. Avenues
for future work include incorporating stronger data-dependence in the estimation of adaptive sequences and
analyzing how resulting Gram matrix approximations translate into downstream performance improvements
for a variety of large-scale learning tasks.
Acknowledgements
The authors would like to thank Josef Dick for useful pointers to literature about improvement of the QMC
sequences; Ha Quang Minh for several discussions on Paley-Wiener spaces and RKHS theory; the anonymous ICML reviewers for pointing out the connection to herding and other helpful comments; the anonymous JMLR reviewers for suggesting weighted sequences and other helpful comments. This research was
supported by the XDATA program of the Defense Advanced Research Projects Agency (DARPA), administered through Air Force Research Laboratory contract FA8750-12-C-0323. This work was done while J.
Yang was a summer intern at IBM Research.
Technical Details
In this section we give detailed proofs of the assertions made in Section 4 and 5.
Proof of Proposition 5
Recall, for any t ∈Rd, for Φ−1(t), we mean
1 (t1), . . . , Φ−1
∈Rd, where Φj(·) is the CDF of
From fu(t) = e−iuT Φ−1(t), for any j = 1, . . . , d, we have
j (tj))e−iuT Φ−1
∂t1 · · · ∂td
In (6), when I = [d],
∂t1 · · · ∂td
dt1 · · · dtd
dt1 · · · dtd
dt1 · · · dtd
With a change of variable, Φj(tj) = vj, for j = 1, . . . , d, (18) becomes
As this is a term in (6), we know that VHK[fu(t)] is unbounded.
Proof of Proposition 6
We need the following lemmas, across which we share some notation.
Lemma 17. Assuming that κ = supx∈Rd h(x, x) < ∞, if f ∈H, where H is an RKHS with kernel h(·, ·),
the integral
Rd f(x)p(x)dx is ﬁnite.
Proof. For notational convenience, we note that
Rd f(x)p(x)dx = E [f(X)] ,
where E [·] denotes expectation and X is a random variable distributed according to the probability density
p(·) on Rd.
Now consider a linear functional T that maps f to E [f(X)], i.e.,
T[f] = E [f(X)] .
The linear functional T is a bounded linear functional on the RKHS H. To see this:
|E [f(X)] |
E [|f(X)|]
(Jensen’s Inequality)
E [|⟨f, h(X, ·)⟩H|]
(Reproducing Property)
∥f∥HE [∥h(X, ·)∥H]
(Cauchy-Schwartz)
This shows that the integral
Rd f(x)p(x)dx exists.
Lemma 18. The mean µh,p(u) =
Rd h(u, x)p(x)dx is in H. In addition, for any f ∈H,
E [f(X)] =
Rd f(x)p(x)dx = ⟨f, µh,p⟩H .
Proof. From the Riesz Representation Theorem, every bounded linear functional on H admits an inner
product representation. Therefore, for T deﬁned in (19), there exists µh,p ∈H such that,
T[f] = E [f(X)] = ⟨f, µh,p⟩H .
Therefore we have, ⟨f, µh,p⟩H =
f(x)p(x)dx for all f ∈H. For any z, choosing f(·) = h(z, ·), where
h(·, ·) is the kernel associated with H, and invoking the reproducing property we see that,
µh,p(z) = ⟨h(z, ·), µh,p⟩H =
Rd h(z, x)p(x)dx .
The proof of Proposition 6 follows from the existence Lemmas above, and the following steps.
Rd f(x)p(x)dx −1
⟨f, µh,p⟩H −1
⟨f, h(wl, ·)⟩H
⟨f, µh,p −1
h(wl, ·)⟩H
= ∥f∥H Dh,p(S) ,
where Dh,p(S) is given as follows,
⟨µh,p, µh,p⟩H −2
⟨µh,p, h(wl, ·)⟩H + 1
⟨h(wl, ·), h(wj, ·)⟩H
E [µh,p(X)] −2
E [h(wl, ·)] + 1
Rd h(ω, φ)p(ω)p(φ)dωdφ −2
Rd h(wl, ω)p(ω)dω
h(wl, wj) .
Proof of Theorem 9
We apply (9) to the particular case of h = sincb. We have
Rd h(ω, φ)p(ω)p(φ)dωdφ
sin(bj(ωj −φj))
pj(ωj)pj(φj)dωdφ
sin(bj(ωj −φj))
pj(ωj)pj(φj)dωjdφj ,
Rd h(wl, ω)p(ω)dω
sin(bj(wlj −ωj))
sin(bj(wlj −ωj))
pj(ωj)dωj .
So we can consider each coordinate on its own.
Fix j. We have
cos(βx)pj(x)dβdx
eiβxp(x)dxdβ
The interchange in the second line is allowed since the pj(x) makes the function integrable (with respect to
Now ﬁx w ∈R as well. Let hj(x, y) = sin(bj(x −y))/π(x −y). We have
hj(ω, w)pj(ω)dω
sin(bj(ω −w))
pj(x + w)dx
ϕj(β)eiwβdβ ,
where the last equality follows from ﬁrst noticing that the characteristic function associated with the density
function x 7→pj(x + w) is β 7→ϕ(β)eiwβ, and then applying the previous inequality.
We also have,
sin(bj(x −y))
pj(x)pj(y)dxdy
cos(β(x −y))pj(x)pj(y)dβdxdy
eiβ(x−y)pj(x)pj(y)dβdxdy
eiβ(x−y)pj(x)pj(y)dxdydβ
eiβxpj(x)dx
e−iβypj(y)dy
ϕj(β)ϕj(β)∗dβ
|ϕj(β)|2dβ .
The interchange at the third line is allowed because of pj(x)pj(y). In the last line we use the fact that the
ϕj(·) is Hermitian.
Proof of Theorem 12
Let b > 0 be a scalar, and let u ∈[−b, b] and z ∈R. We have,
e−iux sin(b(x −z))
2b y sin(πy)
e−iuz rect(u/2b)
In the above, rect is the function that is 1 on [−1/2, 1/2] and zero elsewhere.
The last equality implies that for every u ∈□b and every x ∈Rd we have
Rd fu(y) sincb(y, x)dy .
We now have for every u ∈□b,
Rd fu(x)p(x)dx −1
Rd fu(y) sincb(y, x)dyp(x)dx −1
Rd fu(y) sincb(y, wi)dy
Rd sincb(y, x)p(x)dx −1
sincb(y, wi)
Let us denote
Rd sincb(y, x)p(x)dx −1
sincb(y, wi) .
ǫS,p[fu] =
Rd fu(y)rS(y)dy
The function rS(·) is square-integrable, so it has a Fourier transform ˆrS(·). The above formula is exactly
the value of ˆrS(u). That is,
ǫS,p[fu] = |ˆrS(u)| .
ǫS,p[fu]2
The equality before the last follows from Plancherel formula and the equality of the norm in PWb to the
L2-norm. The last equality follows from the fact that rS is exactly the expression used in the proof of
Proposition 6 to derive D□
Proof of Corollary 13
In this case, p(x) = Qd
j=1 pj(xj) where pj(·) is the density function of N(0, 1/σj). The characteristic
function associated with pj(·) is ϕj(β) = e
j . We apply (11) directly.
For the ﬁrst term, since
|ϕj(β)|2dβ
|ϕj(β)|2dβ =
For the second term, since
ϕj(β)eiwljβdβ
2σ2 +iwljβdβ
ϕj(β)eiwljβdβ = 2
Combining (21), (22) and (11), (12) follows.
Proof of Corollary 14
The proof is similar to the proof of Theorem 3.6 of Dick et al. . Notice that since supx∈Rd h(x, x) <
∞, we have
Rd h(x, x)p(x)dx < ∞. From Lemma 18 we know that
Rd h(·, y)p(y)dy ∈H, hence from
Lemma 17, we have
Rd h(x, y)p(x)p(y)dxdy < ∞.
By (9), we have
Rd h(ω, φ)p(ω)p(φ)dωdφ
Rd h(wl, ω)p(ω)dω
h(wl, wl) + 1
l,j=1,l̸=j
h(wl, wj) .
 d · · ·
Rd h(ω, φ)p(ω)p(φ)dωdφ −2
Rd h(Φ−1(tl), ω)p(ω)dω
h(Φ−1(tl), Φ−1(tl)) + 1
l,j=1,l̸=j
h(Φ−1(tl), Φ−1(tj))
dt1 · · · dts .
Obviously, the ﬁrst is a constant which is independent to t1, . . . , ts. Since all the terms are ﬁnite, we can
interchange the integral and the sum among rest terms. In the second term, for each l, the only dependence
on t1, . . . , ts is tl, hence all the other tj can be integrated out. That is,
 d · · ·
Rd h(Φ−1(tl), ω)p(ω)dωdt1 · · · dts
Rd h(Φ−1(tl), ω)p(ω)dωdtl
Rd h(φ, ω)p(φ)p(ω)dφdω.
Above, the last equality comes from a change of variable, i.e., tl = (Φ1(φ1), . . . , Φd(φd)).
Similar operations can be done for the third and fourth term. Combining all of these, we have the
following,
Rd h(ω, φ)p(ω)p(φ)dωdφ −2
Rd h(ω, φ)p(ω)p(φ)dωdφ
Rd h(ω, ω)p(ω)dω + s −1
Rd h(ω, φ)p(ω)p(φ)dωdφ
Rd h(ω, ω)p(ω)dω −1
Rd h(ω, φ)p(ω)p(φ)dωdφ .
Proof of Proposition 16
Before we compute the derivative, we prove two auxiliary lemmas.
Lemma 19. Let x ∈Rd be a variable and z ∈Rd be ﬁxed vector. Then,
∂sincb(x, z)
= bj sinc′
bj(xj, zj)
sincbq(xq, zq) .
We omit the proof as it is a simple computation that follows from the deﬁnition of sincb.
Lemma 20. The derivative of the scalar function f(x) = Re
e−ax2 erf (c + idx)
, for real scalars a, c, d
is given by,
∂x = −2axe−ax2 Re [erf (c + idx)] + 2d
√πe−ax2ed2x2−c2 sin(2cdx) .
Proof. Since
e−ax2 erf(c + idx) +
e−ax2 erf(c + idx)
e−ax2 erf(c + idx) + e−ax2 erf(c −idx)
it sufﬁces to compute the the derivative g(x) = e−ax2 erf(c + idx).
Let k(x) = erf(c + idx). We have
g′(x) = −2axe−ax2k(x) + e−ax2k′(x) .
erf(c + idx)
e−y2dy + (id)
e−(c+idt)2dt
√πe−(c+idx)2 = 2d
√πed2x2−c2(sin(2cdx) + i cos(2cdx)) .
We now have
 g′(x) + (g∗(x))′
 g′(x) + (g′(x))∗
−2axe−ax2(k(x) + k∗(x)) + e−ax2(k′(x) + (k′(x))∗)
−4axe−ax2 Re [erf (c + idx)] + e−ax2 4d
√πed2x2−c2 sin(2cdx)
−2axe−ax2 Re [erf (c + idx)] + 2d
√πe−ax2ed2x2−c2 sin(2cdx) .
Proof of Proposition 16. For the ﬁrst term in (12), that is 1
r=1 sincb(wm, wr), to compute the
partial derivative of wlj, we only have to consider when at least m or r is equal to l. If m = j = l,
by deﬁnition, the corresponding term in the summation is one. Hence, we only have to consider the case
when m ̸= r. By symmetry, it is equivalent to compute the partial derivative of the following function
m=1,m̸=l sincb(wl, wm). Applying Lemma 19, we get the ﬁrst term in (14).
Next, for the last term in (12), we only have consider the term associated with one in the summation
and the term associated with j in the product. Since
satisﬁes the
formulation in Lemma 20, we can simply apply Lemma 20 and get its derivative with respect to wlj.
Equation (14) follows by combining these terms.