Analyzing and Improving the Image Quality of StyleGAN
Tero Karras
Samuli Laine
Miika Aittala
Janne Hellsten
Jaakko Lehtinen
NVIDIA and Aalto University
The style-based GAN architecture (StyleGAN) yields
state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of
its characteristic artifacts, and propose changes in both
model architecture and training methods to address them.
In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to
encourage good conditioning in the mapping from latent
codes to images. In addition to improving image quality,
this path length regularizer yields the additional beneﬁt that
the generator becomes signiﬁcantly easier to invert. This
makes it possible to reliably attribute a generated image to
a particular network. We furthermore visualize how well
the generator utilizes its output resolution, and identify a
capacity problem, motivating us to train larger models for
additional quality improvements.
Overall, our improved
model redeﬁnes the state of the art in unconditional image
modeling, both in terms of existing distribution quality metrics as well as perceived image quality.
1. Introduction
The resolution and quality of images produced by generative methods, especially generative adversarial networks
(GAN) , are improving rapidly . The current
state-of-the-art method for high-resolution image synthesis
is StyleGAN , which has been shown to work reliably
on a variety of datasets. Our work focuses on ﬁxing its characteristic artifacts and improving the result quality further.
The distinguishing feature of StyleGAN is its unconventional generator architecture. Instead of feeding the
input latent code z ∈Z only to the beginning of a the network, the mapping network f ﬁrst transforms it to an intermediate latent code w ∈W. Afﬁne transforms then produce styles that control the layers of the synthesis network g
via adaptive instance normalization (AdaIN) .
Additionally, stochastic variation is facilitated by providing
additional random noise maps to the synthesis network. It
has been demonstrated that this design allows the
intermediate latent space W to be much less entangled than
the input latent space Z. In this paper, we focus all analysis solely on W, as it is the relevant latent space from the
synthesis network’s point of view.
Many observers have noticed characteristic artifacts in
images generated by StyleGAN . We identify two causes
for these artifacts, and describe changes in architecture and
training methods that eliminate them. First, we investigate
the origin of common blob-like artifacts, and ﬁnd that the
generator creates them to circumvent a design ﬂaw in its architecture. In Section 2, we redesign the normalization used
in the generator, which removes the artifacts. Second, we
analyze artifacts related to progressive growing that has
been highly successful in stabilizing high-resolution GAN
training. We propose an alternative design that achieves the
same goal — training starts by focusing on low-resolution
images and then progressively shifts focus to higher and
higher resolutions — without changing the network topology during training. This new design also allows us to reason about the effective resolution of the generated images,
which turns out to be lower than expected, motivating a capacity increase (Section 4).
Quantitative analysis of the quality of images produced
using generative methods continues to be a challenging
topic. Fr´echet inception distance (FID) measures differences in the density of two distributions in the highdimensional feature space of an InceptionV3 classiﬁer .
Precision and Recall (P&R) provide additional visibility by explicitly quantifying the percentage of generated
images that are similar to training data and the percentage
of training data that can be generated, respectively. We use
these metrics to quantify the improvements.
Both FID and P&R are based on classiﬁer networks that
have recently been shown to focus on textures rather than
shapes , and consequently, the metrics do not accurately
capture all aspects of image quality. We observe that the
perceptual path length (PPL) metric , originally introduced as a method for estimating the quality of latent space
 
Figure 1. Instance normalization causes water droplet -like artifacts in StyleGAN images. These are not always obvious in the generated
images, but if we look at the activations inside the generator network, the problem is always there, in all feature maps starting from the
64x64 resolution. It is a systemic problem that plagues all StyleGAN images.
interpolations, correlates with consistency and stability of
shapes. Based on this, we regularize the synthesis network
to favor smooth mappings (Section 3) and achieve a clear
improvement in quality. To counter its computational expense, we also propose executing all regularizations less
frequently, observing that this can be done without compromising effectiveness.
Finally, we ﬁnd that projection of images to the latent
space W works signiﬁcantly better with the new, pathlength regularized StyleGAN2 generator than with the original StyleGAN. This makes it easier to attribute a generated
image to its source (Section 5).
Our implementation and trained models are available at
 
2. Removing normalization artifacts
We begin by observing that most images generated by
StyleGAN exhibit characteristic blob-shaped artifacts that
resemble water droplets. As shown in Figure 1, even when
the droplet may not be obvious in the ﬁnal image, it is
present in the intermediate feature maps of the generator.1
The anomaly starts to appear around 64×64 resolution,
is present in all feature maps, and becomes progressively
stronger at higher resolutions. The existence of such a consistent artifact is puzzling, as the discriminator should be
able to detect it.
We pinpoint the problem to the AdaIN operation that
normalizes the mean and variance of each feature map separately, thereby potentially destroying any information found
in the magnitudes of the features relative to each other. We
hypothesize that the droplet artifact is a result of the generator intentionally sneaking signal strength information past
instance normalization: by creating a strong, localized spike
that dominates the statistics, the generator can effectively
scale the signal as it likes elsewhere. Our hypothesis is supported by the ﬁnding that when the normalization step is
removed from the generator, as detailed below, the droplet
artifacts disappear completely.
1In rare cases (perhaps 0.1% of images) the droplet is missing, leading
to severely corrupted images. See Appendix A for details.
2.1. Generator architecture revisited
We will ﬁrst revise several details of the StyleGAN
generator to better facilitate our redesigned normalization.
These changes have either a neutral or small positive effect
on their own in terms of quality metrics.
Figure 2a shows the original StyleGAN synthesis network g , and in Figure 2b we expand the diagram to full
detail by showing the weights and biases and breaking the
AdaIN operation to its two constituent parts: normalization
and modulation. This allows us to re-draw the conceptual
gray boxes so that each box indicates the part of the network
where one style is active (i.e., “style block”). Interestingly,
the original StyleGAN applies bias and noise within the
style block, causing their relative impact to be inversely proportional to the current style’s magnitudes. We observe that
more predictable results are obtained by moving these operations outside the style block, where they operate on normalized data. Furthermore, we notice that after this change
it is sufﬁcient for the normalization and modulation to operate on the standard deviation alone (i.e., the mean is not
needed). The application of bias, noise, and normalization
to the constant input can also be safely removed without observable drawbacks. This variant is shown in Figure 2c, and
serves as a starting point for our redesigned normalization.
2.2. Instance normalization revisited
One of the main strengths of StyleGAN is the ability to
control the generated images via style mixing, i.e., by feeding a different latent w to different layers at inference time.
In practice, style modulation may amplify certain feature
maps by an order of magnitude or more. For style mixing to
work, we must explicitly counteract this ampliﬁcation on a
per-sample basis — otherwise the subsequent layers would
not be able to operate on the data in a meaningful way.
If we were willing to sacriﬁce scale-speciﬁc controls (see
video), we could simply remove the normalization, thus removing the artifacts and also improving FID slightly .
We will now propose a better alternative that removes the
artifacts while retaining full controllability. The main idea
is to base normalization on the expected statistics of the incoming feature maps, but without explicit forcing.
Const 4×4×512
Norm mean/std
Mod mean/std
Norm mean/std
Norm mean/std
Mod mean/std
Norm mean/std
Mod mean/std
Style block
Style block
Style block
(a) StyleGAN
(b) StyleGAN (detailed)
(c) Revised architecture
(d) Weight demodulation
We redesign the architecture of the StyleGAN synthesis network. (a) The original StyleGAN, where A denotes a learned
afﬁne transform from W that produces a style and B is a noise broadcast operation. (b) The same diagram with full detail. Here we have
broken the AdaIN to explicit normalization followed by modulation, both operating on the mean and standard deviation per feature map.
We have also annotated the learned weights (w), biases (b), and constant input (c), and redrawn the gray boxes so that one style is active
per box. The activation function (leaky ReLU) is always applied right after adding the bias. (c) We make several changes to the original
architecture that are justiﬁed in the main text. We remove some redundant operations at the beginning, move the addition of b and B to
be outside active area of a style, and adjust only the standard deviation per feature map. (d) The revised architecture enables us to replace
instance normalization with a “demodulation” operation, which we apply to the weights associated with each convolution layer.
Recall that a style block in Figure 2c consists of modulation, convolution, and normalization. Let us start by considering the effect of a modulation followed by a convolution.
The modulation scales each input feature map of the convolution based on the incoming style, which can alternatively
be implemented by scaling the convolution weights:
ijk = si · wijk,
where w and w′ are the original and modulated weights,
respectively, si is the scale corresponding to the ith input
feature map, and j and k enumerate the output feature maps
and spatial footprint of the convolution, respectively.
Now, the purpose of instance normalization is to essentially remove the effect of s from the statistics of the convolution’s output feature maps. We observe that this goal
can be achieved more directly. Let us assume that the input activations are i.i.d. random variables with unit standard
deviation. After modulation and convolution, the output activations have standard deviation of
i.e., the outputs are scaled by the L2 norm of the corresponding weights. The subsequent normalization aims to
restore the outputs back to unit standard deviation. Based
on Equation 2, this is achieved if we scale (“demodulate”)
each output feature map j by 1/σj. Alternatively, we can
again bake this into the convolution weights:
where ϵ is a small constant to avoid numerical issues.
We have now baked the entire style block to a single convolution layer whose weights are adjusted based on s using
Equations 1 and 3 (Figure 2d). Compared to instance normalization, our demodulation technique is weaker because
it is based on statistical assumptions about the signal instead of actual contents of the feature maps. Similar statistical analysis has been extensively used in modern network
initializers , but we are not aware of it being previously used as a replacement for data-dependent normalization. Our demodulation is also related to weight normalization that performs the same calculation as a part of
reparameterizing the weight tensor. Prior work has identiﬁed weight normalization as beneﬁcial in the context of
GAN training .
Our new design removes the characteristic artifacts (Figure 3) while retaining full controllability, as demonstrated
in the accompanying video. FID remains largely unaffected
(Table 1, rows A, B), but there is a notable shift from precision to recall. We argue that this is generally desirable, since
recall can be traded into precision via truncation, whereas
Conﬁguration
FFHQ, 1024×1024
LSUN Car, 512×384
Path length ↓
Precision ↑
Path length ↓
Precision ↑
A Baseline StyleGAN 
B + Weight demodulation
C + Lazy regularization
D + Path length regularization
E + No growing, new G & D arch.
F + Large networks (StyleGAN2)
Conﬁg A with large networks
Table 1. Main results. For each training run, we selected the training snapshot with the lowest FID. We computed each metric 10 times
with different random seeds and report their average. Path length corresponds to the PPL metric, computed based on path endpoints in W
 , without the central crop used by Karras et al. . The FFHQ dataset contains 70k images, and the discriminator saw 25M images
during training. For LSUN CAR the numbers were 893k and 57M. ↑indicates that higher is better, and ↓that lower is better.
Figure 3. Replacing normalization with demodulation removes the
characteristic artifacts from images and activations.
the opposite is not true . In practice our design can be
implemented efﬁciently using grouped convolutions, as detailed in Appendix B. To avoid having to account for the
activation function in Equation 3, we scale our activation
functions so that they retain the expected signal variance.
3. Image quality and generator smoothness
While GAN metrics such as FID or Precision and Recall
(P&R) successfully capture many aspects of the generator,
they continue to have somewhat of a blind spot for image
quality. For an example, refer to Figures 13 and 14 that
contrast generators with identical FID and P&R scores but
markedly different overall quality.2
2We believe that the key to the apparent inconsistency lies in the particular choice of feature space rather than the foundations of FID or P&R.
It was recently discovered that classiﬁers trained using ImageNet tend
to base their decisions much more on texture than shape , while humans strongly focus on shape . This is relevant in our context because
(a) Low PPL scores
(b) High PPL scores
Figure 4. Connection between perceptual path length and image
quality using baseline StyleGAN (conﬁg A) with LSUN CAT. (a)
Random examples with low PPL (≤10th percentile). (b) Examples with high PPL (≥90th percentile). There is a clear correlation between PPL scores and semantic consistency of the images.
(a) StyleGAN (conﬁg A)
(b) StyleGAN2 (conﬁg F)
(a) Distribution of PPL scores of individual images
generated using baseline StyleGAN (conﬁg A) with LSUN CAT
(FID = 8.53, PPL = 924). The percentile ranges corresponding to
Figure 4 are highlighted in orange. (b) StyleGAN2 (conﬁg F) improves the PPL distribution considerably (showing a snapshot with
the same FID = 8.53, PPL = 387).
We observe a correlation between perceived image quality and perceptual path length (PPL) , a metric that was
originally introduced for quantifying the smoothness of the
mapping from a latent space to the output image by measuring average LPIPS distances between generated images
under small perturbations in latent space. Again consulting
Figures 13 and 14, a smaller PPL (smoother generator mapping) appears to correlate with higher overall image qual-
FID and P&R use high-level features from InceptionV3 and VGG-16
 , respectively, which were trained in this way and are thus expected
to be biased towards texture detection. As such, images with, e.g., strong
cat textures may appear more similar to each other than a human observer
would agree, thus partially compromising density-based metrics (FID) and
manifold coverage metrics (P&R).
ity, whereas other metrics are blind to the change. Figure 4
examines this correlation more closely through per-image
PPL scores on LSUN CAT, computed by sampling the latent space around w ∼f(z). Low scores are indeed indicative of high-quality images, and vice versa.
shows the corresponding histogram and reveals the long tail
of the distribution. The overall PPL for the model is simply the expected value of these per-image PPL scores. We
always compute PPL for the entire image, as opposed to
Karras et al. who use a smaller central crop.
It is not immediately obvious why a low PPL should
correlate with image quality. We hypothesize that during
training, as the discriminator penalizes broken images, the
most direct way for the generator to improve is to effectively
stretch the region of latent space that yields good images.
This would lead to the low-quality images being squeezed
into small latent space regions of rapid change. While this
improves the average output quality in the short term, the
accumulating distortions impair the training dynamics and
consequently the ﬁnal image quality.
Clearly, we cannot simply encourage minimal PPL since
that would guide the generator toward a degenerate solution
with zero recall. Instead, we will describe a new regularizer that aims for a smoother generator mapping without this
drawback. As the resulting regularization term is somewhat
expensive to compute, we ﬁrst describe a general optimization that applies to any regularization technique.
3.1. Lazy regularization
Typically the main loss function (e.g., logistic loss )
and regularization terms (e.g., R1 ) are written as a single expression and are thus optimized simultaneously. We
observe that the regularization terms can be computed less
frequently than the main loss function, thus greatly diminishing their computational cost and the overall memory usage. Table 1, row C shows that no harm is caused when R1
regularization is performed only once every 16 minibatches,
and we adopt the same strategy for our new regularizer as
well. Appendix B gives implementation details.
3.2. Path length regularization
We would like to encourage that a ﬁxed-size step in W
results in a non-zero, ﬁxed-magnitude change in the image.
We can measure the deviation from this ideal empirically
by stepping into random directions in the image space and
observing the corresponding w gradients. These gradients
should have close to an equal length regardless of w or the
image-space direction, indicating that the mapping from the
latent space to image space is well-conditioned .
At a single w ∈W, the local metric scaling properties
of the generator mapping g(w) : W 7→Y are captured by
the Jacobian matrix Jw = ∂g(w)/∂w. Motivated by the
desire to preserve the expected lengths of vectors regardless
of the direction, we formulate our regularizer as
Ew,y∼N(0,I)
where y are random images with normally distributed pixel
intensities, and w ∼f(z), where z are normally distributed.
We show in Appendix C that, in high dimensions, this prior is minimized when Jw is orthogonal (up
to a global scale) at any w. An orthogonal matrix preserves
lengths and introduces no squeezing along any dimension.
To avoid explicit computation of the Jacobian matrix,
we use the identity JT
wy = ∇w(g(w) · y), which is ef-
ﬁciently computable using standard backpropagation .
The constant a is set dynamically during optimization as
the long-running exponential moving average of the lengths
wy∥2, allowing the optimization to ﬁnd a suitable global
scale by itself.
Our regularizer is closely related to the Jacobian clamping regularizer presented by Odena et al. . Practical differences include that we compute the products JT
wy analytically whereas they use ﬁnite differences for estimating
Jwδ with Z ∋δ ∼N(0, I). It should be noted that spectral normalization of the generator only constrains
the largest singular value, posing no constraints on the others and hence not necessarily leading to better conditioning.
We ﬁnd that enabling spectral normalization in addition to
our contributions — or instead of them — invariably compromises FID, as detailed in Appendix E.
In practice, we notice that path length regularization
leads to more reliable and consistently behaving models,
making architecture exploration easier.
We also observe
that the smoother generator is signiﬁcantly easier to invert
(Section 5). Figure 5b shows that path length regularization
clearly tightens the distribution of per-image PPL scores,
without pushing the mode to zero. However, Table 1, row D
points toward a tradeoff between FID and PPL in datasets
that are less structured than FFHQ.
4. Progressive growing revisited
Progressive growing has been very successful in stabilizing high-resolution image synthesis, but it causes its
own characteristic artifacts. The key issue is that the progressively grown generator appears to have a strong location
preference for details; the accompanying video shows that
when features like teeth or eyes should move smoothly over
the image, they may instead remain stuck in place before
jumping to the next preferred location. Figure 6 shows a related artifact. We believe the problem is that in progressive
growing each resolution serves momentarily as the output
resolution, forcing it to generate maximal frequency details,
which then leads to the trained network to have excessively
high frequencies in the intermediate layers, compromising
shift invariance . Appendix A shows an example. These
Figure 6. Progressive growing leads to “phase” artifacts. In this
example the teeth do not follow the pose but stay aligned to the
camera, as indicated by the blue line.
(a) MSG-GAN
(b) Input/output skips
(c) Residual nets
Three generator (above the dashed line) and discriminator architectures. Up and Down denote bilinear up and downsampling, respectively. In residual networks these also include
1×1 convolutions to adjust the number of feature maps. tRGB
and fRGB convert between RGB and high-dimensional per-pixel
data. Architectures used in conﬁgs E and F are shown in green.
issues prompt us to search for an alternative formulation
that would retain the beneﬁts of progressive growing without the drawbacks.
4.1. Alternative network architectures
While StyleGAN uses simple feedforward designs in the
generator (synthesis network) and discriminator, there is a
vast body of work dedicated to the study of better network
architectures. Skip connections , residual networks
 , and hierarchical methods have
proven highly successful also in the context of generative
methods. As such, we decided to re-evaluate the network
design of StyleGAN and search for an architecture that produces high-quality images without progressive growing.
Figure 7a shows MSG-GAN , which connects the
matching resolutions of the generator and discriminator using multiple skip connections. The MSG-GAN generator
is modiﬁed to output a mipmap instead of an image,
and a similar representation is computed for each real im-
D original
D input skips
D residual
G original
G output skips
G residual
D original
D input skips
D residual
G original
G output skips
G residual
Table 2. Comparison of generator and discriminator architectures
without progressive growing. The combination of generator with
output skips and residual discriminator corresponds to conﬁguration E in the main result table.
age as well. In Figure 7b we simplify this design by upsampling and summing the contributions of RGB outputs
corresponding to different resolutions. In the discriminator,
we similarly provide the downsampled image to each resolution block of the discriminator. We use bilinear ﬁltering in
all up and downsampling operations. In Figure 7c we further modify the design to use residual connections.3 This
design is similar to LAPGAN without the per-resolution
discriminators employed by Denton et al.
Table 2 compares three generator and three discriminator architectures: original feedforward networks as used
in StyleGAN, skip connections, and residual networks, all
trained without progressive growing. FID and PPL are provided for each of the 9 combinations. We can see two broad
trends: skip connections in the generator drastically improve PPL in all conﬁgurations, and a residual discriminator
network is clearly beneﬁcial for FID. The latter is perhaps
not surprising since the structure of discriminator resembles classiﬁers where residual architectures are known to be
helpful. However, a residual architecture was harmful in
the generator — the lone exception was FID in LSUN CAR
when both networks were residual.
For the rest of the paper we use a skip generator and a
residual discriminator, without progressive growing. This
corresponds to conﬁguration E in Table 1, and it signiﬁcantly improves FID and PPL.
4.2. Resolution usage
The key aspect of progressive growing, which we would
like to preserve, is that the generator will initially focus on
low-resolution features and then slowly shift its attention to
ﬁner details. The architectures in Figure 7 make it possible
for the generator to ﬁrst output low resolution images that
are not affected by the higher-resolution layers in a significant way, and later shift the focus to the higher-resolution
3In residual network architectures, the addition of two paths leads to a
doubling of signal variance, which we cancel by multiplying with 1/
This is crucial for our networks, whereas in classiﬁcation resnets the
issue is typically hidden by batch normalization.
(a) StyleGAN-sized (conﬁg E)
(b) Large networks (conﬁg F)
Contribution of each resolution to the output of the
generator as a function of training time. The vertical axis shows
a breakdown of the relative standard deviations of different resolutions, and the horizontal axis corresponds to training progress,
measured in millions of training images shown to the discriminator. We can see that in the beginning the network focuses on lowresolution images and progressively shifts its focus on larger resolutions as training progresses. In (a) the generator basically outputs a 5122 image with some minor sharpening for 10242, while in
(b) the larger network focuses more on the high-resolution details.
layers as the training proceeds. Since this is not enforced in
any way, the generator will do it only if it is beneﬁcial. To
analyze the behavior in practice, we need to quantify how
strongly the generator relies on particular resolutions over
the course of training.
Since the skip generator (Figure 7b) forms the image by
explicitly summing RGB values from multiple resolutions,
we can estimate the relative importance of the corresponding layers by measuring how much they contribute to the
ﬁnal image. In Figure 8a, we plot the standard deviation of
the pixel values produced by each tRGB layer as a function
of training time. We calculate the standard deviations over
1024 random samples of w and normalize the values so that
they sum to 100%.
At the start of training, we can see that the new skip
generator behaves similar to progressive growing — now
achieved without changing the network topology. It would
thus be reasonable to expect the highest resolution to dominate towards the end of the training. The plot, however,
shows that this fails to happen in practice, which indicates
that the generator may not be able to “fully utilize” the target resolution. To verify this, we inspected the generated
images manually and noticed that they generally lack some
of the pixel-level detail that is present in the training data —
the images could be described as being sharpened versions
of 5122 images instead of true 10242 images.
This leads us to hypothesize that there is a capacity problem in our networks, which we test by doubling the number
of feature maps in the highest-resolution layers of both networks.4 This brings the behavior more in line with expecta-
4We double the number of feature maps in resolutions 642–10242
while keeping other parts of the networks unchanged. This increases the
total number of trainable parameters in the generator by 22% (25M →
30M) and in the discriminator by 21% (24M →29M).
Resolution
StyleGAN (A)
StyleGAN2 (F)
LSUN CHURCH
LSUN HORSE
Table 3. Improvement in LSUN datasets measured using FID and
PPL. We trained CAR for 57M images, CAT for 88M, CHURCH
for 48M, and HORSE for 100M images.
tions: Figure 8b shows a signiﬁcant increase in the contribution of the highest-resolution layers, and Table 1, row F
shows that FID and Recall improve markedly. The last row
shows that baseline StyleGAN also beneﬁts from additional
capacity, but its quality remains far below StyleGAN2.
Table 3 compares StyleGAN and StyleGAN2 in four
LSUN categories, again showing clear improvements in
FID and signiﬁcant advances in PPL. It is possible that further increases in the size could provide additional beneﬁts.
5. Projection of images to latent space
Inverting the synthesis network g is an interesting problem that has many applications. Manipulating a given image in the latent feature space requires ﬁnding a matching
latent code w for it ﬁrst. Previous research suggests
that instead of ﬁnding a common latent code w, the results
improve if a separate w is chosen for each layer of the generator. The same approach was used in an early encoder implementation . While extending the latent space in this
fashion ﬁnds a closer match to a given image, it also enables
projecting arbitrary images that should have no latent representation. Instead, we concentrate on ﬁnding latent codes
in the original, unextended latent space, as these correspond
to images that the generator could have produced.
Our projection method differs from previous methods
in two ways. First, we add ramped-down noise to the latent code during optimization in order to explore the latent
space more comprehensively. Second, we also optimize the
stochastic noise inputs of the StyleGAN generator, regularizing them to ensure they do not end up carrying coherent
signal. The regularization is based on enforcing the autocorrelation coefﬁcients of the noise maps to match those of
unit Gaussian noise over multiple scales. Details of our projection method can be found in Appendix D.
5.1. Attribution of generated images
Detection of manipulated or generated images is a very
important task. At present, classiﬁer-based methods can
quite reliably detect generated images, regardless of their
exact origin . However, given the rapid
pace of progress in generative methods, this may not be a
lasting situation. Besides general detection of fake images,
we may also consider a more limited form of the problem:
StyleGAN — generated images
StyleGAN2 — generated images
StyleGAN2 — real images
Figure 9. Example images and their projected and re-synthesized counterparts. For each conﬁguration, top row shows the target images
and bottom row shows the synthesis of the corresponding projected latent vector and noise inputs. With the baseline StyleGAN, projection
often ﬁnds a reasonably close match for generated images, but especially the backgrounds differ from the originals. The images generated
using StyleGAN2 can be projected almost perfectly back into generator inputs, while projected real images (from the training set) show
clear differences to the originals, as expected. All tests were done using the same projection method and hyperparameters.
LSUN CAR, StyleGAN
FFHQ, StyleGAN
LSUN CAR, StyleGAN2
FFHQ, StyleGAN2
Figure 10. LPIPS distance histograms between original and projected images for generated (blue) and real images (orange). Despite the higher image quality of our improved generator, it is
much easier to project the generated images into its latent space
W. The same projection method was used in all cases.
being able to attribute a fake image to its speciﬁc source .
With StyleGAN, this amounts to checking if there exists a
w ∈W that re-synthesis the image in question.
We measure how well the projection succeeds by computing the LPIPS distance between original and resynthesized image as DLPIPS[x, g(˜g−1(x))], where x is the
image being analyzed and ˜g−1 denotes the approximate projection operation. Figure 10 shows histograms of these distances for LSUN CAR and FFHQ datasets using the original StyleGAN and StyleGAN2, and Figure 9 shows example projections. The images generated using StyleGAN2
can be projected into W so well that they can be almost
unambiguously attributed to the generating network. However, with the original StyleGAN, even though it should
technically be possible to ﬁnd a matching latent code, it appears that the mapping from W to images is too complex
for this to succeed reliably in practice. We ﬁnd it encouraging that StyleGAN2 makes source attribution easier even
though the image quality has improved signiﬁcantly.
6. Conclusions and future work
We have identiﬁed and ﬁxed several image quality issues in StyleGAN, improving the quality further and considerably advancing the state of the art in several datasets.
In some cases the improvements are more clearly seen in
motion, as demonstrated in the accompanying video. Appendix A includes further examples of results obtainable using our method. Despite the improved quality, StyleGAN2
makes it easier to attribute a generated image to its source.
Training performance has also improved.
resolution, the original StyleGAN (conﬁg A in Table 1)
trains at 37 images per second on NVIDIA DGX-1 with
8 Tesla V100 GPUs, while our conﬁg E trains 40% faster
at 61 img/s. Most of the speedup comes from simpliﬁed
dataﬂow due to weight demodulation, lazy regularization,
and code optimizations. StyleGAN2 (conﬁg F, larger networks) trains at 31 img/s, and is thus only slightly more
expensive to train than original StyleGAN. Its total training
time was 9 days for FFHQ and 13 days for LSUN CAR.
The entire project, including all exploration, consumed
132 MWh of electricity, of which 0.68 MWh went into
training the ﬁnal FFHQ model. In total, we used about
51 single-GPU years of computation (Volta class GPU). A
more detailed discussion is available in Appendix F.
In the future, it could be fruitful to study further improvements to the path length regularization, e.g., by replacing
the pixel-space L2 distance with a data-driven feature-space
metric. Considering the practical deployment of GANs, we
feel that it will be important to ﬁnd new ways to reduce the
training data requirements. This is especially crucial in applications where it is infeasible to acquire tens of thousands
of training samples, and with datasets that include a lot of
intrinsic variation.
Acknowledgements
We thank Ming-Yu Liu for an early
review, Timo Viitanen for help with the public release,
David Luebke for in-depth discussions and helpful comments, and Tero Kuosmanen for technical support with the
compute infrastructure.