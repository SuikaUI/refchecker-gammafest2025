Proceedings of the 2021 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, pages 3511–3535
June 6–11, 2021. ©2021 Association for Computational Linguistics
FUDGE: Controlled Text Generation With Future Discriminators
Kevin Yang
UC Berkeley
 
UC Berkeley
 
We propose Future Discriminators for Generation (FUDGE), a ﬂexible and modular method
for controlled text generation.
Given a preexisting model G for generating text from a distribution of interest, FUDGE enables conditioning on a desired attribute a (for example, formality) while requiring access only to G’s output logits. FUDGE learns an attribute predictor
operating on a partial sequence, and uses this
predictor’s outputs to adjust G’s original probabilities. We show that FUDGE models terms
corresponding to a Bayesian decomposition of
the conditional distribution of G given attribute
a. Moreover, FUDGE can easily compose predictors for multiple desired attributes. We evaluate FUDGE on three tasks — couplet completion in poetry, topic control in language generation, and formality change in machine translation — and observe gains in all three tasks.
Introduction
Recent advances in large pretrained language models allow us to generate increasingly realistic text
by modeling a distribution P(X) over natural language sequences X. The distribution P(X) may
be truly unconditional, as is common in language
modeling, or it may model P(X|I) conditioned on
some input I, as in machine translation or summarization.
We are frequently interested in controlled text
generation, the task of generating text conditioned
on an additional desirable attribute a which is not
already built into P(X). That is, we would like to
model P(X|a) (or possibly P(X|I, a); henceforth
we will drop I from the notation for simplicity).
For example, P(X) may be a pretrained translation model for Spanish inputs I to English outputs
X, but we may wish to additionally constrain the
outputs to possess a new attribute a, e.g., formality,
which we did not optimize for during training.
Unfortunately, once we have already obtained
an unconditioned P(X) deﬁned as the output distribution of some large generative model G, it is
nontrivial to add conditioning on a new attribute a
without either training a new model from scratch
or ﬁne-tuning with additional data. Although in
principle we can trivially sample from P(X|a) via
rejection sampling from P(X), rejection sampling
may be highly inefﬁcient in practice. On the other
hand, while generating according to attribute a,
P(X) should be left otherwise intact: in the previous translation formality example, it is pointless
to generate formal English outputs if they do not
preserve the original Spanish meaning.
In light of these concerns, we propose Future
Discriminators for Generation (FUDGE), a ﬂexible
and modular method for modeling P(X|a) which
accesses only the output probabilities of the generative model G which deﬁnes P(X). FUDGE learns
a binary predictor for whether attribute a will become true in the complete future, based on an incomplete sequence preﬁx (Sec. 3). Multiplying
the output probabilities of this predictor with G’s
original probabilities and then renormalizing yields
a model for the desired P(X|a) via Bayes’ Rule.
We run experiments on three controlled text
generation tasks — couplet completion in poetry,
topic control in language generation, and formality change in machine translation — showing our
method’s broad applicability.
Additionally, we
demonstrate the modularity of FUDGE by composing multiple attribute constraints in both the
couplet and topic control tasks. In our experiments,
we ﬁnd that FUDGE is highly effective at attribute
control, outperforming both a baseline which directly ﬁne-tunes G and also a strong gradientbased method ). Our
code is available at 
naacl-2021-fudge-controlled-generation.
Related Work
Ideally, a controlled text generation method should
efﬁciently control for a while preserving P(X)
as much as possible. Recent work on controlled
text generation has greatly advanced our ability
to control for a required attribute a ﬂexibly and
cheaply, with varying degrees of modiﬁcation to
the original model G which deﬁnes P(X).
One line of work ﬁne-tunes a pretrained model
for a desired attribute . The result is a
class-conditional language model (CCLM). However, it is difﬁcult to isolate the desired attribute
from the distribution shift between G and the ﬁnetuning dataset , i.e., it is nontrivial to preserve the desirable qualities of the P(X) modeled
by G. One may also need to ﬁne-tune separately
for each attribute of interest. CTRL partially addresses these issues by providing 55 attribute control codes for a large language
model trained from scratch, although this is expensive. Very recently, GEDI 
achieves strong performance by using CCLM generators as discriminators, though it relies on several
heuristics. More broadly, text generation models
for style transfer , summarization ,
and machine translation can also be viewed
as CCLM’s for different “attributes.”
A second type of approach instead conditions on
a desired attribute by backpropagating gradients, either to directly modify model activations or to ﬁnd a trigger
string . Such methods
often exhibit a high degree of attribute control, and
can be used in adversarial attacks . In fact, Subramani et al. show that by
carefully modifying the latent state, one can cause
the base G to produce arbitrary outputs.
A third class of methods, referred to as weighted
decoding (WD), assumes access only to P(X) (i.e.,
G’s output logits), and operates directly on these
logits .
Compared to other approaches, WD methods are
relatively interpretable in how they obtain P(X|a)
from P(X), but prior WD implementations have
been observed to perform poorly in controlled text
generation .
While FUDGE shares a Bayesian motivation with
other WD methods, FUDGE follows the Bayesian
factorization more closely in implementation (Sec.
3). The key distinguishing feature of FUDGE is
that it models whether attribute a will be true in
the future, rather than in the present. We ﬁnd that
FUDGE substantially outperforms previous WD
approaches in our experiments (Sec. 4.2).
Future Discriminators for Generation
We now explain the details of our proposed method,
Future Discriminators for Generation (FUDGE),
and show that it corresponds to modeling the desired conditional distribution P(X|a).
For a given language generation task, assume
we have an autoregressive model G (e.g., a
large pretrained language model) which models
P(xi|x1:i−1) for tokens x1 . . . xi. Letting X =
x1:n denote a completed sequence, G can sample
from P(X) = P(x1:n) one token at a time by factoring P(X):
P(xi|x1:i−1)
To condition on attribute a, we instead model
P(X|a). This requires a model for P(xi|x1:i−1, a),
modifying the previous factorization:
P(xi|x1:i−1, a)
If we model P(xi|x1:i−1, a) directly, we obtain a
class-conditional language model (CCLM). We can
learn the CCLM by e.g., ﬁne-tuning G depending
on the available data, possibly with some structural
modiﬁcation to G to accommodate conditioning.
However, FUDGE instead relies on the following Bayesian factorization, exchanging xi and a
conditioned on x1:i−1:
P(xi|x1:i−1, a) ∝P(a|x1:i)P(xi|x1:i−1)
The second term is exactly the quantity modeled by the base G. It then sufﬁces to model the
ﬁrst term, P(a|x1:i), with a binary classiﬁer B for
the attribute a given a preﬁx x1:i. Intuitively, one
can view B as rescoring or reranking G’s original
hypotheses.
We emphasize that although B takes a preﬁx x1:i
as input, it predicts whether attribute a will in the
future be satisﬁed for the completed generation
x1:n. For instance, suppose we are given a dataset
of examples {(x1:n, a′)} with a′ being the values
of binary indicators for the desired a (i.e., if a is
formality, then a′ is 0 or 1 when x1:n is informal
Figure 1: Illustration of one decoding step in FUDGE, for an example where the desired attribute a is formality.
A large pretrained model G (dark blue) outputs unconditioned probabilities. Our binary predictor (red) predicts
whether the eventual completed sequence will be formal for each possible continuation (computed for each candidate x3, e.g., “want”; holding a ﬁxed). The probabilities for each x3 are multiplied (purple) and then renormalized
to obtain P(x3|x1:2, a), from which we sample the next token x3 =“prefer.”
or formal respectively). For each training example (x1:n, a′), we train our classiﬁer B using all
pairs (x1:i, a′); that is, we construct a separate example from each preﬁx x1:i of x1:n. Our approach
contrasts with previous methods such as Dathathri
et al. , which greedily optimize for a on the
immediate extension x1:i+1. One particular beneﬁt is that FUDGE naturally plans for the future:
in the example for generating text on the “space”
topic in Table 6, FUDGE writes about a “mysterious ship” despite “ship” itself not being in the
given “space”-topic bag of words, because “mysterious ship” easily leads into a mention of one of
the targeted “space” words (“Earth”). Similarly,
in the ﬁrst couplet completion example in Table 3,
FUDGE needs to rhyme with “fear” after exactly
ten syllables. After seven syllables, it could reasonably generate the word “clear,” but it ﬁrst generates
the adverb “pretty” in order to set up the generation
of “clear” as the tenth syllable.
FUDGE’s implementation is shown schematically in Figure 1, and is quite simple in practice.
FUDGE just needs to learn a B (red in Figure 1)
sharing tokenization with G (dark blue). It then
converts B’s output into probabilities (red table in
Figure 1), and multiplies with the original output
probabilities from G (dark blue table), to obtain unnormalized probabilities P(xi, a|x1:i−1) (purple table). Finally, renormalizing over the output vocabulary yields the desired distribution P(xi|x1:i−1, a).
In practice, we operate in the log-probability space
for numerical stability.
To improve computational efﬁciency, we typically choose B to be lightweight relative to G. We
also consider only the top 200 possibilities for xi
according to G at each step, as a cheap approximation to the full distribution, and ﬁnd that this
works well in practice.1 In each task in Sec. 4,
running FUDGE on the test set takes no more than
15 minutes on a single Quadro RTX 6000 GPU.
Finally, as with other controlled generation approaches such as Dathathri et al. , it is likely
that augmenting FUDGE with reranking approaches
such as rejection sampling could improve output
quality at the cost of compute time, although we
do not comprehensively evaluate such extensions
in this work.
Advantages and Limitations
We highlight several additional potential advantages of FUDGE compared to directly modeling
P(xi|x1:i−1, a) via e.g., a ﬁne-tuned CCLM:
1. FUDGE requires access only to P(X) (i.e.,
G’s output logits) rather than G itself.
2. G can be freely swapped out for any other
model that shares the same tokenization when
larger models become available.
3. Given multiple conditionally independent attributes with predictors for each, FUDGE can
easily condition on the combination of these
attributes in a modular fashion by summing
their output log-probabilities (Sec. 4.1, 4.2).
Unfortunately, like previous methods, FUDGE
cannot fully guarantee that all outputs possess the
desired attribute a. In FUDGE’s case, this is due to
the approximation inherent in modeling P(a|x1:i),
as well as only considering the top 200 possible xi
for computational efﬁciency.
1See Appendix H for ablations on the top-200 pruning.
Experiments
We run experiments on a range of controlled text
generation tasks to evaluate the effectiveness of our
proposed method: poetry couplet completion (Sec.
4.1), topic-controlled language generation (Sec.
4.2), and machine translation formality change
(Sec. 4.3). For each task we discuss the evaluation setup, the speciﬁc details of our method and
baselines, and ﬁnally experimental results.
Poetry Couplet Completion
So long as men can breathe or eyes can see,
So long lives this and this gives life to thee.
Table 1: An example couplet by William Shakespeare.
Every second syllable is stressed, following iambic meter, and the last words of each line (see/thee) rhyme.
We begin with English poetry generation, a task
that emphasizes well-formedness, and which has
been studied in different forms by many previous works . Our task
here is couplet completion. Given the ﬁrst line of
an iambic pentameter couplet (e.g., Table 1), the
model must generate a second line which (1) satisﬁes iambic pentameter, (2) rhymes with the ﬁrst
line, and (3) ends a sentence. The desired attribute
a is deﬁned as possessing all three properties, as
evaluated by a rule-based checker F (Appendix
A). Our test set is a collection of preﬁx lines of
couplets, collected from the ending couplet of each
of Shakespeare’s 154 sonnets.
Metrics. We consider four metrics.
1. Success, the fraction of couplet completions
with the desired attribute a, as checked by F.
This is the main metric.
2. Grammaticality, the probability of grammaticality given by a Roberta-based CoLA grammaticality model , averaged over all outputs.
3. Perplexity of the completion conditioned on
the preﬁx. Following Dathathri et al. ,
since our models use GPT2-Medium as G, we evaluate perplexity using
GPT .2
2See Appendix E for other perplexity measurements.
4. Distinctness of completions, measured as the
number of unique unigrams, bigrams, and trigrams across all samples, divided by the total
number of words .
At test time, we decode until the model generates ten syllables followed by an end-of-sentence
punctuation mark, or after the eleventh syllable (an
automatic failure, since iambic pentameter requires
exactly ten syllables).
Overall, because we deﬁne a using a rule-based
F which is accessible during training, our formulation of couplet completion is a relatively clean task
for evaluating the effectiveness of FUDGE.
Method and Baselines
FUDGE Instantiation. The obvious approach is to
learn a predictor for F directly. However, the three
components of a — meter, rhyme, and sentenceending — should be roughly independent. Thus we
assume conditional independence, and demonstrate
the modularity of FUDGE by constructing three
separate predictors to be combined at test time:
1. B1(x1:i) takes a text preﬁx x1:i, and predicts
whether the completion x1:n of preﬁx x1:i will
be in iambic meter. The model is an LSTM
followed by a linear output layer.
2. B2(x1:i, t, r) takes preﬁx x1:i, the number of
syllables t between xi and xn for n ≥i,
and a rhyme sound r.3 It predicts whether
the completion x1:n has the rhyme sound r
at the end of token xn.
The model is an
LSTM with attention dependent on t and r,
followed by a shallow feedforward network,
and is trained via noise-contrastive estimation
 .4
3. B3(x1:i, t) takes preﬁx x1:i and the number
of syllables t between xi and xn for n ≥i,
and predicts whether xn ends a sentence. The
model is an LSTM followed by a shallow feedforward network.
The predictors vary in architecture because B2 and
B3 require inputs other than x1:i — in truth, they
are families of related predictors. We ﬁnd that performance is not overly sensitive to the particulars
of the predictor architectures (Appendix D).
3Two words have the same “rhyme sound” r if they rhyme
according to the CMU Pronouncing Dictionary .
4The output logits from B2 are unnormalized, but this
does not affect FUDGE after they are added to the output logits
of G and softmaxed for sampling.
Correctness
Text Quality
Perplexity ↓
44.3 ± 42.2
55.8 ± 98.3
60.8 ± 66.1
70.9 ± 89.4
Shakespeare
333.8 ± 418.9
Table 2: Couplet completion results. Success (main metric), grammaticality, perplexity, and distinctness of different methods, tested on 154 preﬁx lines from Shakespeare sonnets. FUDGE substantially outperforms automated
baselines on success and maintains high diversity, although quality unsurprisingly suffers compared to the base G
due to the difﬁcult constraint F. Note Shakespeare’s work is often “incorrect” due to the narrowness of our metric
F;6 he also scores poorly on text quality because our evaluation models are intended for more modern English.
To train the discriminators, we sample a dataset
of 10 million generations of varied length from
GPT2-Medium. From these generations, we sample random subsequences x1:n of roughly 10 to
30 syllables and truncate t ≤10 ending syllables.
These truncations become inputs x1:i to the predictors. For simplicity, we did not balance the class
labels for e.g., the iambic predictor during training,
although it is likely that doing so would improve
performance.
At test time, we extract r from the given ﬁrst
line of the couplet, and initialize t = 10, updating
at each step. We then modify the output logits of
G by simply adding the log-probabilities from B1,
B2, and B3, demonstrating the ease of composing
constraints in FUDGE.
Baselines. We compare to four baselines.5
1. G, the original GPT2-Medium.
2. FINETUNE, a CCLM which ﬁnetunes G on
similar inputs to those used for B2 in FUDGE.
Since it is not obvious how to compose multiple CCLM’s for different attributes, we train
a single CCLM for all desired properties together. We condition by preﬁxing the input
with (1) whether the last 10 syllables of the
original untruncated x1:n are iambic, (2) the
5A system like Hafez ,
which enforces meter and rhyme at each decoding step using
a hard constraint, could achieve perfect success rate. However, this approach relies on the meter and rhyme attributes
being “preﬁx-checkable” at the word level: one can guarantee
success by simply never selecting a word which immediately
violates the constraint. This is often the case for simple rulebased constraints, but not for many other interesting attributes,
such as the topic and formality attributes in our subsequent
experiments. To preserve generality, FUDGE does not rely on
this “preﬁx-checkable” property, and neither do our baselines.
rhyme sound at the end of xn, and (3) whether
a sentence ends with xn. A special token is
inserted 10 syllables from the end of x1:n.
3. PPLM , which uses shallow predictors learned from G’s top-level hidden layer to modify G’s states toward increasing probability of the desired attribute via gradient ascent. We decompose the predictors
into the same iambic, rhyme sound, and endof-sentence predictors as for FUDGE, inserting
an additional hidden layer in the shallow predictor when needed to incorporate additional
input (the desired rhyme sound and/or number
of syllables until end-of-sentence).
4. Shakespeare’s original couplet completions.
All non-Shakespeare methods use top-k sampling with k = 10.
Even though our GPT2-Medium-generated training dataset is completely different from the test
domain, and contains essentially zero examples of
correct couplets, FUDGE is able to learn the desired
attribute. As shown in Table 2, FUDGE greatly outperforms all automated baselines in success rate.
Surprisingly, the PPLM baseline achieves zero
success. We ﬁnd that its iambic and rhyme predictors are very poor, so we hypothesize that the
relevant information is not easily extractable from
the last hidden layer of G. In contrast, FUDGE’s
predictors operate directly on the raw text.
Funnily enough, FUDGE even matches Shakespeare according to F, although this is largely due
to the narrowness of F and should not be taken se-
riously.6 Similarly, the grammaticality and perplexity metrics are designed for our automated baselines, and thus assign poor scores to Shakespeare’s
antiquated and ﬂowery style.
FUDGE also maintains relatively ﬂuent generation despite lower grammaticality and perplexity
compared to G. See Table 3 for two successful examples. Interestingly, FUDGE also increases diversity compared to G, perhaps due to the difﬁcult constraint F forcing FUDGE to use lower-probability
regions of the base distribution P(X).
And even thence thou wilt be stol’n, I fear,
for this shall be the end. That’s pretty clear.
Or, if they sleep, thy picture in my sight
I will be glad to look upon the night.
Table 3: Two examples of successful couplet completions (in purple) generated by FUDGE.
Finally, it is possible (and trivial) to adjust the
conditioning strength in FUDGE by multiplying the
binary predictors’ output logits by a constant. However, this deviates from our Bayesian factorization
of P(X|a), and we do not do so.
Topic-Controlled Language Generation
Next, we explore topic control in English language
generation. The desired attribute a is to be on-topic
for a given topic, such as science or politics. To
facilitate comparison with prior work, we largely
follow the setup of PPLM :
the model is provided an approximation to the topic
at test time, in the form of a bag of on-topic words
W. The goal is to sample text according to the topic
approximated by W, starting from a generic preﬁx.
There are 7 topics (space, politics, military, legal,
science, religion, and computers) and 20 preﬁxes,
and the model generates 3 80-token7 samples from
each topic-preﬁx pair, for a total of 420 generations.
Metrics. Unfortunately, we cannot easily construct a rule-based F for being “on-topic.” Addi-
6 We deﬁne F using somewhat narrow criteria (Appendix
A), which capture only a subset of what Shakespeare considered to be well-written couplets. The purpose of this task is to
evaluate FUDGE’s ability to satisfy a difﬁcult well-formedness
constraint compared to automated baselines, rather than to
perfectly capture the human notion of an iambic pentameter
couplet. Thus Shakespeare is marked wrong when he (1) uses
archaic pronunciations, (2) uses loose rhymes, (3) elides syllables to ﬁt meter, or (4) uses words missing from the CMU
Pronouncing Dictionary. See Appendix A.1 for details. Of
course, Shakespeare is only included as a whimsical point of
reference; our generations obviously do not hold a candle to
Shakespeare’s originals.
7All models and baselines use GPT2 tokenization.
tionally, use rate of words in W is a poor metric,
because a model can score highly by e.g., simply returning the words in W, without generalizing to the
full topic that W approximates. Instead, we adopt a
notion of success which requires the model to generalize the bag W to the full topic. The remaining
metrics are measures of quality and diversity.
1. Success, the average number of distinct words
in a heldout bag W′ which appear in the model
output. Speciﬁcally, for each word in W, we
add to W′ the closest GloVe word by cosine similarity, such that
the new word does not contain (and is not
contained by) any word in W. (This excludes
e.g., most plurals.) Usage of distinct words in
W′ measures the model’s ability to generalize
W to other on-topic words, of which W′ is a
non-exhaustive set. This is our main metric.
2. Grammaticality, identical to the couplet task.
3. Perplexity, identical to the couplet task.
4. Distinctness, deﬁned as in the couplet task.
However, it is calculated separately within
the 60 generations for each topic, and then
averaged over the 7 topics.
Additionally, following the evaluation procedure
of prior work such as , we
run human evaluations via Amazon Mechanical
Turk for FUDGE against each baseline, comparing
topic control and ﬂuency. For each pairwise comparison, we ask 3 workers to evaluate each of 420
paired outputs. Workers were asked to mark which
generation is more on topic (ﬁrst, second, both, or
neither), and to rate each generation’s ﬂuency on
a Likert scale from 1 to 5. We report the average
fraction of outputs marked as on-topic as well as
the average ﬂuency rating for each method.
Method and Baselines
FUDGE Instantiation. Since we model topics as
bags of words, FUDGE uses a binary predictor
B(x1:i, w) which takes a preﬁx x1:i and word w,
and classiﬁes whether w appears in the future xi:n
for n ≥i. (Since it is desirable to stay on topic
even after successfully getting on topic, we use xi:n
rather than x1:n.) Training examples (x1:i, w) are
sampled from the same dataset of 10 million GPT2-
Medium generations used for the couplet task, and
B is trained using noise-contrastive estimation. B
Text Quality
Perplexity ↓
37.1 ± 26.9
24.9 ± 13.7
33.8 ± 33.7
43.1 ± 23.7
40.7 ± 26.3
Table 4: Topic control results. Success (main metric), grammaticality, perplexity, and distinctness for different
methods. FINETUNE and WDEC often degenerate into repeating the given bag of words W; this is ill-captured by
perplexity, but results in poor grammaticality and distinctness. FUDGE substantially outperforms all baselines on
success, including the strong gradient-based PPLM baseline, while preserving high quality and diversity.
is a lightweight LSTM-based classiﬁer similar to
B2 from the couplet task.
At test time, we can compose individual-word
constraints if we assume conditional independence
between words (although this may be imperfect).
Given a bag of N words {w1 . . . wN} and pre-
ﬁx x1:i, we could condition on all words in the
bag appearing in the future by adding all logprobabilities log P(w1|x1:i) . . . log P(wN|x1:i) to
G’s logits. However, topic control does not require
every word to appear; perhaps some number λ of
on-topic words is enough to be “on-topic.” Therefore, we model the topic constraint as selecting a
random subset of λ words from the original bag,
and requiring that only those λ words all appear.
Since each of the N words is selected with probability λ
N , the quantity we add to the base G logits is
j=1 log P(wj|x1:i) in expectation. In our experiments we use λ = 4, based on a fantasy-topic
bag of words used for validation (Appendix C).
Baselines. We compare to four baselines.
1. G, the original GPT2-Medium.
2. FINETUNE, which ﬁnetunes G on the same
inputs used for FUDGE. The future word is
given as a preﬁx for conditioning. At test time,
we compute logits for each preﬁx in the given
W and use the average as the true logits, as
an ad hoc way to condition on the full W.
3. WDEC, a simple weighted decoding implementation which greedily considers only the
immediate next token when optimizing for a.
Instead of using B, WDEC just adds a ﬁxed
λWDEC to the logit for each word in W. Note
WDEC requires a to be well-deﬁned at the
token level, so it is not easily transferable to
certain tasks (e.g., couplet completion).
4. PPLM , which modiﬁes
the activations of G to make the desired bag of
words more likely at the immediate next position. We use their method without reranking
for fair comparison.
All methods use top-k sampling with k = 10,
following Dathathri et al. ’s setup.
Topic Fluency
Table 5: Topic control human evaluations, pairwise
comparisons. FUDGE achieves a substantially higher
fraction of on-topic outputs compared to each baseline,
in addition to higher average ﬂuency (rated 1 to 5).
FUDGE achieves the highest success by a substantial margin (Table 4), and outperforms all baselines on human evaluations in both topic relevance
and ﬂuency (Table 5). FUDGE simultaneously preserves high quality and diversity according to automated metrics. Table 6 shows two examples.
Unsurprisingly, G performs poorly on success.
WDEC and FINETUNE also perform poorly, in success and especially in distinctness.
WDEC frequently degenerates into repeating the given words
in the bag W, despite tuning λWDEC (Appendix C).
Space: The issue focused on the original plot, which was
about a mysterious ship that would land on Earth, and
would lead to humanity’s ﬁrst interstellar expedition. The
original plan called for humanity to use the spacecraft to
colonize outer space and build the ﬁrst city on Mars. But
this idea fell by the wayside in the ﬁnal drafts.\n\n"It was
just not a very popular idea and it wasn’
Politics: The issue focused on whether the two institutions
were operating within the bounds set by the constitution
and the law.\n\nThe Constitutional Court said that both
governments "have a duty to ensure the integrity of the
electoral process and its effective administration, especially
in light of the current political climate that is threatening
the functioning of elections"
Table 6: The ﬁrst output from FUDGE when using the
preﬁx “The issue focused on” for two topics. We use
red to highlight words in the given bag of words W
along with obvious forms (e.g., plurals), and cyan for
other on-topic words, including related words not in the
heldout bag W′. More examples in Appendix J.
FINETUNE also suffers from repetition, which appears to be the result of distribution shift from ﬁnetuning. Our ﬁne-tuning dataset was built by sampling directly from the original P(X) modeled by
G to mitigate distribution shift, but it is well-known
that language model generations are more repetitive than natural language . We hypothesize that FINETUNE, being ﬁnetuned on language model generations rather than
natural language, ampliﬁes this repetitiveness. This
repetition is reﬂected in the poor grammaticality for
both FINETUNE and especially WDEC. In contrast,
FUDGE does not touch the original P(X), largely
avoiding FINETUNE’s distribution shift problem on
this task.
Finally, FUDGE outperforms the strong gradientbased PPLM method, despite requiring access only
to G’s output logits. Non-reliance on gradients
means FUDGE is also many times faster than PPLM,
which takes a few hours compared to FUDGE’s
15 minutes for the full set of 420 generations on
our hardware. Sometimes we do not even have
gradients: for example, gradients are unavailable
in the API for GPT3 at time of writing.
Machine Translation Formality Change
Finally, we turn to a somewhat more challenging
task, changing formality in machine translation
— speciﬁcally, from informal to formal. Given a
source sentence written in an informal and conversational style, the goal is to output a translation which is also more formal. We test on the
Fisher and CALLHOME Spanish–English Speech
Translation Corpus , a collection
of transcribed Spanish conversations with English
translations. Both the source Spanish and target
English are highly informal and disﬂuent. Salesky
et al. augment the Fisher dataset with additional parallel English translations, rewritten to be
more ﬂuent (and hence more formal); see Table 7
for an example. Our task is to translate the original informal Spanish to into more formal English.
However, we assume that Salesky et al. ’s
ﬂuent references are unavailable during training.
entonces de verdad sí sí pero entonces tu estudiando para
es es digo es más porque es exactamente
Then, if it’s business, but then you are a student for a PHD,
the Master’s is that exactly.
If it’s business, then you are a student for a PhD. The
masters is exactly that.
Table 7: An example from the Fisher dataset.
Top: The original Spanish transcription.
Middle: The original English translation.
Bottom: Salesky et al. ’s more ﬂuent version.
Metrics. The desired attribute a is formality,
but we cannot sacriﬁce the source sentence’s meaning. The latter requirement makes generation more
constrained than in the couplet and topic tasks, so
perplexity and distinctness are less relevant. Instead, we use the following:
1. BLEU Score , using two
of Salesky et al. ’s ﬂuent references per
test example. This is our main metric.
2. Formality, the average probability that the
model’s outputs are formal, according to an
evaluator trained on the Family/Relationships
domain of the GYAFC formality dataset .
The evaluator is an
LSTM followed by a linear layer.
Method and Baselines
FUDGE Instantiation.
We assume that the attribute a, formality, is conditionally independent
from the original conditioning in G, i.e., the meaning of the Spanish input. FUDGE uses a binary
predictor B(x1:n) which classiﬁes whether the text
starting with preﬁx x1:n is written in a formal style.
B is an LSTM followed by a linear layer, trained
on the Entertainment/Music domain of GYAFC.
At test time, FUDGE directly augments G’s logits using log-probabilities from B.
G is a pretrained Marian 
transformer model for Spanish-English. We evaluate both when G is ﬁne-tuned on the original Fisher
training dataset ’s more ﬂuent targets) as well
as zero-shot with no ﬁne-tuning, which is challenging due to the highly informal and disﬂuent text.
Baselines. We compare to two baselines.
1. G, the original machine translation model.
2. G + ST, a pipeline consisting of G followed
by a style transfer model. Our style transfer
model is T5 , ﬁne-tuned
on the same GYAFC Entertainment/Music domain that we used to train B in FUDGE.
Since we do not assume access to Salesky et al.
 ’s more formal targets during training, it is
difﬁcult to apply PPLM to this task: PPLM’s predictor would operate on the pretrained translation
model’s hidden states, thus requiring a Spanish-
English translation dataset with both formal and
informal English.8 We omit FINETUNE for the
same reason. In contrast, FUDGE requires only the
original English dataset with formality annotations.
All methods use greedy decoding.
G (No ﬁne-tune)
G (Fine-tune)
Method BLEU ↑Form. ↑BLEU ↑Form. ↑
Table 8: Machine translation formality results. BLEU
(main metric) and average formality for different methods, with and without ﬁne-tuning G on the Fisher domain. FUDGE increases the formality of translations
compared to the base model G while preserving or increasing BLEU score. Conversely, G with style transfer
overﬁts to the formality data, resulting in near-perfect
formality but losing the original meaning.
As shown in Table 8, FUDGE increases the formality of outputs compared to G, even though the
test-time formality predictor is trained on a different domain (Family/Relationships, rather than
Entertainment/Music). Note that formality unsurprisingly decreases after ﬁne-tuning G, simply due
to the informality of the ﬁne-tuning dataset. As in
8We nevertheless ran PPLM in a somewhat convoluted
setup, but found that it performed poorly (Appendix B).
the couplet task, one could adjust the strength of
the formality control in FUDGE, although this is
unprincipled from the view of modeling P(X|a).
Moreover, while FUDGE and G achieve similar
BLEU after ﬁne-tuning G, FUDGE achieves higher
BLEU compared to G when G is not ﬁne-tuned on
the Fisher training set. In the latter case, controlling
for formality somewhat remedies the struggles of
G when not ﬁne-tuned on such disﬂuent text.
In contrast, the G + ST baseline achieves nearperfect formality but less than half the BLEU of
G, due to the style transfer model overﬁtting to
the GYAFC Entertainment/Music dataset. This
is similar to the distribution shift issue that we
observed in topic control for FINETUNE, an issue
which FUDGE largely avoids. Nevertheless, there
remains substantial room for improvement on this
difﬁcult task.
que era lo que tenía que tienes que hacer
that was what you had to do
That was what you had to do
What’s there to do?
ah en mi en inglaterra por ejemplo
Ah, in my, in England, for example.
Ah, in England, for example.
In England, for example?
Table 9: Example translations by G (ﬁne-tuned on the
Fisher dataset) and FUDGE using the same G. Original Spanish and Salesky et al. references also
shown. In this setting, FUDGE achieves similar BLEU
to G while increasing formality. While FUDGE often
simply corrects punctuation or capitalization (top), it
also makes more complex adjustments (bottom). More
examples in Appendix L.
Discussion
FUDGE is a principled approach to controlled text
generation which models P(X|a) by closely following a Bayesian factorization, thus preserving
the base P(X) as much as possible.
achieves strong performance on a wide range of
different tasks: poetry couplet completion, topic
control, and informal-to-formal machine translation. Additionally, FUDGE can easily compose
different attributes in a modular fashion: the meter,
rhyme, and end-of-sentence constraints for couplet
completion, and the individual words within each
topic bag for topic control. In principle, FUDGE is
applicable to any controlled generation task where
we can train discriminators for the desired attribute
or attributes.
Ethics of Controlled Text Generation
We recognize that strong controlled generation
methods have the potential to produce harmful outputs and/or misinformation when used adversarially . However, such
methods can also be a powerful tool for mitigating
harmful biases learned by large pretrained language
models ,
for example by detoxifying language . Overall, we believe it is still beneﬁcial to continue research into
general controlled text generation methods such as
Acknowledgements
We thank Daniel Fried, David Gaddy, Eric Wallace,
Kevin Lin, Nicholas Tomlin, Ruiqi Zhong, and
the three anonymous reviewers for their helpful
comments and feedback, which aided us in greatly
improving the paper. We also thank the authors of
Dathathri et al. for clarifying our questions
about their topic control setup. This work was
supported by Berkeley AI Research, DARPA under
agreement HR00112020054, and the NSF through
a fellowship to the ﬁrst author. The content does
not necessarily reﬂect the position or the policy
of the government, and no ofﬁcial endorsement
should be inferred.