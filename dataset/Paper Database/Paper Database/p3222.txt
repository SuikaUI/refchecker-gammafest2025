List of Tables
Guaranteed equality between µ and the upper bound . . . . . . . . . . . . . . . .
List of Figures
M −∆Feedback Connection
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Linear Fractional Transformation . . . . . . . . . . . . . . . . . . . . . . . . . . .
General Star Product
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Mass-Spring-Damper System
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Example Interconnection of LFT’s
. . . . . . . . . . . . . . . . . . . . . . . . . .
Macroscopic representation of Figure 5 . . . . . . . . . . . . . . . . . . . . . . . .
Scaling for Main Loop Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Equivalent LFT’s . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Uncertain System for Robustness Tests . . . . . . . . . . . . . . . . . . . . . . . .
Uncertain System as an LFT
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
The Complex Structured Singular Value
Andy Packard
John Doyle
Mechanical Engineering
Electrical Engineering, 116-81
University of California
Berkeley, CA, USA 94720
Pasadena, CA USA 91125
 
 
(fax) 510.642.6163
July 17, 1992, to appear, Automatica, January 1993
A tutorial introduction to the complex structured singular value (µ) is presented, with
an emphasis on the mathematical aspects of µ. The µ-based methods discussed here have
been useful for analyzing the performance and robustness properties of linear feedback systems. Several tests for robust stability and performance with computable bounds for transfer
functions and their state-space realizations are compared, and a simple synthesis problem is
studied. Uncertain systems are represented using Linear Fractional Transformations (LFTs)
which naturally unify the frequency-domain and state-space methods.
Subtitle: A tutorial introduction to the complex structured singular value (µ) is presented, with
an emphasis on computable bounds and robust stability and performance tests for transfer
functions and their state-space realizations.
Keywords: Computational methods, Control system analysis, Disturbance rejection, Frequency
domain, Matrix algebra, Multivariable control systems, Performance bounds, Robust control, Sensitivity analysis, State-space methods.
Introduction
This paper gives a fairly complete introduction to the Structured Singular Value (µ) for complex
perturbations. This paper is intended to be of tutorial value on the mathematical aspects of µ,
and it is assumed that the reader is familiar with the control engineering motivation. The µ-based
methods discussed here have been useful for analyzing the performance and robustness properties
of linear feedback systems.
The more elementary methods are now available in commercial
software products and the manual for one such product would serve as
a tutorial introduction to the engineering motivation. The interested reader might also consult
the tutorial in or other application-oriented papers, such as . We present very few new results in this paper, although many of the results
have appeared only in reports and conference proceedings. The paper is reasonably self-contained,
skipping only those proofs which are readily available in the literature.
Section 3 begins with the deﬁnition of µ and some of its elementary properties, including simple
bounds that form the basis for computational schemes. This section also introduces the relationship between the upper bound for µ and Linear Matrix Inequalities (LMIs), which results in
a simple characterization of the convexity properties of the upper bound. The connections between µ and Linear Fractional Transformations are introduced in Section 4. These connections,
especially the Main Loop Theorem, form the basis for most of the applications of µ to linear
systems. In Section 5, using the deﬁnition of µ, and the Main Loop theorem, robust stability
and robust performance theorems are derived for linear systems with structured linear fractional
uncertainty.
Section 6 covers a maximum-modulus theorem for linear fractional transformations. Section 7
presents a generalization of the standard power algorithms for computing the spectral radius or
maximum singular value of a matrix to the computation of µ. This power algorithm provides an
attractive method for computing lower bounds for µ. Sections 8 and 9 considers issues associated
with the upper bound, focusing particularly on conditions under which the upper bound is equal
to µ. For certain simple block structures, this equality is guaranteed.
The remainder of the paper discusses applications of µ to problems motivated by control systems.
Section 10 considers how various µ problems can be viewed in transfer function and state-space
formulations. This leads to a variety of tests for robust performance, each with an interesting and
useful interpretation. In Section 11, many (computable) necessary and suﬃcient conditions for
quadratic stability of uncertain systems are given, for a wide variety of uncertainty structures.
The proof techniques used in each diﬀerent case are identical, giving a unifying treatment of
many known and new results.
Section 12 considers a simple special case of µ-synthesis, the problem of minimizing µ as a function
of some free parameter, such as a controller.
Not surprisingly, µ-synthesis is a much harder
problem than µ-analysis. For example, unlike µ-analysis problems, no method for minimizing
the upper bound for µ in the synthesis problem using convex optimization has been found.
Finally, the paper outlines some related work in Section 13, beginning with a brief history of the
early development of the µ theory. This outline is not intended to be exhaustive or complete, but
simply to touch on a few of the topics nearest to this paper that were not considered in detail.
LMIs are presented as potentially unifying theoretical and computational tools. The relationship
between µ and quadratic versus L1 notions of robust performance and robust stability is then
discussed, followed by µ with mixed real and complex perturbations. The section ends with a
discussion of model validation and generalizations of µ.
The notation is standard. R denotes the set of real numbers; C denotes the set of complex
numbers; |·| is the absolute value of elements in R or C; Rn is the set of real n vectors; Cn is the set
of complex n vectors; ∥v∥is the Euclidean norm for v ∈Cn, ∥v∥2 := Pn
i=1 |vi|2; ln
2 denotes the set
of square summable sequences in Cn; ∥e∥2 is the l2 norm of sequence e ∈ln
k=1 ∥ek∥2;
Rn×m is the set of n × m real matrices; Cn×m is the set of n × m complex matrices; Hn is the
set of Hermitian n × n complex matrices; In is a n × n identity matrix; and 0n or 0n×m is an
entirely zero matrix of obvious dimensions. For M ∈Cn×m: MT is the transpose of M; M ∗is
the complex-conjugate transpose of M; σ(M) is the minimum singular value of M; σi(M) is a
singular value of M; ¯σ (M) is the maximum singular value of M. For M ∈Cn×n: λi (M) is an
eigenvalue of M; ρ(M) is the spectral radius of M, ρ (M) := max
i |λi(M)|; tr(M) is the trace of
M, tr(M) := Pn
i=1 Mii. If M ∈Cn×n satisﬁes M = M ∗then M > 0 denotes that M is positive
deﬁnite, and M
2 denotes the unique positive deﬁnite Hermitian square root.
Structured Singular Value
This section is devoted to deﬁning the structured singular value, a matrix function denoted by
We consider matrices M ∈Cn×n.
In the deﬁnition of µ (M), there is an underlying
structure ∆, (a prescribed set of block diagonal matrices) on which everything in the sequel
depends. This structure may be deﬁned diﬀerently for each problem depending on the uncertainty and performance objectives of the problem. Deﬁning the structure involves specifying
three things: the total number of blocks, the type of each block, and their dimensions.
In this paper, we consider two types of blocks—repeated scalar and full blocks.
Two nonnegative integers, S and F, denote the number of repeated scalar blocks and the number of
full blocks, respectively.
To bookkeep the block dimensions, we introduce positive integers
r1, . . . , rS; m1, . . . , mF .
The i’th repeated scalar block is ri × ri, while the j’th full block is
mj × mj. With those integers given, deﬁne ∆⊂Cn×n as
©diag [δ1Ir1, . . . , δSIrS, ∆S+1, . . . , ∆S+F ] : δi ∈C, ∆S+j ∈Cmj×mj, 1 ≤i ≤S, 1 ≤j ≤F
For consistency among all the dimensions, we must have PS
i=1 ri + PF
j=1 mj = n. Often, we will
need norm bounded subsets of ∆, and we introduce the notation
B∆= {∆∈∆: ¯σ (∆) ≤1}
Note that in (3.1) all of the repeated scalar blocks appear ﬁrst and the full blocks are square.
This is done to keep the notation as simple as possible and can easily be relaxed.
Deﬁnition 3.1 For M ∈Cn×n, µ∆(M) is deﬁned
min {¯σ (∆) : ∆∈∆, det (I −M∆) = 0}
unless no ∆∈∆makes I −M∆singular, in which case µ∆(M) := 0.
Remark 3.2 The set ∆deﬁnes a multi-index of integers and vice versa, so it makes sense to
identify as one object the set, the block structure, and the associated multi-index of integers and
refer simply to a block structure ∆. Clearly, µ∆(M) depends on the block structure ∆as well
as the matrix M.
Remark 3.3 Without loss in generality, the full blocks in the minimal norm ∆can each be
chosen to be dyads (rank = 1). To see this, ﬁrst consider the case of only 1 full block, ∆= Cn×n.
Suppose that I −M∆is singular. Then for some unit-norm vector x ∈Cn, M∆x = x. Deﬁne
y := ∆x. It follows that y ̸= 0, and ∥y∥≤¯σ (∆). Hence, deﬁne a new perturbation, ˜∆∈Cn×n
Note that ¯σ
= ∥y∥≤¯σ (∆), and y = ˜∆x, so that I −M ˜∆is also singular. Repeating this
on a block-by-block basis allows for each full block to be a dyad.
Remark 3.4 It is instructive to consider a “feedback” interpretation of µ∆(M) at this point.
Let M ∈Cn×n be given, and consider the loop shown in Figure 1. This picture is meant to
represent the loop equations u = Mv, v = ∆u. As long as I −M∆is nonsingular, the only
solutions u, v to the loop equations are u = v = 0. However, if I −M∆is singular, then there
are inﬁnitely many solutions to the equations, and the norms ∥u∥, ∥v∥of the solutions can be
arbitrarily large. Motivated by connections with stability of systems, which will be explored
in detail in the sequel, we call this constant matrix feedback system “unstable”. Likewise, the
term “stable” will describe the situation when the only solutions are identically zero. In this
context then, µ∆(M) provides a measure of the smallest structured ∆that causes “instability”
of the constant matrix feedback loop in ﬁgure 1. The norm of this “destabilizing” ∆is exactly
Remark 3.5 It is immediate from the deﬁnition that for any α ∈C, µ (αM) = |α|µ (M).
However, for all nontrivial block structures, the function µ : Cn×n →R is not a norm, since it
doesn’t satisfy the triangle inequality.
Remark 3.6 A natural question is why we work with µ and not 1/µ, especially in view of
equation 3.3 in the deﬁnition of µ. While it’s clearly a matter of taste, there are important
reasons. Mathematically, µ is continuous and bounded and scales as indicated above. Perhaps
more importantly, it connects more naturally with LFTs and generalizes the spectral radius and
maximum singular value, as will be seen below.
An alternative expression for µ∆(M) follows easily from the deﬁnition.
µ∆(M) = max
∆∈B∆ρ (∆M)
Proof: Since for any α ∈C, µ∆(αM) = |α|µ∆(M), we need only consider two cases:
∆∈B∆ρ (∆M) = 1 and µ∆(M) = 0 iﬀmax
∆∈B∆ρ (∆M) = 0. These facts can be veriﬁed
directly from the deﬁnition. ♯
This lemma implies continuity of the function µ:Cn×n →R based on continuity of the spectral
radius and max functions, and the compactness of B∆.
We can relate µ∆(M) to familiar linear algebra quantities when ∆is one of two extreme sets:
• If ∆= {δI : δ ∈C} (S =1, F =0, r1 =n), then µ∆(M) = ρ (M), the spectral radius of M.
Proof: This follows immediately from Lemma 3.7. ♯
• If ∆= Cn×n (S =0, F =1, m1 =n), then µ∆(M) = ¯σ (M)
Proof: If ¯σ (∆) <
¯σ(M), then ¯σ (M∆) < 1, so I −M∆is nonsingular. Applying equation
(3.3) implies µ∆(M) ≤¯σ (M).
On the other hand, let u and v be unit vectors
satisfying Mv = ¯σ (M) u, and deﬁne ∆:=
¯σ(M)vu∗. Then ¯σ (∆) =
¯σ(M) and I −M∆
is obviously singular. Hence, µ∆(M) ≥¯σ (M). ♯
Obviously, for a general ∆as in (3.1) we must have {δIn : δ ∈C} ⊂∆⊂Cn×n. Hence directly
from the deﬁnition of µ, and the two special cases above, we conclude that
ρ (M) ≤µ∆(M) ≤¯σ (M)
These bounds by themselves may proved little information on the value of µ, because the gap
between ρ and ¯σ can be arbitrarily large. They are reﬁned with transformations on M that do
not aﬀect µ∆(M), but do aﬀect ρ and ¯σ. To do this, deﬁne two subsets of Cn×n
Q = {Q ∈∆: Q∗Q = In}
©diag [D1, . . . , DS, dS+1Im1, . . . , dS+F ImF ] :
Di ∈Cri×ri, Di = D∗
i > 0, dS+j ∈R, dS+j > 0
The reasons for taking D positive will be clear shortly. Note that for any ∆∈∆, Q ∈Q, and
¯σ (Q∆) = ¯σ (∆Q) = ¯σ (∆)
Theorem 3.8 For all Q ∈Q and D ∈D
µ∆(MQ) = µ∆(QM) = µ∆(M) = µ∆
Proof: For all D ∈D and ∆∈∆,
det (I −M∆) = det
since D commutes with ∆. Therefore µ∆(M) = µ∆
. Also, for each Q ∈Q,
det (I −M∆) = 0 if and only if det (I −MQQ∗∆) = 0. Since Q∗∆= ∆and ¯σ (Q∗∆) =
¯σ (∆), we get µ∆(MQ) = µ∆(M) as desired. The argument for QM is the same. ♯
Therefore, the bounds in (3.4) can be tightened to
Q∈Q ρ(QM) ≤max
∆∈B∆ρ (∆M) = µ∆(M) ≤inf
where the equality comes from Lemma 3.7. Note that in computing the inﬁmum, any one of
the diagonal entries of the elements of D can be assumed to be equal to 1. This is without
loss in generality, since for any nonzero scalar γ, D
2 M (γD)−1
2 . Hence, from
this point on, we assume that dS+F ≡1.
Also, using the polar decomposition theorem for
invertible matrices, it is easy to see that restricting the elements of D to be Hermitian, positive
deﬁnite, as opposed to just invertible, does not aﬀect the inﬁmum. Certain convexity properties
make the upper bound computationally attractive. For block structures with S = 0, it is shown
by Safonov and Doyle, 1984, that by using an exponential parametrization of D, the function
is convex in log
. In a very elegant and simple
proof shows that the function ¯σ
is convex on any convex set of commuting matrices
X. This generalizes the results in , and relies only on elementary linear
algebra. The simplest convexity property is given in the following theorem, which shows that
the function ¯σ
has convex level sets.
Theorem 3.9 Let M ∈Cn×n be given, along with a scaling set D, and β > 0. Then
is convex.
Proof: The following chain of equivalences comprises the proof:
2 −β2I < 0
M ∗DM −β2D < 0
The latter is clearly a convex condition in D. ♯
Remark 3.10 The ﬁnal condition in equation (3.11) is called a Linear Matrix Inequality (LMI).
Note that although it is equivalent to the condition in Theorem 3.9, the functional dependence on
D is much simpler and makes the convexity property clearer. For these reasons, LMIs appear to
be attractive for computation. General linear matrix inequalities are discussed in greater detail
in Section 13.2
Linear Fractional Transformations and µ
The use of µ in control theory depends to a great extent on its intimate relationship with a class of
general linear feedback loops called Linear Fractional Transformations (LFTs). This section
explores this relationship with some simple theorems that can be obtained almost immediately
from the deﬁnition of µ. To introduce these, consider a complex matrix M partitioned as
and suppose there is a deﬁned block structure ∆2 which is compatible in size with M22 (for any
∆2 ∈∆2, M22∆2 is square). For ∆2 ∈∆2, consider the loop equations
e = M11d + M12w
z = M21d + M22w
which correspond to the block diagram in Figure 2.
Deﬁnition 4.1 Given complex matrices M and ∆2 as described above. This set of equations
(4.2) is called well posed if for any vector d, there exist unique vectors w, z, and e satisfying
the loop equations.
It is easy to see that the set of equations is well posed if and only if the inverse of I −M22∆2
exists. If this inverse does not exist, then depending on d and M, there is either no solution to
the loop equations, or there are an inﬁnite number of solutions. When the inverse does exist, the
vectors e and d satisfy e = S (M, ∆2) d, where
S (M, ∆2) := M11 + M12∆2(I −M22∆2)−1M21
S (M, ∆2) is called a Linear Fractional Transformation (LFT). If ∆1 is a block structure
compatible in dimension with M11, then for ∆1 ∈∆1 an analogous formula describes S (∆1, M),
S (∆1, M) := M22 + M21∆1 (I −M11∆1)−1 M12
where the upper loop of M is closed with a matrix ∆1.
The S (M, ∆2) and S (∆1, M) notation can be somewhat confusing on ﬁrst encounter. It comes
from the “star-product” of Redheﬀer. Suppose that Q and M are complex matrices, partitioned
with the matrix product Q22M11 well deﬁned and square. If I −Q22M11 is invertible, then the
block diagram in Figure 3 is well-deﬁned. We can extend the deﬁnition of S so that it equals the
result of this interconnection,
= S (Q, M)
Simple manipulation gives
S (Q, M) :=
S (Q, M11)
Q12 (I −M11Q22)−1 M12
M21 (I −Q22M11)−1 Q21
S (Q22, M)
where S (Q, M11) and S (Q22, M) are deﬁned as above. Note that this deﬁnition is dependent
on the partitioning of the matrices Q and M above; it may be well deﬁned for one partition and
not well deﬁned for another.
For clarity, the notation S (•, •) should have 2 additional arguments, specifying dimensions of the
partitions. However, in this paper, the only situations using S that will arise are LFTs, namely
1. the number of rows of Q is less than the number of columns of M, and the number of
columns of Q is smaller than the number of rows of M, or;
2. the number of columns of M is less than the number of rows of Q, and the number of rows
of M is smaller than the number of columns of Q,
and all inputs/outputs into the (dimensionally) smaller matrix are closed in the interconnecting
transformation. Hence, we do not need to specify the dimensions of the interconnecting channels,
since they are equal to the dimension of the smaller matrix.
Alternative notation for LFTs has been used in previous papers, most notably
S(M, ∆2) = Fl(M, ∆2),
S(∆1, M) = Fu(M, ∆1)
where l and u indicate that the lower and upper loop, respectively, are closed. We believe that
the S notation is more natural, easier to work with, and generalizes smoothly to S(M, Q) in
Examples of LFTs
Given the state space realization of a discrete time system
then its transfer matrix is
G(z) = D + C(zI −A)−1B = S(1
Systems with uncertainty can also be easily represented using LFTs. One natural type of uncertainty is unknown coeﬃcients in a state space model. As a simple example, we will begin with a
familiar idealized mass/spring/damper system shown in Figure 4. Suppose m, c, and k are ﬁxed
but uncertain, with m = ¯m(1 + wmδm), c = ¯c(1 + wcδc), k = ¯k(1 + wkδk). Then deﬁning x1 = y
and x2 = ˙y we can write the diﬀerential equation in state-space form as
= S(M, ∆)
∆= diag(δm, δc, δk)
More generally, the perturbed state-space system
A(δ)xk + B(δ)dk
C(δ)xk + D(δ)dk
where δ is a vector of parameters that enter rationally can be written as an LFT on a diagonal
matrix ∆made up of the elements of δ, possibly repeated. The form of the LFT is 
with perturbation wk = ∆zk yielding
= S (M, ∆)
In general, for problems of this type it is easy to obtain realizations, but it is diﬃcult to insure
that they are minimal, except in the case where the parameters enter linearly.
A fundamental property of LFTs that contributes to their importance in linear systems theory
is that interconnections of LFTs are again LFTs. For example, consider a situation with three
components, each with a LFT uncertainty model. The interconnection is shown in Figure 5.
By simply reorganizing the diagram, collecting all of the known systems together, and collecting
all of the perturbations (the ∆i’s) together, we end up with the diagram in Figure 6, where
P depends only on G1, G2, G3 and the diagram layout. Note how general uncertainty at the
component level becomes structured uncertainty at the system level. Additional information on
LFT’s and how they arise in engineering problems is found in .
The Main Loop Theorem
For notational ease, let Bi := {∆i ∈∆i : ¯σ (∆i) ≤1}. In this formulation, the matrix M11 =
S(M, 0) may be thought of as the nominal map and ∆2 ∈B2 viewed as a norm bounded
perturbation from an allowable perturbation class, ∆2. The matrices M12, M21, and M22 and
the formula S(M, •) reﬂect prior knowledge on how the unknown perturbation aﬀects the nominal
map, M11. This type of uncertainty, called linear fractional, is natural for many control problems,
and encompasses many other special cases considered by researchers in robust control and matrix
perturbation theory.
The constant matrix problem to solve is: determine whether the LFT is well posed for all ∆2 in
B2 and, if so, then determine how “large” S (M, ∆2) can get for ∆2 ∈B2.
Deﬁne a third structure ∆as
: ∆1 ∈∆1, ∆2 ∈∆2
Now there are three structures with respect to which we may compute µ. The notation we use
to keep track is as follows: µ1 (·) is with respect to ∆1, µ2 (·) is with respect to ∆2, µ∆(·) is
with respect to ∆. In view of this, µ1 (M11), µ2 (M22) and µ∆(M) are all deﬁned, though for
instance, µ1 (M) is not deﬁned. The ﬁrst theorem follows immediately from the deﬁnition of µ.
Theorem 4.2 The linear fractional transformation S (M, ∆2) is well posed for all ∆2 ∈B2 if
and only if µ2 (M22) < 1.
As the perturbation ∆2 deviates from zero, the matrix S (M, ∆2) deviates from M11. The range
of values that µ1 (S (M, ∆2)) takes on is intimately related to µ∆(M), as follows:
Theorem 4.3 (Main Loop theorem)
µ2 (M22) < 1
∆2∈B2µ1 (S (M, ∆2)) < 1
Proof: First note that µ∆(M) < 1 implies that µ2 (M22) < 1, so we may assume the latter
and prove the equivalence of the two remaining conditions. Let ∆i ∈∆i be given, with
¯σ (∆i) ≤1, and deﬁne ∆= diag [∆1, ∆2] so that ∆∈∆. Now
det (I −M∆) = det
By hypothesis I −M22∆2 is invertible, hence
det (I −M∆) = det (I −M22∆2) det
I −M11∆1 −M12∆2 (I −M22∆2)−1 M21∆1
Collecting the ∆1 terms leaves
det (I −M∆) = det (I −M22∆2) det (I −S (M, ∆2) ∆1)
By the deﬁnition of µ, the left-hand side of (4.7) is nonzero for all ∆∈B∆iﬀµ∆(M) < 1.
Similarly, the right hand side is nonzero for all ∆= diag [∆1, ∆2] ∈B∆iﬀµ1 (S (M, ∆2)) <
1 for all ∆2 ∈B2. This completes the proof. ♯
Remark 4.4 This theorem forms the basis for most uses of µ in linear system robustness analysis, whether from a state-space, frequency domain, or Lyapunov approach.
Remark 4.5 This theorem is stated in terms of feasability conditions, testing whether some
quantity is less than 1. This allows for an elegant statement and proof, but other versions are
possible, with some complication in notation. Scaled versions of the Main Loop Theorem appear
later in this section.
Remark 4.6 The importance of the theorem can be highlighted by a slight restatement. Suppose a property P, of a matrix W can be related to a µ test on the matrix. That is, there exists
some block structure ∆P such that
matrix W satisﬁes property P
µ∆P (W) < 1
Then the perturbed matrix S (M, ∆) is well deﬁned, and has the property P for every ∆∈B∆
if and only if µ ˜∆(M) < 1, where ˜∆:= {diag [∆P, ∆] : ∆P ∈∆P, ∆∈∆}.
In other words,
whenever a property of a matrix can be related to a µ test, then there will be a µ test of greater
complexity to determine if the property is robust to structured perturbations in the form of
The role of the block structure ∆2 is clear in this theorem — it is the structure for the original
perturbation. However the role of the perturbation structure ∆1 is often misunderstood. Note
that µ1 (·) appears on the right hand side of the theorem, so that the set ∆1 deﬁnes what
function of the matrix S (M, ∆2) is to be computed.
This theorem can be illustrated by a system-theoretic example with a transfer function and its
state-space realization. This example involves two of the simplest cases of LFTs. Suppose that
∆1 := {δ1In : δ1 ∈C} and ∆2 = Cm×m, which are the special cases considered in Section 3.
Recall that for A ∈Cn×n, µ1 (A) = ρ (A), and for D ∈Cm×m, µ2 (D) = ¯σ (D). Now, let ∆be
the diagonal augmentation ∆1 and ∆2, namely
: δ1 ∈C, ∆2 ∈Cm×m
⊂C(n+m)×(n+m)
Let A ∈Cn×n, B ∈Cn×m, C ∈Cm×n, and D ∈Cm×m, be given, and interpret them as the state
space model of a discrete time system
and let M ∈C(n+m)×(n+m) be the block state space matrix of the system,
Applying the theorem with this data implies that the following are equivalent:
• The spectral radius of A satisﬁes ρ (A) < 1, and
D + Cδ1 (I −Aδ1)−1 B
• The maximum singular value of D satisﬁes ¯σ (D) < 1, and
A + B∆2 (I −D∆2)−1 C
• The structured singular value of M satisﬁes µ∆(M) < 1.
The ﬁrst condition implies two things: the system is stable, and the || · ||∞norm on the transfer
function from u to y obtained by setting δ1 = 1
z) is less than 1. That is
∥G∥∞:= max
D + C (zI −A)−1 B
D + Cδ1 (I −Aδ1)−1 B
The second condition implies that (I −D∆2)−1 is well deﬁned for all ¯σ (∆2) ≤1, and that the
uncertain diﬀerence equation
A + B∆2 (I −D∆2)−1 C
is stable for all such ∆2.
The equivalence between the small gain condition, ∥G∥∞< 1, and the stability robustness of
the uncertain diﬀerence equation is well known as the small gain theorem, in its necessary and
suﬃcient form for linear time invariant systems. What is important to see is that both of these
conditions are in fact equivalent to one condition on the structured singular value. Already we
have seen that the spectral radius and maximum singular value are special cases of µ. Here we
see that additional important linear system properties, namely robust stability and input-output
gain are also related to a particular case of the structured singular value.
Returning to the main loop theorem, note that the bound on the performance is the same as the
bound on the perturbation, namely 1. Scaling the matrix M by 1
β, for some positive scalar β,
and then applying the theorem gives:
Corollary 4.7 Let β > 0 be given. Then
µ2 (M22) < β
µ1 (S (M, ∆2)) < β
The bound on performance and the bound on the perturbation are related, they are reciprocals.
For nonreciprocal values, certain blocks of M must be scaled and µ recomputed. Speciﬁcally, for
α ≥0, deﬁne Mα as
Some simple facts about Mα:
1. if α = 0 then µ∆(Mα) = µ2 (M22)
2. for any ∆2 ∈∆2, with I −M22∆2 nonsingular, S (Mα, ∆2) = αS (M, ∆2)
3. max {αµ1 (M11) , µ2 (M22)} ≤µ∆(Mα) ≤max {1, α} µ∆(M)
4. µ∆(Mα) is a continuous, nondecreasing function of α
Theorem 4.8 Let β > µ2 (M22) be given, and αβ := max {α > 0 : µ∆(Mα) = β}. Then
µ1 (S (M, ∆2)) = β
Upper bound LFT results
Theorem 4.3 gives necessary and suﬃcient conditions for performance/robustness characteristics
in terms of a µ evaluation.
The µ test always takes on the form “Is µ (M) < 1?”
upper and lower bounds on µ can be used in the following manner: an upper bound gives a
suﬃcient condition for the robustness/performance characteristic of the theorem; a lower bound
gives a suﬃcient condition for when the robustness/performance will not be met.
both are important. The upper bound guarantees robustness of a property of a linear fractional
transformation for perturbations up to a certain size, and a lower bound exhibits perturbations
which cause a degree of degradation in the LFT’s properties.
The above comments apply for any upper and lower bound.
Of speciﬁc interest is the additional information that is obtained in using the ¯σ
upper bound.
< 1 implies a great deal more than µ∆(M) < 1. As usual, let ∆1 and ∆2 be two
given structures, and let ∆= {diag [∆1, ∆2] : ∆i ∈∆i}. Similarly, let Di be the appropriate D
scaling sets for the two structures, (equation 3.6) and denote D as the diagonal augmentation of
these two sets, D := {diag [D1, D2] : Di ∈Di}.
Lemma 4.9 ) Let M be given as in equation 4.1. Suppose there is
a D ∈D such that ¯σ
< 1. Then there exists a D1 ∈D1 such that
1 S (M, ∆2) D
Proof: The easiest method of proof is just to track the norms of the various vectors in the loop
equations for the linear fractional transformations shown in Figure 8. Let D1 and D2 be the
separate parts of the D ∈D which achieves ¯σ
< 1. Obviously, µ2 (M22) < 1,
so for any ∆2 ∈B2 the two LFT’s are well posed, and from d to e are the same. Let
d ̸= 0 be any given complex vector of appropriate dimension, and let e, w, and z be the
unique solutions to the loop equations for the linear fractional transformation on the right
in Figure 8. By hypothesis, we have
∥z∥2 + ∥e∥2 < ∥w∥2 + ∥d∥2
and since ¯σ (∆2) ≤1
∥w∥2 ≤∥z∥2
Combining these gives
∥e∥2 < ∥d∥2
Equation (4.12) also holds for the linear fractional transformation on the left, since the
matrix relating d to e is the same for both linear fractional transformations. This implies
1 S (M, ∆2) D
< 1 as desired. ♯
Consider the problem determining the value of
1 S (M, ∆2) D
and also ﬁnding a D1 ∈D1 that achieves a cost arbitrarily close to the inﬁmum. Suppose the
dimension of M11 is n × n. Deﬁne an additional structure
©diag [∆, ∆2] : ∆∈Cn×n, ∆2 ∈∆2
Theorem 4.10 Let M, ∆2, D1, and ∆N be given as above. Suppose that µ2 (M22) < 1. Deﬁne
1 S (M, ∆2) D
In this section, all of the results were stated for S (M, ∆2). Analogous results hold for S (∆1, M).
Robustness tests with µ
The structured singular value can be used to quantify robustness margins for a linear system
with linear fractional uncertainty. Speciﬁcally, suppose that P(s) is a rational, proper matrix,
of size (n1 + n2) × (n1 + n2) and block structures ∆1 ⊂Cn1×n1 and ∆2 ⊂Cn2×n2 are given.
Partition P(s) as
For ∆1 ∈∆1, consider the interconnection shown in Figure 9. For any ∆1 ∈B1, S (∆1, P(s)) is
the transfer function from d3 →e3. The closed-loop system is said to be:
• well-posed if det (I −P11(∞)∆1) ̸= 0. This is the necessary and suﬃcient condition that
all closed-loop transfer functions in Figure 5.1 be proper.
• stable if all closed-loop transfer functions in Figure 5.1 are analytic in the closed righthalf-plane.
Theorem 5.1 Suppose that P(s) has all of its poles in the open left-half plane. Let β > 0.
1. For all ∆1 ∈∆1 with ¯σ (∆1) ≤β, the perturbed closed-loop system is well-posed and
stable if and only if
µ1 (P11(s)) < 1
2. For all ∆1 ∈∆1 with ¯σ (∆1) ≤β, the perturbed closed-loop system is well-posed, stable
µ2 [S (∆1, P(s))] < 1
if and only if
µ∆(P(s)) < 1
Proof: The proof follows from the deﬁnitions of µ, well-posedness, stability, and invertibility of
matrices with elements in a commutative ring. ♯
Remark 5.2 Although the structured singular value is not necessarily a norm, we introduce
the following notation: for a proper, rational matrix P, analytic in the closed-right-half-plane,
and a block structure ∆of appropriate dimensions, deﬁne
Remark 5.3 In Section 6, techniques that allow the right-half plane supremums to be replaced
(equivalently) by imaginary-axis supremums will be developed.
It is possible to easily generalize these robustness theorems to the case where ∆is a blockdiagonal, ﬁnite dimensional, stable, linear time-invariant system (as opposed to a constant, complex matrix). Let ∆be a block structure, as in equation 3.1. We want to consider feedback
perturbations to P which are themselves dynamical systems, with the block-diagonal structure
of the set ∆. To do so, ﬁrst let M (S) denote the set of rational, proper, stable, transfer matrices.
Associated with any block structure ∆, let M (∆) denote the set of all block diagonal, stable
rational transfer functions, with block structure like ∆.
©∆(·) ∈M (S) : ∆(so) ∈∆for all so ∈¯C+
For any ∆1 ∈M (∆1), the closed-loop system is said to be:
• well-posed if det (I −P11(∞)∆1(∞)) ̸= 0. This is the necessary and suﬃcient condition
that all closed-loop transfer functions in Figure 5.1 be proper.
• stable if all closed-loop transfer functions in Figure 5.1 are analytic in the closed righthalf-plane.
Theorem 5.4 Suppose that P(s) has all of its poles in the open left-half plane. Let β > 0.
1. For all ∆1 ∈M (∆1) with ∥∆1∥∞≤β, the perturbed closed-loop system is well-posed and
stable if and only if
µ1 (P11(s)) < 1
2. For all ∆1 ∈M (∆1) with ∥∆1∥∞≤β, the perturbed closed-loop system is well-posed,
stable and
µ2 [S (∆1(s), P(s))] < 1
if and only if
µ∆(P(s)) < 1
In summary, the peak value on the µ plot of the frequency response that the perturbation “sees”
determines the size of perturbations that the loop is robustly stable (and/or performing) against.
Other, more sophisticated assumptions about the perturbations may be formulated, and solved
with µ. These include gap/graph topology uncertainty, , , and diﬀerent induced norms to measure the size of the uncertainty,
 .
Maximum modulus theorem for LFT’s with µ
This section describes a maximum modulus theorem that µ satisﬁes: µ of an LFT on a normbounded structured set achieves its maximum on the unitary elements of this set. This is a
generalization of the ordinary maximum modulus theorem for rational functions of a complex
variable. We begin by stating a well known result from complex analysis namely that the roots
of a polynomial are continuous functions of the coeﬃcients of the polynomial.
Lemma 6.1 Let f(z) = Pn
i=0 aizi be an n’th order polynomial, an ̸= 0. Let ¯z1, ¯z2, . . . , ¯zn be
the roots of f. For any ϵ > 0 and any integer m > 0, there exists a δm,ϵ > 0 such that if g(z),
has coeﬃcients bi ∈C which satisfy |bi| < δm,ϵ, then there are n roots of f + g , labeled
˜z1, ˜z2, . . . , ˜zn that satisfy |¯zi −˜zi| < ϵ.
Next, we shift our attention to polynomials in several dimensions, that is, polynomials taking
Ck →C. If z ∈Ck, we let ∥z∥∞:= max
i≤k |zi|. For p:Ck →C, a polynomial, deﬁne βp as
βp = min {∥z∥∞: p(z) = 0}
In other words, βp is the norm of the minimum norm roots of the polynomial. The next two
lemmas are from Doyle, 1982. The ﬁrst concerns minimum norm roots and is a direct consequence
of Lemma 6.1. The second is provides the essential argument of the maximum modulus theorem
Lemma 6.2 Let p be a polynomial from Ck →C. Deﬁne βp via (6.1). Then there exists a
z ∈Ck such that |zi| = βp for each i, and p(z) = 0.
Sketch of proof: A proof is given in Doyle, 1982. The main idea is as follows: let ¯z ∈Ck
be a root of p with βp = ∥¯z∥∞.
If all of the coordinates of ¯z satisfy |¯zi| = βp, then
stop. Otherwise, take one of the coordinates of ¯z, say ¯z1 whose magnitude is less than βp.
Consider the polynomial q(z1) := p (z1, ¯z2, . . . , ¯zk). This has a root at ¯z1, and |¯z1| < βp. If
this is a nontrivial polynomial, then, by slightly reducing (in magnitude) all of the ¯zi, i ≥2,
the coeﬃcients of the polynomial change slightly, and it has a root very close to ¯z1. This
implies that p has a root ˜z ∈Ck such that ∥˜z∥∞< βp, which contradicts the deﬁnition.
On the other hand, if the polynomial q ≡0, then the variable z1 does not matter, and we
can repeat the argument on a diﬀerent coordinate, say z2, that satisﬁes |¯z2| < βp. ♯
Lemma 6.3 Let ∆⊂Cn×n be a given block structure, and let Q be deﬁned as in section 3. If
M ∈Cn×n has µ∆(M) = 1, then there is a Q ∈Q such that det (I −MQ) = 0.
Proof: Since µ∆(M) = 1, there is a ˆ∆∈∆with ¯σ
= 1 and det
= 0. Also, for
any ∆∈∆with ¯σ (∆) < 1, the matrix I −M∆is nonsingular.
Do a singular value decomposition on each block that makes up ˆ∆. This gives U, V ∈Q,
and a diagonal ˆΣ ∈∆, such that
I + MU ˆΣV ∗´
Since ˆΣ ∈∆is diagonal, it appears as
hˆδ1Ir1, . . . , ˆδSIrS, ˆα1, . . . , ˆαw
for some nonnegative real numbers ˆδi and ˆαj, and w = PF
j=1 mj (recall that the j’th full
block is mj × mj, hence each full block contributes mj of the α’s). With ¯σ
least one of the ˆδi or ˆαj is 1.
Consider S + w complex variables, z1, . . . , zS+w. Deﬁne a variable Σ by
Σ = diag [z1Ir1, . . . , zSIrS, zS+1, . . . , zS+w]
Then det (I + MUΣV ∗) is a polynomial on CS+w, since the determinant involves only
multiplications and additions of its argument. Since µ∆(M) = 1, a minimum norm (using
∥· ∥∞on CS+w, as above) root of this polynomial has norm equal to 1. Let ¯Σ be the
particular minimizing root with all components of equal magnitude, namely 1. Then we
can write ¯Σ = Φ for some Φ ∈Q. This gives
det (I + MUΦV ∗) = 0.
Deﬁning Q := UΦV ∗completes the proof. ♯
The next theorem from follows immediately from Lemma 6.3 and the facts that
Q ∈Q implies ρ(QM) ≤µ∆(M), and det (I −MQ) = 0 implies that ρ(QM) ≥1.
Theorem 6.4
Q∈Q ρ (QM) = max
∆∈B∆ρ (∆M) = µ∆(M)
Hence the lower bound given for µ in equation (3.10) is actually not just a bound, but an equality.
To motivate the main result of the section, recall the general setup for the linear fractional
transformation S (M, ∆). Suppose M ∈C(n1+n2)×(n1+n2) is given. We partition it in the obvious
with Mij ∈Cni×nj. Let ∆1 ⊂Cn1×n1 and ∆2 ⊂Cn2×n2 be two block structures and deﬁne
B1, B2, Q1, and Q2 correspondingly.
The maximum modulus theorem from is:
Theorem 6.5 Let M be given as in (6.2), along with two block structures ∆1 and ∆2. Suppose
that µ2 (M22) < 1. Then
Q2∈Q2 µ1 (S (M, Q2)) = max
∆2∈B2 µ1 (S (M, ∆2)) .
Proof: A detailed proof can be found in the reference. The main idea is as follows: suppose
(by an appropriate scaling) that the maximum on the right hand side of equation (6.3)
is 1. Then, since µ2 (M22) < 1, it is possible to show that µ∆(M) = 1. Using Lemma
6.3, construct matrices Q1 and Q2 such that I −Mdiag [Q1, Q2] is singular. Again, use
the fact that µ2 (M22) < 1 to conclude that I −S (M, Q2) Q1 is singular. This shows that
µ1 (S (M, Q2)) ≥1, completing the argument. ♯
Remark 6.6 This is similar to a result in : that for functions H(z) analytic on the disk, the function µ (H(z)) achieves its maximum on the boundary: max
|z|≤1 µ (H(z)) =
|z|=1 µ (H(z)). It is possible to use their result to derive Theorem 6.5 and vice versa.
It is instructive to see how Theorem 6.4 can be obtained as a special case of Theorem 6.5. Let ∆⊂
Cn×n be a given block structure, with associated sets B∆and Q. Deﬁne ∆1 := {δIn : δ ∈C},
and for each M ∈Cn×n, deﬁne ¯
By Theorem 6.5, and noting that µ1 (·) = ρ (·), we have
Q∈Q ρ (MQ) = max
∆∈B∆ρ (M∆) = µ∆(M)
This is exactly Theorem 6.4.
Lower bound power algorithm
This section presents an iterative algorithm to compute lower bounds for the structured singular
value. The algorithm resembles a mixture of power methods for eigenvalues and singular values,
which is not surprising, since the structured singular value can be viewed as a generalization of
both. If the algorithm converges, a lower bound for µ results. We prove that µ is always an
equilibrium point of the algorithm.
In the calculation of µ is reformulated as a smooth optimization problem. As
with all of the known exact expressions for µ, the function to be maximized has local maximums
which are not global, so in general the method yields only lower bounds for µ. Similar comments
can be made for the ideas in and , as well as the algorithm in this
section. The contribution here is yet another lower bound algorithm to aid in the analysis of
robustness of systems with structured uncertainty, along with a deeper conceptual understanding
of the structured singular value.
We begin by noting that both the functions r : B∆→R, deﬁned by r (∆) := ρ (∆M) and
˜r : Q →R, deﬁned by ˜r (Q) := ρ (QM) have local maximums which are not global.
though, that the function ˜r is a restriction of r, and it is possible to construct examples where
a point Q ∈Q is a local maximum of ˜r, but not a local maximum of r. Such a point
deﬁnitely does not correspond to the maximizer that gives µ∆(M), and so we will not consider
the corresponding lower bound from such points as acceptable. Rather, acceptable lower bounds
will correspond to points Q ∈Q which are local maximums of the function r.
Roughly speaking, this section develops an iterative algorithm which ultimately generates a point
Q ∈Q that is a local maximum of the function r. In general, these are a proper subset of the
local maximums of the function ˜r, though the global maximums over the two sets are the same.
Some of the preliminary results are generalizations of those found in and
 
We will be interested in local maximums of the function r (∆) = ρ (∆M), therefore we begin
with some facts from perturbation theory, which assist in characterizing local maximums.
Matrix Facts
In this section, we collect a few useful facts.
Suppose W : R →Cn×n is an analytic function of the real parameter t. If λo is a eigenvalue
of Wo := W(0) of multiplicity one, then for some open interval containing 0, this eigenvalue
is a analytic function of t, as are the eigenvectors associated with it. That is, suppose there
are xo, yo ∈Cn, satisfying y∗
oxo = 1, Woxo = λoxo,
Then there is an
ϵ > 0 and analytic functions x : (−ϵ, ϵ) →Cn, y : (−ϵ, ϵ) →Cn, and λ : (−ϵ, ϵ) →C, such that
x(0) = xo, y(0) = yo, λ(0) = λo and for all t ∈(−ϵ, ϵ)
y∗W = λy∗.
This follows from . We can then diﬀerentiate and obtain ˙λ(0) = y∗
o ˙W(0)xo.
The next two lemmas follow from elementary linear algebra. They will be used in the main
theorem of the next section.
Lemma 7.1 Let y, x ∈Cn with y ̸= 0 and x ̸= 0.
There exists d ∈R, d > 0, such that
dx if and only if Re (y∗Gx) ≤0 for every G ∈Cn×n satisfying G + G∗≤0.
Lemma 7.2 Let x and y be two nonzero vectors in Cn. Then there exists a Hermitian, positive
deﬁnite D ∈Cn×n, such that D
2 y if and only if Re (gy∗x) ≤0 for every g ∈C with
g + ¯g ≤0.
The condition in Lemma 7.2 on y∗x involving g ∈C is equivalent to y∗x being real and positive.
We have chosen to write it in the form above so that it is a natural analog to Lemma 7.1 and is
stated exactly as it will be applied in Theorem 7.3 of the next section.
Eigenvector characterization of local maximums
Consider the function r:B∆→R, deﬁned by r (∆) = ρ (∆M). Recall that µ∆(M) = max
∆∈B∆r (∆),
and that the global maximum occurs on the subset Q ⊂B∆. In this section, we characterize the
occurance of a local maximum of r at ∆= I ∈Q ⊂B∆, in terms of the eigenvectors of M. We
begin with some notation.
Let x and y be nonzero right and left eigenvectors of M, associated with an eigenvalue λ: Mx =
λx and y∗M = λy∗. Partition x and y compatibly with the block structure ∆,




where xri, yri ∈Cri and xmj, ymj ∈Cmj for each i and j. We call these the “block components”
of x and y, and for technical reasons, we deﬁne a nondegeneracy condition: x and y are nondegenerate if for every i, yri
∗xri ̸= 0, and for each j, xmj ̸= 0, ymj ̸= 0. We will also assume that
ρ(M) = λo > 0 is a distinct eigenvalue of M.
The condition that ρ(M) = λo > 0 is without loss of generality, because ∆can always be used to
enforce this (for any φ, ejφ∆= ∆). The conditions of nondegeneracy and λo distinct are not so
easily dispensed with and there are basically two approaches to deal with them. The ﬁrst would
be to argue that the are generic conditions and thus unlikely to cause problems in practice. A far
more satisfactory solution is to generalize the theorems and proofs in this section to remove them.
In fact, this can be done, but not without substantial additional technical complication. Since
the results in this subsection are presented primarily to give insight into the power algorithms to
be presented in the next subsection, these additional technicalities have been foregone in favor
of a simpler development.
Theorem 7.3 Let M ∈Cn×n be given, and suppose λo > 0 is a distinct eigenvalue of M, with
nondegenerate right and left eigenvectors x and y. Suppose that ρ(M) = λo. If the function
r : B∆→R deﬁned by r(∆) = ρ(∆M) has a local maximum (with respect to the set B∆) at
∆= I, then there exists a D ∈D such that D−1
Proof: Let G ∈∆with G + G∗≤0 so that G has the form
diag [g1Ir1, . . . , gSIrS, G1, . . . , GF ]
where Re(gi) ≤0, and Gj + G∗
j ≤0 for all i and j and eGt ∈B∆for all t ≥0 with eGt = I
for t = 0. Deﬁne a matrix function W :R→Cn×n by W(t) := eGtM. Note that at t = 0,
λo is a simple eigenvalue of W(0), with x and y the right and left eigenvectors. For some
nonempty interval containing 0, this eigenvalue is always simple, and hence there is an
analytic function of the real variable t, λ(t), deﬁned on that interval, such that λ(t) is an
eigenvalue of W(t) for all t and λ(0) = λo. It is easy to calculate ˙λ(0), namely
˙λ(0) = y∗˙W(0)x = λoy∗Gx
By hypothesis, λo > 0, ρ(M) = λo and the function ρ (∆M) has a local maximum (with
respect to B∆) at ∆= I. Therefore
which says that the magnitude of λ must be nonincreasing at t = 0. Using the “block
notation” of (7.2) and substituting (7.3) and (7.4) into (7.5) yields
This must hold for arbitrary G ∈∆satisfying G + G∗≤0. Applying Lemmas 7.1 and 7.2
we conclude that for each i, there is a Di = D∗
i ∈Cn×n, Di > 0 such that D
and for each j, there is a dj ∈R, dj > 0 such that
pdjxmj. Arranging all of
these Di’s and dj’s into one block diagonal D completes the proof. ♯
Remark 7.4 Note that assuming λo is distinct assures diﬀerentiability, . Since λo
is a solution of max
i |λi (∆M) |, it is likely that at the maximum it will be distinct. In any
case, if λo is not distinct, it can still be shown to be diﬀerentiable at a local maximum, and the
rest of the proof remains. Unfortunately, this proof of diﬀerentiability is tedious and technical,
and for this reason has been omitted.
Theorem 7.5 Let Qo ∈Q achieve the global optimum for the problem max
Q∈Q ρ (QM). Suppose
that the eigenvalue associated with ρ (QoM) is distinct, real and positive, and hence equal to
µ = µ∆(M). If x and y are nondegenerate right and left eigenvectors of the eigenvalue µ, then
there exists a D ∈D, and ξ ∈Cn, ∥ξ∥= 1 such that
Proof: By Theorem 6.4, any global maximizer of max
Q∈Q ρ (QM), is also a maximizer of max
∆∈B∆ρ (∆M).
M := QoM, then ∆= I is a local (in fact global) maximizer for max
Apply Theorem 7.3 to the matrix ˜
M and deﬁne ξ = D
2 y to prove the theorem. ♯
Remark 7.6 This result was ﬁrst shown in for the case of S = 0. It is
also similar to the “principal direction alignment” ideas in .
Theorem 7.5 is more general, though, since it handles repeated scalar blocks as well as full blocks.
Remark 7.7 This theorem is not true if we consider local maximums that are not global of the
function ˜r:Q→R deﬁned as ˜r (Q) := ρ (QM).
Remark 7.8 Any real number β > 0 satisfying
for some Q ∈Q, D ∈D and nonzero ξ ∈Cn is a lower bound for µ∆(M). This follows because
βQM is a singular matrix.
Lower bound power algorithm
In this section, we propose an iterative algorithm (reminiscent of the power algorithm for spectral
radius) to ﬁnd solutions to the equations (7.7), and therefore get lower bounds for µ.
Rewriting (7.7), and changing notation a bit, we want to ﬁnd a Q ∈Q, D ∈D, β > 0, and
ξ ∈Cn with ∥ξ∥= 1 such that
2 Q∗ξ = βξ.
These two constraint equations can be rewritten as
For a given D, Q, and ξ, deﬁne vectors a, b, z, and w by
With this deﬁnition, we have Mb = βa and M ∗z = βw. We can eliminate ξ from (7.9) to get
b = Qa = D−1w
z = Da = Q∗w
Since the unknowns Q and D generally may have high dimension, we would like to write the
four relationships from equation (7.10) in a manner that does not involve the matrices Q and D.
With a few technical conditions, this can be done. In order to simplify the upcoming formulas,
we will consider a block structure with S = 1, F = 1 (by duplicating the appropriate formulas
for additional blocks, whether they are repeated scalar blocks or full blocks, it is straightforward
to extend the algorithm to more general structures). Hence the sets D and Q are
©diag [D1, d2Im1] : D1 ∈Cr1×r1, D1 = D∗
1 > 0, d2 > 0
©diag [q1Ir1, Q2] : ¯q1q1 = 1, Q2 ∈Cm1×m1, Q∗
With respect to this, we will partition the vectors accordingly, so z =
, where z1 ∈Cr1
and z2 ∈Cm1, and likewise for the other vectors.
Lemma 7.9 Let r1 and m1 be positive integers. Let z1 , w1 , b1 , a1 ∈Cr1 and z2, w2, b2, a2 ∈
Cm1 be nonzero vectors with a∗
1w1 ̸= 0. Then, there exists a D ∈D and Q ∈Q such that
if and only if
→The relations for z2 and b2 follow by direct substitution. For z1 and b1, it is easiest to deﬁne
an auxiliary variable ζ := D
2 b, and then verify via substitutions.
1w1|, since this is well deﬁned. Likewise, choose d2 = ∥w2∥
∥a2∥. By assumption, d2 is
well deﬁned, and nonzero. Since ∥w2∥= ∥z2∥, let Q2 be any unitary matrix that takes w2
into z2. The matrix Q2 also rotates b2 into a2,
Next, we calculate a∗
1w1|, which is nonzero by assumption; hence Lemma 7.2 yields
a Hermitian, positive deﬁnite D1 such that D1a1 = z1. As we hope, D1 takes b1 into w1
D1b1 = q1D1a1 = q1z1 = w1.
Deﬁning D and Q in the obvious manner completes the proof. ♯
We are now prepared for the main theorem.
Theorem 7.10 Let M ∈Cn×n be given, and let ∆be the two block (S = 1, F = 1) structure
deﬁned above, with block sizes r1 and m1, where r1 + m1 = n. Suppose β > 0 is given. Then
there exists Q ∈Q, D ∈D, ξ =
∈Cn, ∥ξ∥= 1, ξ1 ̸= 0 , ξ2 ̸= 0 with
if and only if there exists nonzero vectors z1, w1, b1, a1 ∈Cr1 and z2, w2, b2, a2 ∈Cm1 with
1w1 ̸= 0 and
Remark: In order to ﬁnd decompositions using the representation that this theorem allows
(equation (7.14) — free of Q’s and D’s), we can restrict ourselves to unit vectors a, b, z, w.
Why? Suppose there are nonzero vectors satisfying (7.14). Examining the equations, it is
clear that scaling z and w by some α ̸= 0 and scaling b and a by some γ ̸= 0 does not
aﬀect any of the equalities in (7.14). Moreover, the equalities in (7.14) always imply that
∥z∥= ∥w∥, and ∥a∥= ∥b∥, so by proper scaling, all the vectors would be unit norm.
In the above theorem, we have purposefully written the conditions (7.14) in a manner that
suggests attempting to ﬁnd a solution in an iterative fashion. In particular, for i = 1, 2, let
vectors aik, bik, zik, and wik, and positive scalars ˜βk, ˆβk evolve as
˜βk+1ak+1 = Mbk
z1k+1 = w∗
1ka1k+1|w1k
z2k+1 = ∥w2k∥
∥a2k+1∥a2k+1
ˆβk+1wk+1 = M ∗zk+1
1k+1w1k+1|a1k+1
b2k+1 = ∥a2k+1∥
∥w2k+1∥w2k+1
where the values of ˜βk+1 and ˆβk+1 are chosen > 0, so that ∥ak+1∥= ∥wk+1∥= 1.
Note also that if the initial b and w vectors that start the iteration are unit vectors, then at every
step, all vectors, a, b, z, and w will be unit length.
7.a Potential problems within the iteration are:
• Mbk = 0 or M ∗zk = 0, then ak+1 or wk+1 is not well deﬁned.
1kw1k = 0, then the vectors z1k+1 and/or b1k+1 are not well deﬁned.
• Either ∥w2k∥= 0 or ∥a2k∥= 0, making b2k and/or z2k not well deﬁned.
If any of these conditions occur, then one possibility is to restart the algorithm at a diﬀerent
initial condition (ie., a new b10, b20, w10 and w20). A more sophisticated approach is to
examine the above conditions and recognize that a sensible iteration can still be deﬁned
even if these conditions occur. Algorithms have been developed along these lines and will
be discussed elsewhere.
7.b If the iteration does converge to an equilibrium point, then the β values must be equal, that
is ˜β = ˆβ. This is easy to see: suppose the equations in (7.14) are satisﬁed (convergence
of the algorithm in (7.15)), but the β associated with b and a is ˜β and the β associated
with z and w is ˆβ. The converged equations imply that there exists a Q ∈Q and D ∈D
such that QD
the β’s are real, they must be equal. Hence, when verifying convergence of the algorithm,
it is necessary to begin checking the convergence of the vectors only after the ˜βk and ˆβk
values are nearly equal. This saves some computations early in the iteration.
• If there were only the ﬁrst block, which is a repeated scalar block, the iteration would
be a power iteration for the largest (in magnitude) eigenvalue of the matrix M. Since
µ for 1 repeated scalar block is the spectral radius, the algorithm we have proposed
reduces to a valid algorithm in the special case of 1 repeated scalar block.
• If there were only the second block, which is a full block, the iteration becomes a
eigenvalue power algorithm for M ∗M, hence it will give the largest singular value of
M. Again, with respect to this speciﬁc block structure, this is what we want.
Hence, the iteration we have proposed is a mix of two separate, well understood iterations,
both of which converge to the largest eigenvalue/singular value. We might hope that this
algorithm will converge to the largest β for which the equations in (7.8) are solved, which
by Theorem 7.5 is equal to µ∆(M). Unfortunately, this is not always the case.
Extensive computational experience, and ,
has led to the following conclusions:
1. The algorithm works well in practice, and versions of it have been used very extensively in
universities and industry. It appears to have roughly order n2 growth rates for computation
as a function of problem size. The main diﬃculty is that it occasionally doesn’t converge
or converges to a value of β which is not µ.
2. The diﬃculties described in 7.a above do not seem to occur in practice, however there
are matrices whose optimally scaled eigenvector block components (eq. 7.2) do not satisfy
the “nonzero” block conditions described at the beginning of section 7.2. This type of
situation will lead to the diﬃculties mentioned. In any event, while it is easy to construct
matrices where these problems happen, running the algorithm on frequency responses of
actual closed loop systems has not been a problem.
3. Limit cycles can occur, and seem to occur more often when there are large repeated scalar
blocks. Unlike an equilibrium point, the presence of a stable limit cycle does not immediately give rise to a lower bound for µ.
4. In general, there are several stable equilibrium points, with diﬀerent values of β. This is in
contrast with the conventional power algorithms for ρ and ¯σ, where only the largest ones
are stable. It is even possible that the algorithm converges to a value of β which is smaller
than ρ (M).
5. It is possible to reﬁne the power algorithm to guarantee convergence to some local maximum, but at the expense of greater computation time.
We are currently researching
algorithms that give favorable tradeoﬀs between convergence properties and running times.
Relating µ and inf
The purpose of this section is to study the relationship between µ∆(M) and the upper bound.
The two-step strategy we take ﬁrst involves characterizing the optimality conditions for the upper
bound, and then determining under what situations these optimality conditions imply anything
about the existence of a block structured perturbation matrix ∆satisfying det (I −M∆) = 0.
Optimality Conditions for inf
We want to characterize when ¯σ (M) = inf
, that is, when D := I is optimal.
Begin with M ∈Cn×n, and let its singular value decomposition be
M = σ1UV ∗+ U2Σ2V ∗
where σ1 > 0 is the maximum singular value of M and has multiplicity r; U, V ∈Cn×r; U2, V2 ∈
Cn×(n−r); U∗U = V ∗V = Ir; U∗
2 U2 = V ∗
2 V2 = In−r; U∗U2 = 0; V ∗V2 = 0; and Σ2 ∈R(n−r)×(n−r)
is nonnegative and diagonal with σ1In−r −Σ2 > 0.
We need some additional notation, in particular
D −˜D : D, ˜D ∈D
Note that the elements of Z are not invertible, and in general are of the form (since dS+F ≡1)
£Z1, . . . , ZS, zS+1Im1, . . . , zS+F−1ImF −1, 0mF
where for each i ≤S, Zi = Z∗
i ∈Cri×ri, and for j ≤F −1, zS+j ∈R. Later, we will use the fact
that Z is a real inner product space, with inner product deﬁned by P, T ∈Z
trace (PiTi) +
For notational purposes, partition U and V compatibly with ∆as




where Ai, Bi ∈Cri×r, Ei, Hi ∈Cmi×r. With this notation, and a little manipulation, for any
Z ∈Z, we can write λmin (U∗ZU −V ∗ZV ) in terms of inner products in Z,
λmin (U∗ZU −V ∗ZV ) = min
where for each η ∈Cn, P η ∈Z is deﬁned by its block components
i := Aiηη∗A∗
i −Biηη∗B∗
S+j := η∗³
Let ∇M ⊂Z be the set of all such P η. That is
1 , . . . , P η
S+1Im1, . . . , pη
S+F−1ImF −1, 0mF
S+j as in (8.5), η ∈Cr, ∥η∥= 1
Although the matrices U and V are not unique, the set ∇M does not depend on their particular
choice. For a given Z ∈Z, we have
λmin (U∗ZU −V ∗ZV ) = min
P∈∇M⟨Z, P⟩.
Hence, it is the set ∇M that determines whether or not there is a Z such that
λmin (U∗ZU −V ∗ZV ) > 0.
Let the convex hull of a set V ⊂Z be denoted co (V).
Theorem 8.1 There exists a Z ∈Z such that λmin (U∗ZU −V ∗ZV ) > 0 if and only if
Proof This is a consequence of (8.7), and a standard result about convex hulls of sets in inner
product spaces, . ♯
Now the optimality condition can be derived. In the proof that follows, note that no appeal to
diﬀerentiability of eigenvalues is necessary, and all of the steps of the proof are elementary linear
algebra. The idea for such a simple approach is from , and .
Theorem 8.2
= ¯σ (M) if and only if 0 ∈co (∇M).
Proof ⇒Suppose that 0 ̸∈co (∇M). Choose a matrix Z ∈Z such that
λmin (U∗ZU −V ∗ZV ) > 0.
Equivalently,
λmax (V ∗ZV −U ∗ZU) < 0.
Now, note that for every α > 0
M∗(I −αZ) M −σ2
is equal to
1α (V ∗ZV −U ∗ZU)
ασ1 (σ1V ∗ZV2 −U ∗ZU2Σ2)
ασ1 (σ1V ∗
2 ZV −Σ2U∗
2 ZV2 −Σ2U∗
Call T := σ1 (σ1V ∗ZV2 −U ∗ZU2Σ2), and L :=
2 ZV2 −Σ1U∗
Using this
notation, the matrix in question becomes
1α (V ∗ZV −U ∗ZU)
Choose α > 0 small enough so that the three conditions
1I + αL < 0
1 (V ∗ZV −U ∗ZU) −αT
¢−1 T ∗< 0
are satisﬁed. This is possible, since I > 0, (V ∗ZV −U ∗ZU) < 0, and Σ2
1I < 0. Using
Schur complements, it is clear that for such α, the matrix
1α (V ∗ZV −U ∗ZU)
This implies that
M∗(I −αZ) M −σ2
Deﬁne D := I −αZ, and note that
< σ1 = ¯σ (M)
as desired.
(⇐) Suppose that inf
< ¯σ (M). Choose D ∈D such that ¯σ
¯σ (M). Deﬁne Z ∈Z via Z := I −D. Note that
1D = M ∗(I −Z) M −σ2
1 (I −Z) < 0
Hence, for all η ∈Cr, with ∥η∥= 1, we have
η∗V ∗£M∗(I −Z) M −σ2
1 [η∗U∗(I −Z) Uη −η∗V ∗(I −Z) V η]
1 η∗(V ∗ZV −U ∗ZU) η
Hence, this Z := I −D ∈Z satisﬁes
λmax (V ∗ZV −U ∗ZU) < 0
which is equivalent to
λmin (U∗ZU −V ∗ZV ) > 0.
By Theorem 8.1, it must be that 0 ̸∈co (∇M), as desired. ♯.
Connecting µ with ¯σ (M)
The convex hull of ∇M determines whether or not D := I is the optimum scaling. Following
 we ask, “what is true about M if 0 ∈∇M?” Since ∇M ⊂co (∇M), certainly
D := I is optimal, but is anything else true? The answer links the upper bound and µ.
Theorem 8.3 Let M ∈Cn×n be given, along with a block structure ∆, and deﬁne ∇M accordingly (equations (8.1), (8.3), (8.5), and (8.6)). Then, ¯σ (M) = µ∆(M) if and only if 0 ∈∇M.
Proof: The following four statements are equivalent:
2. There exists η ∈Cr, ∥η∥= 1 and Q ∈Q with QUη = V η
3. There exists ξ ∈Cn, ∥ξ∥= 1 and Q ∈Q with QMξ = ¯σξ
4. ¯σ(M) = µ∆(M). ♯
1 →2 : From the deﬁnition of ∇M, (8.6), 0 ∈∇M implies that for some η ∈Cr, ∥η∥= 1,
Aiηη∗Ai∗−Biηη∗Bi∗= 0
η∗(Ej∗Ej −Hj∗Hj) η = 0
Obviously, for i ≤S, there is a phase ejθi such that ejθiAiη = Biη. For j ≤F −1, ∥Ejη∥=
∥Hjη∥, so there exists a unitary matrix Qj such that QjEjη = Hjη. The only thing left
is the last full block. Since ∥Uη∥= ∥V η∥we must have ∥EF η∥= ∥HF η∥. This gives a
unitary matrix QF with QF EF η = HF η. Arranging the phases and Q’s in a block diagonal
fashion gives statement 2.
2 →1 : This follows along the lines of 1 →2.
2 →3 : The matrix M has a SVD of M = ¯σUV ∗+ U2Σ2V ∗
2 . Hence QM(V η) = ¯σQUη = ¯σV η.
Deﬁning ξ = V η gives statement 3.
3 →2 : A SVD of QM is QM = ¯σ(QU)V ∗+ (QU2)Σ2V ∗
2 . If QMξ = ¯σξ, then ξ must lie in the
subspace spanned by the right singular vectors associated with ¯σ. Hence there is a vector
η, satisfying ξ = V η. Obviously ∥η∥= 1 and
QUη = QUV ∗ξ = 1
¯σQMξ = ξ = V η
3 →4 : QMξ = ¯σξ implies that µ∆(M) = max
Q∈Q ρ (QM) ≥ρ(QM) ≥¯σ(M). However, ¯σ is always
an upper bound for µ, hence we must have equality.
4 →3 : This is clear, since max
Q∈Q ρ (QM) = µ∆(M). ♯
Theorem 8.3 can be used to relate the upper bound and µ∆(M). In particular, we consider block
structures ∆that have the following property: for all W ∈Cn×n, 0 ∈co(∇W ) always implies
0 ∈∇W . Note that while this property is stated in terms of ∇W , it is actually a property of
the underlying block structure. We will say that a block structure satisfying this property is
µ-simple. In Section 9, we will completely characterize which block structures are µ-simple, and
which block structures are not. For now, we prove that µ-simple block structures always have µ
equal to the upper bound.
Theorem 8.4 Suppose the block structure ∆is µ-simple. Then, for every M ∈Cn×n,
µ∆(M) = inf
Proof: Let β =
Let Dk be a sequence in D such that ¯σ
converges to β as k →∞. Denote Wk = D
. Since the sequence Wk is bounded, it
has a convergent subsequence with limit W. Obviously, by continuity of ¯σ and µ, ¯σ (W) = β
and µ∆(M) = µ∆(W). We claim that 0 ∈co(∇W ). If not, then there exist D ∈D and
ϵ > 0 such that ¯σ
= β −ϵ. Choose k so that ∥Wk −W∥<
κ(D), where
κ (·) denotes condition number. Then
2 (Wk −W) D−1
which yields
This contradicts that β was the inﬁmum, thus indeed 0 ∈co(∇W ). By hypothesis, this
means 0 ∈∇W so by Theorem 8.3, µ∆(W) = ¯σ (W). Recalling continuity, we get µ∆(M) =
β as desired. ♯
Consider the minimization over the D’s. Since we are minimizing the maximum singular value,
the top singular values tend to coalesce, so that at the minimum, the multiplicity of ¯σ is greater
than or equal to 2. This is typical of any “min max” problem. Suppose though, that at the
minimum, ¯σ
was distinct. Obviously, since we are at a minimum, we must have
0 ∈co (∇). But if the multiplicity of ¯σ is only 1, then ∇is a single point, hence ∇= {0}. This
reasoning gives:
Corollary 8.5 If, at the minimum of ¯σ
, the maximum singular value has multiplicity of 1, then µ (M) = min
Properties of ∇
In this section, we study the convexity properties of the set ∇, since the relationship between
µ∆(M) and inf
depend on the relationship between ∇and co (∇). It will be
shown, that for some block structures ∆, the implication
0 ∈co (∇W ) →0 ∈∇W
holds for every complex matrix W of appropriate dimensions. Hence, for those block structures,
for every matrix M. Likewise, for other block structures, speciﬁc matrices can be constructed
for which the upper bound can be shown to be greater than µ. The upper bound may be equal
to µ for certain matrices (see Theorem 8.3 for example) but in general, the upper bound is not
equal to µ.
These upcoming results are summarized in the Table reftab.mubnd, which indicates section
numbers for the accompanying derivation or example. Note that the (S = 0, F = 1) entry is
trivial, and the (S = 1, F = 0) entry implies that for any M ∈Cn×n
which is a well known fact.
Before beginning, we make a notational change, for ease of exposition. Although the set ∇was
deﬁned as a subset of block diagonal, n × n Hermitian matrices, in this section we identify ∇
with the set Hr1 × Hr2 × · · · × HrS × RF−1.
S = 0, F = 2
The situation with two full blocks is relatively simple. Referring back to (8.5), ∇will always
have the form
∇= {η∗(E∗E −F ∗F) η : η ∈Cr, ∥η∥= 1}
for some given r > 0 and E, F ∈Cm1×r. Since E∗E −F ∗F is Hermitian, ∇is just a closed
interval in the real line. Obviously, this is always convex, so if 0 ∈co (∇), in fact, 0 ∈∇. Hence
by Theorem 8.4:
Theorem 9.1 If ∆consists of two full blocks (S = 0, F = 2), then
µ∆(M) = inf
Remark 9.2 The two block case was ﬁrst solved by Redheﬀer with a quite diﬀerent
approach involving the use of Schauder’s ﬁxed point theorem 
Consider the case when ∆consists of four 1 × 1 blocks, so S = 0, F = 4, and mj = 1 for each
j. Let a, b, and c be positive real numbers, d and f be complex numbers, and ψ1 and ψ2 be real
numbers. Deﬁne matrices U, V ∈C4×2 by
For the time being, suppose that these are both unitary matrices, so that U ∗U = V ∗V = I2.
Later we will actually assign the correct values, but at the moment we just assume this is already
done. Then deﬁne M ∈C4×4 by
With the assumptions of unitariness on U and V , (9.2) is a singular value decompostion of M.
M has two singular values at 1, and two singular values at 0. With respect to the block structure
∆that we have deﬁned, what properties does the set ∇M have? In particular:
is 0 ∈co (∇M)? If so, then inf
= 1, otherwise, it is less than 1.
is 0 ∈∇M? If so, then µ (M) = ¯σ (M) = 1, otherwise it is less than 1.
Since the multiplicity of the maximum singular value is 2, we can parametrize all unit vectors in
C2, and get a parametric representation of ∇M. It is easy to see that any vector η ∈C2, with
∥η∥= 1 is of the form
ejφ1 cos θ
ejφ2 sin θ
for some real φ1, φ2, and θ. As it turns out, ∇M depends only on the diﬀerence φ1 −φ2, which
we will denote as φ. Simply plugging in for the deﬁnition of ∇M from section 8, we get
a2 ¡cos2 θ −sin2 θ
4b2 sin θ cos θ cos φ
4c2 sin θ cos θ sin φ
∈R3 : φ, θ ∈R
It is apparent that 0 ̸∈∇M. That would require (from the ﬁrst coordinate in (9.3)) that θ =
π, for some integer n. The second and third coordinates being zero would then require both
cos φ = 0 and sin φ = 0, which is impossible. Hence 0 ̸∈∇M, and µ (M) < 1.
On the other hand, setting θ = 0, and then θ = π
2 , gives that both
elements of ∇M. Consequently, 0 ∈co (∇M). Therefore
= ¯σ (M) = 1
In order to complete the counterexample, we must choose the free variables so that U and V in
(9.2) are unitary, as we said we could.
and deﬁne a =
√γ , d = −
γβ, ψ1 = −π
2 and ψ2 = π. Some algebra later, we conclude that ∇M is the set of all
x ∈R3, such that ∥x∥=
3. Obviously, 0 ̸∈∇M, but 0 ∈co (∇M). Extensive searching
over the set Q in the lower bound formula has revealed that for M deﬁned above, µ∆(M) is
approximately 0.87326. This counterexample proves that for every block structure ∆satisfying
S + F ≥4, there exist matrices M with
µ∆(M) < inf
S = 0, F = 3 
In this problem, for every matrix M, ∇M ⊂R2, of the form
∈R2 : η ∈Cr, ∥η∥= 1
for some integer r, and Hermitian matrices H1 and H2 ∈Cr×r. In , it is shown that
this set is always convex, so that the upper bound is exactly equal to µ∆(M). For completeness,
the results needed to prove this are stated below.
Begin with some notation from . For any positive integer r, deﬁne the sets P r :=
{x ∈Cr : ∥x∥= 1} and Sr :=
©v ∈Rr+1 : ∥v∥= 1
ª. If H1, H2, . . . , Hq are Hermitian matrices
in Cr×r, deﬁne a function fH :P r →Rq by
for each η ∈P r.
Lemma 9.3 Let q be a positive integer. Let ai, ci ∈R, and bi ∈C for i = 1, . . . , q. For each i,
deﬁne a Hermitian 2 × 2 matrix Hi by
Then there exists a vector d ∈Rq and a matrix V ∈Rq×3 such that
fH (η) : η ∈P 2o
d + V u : u ∈S2o
where fH is deﬁned in (9.5).
Lemma 9.4 Let d ∈R2 and V ∈R2×3. Then the set
©d + V u : u ∈S2ª ⊂R2 is convex.
Hence, for q = 2 and r = 2, the set f
¡P 2¢ ∈R2 is convex. For a block structure with S =
0, F = 3, the set ∇is always of the form f (P r) ∈R2 (ie. q = 2). Recall though, that r is the
multiplicity of the maximum singular value. Conceivably, this can be any positive number, hence
the above reasoning needs to be generalized for r > 2.
Theorem 9.5 Let r be any positive integer. Let H1, H2 ∈Cr×r be Hermitian matrices. Then
fH (P r) =
∈R2 : η ∈Cr, ∥η∥= 1
is convex.
S = 1, F = 1
Consider a block structure of one repeated scalar block, and one full block, S =F =1. Recall the
deﬁnition of ∇M, equation (8.6). With this structure, the set ∇M will always be of the form
∇= {Aηη∗A∗−Bηη∗B∗: η ∈Cr, ||η|| = 1}
for some given r > 0 and A, B ∈Cr1×r. It is easy to see that in general, ∇is not convex. For
instance, take A = I and B = 0. However the following is always true.
Theorem 9.6 Let ∇be deﬁned as in (9.7), for arbitrary matrices A and B of appropriate
dimensions. If 0 ∈co (∇), then 0 ∈∇.
Proof: Suppose that 0 ∈co (∇). Then, for some integer p, there exist nonnegative αi with
i=1 αi = 1 and vectors ηi ∈Cr with ||ηi|| = 1 such that
αi (Aηiηi∗A∗−Bηiηi∗B∗) = 0
which is rewritten as
Since the αi are nonnegative, and not all 0, the dyad summation in (9.9) is a positive
semideﬁnite matrix that is not zero. Let X
2 be its Hermitian, positive semideﬁnite square
root. Therefore AX
2 B∗. Hence, there is a unitary matrix V such that
2 V . Let v be an eigenvector of V (with eigenvalue ejθ ) such that X
and deﬁne u := X
2 v. Note that u is nonzero. This gives Au = ejθBu, which implies that
This theorem, along with the LFT machinery developed earlier, can be used to give µ-based
derivations of several standard results in linear systems theory, such as the Bounded Real Lemma
and the Kalman-Yacobovitch-Popov Lemma. These are relatively straightforward exercises and
will not be pursued further here.
S = 2, F = 0
The block structure considered has S = 2 and F = 0. A cumbersome example which established
the same conclusion appeared in . The example presented
here is minimal, in the sense that no smaller problem (“smaller” meaning dimension of blocks
and M) could be a counterexample (by results in section 9.4). The dimension of each repeated
scalar block is 2.
(a) Let a ∈(0, 1) and γ ∈(0, 1) be given. Deﬁne the matrix M ∈R4×4 by
Deﬁne a block structure ∆2 := {δ2I2×2 : δ2 ∈C}.
(b) For all ∆2 ∈B2 the LFT S (M, ∆2) is well deﬁned, and appears as
S (M, ∆2) =
Note that for each such ∆2 = δ2I2, the spectral radius of S (M, ∆2) is simply √γ, which
by assumption is less than 1. With respect to the structure
∆:= {diag [δ1I2, δ2I2] : δi ∈C} ,
Theorem 4.3 implies that µ∆(M) < 1.
(c) Consider the product of two linear fractional transformations with diﬀerent ∆2’s in B2.
S (M, −I2) S (M, I2) =
γ (1 + a)2
For any γ ∈(0, 1), it is easy to choose a ∈(0, 1) so that the spectral radius of the above
product is greater than 1. For such choices, then, we must have
where D be the scaling set associated with ∆. Otherwise, by Lemma 4.9, the spectral
radius of any product of these LFT’s would be less than 1.
Remark 9.7 A deeper analysis can show that by proper choice of γ and a, the value of
can be made arbitrarily close to 1 +
2 while µ∆(M) < 1.
S = 1, F = 2
Next is an example for a block structure with S = 1 and F = 2. Again, the example here is
minimal — no smaller example could be a counterexample for this block structure. It is broken
down into 8 facts.
(a) Let ∆2 = {diag [δ1, δ2] : δi ∈C}. Then for any complex τ ̸= 0,
(b) Let a ∈C with |a| < 1. Deﬁne G on |δ| ≤1 as
Note that everywhere in the unit disk, G is deﬁned and looks like
Hence from
µ∆2(G(δ)) = 1
(c) G(δ) in (9.11) can be written as a linear fractional transformation. In particular, deﬁne the
matrix M by
It is simple to verify that for each |δ| ≤1, G(δ) = S (δI2, M).
(d) Deﬁne ∆1 := {δI2 : δ ∈C}, and ∆2 := {diag [δ1, δ2] : δi ∈C} and ∆the augmentation of
the two sets. Certainly µ∆(M) makes sense (dimensions are compatible), and µ∆(M) ≥1,
since µ2 (M22) = 1. Using (b) and (c), and Theorem 4.3, gives µ∆(M) ≤1. Therefore
µ∆(M) = 1.
(e) Deﬁne the usual scaling sets D1 and D2 compatible with ∆1 and ∆2. For any D2 ∈D2
2 S (δI2, M) D
Hence, with some simple calculus, it is easy to verify that for any β ≥1,
2 S (δI2, M) D
(f) Fact: Let γ > 0. If there is a ∆1 ∈∆1, ¯σ (∆1) ≤1
• I −M11∆1 is invertible
• ¯σ [S (∆1, M)] ≥γ
This fact is simply the contrapositive of Lemma 4.9.
(g) If we choose a β ≥1 such that β+|a|
β−|a| ≥β, then we can apply the results from (e) and (f)
above to conclude that
The logic is as follows: ﬁrst suppose β is chosen so that β+|a|
β−|a| ≥β. Then from equation
(9.13) we know that for every D2 ∈D2, there is a δ ∈C with |δ| ≤1
β such that
2 S (δI2, M) D
This satisﬁes the conditions of (f), therefore, for each D2 ∈D2
Carrying out the inﬁmum over D2 in (9.14) yields
where D is the diagonal augmentation of D1 and D2. Therefore the question becomes:
“What is the largest β such that β+|a|
β−|a| ≥β ?” Simple algebra gives the largest β as β =
|a|2+6|a|+1
. Note that as |a| ↗1 , the quantity
(h) In summary: Let ϵ > 0. Choose a ∈C, |a| < 1 such that
|a|2 + 6|a| + 1
Deﬁne M as in (9.12). Then, with respect to the augmented structure described in (d),
M ∈Rn×n, S = 0, F = 2
If M is real, and the block structure ∆consists of two full blocks, then the smallest perturbation
∆∈∆making I −M∆singular will actually be a real matrix, rather than complex. The proof
is rather simple using the ∇set. We also note that this result can be found in .
An elementary result from linear algebra is the key idea.
Lemma 9.8 Suppose r is a positive integer, and H ∈Rr×r is symmetric. Then
{η∗Hη : η ∈Cr, ∥η∥= 1} =
ηT Hη : η ∈Rr, ∥η∥= 1
In view of this, suppose M ∈R(n+m)×(n+m), ∆= {diag [∆1, ∆2] : ∆1 ∈Cn×n, ∆2 ∈Cm×m} and
the optimal D scaling has been computed. Note that in a 2-block problem, if the inﬁmum is not
achieved, then it must be that either M12 = 0 or M21 = 0, and µ∆(M) = max {¯σ (M11) , ¯σ (M22)}.
Then, with singular vectors, it is possible to construct a real perturbation of the form ∆:=
diag [∆1, 0] or ∆:= diag [0, ∆2] such that I −M∆is singular, and ¯σ (∆) =
µ∆(M). Next,
consider the case when the inﬁmum is acheived.
Let D be the optimal scaling, and deﬁne
2 . Since the optimal D scaling is of the form diag [d1In, Im], where d1 > 0, it is
clear that W is still real. Hence, 0 ∈co (∇W ). Then, for some U ∈R(n+m)×r, V ∈R(n+m)×r,
W = σ1UV ∗+ U2Σ2V ∗
µ∆(M) = σ1 = ¯σ (W) .
If U and V are partitioned with respect to the block structure as
then ∇W is
∇W := {η∗(E∗
1 F1) η : η ∈Cr, ∥η∥= 1}
By assumption, the D scaling is optimal, so 0 ∈co (∇W ) = ∇W . Using the lemma, this implies
there is a η ∈Rr, with ∥η∥= 1 such that ∥Eiη∥= ∥Fiη∥for i = 1, 2. It is easy then to construct
real, orthogonal matrices Q1 and Q2 such that QiEiη = Fiη. Deﬁning Q := diag [Q1, Q2] yields
which shows what we had claimed — in the two-block (full blocks) µ problem, with M real, the
minimizing perturbation may be taken to be a real matrix.
The next theorem is a mild generalization of these ideas.
Theorem 9.9 Let ∆R ∈Rn×n be a given structure, and deﬁne the following 4 augmented
structures:
{diag [∆R, ∆1, ∆2] : ∆R ∈∆R, ∆1 ∈Rn1×n1, ∆2 ∈Rn2×n2}
{diag [∆R, ∆1, ∆2] : ∆R ∈∆R, ∆1 ∈Rn1×n1, ∆2 ∈Cn2×n2}
{diag [∆R, ∆1, ∆2] : ∆R ∈∆R, ∆1 ∈Cn1×n1, ∆2 ∈Rn2×n2}
{diag [∆R, ∆1, ∆2] : ∆R ∈∆R, ∆1 ∈Cn1×n1, ∆2 ∈Cn2×n2}
Then for M ∈R(n+n1+n2)×(n+n1+n2),
µ∆rrr (M) = µ∆rrc (M) = µ∆rcr (M) = µ∆rcc (M)
Proof: The proof follows from the previous discussion and Theorem 4.3. ♯
M ∈Rn×n, S = 0, F = 3
Unfortunately, the argument used above breaks down in this case, and no longer may the smallest
perturbation be assumed real. The following is from and . The perturbation set is 3 1 × 1 blocks. Let U, V ∈R3×2 be
where α, γ, β ∈R, and have been chosen so that U T U = V T V = I2 (that is easy to do). Deﬁne
M := UV T ∈R3×3. Then ¯σ (M) = 1, and η ∈C2, ∥η∥= 1, parametrized by
−β2 cos 2θ
¡γ2 −α2¢ cos 2θ + 4 cos (ψ −φ) γα cos θ sin θ
: θ, ψ, φ ∈R
It is easy to see that 0 ∈∇M, by choosing θ = 2n+1
π and ψ −φ = 2m+1
π for any integers n, m.
The only vectors η which lead to 0 ∈∇M are
which is always complex. Consequently, the only matrices satisfying ∆∈∆, ¯σ (∆) = 1, and
I −M∆singular are complex perturbations.
M ∈Rn×n, S = 1, F = 1
Again, the smallest perturbations are in general complex. Suppose G(z) is a stable, n’th order,
SISO transfer function with ∥G∥∞= 1, |G(1)| < 1, and |G(−1)| < 1. The state-space matrix M
of this transfer function will have µ∆(M) = 1, but all of the perturbations ∆= diag [δ1In, δ2]
satisfying ¯σ (∆) = 1, and det (I −M∆) = 0 will be complex.
Optimal scalings for inf
with M ∈Rn×n
If the matrix M is real, then the minimum point in the convex hull of ∇M is always real, so each
block of the optimal D ∈D can be chosen to be real. The proof is very simple.
Theorem 9.10 Let DR be the set of real, symmetric members of D. If M is real, then
Proof: Let D ∈D be given, with D = Dr + jDi, and ¯σ
< β. Note that Dr =
r > 0, Dr ∈DR, and Di = −DT
MT (Dr + jDi) M −β2 (Dr + jDi) < 0.
Hence, the real part of (9.16) is also symmetric, negative deﬁnite and
MT DrM −β2Dr
which implies that
so the inﬁmums are the same. ♯
Transfer functions, state space matrices, µ and Robust Performance
In this section we begin by establishing some relationships between transfer function matrices
and matrices made up of state-space realizations. We have already seen one instance of such
a connection. In Section 4, it was proven that a ﬁnite dimensional linear system is stable, and
has ∥·∥∞< 1 if and only if the structured singular value of the state space system matrix is
less than 1 (recall, the block structure consisted of a repeated scalar block, and a full block).
We explore this type of manipulation in more detail. With these relationships established, the
robust performance properties of an uncertain linear system are investigated, and stated in terms
of structured singular value tests. Finally, the connections between the µ theory and Riccati
equations for testing H∞norm bounds are brieﬂy reviewed.
Transfer Function matrices and State Space matrices
Let M ∈C(n+m)×(n+m) be given, partitioned as usual, and deﬁne the transfer function matrix
= M22 + M21 (zIn −M11)−1 M12.
Suppose that ∆⊂Cm×m is a block structure. Deﬁne ∆P as
∆P := {diag [δ1In, ∆] : δ1 ∈C, ∆∈∆} .
Applying Theorems 4.3 and 6.5, the following statements are equivalent:
1. ρ (M11) < 1 and
θ∈[0,2π] µ∆
2. ρ (M11) < 1 and
θ∈[0,2π] µ∆
3. ρ (M11) < 1 and max
µ∆(S (δ1In, M)) < 1
4. µ∆P (M) < 1
Hence, the peak value of µ of a frequency response is related to a larger µ problem on the state
space matrix of the transfer function in question. This generalizes the example in Section 4,
where the ∥·∥∞norm (maximum singular value across frequency) was considered.
Similar results are possible when the upper bound is used instead of µ. Suppose that D ⊂Cm×m
is the scaling set associated with ∆, as in (3.6). For any D ∈D, deﬁne
{diag [δ1In, ∆2] : δ1 ∈C, ∆2 ∈Cm×m}
Note that µ∆σ(·) is simply the maximum singular value, and that ∆N is µ-simple. Then, the
following are equivalent.
1. ρ (M11) < 1 and
2. ρ (M11) < 1 and inf
2 S (δIn, M) D−1
3. ρ (M11) < 1, and inf
µ∆σ(S (δIn, MD)) < 1
D∈D µ∆N (MD) < 1
X∈Cn×n,X=X∗>0
Also, if M ∈R(n+m)×(n+m), then the scalings can be chosen to be real, so that the following are
equivalent.
1. ρ (M11) < 1 and
2. ρ (M11) < 1 and
X∈Rn×n,X=XT >0
These relationships are very signiﬁcant. Consider the simple situation where D := {Im}, in other
words, an unscaled transfer function. The equivalences imply that the linear system is stable,
and has ∥·∥∞norm less than 1 if and only if there is a state-coordinate transformation . This will be discussed further at the end of this section.
State-Space/Frequency Domain tests for Robust Performance
We begin with a matrix M ∈C(n+np+m)×(n+np+m), partitioned as below, relating several variables of a linear system by
The uncertainty is modeled by a feedback loop from z to w through a structured ∆∈∆, where ∆
is a prescribed m×m block structure (note that we have assumed that the number of disturbance
inputs equals the number of errors, and that the perturbation matrices are square — this can all
be trivially generalized to include nonsquare situations). Hence, the uncertain system’s output
error ek is driven by the input disturbance dk, and the state equations are given as
= S (M, ∆)
With respect to the partition, S (M, ∆) is
∆(I −M33∆)−1 £ M31
This is shown in Figure 10. Deﬁne three augmented block structures, ∆N, ∆S and ∆P as
©diag [δ1In, ∆2] : δ1 ∈C, ∆2 ∈Cnp×npª
∆S := {diag [∆N, ∆] : ∆N ∈∆N, ∆∈∆}
©diag [∆2, ∆] : ∆2 ∈Cnp×np, ∆∈∆
along with the corresponding scaling sets DN, DS and DP . Motivation for this notation is that
the subscript N could mean norm or nominal, S could mean state-space, and P could mean
performance. We begin with the main result for linear, time-invariant perturbations, , .
Theorem 10.1 (Time-invariant, robust performance) Given the matrices and sets as de-
ﬁned above, the following conditions are equivalent:
1. there exists a constant β ∈[0, 1) such that for each ﬁxed ∆∈B∆, the uncertain system
(10.2) is well-posed (I −M33∆is invertible), stable, and for zero-initial-state-response, the
error e satisﬁes ∥e∥2 ≤β∥d∥2
2. µ∆S (M) < 1
(SSµ test)
3. ρ (M11) < 1
θ∈[0,2π] µ∆P
(FDµ test).
Proof: Introduce two intermediate statements:
µ∆(M33) < 1
∆∈B∆µ∆N (S (M, ∆)) < 1
ρ (M11) < 1
δ∈C,|δ|≤1 µ∆P (S (δIn, M)) < 1
The proof that 1 ⇔1.5 follows from the deﬁnition of stability for a ﬁnite dimensional, linear,
time-invariant, discrete time system, the relationship between H∞norms and l2 gain, and
the equivalence between µ and the ∥· ∥∞norms for transfer functions, as developed in
section 10. Items 1.5, 2 & 2.5 are equivalent by Theorem 4.3, while 2.5 and 3 are equivalent
by Theorem 6.5.
Remark 10.2 Item 1 in this theorem is the desired robust performance conclusion. Item 1.5
rephrases Item 1, using the µ characterization of ∥·∥∞< 1. Items 2 and 3 are known respectively
as the “state-space µ test” (SSµ) and the “frequency domain µ test”( FDµ). Both of these tests
involve computing µ for various matrices. Recall that upper and lower bounds for µ are all that
can be computed. Hence, we will investigate the additional conclusions that are possible when
upper bound is used to implement the computational tests of items 2 and 3.
Remark 10.3 The FDµ test is what is most commonly associated with the structured singular
value and is often referred to as a µ-plot. It is essentially a Bode magnitude plot with µ (·)
replacing ¯σ (·) or | · |. The SSµ test was introduced in .
Upper bounds
Using the ¯σ
upper bound in place of µ, we can derive suﬃcient conditions for robust
performance. The resulting state-space upper bound test (SSUB) and the frequency domain
upper bound test (FDUB) are
While the various µ tests given in Theorem 10.1 are all equivalent, these two upper bound tests
are very diﬀerent.
In particular, recalling the results from the previous section on scaling
a transfer function with a constant similarity transformation, the SSUB condition is actually
equivalent to
θ∈[0,2π] ¯σ
This condition is much stronger than the frequency domain upper bound test, since in (10.3),
the same DP ∈DP must work for all θ ∈[0, 2π].
For that reason, we call equation (10.3)
the frequency domain constant D test, FDCD. Listed, from strongest to weakest, the various
conditions are:
θ∈[0,2π] ¯σ
θ∈[0,2π] µ∆P
µ∆S (M) < 1
Condition 1 in Theorem 10.1
Note that in both instances where the implication is given as ⇓rather than ⇕, there truly is a
gap. Also, there are two such gaps between the state space tests, SSUB and SSµ, while there
is only one gap between the frequency domain tests, FDUB and FDµ. The top conditions are
the strongest, and are equivalent to a very strong form of robust Lyapunov stability, .
Given that the upper bound is computable, one might ask which test should be used, the state
space upper bound test, SSUB (equivalently FDCD), or the frequency domain upper bound test,
FDUB? The answer depends on the assumptions that are made about the perturbations. If the
SSUB is used and the bound satisﬁed, then the robust performance conclusion holds for timevarying perturbations (and with proper interpretation, cone bounded nonlinear perturbations).
Theorem 10.4 Let M be given as in (10.1), along with an uncertainty structure ∆. If there is
a DS ∈DS such that
then there exist constants c1 ≥c2 > 0, such that for all perturbation sequences {∆k}∞
∆k ∈∆, ¯σ (∆k) < 1
β, the time-varying, uncertain system
= S (M, ∆k)
is zero-input, exponentially stable, and furthermore, if {dk}∞
k=0 ∈l2, then
2 + c1∥x0∥2
In particular, ∥e∥2
2 + c1∥x0∥2.
Proof: Note that DS will appear as DS = diag [D1, d2I, D], where D1 = D∗
1 > 0, D1 ∈Cn×n.
Using equation (10.4), it is easy to show that regardless of ∆k ∈∆, ¯σ (∆k) < 1
β, the norms
of pertinent vectors satisfy
1 xk+1∥2 + ∥ek∥2 ≤β2
1 xk∥2 + ∥dk∥2
Let c1 and c2 be the square roots of the maximum and minimum singular values of D1.
Summing and taking limits yields the ﬁnal result. ♯
Unfortunately, this test (like the SSµ) does not scale in a convienient manner. In other words,
if there is a DS ∈DS such that ¯σ
= 1.001, it is impossible to conclude anything
about the robust performance characteristics of this system. It is necessary to scale the perturbation channels and/or disturbance channels (this amounts to scaling rows of M to produce
a modiﬁed system Mscl) until a DS can be found such that ¯σ
< 1, and then
robust performance with respect to the scaled down uncertainty and performance norm is guaranteed. For example, let L = diag
In, 0.8Inp, 1
. Suppose that there is a a DS ∈DS such
Then it is possible to conclude that for perturbations satisfying
¯σ (∆k) ≤0.8, the error is bounded by, ∥e∥2
2 ≤(1.2)2∥d∥2
2 + c1∥x0∥2.
Since FDUB is a weaker condition than the SSUB, it is “closer” to the exact condition for robust
performance under linear, time-invariant perturbations. Therefore, if the perturbations are better
modelled as linear, time-invariant perturbations, this frequency domain test is more appropriate.
Also, this test scales, that is, if
then the conclusion is that for all ∆∈∆, with ¯σ (∆) < 1
β, the perturbed system is stable, and
the ∥· ∥∞norm of the transfer function from the disturbance to error is ≤β. Hence, peak values
other than 1 still give useful information.
However, if the frequency domain test is used, no general conclusion can we reached about timevarying perturbations, . In , some connections between
the frequency domain test and robust stability to cone bounded nonlinearities are developed.
For reference, continuous-time versions of these theorems, as well as theorems with more sophisticated assumptions about the structured perturbations are found in ,
 , , ,
and .
H∞norms, Riccati Equations, and LMIs
We now consider the relationship between the bounds given above and Riccati equations for
computing the H∞norm of the discrete time system
Let M ∈C(n+m)×(n+m) be the block state space matrix of the system
Assume that A is stable (ρ(A) < 1) and deﬁne
A + B(I −D′D)−1D′C
−B(I −D′D)−1B′
C′(I −DD′)−1C
Suppose E is nonsingular and deﬁne a symplectic matrix as
· E + GE′−1Q
It can be shown that the following statements are equivalent:
(a) ∥D + C (zIn −A)−1 B∥∞< 1
(b) S has no eigenvalues on the unit circle and ∥C(I −A)−1B + D∥< 1
(e) ∃X ≥0 with I −D′D −B′XB > 0, (I + GX)−1E stable, and
E′XE −X −E′XG(I + XG)−1XE + Q = 0
(f) ∃X > 0 such that I −D′D −B′XB > 0 and
E′XE −X −E′XG(I + XG)−1XE + Q < 0
(g) ∃X > 0 such that
(h) ∃T nonsingular such that
Note that (e) is a Riccati equation, and the SSUB in (h) is equal to µ because of the block
structure. It is equivalent to (g), which is 2 LMIs. The connection between (f) and (g) is just
the Schur complement formula for positive deﬁnite matrices.
Quadratic Lyapunov functions for uncertain systems
Some computable results on the quadratic stability of linear systems under structured, linear
fractional uncertainty are possible.
Again, consider positive integers n and m, and suppose
M ∈C(n+m)×(n+m). Let ∆be a structured perturbation set with ∆⊂Cm×m. Assume that
µ∆(M22) < 1, so that S (M, ∆) is well deﬁned for all ∆∈B∆.
k=0 with ∆k ∈B∆be given, along with an initial condition x0 ∈Cn, deﬁne xk ∈Cn
by the uncertain diﬀerence equation
xk+1 = S (M, ∆k) xk.
In this formulation, the matrix M11 may be thought of as a nominal state space model and
∆k ∈B∆as a norm bounded perturbation from an allowable perturbation class, ∆.
matrices M12, M21, and M22 reﬂect prior knowledge on how the unknown perturbation aﬀects
the nominal dynamics, M11.
Deﬁnition 11.1 The pair (M, ∆) is quadratically stable if there exists a P ∈Cn×n, with
P = P ∗> 0, such that
∆∈B∆λmax ([S (M, ∆)]∗P S (M, ∆) −P) < 0
The deﬁnition simply implies that there is a single quadratic Lyapunov function, V (x) := x∗Px,
that establishes the stability of the entire set
{S (M, ∆) : ∆∈B∆}
Equivalently, the deﬁnition implies that there is a positive deﬁnite P ∈Cn×n such that
2 S (M, ∆) P −1
Hence, with respect to a single coordinate change deﬁned by P
2 , S (M, ∆k) is always a contraction, regardless of ∆k ∈B∆. As the uncertain system in (11.1) evolves, the Euclidean norm of
2 xk∥2, decreases by at least a factor of γ every time step k, and hence robustness with
respect to time varying perturbations is guaranteed. Note that if both M is real, and ∆⊂Rm×m,
then by using an argument similar to that in Theorem 9.10, the matrix P, if it exists, will also
Using Theorem 4.10 and the fact that in some instances ( when 2S + F ≤3), µ and the upper
bound are always equal, we can establish neccessary and suﬃcient conditions for a pair to be
quadratically stable, in terms of a scaled state-space test, and/or a scaled H∞norm test. Several
cases are outlined below, along with a chain of equivalences which produces the result. As usual,
deﬁne the transfer function G(z) as
G(z) := M22 + M21 (zI −M11)−1 M12.
Note that this is the transfer function of the linear system that the perturbation ∆k “sees.”
Real state-space data, 1 full real perturbation
Suppose that M ∈R(n+m)×(n+m), and that ∆= Rm×m. Assume that ¯σ (M22) < 1. For any
P ∈Cn×n with P = P ∗> 0, let
Also, deﬁne
©diag [∆1, ∆2] : ∆1 ∈Rn×n, ∆2 ∈Rm×mª .
Then, using Theorem 4.3 and the results from sections 9.7 and 10, the following statements are
equivalent:
1. There exists P ∈Cn×n, P = P ∗> 0 such that max
2 S (M, ∆) P −1
2. There exists P ∈Rn×n, P = P T > 0 such that max
2 S (M, ∆) P −1
8. ρ (M11) < 1 and ∥G∥∞< 1
The main point here is that the uncertain system is quadratically stable with respect to full
block, norm bounded, real perturbations (condition 1) if and only the H∞norm of the transfer
function that the perturbation sees is less than 1 (condition 8). Conditions 2-7 are intermediate
steps which link the two conditions together. This same style is used in Sections 11.2-11.4.
Complex state-space data, 1 full complex perturbation
Suppose that M ∈C(n+m)×(n+m), and that ∆= Cm×m. Assume that ¯σ (M22) < 1. For any
P ∈Cn×n with P = P ∗> 0, let
Also, deﬁne
©diag [∆1, ∆2] : ∆1 ∈Cn×n, ∆2 ∈Cm×mª .
Then, using Theorem 4.3 and the results from sections 9.1 and 10, the following statements are
equivalent:
1. There exists P ∈Cn×n, P = P ∗> 0 such that max
2 S (M, ∆) P −1
7. ρ (M11) < 1 and ∥G∥∞< 1
Some interesting connections between diﬀerent notions of stability can be made at this point. To
do so, consider the deﬁnition of robust stability given below:
Deﬁnition 11.2 The pair (M, ∆) is robustly stable if
∆∈B∆ρ (S (M, ∆)) < 1.
Recall the example in section 4, which demonstrated an application of the Main Loop theorem.
In that example, LFT arguments were given to prove that the pair (M, Cm×m) is robustly stable
if and only if ∥G∥∞< 1, where G(z) = M22 + M21 (zIn −M11)−1 M12. That result, along with
section 11.1 and this section combine to form the following theorem, , , :
Theorem 11.3 Suppose that M ∈R(n+m)×(n+m), with ¯σ (M22) < 1. Deﬁne G(z) := M22 +
M21 (zIn −M11)−1 M12. Then, the conditions
1. The pair (M, Rm×m) is quadratically stable
2. The pair (M, Cm×m) is quadratically stable
3. The pair (M, Cm×m) is robustly stable
4. ρ (M11) < 1, and ∥G∥∞< 1
are equivalent.
It is important to note that conditions (1) and (3) become incomparable (neither implies the
other) when the perturbation set becomes structured, , .
Complex state-space data, 1 repeated complex perturbation
Suppose M ∈C(n+m)×(n+m) and ∆= {δIm : δ ∈C}. Let ∆S be deﬁned as
©diag [∆1, δ2Im] : ∆1 ∈Cn×n, δ2 ∈C
Then, using Theorem 4.3 and the results from sections 9.4 and 10, the following statements are
equivalent:
1. There exists P ∈Cn×n, P = P ∗> 0 such that max
2 S (M, δ2Im) P −1
6. ρ (M11) < 1 and
In this section, the matrix S (M, δIm) is a rational function of the scalar, complex parameter δ.
We have shown that quadratic stability with respect to such a parameter can be ascertained by
determining if the convex set
: P ∈Cn×n, D2 ∈Cm×m, X = X∗> 0, M ∗XM −X < 0
is nonempty.
Complex state-space data, 2 complex full blocks
Suppose that M ∈C(n+m)×(n+m), and ∆is
©diag [∆1, ∆2] : ∆i ∈Cmi×miª ⊂Cm×m
©diag [∆0, ∆] : ∆0 ∈Cn×n, ∆∈∆
Then, using Theorem 4.3 and the results from sections 9.3 and 10, the following statements are
equivalent:
1. There exists P ∈Cn×n, P = P ∗> 0 such that max
2 S (M, ∆) P −1
d1,d2>0 ¯σ
d1,d2>0 ¯σ
7. ρ (M11) < 1 and
Hence quadratic stability with respect to two full complex blocks of uncertainty is equivalent to
an optimally scaled small gain condition. Note that for any α > 0,
is either empty or is a convex set (an interval).
Conclusions
Some of these results are well-known, and available in the literature, although the treatment here
is more uniﬁed. The results relating quadratic stability and ∥·∥∞for full block perturbations
(sections 11.1 and 11.2) are proven for SISO systems in , , and for
MIMO systems in . The results for 2 complex blocks is from
 , while the result for a single complex repeated scalar perturbation,
is, to our knowledge, new. Similar results are easily derived for continuous-time systems, using
a bilinear transform. By deﬁning
and noting that
A∗P + PA < 0
2 S (B, A) P −1
the results relating ∥·∥∞(possibly a scaled norm, as in sections 11.4 and 11.3) and quadratic
stability can be derived in the same manner.
µ-Synthesis via Optimally Scaled LFTs
This paper so far has only considered µ-analysis.
The problem of µ-synthesis is much more
diﬃcult, and will be discussed in this section. In general, µ-synthesis methods have focused on
minimizing µ of some rational matrix over stabilizing controllers using the frequency-domain
upper bound (FDUB) and have been successfully used in many applications. Nevertheless, the
theoretical basis for µ-synthesis is much weaker than for µ-analysis. This section will consider
a µ-synthesis problem involving only constant matrices, to explore the potential diﬃculties in a
simple setting. For an introduction to µ-synthesis in the rational case, see (Balas, Doyle, et al,
Suppose that a matrix M depends on a free parameter Q. How can Q be found so as to minimize
µ∆(M)? In this section we consider this problem when M depends on a free matrix Q in a linear
fractional manner, and we attempt to minimize the upper bound for µ∆(M), rather than µ∆(M)
itself. This problem is ﬁrst reduced to an aﬃne, rather than linear fractional, transformation,
and then partially solved using a elementary extension to matrix dilation theory , . In the lemmas to follow, F denotes either the real or complex ﬁeld.
Lemma 12.1 Let R ∈Fn×n, U ∈Fn×r, T ∈Ft×r, and V ∈Ft×n, where r, t ≤n. Let Z ⊂Fn×n
be a prescribed set of positive deﬁnite matrices. Then
Q∈Fr×t,Z∈Z
det(I−TQ)̸=0
R + UQ (I −TQ)−1 V
˜Q∈Fr×t,Z∈Z
Proof: For any T ∈Ft×r, the closure of the set
Q (I −TQ)−1 : Q ∈Fr×t, det (I −TQ) ̸= 0
is all of Fr×t, which shows that the inﬁmums are the same. ♯
Hence, in order to solve general linear fractional transformation optimization problems, only
aﬃne transformations need be considered. We also assume (without loss in generality) that U is
full column rank, and that V is full row rank. The ﬁrst lemma addresses the unscaled problem,
and comes from , :
Lemma 12.2 Let R, U, V , be given as above. Suppose U⊥∈Fn×(n−r) and V⊥∈F(n−t)×n are
chosen such that
are both invertible, and that U ∗U⊥= 0r×(n−r), V V ∗
0t×(n−t). Let α > 0. Then
Q∈Fr×t ¯σ [(R + UQV )] < α
if and only if
The next lemma partially answers the synthesis question when similarity scalings are included.
The proof is in ) and .
Lemma 12.3 Let R, U, V, U⊥and V⊥be given as above. Let α > 0 and Z ⊂Fn×n be a given
set of positive deﬁnite, Hermitian matrices. Then
2 (R + UQV ) Z−1
if and only if there is a Z ∈Z such that
RZ−1R∗−α2Z−1´
Note that the condition imposed on Z in equation (12.2), is convex, therefore, if the set Z is itself
convex, determining solutions of equation (12.2) is a convex feasability problem. Similarly, the
condition imposed on Z−1 in equation (12.3) is convex in Z−1, so if the set Z−1 is convex, this
is also a convex feasability problem. The convexity of these “one-sided” problems is exploited
in and , where some robust control
problems are formulated, and recast as convex optimizations, using this scaled linear fractional
transformation approach. Unfortunately, the complete problem, which involves both Z and Z−1
conditions, is more diﬃcult, and at the moment, unsolved. In some special cases, it may be
possible to obtain computable necessary and suﬃcient conditions. For instance, if
then both conditions deﬁne open intervals in the real line, and it is easy to check if these intervals
intersect (moreover, the intersection is either empty or convex). More generally though, the set
of “good Z’s” may be a disconnected set. Speciﬁcally, given matrices R, U and V , and α > 0,
let Zgood be
Zgood(α) :=
2 (R + UQV ) Z−1
It is this set, Zgood(α), which may be disconnected. In particular, let α := 1, and
Z = {diag [z1, z2, 1] : z1 > 0, z2 > 0} ,
and deﬁne matrices
Applying the formulae, we have that Z ∈Zgood if and only if zi > 0, z1+z2 > 2, and 1
Clearly, the region of good Z’s in the z1-z2 plane consists of two slivers near the axis, which is not
a connected set. Unlike the analysis problem the level sets of scalings in the synthesis problem
are not convex. We are currently investigating the implications of this property.
Summary of some related work
This section outlines some work related to this paper, beginning with a brief history of the early
development of the µ theory. This outline is not intended to be exhaustive or complete, but simply
to touch on a few of the topics nearest to this paper that were not considered in detail. LMIs are
discussed as potentially unifying theoretical and computational tools. The relationship between
µ and quadratic versus L1 notions of robust performance and robust stability is considered next,
followed by µ with mixed real and complex perturbations. The section ends with model validation
and generalizations of µ.
History of early work
In this section, we will brieﬂy review the ideas that most inﬂuenced the original development of
the µ theory. These remarks are drawn mainly from earlier papers , , and ), but are repeated here for the convenience of the reader.
An obvious inﬂuence on the development of the µ theory was the work in so-called Robust
Multivariable Control Systems from the late ‘70s, (See, for example, [IEEE]) which in turn drew
heavily on earlier work in stability analysis , ,
 , ), particularly the small gain and circle theorems.
theorems established suﬃcient conditions for stability of nonlinear components connected in
feedback. The emphasis in the early robustness work was on small gain type conditions involving
singular values that were both necessary and suﬃcient for stability of sets of linear systems
involving a single norm bounded but otherwise unconstrained perturbation. Another emphasis
for much of the robustness theory was on using singular value plots as a means of generalizing
Bode magnitude plots to multivariable systems.
While methods based on singular values were gaining in popularity, it became evident that their
assumption of unstructured uncertainty was too crude for many applications. Furthermore, the
problem of robust performance was not adequately treated. Freudenberg, Looze, et al 
studied these issues using diﬀerential sensitivity and suggested that something more than singular values was needed. It was a natural step to introduce structured uncertainty of the type
considered in this paper for an early treatment). The so-called conservativeness of singular values was based the fact that the unscaled bounds ρ (M) ≤µ∆(M) ≤¯σ (M)
could be arbitrarily far oﬀ, and research was begun to provide improved estimates of µ, with an
initial focus on the nonrepeated, complex case (S = 0).
It was obvious that the sharper bounds
Q∈Q ρ(QM) ≤max
∆∈B∆ρ (∆M) = µ∆(M) ≤inf
could help alleviate the conservativeness somewhat. The upper bound is similar to the multiplier
methods that were used in nonlinear stability analysis to reduce the conservativeness of small
gain type methods , but the use of both upper and lower bounds, and the
questions of how close the bounds were and how to eﬃciently compute them were new and open.
As we saw in Section 6, the equality of the lower bound and µ is relatively straightforward and
not surprising. What is remarkable, even in retrospect, is that the upper bound is often close to
µ and is in fact equal to µ for certain simple block structures.
There was substantial numerical evidence for the upper bound results before they were proven.
Engineers at Honeywell’s Systems and Research Center, particularly Joe Wall, began routinely
using a simple generalization of Osborne’s routine to approximate the upper
bound in (13.1) and gradient search methods to ﬁnd a local maximum for the lower bound. Osborne’s algorithm minimizes the Frobenius norm rather than the maximum singular value, and
the scalings produced can be used to approximate the upper bound. The consistent closeness of
the bounds, usually within a few percent, suggested that there was a deeper connection between
the bounds. Ironically, minimizing the Frobenius norm remains the cheapest method of approximating the upper bound. Safonov suggested a somewhat less general approximation to
the upper bound based on Perron eigenvectors which is comparable to Osborne in speed and
While the µ framework arises naturally in studying robust stability with structured uncertainty,
the use of µ to treat directly the problem of robust performance with structured uncertainty was
ﬁrst explicitly noted in . As noted above, this is a consequence of the
intimate connection between µ and LFTs , and Packard, 1988). In retrospect, it is
clear that Redheﬀer had developed the foundation of this connection in his work
on LFTs in the late 1950’s. In fact, as noted earlier, Redheﬀer had even proven that the upper
bound in (13.1) was an equality for the case where S = 0 and F = 2. While Redheﬀer’s results
were not well-known in the control community until the µ theory was already well-developed, the
rediscovery of his work has since had an important inﬂuence, not only on the further development
of µ but in other areas as well ).
Linear Matrix Inequalities
We have seen in this paper how LMIs arise naturally in both µ analysis and synthesis in the
computation of upper bounds. The general LMI problem involves sets of the form
½ diag [X1, . . . , XS, x1I, . . . , xF I] :
Xi ∈Cri×ri, Xi = X∗
and a list of matrices Ai, Bi, Ci, Di. The simplest general LMI problem is to determine whether
there exists X ∈X such that
i XBi + XCi + C∗
i X + Di < 0
Depending on the particular problem, the < may be a ≤. It is easy to see that these inequality
conditions produce a set of solutions which are convex, which makes LMIs attractive computationally. (In the synthesis problem in Section 12, there are additional constraints that destroy
convexity.) This is a decision problem; the answer is yes or no. Sometimes, however, the Ai,
Bi, Ci, and Di are functions of a real, positive parameter α, and we want to know, for example,
what is the largest α for which there is no solution. Typically this involves an iteration on α,
and consequently, answering the decision question many times.
Recall that the upper bound for µ can be rewritten as an LMI of the form
∃X > 0 : M ∗XM −β2X < 0
It has recently been show that a number of other problems can be reduced to solving LMIs. In
 balanced truncation model reduction is extended to uncertain LFT
systems, with similar extensions of the parametrization of all stabilizing controllers in . The LFT/LMI machinery not only extends the standard results in important ways, it
simpliﬁes the proofs, often substantially. Exciting new developments in handling real parametric
uncertainty and model validation will
be outlined in subsequent subsections. In all cases, LMIs play a central role in computation of
solutions. We believe that LMIs will replace Lyapunov and Riccati equations, which are both
special cases of LMIs, as the central computational problems in robust control.
The problem of solving LMIs can be viewed in a number of ways, from solving a set of linear
equalities to minimizing the eigenvalues of a Hermitian matrix function . One of the
goals of our current research is to develop fast, reliable algorithms for solving LMIs which are
comparable to what is available for solving Riccati and Lyapunov equations. Several researchers
have already begun looking at this question.
One approach to solving LMIs is to convert them to eigenvalue optimization problems which
results in convex, non-diﬀerentiable functions for which numerous optimization methods have
been developed.
Boyd and Yang compare the eﬃciency of two convex programming
algorithms, Kelley’s cutting-plane algorithm and Shor’s subgradient algorithm. Boyd and Yang
ﬁnd Kelley’s cutting-plane algortihm to be most eﬀective of the two, since it converges in the
fewest number of iterations. An alternative convex programming method which has been used
for LMI problems is the ellipsoid method, which is also a cutting-plane method. Although these
methods are easy to implement, they are generally too slow to warrant considerable attention.
Overton studies the optimality conditions, and develops quadratically convergent algorithms for LMI’s.
Recently, interior point methods have been applied to LMI problems with favorable results.
Interior and exterior point methods are used to convert constrained minimization problems to
diﬀerentiable, unconstrained minimization problems, to which optimization algorithms such as
Newton’s method are applied . Jarre has used
an interior point algorithm for a problem similar to LMIs which required substantially fewer
iterations than does the cutting-plane algorithm used by Boyd and Yang. A similar approach is
taken by Boyd and El Ghaoui . We are currently investigating alternative functions for
solving LMIs using both interior and exterior point methods .
µ, Q, and L1
We have considered several diﬀerent measures of robust stability and performance in Section 10
from SSµ to the SSUB. We will concentrate on these two measures, and compare them brieﬂy
with another very important measure that has emerged in the L1 theory of robust performance
with structured uncertainty. Space constraints preclude a review of the L1 theory, which has
undergone a dramatic and impressive development in the last 5 years in the work of Khammash
and Pearson , and Dahleh and Khammash and references therein. For simplicity,
we will refer to the SSµ test as µ and the SSUB upper bound as Q (since it is directly related to
quadratic stability), and focus our attention on the robust performance problem, which clearly
includes robust stability as a special case.
The µ, Q, and L1 tests all guarantee robust performance, but with diﬀerent assumptions about
perturbations and the norm used for measuring the performance objective. The µ and Q theories
are used for L2 induced norms, while the L1 theory is used for L∞induced norms. A second
distinction is that the µ theory treats LTI perturbations, and the Q and L1 handle Nonlinear
and Time-Varying perturbations (NTV). This is summarized in the table below.
The cases on the diagonal, L2/LTI and L∞/NTV, are both necessary and suﬃcient for robust
performance. The L∞/LTI case is necessary and suﬃcient for robust stability, but the robust
performance question is open. The Q case (L2/NTV) is suﬃcient for robust performance, and
recent results, obtained independently using very diﬀerent methods by Shamma and Megretskii,
suggest that it is necessary as well. Recall that in general, µ is computed using bounds, but that
Q involves solving LMIs, so is attractive computationally. L1 is also easy to compute, involving
only the evaluation of L1 norms and ﬁnding the spectral radius of a positive matrix .
As a ﬁnal comparison, it can be easily shown that the tests are ordered, with
The interpretation of (13.4) for a given system is that if the Q test passes, the µ test must pass,
and similarly for L1 and Q. It was shown above that µ ≤Q. The inequality Q ≤L1 follows from
the equivalence of the SSUB and the FDCD problems, the fact that the L1 norm of a convolution
kernal is greater than the H∞norm of it’s transform, and the results in . The inequalities are typically strict and it is possible for the gaps to be arbitrarily large.
It is not clear exactly what the implications of these results are for control design or for further
research. Clearly there is a need for more reﬁned results, and the ability to both combine LTI
and NTV uncertainty and exploit additional structure such as the slowly-varying nature of some
perturbations.
The results in , and suggest how this might be done in the LFT/µ/Q framework, but much more work
is needed. We also need more precise modeling and ID methods to exploit the detailed structure
of the uncertainty in our models.
If one accepts Q as the measure of robust performance, a rich theory can be developed, with
generalizations to uncertain systems of the conventional theories of robust stability and performance, balanced realizations and model reduction , stabilization , and model validation . It is not surprising that
the easiest generalizations of standard results to uncertain LFT systems is done using the Q
framework. Indeed, most of the standard results rely on Q machinery, but since µ and Q are
the same for these simple block structures, we are less aware of the distinction. Once we begin
extending our results to systems with uncertainty, the distinction becomes signiﬁcant. Of course,
a key feature of the Q theory is that computation involves solving LMIs.
µ with real perturbations
In recent years a great deal of interest has arisen with regard to robustness problems involving
parametric uncertainty. These problems involve uncertain parameters that are not only norm
bounded, but also constrained to be real. Robustness problems involving parametric uncertainty
can be reformulated as µ problems where the block structured uncertainty description is now
allowed to contain both real and complex blocks. This mixed µ problem can have fundamentally
diﬀerent properties from the complex µ problem studied in this paper (where the block structured uncertainty description contains only complex blocks), and these properties have important
implications for computation. In this section we give a brief review of some recent results in this
It is now well known that real µ problems can be discontinuous in the problem data ). As well as adding computational diﬃculties to the problem this sheds
serious doubt on the usefulness of real µ as a robustness measure in such cases, since the system
model is always a mathematical abstraction from the real world, and is computed to ﬁnite precision. However it is shown in that mixed µ problems containing some
complex uncertainty are, under some mild assumptions, continuous in the problem data (whereas
purely real µ problems are not). This is reassuring from an engineering viewpoint since one is
usually interested in robust performance problems (which therefore contain at least one complex
block), or robust stability problems with some unmodeled dynamics, which are naturally covered
with complex uncertainty. Thus in problems of engineering interest, the potential discontinuity
of mixed µ should not arise.
Recent results in (Rohn and Poljak) show that a special case of computing µ with real perturbations only is NP complete. While these results do not apply to the complex only case, it is
certainly true that the general mixed problem is NP hard as well. These results strongly suggest
that it is futile to pursue exact methods for computing µ in the purely real or mixed case for
even moderate (less than 100) numbers of real perturbations, unless one is prepared not only to
solve the real µ problem but also to make fundamental contributions to the theory of computational complexity. Furthermore, it may be that even approximate methods must have worst-case
combinatoric complexity (Demmel).
These results do not mean, however, that “practical” algorithms are not possible, where “practical” means avoiding combinatoric (nonpolynomial) growth in computation with the number of
parameters for all of the problems which arise in engineering applications. Practical algorithms
for other NP hard problems exist and typically involve approximation, heuristics, branch-andbound, or local search. Results presented in strongly suggest that
an intelligent combination of all these techniques can yield a practical algorithm for the mixed
Upper and lower bounds for mixed µ have recently been developed, and they take the form of
generalizations of the bounds for the complex µ problem presented here (i.e. by applying the
mixed µ bounds to complex µ problems one recovers the standard complex µ bounds). The
upper bound was presented in and involves minimizing the eigenvalues
of a Hermitian matrix. This can also be recast as a singular value minimization which involves
additional scaling parameters to the complex µ upper bound. It is shown in that the mixed µ problem can be recast as a real eigenvalue maximization and that this
in turn can be tackled via a power algorithm, giving a lower bound for mixed µ. A practical
computation scheme for these bounds has recently been developed 
and will be available shortly in a test version in conjunction with the µ-Tools toolbox .
The quality of these bounds, and their computational requirements as a function of problem size,
are explored in . While the bounds are usually accurate enough for
engineering purposes, in a signiﬁcant number of cases of interest, they are not. This is in contrast
with the purely complex nonrepeated case, where no examples of problems with large gaps have
been found. The use of Branch and Bound schemes to improve upon existing bounds has been
suggested by several authors and and references therein). There are some important issues
and tradeoﬀs to be considered in implementing such a scheme, which can greatly impact the
performance. A selection of results from a fairly extensive numerical study of these issues is
presented in , and a Branch and Bound scheme is proposed which
should form the basis of a practical computation scheme for mixed µ.
This will be further
explored in (Newlin, Young and Doyle).
The upper and lower bounds from complex µ theory not only serve as computational schemes,
but are theoretically rich as well. Connections between the bounds and various aspects of linear
system theory have been established, and further work in this area appears to have great promise.
A theoretical study of the mixed µ bounds may yield new insight as well, and this is a subject of
current research. Initial results in this area are presented in (Young and Doyle), where it is seen
that mixed µ inherits many of the (appropriately generalized) properties of complex µ, although
as has already been seen, in some aspects the mixed µ problem can be fundamentally diﬀerent
from the complex µ problem.
Problems involving robustness properties of polynomials with coeﬃcients perturbed by real pa-
rameters have received a great deal of attention in the literature. This type of robustness problem
leads to a (real or) mixed µ problem. Several celebrated “Kharitonov-type” results have been
proven for special cases of this problem, such as the “aﬃne parameter variation” problem for example), and the solutions typically involve checking the edges
or vertices of some polytope in the parameter space. It can be shown that restricting the allowed
perturbation dependence to be aﬃne leads to a real µ problem on a transfer matrix which is rank
The rank one mixed µ problem is studied in detail by Chen, Fan, et al, 1991, (see also the
references therein). The authors develop an analytic expression for the solution to this problem,
which is not only easy to compute, but has sublinear growth in the problem size. They are then
able to solve several problems from the literature, noting that these problems can be treated as
special cases of “rank one µ problems” and are thus “relatively easy to solve”. Even the need to
check (a combinatoric number of) edges is shown to be unnecessary. While many of these results
were apparently well-known, provides a direct comparison between the
polynomial and µ-based approaches.
This rank one case is also studied by Young and Doyle, where it is shown that for such problems
µ equals its upper bound and is hence equivalent to a convex problem. This reinforces the results
of and oﬀers some insight into why the problem becomes so much more
diﬃcult when we move away from the “aﬃne parameter variation” case to the “multilinear” or
“polynomial” cases . These correspond to µ problems which
are not necessarily rank one, and hence may no longer be equal to the upper bound and so may
no longer be equivalent to a convex problem. These results also underline why there are no
practical algorithms based on “edge-type” theorems, as the results appear to be relevant only to
a very special problem. Furthermore, even in the very special “aﬃne parameter case” there are
a combinatoric number of edges to check.
Generalizations of µ
In this section we review an alternative formulation of µ due to Fan and Tits and use
it to consider one of several possible generalizations of µ. The most important motivation for
this generalization comes from the model validation problem , and
 for background).
For simplicity, the Fan-Tits formulation is considered here for the full block only case (S = 0).
For any vector or matrix A with n rows let Ai denote the rows of A corresponding to the ith
block of ∆. Thus Ai has mi rows. Also, let P := In be the identity matrix. An alternative
expression for µ is
x { α : α∥xi∥≤∥Mix∥∀i ∈{1, ..., F}}
To see that this is equivalent to (3.3) in Deﬁnition 3.1, note that when det(I −M∆) = 0 there is
an x ̸= 0 that satisﬁes (I −∆M)x = 0. This x achieves the maximum in (13.5). Conversely, any
x that achieves the maximum provides a way to constructing a a ∆: set ∆i equal to the dyad
that satisﬁes xi = ∆iMix.
An LMI formulation of the upper bound follows easily from (13.5). Again, we consider the full
block only case.
∃x ̸= 0 : α2∥xi∥2 ≤∥Mix∥2 ∀i
∃x ̸= 0 : x∗(M ∗
i Mi −α2P ∗
∃x ̸= 0 : x∗(M ∗DM −α2D)x ≥0 ∀D ∈D
i Pi : di > 0 ∀i
It follows that
α > µ ⇐= ∃D ∈D : M ∗DM −α2D < 0
This is the same as the LMI in equation (3.11).
The generalization of µ that we will study depends on a block structure as before along with an
index that speciﬁes certain blocks as special or distinguished . As an
example, consider equation 13.5 in the case of two full blocks:
x { α : α∥x1∥≤∥M1x∥& α∥x2∥≤∥M2x∥}
Suppose the second block has been designated as special. Then the generalization is
x { α : α∥x1∥≤∥M1x∥& α−1∥x2∥≥∥M2x∥}
In this example, the designation of the second block as special means that the direction of the
second inequality is reversed and the scaling changed.
The lower bound for this generalization of µ, though notationally awkward, is very similar to the
standard lower bound, and a power algorithm is being investigated. There is no upper bound
similar to the ¯σ
upper bound, but there is a generalization of the LMI above. Again
consider our two block example.
1 M1 −α2P ∗
2 M2 −α−2P ∗
∃x ̸= 0 : x∗(M ∗DM −P 2(α)D)x ≥0 ∀D ∈D
D := ( d1P ∗
1 P1 + d2P ∗
2 P2 : d1 > 0; d2 < 0 )
P(α) = α2P ∗
1 P1 + α−2P ∗
It follows that
α > µ ⇐= ∃D ∈D : M ∗DM −P(α)2D < 0
We see that D is just as in the case of standard LMI upper bound for µ except that we require
for some blocks that Di < 0 rather than Di > 0. It is expected that algorithms for computing
positive deﬁnite solutions to LMIs will be easily generalized to solve this problem.
Acknowledgements
The authors gratefully acknowledge helpful discussions with Blaise Morton, Sigurd Skogestad,
Peter Young, Michael Fan, and Kemin Zhou and excellent comments on this paper from Carolyn
Beck and Jorge Tierno. Thanks also to Matt Newlin, Sharon Laubach, Brian Clendenin, and
Sonja Glavaski for useful suggestions. Financial support for this work came from the National Science Foundation (grants ECS-8909499, CTS-9057420, and ECS-8657814), AFOSR, ONR, NASA,
Honeywell, Rockwell International, and the Berkeley Engineering Fund.