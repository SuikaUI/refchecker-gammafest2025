Deep Metric Learning via Lifted Structured Feature Embedding
Hyun Oh Song
Stanford University
 
Stanford University
 
Stefanie Jegelka
 
Silvio Savarese
Stanford University
 
Learning the distance metric between pairs of examples
is of great importance for learning and visual recognition.
With the remarkable success from the state of the art convolutional neural networks, recent works have shown
promising results on discriminatively training the networks
to learn semantic feature embeddings where similar examples are mapped close to each other and dissimilar examples are mapped farther apart. In this paper, we describe an
algorithm for taking full advantage of the training batches
in the neural network training by lifting the vector of pairwise distances within the batch to the matrix of pairwise
distances. This step enables the algorithm to learn the state
of the art feature embedding by optimizing a novel structured prediction objective on the lifted problem. Additionally, we collected Online Products dataset: 120k images
of 23k classes of online products for metric learning. Our
experiments on the CUB-200-2011 , CARS196 ,
and Online Products datasets demonstrate signiﬁcant improvement over existing deep feature embedding methods
on all experimented embedding sizes with the GoogLeNet
 network.
1. Introduction
Comparing and measuring similarities between pairs of
examples is a core requirement for learning and visual competence. Being able to ﬁrst measure how similar a given pair
of examples are makes the following learning problems a
lot simpler. Given such a similarity function, classiﬁcation
tasks could be simply reduced to the nearest neighbor problem with the given similarity measure, and clustering tasks
would be made easier given the similarity matrix. In this
regard, metric learning and dimensionality reduction techniques aim at learning semantic
distance measures and embeddings such that similar input
objects are mapped to nearby points on a manifold and dissimilar objects are mapped apart from each other.
Furthermore, the problem of extreme classiﬁcation with enormous number of categories has recently at-
Figure 1: Example retrieval results on our Online Products
dataset using the proposed embedding. The images in the
ﬁrst column are the query images.
tracted a lot of attention in the learning community. In this
setting, two major problems arise which renders conventional classiﬁcation approaches practically obsolete. First,
algorithms with the learning and inference complexity linear in the number of classes become impractical.
Second, the availability of training data per class becomes
very scarce. In contrast to conventional classiﬁcation approaches, metric learning becomes a very appealing technique in this regime because of its ability to learn the general concept of distance metrics (as opposed to category
speciﬁc concepts) and its compatibility with efﬁcient nearest neighbor inference on the learned metric space.
With the remarkable success from the state of the art convolutional neural networks , recent works 
discriminatively train neural network to directly learn the
the non-linear mapping function from the input image to a
lower dimensional embedding given the input label annotations. In high level, these embeddings are optimized to pull
examples with different class labels apart from each other
and push examples from the same classes closer to each
other. One of the main advantages of these discriminatively
trained network models is that the network jointly learns the
feature representation and semantically meaningful embed-
 
dings which are robust against intra-class variations.
However, the existing approaches cannot take
full advantage of the training batches used during the mini
batch stochastic gradient descent training of the networks
 . The existing approaches ﬁrst take randomly sampled pairs or triplets to construct the training batches and
compute the loss on the individual pairs or triplets within
the batch. Our proposed method lifts the vector of pairwise
distances (O(m)) within the batch to the matrix of pairwise
distances (O(m2)). Then we design a novel structured loss
objective on the lifted problem. Our experiments show that
the proposed method of learning the embedding with the
structured loss objective on the lifted problem signiﬁcantly
outperforms existing methods on all the experimented embedding dimensions with the GoogLeNet network.
We evaluate our methods on the CUB200-2011 ,
CARS196 , and Online Products dataset we collected.
The Online Products has approximately 120k images and
23k classes of product photos from online e-commerce
websites. To the best of our knowledge, the dataset is one of
the largest publicly available dataset in terms of the number
and the variety of classes. We plan to maintain and grow
the dataset for the research community.
In similar spirit of general metric learning where the task
is to learn a generic concept of similarity/distance, we construct our train and test split such that there is no intersection
between the set of classes used for training versus testing.
We show that the clustering quality (in terms of standard
F1 and NMI metrics ) and retrieval quality (in terms
of standard Recall@K score) on images from previously
unseen classes are signiﬁcantly better when using the proposed embedding. Figure 1 shows some example retrieval
results with the Online Products dataset using the proposed
embedding. Although we experiment on clustering and retrieval tasks, the conceptual contribution of this paper - lifting a batch of examples into a dense pairwise matrix and
deﬁning a structured learning problem - is generally applicable to a variety of learning and recognition tasks where
feature embedding is employed.
2. Related works
Our work is related to three lines of active research: (1)
Deep metric learning for recognition, (2) Deep feature embedding with convolutional neural networks, and (3) Zero
shot learning and ranking.
Deep metric learning: Bromley et al. paved the way
on deep metric learning and trained Siamese networks for
signature veriﬁcation. Chopra et al. trained the network
discriminatively for face veriﬁcation. Chechik et al. 
learn ranking function using triplet loss. Qian et al.
 uses precomputed activation features and learns a
feature embedding via distance metric for classiﬁcation.
Deep feature embedding with state of the art convolutional neural networks: Bell et al. learn embedding
for visual search in interior design using contrastive 
embedding, FaceNet uses triplet embedding to
learn embedding on faces for face veriﬁcation and recognition. Li et al. learn a joint embedding shared by
both 3D shapes and 2D images of objects. In contrast to the
existing approaches above, our method computes a novel
structured loss and the gradient on the lifted dense pairwise
distance matrix to take full advantage of batches in SGD.
Zero shot learning and ranking:
Frome et al., Socher
et al., and Weston et al. leverage text data to
train visual ranking models and to constrain the visual predictions for zero shot learning. Wang et al. learns to
rank input triplet of data given human rater’s rank ratings
on each triplets and also released a triplet ranking dataset
with 5,033 triplet examples . However, the approach is
not scalable with the size of the training data because it’s
very costly to obtain ranking annotations in contrast to multiclass labels (i.e., product name) and because the approach
is limited to ranking the data in triplet form. Lampert et
al. does zero shot learning but with attributes (such as
objects’s color or shape) provided for both the train and the
test data. On a related note, do zero-shot learning for visual recognition but rely on the WordNet hierarchy
for semantic information of the labels.
The paper is organized as follows. In section 3, we start with
a brief review of recent state of the art deep learning based
embedding methods . In section 4, we describe how
we lift the problem and deﬁne a novel structured loss. In
section 5 and 6, we describe the implementation details and
the evaluation metrics. We present the experimental results
and visualizations in section 7.
In this section, we brieﬂy review recent works on discriminatively training neural networks to learn semantic
embeddings.
Contrastive embedding is trained on the paired data
{(xi, xj, yij)}. Intuitively, the contrastive training minimizes the distance between a pair of examples with the same
class label and penalizes the negative pair distances for being smaller than the margin parameter α. Concretely, the
cost function is deﬁned as,
i,j + (1 −yi,j) [α −Di,j]2
where m stands for the number of images in the batch, f(·)
is the feature embedding output from the network, Di,j =
||f(xi) −f(xj)||2, and the label yi,j ∈{0, 1} indicates
whether a pair (xi, xj) is from the same class or not. The
[·]+ operation indicates the hinge function max(0, ·). We
direct the interested readers to refer for the details.
Triplet embedding is trained on the triplet data
have the same class
labels and
have different class labels. The x(i)
term is referred to as an anchor of a triplet. Intuitively, the
training process encourages the network to ﬁnd an embedding where the distance between x(i)
a and x(i)
n is larger than
the distance between x(i)
a and x(i)
plus the margin parameter α. The cost function is deﬁned as,
where Dia,ip = ||f(xa
i )|| and Dia,in = ||f(xa
i )||. Please refer to for the complete details.
(a) Contrastive embedding
(b) Triplet embedding
(c) Lifted structured embedding
Figure 2: Illustration for a training batch with six examples.
Red edges and blue edges represent similar and dissimilar
examples respectively. In contrast, our method explicitly
takes into account all pair wise edges within the batch.
4. Deep metric learning via lifted structured
feature embedding
We deﬁne a structured loss function based on all positive
and negative pairs of samples in the training set:
max (0, Ji,j)2 ,
Ji,j = max
α −Di,k, max
Contrastive loss
(a) Training network with contrastive embedding 
Triplet loss
(b) Training network with triplet embedding 
Lifted struct loss
(c) Training network with lifted structure embedding
Figure 3: Illustration for training networks with different embeddings. m denotes the number of images in the
batch. The green round box indicates one example within
the batch. The network (a) takes as input binary labels, network (b) does not take any label input because the ordering
of anchor, positive, negative encodes the label. The proposed network (c) takes as input the multiclass labels.
where bP is the set of positive pairs and b
N is the set of
negative pairs in the training set. This function poses two
computational challenges: (1) it is non-smooth, and (2) both
evaluating it and computing the subgradient requires mining
all pairs of examples several times.
We address these challenges in two ways: First, we
optimize a smooth upper bound on the function instead.
Second, as is common for large data sets, we use a stochastic approach. However, while previous work implements
a stochastic gradient descent by drawing pairs or triplets
of points uniformly at random , our approach
deviates from those methods in two ways: (1) it biases
the sample towards including “difﬁcult” pairs, just like a
subgradient of Ji,j would use the close negative pairs 1; (2)
it makes use of the full information of the mini-batch that
1Strictly speaking, this would be a subgradient replacing the nested
max by a plus.
is sampled at a time, and not only the individual pairs.
Figures 2a and 2b illustrate a sample batch of size
m = 6 for the contrastive and triplet embedding. Red edges
in the illustration represent positive pairs (same class) and
the blue edges represent negative pairs (different class) in
the batch. In this illustration, it is important to note that
adding extra vertices to the graph is a lot more costly than
adding extra edges because adding vertices to the graph
incurs extra I/O time and/or storage overhead.
To make full use of the batch, one key idea is to
enhance the mini-batch optimization to use all O(m2)
pairs in the batch, instead of O(m) separate pairs. Figure
2c illustrates the concept of of transforming a training
batch of examples to a fully connected dense matrix
of pairwise distances.
Given a batch of c-dimensional
embedded features X
Rm×c and the column vector
individual
||f(x1)||2
2, . . . , ||f(xm)||2
⊺, the dense pairwise
squared distance matrix can be efﬁciently constructed
by computing, D2
˜x1⊺+ 1˜x⊺−2XX⊺, where
ij = ||f(xi) −f(xj)||2
However, it is important to
note that the negative edges induced between randomly
sampled pairs carry limited information. Most likely, they
are different from the much sharper, close (“difﬁcult”)
neighbors that a full subgradient method would focus on.
Hence, we change our batch to be not completely random, but integrate elements of importance sampling. We
sample a few positive pairs at random, and then actively
add their difﬁcult neighbors to the training mini-batch. This
augmentation adds relevant information that a subgradient
would use. Figure 4 illustrates the mining process for one
positive pair in the batch, where for each image in a positive pair we ﬁnd its close (hard) negative images. Note that
our method allows mining the hard negatives from both the
left and right image of a pair in contrast to the rigid triplet
structure where the negative is deﬁned only with respect to the predeﬁned anchor point. Indeed, the procedure
of mining hard negative edges is equivalent to computing
the loss augmented inference in structured prediction setting . Our loss augmented inference can be efﬁciently processed by ﬁrst precomputing the pairwise batch
squared distance matrix D2. Figure 3 presents the comprehensive visual comparison of different training structures
(i.e. batch, label, network layout) with different loss functions. In contrast to other approaches (Fig. 3a and 3b), our
method greatly simpliﬁes the network structure (Fig. 3c)
and requires only one branch of the CNN.
Furthermore, mining the single hardest negative with
nested max functions (eqn. 4) in practice causes the network to converge to a bad local optimum. Hence we optix1
Figure 4: Hard negative edge is mined with respect to each
left and right example per each positive pairs. In this illustration with 6 examples in the batch, both x3 and x4 independently compares against all other negative edges and
mines the hardest negative edge.
mize the following smooth upper bound ˜J(D(f(x))). Concretely, our loss function per each batch is deﬁned as,
˜Ji,j = log
exp{α −Di,k} +
exp{α −Dj,l}
input : D, α
J/∂f(xi), ∀i ∈[1, m]
Initialize:
J/∂f(xi) = 0, ∀i ∈[1, m]
for i = 1, . . . , m do
for j = i + 1, . . . , m, s.t. (i, j) ∈P do
J/∂f(xi) ←∂˜
J/∂f(xi) + ∂˜
J/∂Di,j∂Di,j/∂f(xi)
J/∂f(xj) ←∂˜
J/∂f(xj) + ∂˜
J/∂Di,j∂Di,j/∂f(xj)
for k = 1, . . . , m, s.t. (i, k) ∈N do
J/∂f(xi) ←∂˜
J/∂f(xi)+∂˜
J/∂Di,k∂Di,k/∂f(xi)
J/∂f(xk) ←∂˜
J/∂f(xk)+∂˜
J/∂Di,k∂Di,k/∂f(xk)
for l = 1, . . . , m, s.t. (j, l) ∈N do
J/∂f(xj) ←∂˜
J/∂f(xj)+∂˜
J/∂Dj,l∂Dj,l/∂f(xj)
J/∂f(xl) ←∂˜
J/∂f(xl)+∂˜
J/∂Dj,l∂Dj,l/∂f(xl)
Algorithm 1: Backpropagation gradient
where P denotes the set of positive pairs in the batch and N
denotes the set of negative pairs in the batch. The back propagation gradients for the input feature embeddings can be
(a) Contrastive embedding
(b) Triplet embedding
(c) Lifted structured similarity
Figure 5: Illustration of failure modes of contrastive and triplet loss with randomly sampled training batch. Brown circles,
green squares, and purple diamonds represent three different classes. Dotted gray arcs indicate the margin bound (where the
loss becomes zero out of the bound) in the hinge loss. Magenta arrows denote the negative gradient direction for the positives.
derived as shown in algorithm 1, where the gradients with
respect to the distances are,
˜Ji,j 1[ ˜Ji,j > 0]
˜Ji,j 1[ ˜Ji,j > 0] −exp{α −Di,k}
exp{ ˜Ji,j −Di,j}
˜Ji,j 1[ ˜Ji,j > 0] −exp{α −Dj,l}
exp{ ˜Ji,j −Di,j}
where 1[·] is the indicator function which outputs 1 if the
expression evaluates to true and outputs 0 otherwise. As
shown in algorithm 1 and equations 5, 6, and 7, our method
provides informative gradient signals for all negative pairs
as long as they are within the margin of any positive pairs
(in contrast to only updating the hardest negative) which
makes the optimization much more stable.
Having stated the formal objective, we now illustrate and
discuss some of the failure modes of the contrastive 
and triplet embedding in which the proposed embedding learns successfully. Figure 5 illustrates the failure cases in 2D with examples from three different classes.
Contrastive embedding (Fig. 5a) can fail if the randomly
sampled negative (xj) is collinear with the examples from
another class (purple examples in the ﬁgure). Triplet embedding (Fig. 5b) can also fail if such sampled negative
(xn) is within the margin bound with respect to the sampled the positive example (xp) and the anchor (xa).
this case, both contrastive and triplet embedding incorrectly
pushes the positive (xi/xa) towards the cluster of examples
from the third class. However, in the proposed embedding
(Fig. 5c), given sufﬁciently large random samples m, the
hard negative examples (xk’s in Fig. 5c) within the margin
bound pushes the positive xi towards the correct direction.
5. Implementation details
We used the Caffe package for training and testing the embedding with contrastive , triplet ,
and ours. Maximum training iteration was set to 20, 000
for all the experiments. The margin parameter α was set to
1.0. The batch size was set to 128 for contrastive and our
method and to 120 for triplet. For training, all the convolutional layers were initialized from the network pretrained
on ImageNet ILSVRC dataset and the fully connected
layer (the last layer) was initialized with random weights.
We also multiplied the learning rate for the randomly initialized fully connected layers by 10.0 for faster convergence. All the train and test images are normalized to 256
by 256. For training data augmentation, all images are randomly cropped at 227 by 227 and randomly mirrored horizontally. For training, we exhaustively use all the positive
pairs of examples and randomly subsample approximately
equal number of negative pairs of examples as positives.
6. Evaluation
In this section, we brieﬂy introduce the evaluation metrics used in the experiments. For the clustering task, we use
the F1 and NMI metrics. F1 metric computes the harmonic
mean of precision and recall. F1 =
P +R. The normalized mutual information (NMI) metric take as input a set
of clusters Ω= {ω1, . . . , ωK} and a set of ground truth
classes C = {c1, . . . , cK}. ωi indicates the set of examples
with cluster assignment i. cj indicates the set of examples
with the ground truth class label j. Normalized mutual information is deﬁned by the ratio of mutual information and
the average entropy of clusters and the entropy of labels.
Embedding size
F1 score (%)
GoogLeNet pool5
Contrastive
LiftedStruct
Embedding size
NMI score (%)
GoogLeNet pool5
Contrastive
LiftedStruct
Recall@K score (%)
GoogLeNet pool5
Contrastive
LiftedStruct
Figure 6: F1, NMI, and Recall@K score metrics on the test split of CUB200-2011 with GoogLeNet .
Embedding size
F1 score (%)
GoogLeNet pool5
Contrastive
LiftedStruct
Embedding size
NMI score (%)
GoogLeNet pool5
Contrastive
LiftedStruct
Recall@K score (%)
GoogLeNet pool5
Contrastive
LiftedStruct
Figure 7: F1, NMI, and Recall@K score metrics on the test split of CARS196 with GoogLeNet .
NMI (Ω, C) =
2(H(Ω)+H(C)). We direct interested readers
to refer for complete details. For the retrieval task, we
use the Recall@K metric. Each test image (query) ﬁrst
retrieves K nearest neighbors from the test set and receives
score 1 if an image of the same class is retrieved among the
K nearest neighbors and 0 otherwise. Recall@K averages
this score over all the images.
7. Experiments
We show experiments on CUB200-2011 , CARS196
 , and our Online Products datasets where we use the
ﬁrst half of classes for training and the rest half classes
for testing.
For testing, we ﬁrst compute the embedding on all the test images at varying embedding sizes
{64, 128, 256, 512} following the practice in . For
clustering evaluation, we run afﬁnity propagation clustering with bisection method for the desired number
of clusters set equal to the number of classes in the test set.
The clustering quality is measured in the standard F1 and
NMI metrics. For the retrieval evaluation, we report the result on the standard Recall@K metric in log space of
K. The experiments are performed with GoogLeNet .
7.1. CUB-200-2011
The CUB-200-2011 dataset has 200 classes of birds
with 11,788 images. We split the ﬁrst 100 classes for training (5,864 images) and the rest of the classes for testing
(5,924 images). Figure 6 shows the quantitative clustering
quality for the contrastive , triplet , and using
pool5 activation from the pretrained GoogLeNet network on ImageNet , and our method on both F1, NMI,
and Recall@K metrics. Our embedding shows signiﬁcant
performance margin both on the standard F1, NMI, and Recall@K metrics on all the embedding sizes. Figure 8 shows
some example query and nearest neighbors on the test split
of CUB200-2011 dataset. Figure 9 shows the Barnes-
Hut t-SNE visualization on our 64 dimensional embedding. Although t-SNE embedding does not directly translate to the high dimensional embedding, it is clear that similar types of birds are quite clustered together and are apart
from other species.
7.2. CARS196
The CARS196 data set has 198 classes of cars with
16,185 images. We split the ﬁrst 98 classes for training
(8,054 images) and the other 98 classes for testing (8,131
images). Figure 7 shows the quantitative clustering quality for the contrastive , triplet , and using pool5
Figure 9: Barnes-Hut t-SNE visualization of our embedding on the test split (class 101 to 200; 5,924 images) of CUB-
200-2011. Best viewed on a monitor when zoomed in.
Figure 11: Barnes-Hut t-SNE visualization of our embedding on the test split (class 99 to 196; 8,131 images) of
CARS196. Best viewed on a monitor when zoomed in.
Embedding size
F1 score (%)
GoogLeNet pool5
Contrastive
LiftedStruct
Embedding size
NMI score (%)
GoogLeNet pool5
Contrastive
LiftedStruct
Recall@K score (%)
GoogLeNet pool5
Contrastive
LiftedStruct
Figure 12: F1, NMI, and Recall@K score metrics on the test split of Online Products with GoogLeNet .
Figure 8: Examples of successful queries on the CUB200-
2011 dataset using our embedding. Images in the ﬁrst
column are query images and the rest are ﬁve nearest neighbors. Best viewed on a monitor zoomed in.
activation from pretrained GoogLeNet network on ImageNet . Our embedding shows signiﬁcant margin in
terms of the standard F1, NMI, and Recall@K metrics on
all the embedding sizes. Figure 10 shows some example
query and nearest neighbors on the test split of Cars196 
dataset. Figure 11 shows the Barnes-Hut t-SNE visualization on our 64 dimensional embedding. We can observe that the embedding clusters the images from the same
brand of cars despite the signiﬁcant pose variations and the
changes in the body paint.
Figure 10: Examples of successful queries on the Cars196
 dataset using our embedding. Images in the ﬁrst column are query images and the rest are ﬁve nearest neighbors. Best viewed on a monitor zoomed in.
7.3. Online Products dataset
We used the web crawling API from eBay.com to
download images and ﬁltered duplicate and irrelevant images (i.e. photos of contact phone numbers, logos, etc).
The preprocessed dataset has 120,053 images of 22,634 online products (classes) from eBay.com. Each product has
approximately 5.3 images. For the experiments, we split
59,551 images of 11,318 classes for training and 60,502
images of 11,316 classes for testing. Figure 12 shows the
quantitative clustering and retrieval results on F1, NMI, and
Recall@K metric with GoogLeNet.
Figures 13 and 14
show some example queries and nearest neighbors on the
Figure 15: Barnes-Hut t-SNE visualization of our embedding on the test split (class 11,319 to 22,634; 60,502 images)
of Online Products.
Figure 13: Examples of successful queries on our Online
Products dataset using our embedding (size 512). Images
in the ﬁrst column are query images and the rest are ﬁve
nearest neighbors. Best viewed on a monitor zoomed in.
dataset for both successful and failure cases. Despite the
huge changes in the viewpoint, conﬁguration, and illumination, our method can successfully retrieve examples from
the same class and most retrieval failures come from ﬁne
grained subtle differences among similar products. Figure
15 shows the t-SNE visualization of the learned embedding
on our Online Products dataset.
8. Conclusion
We described a deep feature embedding and metric
learning algorithm which deﬁnes a novel structured prediction objective on the lifted dense pairwise distance matrix
within the batch during the neural network training. The experimental results on CUB-200-2011 , CARS196 ,
and our Online Products datasets show state of the art per-
Figure 14: Examples of failure queries on Online Products
dataset. Most failures are ﬁne grained subtle differences
among similar products.
Images in the ﬁrst column are
query images and the rest are ﬁve nearest neighbors. Best
viewed on a monitor zoomed in.
formance on all the experimented embedding dimensions.