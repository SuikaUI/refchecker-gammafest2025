This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
Active Deep Learning for Classiﬁcation of
Hyperspectral Images
Peng Liu, Hui Zhang, and Kie B. Eom
Abstract—Active deep learning classiﬁcation of hyperspectral
images is considered in this paper. Deep learning has achieved
success in many applications, but good-quality labeled samples are
needed to construct a deep learning network. It is expensive getting
good labeled samples in hyperspectral images for remote sensing
applications. An active learning algorithm based on a weighted
incremental dictionary learning is proposed for such applications.
The proposed algorithm selects training samples that maximize
two selection criteria, namely representative and uncertainty. This
algorithm trains a deep network efﬁciently by actively selecting
training samples at each iteration. The proposed algorithm is applied for the classiﬁcation of hyperspectral images, and compared
with other classiﬁcation algorithms employing active learning. It
is shown that the proposed algorithm is efﬁcient and effective in
classifying hyperspectral images.
Index Terms—Active learning, deep learning, remote sensing
classiﬁcation, sparse representation.
I. INTRODUCTION
ECENTLY, a semisupervised learning method called deep
learning has been introduced for remote sensing data
classiﬁcation – . It can be considered as an extension of
an artiﬁcial neural network, and is effective in classiﬁcation of
complex problems. However, training a deep network is quite
expensive and requires a large number of training samples. The
application of a deep network to hyperspectral image classiﬁcation is not practical, because only a limited number of training
samples are available and the dimension of the feature space is
Active learning is an iterative procedure of selecting the most
informative examples from a subset of unlabeled samples. This
choice is based on a ranking of scores that are computed from a
model outcome. The chosen candidates are added to the training
set, and the classiﬁer is trained with the new training samples.
The training done with actively selected samples is more efﬁcient than the one done with randomly selected samples because
it uses samples that are more suitable for training. Therefore, the
active learning method can train a deep network faster and with
Manuscript received February 20, 2016; revised April 27, 2016, July 12,
2016, and July 31, 2016; accepted August 2, 2013. This work was supported in
part by NSFC under Grant 41471368 and Grant 41571413 and in part by RADI
Director Youth foundation. (Corresponding author: Kie B. Eom.)
P. Liu is with the Institute of Remote Sensing and Digital Earth, Chinese
Academy of Sciences, Beijing 100094, China (e-mail: ).
H. Zhang is with the Laboratory of Brain and Cognition, National Institute
of Mental Health, Bethesda, MD 20892 USA (e-mail: ).
K. B. Eom is with the Department of Electrical and Computer Engineering,
George Washington University, Washington, DC 20052 USA 
divergence similarity , Gaussian similarity , and cluster . There are also some mixed methods that employ
criteria in selecting new training samples for active learning.
For example, the density-weighting method employs both
uncertainty and the distribution of the unlabeled samples. The
methods using multiple metrics have the potential to achieve
higher efﬁciency in active learning. Both uncertainty and distribution are utilized in selecting new training samples in the
active learning algorithm proposed in this paper.
Active learning methods have been widely studied for remote
sensing applications. Most of the research on active learning is
combined with a special classiﬁer or a special remote sensing
application. Examples include a kernel-based method, an active learning method combined with a support vector machine
(SVM) , logistic regression (LR) , and Gaussian process
regression . A survey for active learning in remote sensing
before 2011 can be found in .
Although active learning has been applied to many applications in remote sensing, most of these approaches are closely
connected with a speciﬁc type or a speciﬁc structure of the
classiﬁer. Examples are random sampling (RS), maximum uncertainty sampling (MUS) and query-by-committee (QBC)
 . RS is essentially a deep belief network (DBN) without
ﬁne tuning done by active learning. MUS and QBC are not applicable to DBN classiﬁers because both unsupervised feature
learning and supervised ﬁne tuning are employed in training of
DBN. RS samples candidate data randomly, and the classiﬁcation accuracy is usually low. However, it is fast, simple, and
convenient. MUS queries the most uncertain instance by an active learner, and the entropy is used as an uncertainty measure.
MUS has been applied to an SVM and LR. QBC trains committee members on the current labeled set. Different members of
the committee represent different hypotheses of the classiﬁcation problem. QBC selects candidate samples showing maximal
disagreement between different members of the committee.
In this paper, we propose an active learning scheme where the
information from both unsupervised and supervised stages is
utilized. The proposed active learning is applied to a deep learning structure, and its efﬁcacy in classifying remotely sensed
1939-1404 © 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See standards/publications/rights/index.html for more information.
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
hyperspectral images is demonstrated in experiments. The paper
is organized as follows: In Section II, the basic idea of the DBN
is discussed, and it is shown that both unsupervised and supervised stages could provide useful information. In Section III,
criteria for active learning is discussed, and two criteria, uncertainty and representativeness are proposed as criteria for active
learning. In Section IV, an object function is constructed by
combining representativeness and uncertainty of the samples,
and the optimization algorithm for solving the new object
function is presented. Experimental results are presented in
Section V.
Let S = {1, . . . , n, . . . , N} be a set of integers indexing N
pixels of a hyperspectral image, and let L = {1, . . . , c, . . . , C}
be a set of integers indexing C class labels. The image
X = {x1, . . . , xn, . . . xN } is a set composed of N feature vectors, where xn = {x1
n, . . . , xM
n } is corresponding to the nth
M-dimensional pixel. The label Y = {y1, . . . , yn, . . . yN } is
represented as a set composed of N label vectors corresponding to N pixels, where yn = {y1
n, . . . , yC
n } is C-dimensional
label vector. The element yc
n in a label vector yn represents the
possibility that the pixel xn belongs to the class c.
The labeling process can be considered as a mapping process
from the image X to the label Y , and it can be solved by a DBN.
The DBN architecture used in this research is shown in Fig. 1. It
is a fully interconnected belief network with one input layer h0,
L −1 hidden layers h1, . . . , hL−1, and an output layer hL. The
input layer h0 has M units corresponding to feature vector xn.
The output layer hL has C units corresponding to label vector
The DBN architecture transforms high-dimensional data into
low-dimensional code using an adaptive and multilayered encoder network. One popular method for constructing a DBN
deep architecture is the greedy layer-wise restricted Boltzmann
machine (RBM). The RBM is a particular form of a log-linear
Markov random ﬁeld. Consider the layer l, it takes the output from the previous layer hl−1 as input (visible variable) and
generates the output (hidden variable) for the next layer hl. For
notational convenience, visible and hidden variables are denoted
as v and h, respectively.
There are no direct connections between hidden units
in an RBM. The network assigns the following probability
p(v, h) to every possible visible-hidden vector pair with the
aforementioned energy function.
p(v, h) = 1
Z e−E (v,h)
where the normalization term Z is obtained by summing over
all possible pairs of visible and hidden vectors
e−E (v,h).
The probability p(v) that the model assigns to a visible vector
v is obtained by marginalizing over the space of hidden vectors
e−E (v,h).
In RBMs, visible and hidden units are conditionally independent of each other. Therefore, conditional probabilities p(h|v)
and p(v|h) can be written as
where the conditional activation probabilities are deﬁned as
follows :
p(hi = 1|v) = f(Wiv + bi)
p(vj = 1|h) = f(W
where W is the weight matrix, b and c are the offset vectors,
and f(·) is the sigmoid function.
Considering the encoding and decoding of each layer, parameters W, b, and c are related by the energy function E(v, h) of
the RBM, and it is deﬁned as
E(v, h) = −b′v −c′h −h′Wv.
Furthermore, based on a series of derivations , , the
log-likelihood gradient for the parameters of an RBM is obtained
p(h|v)∂E(v, h)
p(v, h)∂E(v, h)
where θ represents parameter W, b, or c.
In (9), the ﬁrst term denotes an expectation with respect to the
data distribution and the second tern denotes an expectation with
respect to the distribution deﬁned by the model. Because there
are no direct connections between hidden units in an RBM, it is
easy to get an unbiased sample of the ﬁrst term. However, getting
an unbiased sample of the second term is much more difﬁcult. It
can be done by starting at any random state of the visible unit and
performing alternating Gibbs sampling for a very long time. A
much faster learning procedure called constrastive divergence
(CD) method was proposed in . The CD performs Gibbs
sampling, and uses a gradient descent procedure to update the
increments of the parameters.
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
LIU et al.: ACTIVE DEEP LEARNING FOR CLASSIFICATION OF HYPERSPECTRAL IMAGES
Proposed active learning algorithm.
Once RBMs are stacked and trained in a greedy manner,
they form a DBN illustrated in Fig. 1. DBNs can be viewed
as graphical models that learn to extract a deep hierarchical
representation of the training data. A complete training of a
DBN includes two stages: the unsupervised features learning
stage and the supervised ﬁne tuning stage. In the unsupervised
features learning stage, RBMs learn one layer at a time by the
CD method. This is an efﬁcient greedy learning scheme. In the
supervised ﬁne tuning stage, all the initial RBM are stacked, and
discriminative ﬁne tuning is performed by a back propagation
(BP) algorithm.
In many applications, deep learning shows better performance
than classical methods such as an SVM. However, deep learning
needs to initialize more parameters than an SVM. A complete
training of the DBN with randomly selected training samples
requires a large number of training samples, despite the efﬁciency of the CD algorithm. Therefore, it is not practical to
apply DBN to problems with limited size of training samples or
with large feature spaces. How to select a training dataset becomes more important in training a deep network. To improve
the training efﬁciency of the DBN, the algorithm proposed in
this paper considers two stages in DBN training, unsupervised
feature learning and supervised ﬁne tuning. In the unsupervised
learning stage, a DBN provides the condition for the estimation
of the representativeness of data, while in the supervised learning stage, it provides the condition for the uncertainty estimation
of data. In the proposed active learning method, the two metrics, representativeness and uncertainty, are integrated into an
object function. The weighted incremental dictionary learning
(WI-DL) algorithm illustrated in Fig. 2 is proposed to optimize
the object function with two metrics. After the optimization, the
samples are ranked and informative samples are selected. Experiments on hyperspectral images conﬁrmed the effectiveness
of the proposed algorithm.
In the following section, criteria for selecting training samples
are discussed. It is also discussed how a deep network is trained
by an active learning scheme.
III. CRITERIA FOR ACTIVE LEARNING
In active learning, the training samples need to be selected
by their importance. Criteria for selecting important samples
for training in active learning have been considered in earlier
research. An example of this research is a density-weighting
method with the information density framework described by
Settles and Craven . It simultaneously considers underlying structure information of data and explicit class labels of the
samples. It is also proposed that the most informative instances
should not only be uncertain, but should also be representative
of the underlying distribution . Therefore, the most informative sample x∗is selected by maximizing the information
density .
where x is a candidate sample, and xn is an arbitrary sample
in an unlabeled dataset X. The function S(x, xn) measures the
similarity between the sample x and xn, and the function Φ(x)
measures the uncertainty of the sample. The success of the information density approach depends on ﬁnding a good uncertainty
function Φ(x) and a good similarity function S(x, xn). The
information density framework has advantages in classiﬁcation
because it utilizes more information than other traditional active
learning methods.
Inspired by the information density method , a similarity
function S(x, xn) and an uncertainty function Φ(x) for a deep
learning architectures are proposed in this paper. It is also important to combine the two criteria (similarity and uncertainty)
together for searching informative samples for training. A new
algorithm called the WI-DL is developed in the next section.
This algorithm ranks the samples by their importance under the
two criteria (similarity and uncertainty). The WI-DL algorithm
selects informative samples that maximize the aforementioned
two criteria as the new training data.
A. Sparsity for Representativeness Estimation
A cosine similarity function was used as a similarity function in , and many other similarity measures, such as KL
divergence similarity , Gaussian similarity , and local
manifold similarity , have been used in active learning. In active learning, a good similarity measure is important in ﬁnding
representative samples. In other words, a good representative
model is needed for an efﬁcient representation of the distribution and structure.
Recently, the sparse representation method, which represents
a signal by a set of basis, has become popular in the machine
learning ﬁeld. A set of basis is also called a dictionary. Unlike decompositions using a predeﬁned analytic basis (such as
wavelet) and its variants, a signal can be represented using an
overcomplete dictionary without analytic form. The basic assumption behind the dictionary learning approach is that the
structure of complex incoherent characteristics can be extracted
directly from the data rather than by using a mathematical description.
A set of atoms in a dictionary can characterize the entire
dataset, as samples searched for active learning are representative. Furthermore, if a subset of samples is used as a dictionary
with high efﬁciency, they are representative samples for an active learning problem.
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
In the deep architecture in Fig. 1, the output at the lth layer
1, . . . , hl
n, . . . , hl
N } can be considered as the projection of the N input feature vectors X = {x1, . . . , xn, . . . xN }.
Since the unsupervised coding stage of deep learning could
be viewed as feature learning and dimension reduction, it is
reasonable to select training samples based on the information of the feature data projected by the DBN. Therefore,
the atoms can be selected from the last (Lth) hidden layer
output HL = {hL
1 , . . . , hL
n , . . . , hL
N }. Further, the output data
n ∈RC can be described as hL
n = Dα, where D ∈RC ×p is
a dictionary with p atoms . Each atom in D is normalized
to a unit vector, and α ∈Rp is the coefﬁcient vector for sparse
representation.
The dictionary is assumed redundant (p > C). The number
of nonzero coefﬁcients in the representation is denoted as k =
∥α∥0, where k is expected to be very small. It implies that the
feature vector hL
n can be viewed as a linear combination of
a few columns from the dictionary D ∈RC ×p, which is also
referred to as the set of atoms. For the convenience of notation,
the dictionary learning problem can be stated without index L.
D,α ∥hn −Dα∥2
subject to
where ∥· ∥2 is L2 norm and ∥· ∥0 is L0 norm.
Let H = {h1, . . . , hN } be the dataset with N feature vectors, and let A = {α1, . . . , αN }, be the set of corresponding
coefﬁcients, where A ∈Rp×N . Then, the dictionary learning
problem can be written as
D,α1 ,...,αN
∥hi −Dαi∥2
where λ is a regularization parameter.
Traditional methods for solving (12), such as K-singular value
decomposition (K-SVD) , nonparametric Bayesian dictionary learning, etc., are not directly applicable to active learning
with a deep network because uncertainty also needs be considered in addition to the representativeness.
B. Information Entropy for Uncertainty Estimation
Uncertainty sampling is a commonly used query framework for active learning. In this framework, an active learner
queries the instances that are least certain. The entropy is
often used as an uncertainty measure.
p(yj|x)log(p(yj|x))
where p(yj|x) is the probability that the sample x belongs to
the jth class. The information entropy Φ(x) of the sample x is
based on the prediction of the current classiﬁer.
In a deep network, an input x is projected to a hidden
layer h. The hidden layer h will obtain its label vector y =
{y1, . . . , yj, . . . , yC } after the classiﬁcation prediction. The entropy Φ(h) of the hidden layer h is deﬁned as
p(yj|h)log(p(yj|h))
where p(yj|h) is the probability that the sample x is mapped
to the h and belongs to the jth class. The uncertainty function
(14) is not directly related to the structures of the classiﬁer,
and is easy to implement. In the next section, the uncertainty
measure is combined with sparse representation to develop an
active learning algorithm.
IV. ACTIVE LEARNING WITH SPARSE REPRESENTATION
AND UNCERTAINTY
In the last section, we deﬁned the representativeness measurement based on sparse representation and the uncertainty
measurement based on entropy. In this section, we construct a
new discriminate function that is used to search the most informative samples and to employ both representativeness and
uncertainty of the samples.
For an active learning problem, when we select the most informative samples from the unlabeled dataset X = {x1, . . . , xN },
the calculation is not directly performed in X but in the corresponding projected data H = {h1, . . . , hN } at the output of
the deep network. This is because H will represent the features
of input data more concisely and efﬁciently after the nonlinear
dimension reduction. Therefore, the samples to be labeled are
in X, but the searching process is done in the set H. There
is a one-to-one relationship between X and H. Once we ﬁnd
appropriate feature vectors in H, the corresponding samples in
X will be labeled and put into the training dataset.
The initial training data are projected by the deep network
trained with the unsupervised learning stage that was explained
in Section II, and the projected (output of the deep network)
data are used as the initial dictionary D (Usually atoms in D are
normalized to unit vector). For active learning, the dictionary
D = {d1, . . . , dn} at the current iteration is appended with
the new set of dictionary atoms E = {dn+1, . . . , dn+m} ⊂H,
and the new dictionary for the next iteration is obtained. The
selection of atoms is done similarly to a batch operation shown
in (12), but the optimization is done incrementally. The object
function for the incremental learning J is given by
J(E, β1, . . . , βN ) =
hi −[DE]
where ∥· ∥2 is L2 norm, ∥· ∥0 is L0 norm, D = [d1, d2, . . . , dn]
is the dictionary from the previous iteration, and E =
[dn+1, dn+2, . . . , dn+m] is the new set of atoms to be appended
to D to form a new dictionary. The ﬁrst term in (15) is the
residual error after adding new training samples. The coefﬁcient vectors αi and βi are for the data hi, and are associated
with dictionaries D and E, respectively.
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
LIU et al.: ACTIVE DEEP LEARNING FOR CLASSIFICATION OF HYPERSPECTRAL IMAGES
Equation (15) can be rewritten as
J(E, β1, . . . , βN ) =
∥ri −Eβi∥2
where ri is the residual after the sparse coding of the data hi,
with the dictionary D from the previous iteration, is done.
ri = hi −Dαi.
As discussed before, to combine the representativeness and
uncertainty, we need to integrate the information of (17) and
(15) to form a new object function
J(E, β1, . . . , βN ) =
∥ri −Eβi∥2
Γ = diag. [Φ(dn+1), . . . , Φ(dn+m)]
and Φ(di) is the entropy of the atom di (as well as h) deﬁned
in (14). The more uncertain the atom di is, the larger Φ(di) is.
This means that, if one feature vector is mainly composed of
very uncertain atoms, it will increase punishments to the object
function. Furthermore, the large coefﬁcients will exacerbate the
inﬂuence of uncertainties from the atoms.
In (19), the problem of minimizing the object function is a
joint optimization problem with respect to the set E of new
dictionary atoms and the set of coefﬁcients B = {β1, . . . , βN }.
The cost function J in (19) is not jointly convex, but is convex
with respect to each of the two sets (E and B) when the other
one is ﬁxed. There has been extensive research that focuses
on how to ﬁnd good dictionary atoms and how to represent
the dataset sparsely. Usually, there are two stages in dictionary
leaning algorithms: the sparse coding stage, which is searching
the optimal solution for B, and the dictionary updating stage,
which is to ﬁnd the solution for E.
For the sparse coding stage, there are variety of atom selection
schemes in many greedy-based algorithms such as orthogonal
matching pursuit (OMP) , compressive sampling matched
pursuit , and StageOMP . However, none of the traditional sparse coding methods consider the uncertainty of atoms,
because classiﬁcation problems based on active learning were
not considered in earlier research. While atoms for the new dictionary are selected from the unlabeled dataset H, all elements
in H = {h1, . . . , hN } are potential candidates. It is difﬁcult
to traverse all the elements in a large dataset repeatedly when
searching for atoms. To narrow it down, the entire dataset H
was sorted ﬁrst by their uncertainty (from the most uncertain to
the least uncertain). Let {hς1 , . . . , hςN } be the samples of the
set H sorted by the uncertainty value Φ(hi), where hς1 is the
most uncertain sample. Assuming that m samples are updated
at each iteration of active learning, the top m uncertain samples
{hς1 , . . . , hςm } from the sorted list are selected as the initial
estimates of new atoms for the new dictionary.
E = {dn+1, . . . , dn+m} = {hς1 , . . . , hςm }.
In the sparse coding stage of (19), E is assumed to be known.
In this paper, for the active learning problem, a weighted OMP
is used for the sparse coding stage of (19). For a general OMP,
each ri, 1 ≤i ≤N, is coded by selecting atoms one by one
that are the most similar to ri. This means that an arbitrary ri is
projected to each candidate atom dη, n + 1 ≤η ≤n + m, and
the best match is selected by using
η=n+1,...,n+m
|ri · dη|.
However, the uncertainty Φ(dη) should be considered for minimizing (19), and the selection of the candidate atom is done
by ﬁnding the match that gives maximum weighted projection
η=n+1,...,n+m
{Φ(dη)|ri · dη|}.
An algorithm that maximizes the aforementioned equation
(called weighted OMP) was developed, and is described in detail
in Algorithm 2.
For the dictionary updating stage, the atom updating algorithm will be different from general dictionary learning methods
because the atoms are the samples selected directly from the candidate dataset. The initial set of atoms E = {dn+1, . . . , dn+m}
contains the m most uncertain samples, but they are not good
enough to be used for the new dictionary and should be updated
one by one. Using the idea of information density in (10), both
representativeness and uncertainty are considered. For the convenience of notation, the symbol d (unit vector) is used without
an index. Representativeness is measured by the square of the
inner product between the atom d and the vector ri, 1 ≤i ≤N,
as they are similar if this measure is large.
(d · ri)2 = (dT ri)2.
Therefore, an atom d is very representative if it is similar to all
vectors in the set R = {r1 . . . rN }. Therefore
However, the importance for each term (dT ri) is different because of the difference in uncertainty. As the uncertainty of the
sample increases, it contributes more to the sum. As a result, the
uncertainty measure Φ(ri) is introduced into the object function
Φ(ri)(dT ri)2.
By changing the form of N
i=1Φ(ri)(dT ri)2, we get
Φ(ri)(dT ri)2 =
dT riΦ(ri)rT
As addressed before, atom d is a unit vector, and dT d =
1. Introducing the Lagrange multiplier ξ, the object function
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
Algorithm 1 The Weighted OMP
Input: The target dataset R = {r1, . . . , rN }, the set of
atoms to be appended to the dictionary
E = {dn+1, . . . , dn+m} and the sparsity level of the
Output: A coefﬁcient matrix B = {β1, . . . , βN }.
for i := 1 to N do
2) Take ri from R, and let ˆr0 = ri.
3) Set index set Λ = ∅and temporal dictionary ˆD = ∅.
for j := 1 to k do
4) Find an index ηj that solves the optimization
η=n+1,...,n+m
Φ(dη)|ˆrj−1 · dη|.
5) Augment the index set and the matrix of chosen
Λ = Λ ∪{ηj}
ˆD = ˆD ∪{dηj }.
6) Solve a least squares problem to obtain a newly
estimated sparse coefﬁcients ˆβj
ˆβj = arg min
∥ˆrj−1 −ˆDβ∥2
7) Calculate the new approximation of the current data
and the new residual
bj = ˆD ˆβj
ˆrj = ˆrj−1 −bj
8) βi = ˆβj, and B = B ∪βi
J1(d) = dT
d −ξ(dT d −1).
By setting the partial derivative of J1 with respect to d to zero,
d −ξd = 0.
This can be rewritten in the following form.
The atom d is the eigenvector of the symmetric matrix (N
i=1 riΦ(ri)rT
i ), and ξ will be the eigenvalue of
i=1 riΦ(ri)rT
i ). To solve this problem, a singular value decomposition (SVD) can be applied.
Incremental
Dictionary
Input: The current residual set R = {r1, . . . , rN }.
Candidate dataset Ω = {h1, . . . , hr}. The number of
dictionary samples updated m.
Output: The new set atoms to be appended to the
dictionary E = {dn+1, . . . , dn+m}, and the new residual
1) Calculate uncertainty functions {Φ(h1), . . . , Φ(hr)} on
the dataset Ω by (14), and then, sort them from the most
uncertain to the least uncertain samples.
Θ = sort(Φ(h1), . . . , Φ(hr)).
2) From the set Θ, select the top m samples as candidates
for the initial dictionary E = {dn+1, . . . , dn+m}.
3) With R and E, the parameter set B is obtained by
calling Algorithm 1.
for a := 1 to m do
4) For atom dn+a ∈E construct ˆRt+1 ˆW ˆRT
t+1 by (32)
5) Perform SVD and get [U, Λ, V ] = svd( ˆRt+1 ˆW ˆRT
6) Take out the ﬁrst column of U = {ˆd1, . . . , ˆdB } and
ﬁnd h∗in Ω by (35).
7) dn+a = h∗, update dictionary E by dn+a, and update
B by (36).
8) delete h∗from Ω.
9) Compute the residual set for the next iteration:
R′ = R −EB
Three-channel color composite image and ground truth of PaviaC data.
(a) Image, (b) Ground truth.
However, it is unnecessary that all vectors in the set R =
{r1, . . . , rN } need to be involved in updating one atom d.
Only a few coefﬁcients in B are nonzero after performing the
weighted OMP of R, since it is sparse coding. As a result, if we
want to update an arbitrary atom dn+a (1 ≤a ≤m) by searching a new d, only samples with nonzero coefﬁcients should
be considered in the search. Therefore, before updating dn+a,
a subset {rζ1 , . . . , rζf } ⊂R of data with nonzero coefﬁcients
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
LIU et al.: ACTIVE DEEP LEARNING FOR CLASSIFICATION OF HYPERSPECTRAL IMAGES
PAVIAC DATASETS
(from B) for atom dn+a is selected. For every rζf , it is encoded by {dn+1, . . . , dn+a, dn+a+1, . . . , dn+m} considering
the inﬂuence without dn+a. Then, the remaining residual ˆrζf is
ˆrζf = rζf −
dn+iβdn + i −
dn+iβdn + i
where dn+i is the atom, and βdn + i is corresponding coefﬁcients
from B by the most recent weighted OMP . Therefore, we deﬁne
ˆR = {ˆrζ1 , . . . ,ˆrζf }, and
ˆW = diag.[Φ(ˆrζ1 ), . . . , Φ(ˆrζf )].
The term (N
i=1 riΦ(ri)rT
i ) in (30) can be rewritten as ˆR ˆW ˆRT .
Similarly, we have ξd = ( ˆR ˆW ˆRT )d. Performing SVD decomposition leads to
[U, Λ, V ] = svd( ˆR ˆW ˆRT )
where U = {ˆd1, . . . , ˆdB } are eigenvectors for matrix ˆR ˆW ˆRT .
Let ˆd1 be the eigenvector corresponding to the largest eigenvalue. A sample from H is selected so that the projection to ˆd1
weighted with uncertainty is the largest. This selection process
can be written as
Now, h∗is used as the new atom of dn+a and h∗is put back
into E in the position of dn+a. At the same time, B also needs
to be updated. The coefﬁcients corresponding to dn+a in B are
calculated by
νn+a = (dT
n+adn+a)−1dT
Then, νn+a is put into its corresponding position in B in place
of the old value. Atoms in E = {dn+1, . . . , dn+m} will be
updated one by one. Another problem is that, in (21), the initial
value of E is selected form H. Actually, if H is too large, a
subset Ω ⊂H can be randomly selected from H and used as a
candidate dataset for the potential atoms in active learning. The
complete algorithm for searching dn+1, . . . , dn+m is given in
Algorithm 2.
Classiﬁcation results of different active learning methods. The proposed WI-DL result shows better results compared with the ground truth in
Fig. 3(b). The accuracies for WI-DL, MUS, RS, and QBC are 97.2%, 94.5%,
94.0%, and 95.5%, respectively. (a) WI-DL, (b) MUS, (c) RS, (d) QBC.
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
Classiﬁcation accuracy of different methods on different datasets.
(a) Dataset 1. (b) Dataset 2. (c) Dataset 3. (d) Cross validation of 12 runs.
When compared with the traditional dictionary learning algorithm, the proposed WI-DL has its own distinctive characteristics. First, it has a different object function from the traditional
one because of the introduced weight parameters Φ(h). Furthermore, the uncertainty metric is considered in samples (as
well as atoms) sorting and selecting procedures. Second, the
traditional dictionary learning is applied to all signal or vector data, while WI-DL is only applied to the current residual
data, because active learning is an incremental learning problem. Overall, WI-DL considered both the representativeness and
the uncertainty while selecting samples to be atoms.
The active learning algorithm presented previously has been
applied to improve the DBN classiﬁer. The size of the training set
COMPUTATION TIME (IN SECONDS) FOR PAVIAC DATASETS
Three-channel color composite image and ground truth of PaviaU data.
(a) Image and (b) Ground truth.
is small, and the speedy unsupervised learning only brings the
DBN to an initial conﬁguration. The complete training of a DBN
requires a large training set, and it is computationally expensive.
The additional dictionary atoms selected at each iteration of active learning trains the DBN classiﬁer much more efﬁciently
than randomly selected training samples as the additional dictionary atoms are the m best (in terms of representativeness and
uncertainty) samples for training. In the experiment, m atoms
are added to the dictionary at each iteration. After each iteration
of active learning, the DBN classiﬁer is trained with the training set that is updated with additional m atoms. This process
is repeated until the DBN classiﬁer is completely trained, and
details are summarized in the experimental results.
V. EXPERIMENTS AND RESULTS
To validate the proposed method, three hyperspectral datasets,
PaviaC, PaviaU, and Botswana are used in the experiment. The
proposed algorithm, WI-DL, is compared on the test datasets
with three other algorithms, namely RS, MUS , and QBC
The DBNs used in this paper have four hidden layers. Computational efﬁciency is considered in selection of number of
layers. The initial weights for DBNs are randomly selected between 0 and 1. Each layer of DBNs is based on an RBM. Once
RBMs are stacked and trained in a greedy manner, they form
a DBN architecture illustrated in Fig. 1. In the unsupervised features learning stage, RBMs learn one layer at a time by the CD
method in . In the supervised ﬁne-tuning stage, a BP 
algorithm is applied.
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
LIU et al.: ACTIVE DEEP LEARNING FOR CLASSIFICATION OF HYPERSPECTRAL IMAGES
PAVIAU DATASETS
MentalSheets
A. Experiment 1
The ﬁrst experiment is done with the Pavia Center (PaviaC)
dataset. It was acquired by a Reﬂective Optics System Imaging
Spectrometer (ROSIS) sensor, and has been widely used in earlier research. The number of bands in the original dataset is 115,
and spatial resolution is 1.3 m. It covers a spectral range from
0.43 to 0.86 μm with 115 hyperspectral bands. From the original
PaviaC dataset, 102 bands are selected by removing low signalto-noise ratio (SNR) bands. Test images are segmented from the
dataset without low SNR bands, and the size of each test image
is 1096 × 715 pixels. Fig. 3(a) shows a test image in false color,
and Fig. 3(b) shows the ground truth with the detailed view at
the lower left corner. It shows ten classes in different colors, and
names of class labels are shown on the right side of the ﬁgure.
Nine classes of interest (Water, Trees, Meadows, Bricks, Soil,
Asphalt, Bitumen, Tiles, and Shadows) have been selected for
the labeled dataset. Four algorithms (WI-DL, RS, MUS, and
QBC) are applied to three sets of data constructed from the
ground-truth data. Each set contains three classes of randomly
selected data, training, candidates, and testing data, of different
percentages. Table I shows the number of samples for each class
of the dataset. The class name and the class number are given in
the ﬁrst and second columns, while the third column shows the
total number of samples, and the rest of columns show numbers
of samples in training, candidates, and test sets.
A DBN having four hidden layers, 102 input nodes corresponding to 102 hyperspectral bands, and nine output nodes
corresponding to nine classes of interest, is created. The training
data are used to conﬁgure the parameters of the DBN classiﬁer
for the preparation of active learning. Then, the active learning
algorithm presented in Section IV is applied to ﬁne-tune the
DBN classiﬁer by actively selecting atoms from the candidate
set. The training data used for the initial conﬁguration of DBN
is used as the initial dictionary, and 50 samples are actively selected from the candidates set at each iteration of active learning.
The newly selected dictionary samples are labeled and added to
the existing dictionary, and the DBN classiﬁer is ﬁne tuned with
the updated dictionary. A total of 20 iterations are performed,
Maps of classiﬁcations of different active learning methods. The accuracies for WI-DL, MUS, RS, and QBC are 92.4%, 78.3%, 72.2%, and 88.5%,
respectively. (a) WI-DL, (b) MUS, (c) RS, (d) QBC.
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
Classiﬁcation accuracy of different methods on different datasets.
(a) Dataset 1. (b) Dataset 2. (c) Dataset 3. (d) Cross validation of 12 runs.
and additional 1000 atoms are added to the dictionary in the
active learning stage.
The test dataset was used to test the performance of the ﬁnetuned DBN classiﬁer. Fig. 4(a) shows the classiﬁcation result
obtained by the proposed WI-DL algorithm, and the boxed area
at the center is enlarged in the lower left corner to show classiﬁcation details. In comparison with the image in Fig. 3(b),
it can be seen that the classiﬁcation result matches reasonably
well with the ground truth. Three other active-learning classi-
ﬁcation algorithms, namely RS, MUS, and QBC methods, are
applied to the same test dataset to compare classiﬁcation performances. The classiﬁcation results are shown in Fig. 4. The
COMPUTATION TIME (IN SECONDS) FOR PAVIAU DATASETS
Gray image and ground truth of Botswana data. (a) Image and
(b) Grand truth.
WI-DL result in Fig. 4(a) shows that more samples are correctly
classiﬁed than compared to the results of other algorithms. It
can be observed that the WI-DL algorithm performs better than
other approaches, especially in the enlarged area in the lower
left corner.
To demonstrate the effectiveness of active learning, the classiﬁcation accuracy is measured with varying amount of training
samples. The result of experiments are shown in Fig. 5. It can be
observed that the performance of WI-DL is better than those of
other algorithms, and the classiﬁcation accuracy of WI-DL improves faster than other algorithms as more samples are added.
The performance of other three algorithms are ranked in the
order of MUS, RS, and QBC in the experiments.
Experiments are performed on a Windows 10 computer
with a 64-bit CPU intel(R) Core(TM) i5-4570s running at
2.90 GHz, and algorithms are implemented with MATLAB
R2013a. Elapsed CPU times for classiﬁcation of three datasets
with four different algorithms are measured and summarized in
Table II. It can be observed that RS is the fastest as random
selection requires no computation. MUS is slower than RS but
close as it only needs to compute entropy for each candidate
sample. The proposed WI-DL and QBC are relatively slow. The
complexity of WI-DL is mainly due to sparse coding, and the
complexity of QBC is mainly due to training of different committee members. WI-DL is usually faster than QBC because a
greedy-based algorithm for sparse coding is used.
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
LIU et al.: ACTIVE DEEP LEARNING FOR CLASSIFICATION OF HYPERSPECTRAL IMAGES
BOTSWANA DATASETS
Hippo grass
Floodplain grasses1
Floodplain grasses2
Island interior
Acacia woodlands
Acacia shrublands
Acacia grasslands
B. Experiment 2
The data used in this experiment are the airborne data from
the ROSIS optical sensor, and was collected under the HySens
project sponsored by the European Union. The images were
acquired over the area of the University of Pavia, in northern
Italy, on July 8, 2002. The number of bands of the ROSIS sensor
is 115 with a spectral coverage ranging from 0.43 to 0.86 μm,
and 103 hyperspectral channels are used for classiﬁcation after
the removal of 12 noisy bands. The spatial resolution is 1.3 m
per pixel. The data have been atmospherically corrected, and
the original image is shown in false color in Fig. 6(a). Fig. 6(b)
shows the ground truth with a detailed view in the lower left
corner. It shows ten classes in different colors, and the names of
the class labels are shown on the right side.
Nine classes of interest (Asphalt, Meadows, Gravel, Trees,
Metal sheets, Bare soil, Bitumen, Bricks and Shadows) have
been selected for the labeled dataset. Four algorithms (WI-DL,
RS, MUS, and QBC) are applied to three sets of data constructed
from the ground truth data. Each set contains three classes of
randomly selected data, training, candidates, and testing data,
of different percentages. Table III shows the number of samples
for each class of the dataset.
Fig. 7 shows results of classiﬁcation done by WI-DL, RS,
MUS, and QBC methods. Details of the boxed area in the middle
are enlarged in the lower left corner of Fig. 7(a)–(d). The result of
the proposed WI-DL algorithm shown in Fig. 7(a) matches reasonably well with the ground truth shown in Fig. 6(b). Also, the
classiﬁcation result of the WI-DL algorithm in Fig. 7(a) is better
than results obtained by the other algorithms in Fig. 7(b)–(d),
as seen in the enlarged area.
Fig. 8 shows changes of classiﬁcation accuracies as number
of training samples increases. It can be observed that the performance of WI-DL is better than those of other algorithms,
and the classiﬁcation accuracy of WI-DL improves faster than
other algorithms as more samples are added. CPU times for
classiﬁcation of three datasets with four different algorithms
are measured and summarized in Table IV. The hardware and
software environments are as same as in Experiments 1.
C. Experiment 3
The dataset used in this experiment is acquired by NASA EO-
1 satellite over the Okavango Delta, in Botswana in 2001. The
Hyperion sensor on EO-1 acquires data at 30-m pixel resolution
over a 7.7-km strip in 242 bands covering a spectrum ranging
400–2500 nm. Preprocessing was performed by the University
of Texas—Center for Space Research. Uncalibrated and noisy
bands that cover water absorption features were removed, and
145 bands remained in the dataset. The data analyzed in this
study, acquired on May 31, 2001, consist of observations from
14 identiﬁed classes. Fig. 9(a) shows a test image in false color,
and Fig. 9(b) shows the ground truth with a detailed view in the
lower left corner. It shows 12 classes in different colors, and the
names of class labels are shown on the right side.
Ten classes of interest have been selected for the labeled
dataset as summarized in Table V. Four algorithms (WI-DL, RS,
MUS, and QBC) are applied to three sets of data constructed
from the ground-truth data. Each set contains three classes of
randomly selected data, training, candidates, and testing data, of
different percentages. Table V shows the number of samples for
each class of the dataset. The class name and the class number
are given in the ﬁrst and second columns, while the third column
shows the total number of samples, and the rest of columns show
numbers of samples in training, candidates, and test sets.
Fig. 10 shows results of classiﬁcation done by WI-DL, RS,
MUS, and QBC methods. Details of the boxed area in the middle
is zoomed at the lower left corner of Fig. 10(a)–(d). The result
of the proposed WI-DL algorithm shown in Fig. 10(a) matches
reasonably well with the ground truth shown in Fig. 9(b). Also,
the classiﬁcation result of the WI-DL algorithm in Fig. 10(a)
is better than the results obtained by other algorithms in
Fig. 10(b)–(d), as seen in the enlarged area.
Fig. 11 shows changes of classiﬁcation accuracies as number
of training samples increases. It can be observed that the performance of WI-DL is better than those of other algorithms, and the
classiﬁcation accuracy of WI-DL improves faster than other algorithms as more samples are added. The performance of other
three algorithms are ranked in the order of QBC, MUS, and RS
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING
Maps of classiﬁcations of different active learning methods. The
accuracies for WI-DL, MUS, RS, and QBC are 91.6%, 83.4%, 76.9%, and
88.6%, respectively. (a) WI-DL, (b) MUS, (c) RS and (d) QBC.
in the experiments. Elapsed CPU times for classiﬁcation of three
datasets with four different algorithms are measured and summarized in Table VI. The hardware and software environments
are as same as in Experiments 1.
Classiﬁcation accuracy of different methods on different datasets.
(a) Dataset 1. (b) Dataset 2. (c) Dataset 3. (d) Cross validation of 12 runs.
COMPUTATION TIME (IN SECONDS) FOR BOTSWANA DATASETS
VI. CONCLUSION
In this paper, we proposed a classiﬁcation algorithm based on
active learning of deep networks. For active learning, additional
samples to the training set are selected using the representativeness and uncertainty of the potential samples. This is achieved
by integrating two criteria into a new object function. A new
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
LIU et al.: ACTIVE DEEP LEARNING FOR CLASSIFICATION OF HYPERSPECTRAL IMAGES
active learning algorithm, the WI-DL algorithm, which is suitable for searching atoms is developed by minimizing the new
object function that has two criteria. The performance of the
WI-DL algorithm is compared with three other methods, RS,
MUS, and QBC. The proposed WI-DL performed well in the
classiﬁcation experiment with remotely sensed hyperspectral
images. It is demonstrated that the proposed algorithm achieves
higher accuracy with fewer training samples by actively selecting training samples.
ACKNOWLEDGMENT
The authors would like to express sincere gratitude to Dr. Q.
Du, associate editor, and three anonymous reviewers for detailed
review and constructive comments.