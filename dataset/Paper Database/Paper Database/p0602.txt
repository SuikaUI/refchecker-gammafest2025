Adversarially Learned One-Class Classiﬁer for Novelty Detection
Mohammad Sabokrou1, Mohammad Khalooei2, Mahmood Fathy1, Ehsan Adeli3
1Institute for Research in Fundamental Sciences
2Amirkabir University of Technology
3Stanford University
Novelty detection is the process of identifying the observation(s) that differ in some respect from the training
observations (the target class). In reality, the novelty class
is often absent during training, poorly sampled or not well
deﬁned. Therefore, one-class classiﬁers can efﬁciently model
such problems. However, due to the unavailability of data
from the novelty class, training an end-to-end deep network
is a cumbersome task. In this paper, inspired by the success
of generative adversarial networks for training deep models
in unsupervised and semi-supervised settings, we propose an
end-to-end architecture for one-class classiﬁcation. Our architecture is composed of two deep networks, each of which
trained by competing with each other while collaborating to
understand the underlying concept in the target class, and
then classify the testing samples. One network works as the
novelty detector, while the other supports it by enhancing
the inlier samples and distorting the outliers. The intuition
is that the separability of the enhanced inliers and distorted
outliers is much better than deciding on the original samples. The proposed framework applies to different related
applications of anomaly and outlier detection in images
and videos. The results on MNIST and Caltech-256 image
datasets, along with the challenging UCSD Ped2 dataset
for video anomaly detection illustrate that our proposed
method learns the target class effectively and is superior to
the baseline and state-of-the-art methods.
1. Introduction
Novelty detection is the process of identifying the new or
unexplained set of data to determine if they are within the
norm (i.e., inliers) or outside of it (i.e., outliers). Novelty
refers to the unusual, new observations that do not occur regularly or is simply different from the others. Such problems
are especially of great interest in computer vision studies,
as they are closely related to outlier detection , image denoising , anomaly detection in images 
and videos . Novelty detection can be portrayed in the
context of one-class classiﬁcation , which aims
Noisy Inlier Samples
Outlier Samples
Figure 1. Example outputs of the proposed model, trained to detect
Penguins (a), in response to inlier and outlier samples (b). The
ﬁrst row of (b) shows some example images, and the second row
contains the output of the R network on them, i.e., R(X). As can
be seen, R enhanced the inlier samples (even in the presence of
noise) but distorted the outliers. Two last rows show the score of
D applied to X and R(X), respectively. R(X) is indeed more
separable than only using the original input image, X.
to build classiﬁcation models when the negative class is
absent, poorly sampled or not well deﬁned. As such, the
negative class can be considered as the novelty (i.e., outlier or anomaly), while the positive (or target) class is well
characterized by instances in the training data.
To accurately chart the intrinsic geometry of the positive
class, the ﬁrst step is to efﬁciently represent the data in a
way that can entangle more or less the different explanatory factors of variation in the data. Recently, deep learning
approaches have gained immense success in representing
visual data for various vision-based applications ,
especially in cases that they are trained in an end-to-end
 
fashion. However, for novelty detection or one-class classi-
ﬁcation applications, due to unavailability of data from the
negative class, training an end-to-end deep network is not
straightforward. Some efforts have been made, in recent
years, to beneﬁt from deep features in learning one-class
classiﬁers [46, 38, 33, 20, ?, ?], few of which could train an
end-to-end feature learning and classiﬁcation model.
Inspired by the recent developments in generative adversarial networks (GANs) , we propose an end-to-end
model for one-class classiﬁcation and apply it to different
applications including outlier detection, novelty detection
in images and anomaly event detection in videos. The proposed architecture, similar to GANs, comprises two modules,
which compete to learn while collaborating with each other
for the detection task. The ﬁrst module (denoted as R) re-
ﬁnes the input and gradually injects discriminative material
into the learning process to make the positive and novelty
samples (i.e., inliers, and outliers) more separable for the
detector, the second module (referred to as D).
These two networks are adversarially and unsupervisedly
learned using the training data, which is composed of only
the target class. Speciﬁcally, R learns to reconstruct the positive samples and tries to fool the detector (i.e., D). Whereas,
D learns to distinguish original (positive) samples from the
reconstructed ones. In this way, D learns merely the concept characterized by the space of all positive samples, and
hence it can be used for distinguishing between positive and
novelty classes. On the other hand, R learns to efﬁciently
reconstruct the positive samples, while for negative (or novelty) samples it is unable to reconstruct the input accurately,
and hence, for negative samples it acts as a decimator (or informally a distorter). In the testing phase, D operates as the
actual novelty detector, while R improves the performance
of the detector by adequately reconstructing the positive or
target samples and decimating (or distorting) any given negative or novelty samples. Fig. 1 depicts example inputs and
outputs of both R and D networks for a model trained to
detect images of Penguins.
In summary, the main contributions of this paper are as
follows: (1) We propose an end-to-end deep network for
learning one-class classiﬁer learning. To the best of our
knowledge, this article is one of the ﬁrsts to introduce an
end-to-end network for one-class classiﬁcation. (2) Almost
all approaches based on GANs in the literature discard
either the generator or the discriminator (analogous to R and
D, respectively, in our architecture) after training. Only one
of the trained models is used, while our setting is more efﬁcient and beneﬁts from both trained modules to collaborate
in the testing stage. (3) Our architecture learns the model
in the complete absence of any training samples from the
novelty class and achieves state-of-the-art performance in
different applications, such as outlier detection in images
and anomaly event detection in videos.
2. Related Works
One-class classiﬁcation is closely related to rare event
detection, outlier detection/removal, and anomaly detection.
All these applications share the search procedure for a novel
concept, which is scarcely seen in the data and hence can
all be encompassed by the umbrella term novelty detection.
Consequently, a wide range of real-world applications can
be modeled by one-class classiﬁers. Conventional research
often models the target class, and then rejects samples not following this model. Self-representation and
statistical modeling are the commonly used approaches
for this task. For data representation, low level features ,
high level features (e.g., trajectories ), deeply learned
features are used in the literature. A brief review of state-of-the-art novelty detection methods especially
the ones based on adversarial learning in deep networks is
provided in this section.
Self-Representation. Several previous works show that
self-representation is a useful tool for outlier or novelty event
detection. For instance, proposed self-representation
techniques for video anomaly detection and outlier detection
through learning a sparse representation model, as a measure
for separating inlier and outlier samples. It is assumed that
outlier or novel samples are not sparsely represented using
the samples from the target class. In some other works
(like ), testing samples are reconstructed using the
samples from the target class. The decision if it is an inlier
or outlier (novel) is made based on the reconstruction error,
i.e., high reconstruction error for a sample indicates that it
is more probably an outlier sample. In another work, Liu et
al. proposed to use a low-rank self-representation matrix
in place of a sparse representation, penalized by the sum of
unsquared self-representation errors. This penalization leads
to more robustness against outliers (similar to ). Similarly,
auto-encoders are also exploited to model and measure the
reconstruction error for the related tasks of outlier removal
and video anomaly detection, in .
Statistical Modeling. More conventional methods tend
to model the target class using a statistical approach. For
instance, after extracting features from each sample, a distribution function is ﬁt on the samples from the target class, and
samples far from this distribution are considered as outliers
or novelty (e.g., ). In another work, Rahmani and
Atia proposed an algorithm termed Coherence Pursuit
(CoP) for Robust Principal Component Analysis (RPCA).
They assumed that the inlier samples have high correlations
and can be spanned in low dimensional subspaces, and hence
they have a strong mutual coherence with a large number
of data points. As a result, the outliers either do not accord
with the low dimensional subspace or form small clusters.
Also, a method proposed in , OutlierPursuit, used convex optimization techniques to solve the PCA problem with
robustness to corrupted entries, which led to the develop-
ment of many recent methods for PCA with robustness to
outliers. Lerman et al. described a convex optimization
problem for detecting the outliers and called it REAPER,
which can reliably ﬁt a low-dimensional model to the target
class samples.
Deep Adversarial Learning. In the recent years, GANs
 have shown outstanding success in generating data
for learning models. They have also been extended to classi-
ﬁcation models even in the presence of not enough labeled
training data (e.g., in ). They are based on a
two-player game between two different networks, both concurrently trained in an unsupervised fashion. One network
is the generator (G), which aims at generating realistic data
(e.g., images), while the second network poses as the discriminator (D), and tries to discriminate real data from the data
generated by G. One of the different types of GANs, closely
related to our work, is the conditional GANs, in which G
takes an image X as the input and generates a new image
X′. Whereas, D tries to distinguish X from X′, while G
tries to fool D producing more and more realistic images.
Very recently Isola et al. proposed an “Image-to-image
translation” framework based on conditional GANs, where
both G and D are conditioned on the real data. They showed
that a U-Net encoder-decoder with skip connections
could be used as the generator coupled with a patch-based
discriminator to transform images with respect to different
representations. In a concurrent work, proposed to
learn the generator as a reconstructor of normal events, and
hence if it cannot properly reconstruct a chunk of the input
frames, that chunk is considered an anomaly. However, in
our work, the ﬁrst module (i.e., R) not only reconstructs the
target class, but it also helps to improve the performance for
the model on any given testing image, by reﬁning samples
belonging to the target class, and decimating/distorting the
anomaly or outlier samples.
3. Proposed Approach
The proposed one-class classiﬁcation framework is composed of two main modules: (1) Network R, and (2) Network D. The former acts as a prepossessing and Reﬁnement
(or Reconstruction) step, while the latter performs the
Discrimination (or Detection). These two networks are
learned in an adversarial and unsupervised manner, within
an end-to-end setting. In this section, we present a detailed
overview of both. The overall schema of the proposed approach is shown in Fig. 2. It can be seen that R reconstructs
its input, X, generates X′, and tries to fool D so that it speculates that the reconstructed sample is the original data, not
a reconstructed sample. On the other hand, D has access
to the original set of data and is familiar with their concept.
Hence it will reject the reconstructed samples. These two
networks play a game, and after the training stage, in which
samples from the target class are presented to the model, R
target class training sample
Target Class
Likelihood
Figure 2. Overview of the proposed structure for one-class classiﬁcation framework. R and D are two modules of the model, which
are adversarially learned. R is optimized to reconstruct samples
belonging to the target class, while it works as a decimator function for outlier inputs, whereas D classiﬁes the input data positive
(target) and negative (outlier or anomaly). D(R(X)) measures the
likelihood of the given input sample belonging to the target class.
will become an expert to reconstruct the samples from the
target class with a minimum error to successfully fool D.
The training procedure leads to a pair of networks, R and D,
which both learn the distribution of the target class. These
two modules are trained in a GAN-style adversarial learning
framework, forming an end-to-end framework for one-class
classiﬁcation for novelty detection. To make the proposed
method more robust against noise and corrupt input samples,
a Gaussian noise (denoted by η in Fig. 2) is added to the
input training samples and fed to R. Detailed descriptions
of each module and the overall training/testing procedures
are described in the following subsections.
3.1. R Network Architecture
It is previously investigated that the reconstruction error of an auto-encoder, trained on samples from the
target class, is a useful measure for novelty sample detection.
Since the auto-encoder is trained to reconstruct target class
samples, the reconstruction error for negative (novelty) samples would be high. We use a similar idea, but in contrast,
we do not use it for the detection or the discrimination task.
Our proposed model uses the reconstructed image to train
another network for the discrimination task.
To implement the R network, we train a decoder-encoder
Convolutional Neural Network (CNN) on samples from the
target class to map any given input sample to the target concept. As a result, R will efﬁciently reconstruct the samples
that share a similar concept as the trained target class, while
for outlier or novelty inputs, it poorly reconstructs them. In
other words, R enhances the inliers (samples from the target
class), while it destructs or decimates the outliers, making
it easier for the discriminator to separate the outliers from
the vast pool of inliers. Fig. 3 shows the structure of R
(5×5×3×64)
(5×5×64×128)
(5×5×128×256)
(5×5×256×512)
(5×5×512×256)
(5×5×256×128)
(5×5×128×64)
Figure 3. R network architecture, composed of encoding (ﬁrst part)
and decoding (second part) layers. The properties of each layer are
indicated with four hyperparameters in this order: (ﬁrst dimension
of the kernel × the second dimension of the kernel × the number
of input channels × the number of output channels).
architecture, which includes several convolution layers (as
the encoder), followed by several deconvolution layers (for
the decoding purpose). For improving the stability of the network similar to , we do not use any pooling layers in this
network. Eventually, R learns the concept shared in the target class to reconstruct its inputs based on that concept. Also,
after each convolutional layer, a batch normalization 
operation is exploited, which adds stability to our structure.
3.2. D Network Architecture
The architecture for D is a sequence of convolution layers, which are trained to eventually distinguish the novel or
outlier sample, without any supervision. Fig. 4 shows the details of this network’s architecture. D outputs a scalar value,
relative to the likelihood of its input following the distribution spanned by the target class, denoted by pt. Therefore,
its output can be considered as a target likelihood score for
any given input.
(5×5×256×512)
(5×5×3×64)
Target Class Likelihood
(5×5×64×128)
(5×5×128×256)
Figure 4. D network architecture, which determines if its input is
from the target class or it is an outlier or novelty. Properties of the
layers are denoted similarly to Fig. 3.
3.3. Adversarial Training of R+D
As mentioned in section 2, Goodfellow et al. introduced an efﬁcient way for adversarial learning of two
networks (denoted by Generator (G) and Discriminator (D)),
called Generative Adversarial Networks (GANs). GANs aim
to generate samples that follow the same distribution as the
real data, through adversarial learning of the two networks.
G learns to map any random vector, Z from a latent space
following a speciﬁc distribution, pz, to a data sample that
follows the real data distribution (pt in our case), and D
tries to discriminate between actual data and the fake data
generated by G. Generator and Discriminator are learned in
a two-player mini-max game, formulated as:
EX∼pt[log(D(X))]
+ EZ∼pz[log(1 −D(G(Z)))]
In a similar way, we train the R+D neural networks in an
adversarial procedure. In contrast to the conventional GAN,
instead of mapping the latent space Z to a data sample with
the distribution pt, R maps
˜X = (X ∼pt) +
 η ∼N(0, σ2I)
in which η is the added noise sampled from the normal
distribution with standard deviation σ, N(0, σ2I). From
now on, the noise model is denoted by Nσ for short. This
statistical noise is added to input samples to make R robust
to noise and distortions in the input images, in the training
stage. As mentioned before, pt is the supposed distribution
of the target class. D is aware of pt, as it is exposed to
the samples from the target class. Therefore, D explicitly
decides if R( ˜X) follows pt or not. Accordingly, R+D can
be jointly learned by optimizing the following objective:
EX∼pt[log(D(X))]
X∼pt+Nσ[log(1 −D(R( ˜X)))]
Based on the above objective (similar to GAN), network
R generates samples with the probability distribution of
pt, and as a result its own distribution is given by pr ∼
R(X ∼pt; θr), where θr is the parameter of the R network.
Therefore, we want to maximize pt(R(X ∼pt; θr)).
To train the model, we calculate the loss LR+D as the
loss function of the joint network R+D. Besides, we need
R’s output to be close to the original input image. As a
result, an extra loss is imposed on the output of R:
LR = ∥X −X′∥2.
Therefore, the model is optimized to minimize the loss
L = LR+D + λLR,
where λ > 0 is a trade-off hyperparameter that controls the
relative importance of the two terms. One of the challenging
issues for training R+D is deﬁning an appropriate stopping
criterion. Analyzing the loss functions of R and D modules
to excerpt a stopping criterion based on is a burdensome
task, and hence we use a subjective criterion. The training
procedure is stopped when R successfully maps noisy images to clean images carrying the concept of the target class
(i.e., favorably fools the D module). Consequently, we have
stopped the training of networks, when R can reconstruct its
input with minimum error (i.e., ∥X −X′∥2 < ρ, where ρ is
a small positive number).
After a joint training of the R+D network, the behavior
of each single one of them can be interpreted as follows:
• R(X ∼pt + η) −→X′ ∼pt, where ∥X −X′∥2 is
minimized. This is because θr is optimized to reconstruct those inputs that follow the distribution pt. Note
that R is trained and operates similar to denoising autoencoders or, denoising CNNs , and its output
will be a reﬁned version of the input data. See Figures
1 and 5 for some samples of its reconstructed outputs.
• For any given outlier or novelty sample (denoted by ˆX)
that does not follow pt, R is confused and maps it into
a sample, ˆX′, with an unknown probability distribution,
p?, (i.e., R( ˆX ≁pt + η) −→
ˆX′ ∼p?). In this
case, ∥ˆX −ˆX′∥2 cannot become very small or close to
zero. This is because R was not trained on the novelty
concept and cannot reconstruct it accordingly (similar
to ). Therefore, as a side effect, R decimates the
outliers. As an example, Fig. 6 shows samples of a
different concept being fed to R of a network trained
to detect digit “1”.
• We can expect that D(X′ ∼pt) > D( ˆX′ ≁pt), since
D is trained to detect samples from the distribution pt.
• It is interesting to note that in most cases D(R(X ∼
pt)) −D(R( ˆX ≁pt)) > D(X ∼pt) −D( ˆX ≁pt).
This signiﬁes that the output of R is more separable
than original images. It is because of this fact that R
supports D for better detection. To further explore this,
Fig. 7 shows the score generated as the output of D for
both cases. In some sensitive applications, it is more
appropriate to avoid making decisions on difﬁcult cases
 , and leave them for human intervention. These hardto-decide cases are known to be in the reject region. As
shown in Fig. 7 the reject region of D(X) is larger than
that of D(R(X)).
3.4. R+D: One-Class Classiﬁcation
In the previous subsection, characteristics of both R and
D networks are explained in details. As discussed, D acts as
the novelty detector, and beneﬁts the support of R. Hence,
the One-Class Classiﬁer (OCC) can be simply formulated
by only using the D network (similar to ) as:
Target Class
if D(X) > τ,
Novelty (Outlier)
otherwise,
where τ is a predeﬁned threshold. Although the above policy
for novelty detection works as well as the state-of-the-art
methods (explained in details in the Section 4), we further
propose to incorporate R in the testing stage. To this end,
R(X, θr) is used as a reﬁnement step for X, in which θr
is the trained model for the R module. θr is trained to
reconstruct and enhance samples that follow pt (i.e., are
from the target class). Consequently, instead of D(X) we
use D(R(X)). Eq. (7) provides a summary of our proposed
once-class classiﬁcation scheme:
Target Class
if D(R(X)) > τ,
Novelty (Outlier)
otherwise.
4. Experiment Results
In this section, the proposed method is evaluated on three
different image and video datasets. The performance results
are analyzed in details and are compared with state-of-theart techniques. To show the generality and applicability of
the proposed framework for a variety of tasks, we test it for
detection of (1) Outlier images, and (2) Video anomalies.
4.1. Setup
All the reported results in this section are from our implementation using the TensorFlow framework , and Python
ran on an NVIDIA TITAN X. The detailed structures of D
and R are explained in details in Sections 3.2 and 3.1, respectively. These structures are kept ﬁxed for different tasks,
and λ in Eq. (5) is set equal to 0.4. The hyperparameters
of batch normalization (as in ) are set as ϵ = 10−6 and
decay= 0.9.
4.2. Outlier Detection
As discussed earlier, many computer vision applications
face considerable amounts of outliers, since they are common in realistic vision-based training sets. On the other
hand, machine learning methods often experience considerable performance degradation in the presence gross outliers,
if they fail to deal with processing the data contaminated by
noise and outliers. Our method can learn the shared concept
among all inlier samples, and hence identify the outliers.
Similar to , we evaluate the performance of our
outlier detection method using MNIST1 and Caltech2
 datasets.
1Available at 
2Available
 
Datasets/Caltech256/
Figure 5. Examples of the output of R for several inlier and outlier samples from the UCSD Ped2 dataset: R is learned on normal patches.
Left and right samples show the inlier (i.e., target) and outlier (i.e., novelty) samples, respectively. As can be seen, the network R enhances
its input and shows to be robust to the noise present in its input. First row: Patch contaminated by some Gaussian noise; Second row:
Original patches; Third row: The output of R on the noisy samples.
Figure 6. Outputs of R trained to detect digit “1” on MNIST dataset.
Samples from classes “6” and “7” are given to the model as outliers.
R failed to reconstruct them and fundamentally distorted them. In
each pair of the images, the ﬁrst one is the original image and the
second one is the output of R.
Reject Region
Outlier Class
Inlier Class
Figure 7. R+D is trained on inlier samples (digit “1”) from MNIST
dataset. Top: D(R(X)) scores; Bottom: D(X) scores. The scores
are generated on 20 inlier and 20 outlier samples. The red squares
indicate inlier samples, while × marks are representatives of the
outliers. Reject region for R(X) is larger than that of D(R(X)).
Outlier Detection Datasets
MNIST: This dataset includes 60,000 handwritten digits from “0” to “9”. Each of the ten categories of digits is
taken as the target class (i.e., inliers), and we simulate outliers by randomly sampling images from other categories
with a proportion of 10% to 50%. This experiment is repeated for all of the ten digit categories.
Caltech-256: This dataset contains 256 object categories with a total of 30,607 images. Each category has at
least 80 images. Similar to previous works , we repeat
the procedure three times and use images from n ∈{1, 3, 5}
randomly chosen categories as inliers (i.e., target). The ﬁrst
150 images of each category are used, if that category has
more than 150 images. A certain number of outliers are randomly selected from the “clutter” category, such that each
experiment has exactly 50% outliers.
Outlier Detection Results
Result on MNIST: The joint network R+D is trained on
images of the target classes, in the absence of outlier samples.
Following , we also report the F1-score as a measure
to evaluate the performance of our method and compare it
with others. Fig. 8 shows the F1-score of our method (and
the state-of-the-art methods) as a function of the portion of
outlier samples. As can be seen, our method (i.e., D(R(X)))
operates more efﬁcient than the other two-state-of-the-art
methods (LOF and DRAE ). Also, it is important
to note that with the increase in the number of outliers, our
method operates consistently robust and successfully detects
the outliers, while the baseline methods fail to detect the
outliers as their portion increases. Furthermore, one interesting ﬁnding of these results is that, based in Fig. 8, D(X)
itself can operate successfully well, and outperform the stateof-the-art methods. Nevertheless, it is even improved more
when we incorporate R module, as it modiﬁes the samples
(i.e., reﬁnes the samples from the target class, and decimates
the ones coming from an outlier concept) and helps distinguishability of the samples.
Result on Caltech-256: In this experiment, similar setup
as in is used, and we compare our method with 
and 6 other methods therein designed speciﬁcally for detecting outliers, including Coherence Pursuit (CoP) ,
OutlierPursuit , REAPER , Dual Principal Component Pursuit (DPCP) , Low-Rank Representation (LRR)
 , OutRank . The results are listed in Table 1, which
are comprised of F1-score and area under the ROC curve
(AUC). The results of other methods are borrowed from .
This table conﬁrms that our proposed method performs at
least as well as others, while in many cases it is superior to
Table 1. Results on the Caltech-256 dataset: Inliers are taken to be images of one, three, or ﬁve randomly chosen categories, and outliers are
randomly chosen from category 257-clutter. Two ﬁrst rows: Inliers are from one category of images, with 50% portion of outliers; Two
second rows: Inliers are from three categories of images, with 50% portion of outliers; Two last rows: Inliers come from ﬁve categories
of images, while outliers compose 50% of the samples. The last two columns have the results or our methods, D(X) and D(R(X)),
respectively. Note that in each row the best result is typeset in bold and the second best in italic typeface.
REAPER 
OutlierPursuit 
R-graph 
Ours D(R(X))
Percentage of outliers (%)
Figure 8. Comparisons of F1-scores on MNIST dataset for different
percentages of outlier samples involved in the experiment.
Normal Patches
Anomaly Patches
Figure 9. Examples of patches (denoted by X) and their reconstructed versions using R (i.e., R(X)): Three left Columns are
normal patches, and two right ones are abnormal. The output of D
is the likelihood of being a normal patch (a scalar in range ).
them. As can be seen, both proposed methods (i.e., D(X)
and D(R(X))) outperform all other methods in most cases.
Interestingly, as we increase the number of inlier classes,
from 1 to 3 and 5 (ﬁrst, second and the last two rows, respectively), our method robustly learns the inlier concept(s).
4.3. Video Anomaly Detection
Anomaly event detection in videos or visual analysis of
suspicious events is a topic of great importance in different
computer vision applications. Due to the increased complexity of video processing, detecting abnormal events (i.e.,
anomaly or novelty events) in videos is even a more burdensome task than image outlier detection. We run our method
on a popular video dataset, UCSD Ped2. The results are
reported on a frame-level basis, as we aligned our experimental setup to previous works for comparison purposes.
Anomaly Detection Dataset
UCSD dataset: This dataset includes two subsets, Ped1
and Ped2, from two different outdoor scenes, recorded with a
static camera at 10 fps and resolutions 158 × 234 and 240 ×
360, respectively. The dominant mobile objects in these
scenes are pedestrians. Therefore, all other objects (e.g.,
cars, skateboarders, wheelchairs, or bicycles) are considered
as anomalies. We evaluate our algorithm on Ped2.
Anomaly Detection Results
Result on UCSD Ped2: For this experiment, we divide the
frames of the video into 2D patches of size 30×30. Training
patches are only composed of frames with normal behaviors. In the testing phase, test patches are given to the joint
network R+D, and the results are recorded. Fig. 9 shows
examples of the output of R on the testing patches. As it
is evident, normal patches (i.e., left part of the ﬁgure) are
successfully reﬁned and reconstructed by the R network,
while the abnormal ones (i.e., the right part of the ﬁgure)
are distorted and not adequately reconstructed. The last two
rows in the ﬁgure show the likelihood score identiﬁed by our
methods (D(X) and D(R(X)), respectively). D(R(X))
shows to yield more distinguishable results, leading to a
better model for one-class classiﬁcation and hence video
anomaly detection. It is fascinating to note that one of the
Table 2. Frame-level comparisons on Ped2
MPCCA 
Ravanbakhsh et al. 
Ravanbakhsh et al. 
Bertini et al. 
Dan Xuet al. 
Dan Xu et al. 
Sabokrou et al. 
Li et al. 
Deep-cascade 
Ours - D(X)
Ours - D(R(X))
most critical dilemmas for video anomaly detection methods is their high false positives. That is, algorithms often
detect many of the ‘normal’ scenes as anomalies. In Fig. 9,
three left columns are three tough ‘normal’ examples, as the
human subject is not completely visible in the patch. We
deliberately visualized these cases to illustrate how using
D(R(X)) can effectively increase the discriminability of
the system, compared to only D(X).
Similar to , we also report frame-level Equal Error Rate (EER) of our method and the compared methods.
For this purpose, in any frame, if a pixel is detected as an
anomaly, that frame is so labeled as ‘anomaly.’ Table 2 shows
the result of our method in comparison to the state-of-the-art.
The right column in Table 2 lists the results from methods
based on variations of deep-learning. This table conﬁrms
that our method is comparable to all these approaches, while
we are solving a more general problem that can be used
for any outlier, anomaly or novelty detection problem. It is
worth noting that other methods (especially Deep-cascade
 ) beneﬁt from both spatial and temporal complex features, while our method operates on a patch-based basis,
considering only spatial features of the frames. Our goal
was to illustrate that the proposed method operates at least
as well as the state-of-the-art, in a very general setting with
no further tuning to the speciﬁc problem type. Simply, one
can use spatiotemporal features and even further improve
the results for anomaly event detection or related tasks.
4.4. Discussion
The experimental results conﬁrmed that R+D detects
the novelty samples at least as well as the state-of-the-art
or better than them in many cases, but ﬁnding the optimal
structure and conducting the proper training procedure for
these networks can be tedious and cumbersome tasks. The
structure used in this paper proved well enough for our applications, while it can still be improved. We observed that
achieving better performance is possible by modifying the
structure, e.g., by some modiﬁcation in the size and the order
of convolutional layers of R+D, we achieved better results
by margins of 0.02 to 0.04 compared to the results reported
in Table 1. Another important point is that it is very critical when to stop the training procedure of the joint network
R+D. Stopping the training too early leads to immature
learned network weights, while overtraining the networks
confuses the R module and yields undesirable outputs. The
stopping criterion outlined in Section 3.3 provides a right balance for the maturity of the joint network in understanding
the underlying concept in the target class.
In addition, it is important note is that training a model
in absence of the novelty/outlier class can be considered as
weak supervision. For many problems this is acceptable,
as all the samples we have are often inliers. When dealing
with outlier detection problems, we can assume that number
samples from the target class is much larger than the outlier
samples. However, if we train the model at the presence of
small number of outlier samples, the model still works. In a
followup experiment, we mixed data from target (90%) and
outlier (10%) classes in the training phase of the Ped2 experiment, and observed that the EER only dropped by 1.3%,
which is still comparable to the state-of-the-art methods.
One of the major concerns in GANs is the mode collapse
issue , which often occurs when the generator only learns
a portion of real-data distribution and outputs samples from
a single mode (i.e., it ignores other modes). For our case, it
is a different story as R directly sees all possible samples
of the target class data and implicitly learns the manifold
spanned by the target data distribution.
5. Conclusion
In this paper, we have proposed a general framework
for one-class classiﬁcation and novelty detection in images
and videos, trained in an adversarial manner. Speciﬁcally,
our architecture consists of two modules, Reconstructor and
Discriminator. The former learns the concept of a target class
to reconstruct images such that the latter is fooled to consider
those reconstructed images as real target class images. After
training the model, R can reconstruct target class samples
correctly, while it distorts and decimates samples that do
not have the concept shared among the target class samples.
This eventually helps D discriminate the testing samples
even better. We have used our models for a variety of related
applications including outlier and anomaly detection in images and videos. The results on several datasets demonstrate
that the proposed adversarially learned one-class classiﬁer is
capable of detecting samples not belonging to the target class
(i.e., they are novelty, outliers or anomalies), even though
there were no samples from the novelty class during training.
Acknowledgement
This research was in part supported by a grant from IPM
(No. CS1396-5-01).