Sensitivity of PCA for Trafﬁc Anomaly Detection
Haakon Ringberg
Department of Computer Science
Princeton University
Augustin Soule
Thomson Research
Jennifer Rexford
Department of Computer Science
Princeton University
Christophe Diot
Thomson Research
Detecting anomalous traﬃc is a crucial part of managing
IP networks.
In recent years, network-wide anomaly detection based on Principal Component Analysis (PCA) has
emerged as a powerful method for detecting a wide variety of anomalies.
We show that tuning PCA to operate
eﬀectively in practice is diﬃcult and requires more robust
techniques than have been presented thus far. We analyze
a week of network-wide traﬃc measurements from two IP
backbones (Abilene and Geant) across three diﬀerent traﬃc
aggregations (ingress routers, OD ﬂows, and input links),
and conduct a detailed inspection of the feature time series for each suspected anomaly. Our study identiﬁes and
evaluates four main challenges of using PCA to detect traf-
ﬁc anomalies: (i) the false positive rate is very sensitive to
small diﬀerences in the number of principal components in
the normal subspace, (ii) the eﬀectiveness of PCA is sensitive to the level of aggregation of the traﬃc measurements,
(iii) a large anomaly may inadvertently pollute the normal
subspace, (iv) correctly identifying which ﬂow triggered the
anomaly detector is an inherently challenging problem.
Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network
Operations; C.4 [Performance of Systems]:
Techniques
General Terms
Measurement, Performance, Reliability
Network Traﬃc Analysis, Principal Component Analysis,
Traﬃc Engineering
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGMETRICS’07, June 12–16, 2007, San Diego, California, USA.
Copyright 2007 ACM 978-1-59593-639-4/07/0006 ...$5.00.
INTRODUCTION
Traﬃc anomalies, such as ﬂash crowds, denial-of-service
attacks, port scans, and the spreading of worms, can have
detrimental eﬀects on Internet services. Detecting and diagnosing these anomalies is critical to network operators, who
must take corrective action to alleviate congestion, block attacks, and warn aﬀected users. Sifting through an immense
amount of measurement data to identify the anomalous traf-
ﬁc is an onerous task, best left to automated analysis. On
the surface, anomaly detection seems straightforward: pick
a statistical deﬁnition of an anomaly, feed the measurement
data into a statistical-analysis technique, and classify the
statistical outliers as anomalies. Unfortunately, anomaly detection is much more complicated than it seems. There are
many ways to represent the traﬃc and pinpoint anomalies,
each with its own set of assumptions, limitations, and tunable parameters that signiﬁcantly aﬀect the results.
paper focuses on that problem.
Principal Component Analysis (PCA) is perhaps the
best-known statistical-analysis technique for detecting network traﬃc anomalies. PCA is a dimensionality-reduction
technique that returns a compact representation of a multidimensional dataset by reducing the data to a lower dimensional subspace. Recent papers in networking literature have
applied PCA to the problem of traﬃc anomaly detection
with promising initial results . Our research
shows that a great deal of manual tuning is necessary to
achieve such results, however, because PCA is very sensitive
to its parameters and the proposed techniques for tuning
them are inadequate. In this paper, we identify and evaluate four main challenges of using PCA for traﬃc anomaly
detection:
The false-positive rate is very sensitive to the dimensionality of the normal subspace: PCA’s eﬀectiveness as a traﬃc anomaly detector is very sensitive to its two
main tunable parameters—the dimensionality of the normal
subspace (the topk parameter) and the detection threshold.
Previous research has required a great deal of manual tuning of these parameters. We show that the false-positive rate
can vary by a factor of three or more within a small range of
topk values. The detection threshold, on the other hand, has
a more predictable impact, and provides operators with an
intuitive knob to strike a balance between the false-positive
rate and the total number of detections.
The eﬀectiveness of PCA is sensitive to the way
the traﬃc measurements are aggregated: The large
volume of IP ﬂow traces must be aggregated before PCA is
applied. We evaluate three diﬀerent ways of aggregating the
data—by input link, by ingress router, and by OD pairs—
and ﬁnd that the choice has a signiﬁcant impact on PCA’s
eﬀectiveness. Choosing a representation that aggregates the
data too much leads to highly smooth data that hides all but
the most blatant anomalies, whereas aggregating too little
often yields ﬂows with wildly varying relative sizes and thus
causes PCA to be overly sensitive to variations among the
small ﬂows. In addition, some representations of the traﬃc,
such as OD ﬂows, are only appropriate for very long traces,
limiting the applicability of PCA.
Large anomalies can contaminate the normal subspace: Suﬃciently large anomalies can inadvertently pollute PCA’s normal subspace, thereby skewing the deﬁnition
of normalcy and increasing the false-positive rate as a result. We support this contention with real measurements of
a short-lived but drastic network event in the Geant network
that goes largely undetected by PCA because of this phenomenon. This argues for preprocessing measurement data
to detect, and ﬁlter, large anomalies before applying PCA.
Pinpointing the anomalous ﬂows is inherently dif-
ﬁcult: The problem of identifying which ingress router, for
example, was responsible for a PCA detection is fundamentally hard. Unfortunately, there is no direct mapping between PCA’s dimensionality-reduced subspace and the original spatial location of the anomaly. We show that the previously employed heuristic for associating a given PCA detection with speciﬁc location (e.g., an ingress router) relies
on an assumption that does not hold in general, and has the
side-eﬀect of associating a large fraction of the detections
with a very small set of locations.
In order to demonstrate these four points, we analyze
one week of IP ﬂow data for both the Geant and Abilene
backbones, including a detailed examination of the feature
time series for each detected anomaly to identify false positives. Comparing the results for Geant and Abilene allows
us to conclude that the appropriate setting of PCA’s tunable parameters varies from one network to another. Still,
our comparison between Geant and Abilene illustrates that
(i) the relative properties of traﬃc aggregations, in terms of
false-positive rate and total number of detections, (ii) the
diﬃculty of identifying which anomalous ﬂow caused a PCA
detection, and (iii) the contamination of PCA’s normal subspace, appear to hold across networks.
The remainder of the paper is organized as follows.
Section 2 we present a brief overview of how to apply PCA
to detect anomalous traﬃc. Next, Section 3 describes our
software for aggregating the measurement data, applying
PCA, and validating the resulting anomalies. Then, we evaluate the sensitivity of PCA to the topk parameter and the
detection threshold for the two networks and three traﬃc
aggregations in Section 4.
In Section 5 we introduce two
limitations intrinsic to PCA, viz. contamination of the normal subspace, and identiﬁcation of the ﬂows responsible for
triggering the detection of an anomaly. We present related
research in Section 6 and conclude in Section 7.
PCA FOR TRAFFIC ANOMALIES
PCA is a dimensionality-reduction technique that has been
applied to many kinds of data. In fact, PCA is the optimal
such linear transform—that is, for any choice for the number of dimensions, PCA returns the subspace that retains
the highest variance . In this section, we describe how to
use PCA to construct a model of “normal” traﬃc, and to
detect and identify the statistical outliers.
Model Construction
Network-wide anomaly detection draws on measurement
data from multiple locations and time periods. We deﬁne
a traﬃc matrix as a timeseries of n measurement vectors
⃗v1, . . . , ⃗vn, where each time-step i has m measurements (i.e.,
|⃗vi| = m). We intentionally leave the precise meaning of the
cell vi,j unspeciﬁed, since the choice may vary from one representation of the data to another. For example, vi,j could
be the number of bytes or packets observed at time-step i
at location j, or could be something more complex, such as
the entropy of the distribution of source IP addresses in the
traﬃc seen at location j during time interval i. Throughout
the paper, we refer to the m columns of the traﬃc matrix
as ’ﬂows’, so we can talk about the matrix without regard
for how we choose to aggregate the data. When we need to
refer to ’IP ﬂows’, we use that term explicitly.
When applied to a matrix, PCA returns a set of orthonormal vectors (called the principal components) such that for
all k ≤m, the k-subspace deﬁned by these vectors captures
the highest variance in the original matrix. Adhering to previous terminology, we refer to the subspace deﬁned by these
ﬁrst k principal components as the “normal subspace”; we
refer to k itself as the topk parameter.
The basic underlying assumption of traﬃc anomaly detection is that the
k-subspace corresponds to the primary regular trends (e.g.,
diurnal, weekly) of the traﬃc matrix. Previous work 
has shown that traﬃc timeseries have low intrinsic dimensionality, which means that k can be a small number. In
section 4.1 we investigate PCA’s sensitivity to this parameter in the context of traﬃc anomaly detection.
Once the “normal” operation of the network has been
gleaned from the traﬃc traces, one may assume that what
is left is either statistical anomalies or mere noise.
is, when the normal subspace has been removed, one is left
with a (n −k)-subspace that can then either be treated as
wholly anomalous or be further split into a ℓ-subspace that
is anomalous and a (n −k −ℓ)-subspace of noise.
two or three subspaces are then the simpliﬁed model that
is retained of the entire traﬃc trace. In constructing this
model of the traﬃc, the m ﬂows of the m × n traﬃc matrix
can be thought of as random variables, of which there are n
observations each. As such, it does not make sense to consider cases where m > n, i.e., one cannot draw statistically
sound conclusions when one has fewer observations than one
has variables1. We will therefore require that the number
of time-steps n must be greater than or equal to the cardinality of the chosen traﬃc aggregation m, which we will
demonstrate in section 3.3 to be a problem for certain traﬃc
aggregations.
Detection and Identiﬁcation
Classiﬁcation of a new measurement vector ⃗v, representing a given moment in time, occurs in relation to the model
constructed in the previous step. ⃗v is projected onto the
relevant subspaces in the model, which decomposes the vector into a linear combination of its normal and anomalous
constituents. ⃗v is then classiﬁed as normal or anomalous
depending on whether it is primarily expressed by the nor-
1Recent theoretical work attempts to overcome this limitation in certain circumstances.
Table 1: Networks studied
mal or anomalous subspaces. The threshold that determines
how statistically signiﬁcant a given event (spike) must be for
it to be ﬂagged as anomalous is another parameter that can
be tuned, and we investigate its impact in section 4.2.
Finally, if ⃗v is classiﬁed as anomalous, we must determine
precisely which set of columns C ⊆[1, m] (i.e., ﬂows) of the
traﬃc matrix were primarily responsible for the detection.
Knowing that an anomaly occurred at a particular time is
typically not suﬃcient—knowing where it happened often
matters as well.
For example, the network operator may
need to know which ingress routers were the entry point for
the anomalous traﬃc. It is important to realize that this
ﬂow identiﬁcation step is separate from PCA, and diﬀerent
heuristics may be employed here. It is a necessary step in the
context of network traﬃc anomaly detection, however, and
we will therefore evaluate the detector that is the combination of the above PCA technique and the heuristic employed
by the line of work that followed Lakhina et al. . We will further discuss this heuristic in section 5.2.
METHODOLOGY
We designed and implemented the architecture shown in
ﬁgure 1 to evaluate PCA’s eﬀectiveness as a traﬃc anomaly
detector. The diagram is organized into four sub-components,
each of which will be individually explained in the following
subsections. Speciﬁcally, section 3.1 will detail the data that
were collected; section 3.2 explains how the data were preprocessed and aggregated according to either ingress routers,
OD ﬂows, or input links; section 3.3 deals with our interface with the PCA anomaly detector written by Lakhina et
al. ; and section 3.4 describes how we labeled the detected anomalies as false positives or true positives.
Data Sources
For this work we studied both the Abilene and Geant 
networks. Their respective properties are summarized in table 1. Abilene is an 11-node research backbone that connects
Internet2 universities and research labs across the continental United States. Abilene does not, however, provide transit services to the Internet at large; instead, its participants
must maintain separate connections to the commodity Internet . Geant is a 23-node network that connects national
research and education networks representing 30 European
countries; unlike Abilene, Geant does provide Internet connectivity to its participants.
Both the Abilene and Geant networks collect ﬂow statistics using Juniper’s J-Flow tool . Abilene samples 1 out of
every 100 packets for inclusion in the ﬂow statistics whereas
Geant samples packets at 1 out of 1000. In Abilene, packets are aggregated into ﬁve-minute time-bins compared to
a ﬁfteen-minute time window for Geant. Finally, Abilene
anonymizes the last eleven bits of the IP address stored in
the ﬂow records to preclude a reader from identifying the
source or destination host.
In order to aggregate the collected IP ﬂows into OD-
ﬂows, we also need to parse the routing data from each
network. Abilene deploys Zebra BGP monitors that record
all BGP messages they receive.
This means that for any
⟨ingress,preﬁx⟩pair, it is suﬃcient to parse the BGP logs
in order to identify the egress point of the IP ﬂow. Geant
has one Zebra BGP monitor embedded in an iBGP mesh
that logs a single BGP record for all routers, which gives us
a set of egress points for a given preﬁx. Subsequently we
must parse the Geant IS-IS logs to produce a minimumcost path from each ingress router to all egress routers,
which, in conjunction with the set of egress routers for a
given preﬁx, uniquely identiﬁes the egress point for a given
⟨ingress,preﬁx⟩pair.
For both networks, we studied a full week of data between
November 21st and 27th of 2005, corresponding to 2016 datapoints for each ﬂow in Abilene (e.g. 7 days × 24 hours × 60
bins per hour) and 672 for Geant.
Timeseries Construction
In the following subsection we will elaborate on how the
data was further preprocessed and transformed into entropy
timeseries before being analyzed by the PCA anomaly detector itself.
Entropy Timeseries
Previous work has demonstrated that entropy timeseries of the four main IP header features (source IP, destination IP, source port, and destination port) is a rich data
type to analyze for traﬃc anomaly detection. That is, for
every measurement vector ⃗vi at time i there are four measurements vi,j, . . . , vi,j+3 for every ingress router (for example). vi,j will be the entropy of the distribution of source IP
addresses for this router, vi,j+1 will be the entropy of the
distribution of destination IP addresses for this router, etc.
Pr[X = xi] log2(Pr[X = xi])
Entropy is studied because it provides a computationally
eﬃcient metric for estimating the dispersion or concentration in a distribution, and a wide variety of anomalies will
impact the distribution of one of the discussed IP features.
The entropy of a random variable X is deﬁned in equation 1,
where Pr[X = xi] is the probability of event xi ∈X occurring. In our context, the events are observations of a given
IP feature. For example, the probability of seeing port 80
is deﬁned to be number of packets using port 80 divided by
the total number of packets in the given time interval. A
sudden ﬂash crowd to a webserver, for example, will cause a
speciﬁc destination IP (the webserver) and destination port
(port 80) to become much more prevalent than in previous
time-steps, which will cause a decrease in the destination IP
and destination port entropy timeseries, respectively, and
hence allow us to detect it. A more complete explanation
of the beneﬁts of using entropy for traﬃc anomaly detection
can be found in .
Trafﬁc Aggregation
In addition to studying two networks, we also studied several traﬃc aggregations. That is, IP ﬂow traces must be further aggregated so that statistical methods such as PCA can
detect correlations and periodic trends in the data. If the
data type used for the traﬃc matrix was byte-counts instead
of entropy values, then a cell item vi,j would uniquely map
Flow Timeseries
Timeseries Construction
Data Sources
2005/11/23 14:45
2005/11/23 15:00
2005/11/23 15:15
2005/11/23 15:30
srcip dstip packets
129.242 122.200 100
122.200 108.200 2
108.108 232.433 54
Timeseries
PCA Anomaly
Manual Validation
Aggregation?
into input
into OD flows
into ingress
Figure 1: The Architecture
to the number of packets carried by router j, for example,
at time i.
There are many ways to perform structural traﬃc aggregation, each with diﬀerent statistical properties, e.g. diﬀerent number of constituent ﬂows, distribution in ﬂow size,
etc. Our research demonstrates that the choice of traﬃc aggregation can signiﬁcantly impact the eﬀectiveness of PCA
as a traﬃc anomaly detector, and hence it is important to
study several such formalisms.
It is often natural to perform this structural aggregation
of IP ﬂows according to where they enter and exit in the
network. We analyzed three such aggregations, viz. ingress
routers, OD ﬂows, and input links. For ingress routers, the
data is aggregated according to which router it entered the
network, e.g. there are 11 such ﬂows for Abilene because
it has 11 routers and they all accept incoming traﬃc. Performing this aggregation is straightforward because there are
separate IP ﬂow logs for each ingress router. For the “input
links” aggregation, IP ﬂow records are aggregated by
⟨ingress router, input interface⟩tuples, which is also computationally uncomplicated because IP ﬂow records contain the necessary interface information. An OD, or origindestination, ﬂow uniquely identiﬁes which ingress and egress
router an IP ﬂow traversed while inside the network. Identiﬁcation of the egress point e for a given
⟨ingress router, preﬁx⟩pair requires parsing of routing logs
as explained in section 3.1.
PCA Anomaly Detector
The Matlab code that performs the PCA calculations was
written by Lakhina et al. and was graciously donated
for our work.
As detailed in section 2, it builds a model
for normal traﬃc for the given traﬃc matrix and topk parameter, and classiﬁes a given time and ﬂow as anomalous
if the statistical outlier at that time exceeds the threshold
parameter. We wrote wrapper code around this software in
order to sweep a range of parameters, as is diagrammed by
the dial knobs in ﬁgure 1, to evaluate PCA’s sensitivity to
these parameters.
Applying PCA to network-wide traﬃc measurements introduces several complications.
First, because statistical
tools such as PCA analyze timeseries, they classify individual time bins as anomalous, which are diﬀerent from the
underlying network events that may have caused the detection. In fact, a given anomalous time bin may contain
multiple anomalous events of interest to a network operator and, vice versa, one anomalous event may span multiple time-bins. For simplicity, we use the term “anomaly”
as shorthand for “anomalous time bin” in the remainder of
this paper, consistent with previous work.
Second, PCA requires the length of the time series (i.e., n)
to be greater than or equal to the number of measurements
(i.e., m). In addition, the value of m depends not only on
the number of locations (e.g., input links, ingress routers, or
OD pairs), but also on the number of measurements included
from each vantage point. Jointly analyzing entropy for the
four IP traﬃc descriptors exploits PCA’s ability to ﬁnd correlations across dimensions, at the expense of requiring an
even longer time series.
For example, the Geant network
has 23 routers, which produces 552 OD ﬂows. This requires
a minimum of 552 × 4 = 2208 time-steps, which is equal to
= 23 days since Geant aggregates its ﬂow records
into 15-minute time bins. Analyzing such a large amount of
data simultaneously can be impractical, which is why we do
not include a Geant OD-ﬂow dataset in our study (see table 3). Moderately sized networks may therefore be unable
to run a PCA-based traﬃc anomaly detector on top of OD
ﬂows, which can be a very fruitful traﬃc aggregation .
In addition to the hard limit on how many time-steps
must be analyzed concurrently, the increase in the number
of variables processed by PCA also comes at a computational
overhead. The algorithm most commonly used for performing PCA—singular value decomposition (SVD)—takes time
Having ̺ measurements per vantage point not
only increases m by a factor of ̺ but may (for the reasons
explained in the previous paragraph) also increase n by the
same factor, leading to an O(̺3) factor increase in the computational overhead associated with applying PCA2.
Manual Validation
To provide qualitative statements about the eﬀectiveness
of PCA, we need some measure of ground truth. The paucity
2These issues could potentially be addressed by the technique proposed in for conceptually combining routers
according to topology, but we have not evaluated its eﬀectiveness in this context.
length of inspected window
heavy hitters inspected
min mean #packets
max times #packets = 0
once an hour
Table 2: Heuristics used in the manual validation
average number of packets
abilene input links
abilene OD flows
abilene ingress routers
geant input links
geant ingress routers
Figure 2: Average Packets Per Flow
of labeled data is a challenge facing research on network
anomaly detection, and our study is no diﬀerent. For our
work, we manually validated the anomalies detected by PCA
across the two networks, three traﬃc aggregations, and range
of tunable parameters we explored. We manually inspected
the entropy time series for a signiﬁcant fraction of the suspected anomalies and classiﬁed them as real anomalies or
false positives. We declared an anomaly to be a false positive if its entropy timeseries plots appeared merely to ﬂuctuate in a random fashion. Unfortunately, statistical tools
such as PCA do not always ﬂag the precise moment in time
that a trained operator might consider the beginning of an
Often, for example, the end of an anomaly is
equally statistically signiﬁcant as the beginning. As such,
we investigate a time window of length δ around each detected anomaly. We found that inspecting a three-hour time
window around any anomaly at time t was more than adequate (i.e., we inspect t ± 1.5 hours).
In our initial exploration of the data, we discovered that
many suspected anomalies involved ﬂows that carry relatively little traﬃc.
An input link, ingress router, or OD
pair with a small amount of traﬃc can experience signiﬁcant
variations in load in response to a modest change in traﬃc
conditions. These large variations in load often caused large
variations in other metrics, such as the entropy features in
the traﬃc matrix. In fact, we found that anomalies were
often triggered by the addition of a single IP ﬂow that is
a sustained ﬁle-transfer (also known as an “alpha ﬂow”).
We decided to ﬂag outliers that occurred on such relatively
small ﬂows as false positives because we deemed them uninteresting to network operators.
To illustrate the skew in traﬃc volumes, we computed the
average number of packets for each input link, ingress router,
Aggregation
ingress routers
input links
ingress routers
input links
Table 3: Classiﬁed anomalies
and OD pair over all of the time-steps of our measurement
data. Figure 2 plots the average number of packets in a ﬂow
for each traﬃc aggregation, as a function of the ﬂow ID. The
graph shows that the number of packets per ﬂow can vary
widely, especially for certain representations of the traﬃc.
For the input-link aggregation, in particular, big ﬂows carry
up to eight orders of magnitude more traﬃc than small ﬂows.
To ﬁlter away such very small ﬂows, we classiﬁed ﬂows that
had less than an average of ω sampled packets within the
inspected time-window of length δ, or carried 0 packets γ
times or more during this same period of time, as “small”.
ω was set to 100 in order to roughly correspond to the gap
seen at the tail of the Abilene and Geant input link ﬂow-size
plots seen in in ﬁgure 2, and γ was set so that ﬂows must
carry more than 0 packets in a time-window at least once
an hour (on average).
Finally, to give us further conﬁdence in our labeling of
anomalies as true or false positives, we manually inspected
the heavy-hitters for each of the four IP header features we
studied. That is, we inspected the top-n source IP heavyhitters, destination IP heavy-hitters, etc. We chose to set
n to 10 because the vast majority of heavy-hitter traﬃc
anomalies appear to involve only a handful of IP header
This validation step produced the datasets summarized
in table 3, where the “Total” column refers to the total
number of detections validated and the “FP” column refers
to the total number of false positives. Both the number of
total detections and false positives are across all topk values
and detection thresholds for the given dataset. The listed
total number of detections are all that were detected by
PCA in the studied week, except for the Abilene OD ﬂows
and input links aggregations, where a 22% and 23% random
sample was chosen, respectively. The false-positive rate and
total number of detections are the metrics we use to evaluate
PCA’s eﬀectiveness across the studied parameter space.
The false-negative rate is a key metric that is conspicuously absent from both table 3 and the preceding discussion. The lack of reliable estimates of the false-negative rate
is a long-standing problem in traﬃc anomaly detection that
is complicated by the magnitude of the datasets studied.
It is diﬃcult to know what anomalies might go undetected
when the size of the datasets studied approach the terabyte
range. We cannot comment on PCA’s false-negative rate in
our study because the set of possible anomalies that we were
considering was deﬁned in terms of the suspected anomalies
PCA detected. This is clearly a limitation of our study, as
well as previous work on traﬃc anomaly detection.
TUNABLE PARAMETERS
The following section will evaluate PCA’s sensitivity to
its two key parameters, viz. the number of dimensions that
constitute its normal subspace in section 4.1 and the detection threshold in section 4.2.
Size of Normal Subspace
The number of principal components included in the normal subspace—the topk parameter—is the most important
parameter to be tuned when using PCA as a traﬃc anomaly
detector. Past literature has made four important claims in
this context: (i) traﬃc traces have low intrinsic dimensionality, which means that topk can be small, (ii) these ﬁrst few
principal components capture the vast majority of the variance in the data, (iii) the same principal components are also
highly periodic and thus capture the diurnal trends sought
to be included in the normal subspace, and (iv) identifying the separation between normal and anomalous principal
components can be done by retaining the ﬁrst k principal
components such that the projection of the traﬃc data does
not contain a 3σ deviation from the mean .
following sub-sections show, in order, that the second claim
does not hold across all networks and traﬃc aggregations,
that the eﬀectiveness of PCA is very sensitive to the topk
parameter, and that the previously proposed techniques for
determining topk are inadequate.
Decoupling Size from Captured Variance
Each of our datasets support the previous ﬁnding that
traﬃc traces have low intrinsic dimensionality, as can be
seen from ﬁgure 3(a).
The ﬁgure contains scree plots for
each of the datasets used in our study. A scree plot is a
plot of percent variance captured by a given principal component.
We can conclude that traﬃc traces have low intrinsic dimensionality because the corresponding scree plots
have very early knees relative to the original dimensionality of the datasets (seen in table 3). This is an important
observation because it means that only a small fraction of
all principal components need to be included in the normal
subspace to capture the periodic trends that these ﬁrst few
principal components have been shown to exhibit .
However, our results do not support the earlier contention
that the ﬁrst few principal components necessarily capture
the vast majority of variability in the traﬃc matrix, which is
demonstrated by ﬁgure 3(b), which is the CDF of 3(a) in logscale. While the plots in ﬁgure 3(a) have knees somewhere
in the range , it is much more diﬃcult to argue that
setting topk to a value in this range would correspond to
a vast fraction of variance in ﬁgure 3(b). For example, if
an Abilene network operator wanted to capture 90% of the
variance for the input-link aggregation, he would need a topk
value that was an order of magnitude larger than previously
reported in the literature (at least 95). If, on the other hand,
he set topk to match up with the knee seen in ﬁgure 3(a),
he would capture less than half of the total variance.
The purpose of this section is not merely to debunk an
earlier coupling of low intrinsic dimensionality and percent
variance captured, but also to highlight that this distinction
is an important one.
One should not determine the topk
variable based on percent variance captured (i.e., plot 3(b))
because diﬀerent networks have diﬀerent natural levels of
variability, and the normal subspace should capture periodicity as opposed to a certain fraction of variance. For example, a research backbone for universities such as Abilene
will likely have a more variable matrix than a tier-1 network
because Abilene is (i) smaller, (ii) is used as an experimen-
abilene input links
% variance
abilene OD flows
% variance
abilene ingress routers
% variance
geant input links
% variance
geant ingress routers
% variance
(a) scree plots
# principal components
cumulative percent variance captured
abilene input links
abilene OD flows
abilene ingress routers
geant input links
geant ingress routers
(b) CDF of scree plots
Figure 3: Intrinsic Dimensionality
tal network, and (iii) only a very limited set of source hosts
gain access to the network. The same heterogeneity is exhibited across diﬀerent traﬃc aggregations also, where a more
highly aggregated traﬃc aggregation such as ingress routers
will have more stable statistical properties than input links,
which may have lots of small ﬂows that are highly variable.
It is therefore important to highlight that the topk parameter should not be determined based on cumulative percent
variance captured.
Sensitivity Analysis
PCA is very sensitive to the topk parameter: We
noted previously that the scree plots for our datasets each
appeared to have knees in the range . While that range
might appear small, our results indicate that PCA is very
sensitive to the number of principal components even within
such a limited range. As can be seen from ﬁgure 4(a), within
the range, the false-positive rate for Geant ingress
routers varies between 3.1% and 15.8%. If one ventures beyond this range, the performance degradation can be even
more rapid. In the same ﬁgure we can see that the false-
# principal components
percent false positives
geant (threshold = 90%)
ingress routers
input links
(a) Geant false-positive rate
# principal components
total detections
geant (threshold = 90%)
ingress routers
input links
(b) Geant total detections
# principal components
percent false positives
abilene (threshold = 90%)
ingress routers
input links
(c) Abilene false-positive rate
# principal components
total detections
abilene (threshold = 90%)
ingress routers
input links
(d) Abilene total detections
Figure 4: Impact of topk on false-positive rate and
total detections
positive rate when going from 6 to 8 principal components
for Geant input links increases from 9.2% to 31.6%. It is
therefore extremely important that the topk parameter be
carefully tuned. For the remainder of this paper, we will
deﬁne the ’appropriate’ topk value as the one that we consider achieves the best trade-oﬀbetween the false-positive
rate and the total number of detections.
The appropriate topk value varies across networks
and traﬃc aggregations: Figure 4 also shows that the
appropriate number of principal components to incorporate
into the normal subspace varies across networks and traﬃc
aggregations. For example, the appropriate choice of principal components is probably 2 for Abilene ingress routers,
3 for Geant ingress routers, and 5 for Abilene OD-ﬂows. It
is interesting to note that the relative order of these three
datasets in terms of topk value is identical to their relative ordering in terms of original dimensionality (see: total
number of ﬂows in table 3). We hypothesize that this phenomenon will hold in general, and further research might
provide rule-of-thumb guidelines that map
⟨original dimensionality, scree knee⟩tuples to a topk value.
Guidelines are not sound methodology, however, and PCA’s
sensitivity to the topk parameter necessitates a robust methodology.
Comparison of traﬃc aggregations: Finally, ﬁgure 4
shows that the choice of traﬃc aggregation has a strong
impact on PCA’s performance.
Choosing the right traf-
ﬁc aggregation is tricky: too much aggregation will lead to
smooth and predictable ﬂow curves whereas too little aggregation yields a very heavy-tailed ﬂow-size distribution (see
ﬁgure 2) and hence some highly variable small ﬂows whose
spikes are not of interest to network operators. In particular,
for both Abilene in ﬁgure 4(d) and Geant in ﬁgure 4(b), it is
clear that the ingress router aggregation consistently detects
fewer anomalies than OD ﬂows and input links. The reason
for this is that at the level of ingress routers, the data is so
aggregated and the ﬂows are so large that most anomalies
are eﬀectively drowned. This also means that the anomalies
that are ﬂagged by PCA when using this aggregation-level
tend to be large and obvious.
Hence, at its appropriate
topk value (e.g. 3 for Geant and 2 for Abilene), the ingress
routers aggregation has the lowest false-positive rate of the
three traﬃc aggregations studied for both networks.
On the other end of our aggregation spectrum, input links’
false-positive rate suﬀers as a result of a large fraction of
small ﬂows. Abilene input links is particularly bad in ﬁgure 4(c) in that its false-positive rate never goes below 40%.
It holds across both networks that, at their respective appropriate topk values, the input links aggregation has the
highest false-positive rate of the three formalisms. We believe this can be largely contributed to an excess of small
ﬂows whose natural variance cause alarms to be raised by
the PCA traﬃc anomaly detector.
For the Abilene network (ﬁgures 4(c) and 4(d)), it seems
clear that OD-ﬂows is the traﬃc aggregation that achieves
the best overall trade-oﬀbetween total detections and falsepositive rate. Our ﬁndings support earlier papers that
have demonstrated that OD-ﬂows is a fruitful traﬃc aggregation for detecting network anomalies. For this same reason, it is doubly frustrating that we are prevented from trying the OD-ﬂow aggregation for the Geant network due to
the reasons explained in section 3.3.
geant ingress routers
PC3−projection
(a) Geant ingress routers
abilene OD flows
PC2−projection
(b) Abilene OD ﬂows
# principal components
percent variance captured
abilene OD flows (threshold = 90%)
scree plot
Humphrey−Ilgen variant
(c) Abilene OD ﬂows
Figure 5: Determining the topk parameter
Evaluating Top-K Selection Techniques
We’ve demonstrated that (i) PCA is very sensitive to the
topk parameter, and (ii) that its appropriate value varies
from one setting to the next. For PCA’s eﬀective operation
as a traﬃc anomaly detector, it is therefore essential that
there are automated techniques for determining the proper
setting of the topk parameter.
Unfortunately, current research does not provide any reliable such techniques. Two
techniques that have been used include (i) determining topk
by visually inspecting the scree plot — a method referred to
as Cattell’s Scree Test in the statistics literature, and (ii)
retaining the ﬁrst k principal components that do not contain a 3σ deviation from the mean when the traﬃc matrix
is projected upon them.
We’ve evaluated the eﬀectiveness of the 3σ heuristic in
ﬁgures 5(a) and 5(b). Each ﬁgure shows the result of projecting the respective traﬃc matrices onto the ﬁrst principal
component that results in a 3σ deviation from the mean
(the ±3σ deviation is represented by the upper and lower
dashed horizontal lines). That is, there is a 3σ deviation for
these principal components because the solid lines exceed
the boundary of the dashed lines. Speciﬁcally, ﬁgure 5(a)
shows such a deviation for the third principal component,
which means that the 3σ heuristic suggests retaining two
principal components in the Geant ingress routers normal
subspace. Our results in ﬁgure 4(a) indicate that this would
lead to a false positive rate that is three times as high as
ideal. Likewise, ﬁgure 5(b) shows that the same heuristic
suggests keeping only a single principal component for the
Abilene OD ﬂow normal subspace. While we are not including the result here, the 3σ heuristic also suggests keeping
zero principal components for the Abilene ingress routers
normal subspace, which is not possible. It is therefore clear
that this heuristic is not robust. On the other hand, however, one can legitimately question whether principal components with such large spikes can capture normalcy; we
will address this question in section 5.1.
In ﬁgure 5(c) we have evaluated the eﬀectiveness of Cattell’s Scree Test. The knee of the scree plot appears to be
at k = 3 but we determined previously that a topk value of
k = 5 seems to achieve the best results. In general, Cattell’s
Scree Test is within one or two principal components, but
there appears to be no predictable pattern to the deviation.
Humphrey-Ilgen parallel analysis is an automated statistical technique for determining the number of principal
components to keep. The method determines the number
of principal components to retain by the intersection point
of two curves representing the cumulative eigenvalues of the
traﬃc matrix and an equivalently-sized random matrix. The
intuition behind this method is to only include principal
components that contribute more variance than a random
vector would (i.e., those before the intersection point). For
our purposes, a more eﬀective metric is to compare where
the respective scree plots intersect.
Figure 5(c) plots the
scree plots for a traﬃc matrix from our study in addition
to an equivalently-sized random matrix. It should not be
surprising that the scree plot for the random matrix (i.e.,
“Humphrey-Ilgen variant”) is nearly horizontal, given that
every principal component of a random matrix is expected
to capture the same variance. The scree plot for the traﬃc
matrix appears to have a knee at k = 2 but Humphrey-Ilgen
retains far more principal components than this (i.e., the two
curves do not intersect anywhere in plotted interval).
We are therefore left with no reliable technique for tuning
the topk parameter3. Cattell’s Scree Test performs the best
in that it is often within one or two principal components of
the operating point that minimizes the false-positive rate,
but we’ve demonstrated that PCA’s false-positive rate can
be very sensitive even within such a small range. While our
results indicate that there does appear to be small ranges
of topk values that perform better than others, there are
fundamental problems with even the concept of the topk parameter that limit the potential success of any such scheme
for determining which principal components to include in
3We also evaluated Kaiser’s Criterion , which is another
automated technique for determining topk, but omit the result because it performed even poorer than Humphrey-Ilgen
the normal subspace. We will discuss this intrinsic limitation in section 5.1.
The Detection Threshold
The threshold parameter speciﬁes how statistically signiﬁcant a given outlier must be for a PCA-based traﬃc anomaly
detector to report it. Therefore the total number of detections will always decrease monotonically as a function of the
threshold. The false-positive rate, while generally decreasing as a function of the threshold, need not always decrease.
The reason for this is that one may cease to detect true positives (that are less statistically signiﬁcant) before ceasing
to detect false positives, as can be observed in ﬁgure 6(c).
Figure 6 provides further support for the conclusion that
the relative properties of traﬃc aggregation formalisms appear to hold across networks.
That is, for a given topk
value but across all thresholds and both networks, the input
links aggregation generally detects more potential anomalies than OD-ﬂows, which detects more potential anomalies
than ingress routers (see ﬁgures 6(b) and 6(d)). We believe
this to be the case because the input links aggregation tends
to produce less multiplexed data than OD-ﬂows, which in
turn produces less multiplexed than the ingress router aggregation.
For both Abilene in ﬁgure 6(c) and Geant in
ﬁgure 6(a), one can conclude that the input links aggregation has a higher false-positive rate than each of the others.
Finally, ﬁgures 6(c) and 6(d) reinforce the perception that
OD ﬂows is probably the aggregation that achieves the best
balance between false-positive rate and total number of detections.
Our results indicate that the threshold provides operators
with an intuitive knob to trade oﬀthe false-positive rate and
total number of detections.
INTRINSIC LIMITATIONS OF PCA
In this section, we highlight two key limitations of PCA
that limit its eﬀectiveness as a traﬃc anomaly detector. Section 5.1 illustrates how a suﬃciently large anomaly may inadvertently pollute PCA’s deﬁnition of normal traﬃc, and
section 5.2 examines the diﬃculty of identifying the set of
ﬂows responsible for a statistical anomaly.
Contamination of the Normal Subspace
Using PCA to detect traﬃc anomalies relies on the assumption that the top few principal components represent
the normal traﬃc, and the anomalies lie in the remaining
components.
However, in some cases, a suﬃciently large
anomaly may introduce so much variance in the traﬃc matrix that it is included in one of the ﬁrst few principal components, thereby contaminating PCA’s deﬁnition of normality.
Our analysis in the previous section intentionally avoided
time periods with dramatic network events to avoid unduly
degrading PCA’s false-positive rates4. Therefore, to demonstrate the eﬀects of polluting the normal subspace, we analyze a separate (unlabeled) trace for the Geant network
between November 12-20, 2005. Since we have not classiﬁed
all of the detected anomalies during this period, we cannot
produce false-positive rates for this trace, though we expect
the erroneous deﬁnition of “normal” traﬃc would increase
the false-positive rate.
4However, ﬁgure 5(b) shows that even moderately sized
events can contaminate the very ﬁrst principal component
detection threshold
percent false positives
ingress routers (topk=3)
input links (topk=2)
(a) Geant false-positive rate
detection threshold
total detections
ingress routers (topk=3)
input links (topk=2)
(b) Geant total detections
detection threshold
percent false positives
ingress routers (topk=2)
OD flows (topk=5)
input links (topk=2)
(c) Abilene false-positive rate
detection threshold
total detections
ingress routers (topk=2)
OD flows (topk=5)
input links (topk=2)
(d) Abilene total detections
Impact of detection threshold on falsepositive rate and total detections
November 15th 2005
(a) Geant network-wide traﬃc
November 15th 2005
cumulative # detections
(b) Geant ingress routers
November 15th 2005
cumulative # detections
(c) Geant input links
Figure 7: A large outage is included in PCA’s model of normalcy and hence goes largely undetected by PCA
Figure 7(a) plots the aggregate traﬃc on the Geant network for eighteen hours on November 15, 2005. The plot
shows several clear outages that caused a signiﬁcant drop
in the aggregate traﬃc; in fact, only two routers carried
any traﬃc at all during these ﬁfteen-minute windows. Figures 7(b) and 7(c) plot the cumulative detections by PCA
over this 18-hour period for the ingress-router and input-link
aggregations, respectively. While ﬁgure 7(b) has three spikes
in the number of detections that coincide with spikes seen
in 7(a), the spikes in 7(b) correspond to only a small fraction of the total number of ingress routers. Although Geant
has 23 ingress routers, only eight detections are made during the entire 18-hour period. The input-link aggregation in
ﬁgure 7(c) fares even worse in that the only visible spike is
correlated with the ﬁrst drastic network event seen in ﬁgure 7(a). In addition, the spike in ﬁgure 7(c) corresponds
to less than 13% of all links, whereas the outage at that
moment caused 75 out of 77 input links to carry 0 packets.
When a large network event contaminates the normal subspace, PCA may not detect the anomaly, and its inclusion
in the normal subspace may yield false positives or false
negatives for other traﬃc. Our results suggest that it is important to preprocess the data to identify and remove large
anomalies before constructing the normal subspace. presented a potential technique for identifying when such large
outliers are included in the normal subspace, but no techniques have been evaluated for subsequently smoothing the
normal subspace (to our knowledge).
Even if smoothing
techniques could be identiﬁed (e.g.
exponential weighted
moving average, EWMA), their applicability could potentially be limited to the large-scale anomalies seen in ﬁgure 7(a). Medium-sized anomalies would be more problematic, as they might easily evade a coarse-grained ﬁltering
scheme and still unwittingly pollute the normal subspace.
Identifying the Anomalous Flows
PCA detects anomalous time bins, not anomalous ﬂows.
That is, PCA reports an anomaly when a measurement vector ⃗vi is expressed primarily by the anomalous subspace.
However, PCA provides no direct mapping between these
subspaces and the original ﬂows, which makes it diﬃcult to
identify the ﬂow(s) responsible for the anomaly. Previous
studies have applied a heuristic that associates
an anomaly with the r ﬂows with the largest contribution to
⃗vi, such that the r ﬂows are big enough to account for the
spike in the anomalous subspace. Unfortunately there is no
a priori reason for why the r ﬂows with the highest entropy
value at time i must necessarily correspond to the ﬂows that
caused PCA to detect an anomaly. In fact, this heuristic can
unduly trigger alarms in some ﬂows much more frequently
than others.
To illustrate this “heavy hitter” phenomenon, ﬁgure 8
plots the CDF of the percentage of the anomalies that are
attributed to the various ﬂows, where we ranked the ﬂows in
order of how many anomalies the heuristic associates with
them. For example, in ﬁgure 8(a), a single ingress router is
associated with 70% of all PCA alarms on the Geant network. Each of the other aggregations show similar types of
skewed ﬂow-identiﬁcation distributions. For example, 29%
of OD-ﬂows in the Abilene network did not contribute to
a single alarm during the studied week.
Although we do
not necessarily expect a uniform distribution, the skew in
the ﬁve graphs is a natural consequence of a heuristic that
ranks ﬂows in order of their entropy values.
Mapping an anomalous time bin to one or more responsible ﬂows is inherently challenging, since PCA operates on
aggregated measurement data and remaps the data to another subspace. Moreover, the inaccuracies of the previously
employed heuristic very likely increased the false-positive
rates reported in our study. That is, the PCA technique
itself may have identiﬁed a legitimate anomalous time-bin,
but it was identiﬁed as a false-positive because the heuristic
associated this anomaly with an incorrect ﬂow. We therefore believe that creating more eﬀective heuristics is a very
important avenue for future work. For example, it may be
better to identify the r ﬂows that exhibit the greatest variance along the anomalous subspace around the time of detection. While still only a heuristic, with associated shortcomings, this approach may more closely capture the notion
of a sudden anomalous event.
The diﬃculty of identifying the anomalous ﬂows is a fundamental problem of PCA,
which begs the question of whether other anomaly-detection
techniques (i.e., that operate on the raw data, rather than
an aggregated and transformed variant of the data) are more
appropriate for applications where network operators need
to pinpoint the location(s) of an anomaly.
RELATED WORK
Lakhina et al. popularized using PCA for traﬃc anomaly
detection in . The work showed that traﬃc
traces have low intrinsic dimensionality, that PCA can de-
topk = 3 and threshold = 90%
cumulative percent detections
(a) Geant ingress routers
topk = 4 and threshold = 90%
cumulative percent detections
(b) Geant input links
topk = 2 and threshold = 90%
cumulative percent detections
(c) Abilene ingress routers
topk = 4 and threshold = 90%
cumulative percent detections
(d) Abilene OD ﬂows
topk = 4 and threshold = 90%
cumulative percent detections
(e) Abilene input links
Figure 8: The heavy-hitter ﬂow phenomenon
tect network-wide anomalies when analyzing the OD ﬂows
aggregation, and can detect a wide variety of types of anomalies when analyzing entropy timeseries of IP header features.
PCA has also recently been combined with sketches 
and distributed monitors to provide more eﬃcient traf-
ﬁc anomaly detection. This entire body of work used the
same dataset, however, for which Lakhina’s PCA code was
highly optimized.
In , PCA was one of many algorithms evaluated in a
general system that aimed to infer network-level anomalies
from available data aggregates. PCA has also been used to
correlate BGP updates with underlying network events such
as link failures, resets, etc . Other statistical methods
that have been used for traﬃc anomaly detection include
Kalman ﬁlters , wavelets , among others. Other inherent limitations of PCA have also been discussed in the
statistics literature .
CONCLUSION
Previous work has shown that PCA can detect real anomalies, but our work demonstrates that the challenges to using PCA as a traﬃc anomaly detector have been understated and current methods for tuning PCA are inadequate.
Lakhina et al.
were able to achieve such promising early
results because of their great familiarity with both the technique and the data. Subsequent PCA work in this lineage
used the same software, heuristics, and labeled data, which
understandably yielded equally strong results by utilizing
already highly optimized parameter-settings for the given
circumstance.
Starting with new data sets and exploring a range of parameter settings, we show that selecting the appropriate
value for topk is surprisingly diﬃcult; small changes in topk
in either direction can have a signiﬁcant inﬂuence on the
false-positive rate. In addition, existing techniques for selecting topk are inadequate. In fact, we’ve shown that topk is
a ﬂawed concept in and of itself because the ﬁrst few principal components need not capture a vast majority of the
variance in a traﬃc trace, nor are they necessarily periodic.
The normal subspace may in fact become polluted by large
anomalies, which degrades the eﬀectiveness of PCA. We also
demonstrated that identifying the ﬂow that caused a PCA
detection is a fundamentally hard problem. We showed that
the previously employed heuristic could fail in many circumstances, and may have the inadvertent side-eﬀect of associating the majority of detections with a small set of ﬂows.
Our study suggests that using PCA for traﬃc anomaly
detection is much more diﬃcult than it appears.
PCA can be used for automated, unsupervised detection
of anomalous traﬃc, we need more eﬀective techniques for
determining the dimensionality of the normal subspace, preventing its contamination, and identifying ﬂows responsible
for a given PCA detection. In our ongoing work, we are also
investigating other statistical techniques that may be able
to detect and identify anomalous traﬃc in a more robust
ACKNOWLEDGMENTS
The authors would like to thank Mark Crovella and Anukool
Lakhina for invaluable advice and feedback on applying PCA
to traﬃc anomaly detection.
The ﬁrst author would further like to thank advisor Kai Li for continued support,
and David Gardner for providing housing in Paris where
the work was performed.