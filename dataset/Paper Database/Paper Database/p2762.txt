This is a repository copy of A survey of outlier detection methodologies.
White Rose Research Online URL for this paper:
 
Hodge, V.J. orcid.org/0000-0002-2469-0224 and Austin, J. orcid.org/0000-0001-5762-8614
 A survey of outlier detection methodologies. Artificial Intelligence Review. pp. 85-
126. ISSN 1573-7462
 
 
 
Items deposited in White Rose Research Online are protected by copyright, with all rights reserved unless
indicated otherwise. They may be downloaded and/or printed for private study, or other acts as permitted by
national copyright laws. The publisher or other rights holders may allow further reproduction and re-use of
the full text version. This is indicated by the licence information on the White Rose Research Online record
for the item.
If you consider content in White Rose Research Online to be in breach of UK law, please notify us by
emailing including the URL of the record and the reason for the withdrawal request.
White Rose Consortium ePrints Repository
 
This is an author produced version of a paper published in Artificial Intelligence Review.
This paper has been peer-reviewed but does not include the final publisher proof-corrections or
journal pagination.
White Rose Repository URL for this paper:
 
Citation for the published paper
Hodge, V.J. and Austin, J. A survey of outlier detection methodologies. Artificial
Intelligence Review, 22 (2). pp. 85-126.
Citation for this paper
Hodge, V.J. and Austin, J. A survey of outlier detection methodologies. Artificial
Intelligence Review, 22 (2). pp. 85-126.
To refer to the repository paper, the following format may be used:
Hodge, V.J. and Austin, J. A survey of outlier detection methodologies. Author
manuscript available at: 
[Accessed: date].
 
Hodge, V.J. and Austin, J. A survey of outlier detection methodologies. Artificial
Intelligence Review, 22 (2). pp. 85-126.
White Rose Consortium ePrints Repository
 
A Survey of Outlier Detection Methodologies.
Victoria J. Hodge ( )∗and Jim Austin
( )
Dept. of Computer Science,
University of York,
York, YO10 5DD
tel: +44 1904 433067
fax: +44 1904 432767
Abstract. Outlier detection has been used for centuries to detect and, where appropriate, remove anomalous observations from data. Outliers arise due to mechanical
faults, changes in system behaviour, fraudulent behaviour, human error, instrument
error or simply through natural deviations in populations. Their detection can
identify system faults and fraud before they escalate with potentially catastrophic
consequences. It can identify errors and remove their contaminating eﬀect on the
data set and as such to purify the data for processing. The original outlier detection
methods were arbitrary but now, principled and systematic techniques are used,
drawn from the full gamut of Computer Science and Statistics. In this paper, we
introduce a survey of contemporary techniques for outlier detection. We identify
their respective motivations and distinguish their advantages and disadvantages in
a comparative review.
Keywords: Outlier, Novelty, Anomaly, Noise, Deviation, Detection, Recognition
Introduction
Outlier detection encompasses aspects of a broad spectrum of techniques. Many techniques employed for detecting outliers are fundamentally identical but with diﬀerent names chosen by the authors. For
example, authors describe their various approaches as outlier detection, novelty detection, anomaly detection, noise detection, deviation
detection or exception mining. In this paper, we have chosen to call
the technique outlier detection although we also use novelty detection where we feel appropriate but we incorporate approaches from
all ﬁve categories named above. Additionally, authors have proposed
many deﬁnitions for an outlier with seemingly no universally accepted
deﬁnition. We will take the deﬁnition of Grubbs and
quoted in Barnett & Lewis :
An outlying observation, or outlier, is one that appears to deviate
markedly from other members of the sample in which it occurs.
∗This work was supported by EPSRC Grant No. GR/R55191/01
c⃝2004 Kluwer Academic Publishers. Printed in the Netherlands.
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.1
A further outlier deﬁnition from Barnett & Lewis (Barnett and Lewis,
An observation (or subset of observations) which appears to be
inconsistent with the remainder of that set of data.
In ﬁgure 2, there are ﬁve outlier points labelled V, W, X, Y and Z
which are clearly isolated and inconsistent with the main cluster of
points. The data in the ﬁgures in this survey paper is adapted from the
Wine data set .
John states that an outlier may also be “surprising veridical data”, a point belonging to class A but actually situated inside class
B so the true (veridical) classiﬁcation of the point is surprising to the
observer. Aggarwal notes that outliers may
be considered as noise points lying outside a set of deﬁned clusters or
alternatively outliers may be deﬁned as the points that lie outside of
the set of clusters but are also separated from the noise. These outliers
behave diﬀerently from the norm. In this paper, we focus on the two
deﬁnitions quoted from above and do not
consider the dual class-membership problem or separating noise and
Outlier detection is a critical task in many safety critical environments
as the outlier indicates abnormal running conditions from which signiﬁcant performance degradation may well result, such as an aircraft
engine rotation defect or a ﬂow problem in a pipeline. An outlier can
denote an anomalous object in an image such as a land mine. An outlier
may pinpoint an intruder inside a system with malicious intentions so
rapid detection is essential. Outlier detection can detect a fault on a
factory production line by constantly monitoring speciﬁc features of
the products and comparing the real-time data with either the features
of normal products or those for faults. It is imperative in tasks such
as credit card usage monitoring or mobile phone monitoring to detect
a sudden change in the usage pattern which may indicate fraudulent
usage such as stolen card or stolen phone airtime. Outlier detection
accomplishes this by analysing and comparing the time series of usage
statistics. For application processing, such as loan application processing or social security beneﬁt payments, an outlier detection system can
detect any anomalies in the application before approval or payment.
Outlier detection can additionally monitor the circumstances of a beneﬁt claimant over time to ensure the payment has not slipped into
fraud. Equity or commodity traders can use outlier detection methods
to monitor individual shares or markets and detect novel trends which
may indicate buying or selling opportunities. A news delivery system
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.2
can detect changing news stories and ensure the supplier is ﬁrst with the
breaking news. In a database, outliers may indicate fraudulent cases or
they may just denote an error by the entry clerk or a misinterpretation
of a missing value code, either way detection of the anomaly is vital for
data base consistency and integrity.
A more exhaustive list of applications that utilise outlier detection is:
Fraud detection - detecting fraudulent applications for credit cards,
state beneﬁts or detecting fraudulent usage of credit cards or mobile phones.
Loan application processing - to detect fraudulent applications or
potentially problematical customers.
Intrusion detection - detecting unauthorised access in computer
Activity monitoring - detecting mobile phone fraud by monitoring
phone activity or suspicious trades in the equity markets.
Network performance - monitoring the performance of computer
networks, for example to detect network bottlenecks.
Fault diagnosis - monitoring processes to detect faults in motors,
generators, pipelines or space instruments on space shuttles for
Structural defect detection - monitoring manufacturing lines to
detect faulty production runs for example cracked beams.
Satellite image analysis - identifying novel features or misclassiﬁed
Detecting novelties in images - for robot neotaxis or surveillance
Motion segmentation - detecting image features moving independently of the background.
Time-series monitoring - monitoring safety critical applications
such as drilling or high-speed milling.
Medical condition monitoring - such as heart-rate monitors.
Pharmaceutical research - identifying novel molecular structures.
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.3
Detecting novelty in text - to detect the onset of news stories, for
topic detection and tracking or for traders to pinpoint equity, commodities, FX trading stories, outperforming or under performing
commodities.
Detecting unexpected entries in databases - for data mining to
detect errors, frauds or valid but unexpected entries.
Detecting mislabelled data in a training data set.
Outliers arise because of human error, instrument error, natural deviations in populations, fraudulent behaviour, changes in behaviour of
systems or faults in systems. How the outlier detection system deals
with the outlier depends on the application area. If the outlier indicates
a typographical error by an entry clerk then the entry clerk can be
notiﬁed and simply correct the error so the outlier will be restored
to a normal record. An outlier resulting from an instrument reading
error can simply be expunged. A survey of human population features
may include anomalies such as a handful of very tall people. Here the
anomaly is purely natural, although the reading may be worth ﬂagging
for veriﬁcation to ensure no errors, it should be included in the classiﬁcation once it is veriﬁed. A system should use a classiﬁcation algorithm
that is robust to outliers to model data with naturally occurring outlier
points. An outlier in a safety critical environment, a fraud detection system, an image analysis system or an intrusion monitoring system must
be detected immediately (in real-time) and a suitable alarm sounded
to alert the system administrator to the problem. Once the situation
has been handled, this anomalous reading may be stored separately
for comparison with any new fraud cases but would probably not be
stored with the main system data as these techniques tend to model
normality and use this to detect anomalies.
There are three fundamental approaches to the problem of outlier
detection:
1. Type 1 - Determine the outliers with no prior knowledge of the
data. This is essentially a learning approach analogous to unsupervised clustering. The approach processes the data as a static
distribution, pinpoints the most remote points, and ﬂags them as
potential outliers. Type 1 assumes that errors or faults are separated from the ‘normal’ data and will thus appear as outliers. In
ﬁgure 3, points V, W, X, Y and Z are the remote points separated
from the main cluster and would be ﬂagged as possible outliers. We
note that the main cluster may be subdivided if necessary into more
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.4
than one cluster to allow both classiﬁcation and outlier detection
as with ﬁgure 4. The approach is predominantly retrospective and
is analogous to a batch-processing system. It requires that all data
be available before processing and that the data is static. However,
once the system possesses a suﬃciently large database with good
coverage, then it can compare new items with the existing data.
There are two sub-techniques commonly employed, diagnosis and
accommodation . An outlier diagnostic
approach highlights the potential outlying points. Once detected,
the system may remove these outlier points from future processing
of the data distribution. Many diagnostic approaches iteratively
prune the outliers and ﬁt their system model to the remaining data
until no more outliers are detected.
An alternative methodology is accommodation that incorporates
the outliers into the distribution model generated and employs a robust classiﬁcation method. These robust approaches can withstand
outliers in the data and generally induce a boundary of normality around the majority of the data which thus represents normal
behaviour. In contrast, non-robust classiﬁer methods produce representations which are skewed when outliers are left in. Non-robust
methods are best suited when there are only a few outliers in the
data set (as in ﬁgure 3) as they are computationally cheaper than
the robust methods but a robust method must be used if there are a
large number of outliers to prevent this distortion. Torr & Murray
 use a cheap Least Squares algorithm if
there are only a few outliers but switch to a more expensive but
robust algorithm for higher frequencies of outliers.
2. Type 2 - Model both normality and abnormality. This approach is
analogous to supervised classiﬁcation and requires pre-labelled
data, tagged as normal or abnormal. In ﬁgure 4, there are three
classes of normal data with pre-labelled outliers in isolated areas.
The entire area outside the normal class represents the outlier class.
The normal points could be classiﬁed as a single class or subdivided
into the three distinct classes according to the requirements of the
system to provide a simple normal/abnormal classiﬁcation or to
provide an abnormal and 3-classes of normally classiﬁer.
Classiﬁers are best suited to static data as the classiﬁcation needs
to be rebuilt from ﬁrst principles if the data distribution shifts
unless the system uses an incremental classiﬁer such as an evolutionary neural network. We describe one such approach later which
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.5
uses a Grow When Required network. A type 2
approach can be used for on-line classiﬁcation, where the classiﬁer
learns the classiﬁcation model and then classiﬁes new exemplars as
and when required against the learned model. If the new exemplar
lies in a region of normality it is classiﬁed as normal, otherwise
it is ﬂagged as an outlier. Classiﬁcation algorithms require a good
spread of both normal and abnormal data, i.e., the data should
cover the entire distribution to allow generalisation by the classiﬁer.
New exemplars may then be classiﬁed correctly as classiﬁcation is
limited to a ‘known’ distribution and a new exemplar derived from
a previously unseen region of the distribution may not be classiﬁed
correctly unless the generalisation capabilities of the underlying
classiﬁcation algorithm are good.
3. Type 3 - Model only normality or in a very few cases model abnormality , .
Authors generally name this technique novelty detection or novelty
recognition. It is analogous to a semi-supervised recognition
or detection task and can be considered semi-supervised as the
normal class is taught but the algorithm learns to recognise abnormality. The approach needs pre-classiﬁed data but only learns data
marked normal. It is suitable for static or dynamic data as it only
learns one class which provides the model of normality. It can learn
the model incrementally as new data arrives, tuning the model to
improve the ﬁt as each new exemplar becomes available. It aims to
deﬁne a boundary of normality.
A type 3 system recognises a new exemplar as normal if it lies
within the boundary and recognises the new exemplar as novel
otherwise. In ﬁgure 5, the novelty recogniser has learned the same
data as shown in ﬁgure 2 but only the normal class is learned and
a boundary of normality induced. If points V, W, X, Y and Z from
ﬁgure 2 are compared to the novelty recogniser they will be labelled
as abnormal as they lie outside the induced boundary. This boundary may be hard where a point lies wholly within or wholly outside
the boundary or soft where the boundary is graduated depending
on the underlying detection algorithm. A soft bounded algorithm
can estimate the degree of “outlierness”.
It requires the full gamut of normality to be available for training
to permit generalisation. However, it requires no abnormal data for
training unlike type 2. Abnormal data is often diﬃcult to obtain or
expensive in many fault detection domains such as aircraft engine
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.6
monitoring. It would be extremely costly to sabotage an aircraft
engine just to obtain some abnormal running data. Another problem with type 2 is it cannot always handle outliers from unexpected
regions, for example, in fraud detection a new method of fraud never
previously encountered or previously unseen fault in a machine may
not be handled correctly by the classiﬁer unless generalisation is
very good. In this method, as long as the new fraud lies outside
the boundary of normality then the system will be correctly detect
the fraud. If normality shifts then the normal class modelled by the
system may be shifted by re-learning the data model or shifting
the model if the underlying modelling technique permits such as
evolutionary neural networks.
The outlier approaches described in this survey paper generally map
data onto vectors. The vectors comprise numeric and symbolic attributes to represent continuous-valued, discrete (ordinal), categorical
(unordered numeric), ordered symbolic or unordered symbolic data.
The vectors may be monotype or multi-type. The statistical and neural network approaches typically require numeric monotype attributes
and need to map symbolic data onto suitable numeric values1 but the
machine learning techniques described are able to accommodate multitype vectors and symbolic attributes. The outliers are determined from
the ‘closeness’ of vectors using some suitable distance metric. Diﬀerent
approaches work better for diﬀerent types of data, for diﬀerent numbers
of vectors, for diﬀerent numbers of attributes, according to the speed
required and according to the accuracy required. The two fundamental considerations when selecting an appropriate methodology for an
outlier detection system are:
selecting an algorithm which can accurately model the data distribution and accurately highlight outlying points for a clustering,
classiﬁcation or recognition type technique. The algorithm should
also be scalable to the data sets to be processed.
selecting a suitable neighbourhood of interest for an outlier. The
selection of the neighbourhood of interest is non-trivial. Many
algorithms deﬁne boundaries around normality during processing
and autonomously induce a threshold. However, these approaches
are often parametric enforcing a speciﬁc distribution model or require user-speciﬁed parameters such as the number of clusters.
Other techniques discussed below require user-deﬁned parameters to deﬁne the size or density of neighbourhoods for outlier
1 There are statistical models for symbolic data (such as log-linear models) but
these are not generally used for outlier analysis
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.7
thresholding. The choice of neighbourhood whether user-deﬁned
or autonomously induced needs to be applicable for all density
distributions likely to be encountered and can potentially include
those with sharp density variations.
In the remainder of this paper, we categorise and analyse broad range of
outlier detection methodologies. We pinpoint how each handles outliers
and make recommendations for when each methodology is appropriate
for clustering, classiﬁcation and/or recognition. Barnett & Lewis and Rousseeuw & Leroy describe and analyse a broad range of statistical outlier techniques and Marsland analyses a wide range of neural
methods. We have observed that outlier detection methods are derived
from three ﬁelds of computing: statistics (proximity-based, parametric, non-parametric and semi-parametric), neural networks (supervised
and unsupervised) and machine learning. In the next 4 sections, we
describe and analyse techniques from all three ﬁelds and a collection of hybrid techniques that utilise algorithms from multiple ﬁelds.
The approaches described here encompass distance-based, set-based,
density-based, depth-based, model-based and graph-based algorithms.
Statistical models
Statistical approaches were the earliest algorithms used for outlier detection. Some of the earliest are applicable only for single dimensional
data sets. In fact, many of the techniques described in both and are single dimensional or at best univariate. One such single dimensional method is
Grubbs’ method (Extreme Studentized Deviate) which
calculates a Z value as the diﬀerence between the mean value for the
attribute and the query value divided by the standard deviation for the
attribute where the mean and standard deviation are calculated from
all attribute values including the query value. The Z value for the query
is compared with a 1% or 5% signiﬁcance level. The technique requires
no user parameters as all parameters are derived directly from data.
However, the technique is susceptible to the number of exemplars in
the data set. The higher the number of records the more statistically
representative the sample is likely to be.
Statistical models are generally suited to quantitative real-valued data
sets or at the very least quantitative ordinal data distributions where
the ordinal data can be transformed to suitable numerical values for
statistical (numerical) processing. This limits their applicability and
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.8
increases the processing time if complex data transformations are necessary before processing.
Probably one of the simplest statistical outlier detection techniques described here, Laurikkala et al. use informal box
plots to pinpoint outliers in both univariate and multivariate data sets.
This produces a graphical representation (see ﬁgure 1 for an example
box plot) and allows a human auditor to visually pinpoint the outlying
points. It is analogous to a visual inspection of ﬁgure 2. Their approach
can handle real-valued, ordinal and categorical (no order) attributes.
Box plots plot the lower extreme, lower quartile, median, upper quartile
and upper extreme points. For univariate data, this is a simple 5-point
plot, as in ﬁgure 1. The outliers are the points beyond the lower and
upper extreme values of the box plot, such as V, Y and Z in ﬁgure 1.
Laurikkala et al. suggest a heuristic of 1.5×inter-quartile range beyond
the upper and lower extremes for outliers but this would need to vary
across diﬀerent data sets. For multivariate data sets the authors note
that there are no unambiguous total orderings but recommend using the
reduced sub-ordering based on the generalised distance metric using the
Mahalanobis distance measure (see equation 2). The Mahalanobis distance measure includes the inter-attribute dependencies so the system
can compare attribute combinations. The authors found the approach
most accurate for multivariate data where a panel of experts agreed
with the outliers detected by the system. For univariate data, outliers
are more subjective and may be naturally occurring, for example the
heights of adult humans, so there was generally more disagreement.
Box plots make no assumptions about the data distribution model but
are reliant on a human to note the extreme points plotted on the box
Statistical models use diﬀerent approaches to overcome the problem
of increasing dimensionality which both increases the processing time
and distorts the data distribution by spreading the convex hull. Some
methods preselect key exemplars to reduce the processing time , . As the dimensionality increases, the
data points are spread through a larger volume and become less dense.
This makes the convex hull harder to discern and is known as the
“Curse of Dimensionality”. The most eﬃcient statistical techniques
automatically focus on the salient attributes and are able to process
the higher number of dimensions in tractable time. However, many
techniques such as k-NN, neural networks, Minimum Volume Ellipsoid
or Convex Peeling described in this survey are susceptible to the Curse
of Dimensionality. These approaches may utilise a preprocessing al-
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.9
gorithm to preselect the salient attributes ,
 , . These feature selection
techniques essentially remove noise from the data distribution and focus
the main cluster of normal data points while isolating the outliers as
with ﬁgure 2. Only a few attributes usually contribute to the deviation
of an outlier case from a normal case. An alternative technique is to
use an algorithm to project the data onto a lower dimensional subspace
to compact the convex hull or use Principal
Component Analysis , .
2.1. Proximity-based Techniques
Proximity-based techniques are simple to implement and make no prior
assumptions about the data distribution model. They are suitable for
both type 1 and type 2 outlier detection. However, they suﬀer exponential computational growth as they are founded on the calculation
of the distances between all records. The computational complexity is
directly proportional to both the dimensionality of the data m and the
number of records n. Hence, methods such as k-nearest neighbour (also
known as instance-based learning and described next) with O(n2m)
runtime are not feasible for high dimensionality data sets unless the
running time can be improved. There are various ﬂavours of k-Nearest
Neighbour (k-NN) algorithm for outlier detection but all calculate the
nearest neighbours of a record using a suitable distance calculation
metric such as Euclidean distance or Mahalanobis distance. Euclidean
distance is given by equation 1
and is simply the vector distance whereas the Mahalanobis distance
given by equation 2
(x −µ)T C−1(x −µ)
calculates the distance from a point to the centroid (µ) deﬁned by
correlated attributes given by the Covariance matrix (C). Mahalanobis
distance is computationally expensive to calculate for large high dimensional data sets compared to the Euclidean distance as it requires
a pass through the entire data set to identify the attribute correlations.
Ramaswamy et al. introduce an optimised k-
NN to produce a ranked list of potential outliers. A point p is an outlier
if no more than n −1 other points in the data set have a higher Dm
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.10
(distance to mth neighbour) where m is a user-speciﬁed parameter. In
ﬁgure 3, V is most isolated followed by X, W, Y then Z so the outlier
rank would be V, X, W, Y, Z. This approach is susceptible to the
computational growth as the entire distance matrix must be calculated
for all points (ALL k-NN ) so Ramaswamy et al. include techniques for
speeding the k-NN algorithm such as partitioning the data into cells.
If any cell and its directly adjacent neighbours contains more than k
points, then the points in the cell are deemed to lie in a dense area of the
distribution so the points contained are unlikely to be outliers. If the
number of points lying in cells more than a pre-speciﬁed distance apart
is less than k then all points in the cell are labelled as outliers. Hence,
only a small number of cells not previously labelled need to be processed
and only a relatively small number of distances need to be calculated
for outlier detection. Authors have also improved the running speed of
k-NN by creating an eﬃcient index using a computationally eﬃcient
indexing structure with linear running time.
Knorr & Ng introduce an eﬃcient type 1 k-
NN approach. If m of the k nearest neighbours (where m < k) lie
within a speciﬁc distance threshold d then the exemplar is deemed to
lie in a suﬃciently dense region of the data distribution to be classiﬁed
as normal. However, if there are less than m neighbours inside the
distance threshold then the exemplar is an outlier. A very similar type 1
approach for identifying land mines from satellite ground images is to take the m-th neighbour and ﬁnd the distance
Dm. If this distance is less than a threshold d then the exemplar lies in
a suﬃciently dense region of the data distribution and is classiﬁed as
normal. However, if the distance is more than the threshold value then
the exemplar must lie in a locally sparse area and is an outlier. This
has reduced the number of data-speciﬁc parameters from Knorr & Ng’s
 approach by one as we now have d and m but
no k value. In ﬁgure 3, points V, W, X, Y and Z are relatively distant
from their neighbours and will have fewer than m neighbours within d
and a high Dm so both Knorr & Ng and Byers & Raftery classify them
as outliers. This is less susceptible to the computational growth than
the ALL k-NN approach as only the k nearest neighbours need to be
calculated for a new exemplar rather than the entire distance matrix
for all points.
A type 2 classiﬁcation k-NN method such as the majority voting approach requires a labelled data set with both
normal and abnormal vectors classiﬁed. The k-nearest neighbours for
the new exemplar are calculated and it is classiﬁed according to the
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.11
majority classiﬁcation of the nearest neighbours .
An extension incorporates the distance where the voting power of each
nearest neighbour is attenuated according to its distance from the
new item with the voting power systematically
decreasing as the distance increases. Tang et al. 
introduce an type 1 outlier diagnostic which uniﬁes weighted k-NN with
a connectivity-based approach and calculates a weighted distance score
rather than a weighted classiﬁcation. It calculates the average chaining
distance (path length) between a point p and its k neighbours. The
early distances are assigned higher weights so if a point lies in a sparse
region as points V, W, X, Y and Z in ﬁgure 4 its nearest neighbours
in the path will be relatively distant and the average chaining distance
will be high. In contrast, Wettschereck requires a data set with good
coverage for both normal and abnormal points. The distribution in
ﬁgure 4 would cause problems for voted k-NN as there are relatively
few examples of outliers and their nearest neighbours, although distant,
will in fact be normal points so they are classiﬁed as normal. Tang’s
underlying principle is to assimilate both density and isolation. A point
can lie in a relatively sparse region of a distribution without being an
outlier but a point in isolation is an outlier. However, the technique
is computationally complex with a similar expected run-time to a full
k-NN matrix calculation as it relies on calculating paths between all
points and their k neighbours. Wettschereck’s approach is less susceptible as only the k nearest neighbours are calculated relative to the single
Another technique for optimising k-NN is reducing the number of features. These feature subset selectors are applicable for most systems
described in this survey, not just the k-NN techniques. Aha & Bankert
 describe a forward sequential selection feature
selector which iteratively evaluates feature subsets adding one extra
dimension per iteration until the extra dimension entails no improvement in performance. The feature selector demonstrates high accuracy
when coupled with an instance-based classiﬁer (nearest-neighbour) but
is computationally expensive due to the combinatorial problem of subset selection. Aggarwal & Yu employ lower
dimensional projections of the data set and focus on key attributes.
The method assumes that outliers are abnormally sparse in certain
lower dimensional projections where the combination of attributes in
the projection correlates to the attributes that are deviant. Aggarwal
& Yu use an evolutionary search algorithm to determine the projections which has a faster running time than the conventional approach
employed by Aha & Bankert.
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.12
Proximity based methods are also computationally susceptible to the
number of instances in the data set as they necessitate the calculation
of all vector distances. Datta & Kibler use
a diagnostic prototype selection to reduce the storage requirements
to a few seminal vectors. Skalak employs both feature
selection and prototype selection where a large data set can be stored as
a few lower dimensional prototype vectors. Noise and outliers will not
be stored as prototypes so the method is robust. Limiting the number of
prototypes prevents over-ﬁtting just as pruning a decision tree can prevent over-ﬁtting by limiting the number of leaves (prototypes) stored.
However, prototyping must be applied carefully and selectively as it will
increase the sparsity of the distribution and the density of the nearest
neighbours. A majority voting k-NN technique such as will be less aﬀected but an approach relying on the distances to
the nth neighbour or counting the number
of neighbours within speciﬁc distances will be
strongly aﬀected.
Prototyping is in many ways similar to the k-means and k-medoids
described next but with prototyping a new instance is compared to the
prototypes using conventional k-nearest neighbour whereas k-means
and k-medoids prototypes have a kernel with a locally deﬁned radius
and the new instance is compared with the kernel boundaries. A prototype approach is also applicable for reducing the data set for neural
networks and decision trees. This is particularly germane for nodebased neural networks which require multiple passes through the data
set to train so a reduced data set will entail less training steps.
Dragon Research and Nairac 
(discussed in section 5) use k-means for novelty detection. Dragon
perform on-line event detection to identify news stories concerning
new events. Each of the k clusters provides a local model of the data.
The algorithm represents each of the k clusters by a prototype vector
with attribute values equivalent to the mean value across all points in
that cluster. In ﬁgure 4, if k is 3 then the algorithm would eﬀectively
circumscribe each class (1, 2 and 3) with a hyper-sphere and these
3 hyper-spheres eﬀectively represent and classify normality. K-means
requires the user to specify the value of k in advance. Determining the
optimum number of clusters is a hard problem and necessitates running
the k-means algorithm a number of times with diﬀerent k values and
selecting the best results for the particular data set. However, the k
value is usually small compared with the number of records in the
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.13
data set. Thus, the computational complexity for classiﬁcation of new
instances and the storage overhead are vastly reduced as new vectors
need only be compared to k prototype vectors and only k prototype
vectors need be stored unlike k-NN where all vectors need be stored
and compared to each other.
K-means initially chooses random cluster prototypes according to a
user-deﬁned selection process, the input data is applied iteratively and
the algorithm identiﬁes the best matching cluster and updates the cluster centre to reﬂect the new exemplar and minimise the sum-of-squares
clustering function given by equation 3
||xn −µj||2
where µ is the mean of the points (xn) in cluster Sj. Dragon use an
adapted similarity metric that incorporates the word count from the
news story, the distance to the clusters and the eﬀect the insertion has
on the prototype vector for the cluster. After training, each cluster has a
radius which is the distance between the prototype and the most distant
point lying in the cluster. This radius deﬁnes the bounds of normality
and is local to each cluster rather than the global distance settings
used in many approaches such as , and k-NN approaches. A new
exemplar is compared with the k-cluster model. If the point lies outside
all clusters then it is an outlier.
A very similar partitional algorithm is the k-medoids algorithm or
PAM (Partition Around Medoids) which represents each cluster using
an actual point and a radius rather than a prototype (average) point
and a radius. Bolton & Hand use a k-medoids
type approach they call Peer Group Analysis for fraud detection. Kmedoids is robust to outliers as it does not use optimisation to solve the
vector placement problem but rather uses actual data points to represent cluster centres. K-medoids is less susceptible to local minima than
standard k-means during training where k-means often converges to
poor quality clusters. It is also data-order independent unlike standard
k-means where the order of the input data aﬀects the positioning of the
cluster centres and Bradley shows that k-medoids
provides better class separation than k-means and hence is better suited
to a novelty recognition task due to the improved separation capabilities. However, k-means outperforms k-medoids and can handle larger
data sets more eﬃciently as k-medoids can require O(n2) running time
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.14
per iteration whereas k-means is O(n). Both approaches can generalise
from a relatively small data set. Conversely, the classiﬁcation accuracy
of k-NN, Least Squares Regression or Grubbs’ method is susceptible to
the number of exemplars in the data set like the kernel-based Parzen
windows and the node-based supervised neural networks described later
as they all model and analyse the density of the input distribution.
The data mining partitional algorithm CLARANS ,
is an optimised derivative of the k-medoids algorithm and can handle
outlier detection which is achieved as a by-product of the clustering
process. It applies a random but bounded heuristic search to ﬁnd an
optimal clustering by only searching a random selection of cluster updates. It requires two user-speciﬁed parameters, the value of k and the
number of cluster updates to randomly select. Rather than searching
the entire data set for the optimal medoid it tests a pre-speciﬁed number of potential medoids and selects the ﬁrst medoid it tests which
improves the cluster quality. However, it still has O(n2k) running time
so is only really applicable for small to medium size data sets.
Another proximity-based variant is the graph connectivity method.
Shekhar et al. introduce an approach to traﬃc
monitoring which examines the neighbourhoods of points but from a
topologically connected rather than distance-based perspective. Shekhar
detects traﬃc monitoring stations producing sensor readings which are
inconsistent with stations in the immediately connected neighbourhood. A station is an outlier if the diﬀerence between its sensor value
and the average sensor value of its topological neighbours diﬀers by
more than a threshold percentage from the mean diﬀerence between all
nodes and their topological neighbours. This is analogous to calculating the average distance between each point i and its k neighbours
i and then ﬁnding any points whose average distance avgDk
diﬀers by more than a speciﬁed percentage. The technique only considers topologically connected neighbours so there is no prerequisite
for specifying k and the number can vary locally depending on the
number of connections. However, it is best suited to domains where
a connected graph of nodes is available such as analysing traﬃc ﬂow
networks where the individual monitoring stations represent nodes in
the connected network.
2.2. Parametric Methods
Many of the methods we have just described do not scale well unless
modiﬁcations and optimisations are made to the standard algorithm.
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.15
Parametric methods allow the model to be evaluated very rapidly for
new instances and are suitable for large data sets; the model grows
only with model complexity not data size. However, they limit their
applicability by enforcing a pre-selected distribution model to ﬁt the
data. If the user knows their data ﬁts such a distribution model then
these approaches are highly accurate but many data sets do not ﬁt one
particular model.
One such approach is Minimum Volume Ellipsoid estimation (MVE)
 which ﬁts the smallest permissible ellipsoid volume around the majority of the data distribution model
(generally covering 50% of the data points). This represents the densely
populated normal region shown in ﬁgure 2 (with outliers shown) and
ﬁgure 5 (with outliers removed).
A similar approach, Convex Peeling peels away the records on the
boundaries of the data distribution’s convex hull and thus peels away the outliers. In contrast MVE maintains all
points and deﬁnes a boundary around the majority of points. In convex
peeling, each point is assigned a depth. The outliers will have the lowest
depth thus placing them on the boundary of the convex hull and are
shed from the distribution model. For example in ﬁgure 2, V, W, X,
Y and Z would each be assigned the lowest depth and shed during the
ﬁrst iteration. Peeling repeats the convex hull generation and peeling
process on the remaining records until it has removed a pre-speciﬁed
number of records. The technique is a type 1, unsupervised clustering
outlier detector. Unfortunately, it is susceptible to peeling away p + 1
points from the data distribution on each iteration and eroding the
stock of normal points too rapidly .
Both MVE and Convex Peeling are robust classiﬁers that ﬁt boundaries
around speciﬁc percentages of the data irrespective of the sparseness
of the outlying regions and hence outlying data points do not skew the
boundary. Both however, rely on a good spread of the data. Figure 2
has few outliers so an ellipsoid circumscribing 50% of the data would
omit many normal points from the boundary of normality. Both MVE
and convex peeling are only applicable for lower dimensional data sets
 (usually three dimensional or less for convex
peeling) as they suﬀer the Curse of Dimensionality where the convex
hull is stretched as more dimensions are added and the surface becomes
too diﬃcult to discern.
Torr and Murray also peel away outlying
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.16
points by iteratively pruning and re-ﬁtting. They measure the eﬀect
of deleting points on the placement of the Least Squares standard
regression line for a diagnostic outlier detector. The LS line is placed
to minimise equation 4
(yi −ˆyi)2
where ˆyi is the estimated value. Torr & Murray repeatedly delete the
single point with maximal inﬂuence (the point that causes the greatest
deviation in the placement of the regression line) thus allowing the
ﬁtted model to stay away from the outliers. They reﬁt the regression to the remaining data until there are no more outliers, i.e., the
next point with maximal inﬂuence lies below a threshold value. Least
Squares regression is not robust as the outliers aﬀect the placement
of the regression line so it is best suited to outlier diagnostics where
the outliers are removed from the next iteration. Figure 6 shows a
Least Squares regression line (dashed line) ﬁtted to a data distribution
with the outliers A and B present and then again after point A and B
have been removed. Although there are only two outliers, they have a
considerable aﬀect on the line placement.
Torr and Murray extend the technique for
image segmentation. They use a computationally cheap non-robust
least median of squares (LMS) regression if the number of outliers is
small which minimises equation 5
i=1(yi −ˆyi)2
or a computationally expensive robust random sampling algorithm if
the number of outliers is high. LMS is able to accommodate more outliers than LS as it uses the median values. However, random sampling
can accommodate larger numbers of outliers which eventually distort
LMS. LMS has also been improved to produce the Least Trimmed
Squares approach which has faster convergence and minimises equation 6
((yi −ˆyi)2)i:n
where (yi −ˆyi)2)1:n ≤... ≤(yi −ˆyi)2)n:n are the ordered square residuals. The summation function accommodates outliers in the distribution
by ﬁtting the regression to only the majority of the data rather than
all of the data as in LMS. This region thus depicts normality and LTS
highlights outliers as the points with large deviations from the majority.
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.17
MVE and Convex Peeling aim to compact the convex hull and circumscribe the data with a decision boundary but are only applicable for
low dimensional data. Principal Component Analysis (PCA) in contrast, is suitable for higher dimensional data. It identiﬁes correlated attributes in the data distribution
and projects the data onto this lower dimensional subspace. PCA is an
unsupervised classiﬁer but is linear and incapable of outperforming the
complex non-linear class boundaries identiﬁed by the Support Vector
Machine (see section 2.4) or neural methods described in section 3. PCA
assumes that the subspaces determined by the principal components
are compact and this limits its applicability particularly for sparse
distributions. However, it is an ideal pre-processor to select a subset of
attributes for methods which suﬀer the Curse of Dimensionality such as
the Multi-Layer Perceptron in section 3, proximity-based techniques or
symplectic transformations described next. PCA identiﬁes the principal
component of greatest variance as each component has an associated
eigenvalue whose magnitude corresponds to the variance of the points
from the component vector. PCA retains the k principal components
with greatest variance and discards all others to preserve maximum
information and retain minimal redundancy.
Faloutsos et al. recommend retaining suﬃcient
components so the sum of the eigenvalues of all retained components
is at least 85% of the sum of all eigenvalues. They use the principal
components to predict attribute values in records by ﬁnding the intersection between the given values for the record (i.e., excluding the
omitted attribute) and the principal components. If the actual value for
an attribute and the predicted value diﬀer then the record is ﬂagged
as an outlier. Parra et al. have developed a type-3
motor fault detector system which applies PCA to the data and then
applies a symplectic transformation to the ﬁrst few principal components. The symplectic transformation may be used with non-linear data
distributions. It maps the input data onto a Gaussian distribution,
conserving the volume and separating the training data (normal data)
from the outliers. This double transformation preserves information
while removing redundancy, generates a density estimation of the data
set and thus allows a circular contour of the density to act as the
decision boundary.
Baker et al. employ one of the hierarchical approaches detailed in this survey. The other hierarchical approaches are
the decision tree and cluster trees detailed in the machine learning
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.18
section. Baker uses a parametric model-based approach for novelty
detection in a news story monitor. A hierarchy allows the domain
knowledge to be represented at various levels of abstraction so points
can be compared for novelty at a ﬁne-grained or less speciﬁc level.
The hierarchical statistical algorithm induces a topic hierarchy from
the word distributions of news stories using Expectation Maximisation
(EM) to estimate the parameter settings followed by Deterministic
Annealing (DA). DA constructs the hierarchy via maximum likelihood
and information theory using a divisive clustering approach to split
nodes into sub-nodes, starting from a single cluster, and build the hierarchy top-down. DA stochastically determines the node to split. The
system detects novelty when new nodes are added to the hierarchy that
represent documents that do not belong to any of the existing event
clusters so new events are eﬀectively described by their position in the
hierarchy. When EM is used in conjunction with DA it avoids some of
the initialisation dependence of EM but at the cost of computational
eﬃciency. DA can avoid local minima which EM is susceptible to but
it may produce sub-optimal results.
2.3. Non-Parametric Methods
Many Statistical methods described in this section have data-speciﬁc
parameters ranging from the k values of k-NN and k-means to distance
thresholds for the proximity-based approaches to complex model parameters. Other techniques such as those based around convex hulls
and regression and the PCA approaches assume the data follows a
speciﬁc model. These all require a priori data knowledge. Such information is often not available or is expensive to compute. Many data
sets simply do not follow one speciﬁc distribution model and are often
randomly distributed. Hence, these approaches may be applicable for
an outlier detector where all data is accumulated beforehand and may
be pre-processed to determine parameter settings or for data where the
distribution model is known. Non-parametric approaches, in contrast
are more ﬂexible and autonomous.
Dasgupta & Forrest introduce a nonparametric approach for novelty detection in machinery operation. The
authors recognise novelty which contrasts to the other type 3 approaches
we describe such as k-means or the ART neural
approach in section 3 which recognise
or classify the normal data space. The machinery operation produces
a time-series of real-valued machinery measurements which Dasgupta
& Forrest map onto binary vectors using quantisation (binning). The
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.19
binary vector (string) eﬀectively represents an encoding of the last n
real-values from the time series. As the machinery is constantly monitored, new strings (binary vector windows) are generated to represent
the current operating characteristics. Dasgupta & Forrest use a set of
detectors where all detectors fail to match any strings deﬁning normality (where two strings match if they are identical in a ﬁxed number of
contiguous positions (r)). If any detectors match a new string (a new
time window of operating characteristics) then a novelty has been detected. The value of r aﬀects the performance of the algorithm and the
value must be selected carefully by empirical testing which inevitably
slows processing. Each individual recogniser represents a subsection of
the input distribution and compares the input. Dasgupta & Forrest’s
approach would eﬀectively model ﬁgure 5 by failing to match any point
within the normal boundary but would match any point from outside.
2.4. Semi-Parametric Methods
Semi-parametric methods apply local kernel models rather than a single
global distribution model. They aim to combine the speed and complexity growth advantage of parametric methods with the model ﬂexibility
of non-parametric methods. Kernel-based methods estimate the density
distribution of the input space and identify outliers as lying in regions
of low density. Tarassenko & Roberts 
and Bishop use Gaussian Mixture Models to learn a
model of normal data by incrementally learning new exemplars. The
GMM is represented by equation 7
αj(x)φj(t|x)
where M is the number of kernels (φ), αj(x) the mixing coeﬃcients,
x the input vector and t the target vector. Tarassenko & Roberts
classify EEG signatures to detect abnormal signals which represent
medical conditions such as epilepsy. In both approaches, each mixture represents a kernel whose width is autonomously determined by
the spread of the data. In Bishop’s approach the number of mixture
models is determined using cross-validation. Tarassenko & Roberts’
technique adds new mixture models incrementally. If the mixture that
best represents the new exemplar is above a threshold distance, then the
algorithm adds a new mixture. This distance threshold is determined
autonomously during system training. Once training is completed, the
ﬁnal distance threshold represents the novelty threshold for new items
to compare against. A Gaussian probability density function is deﬁned
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.20
by equation 8,
exp{−||t −µj(x)||2
where d is the dimensionality of the input space, σ is the smoothing
parameter, µj(x) represents the centre of the jth kernel and σ2
the variance (width). This growing approach is somewhat analogous to
growing neural networks in section 3 as it adds a new mixture where
it is not modelling the data distribution well.
Roberts introduced Extreme Value Theory which uses
a Gaussian mixture model to represent the data distribution for outlier
detection as outliers (extreme values) occur in the tails of the distributions as points V, W, X, Y and Z in ﬁgure 2 are extreme values.
Least Squares regression described earlier compares the outliers against
the correlation of the data distribution whereas EVT compares them
against a model of the distribution. EVT again uses EM to estimate
the algorithm’s parameter set. EVT examines the distribution tails and
estimates the probability that a given instance is an extreme value in
the distribution model given by equation 9
P(extremex) = exp{−exp(−xm −µm
The approach is more principled than the proximity-based thresholding
techniques discussed previously where a point is an outlier if it exceeds
a threshold distance from the normal class as the EVT threshold is set
using a heuristic approach. EVT is ideal for novelty recognition where
abnormal samples are diﬃcult or costly to obtain such as rare medical
cases or expensive machinery malfunctions. It is also not limited to
previously seen classes and is suitable for all three types of outlier
detection. A classiﬁer, such as the multi-layer Perceptron (section 3.1)
or decision trees (section 4), would attempt to place a new exemplar
from a previously unseen class in one of the classes it has previously
learned and would fail to detect the novelty.
The regression techniques and PCA are linear models which are too
simple for many practical applications. Tax & Duin 
and DeCoste & Levine use Support Vector
Machines (SVMs) for type 2 classiﬁcation which use linear models to
implement complex class boundaries. They project the input data onto
higher dimensional kernels using a kernel function in an attempt to
ﬁnd a hyper-plane that separates normal and abnormal data. Such a
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.21
class-deﬁning hyper-plane may not be evident at the lower dimensions.
The kernel functions range from linear dot product, polynomial nonlinear used by DeCoste & Levine, a sigmoid kernel function which is
equivalent to a Multi-Layer Perceptron see section 3.1 with no hidden
layers and a Gaussian function as in equation 8 used in Tax & Duin
which is equivalent to a radial basis function neural network described
in section 3.1. Support vectors functions are positive in the dense regions of the data distribution and negative in the sparsest regions of the
distribution where the outliers lie. A support vector function is deﬁned
by equation 10
SV = sign(
αjLjK(xj, z) + b)
where K is the Kernel function, sign is a function returning +1 if the
data is positive and -1 if the data is negative, Lj is the class label, b
is the bias, z the test input and xj the trained input. The data points
that deﬁne the class boundary of normality are the support vectors.
Only this small set of support vectors need be stored often less than
10% of the training set so a large data set can eﬀectively be stored
using a small number of exemplars.
Tax & Duin use support vector novelty detection
for machine condition monitoring and medical classiﬁcation. SVMs can
induce a classiﬁer from a poorly balanced data set where the abnormal/normal exemplars are disproportional which is particularly true in
medical domains where abnormal or in some case normal data is diﬃcult and costly to obtain. However, SVMs are computationally complex
to determine so heuristics have been devised to prevent this . DeCoste & Levine adapt the conventional SVM for
space instrument event detection by adapting the feature weights and
the cost of false positives as their data set is predominantly negative
with few positive instances of an event available for training.
Neural Networks
Neural network approaches are generally non-parametric and modelbased, they generalise well to unseen patterns and are capable of learning complex class boundaries. After training the neural network forms
a classiﬁer. However, the entire data set has to be traversed numerous
times to allow the network to settle and model the data correctly.
They also require both training and testing to ﬁne tune the network
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.22
and determine threshold settings before they are ready for the classiﬁcation of new data. Many neural networks are susceptible to the
Curse of Dimensionality though less so than the statistical techniques.
The neural networks attempt to ﬁt a surface over the data and there
must be suﬃcient data density to discern the surface. Most neural
networks automatically reduce the input features to focus on the key
attributes. But nevertheless, they still beneﬁt from feature selection or
lower dimensionality data projections.
3.1. Supervised Neural Methods
Supervised neural networks use the classiﬁcation of the data to drive
the learning process. The neural network uses the class to adjust the
weights and thresholds to ensure the network can correctly classify the
input. The input data is eﬀectively modelled by the whole network with
each point distributed across all nodes and the output representing the
classiﬁcation. For example, the data in ﬁgure 4 is represented by the
weights and connections of the entire network.
Some supervised neural networks such as the Multi-Layer Perceptron
interpolate well but perform poorly for extrapolation so cannot classify unseen instances outside the bounds of the training set. Nairac
 and Bishop both exploit this for
identifying novelties. Nairac identiﬁes novelties in time-series data for
fault diagnosis in vibration signatures of aircraft engines and Bishop
monitors processes such as oil pipeline ﬂows. The MLP is a feed forward
network. Bishop and Nairac use a MLP with a single hidden layer. This
allows the network to detect arbitrarily complex class boundaries. The
nodes in the ﬁrst layer deﬁne convex hulls analogous to the convex
hull statistical methods discussed in the previous section. These then
form the inputs for the second layer units which combine hulls to form
complex classes. Bishop notes that the non-linearity oﬀered by the MLP
classiﬁer provides a performance improvement compared to a linear
technique. It is trained by minimising the square error between the
actual value and the MLP output value given by equation 11
[yj(x; w) −⟨tj|x⟩]2p(x)dx
j|x⟩−⟨tj|x⟩2}p(x)dx
from where tj is the target class, yj is the actual class,
p(x) is the unconditional probability density which may be estimated
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.23
by, for example, Parzen Windows (discussed in section
5) and yj(x; w) is the function mapping. Provided the function mapping
is ﬂexible, if the network has suﬃcient hidden units, then the minimum
occurs when yj(x; w) = p⟨tj|x⟩. The outputs of the network are the
regression of the target data conditioned with the input vector. The
aim is thus to approximate the regression by minimising the sum-ofsquares error using a ﬁnite training set. The approximation is highest
where the density of p(x) is highest as this is where the error function
penalises the network mapping if it diﬀers from the regression. Where
the density is low there is little penalisation. Bishop assumes that the
MLP ‘knows’ the full scope of normality after training so the density
p(x) is high, the network is interpolating and conﬁdence is high (where
conﬁdence is given by equation 12)
σy(x) = {p(x)}
If a new input lies outside the trained distribution where the density p(x) is low and the network is extrapolating, the MLP will not
recognise it and the conﬁdence is low. Nairac uses the MLP to predict
the expected next value based on the previous n values. This assumes
that the next value is dependent only on the previous n values and
ignores extraneous factors. If the actual value diﬀers markedly from
the predicted value then the system alerts the human monitoring it.
Japkowicz uses an auto-associative neural network for type 3 novelty recognition. The auto associator network is
also a feed-forward Perceptron-based network which uses supervised
learning. Auto associators decrease the number of hidden nodes during network training to induce a bottleneck. The bottleneck reduces
redundancies by focusing on the key attributes while maintaining and
separating the essential input information and is analogous to Principal
Component Analysis. Japkowicz trains the auto-associator with normal
data only. After training, the output nodes recreate the new exemplars
applied to the network as inputs. The network will successfully recreate
normal data but will generate a high re-creation error for novel data.
Japkowicz counts the number of outputs diﬀerent from the input. If
this value exceeds a pre-speciﬁed threshold then the input is recognised
as novel. Japkowicz sets the threshold at a level that minimises both
false positive and false negative classiﬁcations. Taylor and Addison
 demonstrated that auto-associative neural
networks were more accurate than SOMs or Parzen windows for novelty detection. However, auto-associators suﬀer slow training as with
the MLP and have various data-speciﬁc parameters which must be set
through empirical testing and reﬁnement.
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.24
Hopﬁeld networks are also auto-associative and perform supervised
learning. However, all node weights are +1 or -1 and it is fully connected
with no bottleneck. Hopﬁeld Nets are computationally eﬃcient and
their performance is evident when a large number of patterns is stored
and the input vectors are high dimensional. Crook & Hayes use a Hopﬁeld Network for type 3 novelty detection
in mobile robots. The authors train the Hopﬁeld network with preclassiﬁed normal exemplars. The input pattern is applied to all nodes
simultaneously unlike the MLP where the input ripples through the
layers of the network. The output is fed back into the network as input
to all nodes. Retrieval from a Hopﬁeld network requires several network
cycles while node weights update and to allow the network to settle and
converge towards the “best compromise solution” . However, the energy calculation requires only the calculation of
the sum of the weights see equation 13
Energy = −1
where N is the number of neurons and thus has a ﬁxed execution time
regardless of the number of input patterns stored providing a computationally eﬃcient retrieval mechanism. If the energy is above a threshold
value calculated from the distribution model and the number of patterns stored then Crook & Hayes classify the input as novel. During
empirical testing, the Hopﬁeld network trained faster than Marsland’s
HSOM but the HSOM discerned ﬁner-grained image details than the
Hopﬁeld network.
The MLP uses hyper-planes to classify the data. In contrast, the supervised Radial Basis Function (RBF) neural network in Bishop and Brotherton et al. uses hyper-ellipsoids
deﬁned by equation 14 for k dimensional inputs.
wjkφ(||x −yj||)
where wjk is the weight from the kernel to the output, φ is a Gaussian
function (see equation 8) and ||...|| the Euclidean distance. RBF is a
linear combination of basis functions similar to a Gaussian Mixture
model and guaranteed to produce a solution as hyper-ellipsoids guarantee linear separability . Training the network is much
faster than an MLP and requires two stages: the ﬁrst stage clusters the
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.25
input data into the hidden layer nodes using vector quantisation. The
RBF derives the radius of the kernel from the data as with the Gaussian
mixture model approach (see equation 8) using the kernel centre and
variance. The second training stage weights the hidden node outputs
using least mean squares weighting to produce the required output
classiﬁcation from the network.
Brotherton revises the network topology and
vector quantisation so there is a group of hidden nodes exclusively
assigned for each class with a separate vector quantisation metric for
each class. This means that the groups of hidden nodes represent distinct categories unlike a conventional RBF where weight combinations
of all hidden nodes represent the categories. The output from each
group of nodes corresponds to the probability that the input vector
belongs to the class represented by the group. Brotherton uses the
adapted RBF for type 3 novelty recognition in electromagnetic data or
machine vibration analysis. Brotherton’s adaptation creates an incremental RBF. New nodes can be added to represent new classes just
as new mixtures are added to Roberts and Tarassenko’s adaptive mixture model or new category nodes are
added to evolutionary neural networks.
3.2. Unsupervised Neural Methods
Supervised networks require a pre-classiﬁed data set to permit learning.
If this pre-classiﬁcation is unavailable then an unsupervised neural network is desirable. Unsupervised neural networks contain nodes which
compete to represent portions of the data set. As with Perceptron-based
neural networks, decision trees or k-means, they require a training data
set to allow the network to learn. They autonomously cluster the input
vectors through node placement to allow the underlying data distribution to be modelled and the normal/abnormal classes diﬀerentiated.
They assume that related vectors have common feature values and rely
on identifying these features and their values to topologically model
the data distribution.
Self Organising Maps, Kohonen are competitive, unsupervised neural networks. SOMs perform vector quantisation and
non-linear mapping to project the data distribution onto a lower dimensional grid network whose topology needs to be pre-speciﬁed by
the user. Each node in the grid has an associated weight vector analogous to the mean vector representing each cluster in a k-means system.
The network learns by iteratively reading each input from the training
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.26
data set, ﬁnding the best matching unit, updating the winner’s weight
vector to reﬂect the new match like k-means. However, the SOM also
updates the neighbouring nodes around the winner unlike k-means.
This clusters the network into regions of similarity. The radius of the
local neighbourhood shrinks during training in the standard SOM and
training terminates when the radius of the neighbourhood reaches its
minimum permissible value. Conversely, Ypma 
enlarges the ﬁnal neighbourhood to increase the SOMs “stiﬀness” by
providing broader nodes to prevent over ﬁtting. After training, the
network has evolved to represent the normal class. In ﬁgure 5, the
SOM nodes would model the normal class, distributed according to
the probability density of the data distribution.
During novelty recognition, the unseen exemplar forms the input to the
network and the SOM algorithm determines the best matching unit.
In Saunders & Gero and Vesanto , if the vector distance or quantisation error between the
best matching unit (bmu) and new exemplar exceeds some pre-speciﬁed
threshold (d) then the exemplar is classiﬁed as novel. Equation 15 gives
the minimum vector distance for the bmu and compares this to the
threshold.
(xi(t) −wij(t))2) > d
For example, if the SOM nodes modelled ﬁgure 5 (the normal class
from ﬁgure 2), the points V, W, X, Y and Z from ﬁgure 2 would all be
distant from their respective best matching units and would exceed d so
V, W, X, Y and Z would be identiﬁed as novel. This is analogous to a kmeans approach (section 2.1); the SOM equates to a k-means clustering
with k equivalent to the number of SOM nodes. However, SOMs use a
user-speciﬁed global distance threshold whereas k-means autonomously
determines the boundary during training allowing local setting of the
radius of normality as deﬁned by each cluster. This distance match
can be over simplistic and requires the selection of a suitable distance
threshold so other approaches examine the response of all nodes in the
grid. Himberg look at the quantisation error
from the input to all nodes, given by equation 16
g(x, mi) =
1 + uses a further advancement
for mechanical fault detection. They recommend an approach unifying
normalised distance measure for 2 and 3 nearest neighbours rather
than a single nearest neighbour of Saunders & Gero and Vesanto and
normalised mean-square error of mapping.
Saunders & Gero and Marsland’s systems use an extended SOM, the Habituating SOMs
(HSOMs) where every node is connected to an output neuron through
habituating synapses which perform inverse Hebbian learning by reducing their connection strength through frequency of activation. The
SOM forms a novelty detector where the novelty of the input is proportional to the strength of the output thus simplifying novelty detection
compared to a conventional SOM as the output node indicates novelty,
there is no requirement for identifying best matching units or complex
quantisation error calculations. The HSOM requires minimal computational resources and is capable of operating and performing novelty
detection embedded in a robot . It also trains quickly
and accurately. However, HSOMs are susceptible to saturation where
any input will produce a low strength output as the HSOM cannot
add new nodes and the number of classes (groups of inputs mapping to
particular nodes) in the input distribution is equal to or greater than
the number of nodes. If the number of classes is known in advance,
then a suitable number of nodes can be pre-selected but for many
outlier detection problems, particularly on-line learning problems, this
information may not be available
Evolutionary neural network growth is analogous to Tarassenko and
Roberts approach of adding new Gaussian mixtures when a new datum does not ﬁt any of the existing
mixtures. Marsland introduces the Grow When Required (GWR) evolutionary neural network for type 3 novel feature
detection in a mobile inspection robot (robot neotaxis). It is suited
to on-line learning and novelty recognition as the network can adapt
and accurately model a dynamic data distribution. The growing neural
network connects the best matching node and the second best match
and adds new nodes when the response of the best matching node for an
input vector is below some threshold value. The GWR network grows
rapidly during the ﬁrst phase of training but once all inputs have been
processed (assuming a static training set), then the number of nodes
stabilises and the network accurately represents the data distribution.
As with the SOM, the GWR would learn the normal class in ﬁgure
5 and the points V, W, X, Y and Z from ﬁgure 2 would all then be
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.28
distant from their respective best matching units which are all within
the normal class.
Caudell & Newman introduce a type
3 recogniser for time-series monitoring based on the Adaptive Resonance Theory (ART) incremental
unsupervised neural network. The network is plastic while learning,
then stable while classifying but can return to plasticity to learn again
making them ideal for time-series monitoring. The ART network trains
by taking a new instance vector as input, matching this against the
classes currently covered by the network and if the new data does not
match an existing class given by equation 17
i=1 wij(t)xi
where n is the input dimensionality and ρ is a user-speciﬁed parameter
(vigilance threshold) then it creates a new class by adding a new node
within the network to accommodate it. Caudell & Newman monitor
the current ART classes and when the network adds a new class this
indicates a change in the time series. GWR grows similarly but GWR is
topology preserving with neighbourhoods of node representing similar
inputs whereas ART simply aims to cover the entire input space uniformly. By only adding a new node if the existing mapping is insuﬃcient
and leaving the network unchanged otherwise, the ART network does
not suﬀer from over-ﬁtting which is a problem for MLPs and decision
trees. The training phase also requires only a single pass through the
input vectors unlike the previously described neural networks which
require repeated presentation of the input vectors to train. During
empirical evaluation, Marsland noted that the performance of the ART network was extremely disappointing for robot
neotaxis. The ART network is not robust and thus susceptible to noise.
If the vigilance parameter is set too high the network becomes too
sensitive and adds new classes too often.
Machine Learning
Much outlier detection has only focused on continuous real-valued data
attributes there has been little focus on categorical data. Most statistical and neural approaches require cardinal or at the least ordinal
data to allow vector distances to be calculated and have no mechanism
for processing categorical data with no implicit ordering. John and Skalak & Rissland use a C4.5 decision tree to detect outliers in categorical data and thus identify errors
and unexpected entries in databases. Decision trees do not require any
prior knowledge of the data unlike many statistical methods or neural
methods that require parameters or distribution models derived from
the data set. Conversely, decision trees have simple class boundaries
compared with the complex class boundaries yielded by neural or SVM
approaches. Decision trees are robust, do not suﬀer the Curse of Dimensionality as they focus on the salient attributes, and work well on noisy
data. C4.5 is scalable and can accommodate large data sets and high
dimensional data with acceptable running times as they only require a
single training phase. C4.5 is suited to type 2 classiﬁer systems but not
to novelty recognisers which require algorithms such as k-means which
can deﬁne a boundary of normality and recognise whether new data lies
inside or outside the boundary. However, decision trees are dependent
on the coverage of the training data as with many classiﬁers. They
are also susceptible to over ﬁtting where they do not generalise well
to completely novel instances which is also a problem for many neural
network methods. There are two solutions to over-ﬁtting: pre-selection
of records or pruning.
Skalak & Rissland pre-select cases using
the taxonomy from a case-based retrieval algorithm to pinpoint the
“most-on-point-cases” and exclude outliers. They then train a decision tree with this pre-selected subset of normal cases. Many authors
recommend pruning superﬂuous nodes to improve the generalisation
capabilities and prevent over-ﬁtting. John exploits this
tactic and uses repeated pruning and retraining of the decision tree
to derive the optimal tree representation until no further pruning is
possible. The pruned nodes represent the outliers in the database and
are systematically removed until only the majority of normal points are
Arning also uses pruning but in a set-based machinelearning approach. It processes categorical data and can identify outliers where there are errors in any combination of variables. Arning
identiﬁes the subset of the data to discard that produces the greatest
reduction in complexity relative to the amount of data discarded. He
examines the data as a sequence and uses a set-based dissimilarity
function to determine how dissimilar the new exemplar is compared to
the set of exemplars already examined. A large dissimilarity indicates
a potential outlier. The algorithm can run in linear time though this
relies on prior knowledge of the data but even without prior knowledge,
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.30
it is still feasible to process large data mining data sets with the approach. However, even the authors note that it is not possible to have a
universally applicable dissimilarity function as the function accuracy is
data dependent. This pruning is analogous to the statistical techniques
(see section 2.2) of convex peeling or Torr and Murray’s approach of
shedding points and re-ﬁtting the regression line to the remaining data.
Another machine learning technique exploited for outlier detection is
rule-based systems which are very similar to decision trees as they both
test a series of conditions(antecedents) before producing a conclusion
(class). In fact rules may be generated directly from the paths in the
decision tree. Rule-based systems are more ﬂexible and incremental
than decision trees as new rules may be added or rules amended without
disturbing the existing rules. A decision tree may require the generation
of a complete new tree. Fawcett & Provost 
describe their DC-1 activity monitoring system for detecting fraudulent
activity or news story monitoring. The rule-based module may be either
a classiﬁer learning classiﬁcation rules from both normal and abnormal
training data or a recogniser trained on normal data only and learning
rules to pinpoint changes that identify fraudulent activity. The learned
rules create proﬁling monitors for each rule modelling the behaviour
of a single entity, such as a phone account. When the activity of the
entity deviates from the expected, the output from the proﬁling monitor
reﬂects this. All proﬁling monitors relating to a single entity feed into a
detector which combines the inputs and generates an alarm if the deviation exceeds a threshold. This approach detects deviations in sequences
compared to Nairac’s time-series approach which uses one-step ahead
prediction to compare the expected next value in a sequence with the
actual value measured. There is no prediction in activity monitoring
all comparisons use actual measured values.
Lane & Brodley 
introduced a similar approach for activity monitoring using similaritybased matching. The system stores and records the activities of users
on a computer system as sequences. The system uses similarity-based
matching to compare command sequences issued by users against the
stored sequences. Due to the asynchronous nature of human-computer
interaction, a new fraudulent command sequence is unlikely to exactly
match a previously stored sequence. The matching metric must allow
for the interpolation of gaps and non-matching sub-sequences reﬂecting,
for example, the user collecting an incoming e-mail. Lane & Brodley
take their cue from Michalski’s sequential data modelling techniques and generate a validity analysis for each
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.31
step in the command sequence entered. They incorporate adjacency
into their similarity metric. If the similarity value is between a lower and
upper bound threshold then the current sequence up to and including
the current command is not anomalous
Data mining algorithms such as BIRCH and DB-
SCAN are robust and as such tolerant to outliers
but were speciﬁcally optimised for clustering large data sets. They are
based around tree structured indices and cluster hierarchies. Both techniques are capable of identifying outliers but cannot provide a degree of
novelty score. BIRCH uses local clustering and data
compression and has O(n) running time. BIRCH can operate incrementally, clustering data as and when it becomes available. However, it is
limited to numerical data and is dependent on the order of presentation
of the data set. It uses a specialised balanced tree structure called a
CF-Tree which is designed for large, high dimensional data sets and
induces a good quality clustering from only a single pass through the
data set which can optionally be improved by further passes. It can
only store a limited number of records in each tree node and this can
lead to an artiﬁcial cluster topology.
DBSCAN is density-based with O(n log n) running time and generates an R*-tree to cluster the data and identify
the k nearest neighbours. It can identify clusters of arbitrary shape like
the Gaussian Mixture model described earlier. DBSCAN has two userspeciﬁed parameters which determine the density of the data in the
tree but it autonomously determines the number of clusters unlike for
example the k-means algorithm. One parameter, the number of neighbours is generally set to 4 but the other parameter (the neighbourhood
distance) requires O(n log n) time to calculate. The pre-initialisation
step for DBSCAN calculates the distance between a point and its fourth
nearest neighbours, plots the distance graph and then requires the user
to ﬁnd a valley in the distance plot. The user must identify where the
valley begins from the graph proﬁle and set the parameter accordingly.
Hybrid Systems
The most recent development in outlier detection technology is hybrid systems. The hybrid systems discussed in this section incorporate
algorithms from at least two of the preceding sections (statistical, neural or machine learning methods). Hybridisation is used variously to
overcome deﬁciencies with one particular classiﬁcation algorithm, to
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.32
exploit the advantages of multiple approaches while overcoming their
weaknesses or using a meta-classiﬁer to reconcile the outputs from
multiple classiﬁers to handle all situations. We describe approaches
where an additional algorithm is incorporated to overcome weaknesses
with the primary algorithm next.
The MLP described in section 3 interpolates well but cannot extrapolate to unseen instances. Therefore, Bishop augments
his MLP with a Parzen window novelty recogniser. The MLP classiﬁes new items similar to those already seen with high conﬁdence.
The Parzen window provides an estimate of the probability density
to induce a conﬁdence estimate. Any completely novel instances will
lie in a low density and the MLP prediction conﬁdence will be low.
Parzen windows are kernel-based algorithms which use various kernel
functions to represent the data. Bishop uses one Gaussian kernel (as
deﬁned by equation 8) per input vector with the kernel centred on the
attribute values of the vector. The Parzen window method can provide
a hard boundary for outlier detection by classifying new exemplar as
normal if they belong to a kernel and an outlier otherwise. It can also
provide a soft boundary by calculating the probability that the new
exemplar belongs to a Gaussian kernel. Nairac et al. similarly incorporate a k-means module with the MLP module in an aircraft engine fault diagnosis system. The k-means module
partitions the data and models graph shape normality. It can detect
anomalies in the overall vibration shape of new signatures and the MLP
detects transitions within the vibration graphs through one-step ahead
prediction described in section 3.
Smyth stabilises the output from an MLP to monitor
space probe data for fault detection using a Hidden Markov Model
(HMM) as the MLP is susceptible to rapid prediction changes many of
which are extraneous. The approach uses time-series data, a snapshot of
the space probe transmitted periodically with equal intervals between
transmissions. The feed-forward MLP trained using back propagation
aims to model the probe and predicts the next state of the probe. However, the MLP has a tendency to switch between states at an artiﬁcially
high rate so Smyth uses a HMM to correlate the state estimations and
smooth the outputs producing a predictor with high accuracy. Hollmen
& Tresp incorporate hierarchical HMMs
with EM which helps determine the parameters for time series analysis
in the task of cell-phone fraud detection. The authors use the HMM
to predict one-step ahead values for a fraud variable in the time series
given the current and previous states of a user’s account and the EM
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.33
algorithm is used to provide optimal parameter settings for the HMM
to enable accurate one-step ahead prediction.
Hickinbotham & Austin introduced
a strain gauge fault detection system to detect gauges which are not
reading correctly. The system assimilates Gaussian Basis Function Networks (GBFN) and Principal Components. The technique produces
a Frequency of Occurrence Matrix (FOOM) from data taken from
aircraft strain gauges operating under normal conditions. The FOOM
represents the normal conditions expected during a ﬂight with readings
weighted according to their likelihood of occurrence. The approach
extracts two unlikeliness features and two Principal Components from
the FOOM and feeds them into a series of GBFN, each trained with
diﬀerent parameter settings. The system selects the GBFN with highest classiﬁcation accuracy for the data and uses this to classify new
FOOMs by thresholding for novelty. A high novelty reading indicates a
strain gauge error. Hollier & Austin have
incorporated a further component into the strain gauge system to
detect another form of strain gauge error not covered by the previous approach. They incorporate auxiliary data comprising a vector of
accelerations encountered during a ﬂight. Hollier & Austin ﬁnd the
maximally correlated components between the FOOM and the mean
and ﬁrst principal component of the auxiliary data. By establishing a
relationship between the auxiliary data and normal FOOMs, they can
detect corrupted FOOMs when the relationship breaks down.
Authors are beginning to examine the possibilities of ensemble classi-
ﬁers where a number of diﬀerent classiﬁcation algorithms are used and
the results from each classiﬁer amalgamated using a suitable metric.
From the preceding discussion, it is apparent that all classiﬁers have
diﬀering strengths and weaknesses so a combined approach can exploit
the strengths of each classiﬁer while mitigating their weaknesses.
The JAM system (Java agents for meta-learning) being developed at
Columbia University incorporates ﬁve machinelearning techniques into a single modular architecture. JAM uses metalearning to assimilate the results from the various modules and produce
a single system output. JAM modules are based on the ID3 decision
tree , its successors CART and
C4.5 , Ripper )
and a naive Bayes classiﬁer. In experiments, Prodromidis and Stolfo
 demonstrated that these classiﬁers were
complimentary, each performing well on some data sets and compara-
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.34
tively poorly on other data sets. By identifying which approaches excel
on which data sets and under which conditions the meta-learner can
assess the degree of accuracy and conﬁdence for the outputs from the
various classiﬁers and select the appropriate technique for the data.
Ensemble classiﬁers also permit sub-division of the training data with
individual classiﬁer modules trained on subsets of the data. By using
multiple data models, JAM mitigates for any loss of accuracy in any
one model due to data subdivision. It allows parallel training and classiﬁcation with each module classifying a new instance in parallel and
the meta-learner metric assimilating the results.
Brodley and Friedl also use an ensemble
approach for identifying misclassiﬁed instances in a training set of pixels
from satellite images of land where each pixel needs to be classiﬁed
as for example, grassland, woodland etc. They use three classiﬁers: a
decision tree, a nearest neighbour and a linear machine. The system
combines the outputs from the three classiﬁers using consensus voting.
Consensus voting is a more conservative approach than the widely used
majority voting. In consensus voting, the system only eliminates an
instance from the data set if all three classiﬁers identify it as an outlier
whereas a majority voting system eliminates an instance if the majority
of classiﬁers identify it as an outlier. Consensus voting favours false
negatives over false positives with respect to identifying outliers. The
three classiﬁers each use a diﬀerent distribution model so if all three
agree that a point is an outlier then the user can be conﬁdent about
the decision.
Combining multiple classiﬁers must be done judiciously. An ensemble
should obey Occam’s Razor, it should be as simple as possible with
minimal redundancy, as superﬂuous modules waste resources, increase
complexity and slow processing. Paradoxically, identifying the best
combination of classiﬁers is a combinatorial problem.
Conclusions
There is no single universally applicable or generic outlier detection
approach. From the previous descriptions, authors have applied a wide
variety of techniques covering the full gamut of statistical, neural and
machine learning techniques. We have tried to provide a broad sample
of current techniques but obviously, we are unable to describe all approaches in a single paper. We hope to have provided the reader with
a feel of the diversity and multiplicity of techniques available.
Hodge+Austin_OutlierDetection_AIRE381.tex; 19/01/2004; 13:18; p.35
In outlier detection, the developer should select an algorithm that is
suitable for their data set in terms of the correct distribution model,
the correct attribute types, the scalability, the speed, any incremental
capabilities to allow new exemplars to be stored and the modelling
accuracy. The developer should also consider which of the three fundamental approaches is suitable for their problem, a clustering approach,
a classiﬁcation approach or a novelty approach. This will depend on:
the data type, whether the data is pre-labelled, the ground truth of the
labelling (i.e., whether any novel records will be mislabelled as normal),
how they wish to detect outliers and how they wish to handle them.
How the developers wish to handle outliers is very important, whether
they wish to expunge them from future processing in a diagnostic clustering or a recognition system or retain them with an appropriate label
in a accommodating clustering or a classiﬁcation system. Developers
should also consider whether they wish to simply classify a new exemplar as an outlier or not or whether they wish to apply a scale of
“outlierness” such as a percentage probability that the particular point
is an outlier. The developer should carefully consider any pre-processing
they will perform on the data such as feature extraction or prototyping
to reduce the storage overhead of the data. Alternatively they can select
an approach such as k-means or support vector machines that store
only a minimal data set which eﬀectively covers the data distribution
through a small subset of seminal exemplars. Francis et al. showed equivalent classiﬁcation accuracy when the data
set of a novelty detection system based on neural network approaches
was pre-processed with feature extraction compared to the classiﬁcation
accuracy with that of the novelty detector trained using the full range
of attributes. The authors used linear regression to combine multiple
attributes into single dependent attributes.
Acknowledgement
We wish to thank the reviewers for their insightful comments that
allowed us to improve this paper.