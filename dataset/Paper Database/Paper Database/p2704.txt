THE GEOMETRY OF ALGORITHMS
WITH ORTHOGONALITY CONSTRAINTS
ALAN EDELMAN∗, T.A. ARIAS†, AND STEVEN T. SMITH‡
In press, SIAM J. Matrix Anal. Appl.
In this paper we develop new Newton and conjugate gradient algorithms on the Grassmann and Stiefel
manifolds. These manifolds represent the constraints that arise in such areas as the symmetric eigenvalue problem, nonlinear
eigenvalue problems, electronic structures computations, and signal processing. In addition to the new algorithms, we show
how the geometrical framework gives penetrating new insights allowing us to create, understand, and compare algorithms.
The theory proposed here provides a taxonomy for numerical linear algebra algorithms that provide a top level mathematical
view of previously unrelated algorithms. It is our hope that developers of new algorithms and perturbation theories will
beneﬁt from the theory, methods, and examples in this paper.
Key words. conjugate gradient, Newton’s method, orthogonality constraints, Grassmann manifold, Stiefel manifold,
eigenvalues and eigenvectors, invariant subspace, Rayleigh quotient iteration, eigenvalue optimization, sequential quadratic
programming, reduced gradient method, electronic structures computation, subspace tracking
AMS subject classiﬁcations. 49M07, 49M15, 53B20, 65F15, 15A18, 51F20, 81V55
1. Introduction. Problems on the Stiefel and Grassmann manifolds arise with suﬃcient frequency
that a unifying investigation of algorithms designed to solve these problems is warranted. Understanding
these manifolds, which represent orthogonality constraints (as in the symmetric eigenvalue problem),
yields penetrating insight into many numerical algorithms and uniﬁes seemingly unrelated ideas from
diﬀerent areas.
The optimization community has long recognized that linear and quadratic constraints have special
structure that can be exploited. The Stiefel and Grassmann manifolds also represent special constraints.
The main contribution of this paper is a framework for algorithms involving these constraints, which draws
upon ideas from numerical linear algebra, optimization, diﬀerential geometry, and has been inspired by
certain problems posed in engineering, physics, and chemistry.
Though we do review the necessary
background for our intended audience, this is not a survey paper. This paper uses mathematics as a tool
so that we can understand the deeper geometrical structure underlying algorithms.
In our ﬁrst concrete problem we minimize a function F(Y ), where Y is constrained to the set of
n-by-p matrices such that Y T Y = I (we call such matrices orthonormal), and we make the further
homogeneity assumption that F(Y ) = F(Y Q), where Q is any p-by-p orthogonal matrix. In other words,
the objective function depends only on the subspace spanned by the columns of Y ; it is invariant to any
choice of basis. The set of p-dimensional subspaces in Rn is called the Grassmann manifold. (Grassmann
originally developed the idea in 1848, but his writing style was considered so obscure that it was
appreciated only many years later. One can ﬁnd something of the original deﬁnition in his later work
[48, Chap. 3, Sec. 1,Article 65].) To the best of our knowledge, the geometry of the Grassmann manifold
has never been explored in the context of optimization algorithms, invariant subspace computations,
physics computations, or subspace tracking. Useful ideas from these areas, however, may be put into the
geometrical framework developed in this paper.
In our second problem we minimize F(Y ) without the homogeneity condition F(Y ) = F(Y Q) mentioned above, i.e., the optimization problem is deﬁned on the set of n-by-p orthonormal matrices. This
constraint surface is known as the Stiefel manifold, which is named for Eduard Stiefel, who considered
its topology in the 1930s . This is the same Stiefel who in collaboration with Magnus Hestenes in
1952 originated the conjugate gradient algorithm . Both Stiefel’s manifold and his conjugate gradient
algorithm play an important role in this paper. The geometry of the Stiefel manifold in the context of
∗Department of Mathematics Room 2-380, Massachusetts Institute of Technology, Cambridge, MA 02139, , Supported by a fellowship from the Alfred P. Sloan Foundation
and NSF grants 9501278-DMS and 9404326-CCR.
† Department of Physics, Massachusetts Institute of Technology, Cambridge, MA 02139, . Supported
by an NSF/MRSEC Seed Project grant from the MIT Center for Material Science and Engineering.
‡ MIT Lincoln Laboratory, 244 Wood Street, Lexington, MA 02173, . Sponsored by DARPA under
Air Force contract F19628-95-C-0002. Opinions, interpretations, conclusions, and recommendations are those of the author
and are not necessarily endorsed by the United States Air Force.
Edelman, Arias, and Smith
optimization problems and subspace tracking was explored by Smith . In this paper we use numerical
linear algebra techniques to simplify the ideas and algorithms presented there so that the diﬀerential geometric ideas seem natural and illuminating to the numerical linear algebra and optimization communities.
The ﬁrst author’s original motivation for studying this problem came from a response to a linear
algebra survey , which claimed to be using conjugate gradient to solve large dense eigenvalue problems.
The second and third authors were motivated by two distinct engineering and physics applications. The
salient question became: “What does it mean to use conjugate gradient to solve eigenvalue problems?” Is
this the Lanczos method? As we shall describe, there are dozens of proposed variations on the conjugate
gradient and Newton methods for eigenvalue and related problems, none of which are Lanczos. These
algorithms are not all obviously related. The connections among these algorithms have apparently not
been appreciated in the literature while in some cases numerical experiments have been the only basis for
comparison when no theoretical understanding was available. The existence of so many variations in so
many applications compelled us to ask for the big picture: What is the mathematics that uniﬁes all of
these apparently distinct algorithms. This paper contains our proposed uniﬁcation.
We summarize by itemizing what is new in this paper.
1. Algorithms for Newton and conjugate gradient methods on the Grassmann and Stiefel manifolds
that naturally use the geometry of these manifolds.
In the special cases that we are aware of, our
general algorithms are competitive up to small constant factors with the best known special algorithms.
Conjugate gradient and Newton on the Grassmann manifold have never been studied before explicitly.
Stiefel algorithms have been studied before , but the ideas here represent considerable simpliﬁcations.
2. A geometrical framework with the right mix of abstraction and concreteness to serve as a foundation for any numerical computation or algorithmic formulation involving orthogonality constraints,
including the symmetric eigenvalue problem. We believe that this is a useful framework because it connects apparently unrelated ideas; it is simple and mathematically natural. The framework provides new
insights into existing algorithms in numerical linear algebra, optimization, signal processing, and electronic structures computations, and it suggests new algorithms. For example, we connect the ideas of
geodesics and the cubic convergence of the Rayleigh quotient iteration, the CS decomposition, and sequential quadratic programming. We also interpret the ill-conditioning of eigenvectors of a symmetric
matrix with multiple eigenvalues as the singularity of Stiefel and Grassmann coordinates.
3. Though geometrical descriptions of the Grassmann and Stiefel manifolds are available in many
references, ours is the ﬁrst to use methods from numerical linear algebra emphasizing computational
eﬃciency of algorithms rather than abstract general settings.
The remainder of this paper is organized into three sections. The geometrical ideas are developed
in §2. This §4 provides a self-contained introduction to geometry, which may not be familiar to some
readers, while deriving the new geometrical formulas necessary for the algorithms of §3 and the insights
of §3 provides descriptions of new algorithms for optimization on the Grassmann and Stiefel manifolds.
Concrete examples of the new insights gained from this point of view are presented in §4. Because we wish
to discuss related literature in the context developed in §2 and §3, we defer discussion of the literature to
§4 where speciﬁc applications of our theory are organized.
2. Diﬀerential Geometric Foundation for Numerical Linear Algebra. A geometrical treatment of the Stiefel and Grassmann manifolds appropriate for numerical linear algebra cannot be found in
standard diﬀerential geometry references. For example, what is typically required for practical conjugate
gradient computations involving n-by-p orthonormal matrices are algorithms with complexity of order np2.
In this section we derive new formulas that may be used in algorithms of this complexity in terms of standard operations from numerical linear algebra. These formulas will be used in the algorithms presented
in the following section. Because we focus on computations, our approach diﬀers from the more general
(and powerful) coordinate-free methods used by modern geometers . Boothby 
provides an undergraduate level introduction to the coordinate-free approach.
For readers with a background in diﬀerential geometry, we wish to point out how we use extrinsic
coordinates in a somewhat unusual way. Typically, one uses a parameterization of the manifold (e.g.,
x = cos u sin v, y = sin u sin v, z = cos v for the sphere) to derive metric coeﬃcients and Christoﬀel
symbols in terms of the parameters (u and v). Instead, we only use extrinsic coordinates subject to
constraints (e.g., (x, y, z) such that x2 + y2 + z2 = 1). This represents points with more parameters
than are intrinsically necessary, but we have found that the simplest (hence computationally most useful)
formulas for the metric and Christoﬀel symbol are obtained in this manner. The choice of coordinates
Orthogonality Constraints
Representations of subspace manifolds
Matrix Rep.
Quotient Rep.
Orthogonal Group
Stiefel Manifold
Grassmann Manifold
On/ (Op × On−p)
does not matter abstractly, but on a computer the correct choice is essential.
We now outline this section. After deﬁning the manifolds of interest to us in §2.1, we take a close
look at the Stiefel manifold as a submanifold of Euclidean space in §2.2. This introduces elementary ideas
from diﬀerential geometry and provides the geometric structure of the orthogonal group (a special case of
the Stiefel manifold), which will be used throughout the rest of the paper. However, the Euclidean metric
is not natural for the Stiefel manifold, which inherits a canonical metric from its deﬁnition as a quotient
space. Therefore, we introduce the quotient space point of view in §2.3. With this viewpoint, we then
derive our formulas for geodesics and parallel translation for the Stiefel and Grassmann manifold in §2.4
and §2.5. Finally, we describe how to incorporate these formulae into conjugate gradient and Newton
methods in §2.6.
2.1. Manifolds Arising in Numerical Linear Algebra. For simplicity of exposition, but for
no fundamental reason, we will concentrate on real matrices. All ideas carry over naturally to complex
matrices. Spaces of interest are
1. The orthogonal group On consisting of n-by-n orthogonal matrices
2. The Stiefel manifold Vn,p consisting of n-by-p “tall-skinny” orthonormal matrices
3. The Grassmann manifold Gn,p obtained by identifying those matrices in Vn,p whose columns
span the same subspace (a quotient manifold).
Table 2.1 summarizes the deﬁnitions of these spaces. Our description of Gn,p is necessarily more
abstract than On or Vn,p. Gn,p may be deﬁned as the set of all p-dimensional subspaces of an n-dimensional
We shall beneﬁt from two diﬀerent yet equivalent modes of describing our spaces: concrete representations and quotient space representations. Table 2.2 illustrates how we store elements of Vn,p and Gn,p
in a computer. A point in the Stiefel manifold Vn,p is represented by an n-by-p matrix. A point on the
Grassmann manifold Gn,p is a linear subspace, which may be speciﬁed by an arbitrary orthogonal basis
stored as an n-by-p matrix. An important diﬀerence here is that, unlike points on the Stiefel manifold,
the choice of matrix is not unique for points on the Grassmann manifold.
The second mode of representation, the more mathematical, is useful for obtaining closed-form expressions for the geometrical objects of interest. It is also the “proper” theoretical setting for these manifolds.
Here, we represent the manifolds as quotient spaces. Points in the Grassmann manifold are equivalence
classes of n-by-p orthogonal matrices, where two matrices are equivalent if their columns span the same
p-dimensional subspace. Equivalently, two matrices are equivalent if they are related by right multiplication of an orthogonal p-by-p matrix. Therefore, Gn,p = Vn,p/Op. On the computer, by necessity, we must
pick a representative of the equivalence class to specify a point.
The Stiefel manifold may also be deﬁned as a quotient space but arising from the orthogonal group.
Here we identify two orthogonal matrices if their ﬁrst p columns are identical or, equivalently, if they are
related by right multiplication of a matrix of the form
, where Q is an orthogonal (n−p)-by-(n−p)
Edelman, Arias, and Smith
Computational representation of subspace manifolds
Data Structure Represents
Tangents ∆
Stiefel Manifold
Y T∆= skew-symmetric
Grassmann Manifold
entire equivalence class
Therefore, Vn,p = On/On−p.
With the Stiefel manifold so represented, one has yet another
representation of the Grassmann manifold, Gn,p = On/(Op × On−p).
2.2. The Stiefel Manifold in Euclidean Space. The Stiefel manifold Vn,p may be embedded in
the np dimensional Euclidean space of n-by-p matrices. When p = 1, we simply have the sphere, while
when p = n, we have the group of orthogonal matrices known as On. These two special cases are the
easiest, and arise in numerical linear algebra the most often.
Much of this section, which consists of three subsections, is designed to be a painless and intuitive
introduction to diﬀerential geometry in Euclidean space. §2.2.1 is elementary. It derives formulas for
projections onto the tangent and normal spaces. In §2.2.2, we derive formulas for geodesics on the Stiefel
manifold in Euclidean space. We then discuss parallel translation in §2.2.3.
In the two special cases when p = 1 and p = n, the Euclidean metric and the canonical metric to be
discussed in §2.4 are the same. Otherwise they diﬀer.
2.2.1. Tangent and Normal Space. Intuitively, the tangent space at a point is the plane tangent
to the submanifold at that point, as shown in Figure 2.1. For d-dimensional manifolds, this plane is a
d-dimensional vector space with origin at the point of tangency. The normal space is the orthogonal
complement. On the sphere, tangents are perpendicular to radii, and the normal space is radial. In this
subsection, we will derive the equations for the tangent and normal spaces on the Stiefel manifold. We
also compute the projection operators onto these spaces.
Fig. 2.1. The tangent and normal spaces of an embedded or constraint manifold.
An equation deﬁning tangents to the Stiefel manifold at a point Y is easily obtained by diﬀerentiating
Y T Y = I, yielding Y T∆+ ∆T Y = 0, i.e., Y T∆is skew-symmetric. This condition imposes p(p + 1)/2
constraints on ∆, or equivalently, the vector space of all tangent vectors ∆has dimension
np −p(p + 1)
+ p(n −p).
Orthogonality Constraints
Both sides of Eq. (2.1) are useful for the dimension counting arguments that will be employed.
The normal space is deﬁned to be the orthogonal complement of the tangent space. Orthogonality
depends upon the deﬁnition of an inner product, and because in this sub-section we view the Stiefel
manifold as an embedded manifold in Euclidean space, we choose the standard inner product
ge(∆1, ∆2) = tr ∆T
in np-dimensional Euclidean space (hence the subscript e), which is also the Frobenius inner product
for n-by-p matrices. We shall also write ⟨∆1, ∆2⟩for the inner product, which may or may not be the
Euclidean one. The normal space at a point Y consists of all matrices N which satisfy
tr ∆T N = 0
for all ∆in the tangent space. It follows that the normal space is p(p + 1)/2 dimensional. It is easily
veriﬁed that if N = Y S, where S is p-by-p symmetric, then N is in the normal space. Since the dimension
of the space of such matrices is p(p + 1)/2, we see that the normal space is exactly the set of matrices
{ Y S }, where S is any p-by-p symmetric matrix.
Let Z be any n-by-p matrix. Letting sym(A) denote (A + AT )/2 and skew(A) = (A −AT )/2, it is
easily veriﬁed that at Y
πN(Z) = Y sym(Y TZ)
deﬁnes a projection of Z onto the normal space. Similarly, at Y ,
πT (Z) = Y skew(Y TZ) + (I −Y Y T )Z
is a projection of Z onto the tangent space at Y (this is also true of the canonical metric to be discussed
in §2.4). Eq. (2.4) suggests a form for the tangent space of Vn,p at Y that will prove to be particularly
useful. Tangent directions ∆at Y then have the general form,
∆= Y A + Y⊥B
= Y A + (I −Y Y T )C
where A is p-by-p skew-symmetric, B is (n −p)-by-p, C is n-by-p, B and C are both arbitrary, and Y⊥is
any n-by-(n−p) matrix such that Y Y T +Y⊥Y⊥
T = I; note that B = Y⊥
T C. The entries in the matrices A
and B parameterize the tangent space at Y with p(p −1)/2 degrees of freedom in A and p(n −p) degrees
of freedom in B, resulting in p(p −1)/2 + p(n −p) degrees of freedom as seen in Eq. (2.1).
In the special case Y = In,p ≡
(the ﬁrst p columns of the n-by-n identity matrix), called the
origin, the tangent space at Y consists of those matrices
for which A is p-by-p skew-symmetric and B is (n −p)-by-p arbitrary.
2.2.2. Embedded Geodesics. A geodesic is the curve of shortest length between two points on a
manifold. A straightforward exercise from the calculus of variations reveals that for the case of manifolds
embedded in Euclidean space the acceleration vector at each point along a geodesic is normal to the
submanifold so long as the curve is traced with uniform speed. This condition is necessary and suﬃcient.
In the case of the sphere, acceleration for uniform motion on a great circle is directed radially and therefore
normal to the surface; therefore, great circles are geodesics on the sphere. One may consider embedding
manifolds in spaces with arbitrary metrics. See Spivak [79, vol. 3, p. 4] for the appropriate generalization.
Through Eq. (2.3) for the normal space to the Stiefel manifold, it is easily shown that the geodesic
equation for a curve Y (t) on the Stiefel manifold is deﬁned by the diﬀerential equation
¨Y + Y ( ˙Y T ˙Y ) = 0.
To see this, we begin with the condition that Y (t) remains on the Stiefel manifold,
Y T Y = Ip.
Edelman, Arias, and Smith
Taking two derivatives,
Y T ¨Y + 2 ˙Y T ˙Y + ¨Y T Y = 0.
To be a geodesic, ¨Y (t) must be in the normal space at Y (t) so that
¨Y (t) + Y (t)S = 0
for some symmetric matrix S. Substitute Eq. (2.10) into (2.9) to obtain the geodesic equation Eq. (2.7).
Alternatively Eq. (2.7) could be obtained from the Euler-Lagrange equation for the calculus of variations
d(Y1, Y2) = min
(tr ˙Y T ˙Y )1/2 dt
such that Y (t1) = Y1, Y (t2) = Y2.
We identify three integrals of motion of the geodesic equation Eq. (2.7). Deﬁne
C = Y T Y,
A = Y T ˙Y ,
S = ˙Y T ˙Y .
Directly from the geodesic equation Eq. (2.7),
˙C = A + AT ,
˙A = −CS + S,
˙S = [A, S],
[A, S] = AS −SA
is the Lie bracket of two matrices. Under the initial conditions that Y is on the Stiefel manifold (C = I)
and ˙Y is a tangent (A is skew-symmetric) then the integrals of the motion have the values
A(t) = A(0),
S(t) = eAtS(0)e−At.
These integrals therefore identify a constant speed curve on the Stiefel manifold. In most diﬀerential
geometry books, the equation of motion for geodesics is written in intrinsic coordinates in terms of socalled Christoﬀel symbols which specify a quadratic form of the tangent vectors. In our formulation, the
form Γe( ˙Y , ˙Y ) = Y ˙Y T ˙Y is written compactly in extrinsic coordinates.
With these constants of the motion, we can write an integrable equation for the ﬁnal geodesic,1
Y eAt, ˙Y eAt
Y eAt, ˙Y eAt 
with integral
Y (0), ˙Y (0)
I2p,pe−At.
This is an exact closed form expression for the geodesic on the Stiefel manifold, but we will not use
this expression in our computation. Instead we will consider the non-Euclidean canonical metric on the
Stiefel manifold in §2.4.
We mention in the case of the orthogonal group (p = n), the geodesic equation is obtained simply
from A = QT ˙Q = constant, yielding the simple solution
Q(t) = Q(0)eAt.
From Eq. (2.14) it is straightforward to show that on connected components of On,
d(Q1, Q2) =
where {eiθk} are the eigenvalues of the matrix QT
1 Q2 (cf. Eq. (2.67) and §4.3).
1We thank Ross Lippert for this observation.
Orthogonality Constraints
2.2.3. Parallel Translation. In Euclidean space, we move vectors parallel to themselves simply by
moving the base of the arrow. On an embedded manifold, if we move a tangent vector to another point on
the manifold by this technique, it is generally not a tangent vector. One can, however, transport tangents
along paths on the manifold by inﬁnitesimally removing the component of the transported vector in the
normal space.
Fig. 2.2. Parallel transport in a submanifold of Euclidean space (inﬁnitesimal construction).
Figure 2.2 illustrates the idea: Imagine moving a tangent vector ∆along the curve Y (t) in such
a manner that every inﬁnitesimal step consists of a parallel displacement of ∆in the Euclidean np
dimensional space which is then followed by the removal of the normal component. If we move from
Y (0) = Y to Y (ǫ) then to ﬁrst order, our new location is Y + ǫ ˙Y .
The equation for inﬁnitesimally
removing the component generated in the normal space as we move in the direction ˙Y is obtained by
diﬀerentiating Eq. (2.3):
˙∆= −Y ( ˙Y T∆+ ∆T ˙Y )/2,
We are unaware of any closed form solution to this system of diﬀerential equations along geodesics.
By diﬀerentiation, we see that parallel transported vectors preserve the inner product. In particular,
the square length of ∆(tr ∆T∆) is preserved. Additionally, inserting ˙Y into the parallel transport equation,
one quickly sees that a geodesic always parallel transports its own tangent vector. This condition may be
taken as the deﬁnition of a geodesic.
Observing that tr ∆T∆is the sum of the squares of the singular values of ∆, we conjectured that
the individual singular values of ∆might also be preserved by parallel transport. Numerical experiments
show that this is not the case.
In the case of the orthogonal group (p = n), however, parallel translation of ∆along the geodesic
Q(t) = Q(0)eAt is straightforward.
Let ∆(t) = Q(t)B(t) be the solution of the parallel translation
˙∆= −Q( ˙QT∆+ ∆T ˙Q)/2,
where B(t) is a skew-symmetric matrix. Substituting ˙∆= ˙QB + Q ˙B and ˙Q = QA, we obtain
whose solution is B(t) = e−At/2B(0)eAt/2; therefore,
∆(t) = Q(0)eAt/2B(0)eAt/2.
These formulas may be generalized to arbitrary connected Lie groups [47, Chap. 2, Ex. A.6].
So as to arrive at the general notion of parallel transport, let us formalize what we did here. We saw
that the geodesic equation may be written
¨Y + Γe( ˙Y , ˙Y ) = 0,
Edelman, Arias, and Smith
where in the Euclidean case
Γe(∆1, ∆2) = Y (∆T
Anticipating the generalization, we interpret Γ as containing the information of the normal component
that needs to be removed. Knowing the quadratic function Γ(∆, ∆) is suﬃcient for obtaining the bilinear
function Γ(∆1, ∆2); the process is called polarization. We assume that Γ is a symmetric function of its
arguments (this is the so-called torsion-free condition), and we obtain
4Γ(∆1, ∆2) = Γ(∆1 + ∆2, ∆1 + ∆2) −Γ(∆1 −∆2, ∆1 −∆2).
For the cases we study in this paper, it is easy in practice to guess a symmetric form for Γ(∆1, ∆2) given
We will give a speciﬁc example of why this formalism is needed in §2.4. Let us mention here that the
parallel transport deﬁned in this manner is known to diﬀerential geometers as the Levi-Civita connection.
We also remark that the function Γ when written in terms of components deﬁnes the Christoﬀel symbols.
Switching to vector notation, in diﬀerential geometry texts the ith component of the function Γ(v, w)
would normally be written as P
jkvjwk, where the constants Γi
jk are called Christoﬀel symbols. We
prefer the matrix notation over the scalar notation.
2.3. Geometry of Quotient Spaces. Given a manifold whose geometry is well understood (where
there are closed form expressions for the geodesics and, perhaps also, parallel transport), there is a
very natural, eﬃcient, and convenient way to generate closed form formulas on quotient spaces of that
manifold. This is precisely the situation with the Stiefel and Grassmann manifolds, which are quotient
spaces within the orthogonal group. As just seen in the previous section, geodesics and parallel translation
on the orthogonal group are simple. We now show how the Stiefel and Grassmann manifolds inherit this
simple geometry.
2.3.1. The Quotient Geometry of the Stiefel Manifold. The important ideas here are the
notions of the horizontal and vertical spaces, the metric, and their relationship to geodesics and parallel
translation. We use brackets to denote equivalence classes. We will deﬁne these concepts using the Stiefel
manifold Vn,p = On/On−p as an example. The equivalence class [Q] is the set of all n-by-n orthogonal
matrices with the same ﬁrst p columns as Q. A point in the Stiefel manifold is the equivalence class
: Qn−p ∈On−p
that is, a point in the Stiefel manifold is a particular subset of the orthogonal matrices. Notice that in
this section we are working with equivalence classes rather than n-by-p matrices Y = QIn,p.
The vertical and horizontal spaces at a point Q are complementary linear subspaces of the tangent
space at Q. The vertical space is deﬁned to be vectors tangent to the set [Q]. The horizontal space is
deﬁned as the tangent vectors at Q orthogonal to the vertical space. At a point Q, the vertical space is
the set of vectors of the form
where C is (n −p)-by-(n −p) skew-symmetric, and we have hidden post-multiplication by the isotropy
. Such vectors are clearly tangent to the set [Q] deﬁned in Eq. (2.19). It follows that
the horizontal space at Q is the set of tangents of the form
(also hiding the isotropy subgroup), where A is p-by-p skew-symmetric. Vectors of this form are clearly
orthogonal to vertical vectors with respect to the Euclidean inner product. The matrices A and B of
Eq. (2.21) are equivalent to those of Eq. (2.5).
The signiﬁcance of the horizontal space is that it provides a representation of tangents to the quotient
space. Intuitively, movements in the vertical direction make no change in the quotient space. Therefore,
Orthogonality Constraints
the metric, geodesics, and parallel translation must all be restricted to the horizontal space. A rigorous
treatment of these intuitive concepts is given by Kobayashi and Nomizu and Chavel .
The canonical metric on the Stiefel manifold is then simply the restriction of the orthogonal group
metric to the horizontal space (multiplied by 1/2 to avoid factors of 2 later on). That is, for ∆1 and ∆2
of the form in Eq. (2.21),
gc(∆1, ∆2) = 1
1A2 + tr BT
which we shall also write as ⟨∆1, ∆2⟩. It is important to realize that this is not equal to the Euclidean
metric ge deﬁned in §2.2 (except for p = 1 or n), even though we use the Euclidean metric for the orthogonal group in its deﬁnition. The diﬀerence arises because the Euclidean metric counts the independent
coordinates of the skew-symmetric A matrix twice and those of B only once, whereas the canonical metric
counts all independent coordinates in A and B equally. This point is discussed in detail in §2.4.
Notice that the orthogonal group geodesic
Q(t) = Q(0) exp t
has horizontal tangent
˙Q(t) = Q(t)
at every point along the curve Q(t). Therefore they are curves of shortest length in the quotient space as
well, i.e., geodesics in the Grassmann manifold are given by the simple formula
Stiefel geodesics = [Q(t)],
where [Q(t)] is given by Eqs. (2.19) and (2.23). This formula will be essential for deriving an expression
for geodesics on the Stiefel manifold using n-by-p matrices in §2.4.
In a quotient space, parallel translation works in a way similar to the embedded parallel translation
discussed in §2.2.3. Parallel translation along a curve (with everywhere horizontal tangent) is accomplished
by inﬁnitesimally removing the vertical component of the tangent vector.
The equation for parallel
translation along the geodesics in the Stiefel manifold is obtained by applying this idea to Eq. (2.17),
which provides translation along geodesics for the orthogonal group. Let
be two horizontal vectors at Q = I. The parallel translation of B along the geodesic eAt is given by the
diﬀerential equation
where the subscript H denotes the horizontal component (lower right block set to zero). Note that the
Lie bracket of two horizontal vectors is not horizontal, and that the solution to Eq. (2.27) is not given by
the formula (e−At/2B(0)eAt/2)H. This is a special case of the general formula for reductive homogeneous
spaces . This ﬁrst order linear diﬀerential equation with constant coeﬃcients is integrable in closed
form, but it is an open question whether this can be accomplished with O(np2) operations.
2.3.2. The Quotient Geometry of the Grassmann Manifold. We quickly repeat this approach
for the Grassmann manifold Gn,p = On/(Op×On−p). The equivalence class [Q] is the set of all orthogonal
matrices whose ﬁrst p columns span the same subspace as those of Q. A point in the Grassmann manifold
is the equivalence class
: Qp ∈Op, Qn−p ∈On−p
Edelman, Arias, and Smith
i.e., a point in the Grassmann manifold is a particular subset of the orthogonal matrices, and the Grassmann manifold itself is the collection of all these subsets.
The vertical space at a point Q is the set of vectors of the form
where A is p-by-p skew-symmetric and C is (n −p)-by-(n −p) skew-symmetric. The horizontal space at
Q is the set of matrices of the form
Note that we have hidden post-multiplication by the isotropy subgroup
in Eqs. (2.29) and
The canonical metric on the Grassmann manifold is the restriction of the orthogonal group metric to
the horizontal space (multiplied by 1/2). Let ∆1 and ∆2 be of the form in Eq. (2.30). Then
gc(∆1, ∆2) = tr BT
As opposed to the canonical metric for the Stiefel manifold, this metric is in fact equivalent to the
Euclidean metric (up to multiplication by 1/2) deﬁned in Eq. (2.2).
The orthogonal group geodesic
Q(t) = Q(0) exp t
has horizontal tangent
˙Q(t) = Q(t)
at every point along the curve Q(t); therefore,
Grassmann geodesics = [Q(t)],
where [Q(t)] is given by Eqs. (2.28) and (2.32). This formula gives us an easy method for computing
geodesics on the Grassmann manifold using n-by-p matrices, as will be seen in §2.5.
The method for parallel translation along geodesics in the Grassmann manifold is the same as for the
Stiefel manifold, although it turns out the Grassmann manifold has additional structure that makes this
task easier. Let
be two horizontal vectors at Q = I. It is easily veriﬁed that [A, B] is in fact a vertical vector of the
form of Eq. (2.29). If the vertical component of Eq. (2.17) is inﬁnitesimally removed, we are left with the
trivial diﬀerential equation
Therefore, the parallel translation of the tangent vector Q(0)B along the geodesic Q(t) = Q(0)eAt is
simply given by the expression
τB(t) = Q(0)eAtB,
which is of course horizontal at Q(t). Here we introduce the notation τ to indicate the transport of a
vector; it is not a scalar multiple of the vector. It will be seen in §2.5 how this formula may be computed
using O(np2) operations.
Orthogonality Constraints
As an aside, if H and V represent the horizontal and vertical spaces, respectively, it may be veriﬁed
[V, V ] ⊂V,
[V, H] ⊂H,
[H, H] ⊂V.
The ﬁrst relationship follows from the fact that V is a Lie algebra, the second follows from the reductive
homogeneous space structure of the Grassmann manifold, also possessed by the Stiefel manifold, and
the third follows the symmetric space structure of the Grassmann manifold, which the Stiefel
manifold does not possess.
2.4. The Stiefel Manifold with its Canonical Metric.
2.4.1. The Canonical Metric (Stiefel). The Euclidean metric
ge(∆, ∆) = tr ∆T∆
used in §2.2 may seem natural, but one reasonable objection to its use is that it weighs the independent
degrees of freedom of the tangent vector unequally. Using the representation of tangent vectors ∆=
Y A + Y⊥B given in Eq. (2.5), it is seen that
ge(∆, ∆) = tr ATA + tr BTB.
The Euclidean metric counts the p(p + 1)/2 independent coordinates of A twice. At the origin In,p, a
more equitable metric would be gc(∆, ∆) = tr ∆T (I −1
2 tr ATA + tr BTB. To be equitable
at all points in the manifold, the metric must vary with Y according to
gc(∆, ∆) = tr ∆T (I −1
2Y Y T )∆.
This is called the canonical metric on the Stiefel manifold. This is precisely the metric derived from the
quotient space structure of Vn,p in Eq. (2.23); therefore, the formulas for geodesics and parallel translation
for the Stiefel manifold given in §2.3.1 are correct if we view the Stiefel manifold as the set of orthonormal
n-by-p matrices with the metric of Eq. (2.39). Note that if ∆= Y A + Y⊥B is a tangent vector, then
gc(∆, ∆) = 1
2 tr ATA + tr BTB, as seen previously.
2.4.2. Geodesics (Stiefel). The path length
gc( ˙Y , ˙Y )1/2 dt
may be minimized with the calculus of variations. Doing so is tedious but yields the new geodesic equation
¨Y + ˙Y ˙Y T Y + Y
 (Y T ˙Y )2 + ˙Y T ˙Y
Direct substitution into Eq. (2.41) using the fact that
(I −In,pIT
n,p)X(I −In,pIT
if X is a skew-symmetric matrix of the form
veriﬁes that the paths of the form
Y (t) = QeXtIn,p
are closed form solutions to the geodesic equation for the canonical metric.
Edelman, Arias, and Smith
We now turn to the problem of computing geodesics with algorithms of complexity O(np2). Our
current formula Y (t) = Q exp t
In,p for a geodesic is not useful. Rather we want to express
the geodesic Y (t) in terms of the current position Y (0) = Y and a direction ˙Y (0) = H. For example,
A = Y TH and we have C := BTB = HT(I −Y Y T )H. In fact the geodesic only depends on BTB rather
than B itself. The trick is to ﬁnd a diﬀerential equation for M(t) = IT
The following theorem makes clear that the computational diﬃculty inherent in computing the
geodesic is the solution of a constant coeﬃcient second order diﬀerential equation for M(t). The answer is obtained not by a diﬀerential equation solver but rather by solving the corresponding quadratic
eigenvalue problem:
Theorem 2.1. If Y (t) = Qet(
0 )In,p, with Y (0) = Y and ˙Y (0) = H, then
Y (t) = Y M(t) + (I −Y Y T )H
where M(t) is the solution to the second order diﬀerential equation with constant coeﬃcients,
M −A ˙M + CM = 0;
M(0) = Ip,
˙M(0) = A,
A = Y TH is skew-symmetric, and C = HT(I −Y Y T )H is non-negative deﬁnite.
Proof. A direct computation veriﬁes that M(t) satisﬁes Eq. (2.44). By separately considering Y T Y (t)
and (I −Y Y T )Y (t), we may derive Eq. (2.43).
The solution of the diﬀerential equation Eq. (2.44) may be obtained by solving the quadratic
eigenvalue problem
(λ2I −Aλ + C)x = 0.
Such problems are typically solved in one of three ways: (1) by solving the generalized eigenvalue problem
(2) by solving the eigenvalue problem
or (3) any equivalent problem obtained by factoring C = KTK and then solving the eigenvalue problem
Problems of this form arise frequently in mechanics, usually with A symmetric. Some discussion of
physical interpretations for skew-symmetric matrices may be found in the context of rotating machinery
 . If X is the p-by-2p matrix of eigenvectors and Λ denotes the eigenvalues, then M(t) = XeΛtZ, and
its integral is
M(t) dt = XeΛtΛ−1Z, where Z is chosen so that XZ = I and XΛZ = A.
Alternatively, the third method along with the matrix exponential may be employed:
Corollary 2.2.
Let Y and H be n-by-p matrices such that Y T Y = Ip and A = Y TH is skewsymmetric. Then the geodesic on the Stiefel manifold emanating from Y in direction H is given by the
Y (t) = Y M(t) + QN(t),
QR := K = (I −Y Y T )H
is the compact QR-decomposition of K (Q n-by-p, R p-by-p), and M(t) and N(t) are p-by-p matrices
given by the matrix exponential
Orthogonality Constraints
Note that Eq. (2.47) is easily computed by solving a 2p-by-2p skew-symmetric eigenvalue problem,
which can be accomplished eﬃciently using the SVD or algorithms specially tailored for this problem .
2.4.3. Parallel Translation (Stiefel). We now develop a notion of parallel transport that is consistent with the canonical metric. The geodesic equation takes the form ¨Y + Γ( ˙Y , ˙Y ) = 0, where, from
Eq. (2.41), it is seen that the Christoﬀel function for the canonical metric is
Γc(∆, ∆) = ∆∆T Y + Y ∆T (I −Y Y T )∆.
By polarizing we obtain the result
Γc(∆1, ∆2) = 1
2(I −Y Y T )∆1
1 (I −Y Y T )∆2
Parallel transport is given by the diﬀerential equation
˙∆+ Γc(∆, ˙Y ) = 0,
which is equivalent to Eq. (2.27). As stated after this equation, we do not have an O(np2) method to
compute ∆(t).
2.4.4. The Gradient of a Function (Stiefel). Both conjugate gradient and Newton’s method
require a computation of the gradient of a function, which depends upon the choice of metric. For a
function F(Y ) deﬁned on the Stiefel manifold, the gradient of F at Y is deﬁned to be the tangent vector
∇F such that
Y∆= gc(∇F, ∆) ≡tr(∇F)T (I −1
for all tangent vectors ∆at Y , where FY is the n-by-p matrix of partial derivatives of F with respect to
the elements of Y , i.e.,
(FY )ij = ∂F
Solving Eq. (2.51) for ∇F such that Y T (∇F) = skew-symmetric yields
∇F = FY −Y F T
Eq. (2.53) may also be derived by diﬀerentiating F
, where Y (t) is the Stiefel geodesic given by
Eq. (2.45).
Edelman, Arias, and Smith
2.4.5. The Hessian of a Function (Stiefel). Newton’s method requires the Hessian of a function,
which depends upon the choice of metric. The Hessian of a function F(Y ) deﬁned on the Stiefel manifold
is deﬁned as the quadratic form
Hess F(∆, ∆) = d2
where Y (t) is a geodesic with tangent ∆, i.e., ˙Y (0) = ∆. Applying this deﬁnition to F(Y ) and Eq. (2.45)
yields the formula
Hess F(∆1, ∆2) = FY Y (∆1, ∆2) + 1
Y∆1Y T + Y T∆1F T
 (Y T FY + F T
where Π = I −Y Y T , FY is deﬁned in Eq. (2.52), and the notation FY Y (∆1, ∆2) denotes the scalar
ij,kl(FY Y )ij,kl(∆1)ij(∆2)kl, where
(FY Y )ij,kl =
This formula may also readily be obtained by using Eq. (2.50) and the formula
Hess F(∆1, ∆2) = FY Y (∆1, ∆2) −tr F T
Y Γc(∆1, ∆2).
For Newton’s method, we must determine the tangent vector ∆such that
Hess F(∆, X) = ⟨−G, X⟩
for all tangent vectors X,
where G = ∇F. Recall that ⟨, ⟩≡gc( , ) in this context. We shall express the solution to this linear
equation as ∆= −Hess−1 G, which may be expressed as the solution to the linear problem
FY Y (∆) −Y skew(F T
Y∆) −skew(∆F T
2Π∆Y T FY = −G,
Y T∆= skew-symmetric, where skew(X) = (X −XT )/2 and the notation FY Y (∆) means the unique
tangent vector satisfying the equation
FY Y (∆, X) = ⟨FY Y (∆), X⟩
for all tangent vectors X.
Example problems are considered in §3.
2.5. The Grassmann Manifold with its Canonical Metric. A quotient space representation
of the Grassmann manifold was given in §2.3.2; however, for computations we prefer to work with n-by-p
orthonormal matrices Y . When performing computations on the Grassmann manifold, we will use the
n-by-p matrix Y to represent an entire equivalence class
[Y ] = { Y Qp : Qp ∈Op },
i.e., the subspace spanned by the columns of Y . Any representative of the equivalence class will do.
We remark that an alternative strategy is to represent points on the Grassmann manifold with projection matrices Y Y T . There is one unique such matrix corresponding to each point on the Grassmann
manifold. On ﬁrst thought it may seem foolish to use n2 parameters to represent a point on the Grassmann
manifold (which has dimension p(n−p)), but in certain ab initio physics computations , the projection
matrices Y Y T that arise in practice tend to require only O(n) parameters for their representation.
Orthogonality Constraints
Returning to the n-by-p representation of points on the Grassmann manifold, the tangent space is
easily computed by viewing the Grassmann manifold as the quotient space Gn,p = Vn,p/Op. At a point
Y on the Stiefel manifold then, as seen in Eq. (2.5), tangent vectors take the form ∆= Y A + Y⊥B, where
A is p-by-p skew-symmetric, B is (n −p)-by-p, and Y⊥is any n-by-(n −p) matrix such that (Y, Y⊥) is
orthogonal. From Eq. (2.61) it is clear that the vertical space at Y is the set of vectors of the form
therefore, the horizontal space at Y is the set of vectors of the form
Because the horizontal space is equivalent to the tangent space of the quotient, the tangent space of the
Grassmann manifold at [Y ] is given by all n-by-p matrices ∆of the form in Eq. (2.63) or, equivalently,
all n-by-p matrices ∆such that
Physically, this corresponds to directions free of rotations mixing the basis given by the columns of Y .
We already saw in §2.3.2 that the Euclidean metric is in fact equivalent to the canonical metric for
the Grassmann manifold. That is, for n-by-p matrices ∆1 and ∆2 such that Y T∆i = 0 (i = 1, 2),
gc(∆1, ∆2) = tr ∆T
2Y Y T )∆2,
= ge(∆1, ∆2).
2.5.1. Geodesics (Grassmann). A formula for geodesics on the Grassmann manifold was
given via Eq. (2.32); the following theorem provides a useful method for computing this formula using
n-by-p matrices.
Theorem 2.3. If Y (t) = Qet( 0
0 )In,p, with Y (0) = Y and ˙Y (0) = H, then
Y (t) = ( Y V
where UΣV T is the compact singular value decomposition of H.
Proof 1. It is easy to check that either formulation for the geodesic satisﬁes the geodesic equation
¨Y + Y ( ˙Y T ˙Y ) = 0, with the same initial conditions.
Proof 2. Let B = (U1, U2)
V T be the singular value decomposition of B (U1 n-by-p, U2 p-by-(n−p),
Σ and V p-by-p). A straightforward computation involving the partitioned matrix
veriﬁes the theorem.
A subtle point in Eq. (2.65) is that if the rightmost V T is omitted, then we still have a representative
of the same equivalence class as Y (t); however, due to consistency conditions along the equivalent class
[Y (t)], the tangent (horizontal) vectors that we use for computations must be altered in the same way.
This amounts to post-multiplying everything by V , or for that matter, any p-by-p orthogonal matrix.
The path length between Y0 and Y (t) (distance between subspaces) is given by 
 Y (t), Y0
= t∥H∥F = t
where σi are the diagonal elements of Σ. (Actually, this is only true for t small enough to avoid the issue
of conjugate points, e.g., long great circle routes on the sphere.) An interpretation of this formula in
terms of the CS decomposition and principal angles between subspaces is given in §4.3.
Edelman, Arias, and Smith
2.5.2. Parallel Translation (Grassmann). A formula for parallel translation along geodesics of
complexity O(np2) can also be derived:
Theorem 2.4. Let H and ∆be tangent vectors to the Grassmann manifold at Y . Then the parallel
translation of ∆along the geodesic in the direction ˙Y (0) = H [Eq. (2.65)] is
U T + (I −UU T)
computation
Eq. (2.16).
Proof 2. Parallel translation of ∆is given by the expression
τ∆(t) = Q exp t
(which follows from Eq. (2.37)), where Q = (Y, Y⊥), H = Y⊥A, and ∆= Y⊥B. Decomposing
as in Eq. (2.66) (note well that A has replaced B), a straightforward computation veriﬁes the theorem.
2.5.3. The Gradient of a Function (Grassmann). We must compute the gradient of a function
F(Y ) deﬁned on the Grassmann manifold. Similarly to §2.4.4, the gradient of F at [Y ] is deﬁned to be
the tangent vector ∇F such that
Y∆= gc(∇F, ∆) ≡tr(∇F)T ∆
for all tangent vectors ∆at Y , where FY is deﬁned by Eq. (2.52). Solving Eq. (2.69) for ∇F such that
Y T (∇F) = 0 yields
∇F = FY −Y Y T FY .
Eq. (2.70) may also be derived by diﬀerentiating F
, where Y (t) is the Grassmann geodesic given
by Eq. (2.65).
2.5.4. The Hessian of a Function (Grassmann). Applying the deﬁnition for the Hessian of F(Y )
given by Eq. (2.54) in the context of the Grassmann manifold yields the formula
Hess F(∆1, ∆2) = FY Y (∆1, ∆2) −tr(∆T
1 ∆2Y TFY ),
where FY and FY Y are deﬁned in §2.4.5. For Newton’s method, we must determine ∆= −Hess−1 G
satisfying Eq. (2.58), which for the Grassmann manifold is expressed as the linear problem
FY Y (∆) −∆(Y T FY ) = −G,
Y T∆= 0, where FY Y (∆) denotes the unique tangent vector satisfying Eq. (2.60) for the Grassmann
manifold’s canonical metric.
Example problems are considered in §3.
2.6. Conjugate Gradient on Riemannian Manifolds. As demonstrated by Smith , the
beneﬁts of using the conjugate gradient algorithm for unconstrained minimization can be carried over to
minimization problems constrained to Riemannian manifolds by a covariant translation of the familiar
operations of computing gradients, performing line searches, the computation of Hessians, and carrying
vector information from step to step in the minimization process.
In this section we will review the
ideas in and then in the next section we formulate concrete algorithms for conjugate gradient on
the Stiefel and Grassmann manifolds. Here one can see how the geometry provides insight into the true
diﬀerence among the various formulas that are used in linear and nonlinear conjugate gradient algorithms.
Orthogonality Constraints
Figure 3.1 sketches the conjugate gradient algorithm in ﬂat space and Figure 3.2 illustrates the
algorithm on a curved space. An outline for the iterative part of the algorithm (in either ﬂat or curved
space) goes as follows: at the (k −1)st iterate xk−1, step to xk, the minimum of f along the geodesic in
the direction Hk−1, compute the gradient Gk = ∇f(xk) at this point, choose the new search direction to
be a combination of the old search direction and the new gradient:
Hk = Gk + γkτHk−1,
and iterate until convergence. Note that τHk−1 in Eq. (2.73) is the parallel translation of the vector
Hk−1 deﬁned in §2.2.3, which in this case is simply the direction of the geodesic (line) at the point xk
(see Figure 3.2). Also note the important condition that xk is a minimum point along the geodesic:
⟨Gk, τHk−1⟩= 0.
Let us begin our examination of the choice of γk in ﬂat space before proceeding to arbitrary manifolds.
Here parallel transport is trivial so that
Hk = Gk + γkHk−1.
In both linear and an idealized version of nonlinear conjugate gradient, γk may be determined by the
exact conjugacy condition for the new search direction:
fxx(Hk, Hk−1) = 0,
i.e., the old and new search direction must be conjugate with respect to the Hessian of f. (With fxx = A,
the common notation [45, page 523] for the conjugacy condition is pT
k−1Apk = 0.) The formula for γk is
Exact Conjugacy:
γk = −fxx(Gk, Hk−1)/fxx(Hk−1, Hk−1).
The standard trick to improve the computational eﬃciency of linear conjugate gradient is to use a
formula relating a ﬁnite diﬀerence of gradients to the Hessian times the direction (rk −rk−1 = −αkApk
as in ). In our notation,
⟨Gk −Gk−1, ·⟩≈αfxx(·, Hk−1),
where α = ∥xk −xk−1∥/∥Hk−1∥.
The formula is exact for linear conjugate gradient on ﬂat space, otherwise it has the usual error in ﬁnite
diﬀerence approximations. By applying the ﬁnite diﬀerence formula Eq. (2.76) in both the numerator
and denominator of Eq. (2.75), and also applying Eq. (2.74) twice (once with k and once with k −1), one
obtains the formula
Polak-Ribi`ere:
γk = ⟨Gk −Gk−1, Gk⟩/⟨Gk−1, Gk−1⟩.
Therefore the Polak-Ribi´ere formula is the exact formula for conjugacy through the Hessian, where one
uses a diﬀerence of gradients as a ﬁnite diﬀerence approximation to the second derivative. If f(x) is well
approximated by a quadratic function, then ⟨Gk−1, Gk⟩≈0, and we obtain
Fletcher-Reeves:
γk = ⟨Gk, Gk⟩/⟨Gk−1, Gk−1⟩.
For arbitrary manifolds, the Hessian is the second derivative along geodesics. In diﬀerential geometry
it is the second covariant diﬀerential of f. Here are the formulas:
Exact Conjugacy:
γk = −Hessf(Gk, τHk−1)/ Hessf(τHk−1, τHk−1)
Polak-Ribi`ere:
γk = ⟨Gk −τGk−1, Gk⟩/⟨Gk−1, Gk−1⟩
Fletcher-Reeves:
γk = ⟨Gk, Gk⟩/⟨Gk−1, Gk−1⟩
which may be derived from the ﬁnite diﬀerence approximation to the Hessian,
⟨Gk −τGk−1, ·⟩≈α Hessf(·, τHk−1),
α = d(xk, xk−1)/∥Hk−1∥.
Asymptotic analyses appear in §3.6.
Edelman, Arias, and Smith
3. Geometric Optimization Algorithms. The algorithms presented here are our answer to the
question: “What does it mean to perform the Newton and conjugate gradient methods on the Stiefel and
Grassmann manifolds?” Though these algorithms are idealized, they are of identical complexity up to
small constant factors with the best known algorithms. In particular, no diﬀerential equation routines are
used. It is our hope that in the geometrical algorithms presented here, the reader will recognize elements
of any algorithm that accounts for orthogonality constraints. These algorithms are special cases of the
Newton and conjugate gradient methods on general Riemannian manifolds. If the objective function is
nondegenerate, then the algorithms are guaranteed to converge quadratically .
Fig. 3.1. Conjugate gradient in ﬂat space.
Fig. 3.2. Conjugate gradient in curved space.
Orthogonality Constraints
3.1. Newton’s Method on the Grassmann Manifold. In ﬂat space, Newton’s method simply
updates a vector by subtracting the gradient vector pre-multiplied by the inverse of the Hessian. The
same is true on the Grassmann manifold (or any Riemannian manifold for that matter) of p-planes in
n-dimensions, with interesting modiﬁcations. Subtraction is replaced by following a geodesic path. The
gradient is the usual one (which must be tangent to the constraint surface), and the Hessian is obtained
by twice diﬀerentiating the function along a geodesic. We show in §4.9 that this Hessian is related to the
Hessian of the Lagrangian; the two Hessians arise from the diﬀerence between the intrinsic and extrinsic
viewpoints. It may be suspected that following geodesics may not be computationally feasible, but because
we exploit the structure of the constraint surface, this operation costs O(np2), which is required even for
traditional algorithms for the eigenvalue problem—our simplest example.
Let F(Y ) be a smooth function on the Grassmann manifold, i.e., F(Y ) = F(Y Q) for any p-by-p
orthogonal matrix Q, where Y is an n-by-p matrix such that Y T Y = Ip.
We compute formulas for
FY and FY Y (∆) using the deﬁnitions given in §2.5.4. Newton’s method for minimizing F(Y ) on the
Grassmann manifold is:
Newton’s Method for Minimizing F(Y ) on the Grassmann Manifold
• Given Y such that Y T Y = Ip,
◦Compute G = FY −Y Y T FY .
◦Compute ∆= −Hess−1 G such that Y T∆= 0 and
FY Y (∆) −∆(Y T FY ) = −G.
• Move from Y in direction ∆to Y (1) using the geodesic formula
Y (t) = Y V cos(Σt)V T + U sin(Σt)V T
where UΣV T is the compact singular value decomposition of ∆(meaning U is n-by-p and both
Σ and V are p-by-p).
The special case of minimizing F(Y ) =
2 tr Y TAY (A n-by-n symmetric) gives the geometrically
correct Newton method for the symmetric eigenvalue problem. In this case FY = AY and FY Y (∆) =
(I −Y Y T )A∆. The resulting algorithm requires the solution of a Sylvester equation. It is the idealized
algorithm whose approximations include various forms of Rayleigh quotient iteration, inverse iteration,
a number of Newton style methods for invariant subspace computation, and the many variations of
Davidson’s eigenvalue method. These ideas are discussed in §4.1 and 4.8.
Edelman, Arias, and Smith
3.2. Newton’s Method on the Stiefel Manifold. Newton’s method on the Stiefel manifold is
conceptually equivalent to the Grassmann manifold case. Let Y be an n-by-p matrix such that Y T Y = Ip,
and let F(Y ) be a smooth function of Y , without the homogeneity condition imposed for the Grassmann
manifold case. Compute formulas for FY and FY Y (∆) using the deﬁnitions given in §2.4.5. Newton’s
method for minimizing F(Y ) on the Stiefel manifold is:
Newton’s Method for Minimizing F(Y ) on the Stiefel Manifold
• Given Y such that Y T Y = Ip,
◦Compute G = FY −Y F T
◦Compute ∆= −Hess−1 G such that Y T∆= skew-symmetric and
FY Y (∆) −Y skew(F T
Y∆) −skew(∆F T
2Π∆Y T FY = −G,
where skew(X) = (X −XT )/2 and Π = I −Y Y T .
• Move from Y in direction ∆to Y (1) using the geodesic formula
Y (t) = Y M(t) + QN(t)
where QR is the compact QR decomposition of (I −Y Y T )∆(meaning Q is n-by-p and R
is p-by-p), A = Y T∆, and M(t) and N(t) are p-by-p matrices given by the 2p-by-2p matrix
exponential
For the special case of minimizing F(Y ) = 1
2 tr Y TAY N (A n-by-n symmetric, N p-by-p symmetric)
 , FY = AY N and FY Y (∆) = A∆N −Y N∆TAY . Note that if N is not a multiple of the identity,
then F(Y ) does not have the homogeneity condition required for a problem on the Grassmann manifold.
If N = diag(p, p −1, . . . , 1), then the optimum solution to maximizing F over the Stiefel manifold yields
the eigenvectors corresponding to the p largest eigenvalues.
For the orthogonal Procrustes problem , F(Y ) =
F (A m-by-n, B m-by-p, both
arbitrary), FY = ATAY −AT B and FY Y (∆) = ATA∆−Y ∆TATAY . Note that Y T FY Y (∆) = skewsymmetric.
Orthogonality Constraints
3.3. Conjugate Gradient Method on the Grassmann Manifold. Conjugate gradient techniques are considered because they are easy to implement, have low storage requirements, and provide
superlinear convergence in the limit. The Newton equations may be solved with ﬁnitely many steps of
linear conjugate gradient; each nonlinear conjugate gradient step, then, approximates a Newton step. In
ﬂat space, the nonlinear conjugate gradient method performs a line search by following a direction determined by conjugacy with respect to the Hessian. On Riemannian manifolds, conjugate gradient performs
minimization along geodesics with search directions deﬁned using the Hessian described above .
Both algorithms approximate Hessian conjugacy with a subtle formula involving only the gradient directions, resulting in an algorithm that captures second derivative information by computing only ﬁrst
derivatives. To “communicate” information from one iteration to the next, tangent vectors must parallel
transport along geodesics. Conceptually, this is necessary because, unlike ﬂat space, the deﬁnition of
tangent vectors changes from point to point.
Using these ideas and formulas developed in §3.1, the conjugate gradient method on the Grassmann
manifold is:
Conjugate Gradient for Minimizing F(Y ) on the Grassmann Manifold
• Given Y0 such that Y T
0 Y0 = I, compute G0 = FY0 −Y0Y T
0 FY0 and set H0 = −G0.
• For k = 0, 1, . . . ,
◦Minimize F
over t where
Y (t) = Y V cos(Σt)V T + U sin(Σt)V T
and UΣV T is the compact singular value decomposition of Hk.
◦Set tk = tmin and Yk+1 = Yk(tk).
◦Compute Gk+1 = FYk+1 −Yk+1Y T
◦Parallel transport tangent vectors Hk and Gk to the point Yk+1:
τHk = (−YkV sin Σtk + U cos Σtk)ΣV T ,
τGk = Gk −
 YkV sin Σtk + U(I −cos Σtk)
◦Compute the new search direction
Hk+1 = −Gk+1 + γkτHk
γk = ⟨Gk+1 −τGk, Gk+1⟩
and ⟨∆1, ∆2⟩= tr ∆T
◦Reset Hk+1 = −Gk+1 if k + 1 ≡0 mod p(n −p).
3.4. Conjugate Gradient Method on the Stiefel Manifold. As with Newton’s method, conjugate gradient on the two manifolds is very similar. One need only replace the deﬁnitions of tangent
vectors, inner products, geodesics, gradients, and parallel translation. Geodesics, gradients, and inner
products on the Stiefel manifold are given in §2.4. For parallel translation along geodesics on the Stiefel
manifold, we have no simple, general formula comparable to Eq. (3.2). Fortunately, a geodesic’s tangent
direction is parallel, so a simple formula for τHk comparable to Eq. (3.1) is available, but a formula for
τGk is not. In practice, we recommend setting τGk := Gk and ignoring the fact that τGk will not be
tangent at the point Yk+1. Alternatively, setting τGk := 0 (also not parallel) results in a Fletcher-Reeves
conjugate gradient formulation. As discussed in the next section, either approximation does not aﬀect
the superlinear convergence property of the conjugate gradient method.
The conjugate gradient method on the Stiefel manifold is:
Edelman, Arias, and Smith
Conjugate Gradient for Minimizing F(Y ) on the Stiefel Manifold
• Given Y0 such that Y T
0 Y0 = I, compute G0 = FY0 −Y0F T
Y0Y0 and set H0 = −G0.
• For k = 0, 1, . . . ,
◦Minimize F
over t where
Yk(t) = YkM(t) + QN(t),
QR is the compact QR decomposition of (I −YkY T
k )Hk, A = Y T
k Hk, and M(t) and N(t) are
p-by-p matrices given by the 2p-by-2p matrix exponential appearing in Newton’s method
on the Stiefel manifold in §3.2.
◦Set tk = tmin and Yk+1 = Yk(tk).
◦Compute Gk+1 = FYk+1 −Yk+1F T
◦Parallel transport tangent vector Hk to the point Yk+1:
τHk = HkM(tk) −YkRT N(tk).
As discussed above, set τGk := Gk or 0, which is not parallel.
◦Compute the new search direction
Hk+1 = −Gk+1 + γkτHk
γk = ⟨Gk+1 −τGk, Gk+1⟩
and ⟨∆1, ∆2⟩= tr ∆T
2Y Y T )∆2.
◦Reset Hk+1 = −Gk+1 if k + 1 ≡0 mod p(n −p) + p(p −1)/2.
Orthogonality Constraints
3.5. Numerical Results and Asymptotic Behavior.
3.5.1. Trace Maximization on the Grassmann Manifold. The convergence properties of the
conjugate gradient and Newton’s methods applied to the trace maximization problem F(Y ) = tr Y TAY
are shown in Figure 3.3, as well as the convergence of an approximate conjugate gradient method and
the Rayleigh quotient iteration for comparison. This example shows trace maximization on G5,3, i.e.,
3-dimensional subspaces in 5 dimensions. The distance between the subspace and the known optimum
subspace is plotted versus the iteration number, where the distance in radians is simply the square root
of the sum of squares of the principal angles between the subspaces. The dimension of this space equals
3(5 −3) = 6; therefore, a conjugate gradient algorithm with resets should at least double in accuracy
every six iterations.
Newton’s method, which is cubically convergent for this example (this point is
discussed in §4.1), should triple in accuracy every iteration. Variable precision numerical software is used
to demonstrate the asymptotic convergence properties of these algorithms.
ITERATIONS
ERROR (rad)
CG (Polak−Ribière)
CG (Fletcher−Reeves)
APP. CG (Polak−Ribière)
APP. CG (A−Conjugacy)
GRASSMANN NEWTON
Convergence of the conjugate gradient and Newton’s method for trace maximization on the Grassmann
manifold G5,3. The error (in radians) is the arc length distance between the solution and the subspace at the ith iterate
[Eq. (2.67) and §4.3]. Quadratic convergence of CG is evident, as is cubic convergence of Newton’s method, which is a
special property of this example.
The thick black curve (CG-1) shows the convergence of the conjugate gradient (CG) algorithm using
the Polak-Ribi`ere formula. The accuracy of this algorithm is at least doubled between the ﬁrst and sixth
and the seventh and twelfth iterations, demonstrating this method’s superlinear convergence. Newton’s
method is applied to the twelfth CG iterate, which results in a tripling of the accuracy and demonstrates
cubic convergence of Newton’s method, shown by the dashed thick black curve (NT-1).
The thin black curve (CG-2) shows CG convergence using the Fletcher-Reeves formula
γk = ⟨Gk+1, Gk+1⟩/⟨Gk, Gk⟩.
As discussed below, this formula diﬀers from the Polak-Ribi`ere formula by second order and higher
terms, so it must also have superlinear convergence. The accuracy of this algorithm more than doubles
between the ﬁrst and sixth, seventh and twelfth, and thirteenth and eighteenth iterations, demonstrating
this fact.
Edelman, Arias, and Smith
The algorithms discussed above are actually performed on the constraint surface, but extrinsic approximations to these algorithms are certainly possible. By perturbation analysis of the metric given below, it
can be shown that the CG method diﬀers from its ﬂat space counterpart only by cubic and higher terms
close to the solution; therefore, a ﬂat space CG method modiﬁed by projecting search directions to the
constraint’s tangent space will converge superlinearly. This is basically the method proposed by Bradbury
and Fletcher and others for the single eigenvector case. For the Grassmann (invariant subspace) case,
we have performed line searches of the function φ(t) = tr Q(t)TAQ(t), where Q(t)R(t) := Y + t∆is the
compact QR decomposition, and Y T∆= 0. The QR decomposition projects the solution back to the
constraint surface at every iteration. Tangency of the search direction at the new point is imposed via
the projection I −Y Y T .
The thick gray curve (CG-3) illustrates the superlinear convergence of this method when the Polak-
Ribi`ere formula is used. The Fletcher-Reeves formula yields similar results. In contrast, the thin gray curve
(CG-4) shows convergence when conjugacy through the matrix A is used, i.e., γk = −(HT
kAGk+1)/(HT
which has been proposed by several authors [67, Eq. (5)], [19, Eq. (32)], [36, Eq. (20)]. This method cannot be expected to converge superlinearly because the matrix A is in fact quite diﬀerent from the true
Hessian on the constraint surface. This issue is discussed further in §4.4.
CONJUGATE GRADIENT
STEEPEST DESCENT
Convergence of the conjugate gradient and Newton’s method for the orthogonal Procrustes problem on
the Stiefel manifold V5,3.
The error is the Frobenius norm between the ith iterate and the known solution.
convergence of the CG and Newton methods is evident. The Newton iterates correspond to those of Table 3.1.
To compare the performance of Newton’s method to the Rayleigh quotient iteration (RQI), which
approximates Newton’s method to high order (or vice versa), RQI is applied to the approximate CG
method’s twelfth iterate, shown by the dashed thick gray curve (NT-2).
3.5.2. Orthogonal Procrustes Problem on the Stiefel Manifold. The orthogonal Procrustes
problem 
Y ∈Vn,p ∥AY −B∥F
A, B given matrices,
is a minimization problem deﬁned on the Stiefel manifold that has no known analytical solution for
p diﬀerent from 1 or n. To ensure that the objective function is smooth at optimum points, we shall
consider the equivalent problem
Orthogonality Constraints
∥Yi −ˆY ∥F
2.68 × 10−01
0.98341252163956 −0.09749309852408 −0.06630579165572
0.08482117605077
0.99248149019173 −0.02619408666845
0.08655810575052
0.02896396566088
0.98816425471159
0.01388126419090
0.00902267322408
0.00728525462855
0.13423928340551
0.06749272129685 −0.13563090573981
6.71 × 10−02
0.99954707914921
0.01554828497046
0.00423211303447
−0.01656743168179
0.99905154070826
0.01216605832969
−0.00306529752246 −0.01070234416262
0.99915251911577
−0.00910501510207 −0.01286811040265
0.00924631200657
−0.02321334579158 −0.03706941336228
0.03798454294671
1.49 × 10−02
0.99993878247585
0.00296823825310
0.00486487784745
−0.00301651579786
0.99998521441661
0.00192519989544
−0.00479673956404 −0.00191288709538
0.99996440819180
−0.00311307788732 −0.00157358730922
0.00121316839587
−0.00897953054292 −0.00382429023234
0.00650669969719
9.77 × 10−05
0.99999999888990
0.00000730457866 −0.00003211124313
−0.00000730341460
0.99999999951242
0.00000603747062
0.00003210887572 −0.00000603508216
0.99999999682824
0.00000457898008 −0.00001136276061
0.00002209393458
0.00003339025497 −0.00002750041840
0.00006919392999
4.81 × 10−08
1.00000000000000
0.00000000813187
0.00000001705718
−0.00000000813187
1.00000000000000
0.00000000613007
−0.00000001705718 −0.00000000613007
1.00000000000000
−0.00000001001345 −0.00000000397730
0.00000000429327
−0.00000002903373 −0.00000000827864
0.00000002197399
2.07 × 10−15
1.00000000000000
0.00000000000000
0.00000000000000
0.00000000000000
1.00000000000000
0.00000000000000
0.00000000000000
0.00000000000000
1.00000000000000
0.00000000000000
0.00000000000000
0.00000000000000
0.00000000000000
0.00000000000000
0.00000000000000
0.59792470347241 −1.60148995048070
1.29611959631725
0.00742708895676 −0.09653196026400
−0.34991267564713
1.03005546700300
0.38145454055699
0.14195063498923 −0.16309797180034
0.16783050038338
0.51739189509778 −0.42204935150912
1.75394028742695 −0.63865179066515
0.24927536521443 −1.34694675520019
0.92362255783368
0.62648865033822 −0.31561702752866
−0.24846337483192 −0.44239067350975 −1.52598136000449
0.89515519875598
0.87362106204727
Newton’s method applied to the orthogonal Procrustes problem on the Stiefel manifold using the MATLAB code given
in this section. The matrix A is given below the numerical results, and B = AI5,3. The quadratic convergence of Newton’s
method, shown by the Frobenius norm of the diﬀerence between Yi and ˆY = I5,3, is evident. This convergence is illustrated
in Figure 3.4.
It is clear from this example that the diﬀerence Yi −ˆY approaches a tangent vector at ˆY = In,p, i.e.,
ˆY T (Yi −ˆY ) →skew-symmetric.
Derivatives of this function appear at the end of §3.2. MATLAB code for Newton’s method applied
to this problem appears below. Convergence of this algorithm for the case V5,3 and test matrices A and
B is illustrated in Table 3.1 and Figure 3.4. The quadratic convergence of Newton’s method and the
conjugate gradient algorithm is evident. The dimension of V5,3 equals 3(3 −1)/2 + 6 = 9; therefore, the
accuracy of the conjugate gradient should double every nine iterations, as it is seen to do in Figure 3.4.
Note that the matrix B is chosen such that a trivial solution ˆY = In,p to this test optimization problem
MATLAB Code for Procrustes Problem on the Stiefel Manifold
n = 5; p = 3;
A = randn(n);
B = A*eye(n,p);
Y0 = eye(n,p);
% Known solution Y0
H = 0.1*randn(n,p);
H = H - Y0*(H’*Y0);
% small tangent vector H at Y0
Y = stiefgeod(Y0,H);
% Initial guess Y (close to know solution Y0)
% Newton iteration (demonstrate quadratic convergence)
d = norm(Y-Y0,’fro’)
Edelman, Arias, and Smith
while d > sqrt(eps)
Y = stiefgeod(Y,procrnt(Y,A,B));
d = norm(Y-Y0,’fro’)
function stiefgeod
function [Yt,Ht] = stiefgeod(Y,H,t)
%STIEFGEOD Geodesic on the Stiefel manifold.
STIEFGEOD(Y,H) is the geodesic on the Stiefel manifold
emanating from Y in direction H, where Y’*Y = eye(p), Y’*H =
skew-hermitian, and Y and H are n-by-p matrices.
STIEFGEOD(Y,H,t) produces the geodesic step in direction H scaled
[Yt,Ht] = STIEFGEOD(Y,H,t) produces the geodesic step and the
geodesic direction.
[n,p] = size(Y);
if nargin < 3, t = 1; end
A = (A - A’)/2;
% Ensure skew-symmetry
[Q,R] = qr(H - Y*A,0);
MN = expm(t*[A,-R’;R,zeros(p)]);
MN = MN(:,1:p);
Yt = Y*MN(1:p,:) + Q*MN(p+1:2*p,:);
% Geodesic from Eq. (2.45)
if nargout > 1, Ht = H*MN(1:p,:) - Y*(R’*MN(p+1:2*p,:)); end
% Geodesic direction from Eq. (3.3)
function procrnt
function H = procrnt(Y,A,B)
%PROCRNT Newton Step on Stiefel Manifold for 1/2*norm(A*Y-B,’fro’)^2.
H = PROCRNT(Y,A,B) computes the Newton step on the Stiefel manifold
for the function 1/2*norm(A*Y-B,’fro’)^2, where Y’*Y = eye(size(Y,2)).
[n,p] = size(Y);
AA = A’*A;
FY = AA*Y - A’*B;
YFY = Y’*FY;
G = FY - Y*YFY’;
% Linear conjugate gradient to solve a Newton step
dimV = p*(p-1)/2 + p*(n-p);
% == dim Stiefel manifold
% This linear CG code is modified directly from Golub and Van Loan 
H = zeros(size(Y));
P0 = zeros(size(Y));
for k=1:dimV
normR1 = sqrt(stiefip(Y,R1,R1));
if normR1 < prod(size(Y))*eps, break; end
if k == 1, beta = 0; else, beta = (normR1/normR0)^2; end
P = R1 + beta*P;
FYP = FY’*P;
YP = Y’*P;
LP = AA*P - Y*(P’*AA*Y) ...
% Linear operation on P
- Y*((FYP-FYP’)/2) - (P*YFY’-FY*YP’)/2 - (P-Y*YP)*(YFY/2);
alpha = normR1^2/stiefip(Y,P,LP);
H = H + alpha*P;
normR0 = normR1;
R1 = R1 - alpha*LP;
function stiefip
function ip = stiefip(Y,A,B)
%STIEFIP Inner product (metric) for the Stiefel manifold.
ip = STIEFIP(Y,A,B) returns trace(A’*(eye(n)-1/2*Y*Y’)*B),
where Y’*Y = eye(p), Y’*A & Y’*B = skew-hermitian, and Y, A,
and B are n-by-p matrices.
ip = sum(sum(conj(A).*(B - Y*((Y’*B)/2))));
% Canonical metric from Eq. (2.39)
Orthogonality Constraints
3.6. Convergence Rates of Approximate Methods. The algorithms presented in the previous
sections are idealized in that geometrically natural ideas such as geodesics and parallel translation are
used in their deﬁnitions. However, approximations can yield quadratic rates of convergence. In the limit,
the Riemannian algorithms approach their Euclidean counterparts in the tangent plane of the solution
point. A perturbation analysis shows which terms are necessary and which terms are not necessary to
achieve quadratic convergence. The following argument holds for any Riemannian manifold, and therefore
applies to either the Grassmann or Stiefel manifold case.
Consider the CG method applied to a function F(Y ) starting at a point Y within distance ǫ (small)
of the solution ˆY . For a manifold of dimension d, we must perform a sequence of d steps that take us
within distance O(ǫ2) of the solution ˆY . The Riemannian CG method
Hnew = −Gnew + γτHold,
γ = ⟨Gnew −τGold, Gnew⟩
Ynew = Y (tmin),
Y (0) = Yold,
˙Y (0) = Hnew;
does this, but we wish to approximate this procedure. Within a ball of size O(ǫ) around ˆY , these quantities
have sizes of the following orders:
G, H (new or old)
∥G∥2, ∥H∥2 (new or old)
⟨τGold, Gnew⟩
Also, by perturbation analysis of the Riemannian metric [18, 79, Vol. 2, Chap. 4, Props. 1 and 6], we have
Y (ǫ) = Y (0) + ǫ∆+ O(ǫ3)
τG(ǫ) = G + O(ǫ2)
⟨, ⟩= I + O(ǫ2)
where Y (ǫ) is a geodesic in direction ∆, τG(ǫ) is parallel translation of G along Y (ǫ), and the last
approximation is valid for an orthonormal basis of the tangent plane at Y (ǫ∆) and I is the identity.
Inserting these asymptotics into the formulas for the CG method show that near the solution, eliminating the Riemannian terms gives O(ǫ3) perturbations of the CG sequence, and therefore does not aﬀect the
quadratic rate of convergence. Furthermore, it can also be seen that eliminating the Polak-Ribi`ere term
−⟨τGold, Gnew⟩
∥Gold∥2, yielding the Fletcher-Reeves algorithm, perturbs the CG sequence by O(ǫ2)
terms, which does not aﬀect the quadratic rate of convergence.
Thus the approximate CG methods
discussed in 3.5.1 converge quadratically.
4. Examples: Insights and Applications. In this section, we consider ideas from the literature as
applications of the framework and methodology developed in this paper. It is our hope that some readers
who may be familiar with the algorithms presented here, will feel that they now really see them with a new
deeper, but ultimately clearer understanding. It is our further hope that developers of algorithms that
may somehow seem new will actually ﬁnd that they also already ﬁt inside of our geometrical framework.
Finally, we hope that readers will see that the many algorithms that have been proposed over the past
several decades are not just vaguely connected to each other, but are elements of a deeper mathematical
structure. The reader who sees the depth and simplicity of §4.10, say, has understood our message.
4.1. Rayleigh Quotient Iteration. If A is a symmetric matrix, it is well known that Rayleigh
quotient iteration (RQI) is a cubically convergent algorithm. It is easy to derive formulas and show that
it is true, here we will explain our view of why it is true. Let r(x) denote the Rayleigh quotient xTAx
and, abusing notation, let r(θ) denote the Rayleigh quotient on a geodesic with θ = 0 corresponding to
an eigenvector of A.
Here is the intuition. Without writing down any formulas, it is obvious that r(θ) is an even function
of θ; hence θ = 0 is an extreme point. Newton’s optimization method, usually quadratically convergent,
converges cubically on nondegenerate even functions.
Keeping in mind that A −r(x)I is the second
covariant derivative of the Rayleigh quotient, inverting it must amount to applying Newton’s method.
Following this intuition, RQI must converge cubically. The intution is that simple.
Edelman, Arias, and Smith
Fig. 4.1. Cubic convergence of RQI and Newton’s method applied to Rayleigh’s quotient. The vector ξ is an eigenvector.
Indeed, along a geodesic, r(θ) = λ cos2 θ + α sin2 θ (we ignore the degenerate case α = λ). The kth
step of Newton’s method for the univariate function r(θ) is readily veriﬁed to be
θk+1 = θk −1
2 tan(2θk) = −4
We think of updating θ as moving along the circle. If we actually moved tangent to the circle by the Newton
2 tan(2θk) and then projected to the circle, we would have the Rayleigh quotient iteration
θk+1 = θk −arctan
2 tan(2θk)
This is the mechanism that underlies Rayleigh quotient iteration. It “thinks” Newton along the geodesic,
but moves along the tangent. The angle from the eigenvector goes from θ to −θ3 almost always. (Readers
comparing with Parlett [65, Eq. (4-7-3)] will note that only positive angles are allowed in his formulation.)
When discussing the mechanism, we only needed one variable: θ. This is how the mechanism should
be viewed because it is independent of the matrix, eigenvalues, and eigenvectors. The algorithm, however,
takes place in x space. Since A−r(x)I is the second covariant derivative of r(x) in the tangent space at x,
the Newton update δ is obtained by solving Π(A−r(x)I)δ = −ΠAx = −(A−r(x)I)x, where Π = I −xxT
is the projector. The solution is δ = −x + y/(xT y), where y = (A −r(x)I)−1x. The Newton step along
the tangent direction is then x →x + δ = y/(xT y), which we project to the unit sphere. This is exactly
an RQI step. These ideas are illustrated in Figure 4.1.
One subtlety remains. The geodesic in the previous paragraph is determined by x and the gradient
rather than x and the eigenvector. The two geodesics converge to each other by the inverse iteration
process (almost always) allowing the underlying mechanism to drive the algorithm.
One trivial example where these issues arise is the generalization and derivation of Davidson’s method
 . In this context there is some question as to the interpretation of D −λI as a preconditioner.
One interpretation is that it preconditions the eigenproblem by creating better eigenvalue spacings. We
believe that there is a more appropriate point of view. In linear conjugate gradient for Ax = b, preconditioners are used to invert M which is an approximation to A (the Hessian of 1
2xTAx −xT b) against
the gradient. This is an approximate Newton step. In nonlinear conjugate gradient, there is no consensus as to whether inverting the Hessian (which is approximated by D −λI!) would constitute the ideal
preconditioner, but it is a Newton step. Therefore with the link between nonlinear conjugate gradient
preconditioning and approximate Newton step, we see that Davidson’s method is deserving of being called
a preconditioner from the conjugate gradient point of view.
Orthogonality Constraints
4.2. Coordinate Singularities of Symmetric Matrices. An important open problem in numerical linear algebra is the complete understanding of the inﬂuence of singularities on computations .
In this section we shall describe the singularity associated with multiple eigenvalues of symmetric matrices
in terms of coordinate singularities, i.e., the breakdown of the coordinate representation. In §4.10, we will
describe how understanding this coordinate singularity underlies a regularization approach to eigenvalue
optimization.
Matrix factorizations are nothing more than changes in variables or coordinate changes. In the plane,
Cartesian and polar coordinates both give orthogonal systems, but polar coordinates have a coordinate
singularity at the origin. A small perturbation near the origin can violently change the angle coordinate.
This is ill-conditioning. If the r coordinate goes through the origin we have a singularity of the form |r|.
Consider traceless, symmetric, 2-by-2 matrices:
The positive eigenvalue is r =
x2 + y2, and one of the orthogonal eigenvectors is
, where tan θ =
y/x. The conversion between matrix elements and the eigendecomposition is exactly the conversion from
Cartesian to polar coordinates. Whatever ill-conditioning one associates with a symmetric matrix with
two close eigenvalues, it is the same numerical diﬃculty associated with the origin in polar coordinates.
The larger eigenvalue behaves like |r| at the origin, and the eigenvector behaves like θ changing violently
when perturbed. If one wants to think about all 2-by-2 symmetric matrices, add z as the trace, and the
resulting interpretation is cylindrical coordinates.
We now generalize. Let Sn be the space of n-by-n symmetric matrices. Suppose that the largest
p eigenvalues λ1, . . . , λp coalesce. The corresponding eigenvectors are not uniquely determined, but the
invariant subspace is. Convenient parameterizations are
Symmetric Matrices
= Rp × Vn,p × Sn−p
Sn,p ≡{ Sn : λ1 has multiplicity p } = R × Gn,p × Sn−p
That is, any symmetric matrix may be parameterized by its p largest eigenvalues, the corresponding
eigenvectors in order, and the (n −p)-by-(n −p) symmetric operator on the space orthogonal to these
eigenvectors. To parameterize a symmetric matrix with eigenvalue λ of multiplicity p, we must specify
the invariant subspace corresponding to this eigenvalue and, once again, the (n−p)-by-(n−p) symmetric
operator on the orthogonal subspace. It is worth mentioning that the parameters in these decompositions
give an orthonormal system (surfaces with constant parameters intersect orthogonally). The codimension
of Sn,p in Sn is p(p + 1)/2 −1, obtained by adding p −1 (corresponding to λ2, . . . , λp) to p(p −1)/2 (the
codimension of Gn,p in Vn,p).
Another interpretation of the well-known fact that when eigenvalues coalesce, eigenvectors, but not
invariant subspaces, are ill-conditioned, is that the Stiefel manifold collapses to the Grassmann manifold.
As with polar coordinates we have a coordinate singularity corresponding to ill-conditioning near Sn,p.
Near this set, a small perturbation will violently move the Stiefel component. The singularity associated
with the coallescing of eigenvalues is very much the singularity of the function f(x) = |x|.
4.3. The CS Decomposition. The CS decomposition should be recognized as the geodesic
between two points on the Grassmann manifold. Any n-by-n orthogonal matrix Q may be written as
for some p-by-p orthogonal matrices V and ˜V and (n −p)-by-(n −p) orthogonal matrices U and ˜U, and
p angles θi where C = diag(cos θ1, . . . , cos θp) and S = diag(sin θ1, . . . , sin θp). Comparing this with the
geodesic formula Eq. (2.65) and letting θi = tσi (i = 1, . . . , p) where σi are the diagonal elements of Σ,
we see that the ﬁrst p columns of the CS decomposition traverse a geodesic emanating from Y (0) =
(the origin). The next p columns give an orthogonal basis for the velocity vector along the geodesic (in
fact, they are the orthogonal component of its polar decomposition).
Edelman, Arias, and Smith
As is well known, the θi are the principal angles between the subspaces spanned by the ﬁrst p columns
of Q and the origin. In general, let θi (i = 1, . . . , p) be the principal angles between the two subspaces
spanned by the columns of n-by-p orthonormal matrices Y1 and Y2, i.e., U(cos Θ)V T is the singular value
decomposition of Y T
1 Y2, where Θ is the diagonal matrix of principal angles. Also let θ and sin θ represent
the p-vectors formed by the θi and sin θi. These principal angles provide several diﬀerent deﬁnitions of
the distance between two subspaces:
1. arc length:
d(Y1, Y2) = ∥θ∥2
2. Fubini-Study:
dFS(Y1, Y2) = arccos| det Y T
1 Y2| = arccos(Q
3. chordal 2-norm:
dc2(Y1, Y2) = ∥Y1U −Y2V ∥2 = ∥2 sin 1
4. chordal Frobenius-norm:
dcF (Y1, Y2) = ∥Y1U −Y2V ∥F = ∥2 sin 1
5. projection 2-norm :
dp2(Y1, Y2) = ∥Y1Y T
2 ∥2 = ∥sin θ∥∞
6. projection F-norm:
dpF (Y1, Y2) = 2−1/2∥Y1Y T
2 ∥F = ∥sin θ∥2
The arc length distance is derived from the intrinsic geometry of the Grassmann manifold.
chordal 2-norm and Frobenius-norm distances are derived by embedding the Grassmann manifold in the
vector space Rn×p, then using the the 2- and Frobenius-norms, respectively, in these spaces. Note that
these distances may be obtained from the minimization problems
dc2 or cF (Y1, Y2) =
Q1,Q2∈Op ∥Y1Q1 −Y2Q2∥2 or F .
The projection matrix 2-norm and Frobenius-norm distances are derived by embedding the Grassmann
manifold in the set of n-by-n projection matrices of rank p, then using the 2- and Frobenius-norms, respectively. The Fubini-Study distance is derived via the Pl¨ucker embedding of Gn,p into the projective space
P(Vp(Rn)) (by taking wedge products between all columns of Y ), then using the Fubini-Study metric
 .2 Note that all metrics except the chordal and projection matrix 2-norm distances are asymptotically
equivalent for small principal angles, i.e., these embeddings are isometries, and that for Y1 ̸= Y2 we have
the strict inequalities
d(Y1, Y2) > dFS(Y1, Y2),
d(Y1, Y2) > dcF (Y1, Y2) > dpF (Y1, Y2),
d(Y1, Y2) > dcF (Y1, Y2) > dc2(Y1, Y2),
d(Y1, Y2) > dpF (Y1, Y2) > dp2(Y1, Y2).
These inequalities are intuitively appealing because by embedding the Grassmann manifold in a higher
dimensional space, we may “cut corners” in measuring the distance between any two points.
4.4. Conjugate Gradient for the Eigenvalue Problem. Conjugate gradient algorithms to minimize 1
2yTAy (A symmetric) on the sphere (p = 1) is easy and has been proposed in many sources. The
correct model algorithm for p > 1 presented in this paper is new. We were at ﬁrst bewildered by the
number of variations most of which propose “new”
algorithms for conjugate gradient for the eigenvalue problem. Most of these algorithms are for computing
extreme eigenvalues and corresponding eigenvectors. It is important to note that none of these methods
are equivalent to Lanczos . It seems that the correct approach to the conjugate gradient algorithm
for invariant subspaces (p > 1) has been more elusive. We are only aware of three papers that
directly consider conjugate gradient style algorithms for invariant subspaces of dimension p > 1. None of
the proposed algorithms are quite as close to the new idealized algorithms as the p = 1 algorithms are.
Each is missing important features which are best understood in the framework that we have developed.
We discuss these algorithms below.
The simplest non-trivial objective function on the Grassmann manifold Gn,p is the quadratic form
2 tr Y TAY,
where A is a symmetric n-by-n matrix. It is well known that the solution to the minimization of F is the
sum of the p smallest eigenvalues of A, with an optimal Y providing a basis for the invariant subspace
corresponding to the p smallest eigenvalues.
2We thank Keith Forsythe for reminding us of this distance.
Orthogonality Constraints
To solve the eigenvalue problem, one may use the template directly from §3.3 after deriving the
∇F(Y ) = AY −Y (Y TAY )
and the second covariant derivative of F(Y ):
Hess F(∆1, ∆2) = tr
1 ∆2)Y TAY
The line minimization problem may be solved as p separate two-by-two problems in parallel, or it may
be solved more completely by solving the 2p-by-2p eigenvalue problem. This does not follow the geodesic
directly, but captures the main idea of the block Lanczos algorithm which in some sense is optimal .
If one is really considering the pure linear symmetric eigenvalue problem then pure conjugate gradient
style procedures must be inferior to Lanczos. Every step of all proposed non-preconditioned conjugate
gradient algorithms builds vectors inside the same Krylov space in which Lanczos gives an optimal solution.
However, exploring conjugate gradient is worthwhile. When the eigenvalue problem is non-linear or the
matrix changes with time, the Lanczos procedure is problematic because it stubbornly remembers past
information that perhaps it would do well to forget. (Linear conjugate gradient, by contrast, beneﬁts from
the memory of this past information.) Applications towards non-linear eigenvalue problems or problems
that change in time drive us to consider the conjugate gradient method. Even the eigenvalue problem
still plays a worthy role: it is the ideal model problem that allows us to understand the procedure much
the way the Poisson equation on the grid is the model problem for many linear equation solvers.
Conjugate gradient on the sphere (p = 1) computes the smallest eigenvalue of a symmetric matrix
A. Two papers consider imposing conjugacy through A. This is an unfortunate choice by itself
because A is quite diﬀerent from the Hessian A−r(x)I, where r(x) is the Rayleigh quotient. A few authors
directly consider conjugacy through the unconstrained Hessian . Others attempt to approximate
conjugacy through the Hessian by using Polak-Ribi´ere or Fletcher-Reeves .
It is quite possible that most of these variations might well be competitive with each other and also our
idealized algorithm, but we have not performed the numerical experiments because ultimately the p = 1
case is so trivial. A comparison that may be of more interest is the comparison with restarted Lanczos. We
performed an informal numerical experiment that showed that the conjugate gradient method is always
superior to two step Lanczos with restarts (as it should be since this is equivalent to the steepest descent
method), but is typically slightly slower than four step Lanczos. Further experimentation may be needed
in practice.
Turning to the p > 1 case, the three papers that we are aware of are . The algorithm
proposed in Als´en , has a built-in extra feature not in the idealized algorithm. Though this may not
be obvious, it has one step of orthogonal iteration built in. This may be viewed as a preconditioning
procedure giving the algorithm an advantage. The Sameh-Wisniewski algorithm begins with many
of the ideas of an idealized Grassmann algorithm, including the recognition of the correct tangent on the
Grassmann manifold (though they only mention imposing the Stiefel constraint). Informal experiments
did not reveal this algorithm to be competitive, but further experimentation might be appropriate. The
more recent Fu and Dowling algorithm imposes conjugacy through A and therefore we do not expect
it to be competitive.
Edelman, Arias, and Smith
4.5. Conjugate Gradient for the Generalized Eigenvalue Problem. It is well known that
the generalized eigenvalue problem Ax = λBx may also be posed as a constrained optimization problem.
Now we must ﬁnd
min tr Y TAY
subject to the constraint that
Y TBY = Ip.
With the change of variables
¯Y = B1/2Y
¯A = B−1/2AB−1/2
the problem becomes
min tr ¯Y T ¯A¯Y
subject to
¯Y T ¯Y = Ip.
The numerical algorithm will be performed on the non-overlined variables, but the algorithm will be
mathematically equivalent to one performed on the overlined variables.
Notice that the condition on tangents in this new coordinate system is that
It is readily checked that the gradient of the trace minimization problem becomes
G = (B−1 −Y Y T )AY
(note that GTBY = 0).
Geodesics may be followed in any direction ∆for which ∆TBY = 0 by computing a compact variation
on the SVD of ∆:
∆= UΣV T ,
where U TBU = I.
For simplicity, let us assume that ∆has full rank p. The V vectors are the eigenvectors of the matrix
∆T B∆, while the U vectors are the eigenvectors of the matrix ∆∆T B corresponding to the non-zero
eigenvalues. There is also a version involving the two matrices
This SVD may be expressed in terms of the quotient SVD .
Given the SVD, we may follow geodesics by computing
Y (t) = ( Y V
All the Y along this curve have the property that Y TBY = I. For the problem of minimizing 1
2 tr Y TAY ,
line minimization decouples into p two-by-two problems just as in the ordinary eigenvalue problem.
Parallel transport, conjugacy, and the second covariant derivative may all be readily worked out.
4.6. Electronic Structures Computations. In this section, we brieﬂy survey a research area
where conjugate gradient minimization of non-quadratic but smooth functions on the Stiefel and Grassmann manifolds arise, the ab initio calculation of electronic structure within the local density approximation. Such approaches use only the charge and mass of electrons and atomic nuclei as input and have
greatly furthered understanding of the thermodynamic properties of bulk materials , the structure and
dynamics of surfaces , the nature of point defects in crystals , and the diﬀusion and interaction
of impurities in bulk materials . Less than ten years ago, Car and Parrinello in a watershed paper
proposed minimization through simulated annealing. Teter and Gillan later introduced conjugate
gradient based schemes and demonstrated an order of magnitude increase in the convergence rate. These
initial approaches, however, ignored entirely the eﬀects of curvature on the choice of conjugate search
directions. Taking the curvature into partial account using a generalization of the Riemannian projection
led to a further improvement in computation times by over a factor of three under certain conditions .
Orthogonality Constraints
Our ability to compute ab initio, using only the charge and mass of electrons and atomic nuclei
as input, the behavior of systems of everyday matter has advanced greatly in recent years. However,
the computational demands of the approach and the attendant bounds on the size of systems which
may be studied (several hundred atoms) have limited the direct impact of the approach on materials and
chemical engineering. Several ab initio applications which will beneﬁt technology tremendously remain out
of reach, requiring an order of magnitude increase in the size of addressable systems. Problems requiring
the simultaneous study of thousands of atoms include defects in glasses (ﬁber optics communications),
complexes of extended crystalline defects (materials’ strength and processing), and large molecules (drug
The theoretical problem of interest is to ﬁnd the smallest eigenvalue E0 of the Schr¨odinger equation
in the space of 3N dimensional skew-symmetric functions,
where the Hamiltonian operator H is deﬁned by
n + Vion(rn)
∥rn −rm∥2 .
Here, N is the number of electrons in the system under study, now typically on the order of several
hundred, ri is the position of the ith electron, Vion(r) is the potential function due to the nuclei and
inner electrons, and the second summation is recognized as the usual Coulomb interactions. Directly
discretizing this equation at M grid-points in space would lead to absurdly huge eigenvalue problems
where the matrix would be M N-by-M N. This is not just a question of dense versus sparse methods, a
direct approach is simply infeasible.
The fundamental theorems which make the ab initio approach tractable come from the density functional theory of Hohenberg and Kohn and Kohn and Sham . Density functional theory states that
the ground states energy of a quantum mechanical system of interacting electrons and ions is equal to the
solution of the problem of minimizing an energy function over all possible sets of N three-dimensional
functions (electronic orbitals) obeying the constraints of orthonormality. Practical calculations generally
use a ﬁnite basis to expand the orbitals, but for purposes of discussion, we may discretize the problem
onto a ﬁnite spatial grid consisting of M points. The Kohn-Sham minimization then becomes,
tr(XTHX) + f
where each column of X is a diﬀerent electronic orbital sampled on the spatial grid, ρ is the vector
n |Xin|2, H is an M-by-M matrix (single-particle Hamiltonian), and f is a function which we
leave unspeciﬁed in this discussion. In full generality the X are complex, but the real case applies for
physical systems of large extent that we envisage for this application , and we, accordingly, take X to
be real in this discussion.
Recent advances in computers have enabled such calculations on systems with several hundreds of
atoms . Further improvements in memory and performance will soon make feasible computations
with upwards of a thousand atoms.
However, with growing interest in calculations involving larger
systems has come the awareness that as the physical length of systems under study increases, the Hessian
about the minimum of Eq. (4.9) becomes increasingly ill-conditioned and non-conjugate minimization
approaches exhibit a critical slowing down . This observation prompted workers to apply
conjugate gradient concepts to the problem, and now dozens of researchers have written papers using
some form of the conjugate gradient method. In particular, one has a Grassmann problem when the
number of electrons in each state is constant (i.e., two one spin up and one spin down). This is what
happens in calculations on semiconductors and “closed shell” atoms and molecules. Otherwise, one has a
Stiefel problem such as when one has metals or molecules with partially ﬁlled degenerate states.
Edelman, Arias, and Smith
The framework laid out in this discussion may be of practical use to the ab initio density-functional
community when the inner product computation through the Hessian of E(X) is no more computationally complex to evaluate than calculating the energy function E(X) or maintaining the orthonormality
constraints XT X = IN. A suitable form for this inner product computation is
 Y T (H + V )Z
 XT (H + V )(XY T Z)
where V is the diagonal matrix deﬁned by Vij = (∂f/∂ρi)δij, σi ≡P
n YinXin, τi ≡P
n ZinXin. Written
this way, the ﬁrst two terms of Eq. (4.10) have the same form and may be evaluated in the same manner
as the corresponding terms in Eq. (4.9), with σ and τ playing roles similar to ρ. The third term, coming
from the curvature, may be evaluated in the same way as the ﬁrst term of Eq. (4.10) once given the object
XY T Z, which is no more computationally complex to obtain than the Gram-Schmidt orthonormalization
of an object like X.
4.7. Subspace Tracking. The problem of computing the principal invariant subspace of a symmetric or Hermitian matrix arises frequently in signal processing applications, such as adaptive ﬁltering and
direction ﬁnding . Frequently, there is some time-varying aspect to the signal processing
problem, and a family of time-varying principal invariant subspaces must be tracked. The variations may
be due to either the addition of new information as in covariance matrix updating, a changing signal
environment, or both. For example, compute the principal invariant subspace of either of the covariance
Rk = Rk−1 + xkxT
k = 1, 2, . . . , and xk is given
R(t) = a continuous function of t
at every iteration or at discrete times. Eq. (4.11) typically arises from updating the sample covariance
matrix estimate; Eq. (4.12), the more general case, arises from a time-varying interference scenario, e.g.,
interference for airborne surveillance radar . Solving this eigenvalue problem via the eigenvalue or
singular value decompositions requires a large computational eﬀort. Furthermore, only the span of the ﬁrst
few principal eigenvectors may be required, whereas decomposition techniques compute all eigenvectors
and eigenvalues, resulting in superﬂuous computations. Approaches to this problem may be classiﬁed as
standard iterative methods , methods exploiting rank 1 updates , i.e.,
Eq. (4.11), Lanczos based methods , gradient based methods , conjugate gradient
based methods , which are surveyed by Edelman and Smith , Rayleigh-Ritz
based methods , and methods that exploit covariance matrix or array structure .
If the subspace does not change quickly over (discrete or continuous) time, then the desired solution will be close to the previously computed solution, and an iterative gradient-based algorithm such
as the conjugate gradient algorithm may be computationally attractive for the subspace tracking problem.
Thus the subspace tracking problem is treated as a time-varying optimization problem.
conjugate gradient methods for computing principal invariant subspaces in a signal processing context
have appeared ; however, these conjugate gradient techniques do not exploit the structure of the subspace constraint (see §4.4). Instead, we employ the conjugate gradient method on the
Grassmann manifold, or an approximation of it discussed in §3.5. Comon and Golub describe and
compare a wide variety of diﬀerent algorithms for the problem of exponential covariance matrix updates,
with particular emphasis on Lanczos and gradient-based algorithms. Yang, Sarkar, and Arvas survey some conjugate gradient algorithms applied to computing the principal invariant subspace of a ﬁxed
symmetric matrix. We adopt the general assumption that the matrix may change arbitrarily over time,
but that it must vary “slowly enough” so that using a conjugate gradient based approach is computationally eﬃcient. This last constraint is, of course, dependent upon the application. For the example of
space-time adaptive processing for airborne radar with a rotating antenna, Smith shows that this
method is capable of tracking the principal invariant subspace of clutter interference; however, when
the interference dimension p is increased to account for new interference eigenvalues, one does better to
compute the eigendecomposition from scratch and use it to initiate a new subspace track.
Orthogonality Constraints
4.8. Newton’s Method for Invariant Subspace Computations. Methods for reﬁning estimates
for invariant subspace computations have been proposed by Chatelin , Dongarra, Moler, and
Wilkinson , and Stewart . Demmel [28, §3] proposes a uniﬁed approach by showing that they are
all solutions to a Riccati equation.
These algorithms, when applied to symmetric matrices, are all variations on our geometrical Newton
algorithm and may be understood in this context. There is nothing special about the eigenvalue problem;
Newton’s method for any function on the Grassmann manifold yields a Sylvester equation in the tangent
space. The reason a Riccati equation arises rather than a Sylvester equation is that the previous algorithms
formulate the problem in an aﬃne space with arbitrary constraints.
Previous researchers knew the
quadratic term in the Riccati equation belonged there, and knew that it somehow is related to the
orthogonality constraints, but we now see that it is an artifact of a ﬂat space derivation.
Let us take a closer look. Previous researchers proposed algorithms for invariant subspaces by asking
for a solution to the matrix equation
AY −Y B = 0
made nondegenerate by imposing the aﬃne constraint
for some arbitrary choice of Z. In the Dongarra et al. case, Z may be obtained by inverting and transposing
an arbitrary p×p minor of the n×p matrix Y . In Moler’s Matlab notation Z=zeros(n,p); Z(r,:)=inv(Y(r,:))’,
where r denotes a p-vector of row indices. For Stewart, Z = Y (Y T Y )−1.
A mathematically insightful approach would require no arbitrary choice for Z. We would simply
specify the problem by performing Newton’s method on the function F(Y ) = 1
2 tr Y TAY on the Grassmann
manifold. The stationary points of F(Y ) are the invariant subspaces. There is no need to specify any
further constraints and there are no degeneracies. (Notice that asking for the solution to AY = Y (Y TAY )
subject to Y T Y = I is a degenerate problem.)
Newton’s method requires the solution ∆to the Sylvester equation
 A∆−∆(Y TAY )
where Π = (I −Y Y T ) denotes the projection onto the tangent space of the Grassmann manifold and
G = ΠAY is the gradient. The solution is ∆= −Y +Z(Y T Z)−1, where Z is the solution to the Sylvester
equation AZ −Z(Y TAY ) = Y .
Y may be chosen so that Y TAY is diagonal, yielding simultaneous
Rayleigh quotient iterations. If we move along the tangent and project rather than the geodesic we have
the iteration sending Y to the Q factor in the QR decomposition of Z.
4.9. Reduced Gradient Methods, Sequential Quadratic Programming, and Lagrange
Multipliers. In this section, we generalize beyond the Stiefel and Grassmann manifolds to show how
the language and understanding of diﬀerential geometry provides insight into well-known algorithms for
general non-linear constrained optimization. We will show the role that geodesics play in these algorithms.
In the next subsection, we will then apply the geometrical intuition developed here to directly formulate
regularized sequential quadratic programs as is needed in eigenvalue optimization.
Here we study sequential quadratic programming (SQP) and reduced gradient methods (RGM). By
SQP we mean the algorithm denoted as Newton SQP by Boggs and Tolle [7, p. 14], SQP by Nash and
Sofer [59, p. 512], and QP-based projected Lagrangian by Gill, Murray, and Wright [41, p. 238, Eq. (6.41)].
By RGM, we speciﬁcally mean the method sometimes denoted as the reduced Hessian method [7, p. 25],
other times simply denoted RGM [59, p. 520], and yet other times considered an example of an RGM [41,
p. 221, Eq. (6.17)]. The diﬀerence is that RGM is derived based (roughly) on the assumption that one
starts at a feasible point, whereas SQP does not.
Edelman, Arias, and Smith
We begin by interpreting geometrically the Lagrangian function as it is used in constrained optimization. Consider the optimization problem
given the constraint that
h(x) = 0 ∈Rp.
For simplicity we consider the case where the level surfaces h(x) = c are manifolds (∂h/∂x has full rank
everywhere) and we work with the Euclidean metric. In the Euclidean case, the formulations are routine
in the optimization community, but we have not seen the geometric intuition (particularly geometric
interpretations away from the optimization point and the role that geodesics play “behind-the-scenes”) in
the optimization references that we have consulted. Numerical Lagrange multiplier issues are discussed
in and , for example.
In this paper, we give the new interpretation that the Hessian of the
Lagrangian is the correct matrix for computing second derivatives along geodesics at every point, not
only as an approximation to the result at the optimal point.
At every point x ∈Rn, it is possible to project the gradient of f onto the tangent space of the level
surface through x. This deﬁnes a sort of ﬂattened vector ﬁeld. In terms of formulas, projection onto the
tangent space (known as computing least-squares Lagrange multiplier estimates) means ﬁnding λ that
minimizes the norm of
Lx = fx −λ · hx,
At every point x ∈Rn (not only the optimal point) Lagrange multipliers are the coordinates of fx in the
normal space to a level surface of the constraint, i.e., the row space of hx. (Our convention is that fx is
a 1-by-n row vector, and hx is a p-by-n matrix whose rows are the linearizations of the constraints.)
If x(t) is any curve starting at x(0) = x that is constrained to the level surface at x, then Lx ˙x
computes the derivative of f along the curve. (In other words, Lx is the ﬁrst covariant derivative.) The
second derivative of f along the curve is
= ˙xT Lxx ˙x + Lx¨x.
At the optimal point Lx is 0, and therefore Lxx is a second order model for f on the tangent space to the
level surface. The vanishing of the term involving Lx at the optimal point is well-known.
The idea that we have not seen in the optimization literature and that we believe to be new is the
geometrical understanding of the quantity at a non-optimal point: At any point at all, Lx is tangent to
the level surface while ¨x(t) is normal when x is a geodesic. The second term in Eq. (4.16) conveniently
vanishes here too because we are diﬀerentiating along a geodesic! Therefore, the Hessian of the Lagrangian
has a natural geometrical meaning, it is the second derivative of f along geodesics on the level surface,
i.e., it is the second covariant derivative in the Euclidean metric.
We now describe the RG method geometrically. Starting at a point x on (or near) the constraint
surface h(x) = 0, the quadratic function
2 ˙xT Lxx ˙x
models f (up to a constant) along geodesics emanating from x. The ˙x that minimizes this function is the
Newton step for the minimum for f. Intrinsic Newton would move along the geodesic in the direction
of ˙x a length equal to ∥˙x∥. Extrinsically, we can move along the tangent directly from x to x + ˙x and
then solve a set of nonlinear equations to project back to the constraint surface. This is RGM. It is a
static constrained Newton method in that the algorithm models the problem by assuming that the points
satisfy the constraints rather than trying to dynamically move from level surface to level surface as does
Orthogonality Constraints
In SQP, we start on some level surface. We now notice that the quadratic function
2 ˙xT Lxx ˙x
can serve as a model not only the ﬁrst and second covariant derivative of f on the level surface through
x but also on level surfaces for points near x. The level surface through x is speciﬁed by the equation
hx ˙x = 0. Other parallel level surfaces are hx ˙x + c = 0. The right choice for c is h(x), which is a Newton
step towards the level surface h(x) = 0. Therefore if the current position is x, and we form the problem of
minimizing Lx ˙x + 1
2 ˙xT Lxx ˙x subject to the constraint that hx ˙x + h(x) = 0, we are minimizing our model
of f along geodesics through a level surface that is our best estimate for the constraint h(x) = 0. This is
the SQP method.
Practicalities associated with implementing these algorithms are discussed in the aforementioned
texts. Generalizations to other metrics (non-Euclidean) are possible, but we do not discuss this in detail.
Instead we conclude by making clear the relationship between Lagrange multipliers and the Christoﬀel
symbols of diﬀerential geometry.
To derive the geodesic equation, let f(x) = xk, the kth coordinate of x. From Eq. (4.15), the Lagrange
multipliers are hT
x )−1. Since fxx = 0 we then have that the geodesic equations are ¨xk = ˙xT Lk
(k = 1, . . . , n), where Lk
xx denotes, −hT
x )−1 · hxx, the Hessian of the Lagrangian function of xk.
The matrix Γk = −Lk
xx is the Christoﬀel symbol of diﬀerential geometry.
4.10. Eigenvalue Optimization. The geometric approach allows the formulation of sequential
quadratic programming problems when the Lagrange multiplier formalism breaks down due to coordinate
singularities. Speciﬁcally, the geometric insight from the previous subsection is that during the execution
of a sequential quadratic program there are three types of directions. The ﬁrst direction is towards the
constraint manifold. SQP performs a Newton step in that direction. The second family of directions is
parallel to the constraint manifold. SQP forms a quadratic approximation to the objective function in
the parallel level surface obtained from the Newton step. The remaining directions play no role in an
SQP and should be ignored.
Consider the problem of minimizing the largest eigenvalue of A(x), an n-by-n real symmetric matrixvalued function of x ∈Rm when it is known that at the minimum, exactly p of the largest eigenvalues
coalesce. Overton and Womersley formulated SQPs for this problem using Lagrange multipliers and
sophisticated perturbation theory. The constraint in their SQP was that the p largest eigenvalues were
identical. We will here consider the case of m > p(p + 1)/2. One interesting feature that they observed
was the non-diﬀerentiability of the largest eigenvalue at the optimum. Following the geometry of the
previous section, a new algorithm without Lagrange multipliers may be readily devised. There will be no
Lagrange multipliers because there will be no consideration of the third directions mentioned above.
We will write A for A(x). Let Λ = Y TAY , where the orthonormal columns of Y span the invariant
subspace for the p largest eigenvalues of A, λ1, . . . , λp. We let F(A) = λ1 and L(A) = tr(Λ) = λ1+· · ·+λp.
Unlike the function F(A), L(A) is a diﬀerentiable function at the optimal point. One might have guessed
that this L(A) was the right L(A), but here is how one can logically deduce it.
The trick is to rely not on the Lagrange multiplier formalism of constraint functions, but rather on
the geometry. Geometry has the power to replace a long complicated derivation with a short powerful
Once the techniques are mastered, geometry provides the more intuitive understanding.
is no convenient h(A) to express the constraint of multiple eigenvalues; artiﬁcially creating one leads
to unnecessary complications due to the coordinate singularity when one moves from the level surface
h(A) = 0 to another level surface. The right way to understand the coordinate singularity was described
in §4.2. The direction of the Newton step must be the ﬁrst order constraint of the coallescing of the
eigenvalues. Using the notation of §4.2, the parallel directions are the tangent vectors of Sn,p. All other
directions play no role. The natural level surfaces are thereby obtained by shifting the p largest eigenvalues
by a constant, and developing the orthogonal eigenvector matrix Q(0) as in Eq. (2.32).
Edelman, Arias, and Smith
The message from §4.9 is that whatever function we are interested in, we are only interested in the
component of the gradient in the direction parallel to Sn,p. The very construction of a Lagrangian L then
may be viewed as the construction of an appropriate function with the property that Lx is parallel to the
tangent vectors of Sn,p. Of course the tangent space to Sn,p (see §4.2) includes projection matrices of the
i=1 αiyiyT
i , where yi is the eigenvector corresponding to λi, only when the αi are all equal. This corresponds to an identical shift of these eigenvalues. Therefore to form the correct gradient of the objective
function F(A) = λ1 everywhere, we should replace the true gradient, which is well known to be the spectral
projector y1yT
1 , with its component in the direction Y Y T , which is an Sn,p tangent vector. Integrating, we
now see that the act of forming the Lagrangian, which we now understand geometrically to mean replacing
1 with Y Y T (projecting the gradient to the surface of uniform shifts) amounts to nothing more than
changing the objective function from F(x) to L(x) = tr(Λ) = tr Y TAY . While one might have guessed
that this was a convenient Langrangian, we deduced it by projecting the gradient of f(x) on the tangent
space of a level surface. The components of f(x) that we removed implicitly would have contained the
Lagrange multipliers, but since these components are not well deﬁned at the coordinate singularity, it is
of little value to be concerned with them.
Now we must explicitly consider the dependence of L on x. Our optimization step is denoted ∆x,
and ˙A and ¨A respectively denote [Ax∆x] and [Axx∆x∆x] (notation from ). It is easy to verify that
Lx = tr Y T ˙AY,
Lxx = tr(Y T ¨AY + Y T ˙A ˙Y + ˙Y T ˙AY ),
where ˙Y is the solution to
˙Y Λ −(I −Y Y T )A ˙Y = (I −Y Y T ) ˙AY
that satisﬁes Y T ˙Y = 0. The resulting sequential quadratic program over ∆x is then
min Lx + 1
subject to the linear constraint (on ∆x) that
Y T ˙AY + Λ = αI,
where the scalar α is arbitrary.
Let us explain all of these steps in more detail. The allowable ˙Y are Grassmann directions, Y T ˙Y = 0.
Otherwise, we are not parallel to the constraint surface. Equation (4.18) is the derivative of Y TAY .
Noting that AY = Y Λ and Y T ˙Y = 0, two terms disappear. Equation (4.19) is trivial but we note the
problem that we do not have an explicit expression for ˙Y , we only have A, Y and ˙A. Fortunately, the
perturbation theory for the invariant subspace is available from Equation (4.20). It may be derived by
diﬀerentiating AY = Y Λ and substituting ˙Λ = Y T ˙AY .3 The solution to Equation (4.20) is unique so long
as no other eigenvalue of A is equal to any of λ1, . . . , λp.
The linear constraint on ∆x is the one that inﬁnitesimally moves us to the constraint surface. It is
the condition that moves us to a diagonal matrix. Therefore, ˙Λ = Y T ˙AY when added to Λ must be a
scalar multiple of the identity. This is a linear condition on ˙A and therefore on ∆x. The α does not
explicitly appear in the constraint.
5. Conclusions. This paper oﬀers a new approach to the algorithms in numerical analysis involving
orthogonality constraints. We have found that these algorithms should be understood as optimization
algorithms in the correct geometrical setting; however, they rarely are.
3 Alert readers may notice that this is really the operator used in the deﬁnition of “sep” in numerical linear algebra texts.
The reader really understands the theory that we have developed in this paper if he or she now can picture the famous “sep”
operator as a Lie bracket with a Grassmann tangent and is convinced that this is the “right” way to understand “sep”.
Orthogonality Constraints
Block Rayleigh
Newton Subspace
Improvement
Chatelin 84,93
Dongarra,Moler,Wilkinson 83
Stewart 73
Newton on the
Approximate
Newton on the
Nonlinear PCG
Payne, Teter, Allan 92
Blk Inv Iteration
Inv Iteration
Nonlinear CG
Linear Eigenvalue
Sameh, Wisniewski 82
Fu, Dowling 95
Chen, Sarkar 86
Gradient Flows
Perdon, Gamb.89
Geradin 71
Bradbury, Flet.66
Fox, Kapoor 69
Fried 69, 72
Anderson 71
Haimi-cohen
Yang, Sarkar,
Fuhrmann, Liu 84
Fig. 4.2. Taxonomy of algorithms deﬁned from the Grassmann manifold.
As a concluding example of the insight gained, we propose a Grassmann based taxonomy for problems
related to the symmetric eigenproblem.
This taxonomy allows us to view algorithms not as isolated
entities, but as objects with a coherent mathematical structure. It is our hope that developers of new
algorithms and perturbation theories will beneﬁt from the analytical approach that lead to our taxonomy.
Edelman, Arias, and Smith
In this taxonomy, algorithms are viewed as either restrictions or approximations of their parent. Ultimately, we have Newton’s method on arbitrary Riemannian manifolds as the root. One can then restrict
to a particular manifold such as the Stiefel manifold or, as we illustrate in Figure 4.2, the Grassmann
manifold. Along the vertical axis in the left column we begin with Newton’s method which may be approximated ﬁrst with PCG or approximate Newton methods, then pure conjugate gradient, and ﬁnally
steepest descent. Moving from left to right the idealized algorithms are replaced with more practical
versions that specialize for particular problems. The second column contains block algorithms, while
the third contains single eigenvector related algorithms. This abstraction would not be possible without
Acknowledgments. The ﬁrst author would like to thank Jim Demmel, Velvel Kahan, and Beresford
Parlett who have stressed the importance of geometry in numerical linear algebra. We also thank Scott
Axelrod, Victor Guillemin, and Shoshichi Kobayashi for a number of interesting conversations concerning
diﬀerential geometry, and Roger Brockett for the idea of investigating conjugate gradient methods on
symmetric spaces. We further wish to thank Dianne O’Leary, Mike Todd, Mike Overton, and Margaret
Wright for equally interesting conversations in the area of numerical optimization. We are indebted to the
San Diego crowd consisting of Scott Baden, Beth Ong, Ryoichi Kawai (University of Alabama), and John
Weare for working together towards understanding the electronic structure problem. In particular the
ﬁrst author thanks John Weare for his hospitality during visits in San Diego where we explored the issue
of conjugate gradient minimization. Steve Vavasis asked the penetrating question of how this relates to
Lagrange multipliers which we answer in §4.9. Furthermore, Ross Lippert has made a number of valuable
suggestions that are acknowledged in this paper. We are indebted to Gene Golub for inviting Steve Smith
to the 1993 Householder Symposium in Lake Arrowhead, California which serendipitously launched this
collaboration.
We would especially like to thank our editor Mike Overton and the reviewers who went far beyond
the call of duty with many very valuable suggestions that enhanced the readability and relevance of this
Orthogonality Constraints