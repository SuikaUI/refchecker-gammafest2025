MIT Open Access Articles
Interpretable Basis Decomposition for Visual Explanation
The MIT Faculty has made this article openly available. Please share
how this access benefits you. Your story matters.
Citation: Zhou, Bolei et al. "Interpretable Basis Decomposition for Visual Explanation." European
Conference on Computer Vision, September 2018, Munich, Germany, Springer Nature, October
2018 © 2018 Springer Nature
As Published: 
Publisher: Springer International Publishing
Persistent URL: 
Version: Author's final manuscript: final author's manuscript post peer review, without
publisher's formatting or copy editing
Terms of use: Creative Commons Attribution-Noncommercial-Share Alike
Interpretable Basis Decomposition
for Visual Explanation
Bolei Zhou1*, Yiyou Sun2*, David Bau1*, Antonio Torralba1
1MIT CSAIL
{bzhou,davidbau,torralba}@csail.mit.edu, 
* indicates equal contribution
Abstract. Explanations of the decisions made by a deep neural network
are important for human end-users to be able to understand and diagnose the trustworthiness of the system. Current neural networks used
for visual recognition are generally used as black boxes that do not
provide any human interpretable justiﬁcation for a prediction. In this
work we propose a new framework called Interpretable Basis Decomposition for providing visual explanations for classiﬁcation networks. By
decomposing the neural activations of the input image into semantically
interpretable components pre-trained from a large concept corpus, the
proposed framework is able to disentangle the evidence encoded in the
activation feature vector, and quantify the contribution of each piece of
evidence to the ﬁnal prediction. We apply our framework for providing
explanations to several popular networks for visual recognition, and show
it is able to explain the predictions given by the networks in a humaninterpretable way. The human interpretability of the visual explanations
provided by our framework and other recent explanation methods is evaluated through Amazon Mechanical Turk, showing that our framework
generates more faithful and interpretable explanations1.
Introduction
As deep networks continue to prove their capabilities on an expanding set of applications in visual recognition such as object classiﬁcation , scene recognition
 , image captioning , and visual question answering , it is increasingly
important not only for a network to make accurate predictions, but also to be
able to explain why the network makes each prediction.
A good explanation of a deep network should play two roles: ﬁrst, it should
be a faithful representation of the operation of the network; and second, it should
be simple and interpretable enough for a human to understand. There are two
approaches for creating human-understandable explanations for the internals of
a deep network. One is to identify the evidence that a network uses to make a
speciﬁc decision by creating a heatmap that indicates which portions of an input
are most salient to the decision . Such heatmaps can be created using
1 The code and data are available at 
B. Zhou, Y. Sun, D. Bau, A. Torralba
a variety of techniques and can be applied to identify the most salient parts of
images and training sets. A second approach is to identify the purpose of the
internal representations of a network by identifying the concepts that each part
of the network detects . Such concept dictionaries can be created by
matching network units to a broad concept data set, by generating or sampling
example inputs that reveal the sensitivity of a unit, or by training parts of the
network to solve interpretable subproblems.
In this paper we describe a framework called Interpretable Basis Decomposition (IBD), for bringing these two approaches together to generate explanations
for visual recognition. The framework is able to decompose the evidence for
a prediction for image classiﬁcation into semantically interpretable components,
each with an identiﬁed purpose, a heatmap, and a ranked contribution, as shown
in Fig. 1. In addition to showing where a network looks, we show which concepts
a network is responding to at each part of the input image.
Prediction: topiary garden
hedge (20.99%)
palm (7.57%)
tail (6.60%)
brush (5.72%)
residual (45.97%)
sculpture (5.17%)
sheep (4.53%)
ﬂower (3.44%)
Fig. 1. Interpretable Basis Decomposition provides an explanation for a prediction by
decomposing the decision into the components of interpretable basis. Top contributing
components are shown with a label, contribution, and heatmap for each term.
Our framework is based on the insight that good explanations depend on
context. For example, the concepts to explain what makes up a ‘living room’ are
diﬀerent from the concepts to explain an ‘airport’. A overstuﬀed pillow is not
an airliner, nor vice-versa. We formalize the idea of a salient set of concepts as a
choice of a interpretable basis in the feature space, and describe how to construct
a context-speciﬁc concept basis as the solution to a least-squares problem.
Each explanation we describe is both a visualization and a vector decomposition of a layer’s internal state into interpretable components. As a vector
decomposition, each explanation is faithful to the network, quantifying the contribution of each component and also quantifying any uninterpreted residual.
Interpretable Basis Decomposition for Visual Explanation
The framework also provides explanations that are simple enough for a person
to understand. We conduct human evaluations to show that the explanations
give people accurate insights about the accuracy of a network.
We summarize our contributions as follows: 1) A new framework called Interpretable Basis Decomposition to provide semantic explanations with labels and
heatmaps for neural decision making. 2) Application of the proposed framework
on a wide range of network architectures, showing its general applicability. 3)
Human evaluations to demonstrate that the explanations are understandable to
people, outperforming previous heatmap and unit-based explanation methods.
Related Work
Visualizing neural networks. A number of techniques have been developed
to visualize the internal representations of convolutional neural networks. The
behavior of a CNN can be visualized by sampling image patches that maximize
activation of hidden units , and by backpropagation to identify or generate
salient image features . An image generation network can be trained to
invert the deep features by synthesizing the input images . The semantics of visualized units can be annotated manually or automatically by measuring
alignment between unit activations and a predeﬁned dictionary of concepts.
Explaining neural network decisions. Explanations of individual network decisions have been explored by generating informative heatmaps such as
CAM and grad-CAM , or through back-propagation conditioned on the
ﬁnal prediction and layer-wise relevance propagation . The attribution of
each channel to the ﬁnal prediction has been studied . Captioning methods
have been used to generate sentence explanations for a ﬁne-grained classiﬁcation
task . The limitation of the heatmap-based explanation methods is that the
generated heatmaps are qualitative and not informative enough to tell which
concepts have been detected, while the sentence-based explanation methods require an ad-hoc corpus of sentence description in order to train the captioning
models. Our work is built upon previous work interpreting the semantics of units
 and on heatmaps conditioned on the ﬁnal prediction . Rather than using the semantics of activated units to build explanations as in , we learn a
set of interpretable vectors in the feature space and decompose the representation in terms of these vectors. We will show that the proposed method is able
to generate faithful explanations which are more informative than the previous
heatmap-based and unit-activation methods.
Component analysis. Understanding an input signal by decomposing it
into components is an old idea. Principal Component Analysis and Independent Component Analysis have been widely used to disentangle a lowdimensional basis from high-dimensional data. Other decomposition methods
such as Bilinear models and Isomap are also used to discover meaningful subspaces and structure in the data. Our work is inspired by previous work on
component decomposition. Rather than learning the components unsupervised,
we learn the set of components from a fully annotated dataset so that we have a
B. Zhou, Y. Sun, D. Bau, A. Torralba
ground-truth label for each component. After projecting, the labeled components
provide interpretations, forming human-understandable explanations.
Concurrent work proposes examining the behavior of representations in
the direction of a set of semantic concept vectors learned from a pre-deﬁned
dataset. Those Concept Activation Vectors play a similar role as our Interpretable Basis Vectors, but while that work focuses on using a single feature
at a time for retrieval and scoring of samples, our work uses basis sets of vectors
to create explanations and decomposed heatmaps for decisions.
Framework for Interpretable Basis Decomposition
The goal of Interpretable Basis Decomposition is to decode and explain every bit
of information from the activation feature vector in a neural network’s penultimate layer. Previous work has shown that it is possible to roughly invert a
feature layer to recover an approximation to the original input image using a
trained feature inversion network . Instead of recovering the input image, our
goal is to decode the meaningful nameable components from the feature vector
so that we can build an explanation of the ﬁnal prediction.
We will describe how we decompose feature vectors in three steps. We begin
by describing a way to decompose an output class k into a set of interpretable
components c. In our decomposition, both the class and the concepts are represented as vectors wk and qc that correspond to linear classiﬁers in the feature
space, and the decomposition is expressed as an optimal choice of basis for wk.
The result of this step is a set of elementary concepts relevant to each class.
Next, we describe how to derive vectors qc corresponding to a broad dictionary of elementary interpretable concepts c. Each qc is learned by training a
linear segmentation model to locate the concept within the feature space.
Finally, we describe how to create explanations of instance decisions. This
is done by projecting the feature vector into the learned interpretable basis and
measuring the contribution of each interpretable component. An explanation
consists of a list of concepts that contribute most to the ﬁnal score, together
with a heatmap for each concept that shows where the contributions arise for
the ﬁnal prediction. The framework is illustrated in Fig. 2.
Deﬁning an Interpretable Basis
Explaining a layer can be done by choosing an interpretable basis for the layer’s
input representation. To see why, set f(x) ∈RK as a deep net with K output
dimensions, considered without the ﬁnal softmax. We are interested in explaining
properties of x which determine the score fk(x) for a particular class k ≤K: for
example, we may wish to know if a concept c such as crowds of people tends to
cause the input to be classiﬁed as an output class k such as airports.
We can express our question in terms of an intermediate representation. Write
f(x) = h(g(x)) where h(a) is the top of the network and a = g(x) ∈RD is a
point in the representation space of the layer of interest. Then to investigate the
Interpretable Basis Decomposition for Visual Explanation
conv layers
wall, contributition=24.8%
sofa, contribution=9.3%
table, contribution=6.25%
unlabeled residual=40.2%
Interpretable Basis Decomposition
probability
Prediction:
Living room
Class Activation Map
class weight vector for living room
Fig. 2. Illustration of Interpretable Basis Decomposition. The class weight vector wk
is decomposed to a set of interpretable basis vectors P sciqci, each corresponding
to a labeled concept ci as well as a projection qT
ciA that reveals a heatmap of the
activations. An explanation of the prediction k consists of the concept labels ci and the
corresponding heatmaps for the most signiﬁcant terms in the decomposition of wT
For this particular example, wall, sofa, table (and some others are not shown) are labels
of the top contributing basis elements that make up the prediction of living room.
properties of x that determine fk(x), we can ask about the properties of the
intermediate representation a = g(x) that determine hk(a).
Let us focus on the simple case where a = g(x) is the output of the secondto-last layer and h(a) is a simple linear operation done by the last layer. Then hk
is a linear function that scores a according to the angle between a and wk ∈RD:
h(a) ≡W (h)a + b(h)
hk(a) = wT
Not all directions in the representation space RD are equally interpretable. Suppose we have a set of directions qci ∈RD that each correspond to elementary
interpretable concepts ci that are relevant to class k but easier to understand
than k itself. Then we can explain wk by decomposing it into a weighted sum of
interpretable components qci as follows.
wk ≈sc1qc1 + · · · + scnqcn
Unless wk lies exactly in the space spanned by the {qci}, there will be some
residual error in the decomposition. Gathering the qci into columns of a matrix
C, we can recognize that minimizing this error is a familiar least-squares problem:
Find sci to minimize ||r|| where wk = sc1qc1 + · · · + scnqcn + r
The optimal s is given by s = C+wk where C+ is the pseudoinverse of C.
B. Zhou, Y. Sun, D. Bau, A. Torralba
When interpreting the decomposition of wk, negations of concepts are not as
understandable as positive concepts, so we seek decompositions for which each
coeﬃcient sci > 0 is positive. Furthermore, we seek decompositions with a small
number of concepts.
We build the basis qci in a greedy fashion, as follows. Suppose we have already
chosen a set of columns C = [qc1| · · · |qcn], and the residual error is in (4) is
ϵ = ||wk −Cs||. Then we can reduce the residual by adding an (n+1)th concept
to reduce error. The best such concept is the one that results in the minimum
residual while keeping the coeﬃcients positive:
s,si>0 ||wk −[C|qc]s||
where [C|qc] indicates the matrix that adds the vector qc for the candidate
concept c to the columns of C.
Learning the Interpretable Basis from Annotations
For explaining an image classiﬁcation task, we build the universe of candidate
concepts C using the Broden dataset . Broden includes pixel-level segmentations for a broad range of both high-level visual concepts such as objects and
parts, as well as low-level concepts such as colors and materials. For each candidate concept c in Broden, we compute an embedding qc ∈C ⊂RD as follows.
Since Broden provides pixel-level segmentations of every concept, we train
a logistic binary classiﬁer hc(a) = sigmoid(wT
c a + bc) to detect the presence of
concept c. Training is done on a mix of images balancing c present or absent at the
center, and hard negative mining is used to select informative negative examples
during the training progress; the training procedure is detailed in Sec. 3.1. The
learned wc captures the features relevant to class c, but it is scaled in a way that
is sensitive to the training conditions for c. To eliminate this arbitrary scaling,
we standardize qc as the normalized vector qc = (wc −wc)/||wc −wc||.
Explaining a Prediction via Interpretable Basis Decomposition
The decomposition of any class weight vector wk into interpretable components
Ck ⊂C ⊂RD allows us to decompose the scoring of activations a into components of Ck in exactly the same way as we decompose wk itself. This decomposition will provide an interpretable explanation of the classiﬁcation.
Furthermore, if we include deﬁne a larger basis C∗
k ⊃Ck that adds the
residual vector r = wk −Cks, we can say something stronger: projecting a into
the basis of C∗
k captures the entire linear relationship described by the network’s
ﬁnal layer score hk(a) for class k.
hk(a) = wT
ks)T a + bk
c1a + · · · + siqT
contribution of concept ci
+ · · · + snqT
cna + rT a
residual contribution
Interpretable Basis Decomposition for Visual Explanation
Thus we can decompose the score into contributions from each concept, and
we can rank each concept according to its contribution. When the activation
a = pool(A) is derived by global average pooling of a convolutional layer A, we
can commute the dot product inside the pooling operation to obtain a picture
that localizes the contribution of concept ci.
cia = siqT
ci pool(A)
= pool(si qT
heatmap for concept ci
The explanation we seek consists of the list of concepts ci with the largest
contributions to hk(a), along with the heatmaps qT
ciA for each concept. The IBD
heatmaps qT
ciA are similar to the CAM heatmap wT
k A and can be used to reconstruct the CAM heatmap if they are all summed. However, instead of summarizing the locations contributing to a classiﬁcation all at once, the interpretable
basis decomposition separates the explanation into component heatmaps, each
corresponding to a single concept that contributes to the decision.
Decomposing gradients for GradCAM: Grad-CAM is an extension of
CAM to generate heatmap for networks with more than one ﬁnal nonconvolutional layers. Starting with the ﬁnal convolutional featuremap a = g(x),
the Grad-CAM heatmap is formed by multiplying this activation by the pooled
gradient of the higher layers h(a) with respect class k.
Here the vector wk(a) plays the same role as the constant vector wk in CAM:
to create an interpretable basis decomposition, wk(a) can be decomposed as
described in Eqs. 4-6 to create a componentwise decomposition of the Grad-
CAM heatmap. Since wk(a) is a function of the input, each input will have its
own interpretable basis.
Experiments
In this section, we describe how we learn an interpretable basis from an annotated
dataset. Then we will show that the concepts of the interpretable basis that
are associated with each prediction class of the networks sheds lights on the
abstractions learned by each network. After that we use the interpretable basis
decomposition to build explanations for the predictions given by the popular
network architectures: AlexNet , VGG , ResNet (18 and 50 layers) ,
each trained scratch on ImageNet and Places365 . Finally we evaluate the
ﬁdelity of the explanations given by our method through Amazon Mechanical
Turk and compare with other visual explanation generation methods.
B. Zhou, Y. Sun, D. Bau, A. Torralba
Interpretable Basis Learned from Broden
We derive an interpretable basis from the fully annotated image dataset Broden
 . Because our focus is to explain high-level features of the neural networks in
terms of human-interpretable concepts, we take a subset of the Broden dataset
consisting of object and part concepts. The annotations of the objects and parts
in Broden dataset originally come from the datasets ADE20K , Pascal Context , and Pascal Parts . We remove concepts with fewer than 10 image
samples, resulting to 660 concepts from 30K images used for training and testing.
For each concept in the Broden dataset, we learn a logistic binary classi-
ﬁer. The input of the classiﬁer is a feature vector a(i,j) ∈RD in activation
A ∈RD×H×W , and the output is the prediction of the probability of the concept appearing at (i, j) ∈(range(H), range(W)). Our ground truth labels for
the segmentations are obtained by downsampling the original concept masks to
H ×W size using nearest neighbor. Note that Broden provides multi-labeled segmentations, and there are often several concepts present in each downsampled
pixel. Therefore it is appropriate for each concept classiﬁer to be trained independent of each other. Because the number of positive samples and the number
of negative samples for some concepts are highly unbalanced, we resample the
training set to keep the ratio of positive and negative examples of each class
ﬁxed at 1 : 20 and use ﬁve rounds of hard negative mining.
We evaluate the accuracy of the deep features learned from several networks
as shown in Table 1. All models are evaluated with mAP on a ﬁxed validation
set of Broden dataset.
Table 1. The mAP of the learned concept classiﬁers for the object and part concepts
in the Broden dataset. The features used are the activations at the ﬁnal convolutional
layer of the network trained from scratch on Places365.
Explaining Classiﬁcation Decision Boundaries
Interpretable Basis Decomposition assigns a basis of interpretable concepts for
each output class. This basis can be seen as a set of compositional rules between
the output classes and the elementary concepts in the Broden datasets. Diﬀerent
networks learn a diﬀerent set of such semantic rules to make predictions, thus
by directly examining the interpretable basis decomposition of a network we
can gain insight about the decision boundaries learned by each network for each
Interpretable Basis Decomposition for Visual Explanation
(b) Comparing diﬀerent concepts that diﬀerent networks
(Resnet18, Resnet50, AlexNet-CAM, VGG16-CAM)
utilize to make predictions (places365 label: dining hall, shoe shop).
(a) Comparing diﬀerent concepts that
Resnet18 utilizes to make
diﬀerent predictions.
Fig. 3. Visualizing how diﬀerent networks compose the ﬁnal prediction classes using
the Broden concepts. The left labels in each graph show the classes of Places365 and
the right labels are the concepts of Broden. The thickness of each link between a class
and a concept indicates the magnitude of the coeﬃcient sci.
Speciﬁcally, our method decomposes each weight vector wk of class k in the
last layer2 as the sum wk = sc1qc1 + · · · + scnqcn + r, where qci represents the
embedding vector for concept ci and sci is the coeﬃcient indicating its contribution to the overall class k. This decomposition indicates a relationship between
the output class k and the concept ci described by the coeﬃcient sci. In Fig. 3,
we visualize a subset of Places365 classes k and how they are decomposed into
Broden concepts ci by diﬀerent networks. The left column of the ﬁgure is the
list of Places365 classes to be decomposed. The right column shows the related
concepts from the Broden dataset. The thicknesses of the arcs between classes
and concepts are drawn to show the magnitude of the coeﬃcients sci. The larger
sci, the more important concept ci is to the prediction of class k.
In Fig. 3.(a), it can be seen how a single network composes concepts to constitute a variety of diﬀerent prediction classes. Note that all the classes shown
in (a) share the same concept “cliﬀ” but diﬀer in the importance given to this
concept, which can be seen as diﬀerent sci. Fig. 3.(b), shows the diﬀerent compositional rules that diﬀerent networks use to make the same prediction for a
class. For example, in the prediction class “shoe shop”, all networks agree that
“shoe” is a key element that contributes to this prediction, while they disagree
on other elements. VGG16 treats “boot” and “price tag” as important indicators of a “shoe shop,” while and AlexNet decomposes “shoe shop” into diﬀerent
concepts such as “glass” and “check-in-desk.”
2 For this experiment, we replace the fc layers in AlexNet and VGG16 with a GAP
layer and retrain them, similar to 
B. Zhou, Y. Sun, D. Bau, A. Torralba
Explaining Image Predictions
Given the interpretable basis decomposition wk = sc1qc1 + · · · + scnqcn + r, the
instance prediction result wT
k a is decomposed as wT
k a = sc1qT
c1a+· · ·+scnqT
rT a where each term sciqT
cia can be regarded as the contribution of concept i to
the ﬁnal prediction. We rank the contribution scores and use the concept labels
of the top contributed basis as an explanation for the prediction. Each term also
corresponds to a contribution to the CAM or Grad-CAM salience heatmap.
Fig. 4 shows qualitative results of visual explanations done by our method.
For each sample, we show the input image, its prediction given by the network, the heatmaps generated by CAM for Resnet18 and Resnet18, and
the heatmaps generated by Grad-CAM heatmap for AlexNet and VGG166,
and the top 3 contributing interpretable basis components with their labels and
numerical contribution.
In Fig. 4(a), we select three examples from Places365 in which VGG16 and
ResNet18 make the same correct predictions. In two of the examples, the explanations provide evidence that VGG16 may be right for the wrong reasons in
some cases: it matches the airplane concept to contribute to the crosswalk prediction, and it matches the sofa concept to contribute to its market prediction.
In contrast, ResNet18 appears to be sensitive to more relevant concepts.
In Fig. 4(b), we show how our method can provide insight on an inconsistent
prediction. ResNet18 classiﬁes the image in last row as an art school because it
sees features described as hand and paper and drawing, while VGG16 classiﬁes
the image as a cafeteria image because VGG16 it is sensitive to table and chair
and map features. Both networks are incorrect because the table is covered with
playing cards, not drawings or maps, and the correct label is recreation room.
In Fig. 4(c), we show the variations generated by diﬀerent models for the
same sample.
Human Evaluation of the Visual Explanations
To measure whether explanations provided by our method are reasonable and
convincing to humans, we ask AMT raters to compare the quality of two different explanations for a prediction. We create explanations of decisions made
by four diﬀerent models (Resnet50, Resnet18, VGG16, and AlexNet, trained on
Places365) using diﬀerent explanation methods (Interpretable Basis Decomposition, Network Dissection, CAM and Grad-CAM).
The evaluation interface is shown in Fig. 5. In each comparison task, raters
are shown two scene classiﬁcation predictions with identical outcomes but with
diﬀerent explanations. One explanation is identiﬁed as Robot A and the other
as Robot B, and raters are asked to decide which robot is more reasonable on a
ﬁve-point Likert scale. Written comments about the diﬀerence are also collected.
In the interface, heatmaps are represented as simple masks that highlight the
top 20% of pixels in the heatmap; explanations are limited to four heatmaps;
and each heatmap can be labeled with a named concept.
Interpretable Basis Decomposition for Visual Explanation
Fig. 4. Explaining speciﬁc predictions. The ﬁrst image pair in each group contains original image (left) and single heatmap (right), with the predicted label and normalized
prediction score in parentheses. Single heatmaps are CAM for ResNet and Grad-CAM
for Alexnet and VGG. This is followed by three heatmaps corresponding to the three
most signiﬁcant terms in the interpretable basis decomposition for the prediction. The
percentage contribution of each component to the score is shown. (a) Examples where
two networks make the same prediction. (b) Explanations where two networks make
diﬀerent predictions. (c) Comparisons of diﬀerent architectures.
Fig. 5. Interface for human evaluations. Two diﬀerent explanations of the same prediction are presented, and human raters are asked to evaluate which is more reasonable.
B. Zhou, Y. Sun, D. Bau, A. Torralba
Baseline CAM, Grad-CAM, and Network Dissection explanations We
compare our method to several simple baseline explanations. The ﬁrst baselines
are CAM and Grad-CAM , which consist of a single salience heatmap
for the image, showing the image regions that most contributed to the classi-
ﬁcation. Using the notation of Section 2.1, the CAM/Grad-CAM heatmap is
given by weighting the pixels of the penultimate feature layer A according to the
classiﬁcation vector wk, or to the pooled gradient wk(A):
CAMk(A) ≡wT
Grad-CAMk(A) ≡wk(A)T A
The second baseline is a simple unit-wise decomposition of the heatmap as
labeled by Network Dissection. In this baseline method, every heatmap corresponds to a single channel of the featuremap A that has an interpretation as
given by Network Dissection . This baseline explanation ranks channels according to the components i that contribute most to wT
i wkiai. Using the
notation of Section 2.1, this corresponds to choosing a ﬁxed basis C where each
concept vector is the unit vector in the ith dimension qci = ei, labeled according
to Network Dissection. Heatmaps are given by:
NetDissectk,i(A) ≡eT
ranked by largest wkiai
CAM and the Network Dissection explanations can be thought of as extremal
cases of Interpretable Basis Decomposition: CAM chooses no change in basis
and visualizes the contributions from the activations directly; while Network
Dissection always chooses the same unit-wise basis.
Comparing Explanation Methods Directly. In the ﬁrst experiment, we
compare explanations generated by our method head-to-head with explanations
generated by Network Dissection and CAM and Grad-CAM . In this
experiment, both Robot A and Robot B are the same model making the same
decision, but the decision is explained in two diﬀerent ways. For each network
and pair of explanation methods, 200 evaluations of pairs of explanations are
done by at least 40 diﬀerent AMT workers. Fig. 8 summarizes the six pairwise
comparisons. Across all tested network architectures, raters ﬁnd our method
more reasonable, on average, than then explanations created by CAM, Grad-
CAM, and Network Dissection.
Representative samples with comments from the evaluation are shown in
Fig. 6. Raters have paid attention to the quality and relevance of the explanatory
regions as well as the quality and relevance of the named concepts. When comparing the single-image explanations of CAM and Grad-CAM with our multipleimage explanations, some raters express a preference for shorter explanations
and others prefer the longer ones. Since is generally assumed that humans have
a strong bias towards simpler explanations ( ), it is interesting to ﬁnd that, on
average, human raters prefer our longer explanations. The second experiment,
described next, controls for this bias by evaluating only comparisons where raters
see the same type of explanation for both Robot A and Robot B.
Interpretable Basis Decomposition for Visual Explanation
A: Resnet-50 explained with NetDissect
B: Resnet-50 explained with our method
Worker 1: B is clearly
more reasonable because
“Explains more items that
are usually in a locker
Worker 2: B is clearly
more reasonable because
“Robot A has features that
are not attributes of a
locker room”
A: Resnet-50 with CAM
B: Resnet-50 with our method
Worker 1: B is slightly
less reasonable because
“A is simpler and slightly
more accurate”
Worker 2: B is clearly
more reasonable because
“Robot B provides more
pictures and details than
Fig. 6. Representative examples of human feedback in head-to-head comparisons of
methods. For each image, one comparison is done. At left, explanations using Net
Dissection and our method are compared on same ResNet50 decision. At right, explanations using CAM and our method are compared on another ResNet50 decision.
A: Resnet-18 explained with our method
B: Resnet-50 explained with our method
Worker 1: B is clearly
more reasonable because
“Both have accurately
identiﬁed ‘stove’, but B is
much closer on identifying
‘door’ objects and
correctly saw ‘work
surface’ while the rest of
A’s are questionable”
Worker 2: Equally
reasonable because
“They both show equal
A: Resnet-18 with CAM
B: Resnet-50 with CAM
Worker 1: B is clearly less
reasonable because
“Search area [of A] is
more focused on the
major factors of a Galley”
Worker 2: Equally
reasonable because
“Same result via the same
Fig. 7. Representative examples of human feedback in trust comparison. For each
image, two independent comparisons are done. At left, a decision of ResNet50 and
ResNet18 are compared using our method of explanation. At right, the same pair of
decisions is compared using a CAM explanation.
Fig. 8. Comparing diﬀerent explanation
methods side-by-side. Each bar keeps the
network the same and compares our explanations to another method. Blue and
green indicate ratings of explanations of
our method that are clearly or slightly
more reasonable, and yellow and orange
indicate ratings for where our method is
slightly or clearly less reasonable than a
diﬀerent explanation method.
Fig. 9. Comparing ability of users to
evaluate trust using diﬀerent explanation
methods. Each bar keeps the explanation
method the same and compares ResNet50
to another model. Blue and green indicate evaluations where ResNet50 explanations are rated clearly and slightly
more reasonable, and yellow and orange
indicate explanations where ResNet50 is
slightly and clearly less reasonable.
B. Zhou, Y. Sun, D. Bau, A. Torralba
Comparing Evaluations of Model Trust. The second experiment evaluates
the ability of users to evaluate trustworthiness of a model based on only a single
pair of explanations. The ordinary way to evaluate the generalization ability of
a model is to test its accuracy on a holdout set of many inputs. This experiment
tests whether a human can compare two models based on a single comparison
of explanations of identical decisions made by the models on one input image.
In this experiment, as shown in Fig. 7, explanations for both Robot A and
Robot B are created using the same explanation method (either our method or
CAM), but the underlying networks are diﬀerent. One is always Resnet50, and
the other is either AlexNet, VGG16, Resnet18, or a mirrored version of Resnet50
(resnet50∗) where all the convolutions are horizontally ﬂipped. Only explanations
where both compared networks make the same decision are evaluated: as can be
seen in the feedback, our explanation method allow raters to discern a quality
diﬀerence between deeper and shallower methods, while the single-image CAM
heatmap makes the two networks seem less diﬀerent.
Fig. 9 summarizes results across several diﬀerent network architectures. With
our explanation method, raters can identify that Resnet50 is more trustworthy
than Alexnet, VGG16 and Resnet18; the performance is similar to or marginally
better than Grad-CAM, and it outperforms CAM. Comparisons of two Resnet50
with each other are evaluated as mostly equivalent, as expected, under both
methods. It is interesting to see that it is possible to discern the diﬀerence
between shallower and deeper networks despite a very narrow diﬀerence in validation accuracy between the models, even after observing only a single case on
which two diﬀerent models perform identical predictions.
Discussion and Conclusion
The method has several limitations: ﬁrst, it can only identify concepts in the
dictionary used. This limitation can be quantiﬁed by examining the magnitude
of the residual. For scene classiﬁcation on ResNet50, explanations derived from
our dataset of 660 concepts have a mean residual of 65.9%, suggesting most of
the behavior of the network remains orthogonal to the explained concepts. A
second limitation is that the residual is not guaranteed to approach zero even if
the concept dictionary were vast: decisions may depend on visual features that
do not correspond to any natural human concepts. New methods may be needed
to to characterize what those features might be.
We have proposed a new framework called Interpretable Basis Decomposition
for providing visual explanations for the classiﬁcation networks. The framework
is able to disentangle the evidence encoded in the activation feature vector and
quantify the contribution of each part of the evidence to the ﬁnal prediction.
Through crowdsourced evaluation, we have veriﬁed that the explanations are
reasonable and helpful for evaluating model quality, showing improvements over
previous visual explanation methods.
Acknowledgement: The work was partially funded by DARPA XAI program
FA8750-18-C0004, the National Science Foundation under Grants No. 1524817, and
the MIT-IBM Watson AI Lab. B.Z is supported by a Facebook Fellowship.
Interpretable Basis Decomposition for Visual Explanation