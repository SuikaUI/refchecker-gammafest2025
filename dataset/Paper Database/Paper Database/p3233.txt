Received January 8, 2019, accepted April 12, 2019, date of publication April 23, 2019, date of current version June 4, 2019.
Digital Object Identifier 10.1109/ACCESS.2019.2912823
TSViz: Demystification of Deep Learning
Models for Time-Series Analysis
SHOAIB AHMED SIDDIQUI
1,2, DOMINIQUE MERCIER1,2, MOHSIN MUNIR
ANDREAS DENGEL
1,2, AND SHERAZ AHMED
1German Research Center for Artiﬁcial Intelligence (DFKI), 67663 Kaiserslautern, Germany
2Technische Universitaet Kaiserslautern, 67663 Kaiserslautern, Germany
Corresponding author: Shoaib Ahmed Siddiqui ( )
This work was supported in part by the BMBF project DeFuseNN under Grant 01IW17002, and in part by the NVIDIA AI Lab (NVAIL)
ABSTRACT This paper presents a novel framework for the demystiﬁcation of convolutional deep learning
models for time-series analysis. This is a step toward making informed/explainable decisions in the domain
of time series, powered by deep learning. There have been numerous efforts to increase the interpretability
of image-centric deep neural network models, where the learned features are more intuitive to visualize.
Visualization in the domain of time series is signiﬁcantly challenging, as there is no direct interpretation of
the ﬁlters and inputs compared with the imaging modality. In addition, a little or no concentration has been
devoted to the development of such tools in the domain of time series in the past. TSViz provides possibilities
to explore and analyze the network from different dimensions at different levels of abstraction, which
includes the identiﬁcation of the parts of the input that were responsible for a particular prediction (including
per ﬁlter saliency), importance of the different ﬁlters present in the network, notion of diversity present in
the network through ﬁlter clustering, understanding of the main sources of variation learned by the network
through inverse optimization, and analysis of the network’s robustness against adversarial noise. As a sanity
check for the computed inﬂuence values, we demonstrate our results on pruning of neural networks based on
the computed inﬂuence information. These representations allow the user to better understand the network
so that the acceptability of these deep models for time-series analysis can be enhanced. This is extremely
important in domains, such as ﬁnance, industry 4.0, self-driving cars, health care, and counter-terrorism,
where reasons for reaching a particular prediction are equally important as the prediction itself. We assess
the proposed framework for interpretability with a set of desirable properties essential for any method in this
direction. Code is available at: 
INDEX TERMS Deep learning, representation learning, convolutional neural networks, time-series analysis,
time-series forecasting, feature importance, visualization, demystiﬁcation.
I. INTRODUCTION
Despite the astonishing results from deep learning based
models in a range of different applications which includes
computer vision , speech analysis , translation systems etc., there has been limited applicability of these
high-performance models due to their black-box nature and
lack of explainability of their decisions . This is speciﬁcally true for domains like business, ﬁnance, natural disaster
management, health-care, self-driving cars, industry 4.0 and
counter-terrorism where reasons for reaching a particular
decision are equally important as the prediction itself .
The associate editor coordinating the review of this manuscript and
approving it for publication was Bora Onat.
There have been signiﬁcant attempts to uncover the blackbox nature of these deep learning based models – ,
where visualization of the model has been the most common
strategy. Almost all of the proposed visualization systems are
image-centric where visualizing the images is directly interpretable for humans (natural association to similar looking
objects like eyes, faces, dogs, cars etc.). Most of the ideas are
equally applicable to time-series, but the unintuitive nature of
the time-series data makes it difﬁcult to directly transfer these
ideas to aid human understanding.
This paper presents a novel system i.e. Time-series Visualization framework (TSViz), for the demystiﬁcation of deep
models for time-series analysis. In particular, the contributions of this paper are manifold:
VOLUME 7, 2019
2019 IEEE. Translations and content mining are permitted for academic research only.
Personal use is also permitted, but republication/redistribution requires IEEE permission.
See for more information.
S. A. Siddiqui et al.: TSViz: Demystification of Deep Learning Models for Time-Series Analysis
• An inﬂuence tracing algorithm to compute the input
saliency map, which enables an understanding of the
regions of the input that were responsible for a particular
prediction.
• An approach to compute the ﬁlter’s inﬂuence using the
proposed inﬂuence tracing algorithm. Filter importance
is computed based on its inﬂuence on the ﬁnal output.
This information provides an idea to the user regarding
the ﬁlters of the network that were important for a
particular prediction.
• An approach to discover the diversity present in the
network based on ﬁlter clustering. Filters belonging to
the same cluster exhibit similar behavior in terms of their
activation pattern, therefore, responding to the same
concept/feature.
• An evaluation of pruning of the network leveraging the
inﬂuence information as a sanity check for the utility of
the information encapsulated into the computed inﬂuence. The aim of this evaluation is to move towards
a principled design of network where the complexity
of the problem aligns well with the complexity of the
network. With the computed inﬂuences, it is possible to
identify parts of the network responding/tuned to highly
speciﬁc stimulus. These parts which contribute to the
overﬁtting of the network can be pruned to promote
generalization.
• An inverse optimization framework where we optimize
the input considering the network parameters as ﬁxed.
This inverse optimization based framework enables the
user to understand the main sources of variation learned
by the network in the input space.
• An approach for the demystiﬁcation of network through
white-box gradient based adversarial attack methods
(FGSM and iterative FGSM ) which provides
an understanding of the impact of adversarial noise to
the inspected model. To the best of our knowledge, this is
the ﬁrst attempt to understand the impact of adversarial
noise on deep learning models for time-series analysis.
This evaluation helps in answering two different questions: (a) Robustness of the network against adversarial
noise and (b) network’s understanding of the changes in
input which can bring maximal changes to the output.
The second question, albeit being more interesting from
our perspective, is sometimes difﬁcult to answer in cases
where the network is highly susceptible to adversarial
• A novel 3D visualization framework for time-series deep
learning models. This framework is generic and capable
of visualizing convolutional deep learning models for
time-series analysis. TSViz provides an opportunity to
explore the network at different levels of abstraction i.e.
from abstract to detailed view.
This paper is structured as follows. We provide a brief
overview of the previous work in the direction of the demystiﬁcation of deep learning models in Section II. We then
provide details regarding the datasets and the network
architectures that we employed in our experimental setup in
Section III. The proposed approach is presented in Section IV
which covers both the method and the corresponding experimentation. We then present an assessment of the proposed
method based on a set of desirable properties essential for
any method related to network interpretability in Section V.
Finally, we present the implementation details (Section VI)
and provide a brief discussion regarding the obtained results
(Section VII) before concluding the paper.
II. RELATED WORK
Signiﬁcant efforts have been made to understand the learned
features of a model in order to demystify the deep learning
black-box. This is speciﬁcally the case for visual recognition
models since visualizing the kernels themselves and their
activations can give hints about the feature that the system is
learning and the kernels themselves are directly interpretable
by humans – , . One of the most common and
effective techniques for network visualization (speciﬁcally
for visual modality) is saliency maps which highlights
the regions which the network focused on in order to generate
a particular prediction. The Layer-wise Relevance Propagation (LRP) framework presented a different approach for
tracing back the inﬂuence using relevance instead of the
gradients . In order to visualize human understandable
features/concept that the neuron is responding to, there have
been signiﬁcant efforts in detecting which input maximally
activates a neuron . The problem of discovering the
part of the input that was most responsible for a particular prediction has also been extensively studied – .
Shrikumar et al. presented an alternate formulation from gradient-based methods for obtaining the maximally activating input. Li et al. used saliency
maps to identify focus regions for textual data.
Despite these advancements, the area of network visualizations for time-series analysis has remained unexplored
until now. A recent attempt has been made by Kumar et al.
 to visualize the input points which were most inﬂuential for a particular prediction through gradients (saliency).
Theoretical contributions have also been made in order to
understand the amazing generalization capabilities of these
deep models. Zhang et al. presented an empirical analysis to divert attention to the philosophical topic
of what is actually perceived as generalization. Information
bottleneck theory has also gained popularity as a method
for explaining the generalization and learning capabilities
of deep learning based models. Koh and Liang 
presented inﬂuence functions as a methodology to trace back
model predictions in terms of its training data. This analysis
enabled the discovery of dataset errors, model debugging,
and creation of visually-indistinguishable adversarial training examples which are able to ﬂip the network’s test time
predictions.
We refer readers to for a comprehensive review of the
previous work in this direction based on the tutorial given at
ICASSP .
VOLUME 7, 2019
S. A. Siddiqui et al.: TSViz: Demystification of Deep Learning Models for Time-Series Analysis
III. DATASETS AND ARCHITECTURES
We opted for two different problem formulations common
in time-series context: Time-series regression and time-series
classiﬁcation.
A. REGRESSION
For the regression setting, we trained a network on the Internet Trafﬁc Dataset for time-series forecasting (we only
used the B5M dataset). The network operates on an input
size of 50 time-steps and is comprised of three convolutional,
two max-pooling and one dense layer. The network is trained
using Adam optimizer with a learning rate of 0.001 for
5000 epochs using a batch size of 5 and Mean Squared
Error (MSE) as the loss function. This network was able to
achieve state-of-the-art forecasting results on the aforementioned dataset. The network takes in a bi-channel input where
the ﬁrst channel corresponds to the original input signal while
the second channel is comprised of ﬁrst-order derivative of
the original signal.
B. CLASSIFICATION
For the classiﬁcation setting, we created a dummy timeseries anomaly detection dataset for binary classiﬁcation1.
The dataset is comprised of 60000 sequences of 50 timesteps each where each time-step contains values for pressure,
temperature and torque. We randomly introduce point anomalies in the dataset and mark sequences containing such point
anomalies as anomalous. We never introduced any anomalies
in the pressure signal. The dataset is split into 50000 train
and 10000 test sequences. The network is comprised of three
convolutional layers comprising of 16, 32 and 64 ﬁlters
respectively, followed by a single dense layer. The network is
trained using binary cross-entropy loss. The hyperparameters
were chosen based on our best guess without cross-validation
since the focus of our work is on interpretability instead of
performance.
IV. TSVIZ: THE PROPOSED APPROACH
TSViz provides the possibility of interpreting any convolutional network from several dimensions, at different levels
of abstraction. This includes the global picture like the types
and ordering of different layers present in the network, and
their corresponding number of ﬁlters, moving onto more
detailed information like parts of the input that each ﬁlter is
responding to (Section IV-B.1) as well as their importance
(Section IV-B.2). This also includes ﬁlter grouping where
ﬁlters which are exhibiting similar behavior are clustered
together (Section IV-C), which captures the notion of network
diversity. TSViz also uncovers other hidden aspects of the
network based on inverse optimization (Section IV-D) and
adversarial examples (Section IV-E).
1Dataset available at: 
A. BACKGROUND
TSViz is based on the principle of backpropagation proposed
by Rumelhart et al. , which is essentially the chain
rule from calculus. Backpropagation algorithm provides an
efﬁcient way to compute the inﬂuence of a tensor w.r.t.
another tensor. The same framework is the workhorse for the
training of the deep learning models where the inﬂuence of
the network parameters is computed on the ﬁnal cost/loss
function. We leverage this capability to compute inﬂuences
for uncovering the deep learning black-box by computing the
inﬂuence of the input on the current ﬁlter, which is the input
saliency map. This also enables the discovery of the ﬁlter’s
importance by computing its inﬂuence on the ﬁnal prediction of the network. Therefore, this section provides a short
recapitulation of the basic concept of the backpropagation
algorithm along with laying out the necessary notation to be
used later.
Training of a neural network is achieved by reducing the
loss function L : R × R 7→R+ (L : RC × RC 7→R+ in
case of multi-class classiﬁcation where C denotes the number of classes). The loss function captures the discrepancy
between the network’s prediction and the desired output.
Ideally, the network must output a value which is the same as
the target. The whole learning process is about reducing the
discrepancy between the two values. As the network output
is calculated based on the weights and biases of the different
neurons involved, these weights and biases are adapted during
the learning process in order to reduce the loss function.
Ultimately, backpropagation is about understanding how the
change in the weights and biases of a network affects its loss
and updating the network parameters in the direction with the
maximum decrease in the loss function. This computation
of the optimal direction can be obtained by calculating the
partial derivatives of the loss function with respect to any
weight W or bias b as ∂L/∂W and ∂L/∂b.
The learning process can be decomposed into four steps
which include: (i) Feed-forward pass through the network,
(ii) Backpropagation to the output layer, (iii) Backpropagation to the hidden layers, (iv) Updating network parameters using an optimizer like SGD. In the feed-forward pass
through the network, the output of all the hidden neurons is
computed which is then used for the computation of the ﬁnal
network output. This evaluation is based on the randomly
initialized weights. Based on the computed output, the ﬁnal
loss function is evaluated. The networks in deep learning
are mostly comprised of both convolutional and dense layers. Rectiﬁed Linear Unit (ReLU) or some variant of it is
commonly used as the non-linearity/activation function. The
activation for the dense and convolutional layers is presented
in Eq. 2 and Eq. 4 respectively. al
j denotes the activation of
the jth neuron in the lth layer (for dense layers) while al
denotes the activation of the jth neuron in the lth layer at the
ith input location (for convolutional layers). k is deﬁned as
⌊FilterSize/2⌋for convolutional layers while |zl−1
VOLUME 7, 2019
S. A. Siddiqui et al.: TSViz: Demystification of Deep Learning Models for Time-Series Analysis
the number of neurons in the previous layer l −1.
The error is backpropagated to the initial layers, and the
gradient with respect to the network parameters is computed
(weights and biases). The error δ of jth neuron at the output
layer L is presented in Eq. 6.
max′(x, 0) =
Once the gradient of the loss w.r.t. the output layer is
computed, the error is backpropagated to all neurons in the
hidden layers using Eq. 7. The gradient of ReLU is 1 where
the value of input x exceeds 0 and remains 0 otherwise as
mentioned in Eq. 8. Similarly, the max-pooling layer has
gradient equal to 1 wherever the maximum quantity occurs
and remains 0 otherwise. The rate of change of loss L w.r.t.
the bias and weights in the lth layer is given in Eq. 9 and Eq. 10
respectively.
After computation of the gradients, the network parameters
(weights and biases) are updated in the negative gradient
direction as this is the optimal direction for maximum reduction in the loss.
B. INFLUENCE COMPUTATION
TSViz contributes to the interpretability of deep learning
models designed speciﬁcally for time-series analysis tasks
at different levels of abstraction. The ﬁrst and one of the
most intuitive explanation for any model is based on the
inﬂuence of the input on the ﬁnal prediction. Consequently,
this inﬂuence can also be computed for the intermediate states
of the network (both in the forward as well as the backward
direction).
There are two different inﬂuences that can be computed
based on any particular ﬁlter in the network. The ﬁrst inﬂuence stems from the fact that the input has an impact on the
output of the particular ﬁlter in question (Section IV-B.1)
while the second inﬂuence is of the ﬁlter itself on the ﬁnal
output (Section IV-B.2). We will now visit each of these
inﬂuences in detail.
1) INPUT INFLUENCE
The ﬁrst inﬂuence originates from the input. This information
provides important insights regarding the data points in the
input that the network is actually responding to for computation of its output. The information regarding the parts of
the input which were responsible for a particular prediction is
considered a viable explanation in many scenarios including
domains like self-driving cars , ﬁnance and medical
imaging . It is important to note that we also compute the
input’s inﬂuence for every ﬁlter along with the ﬁnal output.
This value can be obtained by computing the gradient of the
current layer l w.r.t. the input layer. We use the absolute value
of the gradient as the magnitude is of relevance irrespective of
direction. We denote the input as a0, therefore, this inﬂuence
of the input can be computed using Eq. 13.
input = |δ0|
In order to be able to visualize and compare the saliency of
the different ﬁlters, the absolute values of the inﬂuences are
scaled using the min-max scaling presented in Eq. 14.
Iinput −min
input −min
Fig. 1a visualizes a sample of an anomalous input in the
anomaly detection dataset. Fig. 1b equips the raw ﬁlter output
with the saliency information to provide a direct interpretation of its utility. It is evident from the ﬁgure that the
network focused on sudden peaks present in the input to mark
the sequence as anomalous. This saliency is computed using
Eq. 13 after applying min-max normalization.
2) FILTER INFLUENCE
Another interesting inﬂuence originates from the output,
which can be leveraged to compute the ﬁlter’s inﬂuence. This
information about ﬁlter importance provides hints regarding
the ﬁlters that were most inﬂuential for a particular prediction. Interestingly, many of the ﬁlters in the network contribute nothing for a particular prediction. This information
is complementary to the information regarding parts of the
input that were responsible for a particular prediction.
VOLUME 7, 2019
S. A. Siddiqui et al.: TSViz: Demystification of Deep Learning Models for Time-Series Analysis
FIGURE 1. A particular anomalous example provided to the network form the anomaly detection dataset. The left figure (a) visualizes the raw
input while the figure on the right (b) equips the raw signal with the saliency information.
FIGURE 2. Filters of the third convolutional layer of the network trained on the internet traffic dataset where we first visualize the raw filters (a), followed
by additional information regarding the filter importance and saliency information (b), finally adding the information regarding the filter cluster (c).
In order to obtain this inﬂuence, we compute the gradient
of the output layer L w.r.t. the current layer l. This provides
us with a point-wise estimate about how each value impacts
the output activation aL. In this case, again both positive and
negative inﬂuences are equally important to us. As we are
interested in the overall inﬂuence of a particular ﬁlter, therefore, point-wise inﬂuence estimates can be reduced to a single
value by computing the Manhattan or 1-norm of the inﬂuence
vector. Computing the 1-norm of the inﬂuence vector retains
the information encapsulated in the vector, by taking the sum
of the absolute inﬂuences of each of its components, which
provides a good estimate regarding the overall importance of
the ﬁlter. Eq. 16 provides the mathematical formulation of the
inﬂuence of layer l on the ﬁnal output aL.
Fig. 2a visualizes the third convolutional layer of the network trained on the internet trafﬁc dataset. Fig. 2b enhances
the ﬁlter view by including the ﬁlter importance information
computed using Eq. 16 after applying the min-max normalization along with the input saliency information.
Proposition 1 (Zero inﬂuence): For
predictions (with probability of either 0.0 or 1.0) in case of
classiﬁcation, the inﬂuence dies off.
Let x ∈RD be the input and ˆy ∈R (ˆy ∈RC
in case of multi-class classiﬁcation where C denotes the
number of classes) be the prediction by the system. For binary
classiﬁcation, the output probability is usually obtained by
the application of sigmoid activation, while in the case of
multi-class classiﬁcation, the output probability is obtained
by the application of the softmax activation function. These
activation function can be considered as the last layer L in
the network. The gradient of the sigmoid and the softmax
activation function w.r.t. to its input is presented in Eq. 17
and Eq. 18 respectively.
δL = ˆy(1 −ˆy)
ˆyi(1 −ˆyi),
ˆyi(−ˆyj),
VOLUME 7, 2019
S. A. Siddiqui et al.: TSViz: Demystification of Deep Learning Models for Time-Series Analysis
In case of extremely conﬁdent predictions, the system
makes binary predictions where the probability either goes
to zero or one i.e. ˆy ∈{0, 1} (ˆy ∈{0, 1}C in case of
multi-class classiﬁcation where C denotes the number of
classes). Therefore, during backpropagation, the gradient dies
off due to multiplication by zero as highlighted in Eq. 17
and Eq. 18 (either the ﬁrst term or the second term goes to
zero due to presence of saturated values). This results in no
gradient to previous layers for the computation of the ﬁlter
inﬂuence or saliency for that matter.
A rudimentary way to overcome the problem of obtaining
no inﬂuence values (Proposition 1) is to employ temperatureaugmented softmax in multi-class classiﬁcation settings by
using T > 1 as the temperature (Eq. 19). The temperature T
also negatively impacts the gradient, perturbing the actual
inﬂuence. Therefore, the value of T should be kept close to
one in order to make sure that the overconﬁdent predictions
are avoided along with a minor impact on the computed
inﬂuences.
A preferable way to avoid these over-conﬁdent predictions
is to add activity regularization on the ﬁnal activation where
the network penalizes large activation values, thus avoiding
extremely conﬁdent predictions. This formulation extends
to both binary as well as multi-class classiﬁcation settings.
Therefore, the updated optimization problem can be written
W∗= arg min
(x,y)∈X ×Y
L(φ(x; W), y)
where zL is the activation value of the last layer and W =
{W l, bl}L
l=1 encapsulates all the parameters of the network.
It is important to note that this formulation requires tuning
of yet another hyperparameter i.e. β in order to achieve
reasonable performance while simultaneously avoiding overconﬁdent predictions.
C. FILTER CLUSTERING
Deep networks are great at exploiting redundancy , therefore, it is important to get a measure of the diversity present
in the network. In order to capture this diversity, we perform
ﬁlter clustering. This clustering phase helps us in discovering the distinct types of ﬁlters present in the network as a
notion of the diversity it attained during training. We cluster
ﬁlters based on their activation pattern i.e. ﬁlters with similar
activating patterns are essentially capturing the same concept.
This clustering is also helpful in reducing the information
overload for the user in the visualization phase where only
the most salient ﬁlters from each cluster can be visualized.
As we are only interested in the similarity between the
activation pattern rather than the actual magnitude and the
shifting of the activation pattern (e.g. invariance to the activation at the start or at the end of the peak), we ﬁrst align
the activations of the different ﬁlters in a particular layer.
Since we are operating with 1D signals (each ﬁlter outputs
a 1D activation vector), therefore, in order to compute the
similarities between the ﬁlters, we leveraged a technique
which is very common in time-series analysis community for
alignment called as Dynamic Time Warping (DTW) .
We encode each ﬁlter via its activation vector a ∈Rd
where d denotes the dimensionality of the activation. The
algorithm ﬁrst creates a distance matrix between the every
two activation vectors, am ∈Rd and an ∈Rd. We call the
distance matrix as DTW where DTW[i, j] gives the distance
between the activation vectors a1:i
m and a1:j
with the best
alignment. The DTW matrix can be effectively computed by
a consistent application of Eq. 21 where Euclidean distance
has been employed as the distance metric D(i, j).
DTW[i, j] = D(ai
DTW[i−1, j]
DTW[i, j−1]
DTW[i−1, j−1]
Once the DTW matrix is computed, DTW[d, d] can be
used as a measure of the minimum possible distance to align
the two activation vectors where d is the dimensionality of the
activation vectors. Therefore, we use this distance to cluster
the activation vectors together.
When it comes to clustering, K-Means appears to be the
most common choice for any problem. However, K-Means
operates with Euclidean distance as the distance metric.
Switching to DTW as a distance metric with K-Means results
in either unreliable results or even convergence issues. Therefore, we performed hierarchical (agglomerative) clustering
using DTW where clusters which are closest in terms of
distance are combined together to yield a new cluster during
every iteration of the algorithm until all the data points are
combined into one cluster. There are different possible linkage methods that can be employed to compute the distance
between the clusters. In our case, we used the complete
linkage to compute the distance between clusters. Complete
linkage ﬁnds the maximum possible distance dCL
between the two clusters G and H using pairs of points i and j
from G and H respectively, such that the distance dij ∈R
between the selected points is maximum. This is highlighted
in Eq. 23.
dCL(G, H) =
i∈G, j∈H dij
The system builds a dense hierarchy of clusters. Since we
have a dense hierarchy, the user can navigate the hierarchy
slicing the y-axis at different points to obtain a different
number of clusters. Since it is very sensible to start with the
best estimation of the possible number of clusters rather than
randomly starting in the middle or at the end, we employ
the silhouette method to select the initial number of clusters.
VOLUME 7, 2019
S. A. Siddiqui et al.: TSViz: Demystification of Deep Learning Models for Time-Series Analysis
FIGURE 3. Grid view of all the filters equipped with the cluster information (second convolutional layer of the network trained on the anomaly detection
dataset). Different shades represents different clusters.
FIGURE 4. Silhouette plot for deciding the initial number of clusters for
the second convolutional layer on the anomaly detection dataset.
We compute the silhouette score for each possible number of
clusters (except the case where every point is classiﬁed into
one cluster or where every point is classiﬁed into a separate
cluster) and then select the initial number of clusters to be
at the point where the maximum silhouette score is obtained.
This process is illustrated in Fig. 4.
Fig. 2c provides a depiction of equipping the ﬁlters with
cluster information trained on the internet trafﬁc dataset.
We also tested this clustering strategy on the anomaly detection dataset and visualized the resulting clusters in a grid view
for clarity. This visualization is presented in Fig. 3. It can
be seen that the silhouette method did a reasonable job in
selecting the initial number of clusters.
D. INVERSE OPTIMIZATION
An understanding of the parts of the input that are considered
to be most important ones for a particular prediction is of
tremendous value. This information is partly highlighted by
computing the input inﬂuence (Section IV-B.1). However,
this input inﬂuence is not very stable (Section V-C). Therefore, we use the inverse optimization based framework to
discover the main sources of variation learned by the network
in the input space. We start from a random input and modify
the signal until the desired output is achieved. This optimization based approach gives a true picture of what the network
considers to be the main reason for achieving a particular
prediction.
During training, we use the loss to minimize the discrepancy between the prediction ˆy ∈R and ground-truth y ∈R,
where both x ∈RD and y ∈R are ﬁxed (y ∈RC, ˆy ∈RC in
case of multi-class classiﬁcation where C denotes the number
of classes). The optimization problem to discover the optimal
VOLUME 7, 2019
S. A. Siddiqui et al.: TSViz: Demystification of Deep Learning Models for Time-Series Analysis
FIGURE 5. Application of the inverse optimization framework for the classification use-case. The left figure (a) demonstrates the random starting
position which was classified as non-anomalous with a probability of 91.66% while the figure on the right (b) highlights the final series obtained
after optimization which was classified as anomalous with a probability of 99.88%. (a) Start (8.34% conf). (b) End (99.88% conf).
FIGURE 6. Application of the inverse optimization framework for the regression use-case. The left figure (a) demonstrates the random starting
position which resulted in a forecasted value of -0.49923 while the figure on the right (b) highlights the final series obtained after optimization
which resulted in a forecasted value of -0.95739.
parameters of the network W∗can be written as:
W∗= arg min
(x,y)∈X ×Y
L(φ(x; W), y)
where φ deﬁnes the map from the input space x ∈RD
to the label space ˆy ∈R (ˆy ∈RC in case of multi-class
classiﬁcation) parameterized by the network weights W =
{W l, bl}L
l=1. Once the network is trained, i.e. we have W∗,
the problem can be inverted to discover the input ˆx ∈RD
which produces the same output as a particular time-series.
This helps in the discovery of the main sources of variation
learned by the network. The following optimization problem
can be expressed as Eq. 25. We solve this problem iteratively
using gradient descent as indicated in Eq. 26.
ˆx∗= arg min
ˆx ∈RD L(φ(ˆx; W∗), y)
t+1 = ˆx∗t −α ∂L(φ(ˆx; W∗), y)
This optimization problem can be efﬁciently solved again
using the backpropagation algorithm. We initially sample
0 ∼N(0, I) from a standard-normal distribution. Since
we initialize the input randomly from a normal distribution,
the initial time-series looks distorted. This initialization is
visualized in Fig. 5a for the classiﬁcation use-case. Upon
passing this randomly initialized series to the trained anomaly
detection model, it marked the sequence as non-anomalous
(8.34% probability for the sequence belonging to the anomalous class). We then optimized this sequence as mentioned in
Eq. 26 using gradient descent. The ﬁnal sequence obtained
after optimizing the input for 1000 iterations (t = 1000)
with α = 0.01 is visualized in Fig. 5b. Since the original
dataset contained point anomalies, the network introduced
point anomalies in the initial time-series to convert the nonanomalous sequence to an anomalous one with a probability of 99.88%. It is interesting to note that since we never
introduced any point anomalies in the pressure signal, during
inverse optimization, the network also left the pressure signal intact with only minor changes. We similarly performed
inverse optimization tests on the internet trafﬁc dataset (timeseries forecasting). The seemingly unimportant channel containing the ﬁrst-order derivative based on the saliency map
was the main factor that the network nudged in order to obtain
the same output as the time-series visualized in Fig. 6a. The
inversely optimized series is visualized in Fig. 6b. The last
time-step in the original signal (internet trafﬁc) indicates the
forecasted value.
E. ADVERSARIAL EXAMPLES
Deep learning models have been discovered to be highly
prone to adversarial noise . This problem has been very
VOLUME 7, 2019
S. A. Siddiqui et al.: TSViz: Demystification of Deep Learning Models for Time-Series Analysis
FIGURE 7. Discovered adversarial examples for the classification use-case. The left image (a) highlights the input which was classified as
anomalous with a probability of 95.76% while the figure on the right (b) indicates the discovered adversarial example which was classified as
non-anomalous with a probability of 64.45%. (a) Input (95.76% conf). (b) Adversarial output (35.55% conf).
well-studied in prior literature, speciﬁcally for image-based
classiﬁcation networks . We conducted experiments
using iterative FGSM (iterative variant of FGSM )
attack on the studied time-series data which is to the best of
our knowledge, the very ﬁrst attempt to study these methods
for time-series modality. We perform these attacks on regression as well as classiﬁcation networks for time-series. This
directly provides a hint regarding the network’s robustness
against adversarial noise. In situations where the network is
not highly susceptible to adversarial noise, this optimization
step can help answer a more interesting question, regarding
the network’s interpretation of parts of the input that with very
minor perturbation can bring maximal change to the output.
This highlights the network’s understanding of the parts of the
input that were mainly responsible for a particular prediction.
Having this ability to identify how to modify the input to
change the prediction if the model was not susceptible to
adversarial would have revolutionized design (which was the
main intent which led to the discovery of these adversarial
examples). It is important to note that since we perform
iterative optimization, this is different than direct saliency
computation since the input itself is modiﬁed at each timestep.
The FGSM attack performs a single step of optimization
to obtain an adversarial example. We denote the adversarial
example xadv ∈RD. The FGSM optimization problem can
be represented as:
xadv = x + ϵ sign(∂L(φ(x; W∗), y)
The iterative FGSM, instead of solving a single step of optimization, performs iterative reﬁnement of adversarial noise,
therefore signiﬁcantly boosting the chances of producing a
successful adversarial example. This optimization problem
can be written as:
t+1 = Clipx,ϵ
+α sign(∂L(φ(xadv
where Clipx,ϵ bounds the magnitude of the perturbation to be
within [−ϵ, ϵ] from the original example x. The value of the
original example x is used to initialize the initial adversarial
example xadv
Fig. 7a visualizes an original anomalous sequence present
in the anomaly detection dataset. The network successfully marked the sequence as anomalous with a probability
of 95.76%. We then performed the iterative FGSM attack
using Eq. 28, with α = 0.0001, ϵ = 0.1 and t = 1000.
The inverse optimized sequence was predicted to be nonanomalous (35.55% probability of it being an anomalous
sequence) which is visualized in Fig. 7b. Consistent with
our understanding of the anomaly present in the network,
the network reduced the magnitude of the main peak which
was mainly responsible for the anomalous prediction. It is
interesting to note that the network didn’t had such a drastic
reduction in the magnitude of the peak so as to achieve
such dramatic reduction in the probability of the sequence
belonging to the anomalous class. This is indicative of the
network’s susceptibility to adversarial noise.
Adversarial impact on time-series regression task was
much more profound. The seemingly non-important ﬁrstorder turned out to be the main reason for the network’s
vulnerability. The network mainly altered its prediction due
to a signiﬁcant change in the ﬁrst-order derivative rather than
the original signal. Fig. 8 highlights this case. We used the
same parameters i.e. α = 0.0001, ϵ = 0.1 and t = 1000.
Again, the last time-step in the original signal (internet trafﬁc)
indicates the forecasted value.
The focus of our work is not the exploration of adversarial examples or guarantees against adversarial robustness,
but on an intuitive understanding of their existence and
the network’s susceptibility to it. There are more sophisticated attacks like Carlini and Wagner which are
extremely effective in exploiting the network. The focus
here is to promote interpretability and understanding of the
VOLUME 7, 2019
S. A. Siddiqui et al.: TSViz: Demystification of Deep Learning Models for Time-Series Analysis
FIGURE 8. Discovered adversarial examples for the regression use-case. The left image (a) highlights the input which resulted in a forecasted
value of -0.97946 while the figure on the right (b) indicates the discovered adversarial example which resulted in a forecasted value of -0.27947.
FIGURE 9. Min (a), mean (b) and max (c) filter importance computed over the entire dataset for the first, second and the third convolutional layer on
the internet traffic dataset.
FIGURE 10. Test set performance of the network after pruning the specified number of filters from the first, second and the third convolutional layer
on the internet traffic dataset.
F. NETWORK PRUNING
As a sanity check for the utility of the information contained
in the computed inﬂuences, we performed pruning of the
network based on these computed inﬂuences. We use the
ﬁlter inﬂuence and prune ﬁlters based on their inﬂuence. The
ﬁlters with least inﬂuence are pruned ﬁrst followed by ﬁlters
with maximum inﬂuence. Since we would like to prune the
network based on this information, it is important to average
this inﬂuence value over the entire training set. Therefore,
the ﬁnal inﬂuence w.r.t. the output can be written as:
We visualize the minimum, maximum as well as mean
importance of every ﬁlter computed over the entire training
set of the internet trafﬁc dataset in Fig. 9. Fig. 10 visualizes
the results of pruning based on these inﬂuences. We start
pruning with the least inﬂuential ﬁlter until only one ﬁlter is left. It is important to note that we ﬁne-tune the
network for 10 epochs after the pruning step in order to
adjust the network weights to compensate for the missing
Table 1 provides results regarding faithfulness of the computed inﬂuences where we prune the corresponding most and
least inﬂuential ﬁlter of a particular layer without any ﬁnetuning. We will discuss this in detail in Section V-B. For the
sole purpose of pruning to accelerate inference and reduce
model size, we refer readers to more sophisticated techniques
dedicated to pruning relying on second-order gradient information w.r.t. the loss , .
VOLUME 7, 2019
S. A. Siddiqui et al.: TSViz: Demystification of Deep Learning Models for Time-Series Analysis
TABLE 1. Influence faithfulness test for the internet traffic dataset.
V. EVALUATION
properties
interpretability method are faithfulness, stability and explicitness/intelligibility . This section provides an analysis of
the TSViz framework on the basis of these three properties.
A. EXPLICITNESS/INTELLIGIBILITY
Explicitness or intelligibility captures the notion of understandability of the explanations provided by the system. Both
the input and the output modalities are well-understood by the
humans, but the intermediate representations aren’t. Therefore, we try to interpret these intermediate representations in
terms of their inﬂuence on input and output. Since both the
input as well as the output space are interpretable for humans,
this makes the interpretability of the TSViz inﬂuence tracing
algorithm easy. Adversarial examples and inverse optimization also operate in the input space making them intelligible.
B. FAITHFULNESS
Faithfulness captures the notion of the reliability of the
computed relevance. True relevance is subjective and can
vary from task to task. Inﬂuence of noise can be considered
relevant in some cases but might be counter productive to
consider in others . Since we compute the exact inﬂuence
using gradients and backpropagation, it is a reliable indicator.
As a sanity check which is common among literature, ﬁlters
can be removed from the network to assess their impact on
the ﬁnal performance .
For the regression network trained on Internet trafﬁc
dataset, we removed the most and the least inﬂuential ﬁlters
from the ﬁrst convolutional layer to assess their impact on
the ﬁnal loss. As per our expectation, pruning the most inﬂuential ﬁlter had a strong impact on the ﬁnal performance as
compared to pruning the least important ﬁlter.
Fig. 9 provides a depiction of the ﬁlter importance (minimum, maximum and mean importance) computed over the
entire training set of the internet trafﬁc dataset. We used this
mean importance to remove the most and the least important
ﬁlter from each of the three convolutional layers on the
network. Table 1 summarizes the results for the faithfulness
experiment. It is evident from the results that removing the
most important ﬁlter from a layer had a very signiﬁcant
impact on the performance as compared to pruning the least
important ﬁlter. Since we directly set the weights of the
corresponding ﬁlter to zero, therefore, as we ascent the layer
hierarchy, the impact of pruning a particular ﬁlter was more
FIGURE 11. Network overview screen for the regression use-case.
profound (since it had a direct inﬂuence on the result). Pruning also reduces the expected value of the output resulting in a
signiﬁcantly deviated prediction. These results advocate that
the computed inﬂuence was indeed faithful.
C. STABILITY
Since we use the ﬁrst-order gradient to trace the inﬂuence
due to its direct interpretation for humans, this results in
unstable explanations due to noise. Interpretability, therefore,
sometimes leads to the wrong conclusion regarding the
smoothness of the decision boundary which is not the case
in reality . Most interpretability methods suffer from
this inherent weakness due to reliance on ﬁrst-order gradients . Employing second-order methods can resolve
the stability issue, but will make it signiﬁcantly difﬁcult for
humans to comprehend the gained knowledge.
VI. IMPLEMENTATION
We used Keras with TensorFlow backend , 
developed speciﬁcally for deep learning. TensorFlow is an
automatic differentiation package which enables automatic
computation of the gradients. We leveraged this capability to
compute the inﬂuences as described in section IV.
We developed a novel 3D framework for visualization and
demystiﬁcation of any deep learning model for time-series
analysis leveraging the potential of Unity Game Engine .
The user-interface communicates with the back-end which
is exposed as a RESTful API. This decouples the model
from the visualization aspect. Even though the focus of our
work is on time-series data, the system is generally applicable
to any deep learning model as it is only dependent on the
effective computation of the gradients. Our aim is to develop
VOLUME 7, 2019
S. A. Siddiqui et al.: TSViz: Demystification of Deep Learning Models for Time-Series Analysis
FIGURE 12. Application of the percentile filter on the detailed view (Second level).
a monolithic framework for time-series deep learning models
uncovering all possible demystiﬁcation aspects.
The ﬁrst view in the visualization presents the user with
an overview of the network. This gives the user a chance to
get acquainted with the model in question. A sample visualization of the ﬁrst screen is presented in Fig. 11. The second level provides an overview of most inﬂuential/important
ﬁlters in each layer leveraging the inﬂuence computation
framework (Section IV-B). The third view enhances the presented information by clustering the relevant ﬁlters together
to gain insights regarding the diversity present in the network (Section IV-C). The second and third view are equally
applicable to adversarial examples and inverse optimization
outputs as they only affect the inputs of the model.
There is usually a high interest in visualizing the most
important ﬁlters from the network since they are indicative
of the most important parts of the network leveraged for
prediction. Therefore, we integrated a percentile view where
the user can select the percentile of ﬁlters to be viewed based
on their importance. This signiﬁcantly helps in reducing the
amount of information presented to the user. Fig. 12 provides
an example application of the percentile ﬁlter onto the second
level view of the network in the tool. Another possible way
to reduce information overload for the user is to visualize the
most salient ﬁlters from each cluster (Section IV-C).
VII. DISCUSSION
The visualization enabled a detailed inspection of the network
which highlighted many different aspects of the network’s
learning employed in this study.
• Most of the ﬁlters in the network were useless i.e. they
contributed nothing to the ﬁnal prediction for that particular input. These ﬁlters changed as the stimulus to
the network changed, indicating that some ﬁlters were
specialized for a particular input.
• Many of the ﬁlters had very similar activating patterns in
the network which were assigned to the same ﬁlter cluster. This highlighted the aspect of the lack of diversity in
the trained network. This is consistent with ﬁndings of
Denil et al. where they analyzed the amazing capabilities of deep learning models in exploiting
redundancy.
• Despite the improvement in performance with the addition of the ﬁrst-order derivative of the original signal,
most of the ﬁlters strongly attended to the original signal
as compared to the ﬁrst-order derivative in the timeseries forecasting task. On the other hand, when evaluating adversarial examples and inverse optimization,
the network exploited that ﬁrst-order derivative in order
to signiﬁcantly impact the prediction with only minor
modiﬁcations in the actual signal.
• The network mostly focused on the temperature
and torque for detecting the anomalies as we never
introduced any synthetic anomalies in the pressure signal in the time-series classiﬁcation task.
We argue that there is no perfect way for the interpretability
of these models. Therefore, we inspect the model from many
different angles in order to come up with a range of different
explanations. We are currently working on extending this
work by tracing the inﬂuence of particular training examples
on the network using inﬂuence functions . This will also
enable the discovery of dataset errors and model debugging.
Another area that can greatly enhance the utility of the system is to provide a human understandable description of the
intermediate ﬁlters by computing its relation to some of the
most commonly used operators in the time-series analysis
community.
VIII. CONCLUSION
We proposed a novel framework (TSViz) for interpretability of deep learning based time-series analysis models. The
framework enabled an understanding of the model as a
parametric function. The different views available within
the framework enabled in-depth exploration of the network.
These different views include ﬁlter importance, ﬁlter input
saliency map, ﬁlter clusters, inverse optimization and adversarial examples. We also assess the signiﬁcance of the computed ﬁlter importance by pruning ﬁlters from the network
according to their importance. We argue that there is no one
right way to visualize and understand the network, therefore,
we uncover all these different aspects to aid human understanding. This exploration will help in the understanding
of the network itself as well as enable new improvements
VOLUME 7, 2019
S. A. Siddiqui et al.: TSViz: Demystification of Deep Learning Models for Time-Series Analysis
within the architecture with insights gained by uncovering the
different aspects of the trained model.