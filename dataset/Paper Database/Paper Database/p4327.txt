Loss-Sensitive Generative Adversarial Networks on Lipschitz
Guo-Jun Qi
Laboratory for MAchine Perception and LEarning
 
University of Central Florida
 
In this paper, we present the Lipschitz regularization theory and algorithms for a novel
Loss-Sensitive Generative Adversarial Network (LS-GAN). Speciﬁcally, it trains a loss function
to distinguish between real and fake samples by designated margins, while learning a generator
alternately to produce realistic samples by minimizing their losses. The LS-GAN further regularizes its loss function with a Lipschitz regularity condition on the density of real data, yielding
a regularized model that can better generalize to produce new data from a reasonable number of training examples than the classic GAN. We will further present a Generalized LS-GAN
(GLS-GAN) and show it contains a large family of regularized GAN models, including both
LS-GAN and Wasserstein GAN, as its special cases. Compared with the other GAN models,
we will conduct experiments to show both LS-GAN and GLS-GAN exhibit competitive ability
in generating new images in terms of the Minimum Reconstruction Error (MRE) assessed on
a separate test set. We further extend the LS-GAN to a conditional form for supervised and
semi-supervised learning problems, and demonstrate its outstanding performance on image classiﬁcation tasks.
Keywords: Generative Adversarial Nets (GANs), Lipschitz regularity, Minimum Reconstruction Error (MRE)
Introduction
A classic Generative Adversarial Net (GAN) learns a discriminator and a generator by
playing a two-player minimax game to generate samples from a data distribution. The discriminator
is trained to distinguish real samples from those generated by the generator, and it in turn guides
the generator to produce realistic samples that can fool the discriminator.
However, from both theoretical and practical perspectives, a critical question is whether the
GAN can generate realistic samples from arbitrary data distribution without any prior? If not, what
kind of prior ought to be imposed on the data distribution to regularize the GAN? Indeed, the classic
GAN imposes no prior on the data distribution. This represents an ambitious goal to generate
samples from any distributions. However, it in turn requires a non-parametric discriminator to
prove the distributional consistency between generated and real samples by assuming the model
has inﬁnite capacity (see Section 4 of ).
This is a too strong assumption to establish the theoretical basis for the GAN. Moreover, with
such an assumption, its generalizability becomes susceptible.
Speciﬁcally, one could argue the
learned generator may be overﬁt by an unregularized discriminator in an non-parametric fashion
 
by merely memorizing or interpolating training examples. In other words, it could lack the generalization ability to generate new samples out of existing data. Indeed, Arora et al. have shown
that the GAN minimizing the Jensen-Shannon distance between the distributions of generated and
real data could fail to generalize to produce new samples with a reasonable size of training set.
Thus, a properly regularized GAN is demanded to establish provable generalizability by focusing
on a restricted yet still suﬃciently large family of data distributions.
Objective: Towards Regularized GANs
In this paper, we attempt to develop regularization theory and algorithms for a novel Loss-
Sensitive GAN (LS-GAN). Speciﬁcally, we introduce a loss function to quantify the quality of
generated samples. A constraint is imposed so that the loss of a real sample should be smaller than
that of a generated counterpart. Speciﬁcally, in the learning algorithm, we will deﬁne margins to
separate the losses between generated and real samples. Then, an optimal generator will be trained
to produce realistic samples with minimum losses. The loss function and the generator will be
trained in an adversarial fashion until generated samples become indistinguishable from real ones.
We will also develop new theory to analyze the LS-GAN on the basis of Lipschitz regularity. We
note that the reason of making non-parametric assumption of inﬁnite capacity on the discriminator
in the classic GAN is due to its ambitious goal to generate data from any arbitrary distribution.
However, no free lunch principle reminds us of the need to impose a suitable prior on the
data distribution from which real samples are generated. This inspires us to impose a Lipschitz
regularity condition by assuming the data density does not change abruptly. Based on this mild
condition, we will show that the density of generated samples by LS-GAN can exactly match that
of real data.
More importantly, the Lipschitz regularity allows us to prove the LS-GAN can well generalize
to produce new data from training examples. To this end, we will provide a Probably Approximate
Correct (PAC)-style theorem by showing the empirical LS-GAN model trained with a reasonable
number of examples can be suﬃciently close to the oracle LS-GAN trained with hypothetically
known data distribution, thereby proving the generalizability of LS-GAN in generating samples
from any Lipschitz data distribution.
We will also make a non-parametric analysis of the LS-GAN. It does not rely on any parametric
form of the loss function to characterize its optimality in the space of Lipschtiz functions. It gives
both the upper and lower bounds of the optimal loss, which are cone-shaped with non-vanishing
gradient. This suggests that the LS-GAN can provide suﬃcient gradient to update its LS-GAN
generator even if the loss function has been fully optimized, thus avoiding the vanishing gradient
problem that could occur in training the GAN .
Extensions: Generalized and Conditional LS-GANs
We further present a generalized form of LS-GAN (GLS-GAN) and conduct experiment to
demonstrate it has the best generalization ability. We will show this is not a surprising result as
the GLS-GAN contains a large family of regularized GANs with both LS-GAN and Wasserstein
GAN (WGAN) as its special cases. Moreover, we will extend a Conditional LS-GAN (CLS-GAN)
that can generate samples from given conditions. In particular, with class labels being conditions,
the learned loss function can be used as a classiﬁer for both supervised and semi-supervised learning.
The advantage of such a classiﬁer arises from its ability of exploring generated examples to uncover
intrinsic variations for diﬀerent classes. Experiment results demonstrate competitive performance
of the CLS-GAN classiﬁer compared with the state-of-the-art models.
Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities
Paper Structure
The remainder of this paper is organized as follows. Section 2 reviews the related work, and
the proposed LS-GAN is presented in Section 3. In Section 4, we will analyze the LS-GAN by
proving the distributional consistency between generated and real data with the Lipschitz regularity
condition on the data distribution. In Section 5, we will discuss the generalizability problem arising
from using sample means to approximate the expectations in the training objectives. We will make
a comparison with Wasserstein GAN (WGAN) in Section 6.1, and present a generalized LS-GAN
with both WGAN and LS-GAN as its special cases in Section 6.2. A non-parametric analysis of
the algorithm is followed in Section 7.
Then we will show how the model can be extended to
a conditional model for both supervised and semi-supervised learning in Section 8. Experiment
results are presented in Section 9, and we conclude in Section 10.
Source codes. The source codes for both LS-GAN and GLS-GAN are available at https://
github.com/maple-research-lab, in the frameworks of torch, pytorch and tensorﬂow. LS-GAN
is also supported by Microsoft CNTK at 
LSGAN.html.
Related Work
Deep generative models, especially the Generative Adversarial Net (GAN) , have attracted
many attentions recently due to their demonstrated abilities of generating real samples following
the underlying data densities. In particular, the GAN attempts to learn a pair of discriminator and
generator by playing a maximin game to seek an equilibrium, in which the discriminator is trained
by distinguishing real samples from generated ones and the generator is optimized to produce
samples that can fool the discriminator.
A family of GAN architectures have been proposed to implement this idea.
For example,
recent progresses have shown impressive performances on synthesizing photo-realistic images
by constructing multiple strided and factional-strided convolutional layers for discriminators and
generators.
On the contrary, proposed to use a Laplacian pyramid to produce high-quality
images by iteratively adding multiple layers of noises at diﬀerent resolutions. presented to train
a recurrent generative model by using adversarial training to unroll gradient-based optimizations
to create high quality images.
In addition to designing diﬀerent GAN networks, research eﬀorts have been made to train the
GAN by diﬀerent criteria. For example, presented an energy-based GAN by minimizing an
energy function to learn an optimal discriminator, and an auto-encoder structured discriminator
is presented to compute the energy. The authors also present a theoretical analysis by showing
this variant of GAN can generate samples whose density can recover the underlying true data
However, it still needs to assume the discriminator has inﬁnite modeling capacity to
prove the result in a non-parametric fashion, and its generalizability of producing new data out
of training examples is unknown without theoretical proof or empirical evidence. In addition, 
presented to analyze the GAN from information theoretical perspective, and they seek to minimize
the variational estimate of f-divergence, and show that the classic GAN is included as a special
case of f-GAN. In contrast, InfoGAN proposed another information-theoretic GAN to learn
disentangled representations capturing various latent concepts and factors in generating samples.
Most recently, propose to minimize the Earth-Mover distance between the density of generated
samples and the true data density, and they show the resultant Wasserstein GAN (WGAN) can
address the vanishing gradient problem that the classic GAN suﬀers.
Besides the class of GANs, there exist other models that also attempt to generate natural images.
For example, rendered images by matching features in a convolutional network with respect
to reference images. used deconvolutional network to render 3D chair models in various styles
and viewpoints. introduced a deep recurrent neutral network architecture for image generation
with a sequence of variational auto-encoders to iteratively construct complex images.
Recent eﬀorts have also been made on leveraging the learned representations by deep generative networks to improve the classiﬁcation accuracy when it is too diﬃcult or expensive to label
suﬃcient training examples. For example, presented variational auto-encoders by combining deep generative models and approximate variational inference to explore both labeled and
unlabeled data. treated the samples from the GAN generator as a new class, and explore unlabeled examples by assigning them to a class diﬀerent from the new one. proposed to train a
ladder network by minimizing the sum of supervised and unsupervised cost functions through
back-propagation, which avoids the conventional layer-wise pre-training approach. presented
an approach to learning a discriminative classiﬁer by trading-oﬀmutual information between observed examples and their predicted classes against an adversarial generative model. sought to
jointly distinguish between not only real and generated samples but also their latent variables in
an adversarial process. Recently, presented a novel paradigm of localized GANs to explore the
local consistency of classiﬁers in local coordinate charts, as well as showed an intrinsic connection
with Laplace-Beltrami operator along the manifold. These methods have shown promising results
for classiﬁcation tasks by leveraging deep generative models.
Loss-Sensitive GAN
The classic GAN consists of two players – a generator producing samples from random noises,
and a discriminator distinguishing real and fake samples. The generator and discriminator are
trained in an adversarial fashion to reach an equilibrium in which generated samples become indistinguishable from their real counterparts.
On the contrary, in the LS-GAN we seek to learn a loss function Lθ(x) parameterized with θ
by assuming that a real example ought to have a smaller loss than a generated sample by a desired
margin. Then the generator can be trained to generate realistic samples by minimizing their losses.
Formally, consider a generator function Gφ that produces a sample Gφ(z) by transforming a
noise input z ∼Pz(z) drawn from a simple distribution Pz such as uniform and Gaussian distributions. Then for a real example x and a generated sample Gφ(z), the loss function can be trained
to distinguish them with the following constraint:
Lθ(x) ≤Lθ(Gφ(z)) −∆(x, Gφ(z))
where ∆(x, Gφ(z)) is the margin measuring the diﬀerence between x and Gφ(z). This constraint
requires a real sample be separated from a generated counterpart in terms of their losses by at least
a margin of ∆(x, Gφ(z)).
The above hard constraint can be relaxed by introducing a nonnegative slack variable ξx,z that
quantiﬁes the violation of the above constraint. This results in the following minimization problem
to learn the loss function Lθ given a ﬁxed generator Gφ∗,
s.t., Lθ(x) −ξx,z ≤Lθ(Gφ∗(z)) −∆(x, Gφ∗(z))
Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities
where λ is a positive balancing parameter, and Pdata(x) is the data distribution of real samples.
The ﬁrst term minimizes the expected loss function over data distribution since a smaller loss is
preferred on real samples. The second term is the expected error caused by the violation of the
constraint. Without loss of generality, we require the loss function should be nonnegative.
Given a ﬁxed loss function Lθ∗, on the other hand, one can solve the following minimization
problem to ﬁnd an optimal generator Gφ∗.
Lθ∗(Gφ(z))
We can use PGφ and PGφ∗to denote the density of samples generated by Gφ(z) and Gφ∗(z)
respectively, with z being drawn from Pz(z). However, for the simplicity of notations, we will use
PG and PG∗to denote PGφ and PGφ∗without explicitly mentioning φ and φ∗that should be clear
in the context.
Finally, let us summarize the above objectives. The LS-GAN optimizes Lθ and Gφ alternately
by seeking an equilibrium (θ∗, φ∗) such that θ∗minimizes
S(θ, φ∗) =
 ∆(x, zG) + Lθ(x) −Lθ(zG)
which is an equivalent form of (2) with (a)+ = max(a, 0), and φ∗minimizes
T(θ∗, φ) =
In the next section, we will show the consistency between PG∗and Pdata for LS-GAN.
Theoretical Analysis: Distributional Consistency
Suppose (θ∗, φ∗) is a Nash equilibrium that jointly solves (4) and (5). We will show that as
λ →+∞, the density distribution PG∗of the samples generated by Gφ∗will converge to the real
data density Pdata.
First, we have the following deﬁnition.
Deﬁnition. For any two samples x and z, the loss function F(x) is Lipschitz continuous with
respect to a distance metric ∆if
|F(x) −F(z)| ≤κ ∆(x, z)
with a bounded Lipschitz constant κ, i.e, κ < +∞.
To prove our main result, we assume the following regularity condition on the data density.
Assumption 1. The data density Pdata is supported in a compact set D, and it is Lipschitz continuous wrt ∆with a bounded constant κ < +∞.
The set of Lipschitz densities with a compact support contain a large family of distributions
that are dense in the space of continuous densities. For example, the density of natural images are
deﬁned over a compact set of pixel values, and it can be consider as Lipschitz continuous, since
the densities of two similar images are unlikely to change abruptly at an unbounded rate. If real
samples are distributed on a manifold (or Pdata is supported in a manifold), we only require the
Lipschitz condition hold on this manifold. This makes the Lipschitz regularity applicable to the
data densities on a thin manifold embedded in the ambient space.
Let us show the existence of Nash equilibrium such that both the loss function Lθ∗and the
density PG∗of generated samples are Lipschitz. Let Fκ be the class of functions over D with a
bounded yet suﬃciently large Lipschitz constant κ such that Pdata belongs to Fκ. It is not diﬃcult
to show that the space Fκ is convex and compact if its member functions are supported in a compact
set. In addition, we note both S(θ, φ) and T(θ, φ) are convex in Lθ and in PG. Then, according to
the Sion’s theorem , with Lθ and PG being optimized over Fκ, there exists a Nash equilibrium
(θ∗, φ∗). Thus, we have the following lemma.
Lemma 1. Under Assumption 1, there exists a Nash equilibrium (θ∗, φ∗) such that both Lθ∗and
PG∗are Lipschitz.
Now we can prove the main lemma of this paper. The Lipschitz regularity relaxes the strong
non-parametric assumption on the GAN’s discriminator with inﬁnite capacity to the above weaker
Lipschitz assumption for the LS-GAN. This allows us to show the following lemma that establishes
the distributional consistency between the optimal PG∗by Problem (4)–(5) and the data density
Lemma 2. Under Assumption 1, for a Nash equilibrium (θ∗, φ∗) in Lemma 1, we have
|Pdata(x) −PG∗(x)|dx ≤2
Thus, PG∗(x) converges to Pdata(x) as λ →+∞.
The proof of this lemma is given in Appendix A.
Remark 1. By letting λ go inﬁnitely large, the density PG∗(x) of generated samples should exactly match the real data density Pdata(x).
Equivalently, we can simply disregard the ﬁrst loss
minimization term in (4) as it plays no role as λ →+∞.
Putting the above two lemmas together, we have the following theorem.
Theorem 1. Under Assumption 1, a Nash equilibrium (θ∗, φ∗) exists such that
(i) Lθ∗and PG∗are Lipschitz.
x |Pdata(x) −PG∗(x)|dx ≤2
λ →0, as λ →+∞.
Learning and Generalizability
The minimization problems (4) and (5) cannot be solved directly since the expectations over
the distributions of true data Pdata and noises Pz are unavailable or intractable. Instead, one can
approximate them with empirical means on a set of ﬁnite real examples Xm = {x1, · · · , xm} and
noise vectors Zm = {z1, · · · , zm} drawn from Pdata(x) and Pz(z) respectively.
This results in the following two alternative problems.
Sm(θ, φ∗) ≜
Lθ(xi) + λ
 ∆(xi, Gφ∗(zi)) + Lθ(xi) −Lθ(Gφ∗(zi))
φ Tk(θ∗, φ) = 1
Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities
where the random vectors Z′
i|i = 1, · · · , k} used in (7) can be diﬀerent from Zm used in (6).
The sample mean in the second term of Eq. (6) is computed over pairs (xi, Gφ∗(zi)) randomly
drawn from real and generated samples, which is an approximation to the second expectation term
in Eq. (4).
Generalizability
We have proved the density of generated samples by the LS-GAN is consistent with the real
data density in Theorem 1.
This consistency is established based on the two oracle objectives
(4) and (5). However, in practice, the population expectations in these two objectives cannot be
computed directly over Pdata and PG. Instead, they are approximated in (6) and (7) by sample
means on a ﬁnite set of real and generated examples.
This raises the question about the generalizability of the LS-GAN model. We wonder, with more
training examples, if the empirical model trained with ﬁnitely many examples can generalize to the
oracle model. In particular, we wish to estimate the sample complexity of how many examples
are required to suﬃciently bound the generalization diﬀerence between the empirical and oracle
objectives.
Arora et al. has proposed a neural network distance to analyze the generalization ability
for the GAN. However, this neural network distance cannot be directly applied here, as it is not
related with the objectives that are used to train the LS-GAN. So the generalization ability in
terms of the neural network distance does not imply the LS-GAN could also generalize. Thus, a
direct generalization analysis of the LS-GAN is required based on its own objectives.
First, let us consider the generalization in terms of S(θ, φ∗). This objective is used to train the
loss function Lθ to distinguish between real and generated samples. Consider the oracle objective
(4) with the population expectations
and the empirical objective (6) with the sample means
Sm(θ, φ∗).
We need to show if and how fast the diﬀerence |Sm −S| would eventually vanish as the number
m of training examples grows.
To this end, we need to deﬁne the following notations about the model complexity.
Assumption 2. We assume that for LS-GAN,
I. the loss function Lθ(x) is κL-Lipschitz in its parameter θ, i.e., |Lθ(x) −Lθ′(x)| ≤κL∥θ −θ′∥
for any x;
II. Lθ(x) is κ-Lipschitz in x, i.e., |Lθ(x) −Lθ(x′)| ≤κ∥x −x′∥for any θ;
III. the distance between two samples is bounded, i.e., |∆(x, x′)| ≤B∆.
Then we can prove the following generalization theorem in a Probably Approximately Correct
(PAC) style.
Theorem 2. Under Assumption 2, with at least probability 1 −η, we have
|Sm −S| ≤ε
when the number of samples
 N log κLN
where C is a suﬃciently large constant, and N is the number of parameters of the loss function
such that θ ∈RN.
The proof of this theorem is given in Appendix C. This theorem shows the sample complexity
to bound the diﬀerence between S and Sm is polynomial in the model size N, as well as both
Lipschitz constants log κL and κ.
Similarly, we can establish the generalizability to train the generator function by considering
the empirical objective
φ Tk(θ∗, φ)
and the oracle objective
φ T(θ∗, φ)
over empirical and real distributions, respectively.
We use the following notions to characterize the complexity of the generator.
Assumption 3. We assume that
I. The generator function Gφ(x) is ρG-Lipschitz in its parameter φ, i.e., |Gφ(z) −Gφ′(z)| ≤
ρG∥φ −φ′∥for any z;
II. Also, we have Gφ(z) is ρ-Lipschitz in z, i.e., |Gφ(z) −Gφ(z′)| ≤ρ∥z −z′∥;
III. The samples z’s drawn from Pz are bounded, i.e., ∥z∥≤Bz.
Then we can prove the following theorem to establish the generalizability of the generator in
terms of T(θ, φ).
Theorem 3. Under Assumption 3, with at least probability 1 −η, we have
|Tk −T| ≤ε
when the number of samples
 M log κLρGM
where C′ is a suﬃciently large constant, and M is the number of parameters of the generator
function such that φ ∈RM.
Bounded Lipschitz Constants for Regularization
Our generalization theory in Theorem 2 conjectures that the required number of training examples is lower bounded by a polynomial of Lipschitz constants κL and κ of the loss function wrt θ
and x. This suggests us to bound both constants to reduce the sample complexity of the LS-GAN
to improve its generalization performance.
Speciﬁcally, bounding the Lipschitz constants κ and κL can be implemented by adding two
gradient penalties (I) 1
2Ex∼Pdata∥∇xLθ(x)∥2 and (II) 1
2Ex∼Pdata∥∇θLθ(x)∥2 to the objective (4) as
Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities
the surrogate of the Lipschitz constants. For simplicity, we ignore the second gradient penalty
(II) for κL in experiments, as the sample complexity is only log-linear in it, whose impact on
generalization performance is negligible compared with that of κ. Otherwise, penalizing (II) needs
to compute its gradient wrt θ, which is Ex∼Pdata∇2
θLθ(x)∇θLθ(x) with a Hessian matrix ∇2
this is usually computationally demanding.
Note that the above gradient penalty diﬀers from that used in that aims to constrain the
Lipschitz constant κ close to one as in the deﬁnition of the Wasserstein distance . However,
we are motivated to have lower sample complexity by directly minimizing the Lipschitz constant
rather than constraining it to one. Two gradient penalty approaches are thus derived from diﬀerent
theoretical perspectives, and also make practical diﬀerences in experiments.
Wasserstein GAN and Generalized LS-GAN
In this section, we discuss two issues about LS-GAN. First, we discuss its connection with the
Wasserstein GAN (WGAN), and then show that the WGAN is a special case of a generalized form
of LS-GAN.
Comparison with Wasserstein GAN
We notice that the recently proposed Wasserstein GAN (WGAN) uses the Earth-Mover (EM)
distance to address the vanishing gradient and saturated JS distance problems in the classic GAN
by showing the EM distance is continuous and diﬀerentiable almost everywhere. While both the LS-
GAN and the WGAN address these problems from diﬀerent perspectives that are independently
developed almost simultaneously, both turn out to use the Lipschitz regularity in training their
GAN models. This constraint plays vital but diﬀerent roles in the two models. In the LS-GAN,
the Lipschitz regularity naturally arises from the Lipschitz assumption on the data density and
the generalization bound. Under this regularity condition, we have proved in Theorem 1 that the
density of generated samples matches the underlying data density. On the contrary, the WGAN
introduces the Lipschitz constraint from the Kantorovich-Rubinstein duality of the EM distance
but it is not proved in if the density of samples generated by WGAN is consistent with that of
real data.
Here we assert that the WGAN also models an underlying Lipschitz density. To prove this, we
restate the WGAN as follows. The WGAN seeks to ﬁnd a critic f∗
w and a generator g∗
φ such that
w = arg max
fw∈F1 U(fw, g∗
φ) ≜Ex∼Pdata[fw(x)] −Ez∼Pz(z)[fw(g∗
φ = arg max V (f∗
w, gφ) ≜Ez∼Pz(z)[f∗
φ be the density of samples generated by g∗
φ. Then, we prove the following lemma about
the WGAN in Appendix B.
Lemma 3. Under Assumption 1, given an optimal solution (f∗
φ) to the WGAN such that Pg∗
is Lipschitz, we have
|Pdata(x) −Pg∗
φ(x)|dx = 0
This lemma shows both the LS-GAN and the WGAN are based on the same Lipschitz regularity
condition.
Although both methods are derived from very diﬀerent perspectives, it is interesting to make a
comparison between their respective forms. Formally, the WGAN seeks to maximize the diﬀerence
between the ﬁrst-order moments of fw under the densities of real and generated examples.
this sense, the WGAN can be considered as a kind of ﬁrst-order moment method. Numerically,
as shown in the second term of Eq. (8), fw tends to be minimized to be arbitrarily small over
generated samples, which could make U(fw, g∗
φ) be unbounded above. This is why the WGAN
must be trained by clipping the network weights of fw on a bounded box to prevent U(fw, g∗
becoming unbounded above.
On the contrary, the LS-GAN treats real and generated examples in pairs, and maximizes the
diﬀerence of their losses up to a data-dependant margin. Speciﬁcally, as shown in the second term
of Eq. (4), when the loss of a generated sample zG becomes too large wrt that of a paired real
example x, the maximization of Lθ(zG) will stop if the diﬀerence Lθ(zG)−Lθ(x) exceeds ∆(x, zG).
This prevents the minimization problem (4) unbounded below, making it better posed to solve.
More importantly, paring real and generated samples in (·)+ prevents their losses from being
decomposed into two separate ﬁrst-order moments like in the WGAN. The LS-GAN makes pairwise
comparison between the losses of real and generated samples, thereby enforcing real and generated
samples to coordinate with each other to learn the optimal loss function.
Speciﬁcally, when a
generated sample becomes close to a paired real example, the LS-GAN will stop increasing the
diﬀerence Lθ(zG) −Lθ(x) between their losses.
Below we discuss a Generalized LS-GAN (GLS-GAN) model in Section 6.2, and show that both
WGAN and LS-GAN are simply two special cases of this GLS-GAN.
GLS-GAN: Generalized LS-GAN
In proving Lemma 2, it is noted that we only have used two properties of (a)+ in the objective
function Sθ(θ, φ∗) training the loss function Lθ: 1) (a)+ ≥a for any a; 2) (a)+ = a for a ≥0. This
inspires us to generalize the LS-GAN with any alternative cost function C(a) satisfying these two
properties, and this will yield the Generalized LS-GAN (GLS-GAN).
We will show that both LS-GAN and WGAN can be seen as two extreme cases of
this GLS-GAN with two properly deﬁned cost functions.
Formally, if a cost function C(a) satisﬁes
(I) C(a) ≥a for any a ∈R and
(II) C(a) = a for any a ∈R+,
given a ﬁxed generator Gφ∗, we use the following objective
SC(θ, φ∗) =
x∼Pdata(x)
 ∆(x, Gφ∗(z)) + Lθ(x) −Lθ(Gφ∗(z))
to learn Lθ(x), with SC highlighting its dependency on a chosen cost function C.
For simplicity, we only involve the second term in (4) to deﬁne the generalized objective SC.
But it does not aﬀect the conclusion as the role of the ﬁrst term in (4) would vanish with λ being
set to +∞. Following the proof of Lemma 2, we can prove the following lemma.
Lemma 4. Under Assumption 1, given a Nash equilibrium (θ∗, φ∗) jointly minimizing SC(θ, φ∗)
and T(θ∗, φ) with a cost function C satisfying the above conditions (I) and (II), we have
|Pdata(x) −PG∗(x)|dx = 0.
Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities
In particular, we can choose a leaky rectiﬁed linear function for this cost function, i.e., Cν(a) =
max(a, νa) with a slope ν. As long as ν ∈(−∞, 1], it is easy to verify Cν(a) satisﬁes these two
conditions.
Now the LS-GAN is a special case of this Generalized LS-GAN (GLS-GAN) when ν = 0, as
C0(a) = (a)+. We denote this equivalence as
LS-GAN = GLS-GAN(C0)
What is more interesting is the WGAN, an independently developed GAN model with stable
training performance, also becomes a special case of this GLS-GAN with ν = 1. Indeed, when
ν = 1, C1(a) = a, and
SC1(θ, φ∗) =
x∼Pdata(x)
 ∆(x, Gφ∗(z)) + Lθ(x) −Lθ(Gφ∗(z))
x∼Pdata(x)
Lθ(Gφ∗(z)) +
x∼Pdata(x)
∆(x, Gφ∗(z))
Since the last term Ex∼Pdata,z∼Pz ∆(x, Gφ∗(z)) is a const, irrespective of Lθ, it can be discarded
without aﬀecting optimization over Lθ. Thus, we have
SC1(θ, φ∗) =
x∼Pdata(x)
Lθ(Gφ∗(z))
By comparing this SC1 with U in (8), it is not hard to see that the WGAN is equivalent to the
GLS-GAN with C1, with the critic function fw being equivalent to −Lθ 1. Thus we have
WGAN = GLS-GAN(C1)
Therefore, by varying the slope ν in (−∞, 1], we will obtain a family of the GLS-GANs with
varied Cν beyond the LS-GAN and the WGAN. Of course, it is unnecessary to limit C(a) to a
leaky rectiﬁed linear function. We can explore more cost functions as long as they satisfy the two
conditions (I) and (II).
In experiments, we will demonstrate the GLS-GAN has competitive generalization performance
on generating new images (c.f. Section 9.5).
Non-Parametric Analysis
Now we can characterize the optimal loss functions learned from the objective (6), and this will
provide us an insight into the LS-GAN model.
We generalize the non-parametric maximum likelihood method in and consider non-parametric
solutions to the optimal loss function by minimizing (6) over the whole class of Lipschitz loss functions.
Let x(1) = x1, x(2) = x2, · · · , x(m) = xm, x(m+1) = Gφ∗(z1), · · · , x(2m) = Gφ∗(zm), i.e., the ﬁrst
n data points are real examples and the rest m are generated samples. Then we have the following
Theorem 4. The following functions bLθ∗and eLθ∗both minimize Sm(θ, φ∗) in Fκ:
i −κ∆(x, x(i))
i + κ∆(x, x(i))}
1the minus sign exists as the U is maximized over fw in the WGAN. On the contrary, in the GLS-GAN, SC is
minimized over Lθ.
Figure 1: Comparison between two optimal loss functions eLθ∗and bLθ∗in Fκ for LS-GAN. They
are upper and lower bounds of the class of optimal loss functions Lθ∗to Problem (6). Both the
upper and the lower bounds are cone-shaped, and have non-vanishing gradient almost everywhere.
Speciﬁcally, in this one-dimensional example, both bounds are piecewise linear, having a slope of
±κ almost everywhere.
with the parameters θ∗= [l∗
1, · · · , l∗
2m] ∈R2m. They are supported in the convex hull of {x(1), · · · , x(2m)},
and we have
bLθ∗(x(i)) = eLθ∗(x(i)) = l∗
for i = 1, · · · , 2m, i.e., their values coincide on {x(1), x(2), · · · , x(2m)}.
The proof of this theorem is given in the appendix.
From the theorem, it is not hard to show that any convex combination of these two forms
attains the same value of Sm, and is also a global minimizer. Thus, we have the following corollary.
Corollary 1. All the functions in
Lθ∗= {γbLθ∗+ (1 −γ)eLθ∗|0 ≤γ ≤1} ⊂Fκ
minimize Sm in Fκ.
This shows that the global minimizer is not unique. Moreover, through the proof of Theorem
4, one can ﬁnd that eLθ∗(x) and bLθ∗(x) are the upper and lower bound of any optimal loss function
solution to the problem (6). In particular, we have the following corollary.
Corollary 2. For any Lθ∗(x) ∈Fκ that minimizes Sm, the corresponding bLθ∗(x) and eLθ∗(x) are
the lower and upper bounds of Lθ∗(x), i.e.,
bLθ∗(x) ≤Lθ∗(x) ≤eLθ∗(x)
The proof is given in Appendix D.
Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities
The parameters θ∗= [l∗
1, · · · , l∗
2m] in (10) can be sought by minimizing
Sm(φ∗, θ) ≜
 ∆i,m+i + li −lm+i
s.t., |li −li′| ≤κ∆(x(i), x(i′))
li ≥0, i, i′ = 1, · · · , 2m
where ∆i,j is short for ∆(x(i), x(j)), and the constraints are imposed to ensure the learned loss
functions stay in Fκ. With a greater value of κ, a larger class of loss function will be sought. Thus,
one can control the modeling ability of the loss function by setting a proper value to κ.
Problem (11) is a typical linear programming problem. In principle, one can solve this problem
to obtain a non-parametric loss function for the LS-GAN. Unfortunately, it consists of a large
number of constraints, whose scale is at an order of
. This prevents us from using (11)
directly to solve an optimal non-parametric LS-GAN model with a very large number of training
examples. On the contrary, a more tractable solution is to use a parameterized network to solve
the optimization problem (6) constrained in Lκ, and iteratively update parameterized Lθ and Gφ
with the gradient descent method.
Although the non-parametric solution cannot be solved directly, it is valuable in shedding some
light on what kind of the loss function would be learned by a deep network. It is well known that the
training of the classic GAN generator suﬀers from vanishing gradient problem as the discriminator
can be optimized very quickly.
Recent study has revealed that this is caused by using the
Jensen-Shannon (JS) distance that becomes locally saturated and gets vanishing gradient to train
the GAN generator if the discriminator is over-trained. Similar problem has also been found in
the energy-based GAN (EBGAN) as it minimizes the total variation that is not continuous or
(sub-)diﬀerentiable if the corresponding discriminator is fully optimized .
On the contrary, as revealed in Theorem 4 and illustrated in Figure 1, both the upper and lower
bounds of the optimal loss function of the LS-GAN are cone-shaped (in terms of ∆(x, x(i)) that
deﬁnes the Lipschitz continuity), and have non-vanishing gradient almost everywhere. Moreover,
Problem (11) only contains linear objective and constraints; this is contrary to the classic GAN that
involves logistic loss terms that are prone to saturation with vanishing gradient. Thus, an optimal
loss function that is properly sought in Lκ as shown in Figure 1 is unlikely to saturate between
these two bounds, and it should be able to provide suﬃcient gradient to update the generator by
descending (7) even if it has been trained till optimality. Our experiment also shows that, even if the
loss function is quickly trained to optimality, it can still provide suﬃcient gradient to continuously
update the generator in the LS-GAN (see Figure 5).
Conditional LS-GAN
The LS-GAN can easily be generalized to produce a sample based on a given condition y,
yielding a new paradigm of Conditional LS-GAN (CLS-GAN).
For example, if the condition is an image class, the CLS-GAN seeks to produce images of the
given class; otherwise, if a text description is given as a condition, the model attempts to generate
images aligned with the given description. This gives us more ﬂexibility in controlling what samples
to be generated.
Formally, the generator of CLS-GAN takes a condition vector y as input along with a noise
vector z to produce a sample Gφ(z, y). To train the model, we deﬁne a loss function Lθ(x, y) to
measure the degree of the misalignment between a data sample x and a given condition y.
For a real example x aligned with the condition y, its loss function should be smaller than that
of a generated sample by a margin of ∆(x, Gφ(z, y)). This results in the following constraint,
Lθ(x, y) ≤Lθ(Gφ(z, y), y) −∆(x, Gφ(z, y))
Like the LS-GAN, this type of constraint yields the following non-zero-sum game to train the
CLS-GAN, which seeks a Nash equilibrium (θ∗, φ∗) so that θ∗minimizes
S(θ, φ∗) =
(x,y)∼Pdata
(x,y)∼Pdata
 ∆(x, Gφ∗(z, y)) + Lθ(x, y)
−Lθ(Gφ∗(z, y), y)
and φ∗minimizes
T(θ∗, φ) =
Lθ∗(Gφ(z, y), y)
where Pdata denotes either the joint data distribution over (x, y) in (13) or its marginal distribution
over y in (14).
Playing the above game will lead to a trained pair of loss function Lθ∗and generator Gφ∗. We
can show that the learned generator Gφ∗(z, y) can produce samples whose distribution follows the
true data density Pdata(x|y) for a given condition y.
To prove this, we say a loss function Lθ(x, y) is Lipschitz if it is Lipschitz continuous in its
ﬁrst argument x. We also impose the following regularity condition on the conditional density
Pdata(x|y).
Assumption 4. For each y, the conditional density Pdata(x|y) is Lipschitz, and is supported in a
convex compact set of x.
Then it is not diﬃcult to prove the following theorem, which shows that the conditional density
PG∗(x|y) becomes Pdata(x|y) as λ →+∞. Here PG∗(x|y) denotes the density of samples generated
by Gφ∗(z, y) with sampled random noise z.
Theorem 5. Under Assumption 4, a Nash equilibrium (θ∗, φ∗) exists such that
(i) Lθ∗(x, y) is Lipschitz continuous in x for each y;
(ii) PG∗(x|y) is Lipschitz continuous;
x |Pdata(x|y) −PG∗(x|y)|dx ≤2
In addition, similar upper and lower bounds can be derived to characterize the learned conditional loss function Lθ(x, y) following the same idea for LS-GAN.
A useful byproduct of the CLS-GAN is one can use the learned loss function Lθ∗(x, y) to predict
the label of an example x by
y∗= arg min
y Lθ∗(x, y)
The advantage of such a CLS-GAN classiﬁer is it is trained with both labeled and generated
examples, the latter of which can improve the training of the classiﬁer by revealing more potential
variations within diﬀerent classes of samples. It also provides a way to evaluate the model based
on its classiﬁcation performance. This is an objective metric we can use to assess the quality of
feature representations learned by the model.
Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities
For a classiﬁcation task, a suitable value should be set to λ. Although Theorem 5 shows PG∗
would converge to the true conditional density Pdata by increasing λ, it only ensures it is a good
generative rather than classiﬁcation model. However, a too large value of λ tends to ignore the
ﬁrst loss minimization term of (13) that plays an important role in minimizing classiﬁcation error.
Thus, a trade-oﬀshould be made to balance between classiﬁcation and generation objectives.
Semi-Supervised LS-GAN
The above CLS-GAN can be considered as a fully supervised model to classify examples into
diﬀerent classes. It can also be extended to a Semi-Supervised model by incorporating unlabeled
Suppose we have c classes indexed by {1, 2, · · · , c}. In the CLS-GAN, for each class, we choose
a loss function that, for example, can be deﬁned as the negative log-softmax,
Lθ(x, y = l) = −log
exp(al(x))
l=1 exp(al(x))
where al(x) is the lth activation output from a network layer.
Suppose we also have unlabeled examples available, and we can deﬁne a new loss function for
these unlabeled examples so that they can be involved in training the CLS-GAN. Consider an
unlabeled example x, its groundtruth label is unknown. However, the best guess of its label can
be made by choosing the one that minimizes Lθ(x, y = l) over l, and this inspires us to deﬁne the
following loss function for the unlabeled example as
θ (x) ≜min
Lθ(x, y = l)
Here we modify Lθ(x, y = l) to −log
exp(al(x))
l=1 exp(al(x)) so
l=1 exp(al(x)) can be viewed as the
probability that x does not belong to any known label.
Then we have the following loss-sensitive objective that explores unlabeled examples to train
the CLS-GAN,
Sul(θ, φ∗) ≜
x∼Pdata(x)
 ∆(x, Gφ∗(z)) + Lul
θ (x) −Lul
θ (Gφ∗(z))
This objective is combined with S(θ, φ∗) deﬁned in (13) to train the loss function network by
minimizing
S(θ, φ∗) + γSul(θ, φ∗)
where γ is a positive hyperparameter balancing the contributions from labeled and labeled examples.
The idea of extending the GAN for semi-supervised learning has been proposed by Odena 
and Salimans et al. , where generated samples are assigned to an artiﬁcial class, and unlabeled
examples are treated as the negative examples.
Our proposed semi-supervised learning diﬀers
in creating a new loss function for unlabeled examples from the losses for existing classes, by
minimizing which we make the best guess of the classes of unlabeled examples.
The guessed
labeled will provide additional information to train the CLS-GAN model, and the updated model
will in turn improve the guess over the training course. The experiments in the following section
will show that this approach can generate very competitive performance especially when the labeled
data is very limited.
Table 1: The Network architecture used in CLS-GAN for training CIFAR-10 and SVHN, where
BN stands for batch normalization, LeakyReLU for Leaky Rectiﬁer with a slope of 0.2 for negative
value, and “3c1s96o Conv.” means a 3 × 3 convolution kernel with stride 1 and 96 outputs, while
”UpConv.” denotes the fractionally-stride convolution.
(a) Loss Function Network
Input 32 × 32 × 3
3c1s96o Conv. BN LeakyReLU
3c1s96o Conv. BN LeakyReLU
4c2s96o Conv. BN LeakyReLU
3c1s192o Conv. BN LeakyReLU
3c1s192o Conv. BN LeakyReLU
4c2s192o Conv. BN LeakyReLU
3c1s192o Conv. BN LeakyReLU
3c1s192o Conv. BN LeakyReLU
1c1s192o Conv. BN LeakyReLU
global meanpool
Output 1 × 1 × 10
(b) Generator Network
Input 100-D random vector + 10-D one-hot vector
4c1s512o UpConv. BN LeakyReLU
4c2s256o UpConv. BN LeakyReLU
4c2s128o UpConv. BN LeakyReLU
4c2s3o UpConv. BN LeakyReLU
Elementwise Tanh
Output 32 × 32 × 3
Experiments
Objective evaluation of a data generative model is not an easy task as there is no consensus
criteria to quantify the quality of generated samples. For this reason, we will make a qualitative
analysis of generated images, and use image classiﬁcation to quantitatively evaluate the resultant
LS-GAN model.
First, we will assess the quality of generated images by the LS-GAN in comparison with the
classic GAN model. Then, we will make an objective evaluation on the CLS-GAN to classify images.
This task evaluates the quality of feature representations learned by the CLS-GAN in terms of its
classiﬁcation accuracy directly.
Finally, we will assess the generalizability of various GAN models in generating new images out
of training examples by proposing the Minimum Reconstruction Error (MRE) on a separate test
Architectures
We adopted the ideas behind the network architecture for the DCGAN to build the generator and the loss function networks. Compared with the conventional CNNs, maxpooling layers
were replaced with strided convolutions in both networks, and fractionally-strided convolutions
were used in the generator network to upsample feature maps across layers to ﬁner resolutions.
Batch-normalization layers were added in both networks between convolutional layers, and fully
connected layers were removed from these networks.
However, unlike the DCGAN, the LS-GAN model (unconditional version in Section 3) did not
use a sigmoid layer as the output for the loss function network. Instead, we removed it and directly
Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities
(a) Inception
(b) VGG-16
Figure 2: Images generated by the LS-GAN on the CelebA dataset, in which the margin is computed
as the distance between the features extracted from the Inception and VGG-16 networks. Images
are resized to 128 × 128 to ﬁt the input size of both networks.
output the activation before the removed sigmoid layer.
On the other hand, for the loss function network in CLS-GAN, a global mean-pooling layer was
added on top of convolutional layers. This produced a 1×1 feature map that output the conditional
loss Lθ(x, y) on diﬀerent classes y. In the generator network, Tanh was used to produce images
whose pixel values are scaled to [−1, 1]. Thus, all image examples in datasets were preprocessed
to have their pixel values in [−1, 1]. More details about the design of network architectures can be
found in literature .
Table 1 shows the network architecture for the CLS-GAN model on CIFAR-10 and SVHN
datasets in the experiments. In particular, the architecture of the loss function network was adapted
from that used in with nine hidden layers.
Training Details
The models were trained in a mini-batch of 64 images, and their weights were initialized from a
zero-mean Gaussian distribution with a standard deviation of 0.02. The Adam optimizer was
used to train the network with initial learning rate and β1 being set to 10−3 and 0.5 respectively,
while the learning rate was annealed every 25 epochs by a factor of 0.8. The other hyperparameters
such as γ and λ were chosen based on an independent validation set held out from training examples.
We also tested various forms of loss margins ∆(·, ·) between real and fake samples. For example,
we tried the Lp distance between image representations as the margin, and found the best result
can be achieved when p = 1. The distance between convolutional features was supposed to capture
perceptual dissimilarity between images. But we should avoid a direct use of the convolutional
features from the loss function network, since we found they would tend to collapse to a trivial
point as the loss margin vanishes. The feature maps from a separate pretrained deep network, such
as Inception and VGG-16 networks, could be a better choice to deﬁne the loss margin. Figure 2
shows the images generated by LS-GAN on CelebA with the inception and VGG-16 margins.
However, for a fair comparison, we did not use these external deep networks in other experiments
on image generation and classiﬁcation tasks. We simply used the distance between raw images as
the loss margin, and it still achieved competitive results. This demonstrates the robustness of the
proposed method without having to choose a sophisticated loss margin. This is also consistent with
our theoretical analysis where we do not assume any particular form of loss margin to prove the
For the generator network of LS-GAN, it took a 100-dimensional random vector drawn from
(b) LS-GAN
Figure 3: Images generated by the DCGAN and the LS-GAN on the CelebA dataset. The results
are obtained after 25 epochs of training the models.
Unif[−1, 1] as input. For the CLS-GAN generator, an one-hot vector encoding the image class
condition was concatenated with the sampled random vector.
The CLS-GAN was trained by
involving both unlabeled and labeled examples as in Section 8. This was compared against the
other state-of-the-art supervised and semi-supervised models.
Generated Images by LS-GAN
First we made a qualitative comparison between the images generated by the DCGAN and the
LS-GAN on the celebA dataset.
Figure 3 compares the visual quality of images generated by LS-GAN and DCGAN after they
were trained for 25 epochs, and there was no perceptible diﬀerence between the qualities of their
generated images.
However, the DCGAN architecture has been exhaustively ﬁne-tuned in terms of the classic
GAN training criterion to maximize the image generation performance. It was susceptible that its
architecture could be fragile if we make some change to it. Here we tested if the LS-GAN can be
more robust than the DCGAN when a structure change was made.
For example, one of the most key components in the DCGAN is the batch normalization inserted
between the fractional convolution layers in the generator network. It has been reported in literature
 that the batch normalization not only plays a key role in training the DCGAN model, but also
prevents the mode collapse of the generator into few data points.
The results were illustrated in Figure 4. If one removed the batch normalization layers from
the generator, the DCGAN would collapse without producing any face images. On the contrary,
the LS-GAN still performed very well even if these batch normalization layers were removed, and
there was no perceived deterioration or mode collapse of the generated images. This shows that
the LS-GAN was more resilient than the DCGAN.
We also analyzed the magnitude (ℓ2 norm) of the generator’s gradient (in logarithmic scale)
in Figure 5 over iterations. With the loss function being updated every iteration, the generator
was only updated every 1, 3, and 5 iterations. From the ﬁgure, we note that the magnitude of the
generator’s gradient, no matter how frequently the loss function was updated, gradually increased
until it stopped at the same level. This implies the objective function to update the generator
tended to be linear rather than saturated through the training process, which was consistent with
our non-parametric analysis of the optimal loss function. Thus, it provided suﬃcient gradient to
continuously update the generator. Furthermore, we compared the images generated with diﬀerent
frequencies of updating the loss function in Figure 6, where there was no noticeable diﬀerence
Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities
(b) LS-GAN
Figure 4: Images generated by the DCGAN and the LS-GAN on the CelebA dataset without batch
normalization for the generator networks. The results are obtained after 25 epochs of training the
Iterations
Log of Gradient Norm
Every 1 iteration
Every 3 iterations
Every 5 iterations
Figure 5: The log of the generator’s gradient norm over iterations. The generator is updated every
1, 3, and 5 iterations while the loss function is updated every iteration. The loss function can be
quickly updated to be optimal, and the ﬁgure shows the generator’s gradient does not vanish even
if the loss function is well trained.
in the visual quality. This shows the LS-GAN was not aﬀected by over-trained loss function in
experiments.
Image Classiﬁcation
We conducted experiments on CIFAR-10 and SVHN to compare the classiﬁcation accuracy of
LS-GAN with the other approaches.
The CIFAR dataset consists of 50,000 training images and 10, 000 test images on ten
image categories. We tested the proposed CLS-GAN model with class labels as conditions. In the
supervised training, all labeled examples were used to train the CLS-GAN.
We also conducted experiments with 400 labeled examples per class, which was a more chal-
Figure 6: Images generated by the LS-GAN on CelebA, where its generator is updated every three
times (a) and every ﬁve times (b) the discriminator is updated.
Table 2: Classiﬁcation accuracies on CIFAR-10 dataset. Accuracies with all training examples
labeled (all) and with only 400 labeled examples per class (400) are reported. The best result is
highlighted in bold.
400 per class
1 Layer K-means 
63.7% (± 0.7%)
3 Layer K-means Learned RF 
70.7%(± 0.7%)
View Invariant K-means 
72.6%(± 0.7%)
Examplar CNN 
77.4%(± 0.2%)
Conditional GAN 
75.5%(± 0.4%)
DCGAN 
73.8%(± 0.4%)
Ladder Network 
79.6%(± 0.5%)
CatGAN 
80.4%(± 0.4%)
Improved GAN 
81.4%(± 2.3%)
82.7%(± 0.5%)
lenging task as much fewer labeled examples were used for training. In this case, the remaining
unlabeled examples were used to train the model in a semi-supervised fashion as discussed in Section 8. In each mini-batch, the same number of labeled and unlabeled examples were used to update
the model by stochastic gradient descent. The experiment results on this task were reported by
averaging over ten subsets of labeled examples.
Both hyperparameters γ and λ were chosen via a ﬁve-fold cross-validation on the labeled examples from {0.25, 0.5, 1.0, 2.0} and {0.5, 1.0, 2.0} respectively. Once they were chosen, the model
was trained with the chosen hyperparameters on the whole training set, and the performance was
reported based on the results on the test set. As in the improved GAN, we also adopted the weight
normalization and feature matching mechanisms for the sake of the fair comparison.
We compared the proposed model with the state-of-the-art methods in literature. In particular, we compared with the conditional GAN as well as the DCGAN . For the sake of fair
comparison, the conditional GAN shared the same architecture as the CLS-GAN. On the other
hand, the DCGAN algorithm max-pooled the discriminator’s convolution features from all
Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities
Table 3: Classiﬁcation errors on SVHN dataset with 1, 000 labeled examples. The best result is
highlighted in bold.
Error rate
M1+KNN 
M1+TSVM 
M1+M2 
SWWAE w/o dropout 
SWWAE with dropout 
DCGAN 
Conditional GAN 
21.85%±0.38%
Supervised CNN 
36.02%±0.10%
Virtual Adversarial 
Auxiliary DGN 
Skip DGN 
16.61%±0.24%
Improved GAN 
8.11%±1.3%
5.98%± 0.27%
layers to 4 × 4 grids as the image features, and a L2-SVM was then trained to classify images.
The DCGAN was an unsupervised model which had shown competitive performance on generating photo-realistic images. Its feature representations were believed to reach the state-of-the-art
performance in modeling images with no supervision.
We also compared with the other recently developed supervised and semi-supervised models
in literature, including the baseline 1 Layer K-means feature extraction pipeline, a multi-layer
extension of the baseline model (3 Layer K-means Learned RF ), View Invariant K-means ,
Examplar CNN , Ladder Network , as well as CatGAN .
In particular, among the
compared semi-supervised algorithms, the improved GAN had recorded the best performance
in literature. Furthermore, we also compared with the ALI that extended the classic GAN by
jointly generating data and inferring their representations, which achieved comparable performance
to the Improved GAN. This pointed out an interesting direction to extend the CLS-GAN by directly
inferring the data representation, and we will leave it in the future work.
Table 2 compares the experiment results, showing the CSL-GAN successfully outperformed the
compared algorithms in both fully-supervised and semi-supervised settings.
The SVHN (i.e., Street View House Number) dataset contains 32 × 32 color images of
house numbers collected by Google Street View. They were roughly centered on a digit in a house
number, and the objective is to recognize the digit. The training set has 73, 257 digits while the
test set consists of 26, 032.
To test the model, 1, 000 labeled digits were used to train the model, which are uniformly
(b) CIFAR-10
Figure 7: Images generated by CLS-GAN for MNIST, CIFAR-10 and SVHN. Images in a column
are generated for the same class. In particular, the generated images on CIFAR-10 are airplane,
automobile, bird, cat, deer, dog, frog, horse, ship and truck from the leftmost to the rightmost
selected from ten digit classes, that is 100 labeled examples per digit class. The remaining unlabeled
examples were used as additional data to enhance the generative ability of CLS-GAN in semisupervised fashion.
We expect a good generative model could produce additional examples to
augment the training set.
We used the same experiment setup and network architecture for CIFAR-10 to train the LS-
GAN on this dataset. Table 3 reports the result on the SVHN, and it shows that the LS-GAN
performed the best among the compared algorithms.
Analysis of Generated Images by CLS-GAN
Figure 7 illustrates the generated images by CLS-GAN for MNIST, CIFAR-10 and SVHN
datasets. On each dataset, images in a column were generated for the same class. On the MNIST
and the SVHN, both handwritten and street-view digits are quite legible. Both also cover many
variants for each digit class. For example, the synthesized MNIST digits have various writing styles,
rotations and sizes, and the generated SVHN digits have various lighting conditions, sizes and even
diﬀerent co-occurring digits in the cropped bounding boxes.
On the CIFAR-10 dataset, image
classes can be recognized from the generated images although some visual details are missing. This
is because the images in the CIFAR-10 dataset have very low resolution (32 × 32 pixels), and most
details are even missing from input examples.
We also observe that if we set a small value to the hyperparameter λ, the generated images
would become very similar to each other within each class. As illustrated in Figure 8, the images
were generated by halving λ used for generating images in Figure 7. A smaller λ means a relatively
large weight was placed on the ﬁrst loss minimization term of (6), which tends to collapse generated
images to a single mode as it aggressively minimizes their losses to train the generator. This is also
consistent with Theorem 5 where the density of generated samples with a smaller λ could have a
larger deviation from the underlying density. One should avoid the collapse of trained generator
since diversifying generated images can improve the classiﬁcation performance of the CLS-GAN by
revealing more intra-class variations. This will help improve the model’s generalization ability as
these variations could appear in future images.
However, one should also avoid setting too large value to λ. Otherwise, the role of the ﬁrst
loss minimization term could be underestimated, which can also adversely aﬀect the classiﬁcation
Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities
(b) CIFAR-10
Figure 8: Illustration of generated images that are collapsed to a single mode of the underlying
image density on MNIST, CIFAR-10 and SVHN.
results without reducing the training loss to a satisfactory level. Therefore, we choose a proper
value for λ by cross-validation on the training set in the experiments.
In brief, the comparison between Figure 7 and Figure 8 reveals a trade-oﬀbetween image
generation quality and classiﬁcation accuracy through the hyperparameter λ. Such a trade-oﬀis
intuitive: while a classiﬁcation task usually focuses on learning class-invariant representations that
do not change within a class, image generation should be able to capture many variant factors
(e.g., lighting conditions, viewing angles, and object poses) so that it could diversify generated
samples for each class.
Although diversiﬁed examples can augment training dataset, it comes
at a cost of trading class-invariance for modeling variant generation factors. Perhaps, this is an
intrinsic dilemma between supervised learning and data generation that is worth more theoretical
and empirical studies in future.
Evaluation of Generalization Performances
Most of existing metrics like Inception Score for evaluating GAN models focus on comparing
the qualities and diversities of their generated images. However, even though a GAN model can
produce diverse and high quality images with no collapsed generators, it is still unknown if the
model can generate unseen images out of given examples, or simply memorizing existing ones.
While one of our main pursuits in this paper is a generalizable LS-GAN, we were motivated to
propose the following Minimum Reconstruction Error (MRE) to compare its generalizability with
various GANs.
Speciﬁcally, for an unseen test image x, we aim to ﬁnd an input noise z that can best reconstruct
x with the smallest error, i.e., minz ∥x −G(z)∥1, where G is the GAN generator under evaluation.
Obviously, if G is adequate to produce new images, it should have a small reconstruction error on
a separate test set that has not been used in training the model.
We assessed the GAN’s generalizability on CIFAR-10 and tiny ImageNet datasets. On CIFAR-
10, we split the dataset into 50% training examples, 25% validation examples and 25% test examples; the tiny ImageNet was split into training, validation and test sets in a ratio of 10:1:1. For a
fair comparison, all the hyperparameters, including the number of epochs, were chosen based on
the average MREs on the validation set, and the test MREs were reported for comparison. The
optimal z’s were iteratively updated on the validation and test sets by descending the gradient of
the reconstruction errors.
In Figure 9, we compare the test MREs over 100 epochs by LS-GAN, GLS-GAN, WGAN ,
(a) CIFAR-10
(b) tiny ImageNet
Figure 9: The change of test MREs on CIFAR-10 and tiny ImageNet over epochs. Image pixels
were scaled to [−1, 1] to compute the MREs.
WGAN-GP and DCGAN on CIFAR-10 respectively. For the sake of a fair comparison,
all models were trained with the network architecture used in . The result clearly shows the
regularized models, including GLS-GAN, LS-GAN, WGAN-GP and WGAN, have apparently better
generalization performances than the unregularized DCGAN based on the classic GAN model. On
CIFAR-10, the test MRE was reduced from 0.1506 by DCGAN to as small as 0.1109 and 0.1089
by WGAN and GLS-GAN respectively; on tiny ImageNet, the GLS-GAN reaches the smallest test
MRE of 0.2085 among all compared regularized and unregularized GANs.
In addition, the DCGAN exhibited ﬂuctuating MREs on the CIFAR-10, while the regularized
models steadily decreased the MREs over epochs. This implies regularized GANs have more stable
training than the classic GAN.
We illustrate some examples of reconstructed images by diﬀerent GANs on the test set along
with their test MREs in Figure 11. The results show the GLS-GAN achieved the smallest test
MRE of 0.1089 and 0.2085 with a LeakyReLU cost function of slope 0.01 and 0.5 on CIFAR-10 and
tiny ImageNet, followed by the other regularized GAN models. This is not a surprising result since
it has been shown in Section 6.2 that the other regularized GANs such as LS-GAN and WGAN
are only special cases of the GLS-GAN model that covers larger family of models. Here we only
considered LeakyReLU as the cost function for GLS-GAN. Of course, there exist many more cost
functions satisfying the two conditions in Section 6.2 to expand the family of regularized GANs,
which should have potentials of yielding even better generalization performances.
Conclusions
In this paper, we present a novel Loss-Sensitive GAN (LS-GAN) approach to generate samples
from a data distribution. The LS-GAN learns a loss function to distinguish between generated
and real samples, where the loss of a real sample should be smaller by a margin than that of a
generated sample. Our theoretical analysis shows the distributional consistency between the real
and generated samples based on the Lipschitz regularity. This no longer needs a non-parametric
discriminator with inﬁnite modeling ability in the classic GAN, allowing us to search for the optimal
loss function in a smaller functional space with a bounded Lipschitz constant. Moreover, we prove
the generalizability of LS-GAN by showing its required number of training examples is polynomial
in its complexity. This suggests the generalization performance can be improved by penalizing
the Lipschitz constants (via their gradient surrogates) of the loss function to reduce the sample
Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities
(a) Original test images
(b) LS-GAN(0.1166)
(c) GLS-GAN(0.1089)
(d) WGAN-GP(0.1149)
(e) WGAN(0.1109)
(f) DCGAN(0.1506)
Figure 10: The ﬁgure illustrates the images reconstructed by various GANs on CIFAR-10 with
their MREs on the test set in the parentheses.
complexity. Furthermore, our non-parametric analysis of the optimal loss function shows its lower
and upper bounds are cone-shaped with non-vanishing gradient almost everywhere, implying the
generator can be continuously updated even if the loss function is over-trained. Finally, we extend
the LS-GAN to a Conditional LS-GAN (CLS-GAN) for semi-supervised tasks, and demonstrate it
reaches competitive performances on both image generation and classiﬁcation tasks.