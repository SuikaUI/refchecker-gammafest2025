IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL.XX, NO.X, MONTH, 2003
Foveation Scalable Video Coding with Automatic
Fixation Selection
Zhou Wang, Member, IEEE, Ligang Lu, Member, IEEE, and Alan C. Bovik, Fellow, IEEE
Abstract—Image and video coding is an optimization problem. A successful image and video coding algorithm delivers
a good tradeoﬀbetween visual quality and other coding performance measures, such as compression, complexity, scalability, robustness, and security. In this paper, we follow two
recent trends in image and video coding research. One is to
incorporate human visual system (HVS) models to improve
the current state-of-the-art of image and video coding algorithms by better exploiting the properties of the intended
The other is to design rate scalable image and
video codecs, which allow the extraction of coded visual information at continuously varying bit rates from a single
compressed bitstream.
Speciﬁcally, we propose a foveation scalable video coding
(FSVC) algorithm which supplies good quality-compression
performance as well as eﬀective rate scalability.
idea is to organize the encoded bitstream to provide the best
decoded video at an arbitrary bit rate in terms of foveated
visual quality measurement. A foveation-based HVS model
plays an important role in the algorithm. The algorithm is
adaptable to diﬀerent applications, such as knowledge-based
video coding and video communications over time-varying,
multi-user and interactive networks.
Keywords—video coding, rate scalable coding, human visual system, foveation, image and video quality, wavelet
I. Introduction
It has been envisioned that network visual services, such
as network video broadcasting, video-on-demand, videoconferencing and telemedicine, will become ubiquitous in
the twenty-ﬁrst century. As a result, network visual communication has become an active research area in recent
years. One of the most challenging problems for the implementation of a video communication system is that the
available bandwidth of the networks is usually insuﬃcient
for the delivery of the voluminous amount of the video data.
In order to solve this problem, considerable eﬀort has been
applied in the last three decades for the development of
video compression techniques. These eﬀorts have resulted
in the video coding standards such as H.261 , H.263 ,
MPEG-1 , , MPEG-2 , , and MPEG-4 , .
Designing a video coding and communication system
is a complicated task.
The ﬁrst issue that needs to be
considered is the quality-compression performance, which
aims to provide the best quality decoded video with the
minimal number of bits.
Depending on the application,
Z. Wang was with Laboratory for Image and Video Engineering
(LIVE), The University of Texas at Austin, Austin, TX 78712. He
is now with Laboratory for Computational Vision (LCV), New York
University, New York, NY 10003. L. Lu is with Multimedia Technologies, IBM T. J. Watson Research Center, Yorktown Heights, NY
10598. A. C. Bovik is with Laboratory for Image and Video Engineering (LIVE), The University of Texas at Austin, Austin, TX 78712.
E-mail: , , .
This research was supported in part by IBM Corp., Texas Instruments, Inc., and by State of Texas Advanced Technology Program.
Frame Number
Bitstream scaling in rate scalable video communications.
Each bar represents the bitstream for one frame in the video
there are many other issues related to the goodness of
the video codecs. For example, low computational complexity is usually required for real-time applications.
many cases, parallelizability is a desired feature to improve
speed. Satisfying a low memory requirement is desirable in
many applications to achieve easy buﬀering and easy embedded implementations on digital signal processors. Several communication and networking issues are also relevant,
such as scalability, robustness, security and interactivity.
Although the video coding standards exhibit acceptable
quality-compression performance in many visual communication applications, further improvements are desired and
more features need to be added, especially for some speciﬁc applications. Recently, two interesting research trends
have emerged that are very promising and may lead to signiﬁcantly improved video codecs in comparison with the
current standards.
The ﬁrst trend is to incorporate Human Visual System
(HVS) models into the coding system. Presently, the objective quality measure Peak Signal-to-Noise Ratio (PSNR)
is widely employed to evaluate video quality. However, it
is well accepted that perceived video quality does not correlate well with PSNR. HVS characteristics must be considered to provide better visual quality measurements. In
the literature, many HVS-based algorithms have been proposed for this purpose – . Although the current understanding of the HVS still is insuﬃcient to provide a
precise, generic and robust algorithm to measure perceived
video quality in all circumstances, it is believed that an
appropriate HVS model that takes advantage of some wellunderstood HVS features can signiﬁcantly help to improve
the current state-of-the-art of video coding algorithms.
The second research trend is to develop continuously rate
scalable coding algorithms – , which allow the extraction of coded visual information at continuously varying bit rates from a single compressed bitstream. An ex-
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL.XX, NO.X, MONTH, 2003
ample is shown in Fig.
1, where the original video sequence is encoded with a rate scalable coder and the encoded bitstream is stored frame by frame.
During the
transmission of the coded data on the network, we can
scale, or truncate, the bitstream at any place and send the
most important bits of the bitstream. Such a scalable bitstream can provide numerous versions of the compressed
video at various data rates and levels of quality. This feature is especially suited for video transmission over heterogeneous, multi-user, time-varying and interactive networks such as the Internet, where variable bandwidth video
streams need to be created to meet diﬀerent user requirements.
The traditional solutions, such as layered video
 , , , video transcoding , , and simply repeated
encoding, require more resources in terms of computation,
storage space and/or data management. More importantly,
they lack the ﬂexibility to adapt to time-varying network
conditions and user requirements, because once the compressed video stream is generated, it becomes inconvenient
to change it to an arbitrary data rate. By contrast, with a
continuously rate scalable codec, the data rate of the video
being delivered can exactly match the available bandwidth
on the network.
In this paper, we propose a new video coding approach
called Foveation Scalable Video Coding (FSVC), which
stands at the intersection of the two promising research
trends. Speciﬁcally, wavelet-based embedded bitplane coding techniques are used for rate scalable coding. Further,
we exploit the foveation feature of the HVS, which refers
to the fact that the HVS is a highly space-variant system,
where the spatial resolution is highest at the point of ﬁxation (foveation point) and decreases dramatically with increasing eccentricity. By taking advantage of the this effect, considerable high frequency information redundancy
can be removed from the peripheral regions without signiﬁcant loss of the reconstructed image and video quality. The
foveation factor has been employed in previous work to improve image and video coding eﬃciency – . Foveated
image and video coding is closely related to Region-of-
Interest (ROI) image and video coding (e.g., – ).
“If we deﬁne the area(s) around the point(s) of ﬁxation
as the region of interest, then foveation-based image processing can be viewed as a special case of ROI image processing” .
The major diﬀerence with respect to traditional ROI processing is that the “interest” is continuously space-variant and conforms with HVS characteristics. Most of the foveation algorithms used a ﬁxed foveation
model. These methods lack the ﬂexibility to adapt to different foveation depths and are not convenient to be implemented in a rate scalable manner. Chang et al. made one
of the ﬁrst attempts to develop a wavelet-based scalable
foveated image compression and progressive transmission
system – .
However, human visual characteristics
were not considered in depth, and no eﬃcient coding algorithms were implemented to provide a quality-compression
performance comparable to other state-of-the-art of image
coding techniques.
In , Wang and Bovik proposed a scalable foveated
Fig. 2. (a) 2-D DWT decomposition structure; (b) Spatial orientation
tree in SPIHT algorithm.
Foveation Image Coding (EFIC), which naturally combines foveation ﬁltering with foveated image compression
and provides very good coding performance in terms of
foveated visual quality measurement. This paper attempts
to extend the work in for video coding. There are two
major purposes. The ﬁrst is to establish a prototype for
rate scalable foveated video coding. The prototype must
be very ﬂexible such that diﬀerent foveation point selection schemes can be applied to a single framework. The
second purpose is to implement the prototype in a speciﬁc
application environment, where FSVC is combined with an
automated foveation point selection scheme and an adaptive frame prediction algorithm.
In Section II, we describe brieﬂy the basic methods of
wavelet-based embedded bitplane coding and introduce the
general framework of our FSVC system. Section III develops the foveation-based HVS model. More details about
the implementation of the FSVC algorithm are given in
Section IV. Finally, Section V makes some concluding remarks and provides further discussions.
II. Basic Methods and General Framework
A. Wavelet-Based Image and Video Compression and Embedded Encoding
Recently, wavelet-based methods have achieved great
success in still image coding – , , . The success is due to the energy compaction feature of the Discrete Wavelet Transforms (DWTs) and the eﬃcient organization, quantization, and encoding of the wavelet coeﬃcients. Wavelet-based methods have also been applied to
compress video – , .
Readers can refer to ,
 , and for more introductory information about
wavelets, wavelet transforms, and how wavelet transforms
are used for image and video compression.
A class of embedded bitplane coding algorithms has recently attracted great attention.
The most well-known
algorithms include Shapiro’s Embedded Zerotree Wavelet
(EZW) algorithm , and Said and Pearlman’s Set Partitioning Into Hierarchical Trees (SPIHT) algorithm ,
which is a reﬁned implementation of the EZW idea. The
main objective of embedded wavelet coding is to order the
output bistream, such that the bits with greater contribu-
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL.XX, NO.X, MONTH, 2003
tion to the Mean Squared Error (MSE) between the original
and the compression images are encoded and transmitted
It has been observed that the wavelet coeﬃcients
have structural similarity across the wavelet subbands in
the same spatial orientation.
The zero tree structure in
EZW and the spatial orientation tree structure in SPIHT
capture this structural similarity very eﬀectively. For a 2-D
DWT decomposition shown in Fig. 2(a), the spatial orientation tree used by SPIHT is given in 2(b). In the EZW
and SPIHT encoders, the wavelet coeﬃcients are scanned
multiple times. Each time consists of a sorting pass and
a reﬁnement pass. The sorting pass selects the signiﬁcant
coeﬃcients and encodes the spatial orientation tree structure. A coeﬃcient is signiﬁcant if its magnitude is larger
than a threshold value, which decreases by a factor of 2 for
each successive sorting pass. The reﬁnement pass outputs
one bit for each selected coeﬃcient. An entropy coder is
usually used to further compress the output bitstream.
In HVS-based wavelet image coding algorithms, the
wavelet coeﬃcients are usually weighted according to visual importance before the encoding procedures , ,
 , . In , a modiﬁed SPIHT algorithm is designed
to improve the coding eﬃciency for weighted wavelet coef-
B. Foveation Point(s) Setup
The basic idea of FSVC is to order the output bistream,
such that the information associated with the foveated areas have higher priorities to be encoded earlier.
the encoding process, however, it is necessary to select the
foveation point(s). The best way of foveation point(s) selection is highly application dependant.
We attempt to
have a ﬂexible design in FSVC, so that it can be used in
various cases.
First, we allow FSVC to select multiple foveation points.
The reason is multifold:
1) The usual pattern of human ﬁxation is that the ﬁxation point moves slightly within a small area around the
center point of interest ;
2) There may be multiple human observers watching the
image at the same time;
3) There may exist multiple points and/or regions in
the image that have high probability to attract a human
observer’s attention.
4) Certain foveation points can be put at areas where
the human eye are very sensitive to distortions. This is
actually an extension of the foveation model, so that other
HVS features can be included into the same framework.
Second, we limit the search space of the foveation points.
Theoretically, any pixel in the observed picture could be
visually foveated. In practice, however, testing all the possible pixels require very high computation power.
encoding the locations of the foveation points will consume many bits, leading to signiﬁcant overhead in the encoded bitstream. Further, since small shifts of the foveation
points will not result in signiﬁcant diﬀerence in visual
quality and system encoding performance, it is not worth
spending too many bits and computation power to generate
Prediction
Prediction
General framework of the FSVC encoding system.
and encode the foveation point locations and to calculate
the foveated HVS model. The FSVC system ﬁrst divides
the picture being encoded into blocks with a size of 16×16,
and the candidate foveation points are limited to the centers of these blocks. By using this strategy, computation
is considerably reduced and only one bit for each block is
needed to encode the foveation point selection information.
Using this method, a binary map with a size of
16×16 of the
original image will be generated. This map can be further
compressed with an entropy coding technique such as the
arithmetic coding algorithm .
C. Framework of the Encoding System
Similar to many other video coding methods, FSVC ﬁrst
divides the input video sequence into Groups Of Pictures
(GOPs). Each GOP has one intra-coding frame (I frame)
at the beginning and the rest are predictive coding frames
(P frames). The general framework for the encoding of I
frames and P frames is given in Fig. 3.
The encoding of the I frame is the same as the EFIC algorithm developed for still image coding: ﬁrst apply the
DWT and obtain the wavelet coeﬃcients. The foveation
point selection scheme is applied and the HVS model is calculated to determine the visual importance of the wavelet
coeﬃcients. The importance value of each wavelet coeﬃcient is then used to weight the wavelet coeﬃcient. Finally,
the modiﬁed SPIHT algorithm is employed to generate
the embedded bitstream.
The encoding of the P frames is more complicated. The
idea of using P frames in video coding is to exploit temporal redundancy between adjacent frames in the video
Prediction of the current frame from its previous frame is the key technique to make use of temporal
redundancy. Motion Estimation (ME) and Motion Compensation (MC) techniques have been successfully used for
this purpose. The main diﬀerence between our FSVC algorithm and other video coding algorithms is that it uses
two instead of one version of the previous frames. One is
the original previous frame. The other is a feedback decoded version of the previous frame. The ﬁnal prediction
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL.XX, NO.X, MONTH, 2003
frame is the weighted combination of the two motion compensated prediction frames. The combination is based on
the foveation-based HVS model, which will be discussed in
detail in Section III. The DWT is applied to the prediction
error frame, and the resulting coeﬃcients are weighted and
coded with the embedded encoding algorithm.
The HVS modelling techniques are diﬀerent for I frames
and P frames. This will be discussed in Section III. During
the encoding process, a rate control algorithm is used to
allocate bits to each frame. The allocation is determined
by the available bandwidth, user requirements, the HVS
modelling results and the frame prediction error.
III. Foveation Based HVS Model
A. Foveated Resolution and Sensitivity Model
Psychological experiments have been conducted to measure the contrast sensitivity as a function of retinal eccentricity , – . In , a model that ﬁts the experimental data was given by
CT(f, e) = CT0 exp
Spatial frequency (cycles/degree);
Retinal eccentricity (degrees);
Minimal contrast threshold;
Spatial frequency decay constant;
Half-resolution eccentricity constant;
Visible contrast threshold.
The best ﬁtting parameters given in are α = 0.106,
e2 = 2.3, and CT0 = 1/64.
The contrast sensitivity is
deﬁned as the reciprocal of the contrast threshold:
CS(f, e) =
CT(f, e) .
For a given eccentricity e, equation (1) can be used to
ﬁnd its critical frequency or so called cutoﬀfrequency fc in
the sense that any higher frequency component beyond it
is imperceivable. fc can be obtained by setting CT to 1.0
(the maximum possible contrast) and solving for f:
To apply these models to digital images, we need to calculate the eccentricity for any given point x = (x1, x2)T
(pixels) in the image. For simplicity, we assume the observed image is N-pixel wide and the line from the fovea
to the point of ﬁxation in the image is perpendicular to
the image plane.
Also assume that the position of the
foveation point xf = (xf
2)T (pixels) and the viewing
distance v (measured in image width) from the eye to the
image plane are known. The distance from x to xf is given
by d(x) = ∥x −xf∥2 =
1)2 + (x2 −xf
2)2 (measured in pixels). The eccentricity is then calculated as
e(v, x) = tan−1
Pixel Position (pixels)
Spatial Frequency (cycles/degree)
Normalized contrast sensitivity (Brightness indicates the
strength of contrast sensitivity) for N = 512 and v = 3.
white curves show the cutoﬀfrequency.
With (4), we can convert the foveated contrast sensitivity
and cutoﬀfrequency models into the image pixel domain.
In Fig. 4, we show the normalized contrast sensitivity as a
function of pixel position for N = 512 and v = 3. The cutoﬀfrequency as a function of pixel position is also given.
The contrast sensitivity is normalized so that the highest
value is always 1.0 at 0 eccentricity. It can be observed that
the cut-oﬀfrequency drops quickly with increasing eccentricity and the contrast sensitivity decreases even faster.
In real-world digital images, the maximum perceived resolution is also limited by the display resolution, which is
approximately:
According to the sampling theorem, the highest frequency
that can be represented without aliasing by the display, or
the display Nyquist frequency, is half of the display resolution:
Combining (3) and (6), we obtain the cutoﬀfrequency for
a given location x by:
fm(v, x) = min(fc(e(v, x)), fd(v)) .
Finally, we deﬁne the foveation-based error sensitivity for
given viewing distance v, frequency f and location x as:
Sf(v, f, x) =
CS(f,e(v,x))
f ≤fm(v, x)
Sf is normalized so that the highest value is always 1.0 at
0 eccentricity.
B. Spatial Domain Foveated Weighting Model
In the FSVC system, two foveated weighting models are
developed, one in the spatial domain and the other in the
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL.XX, NO.X, MONTH, 2003
DWT domain. The spatial domain weighting model is employed by the adaptive frame prediction algorithm to adjust
the combination from the original and the decoded motioncompensated frames, and the wavelet domain weighting
model is used to determine the importance of the wavelet
coeﬃcients and help ordering the output bitstream.
The spatial domain weighting model is obtained by normalizing the cutoﬀfrequency model deﬁned in (7):
Ws(v, x) =
· fm(v, x)
where γ is a parameter used to control the shape of the
weighting model.
For a ﬁxed viewing distance v0, this
weighting model can be written as Ws(x) = Ws(v0, x).
This model can easily adapt to multiple foveation points.
Suppose that there are K foveation points xf
K in the image. For each of the points, we can calculate the weighting model individually and have W i
i = 1, 2, · · · , K.
In the worst case, the human observer
would ﬁxate at the foveation point which is the closest with
respect to x. This results in the maximum value of W i
for all i.
Therefore, the overall weighting value for x is
i∈{1,···,K} W i
In practice, it is not necessary to compute each of the
s(x). Because for a given pixel x, the foveation point
that is closest to it must generate the maximum weight,
hence we have
Ws(x) = W j
j ∈arg min
i∈{1,···,K}
By doing this, a large amount of computation is saved.
C. Wavelet Domain Foveated Weighting Model
The wavelet coeﬃcients at diﬀerent subbands and locations supply information of variable perceptual importance
to the HVS. In , psychovisual experiments were conducted to measure the visual sensitivity in wavelet decompositions. Noise was added to the wavelet coeﬃcients of
a blank image with uniform mid-gray level. After the inverse wavelet transform, the noise threshold in the spatial
domain was tested. A model that provided a reasonable ﬁt
to the experimental data is :
log Y = log a + k(log f −log gθf0)2
Visually detectable noise threshold;
Orientation index, representing LL, LH,
HH, and HL subbands, respectively;
Spatial frequency (cycles/degree);
k, f0, gθ:
Constant parameters.
f is determined by the display resolution r and the wavelet
decomposition level λ : f = r2−λ. The constant parameters in (12) are tuned to ﬁt the experimental data.
For gray scale models, a is 0.495, k is 0.466, f0 is 0.401,
Wavelet domain importance weighting mask of a single
foveation point.
Brightness indicates the importance of the
wavelet coeﬃcients (Brightness logarithmically enhanced for display purpose).
and gθ is 1.501, 1, and 0.534 for the LL, LH/HL, and HH
subbands, respectively. The error detection thresholds for
the wavelet coeﬃcients can be calculated by:
Tλ,θ = Yλ,θ
= a10k(log(2λf0gθ/r))2
where Aλ,θ is the basis function amplitude given in .
We deﬁne the error sensitivity in subband (λ, θ) as:
Sw(λ, θ) =
For a given wavelet coeﬃcient at position x ∈Bλ,θ,
where Bλ,θ denotes the set of wavelet coeﬃcient positions
residing in subband (λ, θ), its equivalent distance from the
foveation point in the spatial domain is given by
dλ,θ(x) = 2λ °°°x −xf
λ,θ is the corresponding foveation point in subband
(λ, θ). With the equivalent distance, and also considering
(8), we have
Sf(v, f, x) = Sf(v, r2−λ, dλ,θ(x))
Combining (14) and (16), a wavelet domain foveation-based
visual sensitivity model is achieved:
S(v, x) = [Sw(λ, θ)]β1·
Sf(v, r2−λ, dλ,θ(x))
where β1 and β2 are parameters used to control the magnitudes of Sw and Sf, respectively.
For a given wavelet coeﬃcient at location x, the ﬁnal
weighting model is obtained by integrating S(v, x) over v:
0+ p(v)S(v, x) dv ,
where p(v) is the probability density distribution of the
viewing distance v . Fig. 5 shows the ﬁnal importance
weighting mask in the DWT domain. Similar to the spatial
domain model, for the case of multiple foveation points, the
overall weighting value is obtained by:
Ww(x) = W j
j ∈arg min
i∈{1,···,K}
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL.XX, NO.X, MONTH, 2003
An I frame (a) and a P frame (b) in the “News” video sequence.
IV. Implementation of FSVC
The general framework introduced in Section II is ﬂexible and can adapt to diﬀerent application environments. As
an example, our current implementation of FSVC focuses
on developing an automated foveation setup approach for
video sequences with human faces. Furthermore, an adaptive algorithm is proposed for the prediction of the current
frame from motion compensated previous frames.
A. Determination of Foveation Points
Human faces are probably the most frequently focused
regions by human observers. A face-foveated video coding algorithm will be very useful to eﬀectively enhance the
visual quality in many speciﬁc video communication environments such as videoconferencing.
The face detection
algorithm used in our FSVC implementation is similar to
that in . It consists of three steps.
The ﬁrst step is to identify the possible face regions by
the skin color information . The entire YCrCb color
space is divided into a skin color subspace and a non-skin
color subspace. Each point in the picture can then be assigned to either of the two subspaces.
In Step 2, we detect human faces in those skin-color regions by a technique called binary template matching .
In the last step, we verify every detected face and remove falsely detected faces. The veriﬁcation is based on
the observation that human face areas usually have a certain amount of high frequency content because of the
existing of discontinuities at eyes, nose and mouth.
each detected face region, we calculate the variance of the
pixels in it. Only the regions with variances larger than a
threshold value are ﬁnally veriﬁed as face regions.
The methods to select foveation points for I frames and P
frames are diﬀerent. For I frames, we ﬁrst detect face areas
as regions of interest and put foveation points inside those
regions. The face detection algorithm described above is
very eﬃcient but does not provide precise boundaries of
the face areas.
Since small shifts of foveation points do
not have signiﬁcant eﬀects on visual quality, this kind of
rough face detection is enough for the FSVC system to
work properly. An example is given in Fig. 6 and Fig. 7,
where one I frame extracted from the “News” sequence is
shown in Fig. 6(a). The selected foveation points of this
frame is given in Fig. 7(a).
For the foveation point selection of P frames, a diﬀerent strategy is used because of two reasons. First, more
information is available because the current P frame can
be compared with the previous frame to locate the new information presented in the current frame. Second, the P
frames are not encoded directly. Only the diﬀerence between the current frame and the prediction from the previous frame is of concern to us. If the prediction error of
a local region is very small, then it is not necessary to put
any foveation point in that region, regardless of whether
the region is ﬁxated or not. FSVC focuses on the regions
in the current P frame that provide us with new information from its previous frame. Usually, the prediction errors
in those regions are larger than other regions. Therefore,
FSVC mainly selects foveation points in those regions with
prediction errors larger than a threshold value. For example, for the P frame shown in Fig. 6(b), which follows the
I frame given in Fig.
6(a), the error thresholding-based
method selects the foveation regions shown in Fig. 7(b).
The drawback of this method is that the face regions will
lose ﬁxation.
To solve this problem, we use an unequal
error thresholding method to determine foveation regions
in P frames. This is based on the fact that when human
observers’ attention is ﬁxating on human faces, even very
small changes in these areas are very likely to be noticed.
Therefore, we use a much smaller prediction error threshold
value to capture the changes occurring in the face regions.
Using the unequal error thresholding based method, the
foveation region selection result for Fig. 6(b) is shown in
Fig. 7(c). Compared with Fig. 7(b), some foveation points
in the face regions are added.
B. Adaptive Frame Prediction
In ﬁxed rate ME/MC based video coding algorithms, a
common choice for frame prediction is to use the feedback
decoded previous frame as the reference frame for the pre-
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL.XX, NO.X, MONTH, 2003
Foveation point selection of the “News” video sequence. (a) I frame foveation point selection; (b) P frame foveation point selection
with equal error thresholding; (c) P frame foveation point selection with unequal error thresholding.
diction of the current frame. With this choice, the prediction frames are exactly the same at the encoder and the
decoder. However, this choice is infeasible for continuously
rate scalable coding because the decoding bit rate is the
choice of the decoder and is unavailable to the encoder.
There are several solutions to this problem.
The ﬁrst solution simply uses the original motion compensated frames to do the prediction. Since the original
frames are not available at the decoder, the prediction
frames at the encoder and the decoder sides are diﬀerent,
sometimes very diﬀerent.
The consequence is that very
good frame prediction at the encoder side may produce
poor prediction at the decoder side. In addition, the poor
prediction error will propagate to all the following P frames
in the same GOP.
The second solution is to deﬁne a low base bit rate and
use the decoded and motion compensated frame at the
base bit rate as the prediction. This idea has been used
in , . The advantage of this solution is that the prediction frames at the encoder and the decoder are exactly
the same. Therefore, signiﬁcant error propagation problems are avoided. However, if the decoding bit rate is much
higher than the base bit rate, large prediction errors will
occur. For example, suppose we have a texture region that
does not change between frames. At an I frame, the region
is encoded at a high bit rate with high quality. Since there
is no change between frames, very good prediction with
almost zero prediction error is expected.
However, with
the second prediction solution, the low base rate decoded
frame (with low quality) is selected to do the prediction.
This leads to poor prediction and the ﬁne textures of the
regions are actually encoded repeatedly. In conclusion, this
solution results in less precise prediction and less eﬃcient
compression.
We propose a new solution to this problem, where the
original motion compensated frame and the base bit rate
decoded and motion compensated frame are combined to
make a prediction. The combination is adaptively changed
using the foveation model. The encoder and decoder sides
of the new frame prediction algorithms are shown in Fig.
8 and Fig. 9, respectively.
At the encoder, there are two reference frames.
is the previous frame from the original sequence, and the
other is the previous frame decoded from the base bit rate.
The same motion compensation process is applied to both
of them and generates two motion compensated reference
frames. These two frames are combined by the spatial domain foveation weighting model. Let Ws(x) be the normalized weight at location x. Let PO(x) and PB(x) be the
pixel values at location x of the motion compensated origi-
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL.XX, NO.X, MONTH, 2003
Foveation Setup
HVS Modeling
Prediction
Inverse Weighting
Inverse DWT
Prediction
Error Frame
Adaptive frame prediction: encoder side.
Prediction
Decoding of
Motion Vectors
Inverse Weighting
Inverse DWT
Decoding of
Information
Inverse Weighting
Inverse DWT
Adaptive frame prediction: decoder side.
nal reference frame and base rate decoded reference frame,
respectively. Then the combined encoder prediction value
PE(x) is given by:
PE(x) = [1 −Ws(x)] PO(x) + Ws(x) PB(x) .
At the decoder, the weighting information is decoded and
calculated in exactly the same way as in the encoder. There
are also two versions of the reference frames. One is the
previous frame decoded from the base rate. The other is the
previous frame decoded at the current decoding bit rate.
Motion compensation is applied to both reference frames.
Let PC(x) be the pixel values at location x of the motion
compensated reference frame at the current decoding bit
rate, then the combined decoder prediction value PD(x) is:
PD(x) = [1 −Ws(x)] PC(x) + Ws(x) PB(x) .
The idea behind the weighting equations (20) and (21)
is that for the diﬃcult prediction regions, more weight
is given to the base rate motion compensated reference
frames, while for the easy prediction regions, more weight
is given to the high quality motion compensated reference
frames. If the prediction errors are in the mid-range, the
adaptive frame prediction algorithm will provide a “fuzzy”
solution according to the mid-range values of Ws(x), which
provides a trade-oﬀbetween prediction from the base rate
motion compensated frame and the prediction from high
quality motion compensated reference frame. The frame
predictions at the encoder and decoder are not exactly the
same. Subtracting (21) from (20) yields
PE(x) −PD(x) = [1 −Ws(x)] [PO(x) −PC(x)] .
Since at the diﬃcult prediction regions, the value of Ws(x)
is large, the error between PE(x) and PD(x) is very small
and can be neglected. At the easy prediction regions, the
values of PC(x) is close to PO(x). Therefore, the prediction
diﬀerence between the encoder and the decoder is small. In
this way, the error propagation is well controlled. Also note
that at the easy prediction regions, the value of Ws(x) is
small and the actual prediction in (20) and (21) is mainly
from PO(x) and PC(x). Since PO(x) and PC(x) are from
high quality prediction frames, their prediction values are
much better than the poor prediction of PB(x). In this
way, the prediction errors are reduced.
In conclusion, by using the new frame prediction algorithm, error propagation becomes a small problem, while at
the same time, better frame prediction is achieved, which
leads to smaller prediction errors and better compression
performance.
C. Experimental Results
We test the FSVC system on CIF size (288×352), YCbCr
4:2:0 format video sequences. In order to give a quantitative measurement on how much quality gain is achieved by
using the foveated techniques, it is important to employ an
image quality metric designed for foveated images. Most
image quality measurement methods in the literature are
not appropriate because they are designed for uniform resolution images. In , , a wavelet-based foveated image
quality assessment metric called Foveated Wavelet Quality Index (FWQI) was proposed by combining the wavelet
domain visual sensitivity model (17) and a novel image
quality indexing algorithm , . FWQI has a dynamic
range of , where 1 represents the best quality. One distinct feature of the new quality indexing approach in comparison with the traditional image quality assessment techniques is that it considers image degradations as “structural information loss” or “structural distortions” instead
of “perceived errors”. More insights and discussions about
the new indexing method are provided in , , .
Fig. 10 shows 4 consecutive frames in the “Silence” sequence and the corresponding selected foveation points, in
which the ﬁrst frame is an I frame and the rest are P frames.
The FSVC compression result at 200 Kbits/sec is also given
in the same ﬁgure. It can be observed that the face region
and the relative moving information between frames are
captured very well with the automated foveation point selection algorithm.
11 compares the compression results of the 26th
frame (a P frame) of the “News” sequence to demonstrate the eﬀectiveness of the foveation method against
non-foveation method and the adaptive frame prediction
scheme against traditional frame prediction schemes. The
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL.XX, NO.X, MONTH, 2003
Consecutive frames of the “Silence” sequence (left); the selected foveation points (middle); and the FSVC compression results at
200 Kbits/sec (right).
coding algorithms being compared include (1) Uniform resolution scalable coding without any foveation models applied; (2) Foveated scalable coding with frame prediction
from original previous frames; (3) Foveated scalable coding with frame prediction from base rate coded previous
frames; and (4) FSVC with adaptive frame prediction. At
the same bit rate of 200 Kbits/sec, FSVC with adaptive
frame prediction exhibits the best foveated subjective quality.
To demonstrate the scalable features of FSVC, Fig. 12
compares the FWQI results of the decompressed “Salesman” sequences at 200, 400 and 800 Kbits/sec, respectively. The reconstructed sequences are created from the
same encoded bitstream by truncating the bitstream at different places. Fig. 13 shows the reconstructed 32th frame
of the “Salesman” sequence at 200, 400 and 800 Kbits/sec,
respectively.
The results exhibit not only the rate scalable feature but also the foveation scalable characteristic of
FSVC, in the sense that the foveation depth increases with
the decrease of bit rate.
V. Conclusions and Discussions
In this paper, a new wavelet-based scalable foveated
video coding system, FSVC, is proposed.
A foveationbased HVS model plays an important role in the system. It
helps the coding system to foveate on the visually important components in the video sequence. FSVC is a ﬂexible
prototype that can incorporate various kinds of foveation
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL.XX, NO.X, MONTH, 2003
Frame 26 of the “News” sequence compressed at 200 Kbits/sec with (a) uniform resolution scalable coding; (b) foveated scalable
coding with frame prediction from original previous frames; (c) foveated scalable coding with frame prediction from base rate coded
previous frames; and (d) FSVC with adaptive frame prediction, respectively.
point selection schemes to ﬁt in diﬀerent application environments. Speciﬁcally, we implemented a foveation region
selection algorithm for the encoding of video sequences with
human faces. A novel automated foveation point selection
scheme and an adaptive frame prediction algorithm is proposed. By using the adaptive frame prediction algorithm,
error propagation is well controlled, while at the same time,
better frame prediction is achieved.
The FSVC technique has many potential applications.
One application is knowledge-based video coding. Many different kinds of knowledge about the contents and the contexts of the encoded sequences can be naturally embedded
into the general FSVC system. This implies that FSVC is
very good for special-purpose video communication applications such as videoconferencing and telemedicine, where
a lot of prior information is available to the encoder. If
an Audio-Visual Object (AVO) description , of the
scene is available, then higher visual quality-compression
performance can be expected.
In general, the more we
know about the video signal being encoded, the more we
can improve the performance of FSVC.
FSVC is very suitable for dynamic variable bit rate network video transmission.
For example, if the available
bandwidth drops dramatically on the network, a ﬁxed data
Fig. 12. FWQI measurement results of “Salesman” sequence at 200,
400 and 800 Kbits/sec, respectively.
rate coding system has to stop transmission. A uniform
resolution scalable coding system can still work properly
but might transmit completely unacceptable quality video
to the client. A FSVC system, however, may still deliver
useful information to the client, who might be speciﬁcally
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL.XX, NO.X, MONTH, 2003
interested in certain areas in the video frame during each
time piece.
FSVC also provides greater ﬂexibility for multi-user and
heterogeneous network video communications. If the video
server needs to send video signals to diﬀerent users with
very diﬀerent bandwidth connections, then FSVC, with
only one-time encoding, supports the possibility to provide every user with the best quality video he/she can get
in terms of foveated quality measurement.
Finally, FSVC is also a good choice for interactive video
communications, where the users are involved in giving
feedback information to the other side of the communication system. The feedback information may be regions
or objects of interest and can be converted into knowledge
about the video sequence inside the FSVC encoder. Consequently, improved video quality can be achieved.