Monocular 3D Human Pose Estimation In The Wild
Using Improved CNN Supervision
Dushyant Mehta1, Helge Rhodin2, Dan Casas3, Pascal Fua2,
Oleksandr Sotnychenko1, Weipeng Xu1, and Christian Theobalt1
1MPI for Informatics, Germany
2EPFL, Switzerland
3Universidad Rey Juan Carlos, Spain
We propose a CNN-based approach for 3D human body
pose estimation from single RGB images that addresses the
issue of limited generalizability of models trained solely on
the starkly limited publicly available 3D pose data. Using only the existing 3D pose data and 2D pose data, we
show state-of-the-art performance on established benchmarks through transfer of learned features, while also generalizing to in-the-wild scenes. We further introduce a new
training set for human body pose estimation from monocular images of real humans that has the ground truth captured with a multi-camera marker-less motion capture system. It complements existing corpora with greater diversity in pose, human appearance, clothing, occlusion, and
viewpoints, and enables an increased scope of augmentation. We also contribute a new benchmark that covers outdoor and indoor scenes, and demonstrate that our 3D pose
dataset shows better in-the-wild performance than existing
annotated data, which is further improved in conjunction
with transfer learning from 2D pose data. All in all, we argue that the use of transfer learning of representations in
tandem with algorithmic and data contributions is crucial
for general 3D body pose estimation.
1. Introduction
We present an approach to estimate the 3D articulated
human body pose from a single image taken in an uncontrolled environment. Unlike marker-less 3D motion capture
methods that track articulated human poses from multi-view
video sequences, or use active
RGB-D cameras , our approach is designed to work
from a single low-cost RGB camera.
Data-driven approaches using Convolutional Neural Networks (CNNs) have shown impressive results for 3D pose
regression from monocular RGB, however, in-the-wild
This work was funded by the ERC Starting Grant project CapReal
(335545). Dan Casas was supported by a Marie Curie Individual Fellow
grant (707326), and Helge Rhodin by the Microsoft Research Swiss JRC.
We thank The Foundry for license support.
scenes and motions remain challenging. Aside from the
difﬁculty of the 3D pose estimation problem, it is further
stymied by the lack of suitably large and diverse annotated 3D pose corpora. For 2D joint detection it is feasible to obtain ground truth annotations on in-the-wild data
on a large scale through crowd sourcing , consequently leading to methods that generalize to in-the-wild
scenes . Some
3D pose estimation approaches take advantage of this generalizability of 2D pose estimation, and propose to lift the
2D keypoints to 3D .
This approach however is susceptible to errors from depth
ambiguity, and often requires computationally expensive iterative pose optimization. Recent advances in direct CNNbased 3D regression show promise, utilizing different prediction space formulations and incorporating additional constraints . However,
we show on a new in-the-wild benchmark that existing solutions have a low generalization to in-the-wild conditions.
They are far from the accuracy seen for 2D pose prediction
in terms of correctly located keypoints.
Existing 3D pose datasets use marker-based motion capture, MoCap, for 3D annotation , which restricts
recording to skin-tight clothing, or markerless systems in a
dome of hundreds of cameras , which enables diverse
clothing but requires an expensive studio setup. Synthetic
data can be generated by retargeting MoCap sequences to
3D avatars , however the results lack realism, and learning based methods pick up on the peculiarities of the rendering leading to poor generalization to real images.
Our contributions towards accurate in-the-wild pose estimation are twofold. First, in Section 4, we explore the use
of transfer learning to leverage the highly relevant mid- and
high-level features learned on the readily available in-thewild 2D pose datasets in conjunction with the existing annotated 3D pose datasets. Our experimentally validated mechanism of feature transfer shows better accuracy
and generalizability compared to na¨ıve weight initialization
from 2D pose estimation networks and domain adaptation
based approaches. With this we show previously unseen
levels of accuracy on established benchmarks, as well as
 
(1) Bounding Box Computation
(3) Global 3D Pose Computation
(2) 3D Pose Prediction
Figure 1. We infer 3D pose from single image in three stages: (1) extraction of the actor bounding box from 2D detections; (2) direct
CNN-based 3D pose regression; and (3) global root position computation in original footage by aligning 3D to 2D pose.
generalizability to in-the-wild scenes, with only the existing 3D pose datasets.
Second, in Section 5, we introduce the new MPI-INF-
3DHP dataset 1 real humans with ground truth 3D annotations from a state-of-the-art markerless motion capture
It complements existing datasets with everyday
clothing appearance, a large range of motions, interactions
with objects, and more varied camera viewpoints. The data
capture approach eases appearance augmentation to extend
the captured variability, complemented with improvements
to existing augmentation methods for enhanced foreground
texture variation. This gives a further signiﬁcant boost to
the accuracy and generalizability of the learned models.
The data-side supervision contributions are complemented by CNN architectural supervision contributions in
Section 3.2, which are orthogonal to in-the-wild performance improvements.
Furthermore, we introduce a new test set, including sequences outdoors with accurate annotation, on which we
demonstrate the generalization capability of the proposed
method and validate the value of our new dataset.
The components of our method are thoroughly evaluated on existing test datasets, demonstrating both state-ofthe-art results in controlled settings and, more importantly,
improvements over existing solutions for in-the-wild sequences thanks to the better generalization of the proposed
techniques.
2. Related Work
There has been much work on learning- and model-based
approaches for human body pose estimation from monocular images, with much of the recent progress coming
through CNN based approaches. We review the most relevant approaches, and discuss their relation with our work.
3D pose from 2D estimates. Deep CNN architectures have
dramatically improved 2D pose estimation , with
even real-time solutions .
Graphical models 
continue to ﬁnd use in
modeling multi-person relations
3D pose can be inferred from 2D pose through
geometric and statistical priors .
Optimization
1MPI-INF-3DHP dataset available at gvv.mpi-inf.mpg.de/
3dhp-dataset
of the projection of a 3D human model to the 2D predictions is computationally expensive and ambiguous, but
the ambiguity can be addressed through pose priors and
it further allows incorporation of various constraints such
as inter-penetration constraints , sparsity assumptions
 , joint limits , and temporal constraints
 . Simo-Serra et al. sample noisy 2D predictions to
ambiguous 3D shapes, which they disambiguate using kinematic constraints, and improve discriminative 2D detection
from likely 3D samples . Li et al. look up the nearest
neighbours in a learned joint embedding of human images
and 3D poses to estimate 3D pose from an image. We
choose to use the geometric relations between the predicted
2D and 3D skeleton pose to infer the global subject position.
Estimating 3D pose directly. Additional image information, e.g. on the front-back orientation of limbs, can be exploited by regressing 3D pose directly from the input image
 . Deep CNNs achieve state-of-the-art results
 . While CNNs dominate, regression forests have
also been used to derive 3D posebit descriptors efﬁciently
 . The input and output representations are important
too. To localize the person, the input image is commonly
cropped to the bounding box of the subject before 3D pose
estimation . Video input provides temporal cues, which
translate to increased accuracy . The downside of
conditioning on motion is the increased input dimensionality, and requires motion databases with sufﬁcient motion
variation, which are even harder to capture than pose data
sets. In controlled conditions, ﬁxed camera placement provides additional height cues . Since monocular reconstruction is inherently scale-ambiguous, 3D joint positions
relative to the pelvis, with normalized subject height are
widely used as the output. To explicitly encode dependencies between joints, Tekin et al. regressing to a highdimensional pose representation, learned by an auto encoder. Li et al. report that predicting positions relative
to the parent joint of the skeleton improves performance,
but we show that a pose-dependent combination of absolute
and relative positions leads to further improvements. Zhou
et al. regress joint angles of a skeleton from single images, using a kinematic model.
Addressing the scarcity and limited appearance variability of datasets. Learning-based methods require large
Figure 2. 3D pose, represented as a vector of 3D joint positions,
is expressed variously as 1) P: relative to the root (joint #15), 2)
O1 (blue): relative to ﬁrst order and, 3) O2 (orange): relative to
second order parents in the kinematic skeleton hierarchy.
annotated dataset corpora. 3D annotation is harder to
obtain than 2D pose annotation. Some approaches treat 3D
pose as a hidden variable, and use pose priors and projection to 2D to guide the training . Rogez et al. render
mosaics of in-the-wild human pose images using projected
mocap data . Chen et al. render textured rigged
human models, but still require domain adaptation to inthe-wild images for generalization. Other approaches use
the estimated 2D pose to look up a suitable 3D pose from
a dictionary , or use the ground truth 2D pose based
dictionary lookup to create 3D annotations for in-the-wild
2D pose data , but neither address the 2D to 3D ambiguity. Our new dataset complements the existing datasets,
through extensive appearance and pose variation, by using
marker-less annotation and provides an increased scope for
augmentation.
Transfer Learning is commonly used in computer
vision to leverage features and representations learned on
one task to offset data scarcity for a related task. Low and/or
mid-level CNN features can be shared also among unrelated
tasks . Pretraining on ImageNet is commonly
used for weight initialization in CNNs. We explore different ways of using the low and mid-level features
learned on in-the-wild 2D pose datasets for further improving the generalization of 3D pose prediction models.
3. CNN-based 3D Pose Estimation
We start by introducing the network architecture, utilized
input and output domains, and notation. While the particularities of our architecture are explained in Section 3.2, our
main contributions towards in-the-wild conditions are covered in sections 4 and 5.
Given an RGB image, we estimate the global 3D human
pose P [G] in the camera coordinate system. We estimate
the global positions of the joints of the skeleton depicted in
Figure 2, accounting for the camera viewpoint, which goes
beyond only estimating in a root-centered (pelvis) coordinate system, as is common in many previous works. Our
algorithm consists of three steps, as illustrated in Figure 1.
(1) the subject is localized in the frame with a 2D bounding
box BB, computed from 2D joint heatmaps H, obtained
with a CNN we call 2DPoseNet; (2) the root-centered 3D
pose P is regressed from the BB-cropped input with a second CNN termed 3DPoseNet; and (3) global 3D pose coordinates P [G] and perspective correction are computed in
closed form using 3D pose P, 2D joint locations K and
known camera calibration.
3.1. Bounding Box and 2D Pose Computation
We use our 2DPoseNet to produce 2D joint location
heatmaps H. The heat map maxima provide the most likely
2D joint locations K which can also act as a stand-in person bounding-box BB detector. See Figure 1. The 2D joint
locations K are further used for global pose estimation in
Section 3.3. In case of an alternative BB detector, K comes
from 3DPoseNet. See 2D Auxiliary Task in Figure 3.
Our 2DPoseNet is fully convolutional and is trained on
MPII and LSP datasets. We use a CNN structure based on Resnet-101 , up to the ﬁlter banks at level
4. Striding is removed at level 5, and features in the res5a
block are halved and identity skip connections removed
from res5b and res5c.
For speciﬁcs of the network architecture and the training scheme, refer to the supplementary
3.2. 3D Pose Regression
The 3D pose CNN, termed 3DPoseNet, is used to regress
root-centered 3D pose P from a cropped RGB image, and
makes use of new CNN supervision techniques. Figure 3
depicts the main components of the method, detailed in the
following sections.
Network The base network derives from Resnet-101 as
well, and is identical to 2DPoseNet up to res5a. We remove the remaining layers from level 5. A 3D prediction
stub S comprised of a convolution layer (k5×5, s2) with 128
features and a ﬁnal fully-connected layer that outputs the
3D joint locations is added on top. Additionally we predict 2D heatmaps H as an auxiliary task after res5a and,
use intermediate supervision with pose P at res3b3 and
res4b22. Refer to the supplementary for speciﬁcs of the
loss weights for the intermediate and auxiliary tasks.
Multi-level Corrective Skip Connections
We additionally use a skip connection scheme as a trainingtime regularization architecture. We add skip connections
from res3b3 and res4b20 to the main prediction Pdeep,
leading to Psum. In contrast to vanilla skip-connections ,
we compare both Psum and Pdeep to the ground truth, and
remove the skip connections after training. We show the
improvements due to this approach Section 6.
3D Dataset
Multimodal Prediction
Corrective Skip Connections
3D Pose Fusion
2D Auxiliary Task
Transfer Learning
Figure 3. 3D pose Training overview. The main components are 1) regularization through corrective skip connections, and 2D pose
prediction as auxiliary task, 2) Multi-modal 3D pose prediction and fusion, 3) a new marker-less 3D pose database with appearance
augmentation, and 4) Transfer learning from features learned for 2D pose estimation.
Multi-modal Pose Fusion
Formulating joint location prediction relative to a single local or global location is not always optimal. Existing literature has observed that predicting joint locations relative
to their direct kinematic parents (Order 1 parents) improves
performance. Our experiments reveal that to not universally
hold true. We ﬁnd that depending on the pose and the visibility of the joints in the input image, the optimal relative
joint for each joint’s location prediction differs. Hence, we
use joint locations P relative to the root, O1 relative to Order 1 parents and O2 relative to Order 2 parents along the
kinematic tree as the three modes of prediction, see Figure
2, and fuse them with fully-connected layers.
For the joint set we consider, the kinematic relationships
chosen sufﬁce, as it puts at least one reference joint for each
joint in the relatively low entropy torso . We use three
identical 3D prediction stubs attached to res5a for predicting the pose as P, O1 and O2, and for each we use
corrective skip connections. These predictions are fed into
a smaller network with three fully connected layers, to implicitly determine and fuse the better constraints per joint
into the ﬁnal prediction Pfused. The network has the ﬂexibility to emphasize different combinations of constraints
depending on the pose. This can be viewed as intermediate
supervision with auxiliary tasks, yet the separate streams for
predicting each mode individually are key to its efﬁcacy.
3.3. Global Pose Computation
The bounding box cropping normalizes subject size and
position, which frees 3D pose regression from having to localize the person in scale and image space, but loses global
pose information. We propose a lightweight and efﬁcient
way to reconstruct the global 3D pose P [G] = (R|T) Pfused
from pelvis-centered pose Pfused, camera intrinsics, and K.
Perspective correction.
The bounding box cropping can
be interpreted as using a virtual camera, rotated towards
the crop center and its ﬁeld of view covering the crop area.
Since the 3DPoseNet only ‘sees’ the cropped input, its predictions live in this rotated view, leading to a consistent orientation error in Pfused. To compensate, we compute rotation R that rotates the virtual camera to the original view.
3D localization.
We seek the global translation T that
aligns Pfused and K under perspective projection. We assume weak perspective projection, Π, and solve the linear
least squares equation P
i∥Ki −Π(T + P i
fused)∥2, where i
indexes the joints. This assumption yields global position
[xy] −¯P[xy]∥2
i∥Ki −¯K∥2
in terms of distances to the 3D mean ¯P and 2D mean ¯K
over all joints. P[xy] is the x, y part of Pfused and single
subscripts indicate the respective elements. Please see the
supplemental document for the derivation and evaluation.
Our solution can be considered a generalization of procrustes analysis for projective alignment. Note that this is
different to perspective-n-point 6DOF rigid pose estimation
 , structure-from-motion, and from the convex approach
of Zhou et al. , which require iterative optimization.
4. Transfer Learning
We use the features learned with Resnet-101 from ImageNet to initialize both 2DPoseNet and 3DPoseNet,
Table 1. Evaluation of the mechanisms of transfer learning from
2DPoseNet to 3DPoseNet that were explored in the context of the
Base network. The table compares the effect of various learning rate multiplier combinations for different parts of the network.
For network details, refer to Section 3.2. Human3.6m, Subjects
1,5,6,7,8 used for training, and every 64th frame of 9,11 used for
testing. * = weights randomly initialized
Learning Rate Multiplier
up to res4b22
Total MPJPE (mm)
as common for many vision tasks.
While this affords a
faster convergence while training, there remains room for
improved generalization beyond the gains from potential
supervision and dataset contributions. Due to the similarity of the tasks, features learned for 2D pose estimation on
in-the-wild MPII and LSP training sets can be transferred
to 3D pose estimation. We explore different variants of the,
thus far, un-utilized method of improving generalization by
transferring weights from 2DPoseNet to 3DPoseNet.
A na¨ıve initialization of the weights of 3DPoseNet is inadequate, and there is a tradeoff to be made between the
preservation of transferred features and learning new pertinent features. We achieve this through a learning rate discrepancy between the transferred layers and the new layers. We experimentally determine the mechanism for this
transfer of features through validation. Table 1 shows the
evaluated mechanisms for transfer from 2DPoseNet. Based
on the experiments, we choose to scale down the learning rate of the layers till res4b22 by a factor of 1000.
Through similar experiments for the transfer of ImageNet
features, we choose to scale down the learning rate of layers till res4b22 by 10.
The same approach can be applied to other network architectures, and our experiments on the learning rate discrepancy serve as a sound starting point for the determination of the transfer learning mechanism. Unlike jointly
training with annotated 2D and 3D pose datasets, this approach has the advantage of not requiring the 2D annotations to be consistent between the two datasets, and one can
simply use off-the-shelf trained 2D pose networks. In Section 6 we show that our approach outperforms domain adaptation, see Table 5, ﬁrst row. Additionally, Table 1 validates
that the common ﬁne-tuning of the fully-connected layers
(third row) and ﬁne-tuning of the complete network (ﬁrst
row) is much less effective then the proposed scheme.
5. MPI-INF-3DHP: Human Pose Dataset
We propose a new dataset captured in a multi-camera
studio with ground truth from commercial marker-less mo-
Figure 4. MPI-INF-3DHP dataset.
We capture actors using a
markerless multi-camera in a green screen studio (left), compute
masks for different regions (center left) and augment the captured
footage by compositing different textures to the background, chair,
upper and lower body areas, independently (center right and right).
tion capture . No special suits and markers are needed,
allowing the capture of motions wearing everyday apparel,
including loose clothing. In contrast to existing datasets, we
record in green screen studio to allow automatic segmentation and augmentation. We recorded 8 actors (4m+4f),
performing 8 activity sets each, ranging from walking and
sitting to complex exercise poses and dynamic actions, covering more pose classes than Human3.6m. Each activity set
spans roughtly one minute. Each actor features 2 sets of
clothing split across the activity sets. One clothing set is
casual everyday apparel, and the other is plain-colored to
allow augmentation.
We cover a wide range of viewpoints, with ﬁve cameras
mounted at chest height with a roughly 15◦elevation variation similar to the camera orientation jitter in other datasets
 . Another ﬁve cameras are mounted higher and angled
down 45◦, three more have a top down view, and one camera is at knee height angled up. Overall, from all 14 cameras, we capture >1.3M frames, 500k of which are from the
ﬁve chest high cameras. We make available both true 3D
annotations, and a skeleton compatible with the “universal”
skeleton of Human3.6m .
Dataset Augmentation.
Although our dataset has more
clothing variation than other datasets, the appearance variation is still not comparable to in-the-wild images. There
have been several approaches proposed to enhance appearance variation. Pishchulin et al. warp human size in images
with a parametric body model . Images can be used
to augment background of recorded footage .
Rhodin et al. recolor plain-color shirts while keeping
the shading details, using intrinsic image decomposition to
separate reﬂectance and shading .
We provide chroma-key masks for the background, a
chair/sofa in the scene, as well as upper and lower body segmentation for the plain-colored clothing sets. This provides
an increased scope for foreground and background augmentation, in contrast to the marker-less recordings of Joo et al.
 . For background augmentation, we use images sampled from the internet. For foreground augmentation, we
use a simpliﬁed intrinsic decomposition. Since for plain
colored clothing the intensity variation is solely due to shading, we use the average pixel intensity as a surrogate for the
shading component. We composite cloth like textures with
Figure 5. Representative poses (centroids) of the 20 K-means pose
clusters of the Human3.6m test set (subjects S9,S11), visually
grouped into three broad pose classes, which are used also to perform per-class evaluation. Upright poses are dominant, with complex poses such as sitting and crouching only accounting for 25%
and 8% of the poses respectively. Our multimodal fusion scheme
signiﬁcantly improves the latter two, yielding a 3.5mm improvement for Sit and 5.5mm for Crouch class.
the pixel intensity of the upper body, lower body and chair
marks independently, for a photo-realistic result. Figure 4
shows example captured and augmented frames.
We found the existing test sets for (monocular) 3D pose estimation to be restricted to limited settings due to the difﬁculty of obtaining ground truth labels in general scenes. HumanEva and Human3.6m
 are recorded indoors and test on similar looking
scenes as the training set, the Human3D+ test set was
recorded with sensor suits that inﬂuence appearance and
lacks global alignment, and the MARCoNI set is markerless through manual annotation, but shows mostly walking motions and multiple actors, which are not supported
by most monocular algorithms. We create a new test set
with ground truth annotations coming from a multi-view
markerless motion capture system. It complements existing test sets with more diverse motions (standing/walking,
sitting/reclining, exercise, sports (dynamic poses), on the
ﬂoor, dancing/miscellaneous), camera view-point variation,
larger clothing variation (e.g. dress), and outdoor recordings
from Robertini et al. in unconstrained environments.
This makes the test set suitable for testing the generalization of various methods. See Figure 6 for a representative
sample. We use the “universal” skeleton for evaluation.
Alternate Metric. In addition to the Mean Per Joint Position Error (MPJPE) widely used in 3D pose estimation, we
concur with and suggest a 3D extension of the Percentage of Correct Keypoints (PCK) metric used for
2D Pose evaluation, as well as the Area Under the Curve
(AUC) computed for a range of PCK thresholds. These
metrics are more expressive and robust than MPJPE, revealing individual joint mispredictions more strongly. We pick a
threshold of 150mm, corresponding to roughly half of head
size, similar what is used in MPII 2D Pose dataset. We
propose evaluating on the common set of joints across 2D
and 3D approaches (joints 1-14 in Figure 2), to ensure evaluation compatibility with existing approaches. Joints are
grouped by bilateral symmetry (ankles, wrists, shoulders,
etc), and can be evaluated by scene setting or activity class.
Figure 6. Representative frames from MPI-INF-3DHP test set. We
cover a variety of subjects with a diverse set of clothing and poses
in 3 different settings: studio with green screen (right); studio
without green screen (left); and outdoors (center).
Figure 7. Qualitative evaluation on representative frames of the
LSP test set. We succeed in challenging cases (left), with only few
failure cases (right). The Dance1 sequence of the PanopticDataset
 , is also well reconstructed (bottom).
6. Experiments and Evaluation
We evaluate the contributions proposed in the previous
sections using the standard datasets Human3.6m and HumanEva, as well as our new MPI-INF-3DHP test set. Additionally, we qualitatively observe the performance on LSP
 and the CMU Panoptic datasets, demonstrating robustness to general scenes. Refer to Figure 7. Also refer to
the supplementary video for global 3D pose results.
We evaluate the impact of training 3DPoseNet on Human3.6m, and unaugmented and augmented variants of
MPI-INF-3DHP, both with and without transfer learning
from 2DPoseNet.
We only use Human3.6m compatible
camera views from MPI-INF-3DHP for training. Further
details are in the supplemental document.
6.1. Impact of Supervision Methods
Multi-level corrective skip connections.
In Table 2 we
compare a baseline method without any skip connections,
a network with vanilla skip connections, and our proposed
Table 2. Activity-wise results (MPJPE in mm) on Human3.6m . Adding our model components one-by-one on top of the Base network
shows successive improvement of the total accuracy. Signiﬁcant relative improvements greater than 5mm are underlined. Models are
trained on Human3.6m, with network weights initialized from ImageNet, unless speciﬁed otherwise. The version marked with MPI-INF-
3DHP is trained with Human3.6m and MPI-INF-3DHP. Evaluation with all 17 joints, on every 64th frame, without rescaling to a person
speciﬁc skeleton.
Direct Discuss Eating Greet Phone Posing Purch. Sitting
Smoke Take
Wait Walk Walk Walk
Base + Regular Skip
113.34 112.26
97.40 110.50 108.63 112.09 105.67 125.97 173.41 109.34 120.87 107.75 97.30 126.05 117.45 115.29
98.98 100.14
86.07 101.83 101.34 96.74
94.89 125.28 158.31 100.21 112.49 99.57 83.39 109.61
95.79 104.32
+ Corr. Skip
95.67 123.54 160.98 97.13 107.56 93.86 76.99 110.93
88.73 101.09
95.60 94.48
93.15 119.94 154.61 95.94 106.09 94.13 77.25 108.82
+ Transfer 2DPoseNet
96.19 122.92 70.82
68.45 54.41 82.03
59.79 74.14
+ MPI-INF-3DHP
99.98 117.53 69.44
67.96 55.24 76.50
61.40 72.88
Table 3. Evaluation by scene-setting of our design choices on MPI-
INF-3DHP test set with weight transfer from ImageNet. Training
on our markerless dataset improves accuracy signiﬁcantly, in particular with the proposed augmentation strategy. Fusion yields an
additional gain. GS indicates sequences with green screen.
3D dataset
Network architecture
Studio Outdoor
3DPCK 3DPCK 3DPCK 3DPCK AUC
Human3.6m Base + Corr. Skip
Base + Corr. Skip + Fusion
Ours Unaug. Base + Corr. Skip
Base + Corr. Skip + Fusion
Ours Aug. Base + Corr. Skip
Base + Corr. Skip + Fusion
corrective skip regularization on Human3.6m test set.
observe that networks using vanilla skip connections perform markedly worse than the baseline, while corrective
skip connections yield more than 5mm improvement for 7
classes of activities (marked as underlined).
We veriﬁed
that the effect is not due to a higher effective learning rate
seen by the core network due to the additional loss term.
Multimodal prediction and fusion.
The multi-modal
fusion scheme yields noticeable improvement across all
datasets tested in tables 2 and 3. Since upright poses dominate in pose datasets, and the activity classes are often diluted signiﬁcantly by upright poses, the true extent of improvement by the multi-modal fusion scheme is masked. To
show that the fusion scheme indeed improves challenging
pose classes, we cluster the Human3.6m test set by pose as
shown in Figure 5, which visualizes the centroid of each
cluster. Then we group the clusters visually into three pose
classes, namely Stand/Walk, Sit and Crouch, going by the
cluster representatives. For the Stand/Walk class, adding
fusion has minimal effect, going from 88.4mm to 88.8mm.
However, for Sit class fusion leads to a 3.5mm improvement, from 118.9mm to 115.4mm. Similarly, Crouch class
has the highest improvement of 5.5mm, going from 156mm
to 150.5mm. The improvement is not simply due to additional training, and is less pronounced if predicting P, O1
and O2 with a common stub, even with more features in the
Table 4. Comparison of results on Human3.6m with the state
of the art. Human3.6m, Subjects 1,5,6,7,8 used for training, and
9,11 used for testing. S = Scaled to test subject speciﬁc skeleton,
computed from T-pose.
T= Uses Temporal Information, J14/J17 =
Joint set evaluated, A = Uses Best Alignment To GT per frame, Act
= Activitywise Training, 1/10/64 = Test Set Frame Sampling
Total MPJPE (mm)
Deep Kinematic Pose J17,B
Sparse. Deep. T,J17,B,10,Act
Motion Comp. Seq. T,J17,B
LinKDE J17,B,Act
Du et al. T,J17,B
Rogez et al. (J13),B,64
SMPLify J14,B,A,(First cam.)
3D=2D+Matching J17,B
Distance Matrix J17,B
Volumetric Coarse-Fine J17,B,S*
LCR-Net J17,B
Full model (w/o MPI-INF-3DHP) J17,B
Full model (w/o MPI-INF-3DHP) J17,B,S
Full model (w/o MPI-INF-3DHP) J14,B,A
fully-connected layer. Details in the supplementary.
6.2. Transfer Learning
Our approach of transferring representations from
2DPoseNet to 3DPoseNet yields 64.7% 3DPCK on MPI-
INF-3DHP test-set when trained with only Human3.6m
data, compared to 63.7% 3DPCK of the model trained on
our augmented training set without transfer learning. It also
shows state of the art performance on Human3.6m test set
with an error of ≈74mm, demonstrating the dual advantage
of the approach in improving both the accuracy of pose estimation and generalizability to in-the-wild scenes. Combining our dataset and transfer learning leads to the best results
at ≈72.5% 3DPCK. See Table 5.
In contrast to existing approaches countering data
scarcity, transfer learning does not require complex dataset
synthesis, yet exceeds the performance of Chen et al. 
(with synthetic data and domain adaptation, 28.8% 3DPCK,
after procrustes alignment) and our base model trained with
Table 5. Evaluation on MPI-INF-3DHP test set with weight transfer from 2DPoseNet, by scene setting. Training our full model on
our dataset paired with Human3.6m yields best accuracy over all.
GS indicates sequences with green screen background.
3D dataset
3DPCK 3DPCK 3DPCK
Human3.6m Domain adapt.
Ours (full model)
Ours (full model)
Ours Unaug. Ours (full model)
Ours, w/o persp. corr.
Ours, w/o GT BB
Human3.6m Ours (full model)
the synthetic data of Rogez et al. (21.7% 3DPCK). Our
approach also performs better than domain adaptation 
to in-the-wild data (Table 5). Details in the supplementary.
6.3. Beneﬁt of MPI-INF-3DHP
Evaluating on MPI-INF-3DHP test-set, without any
transfer learning from 2DPoseNet, we see in Table 3 that
our dataset, even without augmentation, leads to a ≈9%
3DPCK improvement on outdoor scenes over Human3.6m.
However, our augmentation strategy is crucial for improved
generalization, as seen from the gains in 3DPCK across
scene settings in Table 3, giving 57.3% 3DPCK overall.
Even when combined with transfer learning, we see in
Table 5 that our dataset (both augmented and unagumented)
consistently performs better than Human3.6m.
performance of 76.5% 3DPCK on MPI-INF-3DHP test set
and of 72.88mm on Human3.6m is obtained when the two
datasets are combined with transfer learning.
6.4. Other Components
Bounding box computation. On MPI-INF-3DHP test set,
we additionally evaluate our best performing network using bounding boxes computed from 2DPoseNet. As shown
in Table 5, the performance drops to 74.4% 3DPCK from
76.5% 3DPCK due to the additional difﬁculty.
Perspective correction.
Table 5 shows that perspective
correction also has a signiﬁcant impact, without which, the
performance drops to 73% 3DPCK from 76.5%.
6.5. Quantitative Comparison
Human3.6m. Table 4 shows comparison of our method
with existing methods, all trained on Human3.6m.
Altogether, with our supervision contributions and transfer
learning, we are the state of the art (74.11mm, without scaling), while also generalizing to in-the-wild scenes. Note
that the Volumetric coarse to ﬁne approach requires estimates of the bone lengths to convert their predictions from
pixels to 3D space. Complementing Human3.6m with our
augmented MPI-INF-3DHP dataset further reduces the error to 72mm.
HumanEva. The improvements on Human3.6m are con-
ﬁrmed with a 30.8 and 33.5 MPJPE score on the S1 Box
and Walk sequences of HumanEva, after alignment. See
supplemental document.
MPI-INF-3DHP. We also evaluated some of the existing
methods on our test set. Deep Kinematic Pose , attains
13.8% 3DPCK overall. Our full model attains signiﬁcantly
higher accuracy: without transfer learning and trained on
Human3.6m obtains 26% 3DPCK, and 64.7% 3DPCK with
transfer learning.
The large discrepancy in performance
between Human3.6m and our new in-the-wild test set highlights the importance of a new benchmark to test generalization to natural images and motions.
7. Discussion
Despite the demonstrated competitive results,
method and others have limitations. Most training sets, also
 , have a strong bias towards chest height cameras. Thus,
estimating 3D pose from starkly different camera views is
still a challenge. Our new dataset provides diverse viewpoints, which can support development towards viewpoint
invariance in future methods. Similar to related approaches,
our per-frame estimation exhibits temporal jitter on video
sequences. In future, we will investigate integration with
model-based temporal tracking to further increase accuracy
and temporal smoothness. At less than 250 ms per frame,
our approach is much faster than model based methods
which work ofﬂine in the order of minutes. There still remains scope for improvement towards real time, through
smaller input resolution and shallower networks.
We also show that joining forces with transfer learning,
in conjunction with algorithmic and data contributions, will
aide progress in 3D pose estimation in many different directions, such as overall accuracy and generalizability.
8. Conclusion
We have presented a fully feedforward CNN-based approach for monocular 3D human pose estimation that attains state-of-the-art on established benchmarks 
and quantitatively outperforms existing methods on the introduced in-the-wild benchmark. State of the art is attained
with enhanced CNN supervision techniques and improved
parent relationships in the kinematic chain. Transfer learning from in-the-wild 2D pose data in tandem with a new
dataset that includes a larger variety of real and augmented
human appearances, activities and camera views, leads to
the signiﬁcantly improved generalization to in-the-wild images. Our method is also the ﬁrst to efﬁciently extract global
3D position in non-cropped images, without time consuming iterative optimization.
Supplemental Document: Monocular 3D
Human Pose Estimation
In The Wild Using Improved CNN
Supervision
This document accompanies the main paper,
and the supplemental video.
1. Further Discussion of Design Choices Regarding Multi-modal Fusion
To demonstrate that the improvement seen due to the
fusion scheme is not simply a result of ﬁne tuning, we
compare the result of fusion with components successively
Using P, O1 and O2, we get an MPJPE of
74.49mm on Human3.6m. On removing O2, the error increases to 74.77mm, and on removing both O1 and O2, the
error increases to 75.27mm. The comparison here is without any multi-level corrective skip training.
For P, O1 and O2 to have different modes of mispredictions, the underlying feature set that they are computed
from has to be as different as possible, because each is related to the other with a linear transform. We achieve some
degree of decorrelation between the three by using 3 different prediction stubs, one each for P, O1 and O2 with a convolutional layer (k5×5, s2) with 128 features followed by a
fully-connected layer. If we replace these three stubs with a
single stub with the convolutional layer having 256 features
followed by a fully-connected layer, the resulting MPJPE is
75.30mm after fusion, in contrast to an MPJPE of 74.49mm
from fusing the result of 3 prediction stubs. Both of these
are without corrective-skip connections.
2. Further Discussion of Multi-level Corrective
Since our multi-level corrective skip scheme adds an additional loss at the last stage (Xdeep, where X is P/O1/O2)
of the network, it increases the effective learning rate seen
by the core network. To verify that the improvements seen
due to the proposed scheme are not caused by this difference in the effective learning rate, we trained a version
of the Base network with loss weights as the sum of the
loss weights for Xdeep and Xsum speciﬁed in Table 4. We
ﬁnd that this network performs worse than the Base network (107.14mm vs 104.32mm MPJPE on Human3.6m),
and does not approach the accuracy attained with multilevel corrective skip scheme (101.09mm).
3. Global Pose Computation
3.1. 3D localization
In this section we describe a simple, yet very efﬁcient,
method to compute the global 3D location T of a noisy
3D point set P with unknown global position. We assume
known scaling and orientation parameters, obtained from its
2D projection estimate K in a camera with known intrinsics parameters (focal length f). We further assume that the
point cloud spread in depth direction is negligible compared
to its distance z0 to the camera and approximate perspective projection of an object near position (x0, y0, z0)⊤with
weak perspective projection (linearizing the pinhole projection model at z0):
, with Π =
Estimates K and P are assumed to be noisy due to
estimation errors.
We ﬁnd the optimal global position T in the least squares sense, by minimizing T
arg min(x,y,z) E(x, y, z), with
 (x, y, z)⊤+ P i
(x, y)⊤+ P i
where P i and Ki denote the ith joint position in 3D and
2D, respectively, and P i
[xy] the xy component of P i. It has
partial derivative
where P[x] denotes the x part of P, and ¯P the mean of P
over all joints. Solving ∂E
∂x = 0 gives the unique closedform solutions x =
f −¯P[x] and equivalently y =
f −¯P[y], for ∂E
Substitution of x and y in E and differentiating with respect to z yields
i(Ki −¯K)⊤(P i
[xy] −¯P[xy])
[xy] −¯P[xy]∥2
Finally, solving ∂E
∂z = 0 gives the depth estimate
[xy] −¯P[xy]∥2
i(Ki −¯K)⊤(P i
[xy] −¯P[xy])
[xy] −¯P[xy]∥2
i∥Ki −¯K∥2
where (Ki −¯K)(P i −¯P) = ∥Ki −¯K∥∥P i −¯P∥cos(θ) is
approximated for θ ≈0. This is a valid assumption in our
case, since the rotation of 3D and 2D pose is assumed to be
Figure 1. The predicted pose (red) is inaccurate for positions away
from the camera center (left), compared against the ground truth
(white). Perspective correction (colored) corrects the orientation
(center) and is closer to the ground truth (right). Here tested on
the walking sequence of HumanEva S1.
Figure 2. Sketch of the input image cropping and resulting change
of ﬁeld of view. The corresponding rotation R of the view direction is sketched in 2D on the right.
Evaluation on HumanEva:
In addition to evaluating
centered pose P, we evaluate the global 3D pose prediction
P [G] on the widely used HumanEva motion capture dataset
— Box and Walk sequences of Subject 1 from the validation set. Note that we do not use any data from HumanEva
for training. We signiﬁcantly improve the state of the art
for the Box sequence (82.1mm vs 58.6mm). Results on
the Walk sequence are of higher accuracy than Bogo et al.
 , but lower than the accuracy of Bo et al. and Yasin
et al. , who, however train on HumanEva or use an
example database dominated by walking motions . Our
skeletal structure does not match that of HumanEva, e.g. the
head prediction has a consistent frontal offset and the hip
is too wide. To compensate, we compute a linear map of
dimension 14x14 (number of joints) that maps our joint positions as a linear combination to the HumanEva structure.
The same mapping is applied at every frame, but is computed only once, jointly on the Box and Walk sequence, to
limit the correction to global inconsistencies of the skeleton
structure. This ﬁne-tuned result is marked by ∼in Table 1.
3.2. Perspective correction
Our 3DPoseNet predicts pose P in the coordinate system of the bounding box crop, which leads to inaccuracies
as shown in Figure 1. The cropped image appears if as it
was taken from a virtual camera with the same origin as the
original camera, but with view direction to the crop center, see Figure 2. To map the reconstruction from the virtual camera coordinates to the original camera, we rotate P
by the rotation R between the virtual and original camera.
Since the existing training sets provide chest-height camera placements with the same viewpoint, the bias in vertical direction is already learned by the network. We apply
perspective correction only in horizontal direction, where
a change in cropping and yaw rotation of the person cannot be distinguished by the network. R is then the rotation
around the camera up direction by the angle between the
original and the virtual view direction, see Figure 2. On our
MPI-INF-3DHP test set perspective correction improves the
PCK by 3 percent points. On HumanEva the improvement
is up to 3 mm MPJPE, see Table 1. The correction is most
pronounced for cameras with a large ﬁeld of view, e.g. Go-
Pro and similar outdoor cameras, and when the subject is
located at the border of the view. Using the vector from the
camera origin to the centroid of 2D keypoints K as the virtual view direction was most accurate in our experiments.
However, the crop center can be used instead. Opposed
to the Perspective-n-Point algorithm applied by Zhou et al.
 , any regression method that works on cropped images
could immediately proﬁt from this perspective correction,
without computing 2D keypoint detections.
4. CNN Architecture and Training Speciﬁcs
4.1. 2DPoseNet
Architecture: The architecture derives from Resnet-101,
using the same structure as is until level 4. Since we are interested in predicting heatmaps, we remove striding at level
5. Additionally, the number of features in the res5a module are halved, identity skip connections are removed from
res5b and res5c, and the number of features gradually tapered to 15 (heatmaps for 14 joints + root). As shown in
Table 2, for 2DPoseNet, our results on MPII and LSP test
sets approach that of the state of the art.
Intermediate Supervision:
Additionally, we employ intermediate supervision at res4b20 and res5a, treating
the ﬁrst 15 feature maps of the layers as the intermediate joint-location heatmaps.
Further, we use a Multilevel Corrective Skip scheme, with skip connections coming from res3b3 and res4b22 through prediction stubs
comprised of a 1 × 1 convolution with 20 feature maps followed by a 3 × 3 convolution with 15 outputs.
Training: For training, we use the Caffe framework,
with the AdaDelta solver with a momentum of 0.9 and
weight decay rate of 0.005. We employ a batch size of 7,
and use Euclidean Loss everywhere. For the Learning Rate
and Loss Weight taper schema, refer to Table 3.
Table 1. Quantitative evaluation on HumanEva-I , with different alignment strategies used in the literature. For reference, we also
show multi-view existing results. Our models use no data from HumanEva for training, while the other methods listed train/ﬁnetune on
HumanEva-I. * = Does not use GT Bounding Box information. † = Translation alignment only. ∼= trained or ﬁne-tuned on HumanEva-I.
(alignS,T)
(alignR,S,T)
(alignS,T)
(alignR,S,T)
Our full model*
w/o Persp. correct.*
Our full model*∼
Zhou et al. ∼
Bo et al. *∼
Yasin et al. *∼
Bogo et al. ∼
Akhter et al. 
Ramakris. et al. 
Mulit-view
Amin et al. 
Rhodin et al. 
Elhayek et al. 
Table 2. Results of our 2DPoseNet on MPII Single Person Pose
 dataset and LSP 2D Pose datasets. * = Trained/Finetuned
only on the corresponding training set
PCKh0.5 AUC PCK0.2 AUC
Our 2DPoseNet
w Person Locali.
w/o Person Locali.
Stacked Hourgl. 
Bulat et al. 
Wei et al. 
DeeperCut 
Gkioxary et al 
Lifshitz et al. 
Belagiannis et al. 
DeepCut 
Hu&Ramanan 
Carreira et al. 
4.2. 3DPoseNet
Architecture: The core network is identical to 2DPoseNet
up to res5a. A 3D Prediction stub is attached on top, comprised of a 5×5 convolution layer with a stride of 2 and 128
features, followed by a fully-connected layer.
Multi-level Corrective Skip:
We attach 3D prediction
stubs to res3b3 and res4b20, similar to the ﬁnal prediction stub, but with 96 convolutional features instead of 128.
The resulting predictions are added to Pdeep to get Psum. We
add a loss term to Pdeep in addition to the loss term at Psum.
Table 3. Loss weight and learning rate, LR, taper scheme used for
2DPoseNet. 2DPoseNet also employs Multi-level Corrective Skip
connections, and the heatmap Hsum is the sum of Hdeep and the skip
connections. Heatmaps H4b20 and H5a are used for intermediate
supervision.
Base # Iter Loss Weights (w × L(Hxx))
Hsum Hdeep
0.0001 40k
0.001 0.0001 0.0001
0.0008 60k
1.0 0.0001 0.0001 0.0001
0.0001 40k
1.0 0.0001 0.0001 0.0001
1.0 0.0001 0.0001 0.0001
Multi-modal Fusion: We add prediction stubs for O1 and
O2, similar to those for P. Note that the predictions for
P, O1 and O2 are done with distinct stubs, and this slight
decorrelation of predictions is important. These predictions
are at a later ﬁnetuning step fed into three fully-connected
layers, with 2k, 1k and 51 nodes respectively.
Intermediate Supervision: We use intermediate supervision at 4b5 and res4b20, using prediction stubs comprised of 7 × 7 convolution with a stride of 3 and 128
features, followed by a fully-connected layer predicting P,
O1 and O2 as a single vector.
Additionally, we predict
joint location heatmaps and part-label maps using a 1 × 1
Table 4. Loss weight and LR taper scheme used for 3DPoseNet.
There is a difference in the number of iterations used when training
with Human3.6m or MPI-INF-3DHP alone, v.s. when training
with the two in conjunction. Part Labels PL are used only when
training with H3.6m solely. Multi-level skip connections add up
with Xdeep to yield Xsum, where X is P or O1 O2.
H3.6m/Our H3.6m+Our
Loss Weights (w × L(Abb))
X = P/O1/O2
X4b5 X4b20 Xdeep Xsum
0.05 0.025
0.01 0.005
0.01 0.005
0.005 0.001
0.005 0.001
Table 5. Loss weight and LR taper scheme used for ﬁne tuning
3DPoseNet for Multi-modal Fusion scheme.
H3.6m/Our H3.6m+Our
Batch = 6 Loss Weights (w × L(Abb))
convolution layer after res5a as an auxiliary task. We
don’t use the part-label maps when training with MPI-INF-
3DHP dataset.
For training, the solver settings are similar to
2DPoseNet, and we use Euclidean Loss everywhere. For
transfer learning, we scale down the learning rate of the
transferred layers by a factor determined by validation.
For ﬁne-tuning in the multi-modal fusion case, we similarly downscale the learning rate of the trained network by
10,000 with respect to the three new fully-connected layers. For the learning rate and loss weight taper schema for
both the main training and multi-modal fusion ﬁne-tuning
stages, refer to Tables 4 and 5. We use different training
durations when using Human3.6m or MPI-INF-3DHP in
isolation, versus when using both in conjunction. This is
reﬂected in the aforementioned tables.
3D Pose Training Data
In the various experiments on 3DPoseNet, for the datasets
we consider, we select ≈37.5k frames for each, yielding
≈75k samples after scale augmentation at 2 scales (0.7 and
Human3.6m: We use the H80k subset of Human3.6m,
and train with the “universal” skeleton, using subjects
S1,5,6,7,8 for training and S9,11 for testing. The predicted
skeleton is not scaled to the test subject skeletons at test
MPI-INF-3DHP: For our dataset, to maintain compatibility of view with Human3.6m and other datasets, we only
pick the 5 chest high cameras for all 8 subjects, sampling
frames such that at least one joint has moved by more than
200mm between selected frames. A random subset of these
frames is used for training, to match the number of selected
Human3.6m frames.
MPI-INF-3DHP Augmented:
The augmented version
uses the same frames as the unaugmented MPI-INF-
3DHP above, keeping ≈25% frames unaugmented, ≈40%
with only BG and Chair augmentation, and the rest with full
augmentation.
Domain Adaptation To In The Wild 2D Pose
We use a domain adaptation stub comprised of conv3×3,256,
conv3×3,128, fc64 and fc1 layers, and cross entropy domain
classiﬁcation loss. It uses Ganin et al.’s gradient inversion approach. The domain adaptation stub is attached after
res4b22 in the network. We found that directly starting out
with λ = −1 performs better than gradually increasing the
magnitude of λ with increasing iterations. We train on the
Human3.6m training set, with 2D heatmap and part label
prediction as auxiliary tasks. Images from MPII and
LSP training sets are used without annotations for
learning better generalizable features. The generalizability
is improved, as evidenced by the 41.4 3DPCK on MPI-INF-
3DHP test set, but does not match up with the 64.7 3DPCK
attained using transfer learning. Detailed results in main
5. MPI-INF-3DHP Dataset
We cover a wide range of poses in our training and test
sets, roughly grouped into various activity classes. A detailed description of the dataset is available in Section 4 of
the main paper. In addition, Figure 3 samples the various
different activity classes, augmentation and subjects represented in our dataset.
Similarly for the test set, we show a sample of the activities and the variety of subjects in Figure 4.
5.1. The Challenge of Learning Invariance to Viewpoint Elevation
In this paper, we only consider the cameras in the training set placed at chest-height, in part to be compatible with
the existing datasets, and in part because viewpoint elevation invariance is a signiﬁcantly more challenging problem.
Existing benchmarks do not place emphasis on this. We will
release an expanded version of our MPI-INF-3DHP testset
with multiple camera viewpoint elevations, to complement
the training data.
Figure 3. A sample of the activities, clothing, subjects as well as augmentation on MPI-INF-3DHP Trainig Set.
Figure 4. A sample of the activities and subjects in the test set of MPI-INF-3DHP