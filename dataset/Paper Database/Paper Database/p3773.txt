DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation
Leonid Pishchulin1, Eldar Insafutdinov1, Siyu Tang1, Bjoern Andres1,
Mykhaylo Andriluka1,3, Peter Gehler2, and Bernt Schiele1
1Max Planck Institute for Informatics, Germany
2Max Planck Institute for Intelligent Systems, Germany
3Stanford University, USA
This paper considers the task of articulated human pose
estimation of multiple people in real world images. We propose an approach that jointly solves the tasks of detection
and pose estimation: it infers the number of persons in a
scene, identiﬁes occluded body parts, and disambiguates
body parts between people in close proximity of each other.
This joint formulation is in contrast to previous strategies,
that address the problem by ﬁrst detecting people and subsequently estimating their body pose. We propose a partitioning and labeling formulation of a set of body-part hypotheses
generated with CNN-based part detectors. Our formulation,
an instance of an integer linear program, implicitly performs
non-maximum suppression on the set of part candidates and
groups them to form conﬁgurations of body parts respecting geometric and appearance constraints. Experiments on
four different datasets demonstrate state-of-the-art results
for both single person and multi person pose estimation1.
1. Introduction
Human body pose estimation methods have become increasingly reliable. Powerful body part detectors in
combination with tree-structured body models show
impressive results on diverse datasets . These
benchmarks promote pose estimation of single pre-localized
persons but exclude scenes with multiple people. This problem deﬁnition has been a driver for progress, but also falls
short on representing a realistic sample of real-world images.
Many photographs contain multiple people of interest (see
Fig 1) and it is unclear whether single pose approaches generalize directly. We argue that the multi person case deserves
more attention since it is an important real-world task.
Key challenges inherent to multi person pose estimation
1Models and code available at 
Figure 1. Method overview: (a) initial detections (= part candidates)
and pairwise terms (graph) between all detections that (b) are jointly
clustered belonging to one person (one colored subgraph = one
person) and each part is labeled corresponding to its part class
(different colors and symbols correspond to different body parts);
(c) shows the predicted pose sticks.
are the partial visibility of some people, signiﬁcant overlap
of bounding box regions of people, and the a-priori unknown
number of people in an image. The problem thus is to infer the number of persons, assign part detections to person
instances while respecting geometric and appearance constraints. Most strategies use a two-stage inference process
 to ﬁrst detect and then independently estimate
poses. This is unsuited for cases when people are in close
 
proximity since they permit simultaneous assignment of the
same body-part candidates to multiple people hypotheses.
As a principled solution for multi person pose estimation
a model is proposed that jointly estimates poses of all people
present in an image by minimizing a joint objective. The
formulation is based on partitioning and labeling an initial
pool of body part candidates into subsets that correspond to
sets of mutually consistent body-part candidates and abide to
mutual consistency and exclusion constraints. The proposed
method has a number of appealing properties. (1) The formulation is able to deal with an unknown number of people,
and also infers this number by linking part hypotheses. (2)
The formulation allows to either deactivate or merge part hypotheses in the initial set of part candidates hence effectively
performing non-maximum suppression (NMS). In contrast
to NMS performed on individual part candidates, the model
incorporates evidence from all other parts making the process more reliable. (3) The problem is cast in the form of
an Integer Linear Program (ILP). Although the problem is
NP-hard, the ILP formulation facilitates the computation of
bounds and feasible solutions with a certiﬁed optimality gap.
This paper makes the following contributions. The main
contribution is the derivation of a joint detection and pose
estimation formulation cast as an integer linear program. Further, two CNN variants are proposed to generate representative sets of body part candidates. These, combined with the
model, obtain state-of-the-art results for both single-person
and multi-person pose estimation on different datasets.
Related work. Most work on pose estimation targets the
single person case. Methods progressed from simple part
detectors and elaborate body models to treestructured pictorial structures (PS) models with strong part
detectors . Impressive results are obtained predicting locations of parts with convolutional neural networks
(CNN) . While body models are not a necessary
component for effective part localization, constraints among
parts allow to assemble independent detections into body
conﬁgurations as demonstrated in by combining CNNbased body part detectors with a body model .
A popular approach to multi-person pose estimation is to
detect people ﬁrst and then estimate body pose independently
 . proposes a ﬂexible mixture-of-parts
model for detection and pose estimation. obtains multiple pose hypotheses corresponding to different root part
positions and then performing non-maximum suppression.
 detects people using a ﬂexible conﬁguration of poselets
and the body pose is predicted as a weighted average of activated poselets. detects people and then predicts poses
of each person using a PS model. estimates poses of multiple people in 3D by constructing a shared space of 3D body
part hypotheses, but uses 2D person detections to establish
the number of people in the scene. These approaches are
limited to cases with people sufﬁciently far from each other
that do not have overlapping body parts.
Our work is closely related to who also propose
a joint objective to estimate poses of multiple people. 
proposes a multi-person PS model that explicitly models
depth ordering and person-person occlusions. Our formulation is not limited by a number of occlusion states among
people. proposes a joint model for pose estimation and
body segmentation coupling pose estimates of individuals
by image segmentation. uses a person detector to
generate initial hypotheses for the joint model. resorts
to a greedy approach of adding one person hypothesis at a
time until the joint objective can be reduced, whereas our
formulation can be solved with a certiﬁed optimality gap.
In addition relies on expensive labeling of body part
segmentation, which the proposed approach does not require.
Similarly to we aim to distinguish between visible
and occluded body parts. primarily focuse on the singleperson case and handles multi-person scenes akin to .
We consider the more difﬁcult problem of full-body pose
estimation, whereas focus on upper-body poses and
consider a simpliﬁed case of people seen from the front.
Our work is related to early work on pose estimation
that also relies on integer linear programming to assemble
candidate body part hypotheses into valid conﬁgurations .
Their single person method employs a tree graph augmented
with weaker non-tree repulsive edges and expects the same
number of parts. In contrast, our novel formulation relies
on fully connected model to deal with unknown number of
people per image and body parts per person.
The Minimum Cost Multicut Problem , known in
machine learning as correlation clustering , has been used
in computer vision for image segmentation but
has not been used before in the context of pose estimation.
It is known to be NP-hard .
2. Problem Formulation
In this section, the problem of estimating articulated poses
of an unknown number of people in an image is cast as an
optimization problem. The goal of this formulation is to state
three problems jointly: 1. The selection of a subset of body
parts from a set D of body part candidates, estimated from
an image as described in Section 4 and depicted as nodes of a
graph in Fig. 1(a). 2. The labeling of each selected body part
with one of C body part classes, e.g., “arm”, “leg”, “torso”,
as depicted in Fig. 1(c). 3. The partitioning of body parts
that belong to the same person, as depicted in Fig. 1(b).
2.1. Feasible Solutions
We encode labelings of the three problems jointly through
triples (x, y, z) of binary random variables with domains
x ∈{0, 1}D×C, y ∈{0, 1}
and z ∈{0, 1}
Here, xdc = 1 indicates that body part candidate d is of
class c, ydd′ = 1 indicates that the body part candidates d
and d′ belong to the same person, and zdd′cc′ are auxiliary
variables to relate x and y through zdd′cc′ = xdcxd′c′ydd′.
Thus, zdd′cc′ = 1 indicates that body part candidate d is
of class c (xdc = 1), body part candidate d′ is of class c′
(xd′c′ = 1), and body part candidates d and d′ belong to the
same person (ydd′ = 1).
In order to constrain the 01-labelings (x, y, z) to welldeﬁned articulated poses of one or more people, we impose
the linear inequalities (1)–(3) stated below. Here, the inequalities (1) guarantee that every body part is labeled with
at most one body part class. (If it is labeled with no body part
class, it is suppressed). The inequalities (2) guarantee that
distinct body parts d and d′ belong to the same person only if
neither d nor d′ is suppressed. The inequalities (3) guarantee,
for any three pairwise distinct body parts, d, d′ and d′′, if d
and d′ are the same person (as indicated by ydd′ = 1) and
d′ and d′′ are the same person (as indicated by yd′d′′ = 1),
then also d and d′′ are the same person (ydd′′ = 1), that is,
transitivity, cf. . Finally, the inequalities (4) guarantee, for
and any cc′ ∈C2 that zdd′cc′ = xdcxd′c′ydd′.
These constraints allow us to write an objective function as
a linear form in z that would otherwise be written as a cubic
form in x and y. We denote by XDC the set of all (x, y, z)
that satisfy all inequalities, i.e., the set of feasible solutions.
∀d ∈D∀cc′ ∈
xdc + xdc′ ≤1
ydd′ + yd′d′′ −1 ≤ydd′′
∀cc′ ∈C2 :
xdc + xd′c′ + ydd′ −2 ≤zdd′cc′
zdd′cc′ ≤xdc
zdd′cc′ ≤xd′c′
zdd′cc′ ≤ydd′
When at most one person is in an image, we further
constrain the feasible solutions to a well-deﬁned pose of a
single person. This is achieved by an additional class of
inequalities which guarantee, for any two distinct body parts
that are not suppressed, that they must be clustered together:
∀cc′ ∈C2 :
xdc + xd′c′ −1 ≤ydd′
2.2. Objective Function
For every pair (d, c) ∈D × C, we will estimate a probability pdc ∈ of the body part d being of class c. In the
context of CRFs, these probabilities are called part unaries
and we will detail their estimation in Section 4.
For every dd′ ∈
and every cc′ ∈C2, we consider a
probability pdd′cc′ ∈(0, 1) of the conditional probability of
d and d′ belonging to the same person, given that d and d′ are
body parts of classes c and c′, respectively. For c ̸= c′, these
probabilities pdd′cc′ are the pairwise terms in a graphical
model of the human body. In contrast to the classic pictorial
structures model, our model allows for a fully connected
graph where each body part is connected to all other parts in
the entire set D by a pairwise term. For c = c′, pdd′cc′ is the
probability of the part candidates d and d′ representing the
same part of the same person. This facilitates clustering of
multiple part candidates of the same part of the same person
and a repulsive property that prevents nearby part candidates
of the same type to be associated to different people.
The optimization problem that we call the subset partition
and labeling problem is the ILP that minimizes over the set
of feasible solutions XDC:
(x,y,z)∈XDC ⟨α, x⟩+ ⟨β, z⟩,
where we used the short-hand notation
αdc := log 1 −pdc
βdd′cc′ := log 1 −pdd′cc′
βdd′cc′ zdd′cc′ .
The objective (6)–(10) is the MAP estimate of a probability measure of joint detections x and clusterings y, z
of body parts, where prior probabilities pdc and pdd′cc′ are
estimated independently from data, and the likelihood is a
positive constant if (x, y, z) satisﬁes (1)–(4), and is 0, otherwise. The exact form (6)–(10) is obtained when minimizing
the negative logarithm of this probability measure.
2.3. Optimization
In order to obtain feasible solutions of the ILP (6) with
guaranteed bounds, we separate the inequalities (1)–(5) in
the branch-and-cut loop of the state-of-the-art ILP solver
Gurobi. More precisely, we solve a sequence of relaxations
of the problem (6), starting with the (trivial) unconstrained
problem. Each problem is solved using the cuts proposed
by Gurobi. Once an integer feasible solution is found, we
identify violated inequalities (1)–(5), if any, by breadth-ﬁrstsearch, add these to the constraint pool and re-solve the
tightened relaxation. Once an integer solution satisfying
all inequalities is found, together with a lower bound that
certiﬁes an optimality gap below 1%, we terminate.
3. Pairwise Probabilities
Here we describe the estimation of the pairwise terms.
We deﬁne pairwise features fdd′ for the variable zdd′cc′
(Sec. 2). Each part detection d includes the probabilities
fpdc (Sec. 4.4), its location (xd, yd), scale hd and bounding box Bd coordinates. Given two detections d and d′,
and the corresponding features (fpdc, xd, yd, hd, Bd) and
(fpd′c, xd′, yd′, hd′, Bd′), we deﬁne two sets of auxiliary
variables for zdd′cc′, one set for c = c′ (same body part class
clustering) and one for c ̸= c′ (across two body part classes
labeling). These features capture the proximity, kinematic
relation and appearance similarity between body parts.
The same body part class (c = c′). Two detections denoting the same body part of the same person should be
in close proximity to each other. We introduce the following auxiliary variables that capture the spatial relations:
∆x = |xd−xd′|/¯h, ∆y = |yd−yd′|/¯h, ∆h = |hd−hd′|/¯h,
IOUnion, IOMin, IOMax. The latter three are intersections over union/minimum/maximum of the two detection
boxes, respectively, and ¯h = (hd + hd′)/2.
Non-linear Mapping.
We augment the feature representation by appending quadratic and exponential terms.
The ﬁnal pairwise feature fdd′ for the variable zdd′cc is
(∆x, ∆y, ∆h, IOUnion, IOMin, IOMax, (∆x)2,
. . . , (IOMax)2, exp (−∆x), . . . , exp (−IOMax)).
Two different body part classes (c ̸= c′). We encode the
kinematic body constraints into the pairwise feature by introducing auxiliary variables Sdd′ and Rdd′, where Sdd′ and
Rdd′ are the Euclidean distance and the angle between two
detections, respectively. To capture the joint distribution of
Sdd′ and Rdd′, instead of using Sdd′ and Rdd′ directly, we
employ the posterior probability p(zdd′cc′ = 1|Sdd′, Rdd′)
as pairwise feature for zdd′cc′ to encode the geometric relations between the body part class c and c′. More speciﬁcally,
assuming the prior probability p(zdd′cc′ = 1) = p(zdd′cc′ =
0) = 0.5, the posterior probability of detection d and d′ have
the body part label c and c′, namely zdd′cc′ = 1, is
p(zdd′cc′ = 1|Sdd′, Rdd′)
p(Sdd′, Rdd′|zdd′cc′ = 1)
p(Sdd′, Rdd′|zdd′cc′ = 1) + p(Sdd′, Rdd′|zdd′cc′ = 0),
where p(Sdd′, Rdd′|zdd′cc′ = 1) is obtained by conducting
a normalized 2D histogram of Sdd′ and Rdd′ from positive training examples, analogous to the negative likelihood
p(Sdd′, Rdd′|zdd′cc′ = 0). In Sec. 5.1 we also experiment
with encoding the appearance into the pairwise feature by
concatenating the feature fpdc from d and fpd′c from d′, as
fpdc is the output of the CNN-based part detectors. The ﬁnal
pairwise feature is (p(zdd′cc′ = 1|Sdd′, Rdd′), fpdc, fpd′c).
3.1. Probability Estimation
The coefﬁcients α and β of the objective function (Eq. 6)
are deﬁned by the probability ratio in the log space (Eq. 7 and
Eq. 8). Here we describe the estimation of the corresponding
probability density: (1) For every pair of detection and part
classes, namely for any (d, c) ∈D × C, we estimate a
probability pdc ∈(0, 1) of the detection d being a body
part of class c. (2) For every combination of two distinct
detections and two body part classes, namely for any dd′ ∈
and any cc′ ∈C2, we estimate a probability pdd′cc′ ∈
(0, 1) of d and d′ belonging to the same person, meanwhile
d and d′ are body parts of classes c and c′, respectively.
Learning. Given the features fdd′ and a Gaussian prior
p(θcc′) = N(0, σ2) on the parameters, logistic model is
p(zdd′cc′ = 1|fdd′, θcc′) =
1 + exp(−⟨θcc′, fdd′⟩). (11)
(|C| × (|C| + 1))/2 parameters are estimated using ML.
Inference Given two detections d and d′, the coefﬁcients
αdc for xdc and αd′c for xd′c are obtained by Eq. 7, the
coefﬁcient βdd′cc′ for zdd′cc′ has the form
βdd′cc′ = log 1 −pdd′cc′
= −⟨fdd′, θcc′⟩.
Model parameters θcc′ are learned using logistic regression.
4. Body Part Detectors
We ﬁrst introduce our deep learning-based part detection
models and then evaluate them on two prominent benchmarks thereby signiﬁcantly outperforming state of the art.
4.1. Adapted Fast R-CNN (AFR-CNN)
To obtain strong part detectors we adapt Fast R-
CNN . FR-CNN takes as input an image and set of
class-independent region proposals and outputs the softmax probabilities over all classes and reﬁned bounding boxes.
To adapt FR-CNN for part detection we alter it in two ways:
1) proposal generation and 2) detection region size. The
adapted version is called AFR-CNN throughout the paper.
Detection proposals. Generating object proposals is essential for FR-CNN, meanwhile detecting body parts is challenging due to their small size and high intra-class variability.
We use DPM-based part detectors for proposal generation. We collect K top-scoring detections by each part
detector in a common pool of N part-independent proposals
and use these proposals as input to AFR-CNN. N is 2, 000
in case of single and 20, 000 in case of multiple people.
Larger context. Increasing the size of DPM detections
by upscaling every bounding box by a ﬁxed factor allows
to capture more context around each part. In Sec. 4.3 we
evaluate the inﬂuence of upscaling and show that using larger
context around parts is crucial for best performance.
Details. Following standard FR-CNN training procedure ImageNet models are ﬁnetuned on pose estimation task. Center
of a predicted bounding box is used for body part location
prediction. See Appendix A for detailed parameter analysis.
Head Sho Elb Wri Hip Knee Ank PCK AUC
oracle 2000
98.8 98.8 97.4 96.4 97.4 98.3 97.7 97.8 84.0
DPM scale 1
48.8 25.1 14.4 10.2 13.6 21.8 27.1 23.0 13.6
AlexNet scale 1
82.2 67.0 49.6 45.4 53.1 52.9 48.2 56.9 35.9
AlexNet scale 4
85.7 74.4 61.3 53.2 64.1 63.1 53.8 65.1 39.0
+ optimal params
88.1 79.3 68.9 62.6 73.5 69.3 64.7 72.4 44.6
VGG scale 4 optimal params 91.0 84.2 74.6 67.7 77.4 77.3 72.8 77.9 50.0
+ ﬁnetune LSP
95.4 86.5 77.8 74.0 84.5 78.8 82.6 82.8 57.0
Table 1. Unary only performance (PCK) of AFR-CNN on the LSP
(Person-Centric) dataset. AFR-CNN is ﬁnetuned from ImageNet to
MPII (lines 3-6), and then ﬁnetuned to LSP (line 7).
4.2. Dense Architecture (Dense-CNN)
Using proposals for body part detection may be suboptimal. We thus develop a fully convolutional architecture
for computing part probability scoremaps.
Stride. We build on VGG . Fully convolutional VGG
has stride of 32 px – too coarse for precise part localization.
We thus use hole algorithm to reduce the stride to 8 px.
Scale. Selecting image scale is crucial. We found that scaling to a standing height of 340 px performs best: VGG
receptive ﬁeld sees entire body to disambiguate body parts.
Loss function. We start with a softmax loss that outputs
probabilities for each body part and background. The downside is inability to assign probabilities above 0.5 to several
close-by body parts. We thus re-formulate the detection as
multi-label classiﬁcation, where at each location a separate
set of probability distributions is estimated for each part. We
use sigmoid activation function on the output neurons and
cross entropy loss. We found this loss to perform better than
softmax and converge much faster compared to MSE .
Target training scoremap for each joint is constructed by
assigning a positive label 1 at each location within 15 px to
the ground truth, and negative label 0 otherwise.
Location reﬁnement. In order to improve location precision we follow : we add a location reﬁnement FC layer
after the FC7 and use the relative offsets (∆x, ∆y) from a
scoremap location to the ground truth as targets.
Regression to other parts. Similar to location reﬁnement
we add an extra term to the objective function where for each
part we regress onto all other part locations. We found this
auxiliary task to improve the performance (c.f. Sec. 4.3).
Training. We follow best practices and use SGD for CNN
training. In each iteration we forward-pass a single image.
After FC6 we select all positive and random negative samples to keep the pos/neg ratio as 25%/75%. We ﬁnetune
VGG from Imagenet model to pose estimation task and use
training data augmentation. We train for 430k iterations with
the following learning rates (lr): 10k at lr=0.001, 180k at
lr=0.002, 120k at lr=0.0002 and 120k at lr=0.0001. Pretraining at smaller lr prevents the gradients from diverging.
Head Sho Elb Wri Hip Knee Ank PCK AUC
MPII softmax
91.5 85.3 78.0 72.4 81.7 80.7 75.7 80.8 51.9
94.6 86.8 79.9 75.4 83.5 82.8 77.9 83.0 54.7
93.5 87.2 81.0 77.0 85.5 83.3 79.3 83.8 55.6
+ location reﬁnement 95.0 88.4 81.5 76.4 88.0 83.3 80.8 84.8 61.5
+ auxiliary task
95.1 89.6 82.8 78.9 89.0 85.9 81.2 86.1 61.6
+ ﬁnetune LSP
97.2 90.8 83.0 79.3 90.6 85.6 83.1 87.1 63.6
Table 2. Unary only performance (PCK) of Dense-CNN VGG on
LSP (PC) dataset. Dense-CNN is ﬁnetuned from ImageNet to MPII
(line 1), to MPII+LSPET (lines 2-5), and ﬁnally to LSP (line 6).
4.3. Evaluation of Part Detectors
Datasets. We train and evaluate on three public benchmarks:
“Leeds Sports Poses” (LSP) (person-centric (PC)), “LSP
Extended” (LSPET) 2, and “MPII Human Pose” (“Single Person”) . The MPII training set (19185 people) is
used as default. In some cases LSP training and LSPET are
added to MPII (marked as MPII+LSPET in the experiments).
Evaluation measures. We use the standard “PCK” metric and evaluation scripts available on the web
page of . In addition, we report “Area under Curve”
(AUC) computed for the entire range of PCK thresholds.
AFR-CNN. Evaluation of AFR-CNN on LSP is shown in
Tab. 1. Oracle selecting per part the closest from 2, 000 proposals achieves 97.8% PCK, as proposals cover majority of
the ground truth locations. Choosing a single proposal per
part using DPM score achieves 23.0% PCK – not surprising
given the difﬁculty of the body part detection problem. Rescoring the proposals using AFR-CNN with AlexNet 
dramatically improves the performance to 56.9% PCK, as
CNN learns richer image representations. Extending the regions by 4x (1x ≈head size) achieves 65.1% PCK, as it incorporates more context including the information about symmetric parts and allows to implicitly encode higher-order part
relations. Using data augmentation and slightly tuning training parameters improves the performance to 72.4% PCK.
We refer to the Appendix A for detailed analysis. Deeper
VGG architecture improves over smaller AlexNet reaching
77.9% PCK. All results so far are achieved by ﬁnetuning
the ImageNet models on MPII. Further ﬁnetuning to LSP
leads to remarkable 82.8% PCK: CNN learns LSP-speciﬁc
image representations. Strong increase in AUC (57.0 vs.
50%) is due to improvements for smaller PCK thresholds.
Using no bounding box regression leads to performance drop
(81.3% PCK, 53.2% AUC): location reﬁnement is crucial
for better localization. Overall AFR-CNN obtains very good
results on LSP by far outperforming the state of the art (c.f.
Tab. 3, rows 7 −9). Evaluation on MPII shows competitive
performance (Tab. 4, row 1).
Dense-CNN. The results are in Tab. 2. Training with VGG
on MPII with softmax loss achieves 80.8% PCK thereby
2To reduce labeling noise we re-annotated original high-resolution images and make the data available at 
mpg.de/hr-lspet/hr-lspet.zip
Normalized distance
0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2
Detection rate, %
PCK total, LSP PC
DeepCut SP AFR-CNN
DeepCut SP Dense-CNN
Tompson et al., NIPS'14
Chen&Yuille, NIPS'14
Fan et al., CVPR'15
Normalized distance
0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
Detection rate, %
PCKh total, MPII
DeepCut SP AFR-CNN
DeepCut SP Dense-CNN
Tompson et al., NIPS'14
Tompson et al., CVPR'15
(a) LSP (PC)
(b) MPII Single Person
Figure 2. Pose estimation results over all PCK thresholds.
outperforming AFR-CNN (c.f. Tab. 1, row 6). This shows
the advantages of fully convolutional training and evaluation. Expectedly, training on larger MPII+LSPET dataset
improves the results (83.0 vs. 80.8% PCK). Using crossentropy loss with sigmoid activations improves the results to
83.8% PCK, as it better models the appearance of close-by
parts. Location reﬁnement improves localization accuracy
(84.8% PCK), which becomes more clear when analyzing
AUC (61.5 vs. 55.6%). Interestingly, regressing to other
parts further improves PCK to 86.1% showing a value of
training with the auxiliary task. Finally, ﬁnetuning to LSP
achieves the best result of 87.1% PCK, which is signiﬁcantly
higher than the best published results (c.f. Tab. 3, rows 7−9).
Unary-only evaluation on MPII reveals slightly higher AUC
results compared to the state of the art (Tab. 4, row 3 −4).
4.4. Using Detections in DeepCut Models
The SPLP problem is NP-hard, to solve instances of it
efﬁciently we select a subset of representative detections
from the entire set produced by a model. In our experiments
we use |D| = 100 as default detection set size. In case of the
AFR-CNN we directly use the softmax output as unary probabilities: fpdc = (pd1, . . . , pdc), where pdc is the probability
of the detection d being the part class c. For Dense-CNN
detection model we use the sigmoid detection unary scores.
5. DeepCut Results
The aim of this paper is to tackle the multi person case.
To that end, we evaluate the proposed DeepCut models on
four diverse benchmarks. We conﬁrm that both single person
(SP) and multi person (MP) variants (Sec. 2) are effective
on standard SP pose estimation datasets . Then, we
demonstrate superior performance of DeepCut MP on the
multi person pose estimation task.
5.1. Single Person Pose Estimation
We now evaluate single person (SP) and more general
multi person (MP) DeepCut models on LSP and MPII SP
benchmarks described in Sec. 4. Since this evaluation setting
implicitly relies on the knowledge that all parts are present
in the image we always output the full number of parts.
Results on LSP. We report per-part PCK results (Tab. 3)
and results for a variable distance threshold (Fig. 2 (a)).
Head Sho Elb Wri Hip Knee Ank PCK AUC
AFR-CNN (unary)
95.4 86.5 77.8 74.0 84.5 82.6 78.8 82.8 57.0
+ DeepCut SP
95.4 86.7 78.3 74.0 84.3 82.9 79.2 83.0 58.4
+ appearance pairwise 95.4 87.2 78.6 73.7 84.7 82.8 78.8 83.0 58.5
+ DeepCut MP
95.2 86.7 78.2 73.5 84.6 82.8 79.0 82.9 58.0
Dense-CNN (unary)
97.2 90.8 83.0 79.3 90.6 85.6 83.1 87.1 63.6
+ DeepCut SP
97.0 91.0 83.8 78.1 91.0 86.7 82.0 87.1 63.5
+ DeepCut MP
96.2 91.2 83.3 77.6 91.3 87.0 80.4 86.7 62.6
Tompson et al. 
90.6 79.2 67.9 63.4 69.5 71.0 64.2 72.3 47.3
Chen&Yuille 
91.8 78.2 71.8 65.5 73.3 70.2 63.4 73.4 40.1
Fan et al. ∗
92.4 75.2 65.3 64.0 75.7 68.3 70.4 73.0 43.2
∗re-evaluated using the standard protocol, for details see project page of 
Table 3. Pose estimation results (PCK) on LSP (PC) dataset.
DeepCut SP AFR-CNN model using 100 detections improves over unary only (83.0 vs.
82.8% PCK, 58.4 vs.
57% AUC), as pairwise connections ﬁlter out some of the
high-scoring detections on the background. The improvement is clear in Fig. 2 (a) for smaller thresholds. Using
part appearance scores in addition to geometrical features in
c ̸= c′ pairwise terms only slightly improves AUC, as the
appearance of neighboring parts is mostly captured by a relatively large region centered at each part. The performance of
DeepCut MP AFR-CNN matches the SP and improves over
AFR-CNN alone: DeepCut MP correctly handles the SP case.
Performance of DeepCut SP Dense-CNN is almost identical
to unary only, unlike the results for AFR-CNN. Dense-CNN
performance is noticeably higher compared to AFR-CNN,
and “easy” cases that could have been corrected by a spatial
model are resolved by stronger part detectors alone.
Comparison to the state of the art (LSP). Tab. 3 compares
results of DeepCut models to other deep learning methods
speciﬁcally designed for single person pose estimation. All
DeepCuts signiﬁcantly outperform the state of the art, with
DeepCut SP Dense-CNN model improving by 13.7% PCK
over the best known result . The improvement is even
more dramatic for lower thresholds (Fig. 2 (a)): for PCK
@ 0.1 the best model improves by 19.9% over Tompson et
al. , by 26.7% over Fan et al. , and by 32.4% PCK
over Chen&Yuille . The latter is interesting, as use a
stronger spatial model that predicts the pairwise conditioned
on the CNN features, whereas DeepCuts use geometric-only
pairwise connectivity. Including body part orientation information into DeepCuts should further improve the results.
Results on MPII Single Person.
Results are shown in
Tab. 4 and Fig. 2 (b). DeepCut SP AFR-CNN noticeably
improves over AFR-CNN alone (79.8 vs. 78.8% PCK, 51.1
vs. 49.0% AUC). The improvement is stronger for smaller
thresholds (c.f. Fig. 2), as spatial model improves part localization. Dense-CNN alone trained on MPII outperforms
AFR-CNN (81.6 vs.
78.8% PCK), which shows the advantages of dense training and evaluation. As expected,
Dense-CNN performs slightly better when trained on the
larger MPII+LSPET. Finally, DeepCut Dense-CNN SP is
slightly better than Dense-CNN alone leading to the best
Head Sho Elb Wri Hip Knee Ank PCKh AUC
AFR-CNN (unary)
91.5 89.7 80.5 74.4 76.9 69.6 63.1
+ DeepCut SP
92.3 90.6 81.7 74.9 79.2 70.4 63.0
Dense-CNN (unary) 93.5 88.6 82.2 77.1 81.7 74.4 68.9
94.0 89.4 82.3 77.5 82.0 74.4 68.7
+DeepCut SP
94.1 90.2 83.4 77.3 82.6 75.7 68.6
Tompson et al. 
95.8 90.3 80.5 74.3 77.6 69.7 62.8
Tompson et al. 
96.1 91.9 83.9 77.8 80.9 72.3 64.8
Table 4. Pose estimation results (PCKh) on MPII Single Person.
result on MPII dataset (82.4% PCK).
Comparison to the state of the art (MPII). We compare the performance of DeepCut models to the best
deep learning approaches from the literature 3.
DeepCut SP Dense-CNN outperforms both (82.4
vs 79.6 and 82.0% PCK, respectively). Similar to them
DeepCuts rely on dense training and evaluation of part detectors, but unlike them use single size receptive ﬁeld and
do not include multi-resolution context information. Also,
appearance and spatial components of DeepCuts are trained
piece-wise, unlike . We observe that performance differences are higher for smaller thresholds (c.f. Fig. 2 (b)).
This is remarkable, as a much simpler strategy for location
reﬁnement is used compared to . Using multi-resolution
ﬁlters and joint training should improve the performance.
5.2. Multi Person Pose Estimation
We now evaluate DeepCut MP models on the challenging
task of MP pose estimation with an unknown number of
people per image and visible body parts per person.
Datasets. For evaluation we use two public MP benchmarks:
“We Are Family” (WAF) with 350 training and 175 testing group shots of people; “MPII Human Pose” (“Multi-
Person”) consisting of 3844 training and 1758 testing
groups of multiple interacting individuals in highly articulated poses with variable number of parts. On MPII, we use
a subset of 288 testing images for evaluation. We ﬁrst pre-
ﬁnetune both AFR-CNN and Dense-CNN from ImageNet to
MPII and MPII+LSPET, respectively, and further ﬁnetune
each model to WAF and MPII Multi-Person. For WAF, we
re-train the spatial model on WAF training set.
WAF evaluation measure. Approaches are evaluated using
the ofﬁcial toolkit , thus results are directly comparable
to prior work. The toolkit implements occlusion-aware “Percentage of Correct Parts (mPCP)” metric. In addition, we
report “Accuracy of Occlusion Prediction (AOP)” .
MPII Multi-Person evaluation measure. PCK metric is
suitable for SP pose estimation with known number of parts
and does not penalize for false positives that are not a part
of the ground truth.
Thus, for MP pose estimation we
use “Mean Average Precision (mAP)” measure, similar to
 . In contrast to evaluating the detection
3 was re-trained and evaluated on MPII dataset by the authors.
Head U Arms L Arms Torso mPCP AOP
AFR-CNN det ROI
DeepCut MP AFR-CNN
Dense-CNN det ROI
DeepCut MP Dense-CNN 99.3
Ghiasi et. al. 
Eichner&Ferrari 
Chen&Yuille 
Table 5. Pose estimation results (mPCP) on WAF dataset.
of any part instance in the image disrespecting inconsistent pose predictions, we evaluate consistent part conﬁgurations. First, multiple body pose predictions are generated and
then assigned to the ground truth (GT) based on the highest
PCKh . Only single pose can be assigned to GT. Unassigned predictions are counted as false positives. Finally, AP
for each body part is computed and mAP is reported.
Baselines. To assess the performance of AFR-CNN and
Dense-CNN we follow a traditional route from the literature
based on two stage approach: ﬁrst a set of regions of interest (ROI) is generated and then the SP pose estimation is
performed in the ROIs. This corresponds to unary only performance. ROI are either based on a ground truth (GT ROI)
or on the people detector output (det ROI).
Results on WAF. Results are shown in Tab. 5. det ROI is
obtained by extending provided upper body detection boxes.
AFR-CNN det ROI achieves 57.6% mPCP and 73.9% AOP.
DeepCut MP AFR-CNN signiﬁcantly improves over AFR-
CNN det ROI achieving 82.2% mPCP. This improvement
is stronger compared to LSP and MPII due to several reasons. First, mPCP requires consistent prediction of body
sticks as opposite to body joints, and including spatial model
enforces consistency. Second, mPCP metric is occlusionaware. DeepCuts can deactivate detections for the occluded
parts thus effectively reasoning about occlusion. This is
supported by strong increase in AOP (85.6 vs. 73.9%). Results by DeepCut MP Dense-CNN follow the same tendency
achieving the best performance of 84.7% mPCP and 86.5%
AOP. Both increase in mPCP and AOP show the advantages
of DeepCuts over traditional det ROI approaches.
Tab. 5 shows that DeepCuts outperform all prior methods.
Deep learning method is outperformed both for mPCP
(84.7 vs. 80.7%) and AOP (86.5 vs. 84.9%) measures. This
is remarkable, as DeepCuts reason about part interactions
across several people, whereas primarily focuses on the
single-person case and handles multi-person scenes akin to
 . In contrast to , DeepCuts are not limited by the number of possible occlusion patterns and cover person-person
occlusions and other types as truncation and occlusion by
objects in one formulation. DeepCuts signiﬁcantly outperform while being more general: unlike DeepCuts
do not require person detector and not limited by a number
of occlusion states among people.
Qualitative comparison to is provided in Fig. 3.
DeepCut MP
Chen&Yuille 
Figure 3. Qualitative comparison of our joint formulation DeepCut MP Dense-CNN (middle) to the traditional two-stage approach
Dense-CNN det ROI (top) and the approach of Chen&Yuille (bottom) on WAF dataset. In contrast to det ROI, DeepCut MP is able
to disambiguate multiple and potentially overlapping persons and correctly assemble independent detections into plausible body part
conﬁgurations. In contrast to , DeepCut MP can better predict occlusions (image 2 person 1 −4 from the left, top row; image 4 person 1,
4; image 5, person 2) and better cope with strong articulations and foreshortenings (image 1, person 1, 3; image 2 person 1 bottom row;
image 3, person 1-2). See Appendix B for more examples.
Results on MPII Multi-Person. Obtaining a strong detector of highly articulated people having strong occlusions and
truncations is difﬁcult. We employ a neck detector as a person detector as it turned out to be the most reliable part. Full
body bounding box is created around a neck detection and
used as det ROI. GT ROIs were provided by the authors .
As the MP approach is not public, we compare to SP
state-of-the-art method applied to GT ROI image crops.
Results are shown in Tab. 6. DeepCut MP AFR-CNN
improves over AFR-CNN det ROI by 4.3% achieving 51.4%
AP. The largest differences are observed for the ankle, knee,
elbow and wrist, as those parts beneﬁt more from the connections to other parts. DeepCut MP UB AFR-CNN using
upper body parts only slightly improves over the full body
model when compared on common parts (60.5 vs 58.2% AP).
Similar tendencies are observed for Dense-CNNs, though improvements of MP UB over MP are more signiﬁcant.
All DeepCuts outperform Chen&Yuille SP GT ROI, partially due to stronger part detectors compared to (c.f.
Tab. 3). Another reason is that Chen&Yuille SP GT ROI does
not model body part occlusion and truncation always predicting the full set of parts, which is penalized by the AP
measure. In contrast, our formulation allows to deactivate the
part hypothesis in the initial set of part candidates thus effectively performing non-maximum suppression. In DeepCuts
part hypotheses are suppressed based on the evidence from
all other body parts making this process more reliable.
Head Sho Elb Wri Hip Knee Ank UBody FBody
AFR-CNN det ROI
71.1 65.8 49.8 34.0 47.7 36.6 20.6
AFR-CNN MP
71.8 67.8 54.9 38.1 52.0 41.2 30.4
AFR-CNN MP UB
75.2 71.0 56.4 39.6
Dense-CNN det ROI
77.2 71.8 55.9 42.1 53.8 39.9 27.4
Dense-CNN MP
73.4 71.8 57.9 39.9 56.7 44.0 32.0
Dense-CNN MP UB
81.5 77.3 65.8 50.0
AFR-CNN GT ROI
73.2 66.5 54.6 42.3 50.1 44.3 37.8
Dense-CNN GT ROI
78.1 74.1 62.2 52.0 56.9 48.7 46.1
Chen&Yuille SP GT ROI 65.0 34.2 22.0 15.7 19.2 15.8 14.2
Table 6. Pose estimation results (AP) on MPII Multi-Person.
6. Conclusion
Articulated pose estimation of multiple people in uncontrolled real world images is challenging but of real world
interest. In this work, we proposed a new formulation as a
joint subset partitioning and labeling problem (SPLP). Different to previous two-stage strategies that separate the detection and pose estimation steps, the SPLP model jointly
infers the number of people, their poses, spatial proximity, and part level occlusions. Empirical results on four
diverse and challenging datasets show signiﬁcant improvements over all previous methods not only for the multi person, but also for the single person pose estimation problem.
On multi person WAF dataset we improve by 30% PCP over
the traditional two-stage approach. This shows that a joint
formulation is crucial to disambiguate multiple and potentially overlapping persons. Models and code available at
 
Appendices
A. Additional Results on LSP dataset
We provide additional quantitative results on LSP dataset
using person-centric (PC) and observer-centric (OC) evaluation settings.
A.1. LSP Person-Centric (PC)
First, detailed performance analysis is performed when
evaluating various parameters of AFR-CNN and results are
reported using PCK evaluation measure. Then, performance of the proposed AFR-CNN and Dense-CNN part
detection models is evaluated using strict PCP measure.
Detailed AFR-CNN performance analysis (PCK). Detailed parameter analysis of AFR-CNN is provided in Tab. 7
and results are reported using PCK evaluation measure. Respecting parameters for each experiment are shown in the
ﬁrst column and parameter differences between the neighboring rows in the table are highlighted in bold. Re-scoring the
2000 DPM proposals using AFR-CNN with AlexNet 
leads to 56.9% PCK. This is achieved using basis scale 1 (≈
head size) of proposals and training with initial learning rate
(lr) of 0.001 for 80k iterations, after which lr is reduced by
0.1, for a total number of 140k SGD iterations. In addition,
bounding box regression and default IoU threshold of 0.5 for
positive/negative label assignment have been used. Extending the regions by 4x increases the performance to 65.1%
PCK, as it incorporates more context including the information about symmetric body parts and allows to implicitly
encode higher-order body part relations into the part detector.
No improvements observed for larger scales. Increasing lr
to 0.003, lr reduction step to 160k and training for a larger
number of iterations (240k) improves the results to 67.4, as
higher lr allows for for more signiﬁcant updates of model
parameters when ﬁnetuned on the task of human body part
detection. Increasing the number of training examples by
reducing the training IoU threshold to 0.4 results into slight
performance improvement (68.8 vs. 67.4% PCK). Further
increasing the number of training samples by horizontally
ﬂipping each image and performing translation and scale
jittering of the ground truth training samples improves the
performance to 69.6% PCK and 42.3% AUC. The improvement is more pronounced for smaller distance thresholds
(42.3 vs. 40.9% AUC): localization of body parts is improved due to the increased number of jittered samples that
signiﬁcantly overlap with the ground truth. Further increasing the lr, lr reduction step and total number of iterations
altogether improves the performance to 72.4% PCK, and
very minor improvements are observed when training longer.
All results above are achieved by ﬁnetuning the AlexNet
architecture from the ImageNet model on the MPII training
set. Further ﬁnetuning the MPII-ﬁnetuned model on the LSP
training set increases the performance to 77.9% PCK, as the
network learns LSP-speciﬁc image representations. Using
the deeper VGG architecture improves over more shallow AlexNet (77.9 vs. 72.4% PCK, 50.0 vs. 44.6% AUC).
Funetuning VGG on LSP achieves remarkable 82.8% PCK
and 57.0% AUC. Strong increase in AUC (57.0 vs. 50%)
characterizes the improvement for smaller PCK evaluation
thresholds. Switching off bounding box regression results
into performance drop (81.3% PCK, 53.2% AUC) thus showing the importance of the bounding box regression for better
part localization. Overall, we demonstrate that proper adaptation and tweaking of the state-of-the-art generic object
detector FR-CNN leads to a strong body part detection
model that dramatically improves over the vanilla FR-CNN
(82.8 vs. 56.9% PCK, 57.8 vs. 35.9% AUC) and signiﬁcantly outperforms the state of the art (+9.4% PCK over the
best known PCK result and +9.7% AUC over the best
known AUC result .
Overall performance using PCP evaluation measure.
Performance when using the strict “Percentage of Correct
Parts (PCP)” measure is reported in Tab. 8. In contrast to PCK measure evaluating the accuracy of predicting
body joints, PCP evaluation metric measures the accuracy
of predicting body part sticks. AFR-CNN achieves 78.3%
PCP. Similar to PCK results, DeepCut SP AFR-CNN slightly
improves over unary alone, as it enforces more consistent
predictions of body part sticks. Using more general multiperson DeepCut MP AFR-CNN model results into similar
performance, which shows the generality of DeepCut MP
method. DeepCut SP Dense-CNN slightly improves over
Dense-CNN alone (84.3 vs. 83.9% PCP) achieving the best
PCP result on LSP dataset using PC annotations. This is
in contrast to PCK results where performance differences
DeepCut SP Dense-CNN vs. Dense-CNN alone are minor.
We now compare the PCP results to the state of the art.
The DeepCut models outperform all other methods by a large
margin. The best known PCP result by Chen&Yuille is
outperformed by 10.7% PCP. This is interesting, as their
deep learning based method relies on the image conditioned
pairwise terms while our approach uses more simple geometric only connectivity. Interestingly, AFR-CNN alone
outperforms the approach of Fan et al. (78.3 vs. 70.1%
PCP), who build on the previous version of the R-CNN detector . At the same time, the best performing dense
architecture DeepCut SP Dense-CNN outperforms by
+14.2% PCP. Surprisingly, DeepCut SP Dense-CNN dramatically outperforms the method of Tompson et al. 
(+17.7% PCP) that also produces dense score maps, but additionally includes multi-scale receptive ﬁelds and jointly
trains appearance and spatial models in a single deep learning
framework. We envision that both advances can further improve the performance of DeepCut models. Finally, all proposed approaches signiﬁcantly outperform earlier non-deep
Head Sho Elb Wri Hip Knee Ank PCK AUC
AlexNet scale 1, lr 0.001, lr step 80k, # iter 140k, IoU pos/neg 0.5
82.2 67.0 49.6 45.4 53.1 52.9 48.2 56.9 35.9
AlexNet scale 4, lr 0.001, lr step 80k, # iter 140k, IoU pos/neg 0.5
85.7 74.4 61.3 53.2 64.1 63.1 53.8 65.1 39.0
AlexNet scale 4, lr 0.003, lr step 160k, # iter 240k, IoU pos/neg 0.5
87.0 75.1 63.0 56.3 67.0 65.7 58.0 67.4 40.8
AlexNet scale 4, lr 0.003, lr step 160k, # iter 240k, IoU pos/neg 0.4
87.5 76.7 64.8 56.0 68.2 68.7 59.6 68.8 40.9
AlexNet scale 4, lr 0.003, lr step 160k, # iter 240k, IoU pos/neg 0.4, data augment 87.8 77.8 66.0 58.1 70.9 66.9 59.8 69.6 42.3
AlexNet scale 4, lr 0.004, lr step 320k, # iter 1M, IoU pos/neg 0.4, data augment
88.1 79.3 68.9 62.6 73.5 69.3 64.7 72.4 44.6
+ ﬁnetune LSP, lr 0.0005, lr step 10k, # iter 40k
92.9 81.0 72.1 66.4 80.6 77.6 75.0 77.9 51.6
VGG scale 4, lr 0.003, lr step 160k, # iter 320k, IoU pos/neg 0.4, data augment
91.0 84.2 74.6 67.7 77.4 77.3 72.8 77.9 50.0
+ ﬁnetune LSP lr 0.0005, lr step 10k, # iter 40k
95.4 86.5 77.8 74.0 84.5 78.8 82.6 82.8 57.0
Table 7. PCK performance of AFR-CNN (unary) on LSP (PC) dataset. AFR-CNN is ﬁnetuned from ImageNet on MPII (lines 1-6, 8), and
then ﬁnetuned on LSP (lines 7, 9).
Torso Upper Lower Upper Fore- Head PCP
AFR-CNN (unary)
+ DeepCut SP
+ appearance pairwise 93.4
+ DeepCut MP
Dense-CNN (unary)
+ DeepCut SP
+ DeepCut MP
Tompson et al. 
Chen&Yuille 
Fan et al. ∗
Pishchulin et al. 
Wang&Li 
∗re-evaluated using the standard protocol, for details see project page of 
Table 8. Pose estimation results (PCP) on LSP (PC) dataset.
learning based methods relying on hand-crafted
image features.
A.2. LSP Observer-Centric (OC)
We now evaluate the performance of the proposed part
detection models on LSP dataset using the observer-centric
(OC) annotations . In contrast to the person-centric (PC)
annotations used in all previous experiments, OC annotations
do not penalize for the right/left body part prediction ﬂips
and count a body part to be the right body part, if it is on the
right side of the line connecting pelvis and neck, and a body
part to be the left body part otherwise.
Evaluation is performed using the ofﬁcial OC annotations
provided by . Prior to evaluation, we ﬁrst ﬁnetune
the AFR-CNN and Dense-CNN part detection models from
ImageNet on MPII and MPII+LSPET training sets, respectively, (same as for PC evaluation), and then further ﬁnetuned
the models on LSP OC training set.
PCK evaluation measure. Results using OC annotations
and PCK evaluation measure are shown in Tab. 9 and in
Fig. 4. AFR-CNN achieves 84.2% PCK and 58.1% AUC.
This result is only slightly better compared to AFR-CNN
evaluated using PC annotations (84.2 vs 82.8% PCK, 58.1
vs. 57.0% AUC). Although PC annotations correspond to
a harder task, only small drop in performance when using PC annotations shows that the network can learn to
accurately predict person’s viewpoint and correctly label
left/right limbs in most cases. This is contrast to earlier
approaches based on hand-crafted features whose perfor-
Head Sho Elb Wri Hip Knee Ank PCK AUC
AFR-CNN (unary)
95.3 88.3 78.5 74.2 87.3 84.2 81.2 84.2 58.1
Dense-CNN (unary)
97.4 92.0 83.8 79.0 93.1 88.3 83.7 88.2 65.0
Chen&Yuille 
91.5 84.7 70.3 63.2 82.7 78.1 72.0 77.5 44.8
Ouyang et al. 
86.5 78.2 61.7 49.3 76.9 70.0 67.6 70.0 43.1
Pishchulin et. 
87.5 77.6 61.4 47.6 79.0 75.2 68.4 71.0 45.0
Kiefel&Gehler 
83.5 73.7 55.9 36.2 73.7 70.5 66.9 65.8 38.6
Ramakrishna et al. 84.9 77.8 61.4 47.2 73.6 69.1 68.8 69.0 35.2
Table 9. Pose estimation results (PCK) on LSP (OC) dataset.
Normalized distance
0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2
Detection rate, %
PCK total, LSP OC
Chen&Yuille, NIPS'14
Ouyang et al., CVPR'14
Pishchulin et., ICCV'13
Ramakrishna et al., ECCV'14
Kiefel&Gehler, ECCV'14
Figure 4. Pose estimation results over all PCK thresholds on LSP
(OC) dataset.
mance drops much stronger when evaluated in PC evaluation
setting (e.g. drops from 71.0% PCK when using OC
annotations to 58.0% PCK when using PC annotations). Similar to PC case, Dense-CNN detection model outperforms
AFR-CNN (88.2 vs. 84.2% PCK and 65.0 vs. 58.1% AUC).
The differences are more pronounced when examining the
entire PCK curve for smaller distance thresholds (c.f. Fig. 4).
performance
Dense-CNN to the state of the art, we observe that both
proposed approaches signiﬁcantly outperform other methods.
Both deep learning based approaches of Chen&Yuille 
and Ouyang et al. are outperformed by +10.7 and
+18.2% PCK when compared to the best performing
Dense-CNN. Analysis of PCK curve for the entire range of
PCK distance thresholds reveals even larger performance
differences (c.f. Fig. 4). The results using OC annotations
conﬁrm our ﬁndings from PC evaluation and clearly show
the advantages of the proposed part detection models over
the state-of-the-art deep learning methods , as well as
over earlier pose estimation methods based on hand-crafted
image features .
PCP evaluation measure. Results using OC annotations
Torso Upper Lower Upper Fore- Head PCP
AFR-CNN (unary)
Dense-CNN (unary)
Chen&Yuille 
Ouyang et al. 
Pishchulin et. 
Kiefel&Gehler 
Ramakrishna et al. 88.1
Table 10. Pose estimation results (PCP) on LSP (OC) dataset.
and PCP evaluation measure are shown in Tab. 10. Overall,
the trend is similar to PC evaluation: both proposed approaches signiﬁcantly outperform the state-of-the-art methods with Dense-CNN achieving the best result of 85.0% PCP
thereby improving by +10% PCP over the best published
result .
B. Additional Results on WAF dataset
Qualitative
comparison
formulation
DeepCut MP Dense-CNN to the traditional two-stage approach Dense-CNN det ROI relying on person detector, and
to the approach of Chen&Yuille on WAF dataset is shown
in Fig. 5. See ﬁgure caption for visual performance analysis.
C. Additional Results on MPII Multi-Person
Qualitative
comparison
formulation
DeepCut MP Dense-CNN to the traditional two-stage approach Dense-CNN det ROI on MPII Multi-Person dataset
is shown in Fig. 6 and 7. Dense-CNN det ROI works well
when multiple fully visible individuals are sufﬁciently separated and thus their body parts can be partitioned based on
the person detection bounding box. In this case the strong
Dense-CNN body part detection model can correctly estimate most of the visible body parts (image 16, 17, 19).
However, Dense-CNN det ROI cannot tell apart the body
parts of multiple individuals located next to each other and
possibly occluding each other, and often links the body parts
across the individuals (images 1-16, 19-20). In addition,
Dense-CNN det ROI cannot reason about occlusions and
truncations always providing a prediction for each body part
(image 4, 6, 10). In contrast, DeepCut MP Dense-CNN is
able to correctly partition and label an initial pool of body
part candidates (each image, top row) into subsets that correspond to sets of mutually consistent body part candidates
and abide to mutual consistency and exclusion constraints
(each image, row 2), thereby outputting consistent body pose
predictions (each image, row 3). c ̸= c′ pairwise terms allow to partition the initial set of part detection candidates
into valid pose conﬁgurations (each image, row 2: personclusters highlighted by dense colored connections). c = c′
pairwise terms facilitate clustering of multiple body part
candidates of the same body part of the same person (each
image, row 2: markers of the same type and color). In addition, c = c′ pairwise terms facilitate a repulsive property
that prevents nearby part candidates of the same type to be
associated to different people (image 1: detections of the
left shoulder are assigned to the front person only). Furthermore, DeepCut MP Dense-CNN allows to either merge
or deactivate part hypotheses thus effectively performing
non-maximum suppression and reasoning about body part
occlusions and truncations (image 3, row 2: body part hypotheses on the background are deactivated (black crosses);
image 6, row 2: body part hypotheses for the truncated body
parts are deactivated (black crosses); image 1-6, 8-9, 13-14,
row 3: only visible body parts of the partially occluded people are estimated, while non-visible body parts are correctly
predicted to be occluded). These qualitative examples show
that DeepCuts MP can successfully deal with the unknown
number of people per image and the unknown number of
visible body parts per person.