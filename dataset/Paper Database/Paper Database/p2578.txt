Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2963–2977
Florence, Italy, July 28 - August 2, 2019. c⃝2019 Association for Computational Linguistics
Interpretable Neural Predictions with Differentiable Binary Variables
Wilker Aziz
University of Amsterdam
 
Ivan Titov
ILLC, University of Amsterdam
ILCC, University of Edinburgh
 
The success of neural networks comes hand
in hand with a desire for more interpretability. We focus on text classiﬁers and make them
more interpretable by having them provide
a justiﬁcation—a rationale—for their predictions.
We approach this problem by jointly
training two neural network models: a latent
model that selects a rationale (i.e.
and informative part of the input text), and
a classiﬁer that learns from the words in the
rationale alone.
Previous work proposed to
assign binary latent masks to input positions
and to promote short selections via sparsityinducing penalties such as L0 regularisation.
We propose a latent model that mixes discrete
and continuous behaviour allowing at the same
time for binary selections and gradient-based
training without REINFORCE. In our formulation, we can tractably compute the expected
value of penalties such as L0, which allows us
to directly optimise the model towards a prespeciﬁed text selection rate. We show that our
approach is competitive with previous work on
rationale extraction, and explore further uses
in attention mechanisms.
Introduction
Neural networks are bringing incredible performance gains on text classiﬁcation tasks . However, this power comes hand in
hand with a desire for more interpretability, even
though its deﬁnition may differ .
While it is useful to obtain high classiﬁcation
accuracy, with more data available than ever
before it also becomes increasingly important to
justify predictions. Imagine having to classify a
large collection of documents, while verifying
that the classiﬁcations make sense. It would be
extremely time-consuming to read each document
to evaluate the results. Moreover, if we do not
pours a dark amber color with decent head that does
not recede much . it ’s a tad too dark to see the
carbonation , but fairs well . smells of roasted malts
and mouthfeel is quite strong in the sense that you
can get a good taste of it before you even swallow .
Rationale Extractor
pours a dark amber color with decent head that does
not recede much . it ’s a tad too dark to see the
carbonation , but fairs well . smells of roasted malts
and mouthfeel is quite strong in the sense that you
can get a good taste of it before you even swallow .
look: ⋆⋆⋆⋆
Figure 1: Rationale extraction for a beer review.
know why a prediction was made, we do not know
if we can trust it.
What if the model could provide us the most
important parts of the document, as a justiﬁcation
for its prediction? That is exactly the focus of this
paper. We use a setting that was pioneered by Lei
et al. . A rationale is deﬁned to be a short
yet sufﬁcient part of the input text; short so that it
makes clear what is most important, and sufﬁcient
so that a correct prediction can be made from the
rationale alone. One neural network learns to extract the rationale, while another neural network,
with separate parameters, learns to make a prediction from just the rationale. Lei et al. model this
by assigning a binary Bernoulli variable to each
input word. The rationale then consists of all the
words for which a 1 was sampled. Because gradients do not ﬂow through discrete samples, the rationale extractor is optimized using REINFORCE
 .
An L0 regularizer is used to
make sure the rationale is short.
We propose an alternative to purely discrete selectors for which gradient estimation is possible
without REINFORCE, instead relying on a repa-
Jasmijn Bastings
University of Amsterdam
 
rameterization of a random variable that exhibits
both continuous and discrete behavior .
To promote compact rationales,
we employ a relaxed form of L0 regularization
 , penalizing the objective as
a function of the expected proportion of selected
text. We also propose the use of Lagrangian relaxation to target a speciﬁc rate of selected input
Our contributions are summarized as follows:1
1. we present a differentiable approach to extractive rationales (§2) including an objective
that allows for specifying how much text is to
be extracted (§4);
2. we introduce HardKuma (§3), which gives
support to binary outcomes and allows for
reparameterized gradient estimates;
3. we empirically show that our approach is
competitive with previous work and that
HardKuma has further applications, e.g. in
attention mechanisms. (§6).
Latent Rationale
We are interested in making NN-based text classiﬁers interpretable by (i) uncovering which parts
of the input text contribute features for classiﬁcation, and (ii) basing decisions on only a fraction
of the input text (a rationale). Lei et al. 
approached (i) by inducing binary latent selectors
that control which input positions are available to
an NN encoder that learns features for classiﬁcation/regression, and (ii) by regularising their architectures using sparsity-inducing penalties on latent
assignments. In this section we put their approach
under a probabilistic light, and this will then more
naturally lead to our proposed method.
In text classiﬁcation, an input x is mapped to a
distribution over target labels:
Y |x ∼Cat(f(x; θ)) ,
where we have a neural network architecture
f(·; θ) parameterize the model—θ collectively denotes the parameters of the NN layers in f. That
is, an NN maps from data space (e.g. sentences,
short paragraphs, or premise-hypothesis pairs) to
the categorical parameter space (i.e. a vector of
class probabilities). For the sake of concreteness,
 
bastings/interpretable_predictions.
consider the input a sequence x = ⟨x1, . . . , xn⟩.
A target y is typically a categorical outcome, such
as a sentiment class or an entailment decision, but
with an appropriate choice of likelihood it could
also be a numerical score (continuous or integer).
Lei et al. augment this model with a
collection of latent variables which we denote by
z = ⟨z1, . . . , zn⟩. These variables are responsible
for regulating which portions of the input x contribute with predictors (i.e. features) to the classiﬁer. The model formulation changes as follows:
Zi|x ∼Bern(gi(x; φ))
Y |x, z ∼Cat(f(x ⊙z; θ))
where an NN g(·; φ) predicts a sequence of n
Bernoulli parameters—one per latent variable—
and the classiﬁer is modiﬁed such that zi indicates
whether or not xi is available for encoding. We
can think of the sequence z as a binary gating
mechanism used to select a rationale, which with
some abuse of notation we denote by x⊙z. Figure
1 illustrates the approach.
Parameter estimation for this model can be done
by maximizing a lower bound E(φ, θ) on the loglikelihood of the data derived by application of
Jensen’s inequality:2
log P(y|x) = log EP(z|x,φ) [P(y|x, z, θ)]
≥EP(z|x,φ) [log P(y|x, z, θ)] = E(φ, θ) .
These latent rationales approach the ﬁrst objective, namely, uncovering which parts of the input
text contribute towards a decision. However note
that an NN controls the Bernoulli parameters, thus
nothing prevents this NN from selecting the whole
of the input, thus defaulting to a standard text classiﬁer. To promote compact rationales, Lei et al.
 impose sparsity-inducing penalties on latent selectors. They penalise for the total number
of selected words, L0 in (4), as well as, for the total number of transitions, fused lasso in (4), and
approach the following optimization problem
φ,θ −E(φ, θ)+λ0
|zi −zi+1|
fused lasso
via gradient-based optimisation, where λ0 and λ1
are ﬁxed hyperparameters. The objective is however intractable to compute, the lowerbound, in
2This can be seen as variational inference where we perform approximate inference using a datadependent prior P(z|x, φ).
particular, requires marginalization of O(2n) binary sequences. For that reason, Lei et al. sample latent assignments and work with gradient estimates using REINFORCE .
The key ingredients are, therefore, binary latent variables and sparsity-inducing regularization, and therefore the solution is marked by nondifferentiability. We propose to replace Bernoulli
variables by rectiﬁed continuous random variables
 , for they exhibit both discrete
and continuous behaviour.
Moreover, they are
amenable to reparameterization in terms of a ﬁxed
random source , in
which case gradient estimation is possible without
REINFORCE. Following Louizos et al. ,
we exploit one such distribution to relax L0 regularization and thus promote compact rationales
with a differentiable objective. In section 3, we introduce this distribution and present its properties.
In section 4, we employ a Lagrangian relaxation to
automatically target a pre-speciﬁed selection rate.
And ﬁnally, in section 5 we present an example for
sentiment classiﬁcation.
Hard Kumaraswamy Distribution
Key to our model is a novel distribution that exhibits both continuous and discrete behaviour, in
this section we introduce it. With non-negligible
probability, samples from this distribution evaluate to exactly 0 or exactly 1.
In a nutshell: i)
we start from a distribution over the open interval (0, 1) (see dashed curve in Figure 2); ii) we
then stretch its support from l < 0 to r > 1 in
order to include {0} and {1} (see solid curve in
Figure 2); ﬁnally, iii) we collapse the probability
mass over the interval (l, 0] to {0}, and similarly,
the probability mass over the interval [1, r) to {1}
(shaded areas in Figure 2). This stretch-and-rectify
technique was proposed by Louizos et al. ,
who rectiﬁed samples from the BinaryConcrete
(or GumbelSoftmax) distribution . We adapted their technique to the Kumaraswamy distribution motivated
by its close resemblance to a Beta distribution, for
which we have stronger intuitions (for example, its
two shape parameters transit rather naturally from
unimodal to bimodal conﬁgurations of the distribution). In the following, we introduce this new
distribution formally.3
3We use uppercase letters for random variables (e.g. K,
T, and H) and lowercase for assignments (e.g. k, t, h). For a
Kuma(0.5, 0.5, ­0.1, 1.1)
Kuma(0.5, 0.5)
Figure 2: The HardKuma distribution: we start from a
Kuma(0.5, 0.5), and stretch its support to the interval
(−0.1, 1.1), ﬁnally we collapse all mass before 0 to {0}
and all mass after 1 to {1}.
Kumaraswamy distribution
The Kumaraswamy distribution is a two-parameters distribution over the
open interval (0, 1), we denote a Kumaraswamydistributed variable by K ∼Kuma(a, b), where
a ∈R>0 and b ∈R>0 control the distribution’s
shape. The dashed curve in Figure 2 illustrates the
density of Kuma(0.5, 0.5). For more details including its pdf and cdf, consult Appendix A.
The Kumaraswamy is a close relative of the
Beta distribution, though not itself an exponential
family, with a simple cdf whose inverse
K (u; a, b) =
for u ∈ , can be used to obtain samples
K (U; α, β) ∼Kuma(α, β)
by transformation of a uniform random source
U ∼U(0, 1). We can use this fact to reparameterize expectations .
Rectiﬁed Kumaraswamy
We stretch the support of the Kumaraswamy distribution to include 0 and 1. The resulting variable
T ∼Kuma(a, b, l, r) takes on values in the open
interval (l, r) where l < 0 and r > 1, with cdf
FT (t; a, b, l, r) = FK((t −l)/(r −l); a, b) .
We now deﬁne a rectiﬁed random variable, denoted by H ∼HardKuma(a, b, l, r), by passing
random variable K, fK(k; α) is the probability density function (pdf), conditioned on parameters α, and FK(k; α) is the
cumulative distribution function (cdf).
a sample T ∼Kuma(a, b, l, r) through a hardsigmoid, i.e.
h = min(1, max(0, t)).
The resulting variable is deﬁned over the closed interval . Note that while there is 0 probability of
sampling t = 0, sampling h = 0 corresponds to
sampling any t ∈(l, 0], a set whose mass under
Kuma(t|a, b, l, r) is available in closed form:
P(H = 0) = FK
That is because all negative values of t are deterministically mapped to zero. Similarly, samples t ∈[1, r) are all deterministically mapped to
h = 1, whose total mass amounts to
P(H = 1) = 1 −FK
See Figure 2 for an illustration, and Appendix A
for the complete derivations.
Reparameterization and gradients
Because this rectiﬁed variable is built upon a
Kumaraswamy, it admits a reparameterisation in
terms of a uniform variable U ∼U(0, 1). We
need to ﬁrst sample a uniform variable in the open
interval (0, 1) and transform the result to a Kumaraswamy variable via the inverse cdf (10a), then
shift and scale the result to cover the stretched support (10b), and ﬁnally, apply the rectiﬁer in order
to get a sample in the closed interval (10c).
K (u; a, b)
t = l + (r −l)k
h = min(1, max(0, t)) ,
We denote this h
s(u; a, b, l, r) for short.
Note that this transformation has two discontinuity
points, namely, t = 0 and t = 1. Though recall,
the probability of sampling t exactly 0 or exactly 1
is zero, which essentially means stochasticity circumvents points of non-differentiability of the rectiﬁer (see Appendix A.3).
Controlled Sparsity
Following Louizos et al. , we relax nondifferentiable penalties by computing them on expectation under our latent model p(z|x, φ). In addition, we propose the use of Lagrangian relaxation to target speciﬁc values for the penalties.
Thanks to the tractable Kumaraswamy cdf, the expected value of L0(z) is known in closed form
Ep(z|x) [L0(z)] ind
Ep(zi|x) [I[zi ̸= 0]]
1 −P(Zi = 0) ,
where P(Zi = 0) = FK
r−l; ai, bi
. This quantity is a tractable and differentiable function of the
parameters φ of the latent model.
We can also
compute a relaxation of fused lasso by computing the expected number of zero-to-nonzero and
nonzero-to-zero changes:
I[zi = 0, zi+1 ̸= 0]
I[zi ̸= 0, zi+1 = 0]
P(Zi = 0)(1 −P(Zi+1 = 0))
+ (1 −P(Zi = 0))P(Zi+1 = 0) .
In both cases, we make the assumption that latent
variables are independent given x, in Appendix
B.1.2 we discuss how to estimate the regularizers
for a model p(zi|x, z<i) that conditions on the pre-
ﬁx z<i of sampled HardKuma assignments.
We can use regularizers to promote sparsity,
but just how much text will our ﬁnal model select? Ideally, we would target speciﬁc values r and
solve a constrained optimization problem. In practice, constrained optimisation is very challenging,
thus we employ Lagrangian relaxation instead:
φ,θ −E(φ, θ) + λ⊤(R(φ) −r)
where R(φ) is a vector of regularisers, e.g. expected L0 and expected fused lasso, and λ is a vector of Lagrangian multipliers λ. Note how this differs from the treatment of Lei et al. shown
in (4) where regularizers are computed for assignments, rather than on expectation, and where
λ0, λ1 are ﬁxed hyperparameters.
Sentiment Classiﬁcation
As a concrete example, consider the case of sentiment classiﬁcation where x is a sentence and y is a
5-way sentiment class (from very negative to very
positive). The model consists of
Zi ∼HardKuma(ai, bi, l, r)
Y |x, z ∼Cat(f(x ⊙z; θ))
where the shape parameters a, b = g(x; φ), i.e.
two sequences of n strictly positive scalars, are
predicted by a NN, and the support boundaries
(l, r) are ﬁxed hyperparameters.
We ﬁrst specify an architecture that parameterizes latent selectors and then use a reparameterized
sample to restrict which parts of the input contribute encodings for classiﬁcation:4
ei = emb(xi)
1 = birnn(en
ui ∼U(0, 1)
ai = fa(hi; φa)
bi = fb(hi; φb)
zi = s(ui; ai, bi, l, r)
where emb(·) is an embedding layer, birnn(·; φr)
is a bidirectional encoder, fa(·; φa) and fb(·; φb)
are feed-forward transformations with softplus
outputs, and s(·) turns the uniform sample ui into
the latent selector zi (see §3). We then use the
sampled z to modulate inputs to the classiﬁer:
ei = emb(xi)
= rnn(h(fwd)
i−1 , zi ei; θfwd)
= rnn(h(bwd)
i+1 , zi ei; θbwd)
o = fo(h(fwd)
where rnn(·; θfwd) and rnn(·; θbwd) are recurrent
cells such as LSTMs that process the sequence in different
directions, and fo(·; θo) is a feed-forward transformation with softmax output. Note how zi modulates features ei of the input xi that are available
to the recurrent composition function.
We then obtain gradient estimates of E(φ, θ) via
Monte Carlo (MC) sampling from
E(φ, θ) = EU(0,I) [log P(y|x, sφ(u, x), θ)] (15)
where z = sφ(u, x) is a shorthand for elementwise application of the transformation from uniform samples to HardKuma samples. This reparameterisation is the key to gradient estimation
through stochastic computation graphs .
4We describe architectures using blocks denoted by
layer(inputs; subset of parameters), boldface letters for vectors, and the shorthand vn
1 for a sequence ⟨v1, . . . , vn⟩.
SVM 
BiLSTM 
BiRCNN 
BiLSTM (ours)
BiRCNN (ours)
Table 1: MSE on the BeerAdvocate test set.
Deterministic predictions.
At test time we
make predictions based on what is the most likely
assignment for each zi. We arg max across con-
ﬁgurations of the distribution, namely, zi = 0,
zi = 1, or 0 < zi < 1. When the continuous
interval is more likely, we take the expected value
of the underlying Kumaraswamy variable.
Experiments
We perform experiments on multi-aspect sentiment analysis to compare with previous work, as
well as experiments on sentiment classiﬁcation
and natural language inference. All models were
implemented in PyTorch, and Appendix B provides implementation details.
When rationalizing predictions, our goal
is to perform as well as systems using the full input
text, while using only a subset of the input text,
leaving unnecessary words out for interpretability.
Multi-aspect Sentiment Analysis
In our ﬁrst experiment we compare directly with
previous work on rationalizing predictions . We replicate their setting.
A pre-processed subset of the BeerAdvocate5 data set is used . It
consists of 220,000 beer reviews, where multiple
aspects (e.g.
look, smell, taste) are rated.
shown in Figure 1, a review typically consists of
multiple sentences, and contains a 0-5 star rating
(e.g. 3.5 stars) for each aspect. Lei et al. mapped
the ratings to scalars in .
We use the models described in §5 with
two small modiﬁcations: 1) since this is a regression task, we use a sigmoid activation in the output
layer of the classiﬁer rather than a softmax,6 and
5 
6From a likelihood learning point of view, we would have
assumed a Logit-Normal likelihood, however, to stay closer
to Lei et al. , we employ mean squared error.
% Precision
% Selected
% Precision
% Selected
% Precision
% Selected
Attention (Lei et al.)
Bernoulli (Lei et al.)
Bernoulli (reimpl.)
Table 2: Precision (% of selected words that was also annotated as the gold rationale) and selected (% of words
not zeroed out) per aspect. In the attention baseline, the top 13% (7%) of words with highest attention weights are
used for classiﬁcation. Models were selected based on validation loss.
2) we use an extra RNN to condition zi on z<i:
ai = fa(hi, si−1; φa)
bi = fb(hi, si−1; φb)
si = rnn(hi, zi, si−1; φs)
For a fair comparison we follow Lei et al. by using
RCNN7 cells rather than LSTM cells for encoding
sentences on this task. Since this cell is not widely
used, we veriﬁed its performance in Table 1. We
observe that the BiRCNN performs on par with the
BiLSTM (while using 50% fewer parameters), and
similarly to previous results.
Evaluation.
A test set with sentence-level rationale annotations is available. The precision of a rationale is deﬁned as the percentage of words with
z ̸= 0 that is part of the annotation. We also evaluate the predictions made from the rationale using
mean squared error (MSE).
Baselines.
For our baseline we reimplemented
the approach of Lei et al. which we call
Bernoulli after the distribution they use to sample
z from. We also report their attention baseline,
in which an attention score is computed for each
word, after which it is simply thresholded to select
the top-k percent as the rationale.
Table 2 shows the precision and the percentage of selected words for the ﬁrst three aspects. The models here have been selected based
on validation MSE and were tuned to select a similar percentage of words (‘selected’). We observe
that our Bernoulli reimplementation reaches the
precision similar to previous work, doing a little
bit worse for the ‘look’ aspect. Our HardKuma
managed to get even higher precision, and it extracted exactly the percentage of text that we spec-
7An RCNN cell can replace any LSTM cell and works
well on text classiﬁcation problems. See appendix B.
Selected Text
Figure 3: MSE of all aspects for various percentages of
extracted text. HardKuma (blue crosses) has lower error than Bernoulli ) for similar amount of extracted text.
The full-text baseline (black star) gets the best MSE.
iﬁed (see §4).8 Figure 3 shows the MSE for all aspects for various percentages of extracted text. We
observe that HardKuma does better with a smaller
percentage of text selected. The performance becomes more similar as more text is selected.
Sentiment Classiﬁcation
We also experiment on the Stanford Sentiment
Treebank (SST) . There are 5
sentiment classes: very negative, negative, neutral,
positive, and very positive. Here we use the Hard-
Kuma model described in §5, a Bernoulli model
trained with REINFORCE, as well as a BiLSTM.
Figure 4 shows the classiﬁcation accuracy for various percentages of selected text. We
observe that HardKuma outperforms the Bernoulli
model at each percentage of selected text. Hard-
Kuma reaches full-text baseline performance already around 40% extracted text. At that point,
it obtains a test score of 45.84, versus 42.22 for
Bernoulli and 47.4±0.8 for the full-text baseline.
8We tried to use Lagrangian relaxation for the Bernoulli
model, but this led to instabilities (e.g. all words selected).
Selected Text
Figure 4: SST validation accuracy for various percentages of extracted text. HardKuma (blue crosses) has
higher accuracy than Bernoulli (red circles) for similar
amount of text, and reaches the full-text baseline (black
star, 46.3 ± 2σ with σ = 0.7) around 40% text.
very negative
very positive
Figure 5: The number of words in each sentiment class
for the full validation set, the HardKuma (24% selected
text) and Bernoulli (25% text).
We wonder what kind of words are
dropped when we select smaller amounts of text.
For this analysis we exploit the word-level sentiment annotations in SST, which allows us to track
the sentiment of words in the rationale. Figure 5
shows that a large portion of dropped words have
neutral sentiment, and it seems plausible that exactly those words are not important features for
classiﬁcation. We also see that HardKuma drops
(relatively) more neutral words than Bernoulli.
Natural Language Inference
In Natural language inference (NLI), given a
premise sentence x(p) and a hypothesis sentence
x(h), the goal is to predict their relation y which
can be contradiction, entailment, or neutral. As
our dataset we use the Stanford Natural Language
Inference (SNLI) corpus .
We use the Decomposable Attention
model (DA) of Parikh et al. .9 DA does not
make use of LSTMs, but rather uses attention to
ﬁnd connections between the premise and the hy-
9Better results e.g. Chen et al. and data sets for
NLI exist, but are not the focus of this paper.
pothesis that are predictive of the relation. Each
word in the premise attends to each word in the
hypothesis, and vice versa, resulting in a set of
comparison vectors which are then aggregated for
a ﬁnal prediction. If there is no link between a
word pair, it is not considered for prediction.
Because the premise and hypothesis interact, it does not make sense to extract a rationale for the premise and hypothesis independently. Instead, we replace the attention between
premise and hypothesis with HardKuma attention.
Whereas in the baseline a similarity matrix is
softmax-normalized across rows (premise to hypothesis) and columns (hypothesis to premise) to
produce attention matrices, in our model each cell
in the attention matrix is sampled from a Hard-
Kuma parameterized by (a, b). To promote sparsity, we use the relaxed L0 to specify the desired
percentage of non-zero attention cells. The resulting matrix does not need further normalization.
With a target rate of 10%, the Hard-
Kuma model achieved 8.5% non-zero attention.
Table 3 shows that, even with so many zeros in the
attention matrices, it only does about 1% worse
compared to the DA baseline. Figure 6 shows an
example of HardKuma attention, with additional
examples in Appendix B. We leave further explorations with HardKuma attention for future work.
LSTM 
DA 
DA (reimplementation)
DA with HardKuma attention
Table 3: SNLI results (accuracy).
Figure 6: Example of HardKuma attention between a
premise (rows) and hypothesis (columns) in SNLI (cell
values shown in multiples of 10−2).
Related Work
This work has connections with work on interpretability, learning from rationales, sparse structures, and rectiﬁed distributions. We discuss each
of those areas.
Interpretability.
Machine learning research has
been focusing more and more on interpretability
 .
However, there are many
nuances to interpretability , and
amongst them we focus on model transparency.
One strategy is to extract a simpler, interpretable model from a neural network, though this
comes at the cost of performance. For example,
Thrun extract if-then rules, while Craven
and Shavlik extract decision trees.
There is also work on making word vectors
more interpretable.
Faruqui et al. make
word vectors more sparse, and Herbelot and Vecchi learn to map distributional word vectors
to model-theoretic semantic vectors.
Similarly to Lei et al. , Titov and McDonald extract informative fragments of text
by jointly training a classiﬁer and a model predicting a stochastic mask, while relying on Gibbs
sampling to do so. Their focus is on using the
sentiment labels as a weak supervision signal for
opinion summarization rather than on rationalizing classiﬁer predictions.
There are also related approaches that aim to
interpret an already-trained model, in contrast to
Lei et al. and our approach where the rationale is jointly modeled. Ribeiro et al. 
make any classiﬁer interpretable by approximating it locally with a linear proxy model in an
approach called LIME, and Alvarez-Melis and
Jaakkola propose a framework that returns
input-output pairs that are causally related.
Learning from rationales.
Our work is different from approaches that aim to improve classiﬁcation using rationales as an additional input
 . Instead, our rationales are latent and we are interested in uncovering them. We
only use annotated rationales for evaluation.
Sparse layers.
Also arguing for enhanced interpretability, Niculae and Blondel propose a
framework for learning sparsely activated attention layers based on smoothing the max operator. They derive a number of relaxations to max,
including softmax itself, but in particular, they
target relaxations such as sparsemax which, unlike softmax, are
sparse (i.e. produce vectors of probability values
with components that evaluate to exactly 0). Their
activation functions are themselves solutions to
convex optimization problems, to which they provide efﬁcient forward and backward passes. The
technique can be seen as a deterministic sparsely
activated layer which they use as a drop-in replacement to standard attention mechanisms. In contrast, in this paper we focus on binary outcomes
rather than K-valued ones. Niculae et al. 
extend the framework to structured discrete spaces
where they learn sparse parameterizations of discrete latent models. In this context, parameter estimation requires exact marginalization of discrete
variables or gradient estimation via REINFORCE.
They show that oftentimes distributions are sparse
enough to enable exact marginal inference.
Peng et al. propose SPIGOT, a proxy
gradient to the non-differentiable arg max operator.
This proxy requires an arg max solver
(e.g. Viterbi for structured prediction) and, like the
straight-through estimator , is
a biased estimator. Though, unlike ST it is efﬁcient for structured variables. In contrast, in this
work we chose to focus on unbiased estimators.
Rectiﬁed Distributions.
The idea of rectiﬁed
distributions has been around for some time. The
rectiﬁed Gaussian distribution ,
in particular, has found applications to factor analysis and approximate
inference in graphical models . Louizos et al. propose to stretch and
rectify samples from the BinaryConcrete (or GumbelSoftmax) distribution .
They use rectiﬁed variables
to induce sparsity in parameter space via a relaxation to L0. We adapt their technique to promote
sparse activations instead. Rolfe learns a
relaxation of a discrete random variable based on a
tractable mixture of a point mass at zero and a continuous reparameterizable density, thus enabling
reparameterized sampling from the half-closed interval [0, ∞). In contrast, with HardKuma we focused on giving support to both 0s and 1s.
Conclusions
We presented a differentiable approach to extractive rationales, including an objective that allows
for specifying how much text is to be extracted.
To allow for reparameterized gradient estimates
and support for binary outcomes we introduced
the HardKuma distribution. Apart from extracting rationales, we showed that HardKuma has further potential uses, which we demonstrated on
premise-hypothesis attention in SNLI. We leave
further explorations for future work.
Acknowledgments
We thank Luca Falorsi for pointing us to
Louizos et al. , which inspired the Hard-
Kumaraswamy distribution.
This work has received funding from the European Research Council (ERC StG BroadSem 678254), the European Union’s Horizon 2020 research and innovation programme (grant agreement No 825299,
GoURMET), and the Dutch National Science
Foundation (NWO VIDI 639.022.518, NWO VICI
277-89-002).