Machine learning with systematic density-functional theory calculations: Application
to melting temperatures of single and binary component solids
Atsuto Seko,1, ∗Tomoya Maekawa,1 Koji Tsuda,2, 3 and Isao Tanaka1, 4, 5
1Department of Materials Science and Engineering, Kyoto University, Kyoto 606-8501, Japan
2Minato Discrete Structure Manipulation System Project, ERATO,
Japan Science and Technology Agency, Sapporo 060-0814, Japan
3Computational Biology Research Center, National Institute of Advanced
Industrial Science and Technology (AIST), Tokyo 135-0064, Japan
4Center for Elements Strategy Initiative for Structure Materials (ESISM), Kyoto University, Kyoto 606-8501, Japan
5Nanostructures Research Laboratory, Japan Fine Ceramics Center, Nagoya 456-8587, Japan
 
A combination of systematic density functional theory (DFT) calculations and machine learning
techniques has a wide range of potential applications. This study presents an application of the
combination of systematic DFT calculations and regression techniques to the prediction of the
melting temperature for single and binary compounds. Here we adopt the ordinary least-squares
regression (OLSR), partial least-squares regression (PLSR), support vector regression (SVR) and
Gaussian process regression (GPR). Among the four kinds of regression techniques, the SVR provides
the best prediction. The inclusion of physical properties computed by the DFT calculation to a set
of predictor variables makes the prediction better. In addition, limitation of the predictive power is
shown when extrapolation from training dataset is required. Finally, a simulation to ﬁnd the highest
melting temperature toward the eﬃcient materials design using kriging is demonstrated. The kriging
design ﬁnds the compound with the highest melting temperature much faster than random designs.
This result may stimulate the application of kriging to eﬃcient materials design for a broad range
of applications.
PACS numbers: 64.70.dj, 89.20.Ff
INTRODUCTION
Computational material design based on data mining technique and high-throughput screening is a rapidly
growing area in materials science.1–11 Recent advances of
computational power and techniques enable us to carry
out density functional theory (DFT) calculations for a
large number of compounds and crystal structures systematically. When the large set of DFT calculation is
combined with machine learning techniques, the exploration of materials can be greatly enhanced. Using the
combination, meaningful information and pattern can be
extracted from existing data to make a prediction model
of a target physical property.
In this paper, we apply the combination of systematic
DFT calculations and several regression techniques to the
estimation of an approximated function describing experimental melting temperatures for single and binary component solids. So far, several theories and formulations
applicable to the prediction of the melting temperature
were proposed on the basis of physical considerations.
About a hundred years ago, Lindemann provided a wellknown model which explains the melting temperature
for single-component and simple ionic binary-component
solids.12 Lindemann assumed that the critical value of the
mean amplitude capable of keeping the atomic orderings
in a crystal is proportional to the bond distance between
atoms or ions. Based upon the assumption and the harmonic theory, a relationship for the melting temperature
Tm was derived as
where c, M, ΘD and V denote the proportionality constant, molecular mass, Debye temperature and molar volume, respectively. Guinea et al. proposed a linear relationship between the melting temperature and cohesive
energy for elemental metals based on the Debye model
and a binding theory of solid that they proposed.13 Also
for covalent crystals, a scaling theory was applied to
predict their melting temperatures.14 Since the theory
is made for covalent crystals, it is not directly applicable to compounds with other types of chemical bondings. Chelikowsky and Anderson demonstrated general
trends of melting temperatures in some 500 AB intermetallic compounds.15 They found a correlation between
the melting temperatures of the intermetallic compounds
and those of the elemental metals A and B.
Meanwhile, a machine learning technique was applied
to the prediction of the melting temperature for AB
suboctet compounds recently.16 They built a prediction
model of the melting temperature using experimental
melting temperatures of 44 suboctet AB compounds and
the regularized linear regression.
They adopted only
quantities of each constituent atom as predictor variables,
such as atomic number, the pseudopotential radii for s
and p orbitals and the heat of vaporization. However,
more accurate prediction models may be constructed by
feeding systematic DFT results for predictors. In addition, the use of more advanced regression technique than
the linear regression used in Ref. 16 may improve the
prediction.
In this study, we estimate prediction models applicable to a wide range of single and binary compounds
using systematic DFT calculations and advanced regression techniques.
The set of the compounds contains a wider range of compounds than that used in
the work of Ref.
We adopt four kinds of regression techniques, i.e.
ordinary least-squares regression
(OLSR), partial least-squares regression (PLSR),17–19
support vector regression (SVR)20–24 and Gaussian process regression (GPR).25 Results by the four regression
methods are compared.
Furthermore, one of the ultimate goals to use machine
learning techniques is to design materials automatically.
Material design can be formulated as a complicate process to optimize target physical properties.
Typically,
the objective functions of the target physical properties
cannot be deﬁned analytically from physical laws, hence
it is regarded as a “black-box”. Since the black-box functions can usually be supposed to be smooth, a regression
function from a limited number of samples can be used
as a surrogate.26 In a black-box optimization technique
called kriging, the measurements are designed to maximize the chance of discovering the optimal compounds.
As a case study, a simulation based on kriging for ﬁnding the compound with the highest melting temperature
based on kriging is here demonstrated.
METHODOLOGY
Multiple linear regressions
The common goal of regressions is to construct a model
that predicts a response variable from a set of predictor variables. The use of the multiple linear regressions
allows us to attempt this goal.
In the multiple linear
regressions, a linear model describes the linear relationship between a response variable and a set of predictor
variables. In the OLSR, the regression coeﬃcients are determined by minimizing the mean-squared error for observed data.
However, when the number of predictor
variables is larger than the number of observations, the
OLSR cannot be applied owing to the multicollinearity.
Some approaches for avoiding the multicollinearity exist. One is to eliminate some predictor variables. Another is to perform the principal component analysis
(PCA) of the predictor matrix and then use the principal components for the regression on the response variable. However, it is not guaranteed that the principal
components are relevant for the response variable. The
PLSR17–19 is an extension of the OLSR and combines
features of OLSR and PCA. By contrast to the PCA,
the PLSR extracts underlying factors from predictor variables that are relevant for the response variable. They
are called latent variables and described by linear combinations of the predictor variables.
By using a small
number of latent variables, it is possible to build a linear
prediction model from a large number of the predictor
variables with avoiding the multicollinearity.
The PLSR with a single response variable ﬁnds a set
of the latent variables that performs a simultaneous decomposition of X and y, where X and y are (N × m)
predictor matrix containing m predictor variables for N
observed data and N-vector of the response variable, respectively. In a PLSR model with H latent variables, the
predictor matrix is decomposed as
in the same fashion as the PCA, where T and P denote
(N ×H) score matrix and (m×H) loading matrix for X,
respectively. The score matrix is a collection of the latent
vectors and expressed as T = [t1, · · · , tH], where th is hth latent vector. The loading matrix is not orthogonal in
the PLSR opposite to the PCA. Similar to the predictor
matrix, the response variable is also decomposed as
where q is a vector with H components equivalent to the
product of a diagonal matrix and loadings for y.
Scores and loadings are obtained by an iterative procedure. The detailed procedure is shown in Table I. Firstly,
a pair of t1 and weight vector w1 with the relationship of
t1 = Xw1 are determined with the constraint that t⊤
maximized. Once the ﬁrst latent vector and loadings are
found, it is subtracted from X and y. This is repeated
until H-th latent vector, weight vector and loadings are
found. Using the weight matrix W = [w1, · · · , wH] obtained by the iterative procedure, the regression model
is written as
y = XW ∗q = Xb,
where W ∗has the equality of W ∗= W
Consequently, the linear regression coeﬃcient vector b
corresponds to W ∗q.
Nonlinear regressions
Support vector regression (SVR)
To approximate complex response functions, many
frameworks beyond the linear regression have been proposed.
SVR is a regression version of support vector
machines that constructs a nonlinear regression function
based on a kernel function.
{(x1, y1), · · · , (xN, yN)},
where xi and yi denote a
vector of predictor variables and the response variable.
Let w and b denote the weight vector and the bias
parameter, respectively. In ǫ-SVR, the response function
f(x) is modeled as
f(x) = w⊤φ(x) + b,
TABLE I. Algorithm for building PLSR model with a single
response variable. ph, qh are h-th components of P and q,
respectively.
Eh and fh are the residual for the predictor
matrix and that for the response variable, respectively.
Input: E0 = X, f0 = y
Output: W , q, T , P
for all h = 1, . . . , H do
Step 1: wh = ET
h−1fh−1/||ET
Step 2: th = Eh−1wh/(wT
Step 3: qh = f T
Step 4: ph = ET
Step 5: Eh = Eh−1 −thpT
Step 6: fh = fh−1 −qhth
where φ(x) maps x into a higher-dimensional space. We
deﬁne φ(x) in an implicit form using a kernel function
k(x, x′) as
k(x, x′) = φ(x)⊤φ(x′).
It has been proven that a mapping φ exists if and only
if the kernel function is positive semideﬁnite. A popular
choice of k includes the Gaussian kernel and the polynomial kernels.
Introducing non-negative slack variables ξi and ξ∗
allow for some errors, the optimization problem is stated
subject to
w⊤φ(xi) + b −yi ≤ǫ + ξi,
yi −w⊤φ(xi) −b ≤ǫ + ξ∗
i ≥0, i = 1, · · · , N
where C denotes a positive regularization parameter.
This corresponds to dealing with a so-called ǫ-insensitive
loss function |ξ|ǫ expressed by
|ξ| −ǫ otherwise.
The ǫ-insensitive loss function ignores errors less than ǫ.
The optimization problem can be solved more easily in
its dual formulation. In general, a standard dualization
method based on Lagrange multipliers is applied to the
optimization. The dual problem is stated as
2 (α −α∗)⊤K(α −α∗)
yi (αi −α∗
subject to
e⊤(α −α∗) = 0,
i ≤C, i = 1, . . . , N
where e is the vector of all ones, and α and α∗are Lagrange multipliers.
Here, K is called a kernel matrix
whose (i, j) component is k(xi, xj). Using the obtained
α and α∗, the response function is written as
i ) k(xi, x) + b.
Gaussian process regression (GPR)
The GPR is one of Bayesian regression techniques and
has been successfully employed to solve nonlinear estimation problems. A Gaussian process is a generalization of
the multivariate Gaussian probability distribution. The
prediction f(x∗) at a point x∗and its variance v(f∗) are
described by using the Gaussian kernel function as follows,
k (xi, xj) = exp
−|xi −xj|2
When the prior distribution has a variance of σ2, the
prediction is given as
f(x∗) = k⊤
∗(K + σ2I)−1y,
where k∗= [k(x1, x∗), · · · , k(xN, x∗)]⊤is the vector of
kernel values between x∗and the training examples, and
I is the unit matrix. The prediction variance is described
v(f∗) = k(x∗, x∗) −k⊤
∗(K + σ2I)−1k∗.
Kriging is built on the Gaussian processes. Figure 1
(a) shows a typical situation where several samples are
available. In kriging, we search for a next sampling point
where the chance of getting beyond the current best target property is optimal. To this aim, a Bayesian regression method such as a Gaussian process is applied, and
the probability distribution of the score at all possible
parameter values is obtained as illustrated in Fig. 1 (b).
Then, the next sampling point is determined as the one
with the highest probability of improvement.
We here apply the kriging to ﬁnd the compound with
the highest melting temperature from a pool of compounds. The procedure used in this study is organized
as follows.
1. An initial training set is ﬁrstly prepared by choosing compounds randomly.
Target property
Illustration of kriging. (a) A typical situation where
several samples are available. The current best target property is shown as a horizontal line. (b) The GPR is applied to
the available samples. The prediction of the target property
by the GPR is shown by the blue line. The probability distribution of the target property at all possible compounds is
also shown by orange closed circles.
2. Then a compound is selected using based on the
GPR. The compound is chosen as the one with the
largest probability of getting beyond the current
best value fcur. Since the probability is a monotonically increasing function of the z-score,
z(x∗) = (f(x∗) −fcur)/
the compound with the highest z-score is chosen
from the pool of unobserved materials.
3. The melting temperature of the selected compound
is observed.
4. The selected compound is added into the training
data set. Then the simulation goes back to step
Steps (2)-(4) are repeated until all data of melting temperatures are included in the training set.
RESULTS AND DISCUSSION
Prediction models are built from a data set containing experimental melting temperatures and predictors for
TABLE II. Physical properties of compounds adopted as predictor variables. They are computed by the DFT calculation.
Physical property
Nearest-neighbor pair distance
Cohesive energy
Bulk modulus
248 compounds. The melting temperatures of the 248
compounds range from room temperature to 3273 K as
shown in Appendix. The set of compounds do not contain transition metals to avoid complexity in the DFT
calculations.
In order to make prediction models, the compounds
are characterized by elemental information and simple
physical properties of the compounds. These features are
used as predictor variables. A key factor for constructing
accurate prediction models is to supply good predictor
variables.
We here adopted (1) cohesive energy, Ecoh,
(2) bulk modulus, B, (3) volume, V , and (4) nearestneighbor pair distance, rNN as the physical properties
of compounds as shown in Table II. They are systematically obtained by the DFT calculation. Besides the
physical properties computed by the DFT calculation,
ten kinds of elemental information are adopted, i.e., (1)
atomic number of elements A and B, ZA, ZB, (2) atomic
mass of elements A and B, mA, mB, (3) number of valence electrons of elements A and B, nA, nB, (4) periods
in periodic table of elements A and B, pA, pB, (5) groups
in periodic table of elements A and B, gA, gB, (6) van der
Waals radius of elements A and B, rvdw
, (7) covalent radius of elements A and B, rcov
B , (8) Pauling
electronegativity of elements A and B, χA, χB, (9) ﬁrst
ionization energy of elements A and B, IA, IB, and (10)
compositions of AxBy compound for elements A and B,
cA = x/(x + y), cB = y/(x + y). In practice, symmetric
forms of the elemental information are introduced so that
predictor variables become symmetric for the exchange
of atomic species in binary compounds.
The symmetric forms are shown in Table III. As a result, the total
number of predictor variables is 23.
The DFT computation of physical properties requires
the crystal structure for each compound. Candidates for
the crystal structure are taken from the Inorganic Crystal
Structure Database (ICSD). When ICSD database has a
unique crystal structure for a compound, the DFT calculation is carried out by using the unique crystal structure.
When ICSD database contains multiple crystal
structures for a compound, DFT calculations for all the
crystal structures are performed. The crystal structure
with the lowest energy is then adopted for computing the
physical properties.
The cohesive energy is computed by the DFT calculation using the formula normalized by the total number
TABLE III.
Symmetric forms of predictor variables composed of elemental information. The elemental information
are taken from Ref. 27.
Product form
Composition, c
Atomic number, Z
ZA + ZB (x6)
Atomic mass, m
mA + mB (x8)
Number of valence electrons, n
nA + nB (x10)
nAnB (x11)
gA + gB (x12)
gAgB (x13)
pA + pB (x14)
pApB (x15)
van der Waals radius, rvdw
Covalent radius, rcov
Electronegativity, χ
χA + χB (x20)
χAχB (x21)
First ionization energy, I
IA + IB (x22)
IAIB (x23)
of atoms, expressed as
Ecoh = (NAEatom
where NA and NB denote the numbers of atoms A and
B included in the simulation cell, respectively. Ebulk is
the total energy of compound at the equilibrium volume.
are the atomic energies of A and B,
respectively.
Here, the energy of an isolated atom in
a large cell (= 10 ˚A×10 ˚A×10 ˚A) is regarded as the
atomic energy. The bulk modulus B is evaluated using
the formula of
where V0 and V1 denotes the equilibrium volume and the
volume that is slightly diﬀerent from the equilibrium volume, respectively. P0 and P1 are the pressure at volumes
V0 and V1, respectively.
DFT calculations are performed by the projector
augmented-wave (PAW) method28,29 within the generalized gradient approximation (GGA)30 as implemented in
the VASP code.31,32 The total energies converge to less
than 10−2 meV. The atomic positions and lattice constants are relaxed until the residual forces become less
than 10−3 eV/˚A.
Regressions
Regressions are carried out using two kinds of predictor variable set. Predictor set (1) is composed only of
symmetric predictor variables of elemental information
as listed in Table III. Predictor set (1) contains no information obtained by the DFT calculation. Predictor
set (2) is composed of symmetric predictor variables of
elemental information and physical properties of compounds computed by the DFT calculation.
In order to estimate the prediction error, we divide the
data set into training and test data. A randomly-selected
quarter of the data set and the rest of the data set are
TABLE IV. CV scores and RMS errors for test data in OLSR,
PLSR, SVR and GPR.
CV score (K) RMS error for test data (K)
Predictor set (1)
Predictor set (2)
regarded as the test and training data, respectively. This
is repeated thirty times and then averages of 10-fold cross
validation (CV) scores and the root-mean-square (RMS)
errors between predicted and experimental melting temperatures for test data are evaluated.
We ﬁrst perform the OLSR for building prediction
models. Table IV shows the CV scores of the OLSR models. When using predictor sets (1) and (2), we construct
prediction models with the CV scores of 473 K and 293
K, respectively. The prediction is improved by considering physical properties of compounds computed by the
DFT calculation as predictor variables. We then perform
the PLSR using two kinds of predictor variable sets. The
PLSR is performed using R package.19 The accuracy of
the PLSR is mainly controlled by the number of the latent factors. The CV scores converge at the number of
latent factors of 18 and 20 using predictor set (1) and
using predictor set (2), respectively. Table IV shows the
CV scores of the optimized PLSR models. When predictor sets (1) and (2) are used, we construct prediction
models with the CV scores of 476 K and 291 K, respectively. They are almost the same as the CV scores of
OLSR models because the OLSR models are made with
less uncertainty.
The RMS errors for test data of the OLSR models using predictor sets (1) and (2) are 472 and 306 K, respectively, which are almost the same as the CV scores. Also
in the case of the PLSR, the prediction errors are almost
the same as the CV scores. Figure 2 shows relationships
of predicted and experimental melting temperatures using predictor sets (1) and (2) in the OLSR. This is obtained from one of the thirty kinds of random divisions
of the data set. As can be seen in Fig. 2, the prediction errors for the training and test data are comparable
in the OLSR models using both predictor sets (1) and
(2) since the CV score and RMS error for test data are
also comparable. The deviation from the straight line, on
which the experimental and predicted melting temperatures are equal, in the OLSR model using predictor set
(1) is larger than that in the OLSR model using predictor
Predicted melting temperature (K)
Experimental melting temperature (K)
(a) Without DFT calculations
(b) With DFT calculations
Melting temperature for 248 compounds predicted
by the OLSR performed by (a) predictor set (1) and (b) predictor set (2). This is obtained from one of the thirty kinds of
random divisions of the data set into training and test data.
Melting temperatures of training and test data are shown by
open and closed circles, respectively. On the broken line, experimental and predicted melting temperatures are exactly
set (2), corresponding to values of the CV score and the
RMS error for test data.
To ﬁnd important predictors for explaining the melting temperature, a selection of predictors within the
OLSR using predictor set (2) is then carried out.
adopt a stepwise regression method with the bidirectorial
elimination33 based on the minimization of the Akaike’s
information criterion (AIC).34 As a result, the best prediction model with the minimum AIC is composed of ten
predictors and has the RMS error of 295 K. Figure 3 (a)
shows the RMS errors for prediction models only with
up to ﬁve predictors obtained during the stepwise regression. The prediction model with ﬁve predictors shows
the RMS error of 320 K which is close to that of the
best prediction model. The selected ﬁve predictors are
Ecoh, χA + χB, B, cAcB and rNN. Three of the ﬁve predictors are physical properties of compounds computed
by the DFT calculation. Figure 3 (b) shows the standardized regression coeﬃcients of the prediction model
with the ﬁve predictors. The earlier the predictors are
selected by the stepwise regression, the larger the absolute value of the standardized regression coeﬃcients for
the predictors are. The absolute value of the standardized regression coeﬃcient for Ecoh, which is the ﬁrst selected by the stepwise regression, is the largest among
the coeﬃcients for the ﬁve predictors, hence it can be
considered that Ecoh contributes the most to the prediction of the melting temperature. The importance of the
predictors for explaining the melting temperature can be
seen in the correlations between the melting temperature
and predictors. Figure 3 (c) shows the correlation coeﬃcients between the melting temperature and predictors.
The correlation coeﬃcients of Ecoh and B, which are selected by the stepwise regression, are positively large. On
the other hand, V is not selected by the stepwise regression in spite of its negatively-large correlation coeﬃcient.
Standardized regression coefficient
RMS error (K)
Correlation coefficient
(a) RMS error of ﬁve prediction models with up to
ﬁve predictors selected during the stepwise regression method.
The predictor sets of models (1)-(5) are composed of Ecoh,
{Ecoh, χA+χB}, {Ecoh, χA+χB, B}, {Ecoh, χA+χB, B, cAcB}
and {Ecoh, χA + χB, B, cAcB, rNN}. (b) Standardized regression coeﬃcients of the prediction model (5) for the ﬁve predictors. (c) Correlation coeﬃcients between the melting temperature and predictors. Orange solid bars show the correlation
coeﬃcients for the predictors of model (5).
This may be ascribed by the fact that the correlations
between V and the other physical properties computed
by the DFT calculation are large.
Next, we perform the SVR and GPR using predictor
sets (1) and (2). The SVR and GPR are performed using R package.35,36 The Gaussian kernel is adopted as the
kernel function in the SVR. The SVR with the Gaussian
kernel has two parameters which control the accuracy of
the prediction model, i.e., the variance of the Gaussian
kernel and the regularization parameter. Therefore, the
two parameters are optimized based on the minimization
of the CV score. Candidates of them are set to 10−3,
Predicted melting point (K)
Experimental melting point (K)
(a) Without DFT calculations
(b) With DFT calculations
The same as in Figure 2 but for SVR models.
10−2, 10−1, 100, 101, 102 and 103. By performing regressions for all combinations of the candidates, the optimal
values of the two parameters are determined.
Table IV shows the CV scores of the optimized SVR
and GPR models.
Using the SVR, we get prediction
models with the CV scores of 376 K and 265 K using
predictor sets (1) and (2), respectively. Using the GPR,
prediction models with the CV scores of 492 K and 334
K are obtained using predictor sets (1) and (2), respectively. As is the case in the OLSR, the prediction of the
melting temperature is improved by considering physical
properties of compounds computed by the DFT calculation as predictors. In addition, when using predictor set
(1), the SVR model is the best among the four kinds of
regression models. On the other hand, when using predictor set (2), the use of the SVR does not improve the
prediction well compared to the linear regressions.
Figure 4 and 5 show relationships of predicted and experimental melting temperatures in the SVR and GPR,
respectively. They are obtained from one of the thirty
kinds of random divisions of the data set, the same as
those in the OLSR. Then the RMS errors for test data
are also estimated. The RMS errors of SVR models using
predictor sets (1) and (2) are 364 and 262 K, respectively,
which are very close to the CV scores. The RMS errors
of GPR models using predictor sets (1) and (2) are 481
and 306 K, respectively, which are also close to the CV
scores. Among the four kinds of regression techniques,
the SVR provides the prediction models with the best
CV scores and RMS errors. This is consistent with the
fact that nonlinear regressions are widely accepted to be
useful for estimating complex response functions.
Prediction
In this section, we examine the predictive power of the
melting temperature of compounds which are missing in
the dataset of Table V (hereafter called dataset I). Some
nitrides are known to decompose releasing nitrogen gas
at a temperature below the melting point under the ambient pressure. The decomposition temperature is shown
instead of the melting temperature in some databases.
Experimental melting temperature (K)
(a) Without DFT calculations
(b) With DFT calculations
Predicted melting temperature (K)
The same as in Figure 2 but for GPR models.
Elemental carbon is another example whose melting temperature under the ambient pressure is not well established by experiments. A series of nitrides and elements
of Group 14 (carbon group) are therefore selected for the
targets of the prediction.
Figure 6 shows melting temperatures for nitrides and
Group 14 elements predicted with dataset I by the SVR
model and the OLSR model, which lead to RMS errors
for test data of 262 K and 295 K. Ten predictors are
optimized by the stepwise method in the OLSR model.
The error bars shown in Fig. 6 correspond to 95% con-
ﬁdence intervals in the OLSR model. The melting temperatures predicted by SVR and OLSR models do not
diﬀer so much for most of compounds included in the
dataset I. They are also close to experimental melting
temperatures. The largest error can be found for AlN.
The reason for the poor prediction may be ascribed to
an experimental error rather than problems in the prediction model, since the experimental data in literature
is widely scattered. It is 3273.15 K in dataset I, while
other databases report 2473.15 K37 and 3473 K.38
Meanwhile, missing compounds in dataset I can be
classiﬁed into two groups according to the width of the
error bar in Fig.
For compounds with narrow error bars, the predictions by SVR and OLSR models are
nearly the same, which is similar to those compounds in
dataset I. In such a compound, the melting temperature
is expected to be predictable with the accuracy comparable to that for compounds in dataset I. We collected
experimental melting temperatures of compounds that
are not included in dataset I and made a new dataset II.
They are estimated from an extrapolation of experimental solid-liquid phase boundary to the ambient pressure
in a pressure-temperature phase diagram.
As can be seen in Fig. 6, the melting temperatures predicted by SVR and OLSR models using dataset I agree
well with the experimental data in dataset II when the error bar of the prediction is narrow, as for Mg3N2. On the
other hand, the prediction is less reliable for compounds
with wide error bars such as C and BN. In contrast to
compounds with narrow error bars, the melting temperature predicted by the OLSR model diﬀers greatly from
those predicted by the SVR model. The prediction with
Melting temperature (K)
Dataset II
Experiment
Prediction
Melting temperatures of nitrides and Group 14 elements predicted by the SVR (blue closed squares) and the
OLSR with ten predictors optimized by the stepwise method
(red closed circles) along with experimental melting temperatures (gray closed bars) in dataset I. The error bars indicate
95% conﬁdence intervals in the OLSR model. Open bars show
melting temperatures of Mg3N2, BN, GaN and C obtained
by extrapolation using the solid-liquid phase boundaries in
pressure-temperature phase diagrams (dataset II).
the wide error bars requires an extrapolation from the
dataset I. As demonstrated in Sec III B, both of the cohesive energies and bulk moduli of the compounds are
important predictors in the OLSR model. Since both C
and BN have larger cohesive energy and bulk modulus
than those of compounds in the dataset I, their melting temperatures need to be predicted by the extrapolation. Hence, the predictive power for these compounds
becomes poor. Inclusion of these new data into the training dataset should decrease the uncertainty of the prediction models, thereby improving the predictive power
drastically.
Finally, we perform a simulation for ﬁnding the compound with the highest melting temperature using the
kriging. Here we start the kriging from a data set of 12
compounds. For comparison, a simulation based on the
random selection of compounds is also performed. Both
the kriging and random simulations are repeated thirty
times and the average number of compounds required for
ﬁnding the compound with the highest melting temperature is observed. Figure 7 shows the highest melting temperature among observed compounds during one of thirty
times kriging and random trials. As can be seen in Fig. 7,
the compound with the highest melting temperature can
be found much eﬃciently using the kriging. The average
number of observed compounds required for ﬁnding the
compounds with the highest melting temperature over
Highest melting Temperature (K)
Number of observed compounds
Highest melting temperature among the observed
compounds in simulations for ﬁnding the compound with the
highest melting temperature based on the kriging and random
compound selections.
thirty times trials using the kriging and random compound selections are 16.1 and 133.4, respectively, hence
kriging substantially improved the eﬃciency of discovery. This is a very encouraging result for application of
kriging to various materials design problems.
CONCLUSION
In summary, we have presented applications of regression techniques to the prediction of the melting temperature of single and binary compounds. Prediction models
are built by four kinds of regression techniques.
found that the SVR prediction model has the highest
predictive power among the four regressions. Also, the
prediction models are much improved by considering the
physical properties computed by the DFT calculation as
predictor variables. The best prediction model has been
constructed by the SVR using the predictor variable set
composed of elemental information and physical properties computed by the DFT calculation. It has the CV
score of 265 K and the RMS error for test data of 262
K. In addition to the construction of prediction models,
limitation of the predictive power is shown when extrapolation from training dataset is required. We have also
demonstrated simulations to ﬁnd the compound with the
highest melting temperature. The simulations are based
on kriging that stands on the GPR. The average number of compounds required for ﬁnding the optimal compound over thirty-times kriging compound selection is
16.1, which are much smaller than that in random compound selections of 133.4, hence the kriging discovers the
optimal compound much eﬃciently. This result strongly
supports that the kriging facilitates eﬃcient discovery of
optimal materials.
ACKNOWLEDGMENTS
This study is supported by a Grant-in-Aid for Scientiﬁc
Research (A) and a Grant-in-Aid for Scientiﬁc Research
on Innovative Areas ”Nano Informatics” (grant number
25106005) from Japan Society for the Promotion of Science (JSPS).
∗ 
1 S. Curtarolo, G. L. W. Hart, M. B. Nardelli, N. Mingo,
S. Sanvito, and O. Levy, Nature Materials 12, 191 .
2 G. Ceder, MRS Bulletin 35, 693 .
3 S. Curtarolo, D. Morgan, K. Persson, J. Rodgers,
G. Ceder, Phys. Rev. Lett. 91, 135503 .
4 C. C. Fischer, K. J. Tibbetts, D. Morgan, and G. Ceder,
Nature Materials 5, 641646 .
J. Mater. Sci. 47, 7317 .
6 K. Rajan, Annu. Rev. Mater. Res. 38, 299 .
Phys. Rev. Lett. 108, 068701 .
8 J. Greeley, T. F. Jaramillo, J. Bonde, I. Chorkendorﬀ, and
J. K. Nørskov, Nature Materials 5, 909 .
Energy Environ. Sci. 5, 5814 .
10 W. Setyawan, R. M. Gaume, S. Lam, R. S. Feigelson, and
S. Curtarolo, ACS Comb. Sci. 13, 382 .
I. Kishida, K. Shitara, C. A. J. Fisher, H. Moriwake, and
I. Tanaka, Adv. Energy Mater. 3, 980 .
12 F. A. Lindemann, Phys. Z. 11, 609 .
13 F. Guinea, J. H. Rose, J. R. Smith, and J. Ferrante, Appl.
Phys. Lett. 44, 53 .
14 J. A. Van Vechten, Phys. Rev. Lett. 29, 769 .
15 J. R. Chelikowsky and K. E. Anderson, J. Phys. Chem.
Solids 48, 197 .
16 Y. Saad, D. Gao, T. Ngo, S. Bobbitt, J. R. Chelikowsky,
and W. Andreoni, Phys. Rev. B 85, 104104 .
17 S. Wold, M. Sj¨ostr¨om, and L. Eriksson, Chemom. Intell.
Lab. Sys. 58, 109 .
18 V. E. Vinzi and R. G, WIREs Comput. Stat. 5, 1 .
19 B. Mevik and R. Wehrens, J. Stat. Softw. 18, 1 .
20 V. Vapnik, The Nature of Statistical Learning Theory
 .
21 V. Vapnik, Statistical Learning Theory .
23 C. M. Bishop, Pattern Recognition and Machine Learning
 .
24 C. C. Chang and C. J. Lin, ACM Trans. Intell. Syst. Technol. 2, 27 .
25 C. E. Rasmussen and C. K. I. Williams, Gaussian Processes
for Machine Learning .
26 D. Jones, J. Global Optim. 21, 345 .
27 W. M. Haynes, CRC Handbook of Chemistry and Physics,
92nd ed. .
28 P. E. Bl¨ochl, Phys. Rev. B 50, 17953 .
29 G. Kresse and D. Joubert, Phys. Rev. B 59, 1758 .
30 J. P. Perdew, K. Burke,
and M. Ernzerhof, Phys. Rev.
Lett. 77, 3865 .
31 G. Kresse and J. Hafner, Phys. Rev. B 47, 558 .
32 G. Kresse and J. Furthm¨uller, Phys. Rev. B 54, 11169
Modern Applied Statistics with S,
 .
34 H. Akaike, in Second international symposium on information theory pp. 267–281.
Dimitriadou,
Weingessel,
e1071: Misc Functions of the Department of Statistics (e1071), TU W
 , R package version 1.6-1.
36 A. Karatzoglou, A. Smola, K. Hornik,
and A. Zeileis,
Journal of Statistical Software 11, 1 .
37 C. Friedrich, G. Berg, E. Broszeit,
and C. Berger,
Mater. Sci. Eng. Technol. 28, 59 .
38 O. Ambacher, Journal of Physics D: Applied Physics 31,
2653 .
Appendix A: Melting temperatures of single and
binary component solids
Table V shows the melting temperatures of single and
binary component solids in the data set.
TABLE V. Melting temperatures of 248 AxBy binary compounds included in the data set, quoted from Ref. 27.
Melting temp. (K)
Melting temp. (K)
Melting temp. (K)
Melting temp. (K)