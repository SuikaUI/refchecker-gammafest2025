Received September 10, 2019, accepted September 29, 2019, date of publication October 14, 2019,
date of current version October 28, 2019.
Digital Object Identifier 10.1109/ACCESS.2019.2947359
Feature Learning Viewpoint of Adaboost
and a New Algorithm
FEI WANG1,2, ZHONGHENG LI
1,2, FANG HE
3, RONG WANG
4, (Member, IEEE),
WEIZHONG YU
1,2, AND FEIPING NIE
4, (Member, IEEE)
1National Engineering Laboratory for Visual Information Processing and Applications, Xi’an Jiaotong University, Xi’an 710049, China
2Institute of Artiﬁcial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an 710049, China
3Xi’an Research Institute of Hi-Tech, Xi’an 710025, China
4Center for Optical Imagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi’an 710072, China
Corresponding author: Feiping Nie ( )
This work was supported in part by the National Natural Science Foundation of China (61603291, 61772427, 61751202), National Major
Science and Technology Projects of China grant (2018ZX01008103), Natural Science Basic Research Plan in Shaanxi Province of China
(2018JM6057), and the Fundamental Research Funds for the Central Universities.
The AdaBoost algorithm has the superiority of resisting overﬁtting. Understanding the
mysteries of this phenomenon is a very fascinating fundamental theoretical problem. Many studies are
devoted to explaining it from statistical view and margin theory. In this paper, this phenomenon is illustrated
by the proposed AdaBoost+SVM algorithm from feature learning viewpoint, which clearly explains the
resistance to overﬁtting of AdaBoost. Firstly, we adopt the AdaBoost algorithm to learn the base classiﬁers.
Then, instead of directly combining the base classiﬁers, we regard them as features and input them to
SVM classiﬁer. With this, the new coefﬁcient and bias can be obtained, which can be used to construct
the ﬁnal classiﬁer. We explain the rationality of this and illustrate the theorem that when the dimension of
these features increases, the performance of SVM would not be worse, which can explain the resistance to
overﬁtting of AdaBoost.
INDEX TERMS AdaBoost, feature learning, overﬁtting, SVM.
I. INTRODUCTION
The Adaboost algorithm , , which learns a ‘‘strong’’
classiﬁer by voting the weighted predictions of a set of
‘‘weak’’ learners (slightly better than random guessing)
 , is one of the most inﬂuential classiﬁcation algorithms
 – . The excellent performance has been demonstrated
both on benchmark datasets and real application , – .
According to Occam’s razor , when a classiﬁer was
trained too complex, the performance of it would be even
worse rather than better. This phenomenon is called overﬁtting, which means that the trained model is so adaptable to the
training data that it would exaggerate the slight ﬂuctuations in
the training data, leading to poor generalization performance
 , . However, the AdaBoost algorithm has the superiority of resisting overﬁtting, which has been observed by
many researches – . Understanding the mysteries of
this phenomenon about AdaBoost algorithm is a fascinating
fundamental theoretical problem , . Many studies are
The associate editor coordinating the review of this manuscript and
approving it for publication was Xinyu Du
devoted to explaining the success of AdaBoost, which can be
divided into statistical view and margin theory .
In the statistical view, great efforts were made to illustrate
the success of AdaBoost algorithm. Friedman et al. 
utilized the well-known statistical principles to understand
AdaBoost that AdaBoost can be viewed as an additive model
optimizing the exponential loss function by the forward stagewise algorithm. Besides, many boosting-style algorithms
were proposed with optimizing the potential loss functions in
a gradient decent way , , . Inspired by this optimal
method, some boosting-style algorithms and their variants
that were consistent to Bayes’s under different conditions
were presented , – . However, the biggest problem
of the statistical view is that these algorithms do not explain
well why AdaBoost is resistant to overﬁtting , .
The margin theory is another direction to explain this
phenomenon. Schapire et al. were the ﬁrst ones to use
the margin theory. Generally speaking, the margin of an
example associated with the classiﬁer is a measuring standard
of the classiﬁcation ability . Schapire et al. demonstrated that AdaBoost model can produce a good margin
distribution which is the key to the success of AdaBoost.
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see 
VOLUME 7, 2019
F. Wang et al.: Feature Learning Viewpoint of Adaboost and a New Algorithm
Soon after that, Breiman put a doubt on this margin
explanation. He proposed a boosting-type algorithm named
arc-gv which directly maximizes the minimum margin for
the generalization error of a voting classiﬁer. In the experiments, arc-gv can generate larger minimum margin than
AdaBoost, but it brought higher generalization error. Breiman
concluded that neither the margin distribution nor the minimum margin has inﬂuence to the generalization error. Later,
Reyzin and Schapire found that, amazingly, Breiman
had not controlled model complexity well in the experiments. They repeated Brieman’s experiments using decision
stumps with two leaves. The results showed that arc-gv
was with larger minimum margin, but worse margin distribution. Therefore, a convincing explanation is urgently
needed .
To support the margin theory, Wang et al. proved a
bound in terms of a new margin measure called the Equilibrium margin (Emargin). The Emargin bound was uniformly sharper than Breiman’s minimum margin bound. The
results suggested that the minimum margin may be not crucial for the generalization error, and a small empirical error
at Emargin implied a smaller bound of the generalization
error. Gao and Zhou proposed the kth margin bound to
defend the margin theory against Breiman’s doubt by a series
mathematical derivation. This model was uniformly tighter
than Breiman’s as well as Schapire’s bounds and considered
the same factors as Schapire et al. and Breiman. Zhou 
proposed the Large margin Distribution Machine (LDM) to
achieve a better generalization performance by optimizing the
margin distribution. The margin distribution was characterized by the statistic margin mean and statistic variance. The
margin mean was tried to be maximum, and at the same time,
the margin variance was tried to be minimum. This method
realized satisfactory results. However, completely explaining
AdaBoost’s resistance to overﬁtting is still difﬁcult.
In this paper, we illustrate the resistance to overﬁtting
of AdaBoost from feature learning viewpoint and using the
SVM classiﬁer to explain it. SVM classiﬁer is very useful
for its clear principles and competitive accuracy – .
We regard the results of base classiﬁers of AdaBoost as
features and input them to SVM and explain the rationality
of doing this. When the iterations of AdaBoost increase,
the features’ dimension increases. We illustrate that the
margin of SVM (not the margin of AdaBoost itself) will
not be smaller when the features’ dimension increase. This
implies that the performance of our AdaBoost+SVM model
will improve when the iterations increase, which can directly
and easily explain the resistance to overﬁtting of AdaBoost
rather than the complex proof.
The rest of this paper is organized as follows. In Section II,
we have a brieﬂy survey on the related work of the AdaBoost
Algorithm. In Section III, we present our AdaBoost+SVM
model. In Section IV, we validate our methods on different
datasets. In Section V, we discuss the results of our experiments and the signiﬁcance of our method. In Section VI, we
come to a conclusion.
II. RELATED WORK
In this section, we brieﬂy review the general AdaBoost algorithm and the popular theoretical explanation to AdaBoost
from the view of margin theory.
A. ADABOOST ALGORITHM
AdaBoost algorithm is one of boosting classiﬁcation algorithms which can boost a group of ‘‘weak’’ classiﬁers to a
‘‘strong’’ classiﬁer. These algorithms usually ﬁrst use a base
classiﬁed algorithm whose classiﬁcation ability is just better
than random guessing to train a base classiﬁer from the initial
training samples. Then adjust the sample weight according
to the result of the base classiﬁer, which makes the samples
that was classiﬁed incorrectly be paid more attention to. And
then the adjusted samples are used to train a next base learner.
After iterations, weighted are added to these base learners to
form the ﬁnal classiﬁer. Next is the description of AdaBoost
algorithm.
Let S = {(x1, y1), · · · , (xi, yi), · · · , (xn, yn)} denote the
training samples set in the binary classiﬁcation situation.
yi ∈Y = {−1, +1} is the class label associated with xi.
AdaBoost algorithm is based on the additive model, which
is the linear combination of the base classiﬁers ht(x):
where t = {1, · · · , T} denotes the iteration number, ht(x) are
the base classiﬁers trained from base classiﬁcation algorithm
L whose classiﬁed ability is just better than random guessing
and αt are the weight coefﬁcients.
In Eq.(1), ht(x) are learned from base classiﬁcation algorithm L based on the training sample with the distribution Dt
at t iteration. The shape of Dt is a normalized weight vector.
ht(x) : X →{−1, +1}.
The distribution Dt denotes the weight of each instance in S
at t iteration. D1 is composed as
i = 1, 2, · · · , n,
and Dt+1 are composed as
Dt+1(i) = Dt(i)
exp(−αtyiht(xi)),
i = 1, 2, · · · , n,
where Zt are the normalization factors and are calculated as
Dt(i) exp(−αtyiht(xi)).
From Eq.(4) and Eq.(5) we know that Dt+1 are adjusted
from Dt. Thus the samples which are classiﬁed incorrectly in
ht(x) will have higher weights in t + 1 iteration.
Given training set S and sample weight Dt, the object of
ht(x) is to minimize the classiﬁcation error ϵt. ϵt is calculated
ϵt = P[ht(xi) ̸= yi] =
Dt(i)I[ht(xi) ̸= yi],
VOLUME 7, 2019
F. Wang et al.: Feature Learning Viewpoint of Adaboost and a New Algorithm
where P[·] denote the probability and I[·] denote the logic
In Eq.(1), αt measures the importance of ht(x) in the ﬁnal
classiﬁer and are calculated in the following way.
2 ln(1 −ϵt
From Eq.(6) and Eq.(7) we can know that when ϵt < 0.5,
αt > 0. And αt would increase with ϵt decrease. In fact,
AdaBoost minimizes the exponential loss function in this
process .
We can also notice that in AdaBoost algorithm, ϵt must
smaller than 0.5. In fact, the error rate of a binary classiﬁer
is always not bigger than 0.5. For binary classiﬁcation problems, if the error rate ϵ of weak classiﬁer
is bigger than 0.5, we can use the classiﬁer
to replace h(x), which makes the error rate ϵc convert to 1−ϵ.
Then the ϵc is smaller than 0.5. This illustrates that ϵt would
always be smaller than 0.5 unless equal to 0.5. But it is almost
impossible that the error rate of a classiﬁer happens to be 0.5.
After continuous iteration, the ﬁnal classiﬁer is
F(x) = sign(f (x)) = sign(
This algorithm is summarized in Algorithm 1.
Algorithm 1 AdaBoost Algorithm
Training set S = {(x1, y1), (x2, y2), · · · , (xn, yn)};
Base classiﬁcation algorithm L;
Number of learning rounds T.
Initialize D1 = (1
n, · · · , 1
n, · · · , 1
for t = 1, · · · , T:
1. Using the base classiﬁcation algorithm L and current
weight Dt to learn the base classiﬁer ht(x) by minimizing the classiﬁcation error ϵt deﬁned in Eq. (6);
2. Calculating the coefﬁcient αt based on Eq. (7);
3. Updating the weight Dt+1 by Eq. (4);
Combining the obtained ht(x) according to Eq. (10) to
complete the ﬁnal classiﬁer.
The ﬁnal classiﬁer F(x).
B. INTUITION OF THE MARGIN THEORY
AdaBoost is one of the most inﬂuential and successful
classiﬁcation algorithms. However, the mystery of the phenomenon of its resistance to overﬁtting attracted many scholars working on it. A theory which is intuitive to explain
this phenomenon is the margin theroy. That is although the
training error reaches zero, the margin of AdaBoost will
increase along with the iterations increase.
Schapire et al. were the ﬁrst ones to use the margin
theory to explain this overﬁtting phenomenon. It is obvious
that the classiﬁcation error occurs when yf (x) ≤0. The
margin theory calls yf (x) as the margin for (x, y) with respect
to f , since the bigger yf (x) means the more conﬁdence on the
classiﬁcation result. Use PD[·] to refer as the probability with
respect to distribution D, and PS[·] to denote the probability
with respect to uniform distribution over the sample S. They
ﬁrstly proved the following theorem to bound the generalization error of the voting classiﬁer:
Theorem 1: Let S be a sample of n examples chosen
independently at random according to D. Assume that the
base hypothesis space H and δ > 0. Then with probability at
least 1−δ over the random choice of the training set S, every
weighted averange function f satisﬁes the following bound
for all θ > 0:
PD [yf (x) ≤0] ≤PS [yf (x) ≤θ]
log n log |H|
If H is ﬁnite, then
PD [yf (x) ≤0] ≤PS [yf (x) ≤θ]
d log2(n/d)
where d is the VC dimension of H.
This theorem illustrates that if a voting classiﬁer generates
a good margin distribution, then the generalization error is
also small.
Then they propose that if θ is not too large, the fraction
of training examples for which yf (x) ≤θ decreases to zero
exponentially fast with the number of base hypotheses.
Theorem 2: Suppose the base learning algorithm, when
called by AdaBoost, generates hypotheses with weighted
training errors ϵ1, · · · , ϵT . Then for any θ,
PS [yf (x) ≤θ] ≤2T
(1 −ϵt)1+θ.
Assume that, for all t, ϵt ≤1/2−γ for some γ > 0, the upper
bound in Eq. 13 can simplify to:
(1 −2γ )1−θ(1 + 2γ )1+θ
If θ < γ , it is easy to ﬁnd that the expression inside the parentheses is smaller than 1 so that the probability that yf (x) < θ
VOLUME 7, 2019
F. Wang et al.: Feature Learning Viewpoint of Adaboost and a New Algorithm
decreases exponentially fast with T. That is to say with the
T increase, AdaBoost can provide better margin distribution,
which seems to explain the resistance to overﬁtting. This
explanation is quite intuitive.
Several years later, margin theory was queried by
Breiman . He considered that according to the margin
theory, the performance would be better if directly maximize the minimum margin. Therefore he designed the arc-gv
algorithm and proved that the generalization bound is tighter
than Eq.(11). Theoretically, arc-gv should perform better than
AdaBoost. However, the experiments results show that arc-gv
does produce uniformly larger minimum margin but the test
error increases. Thus Breiman concluded that the margin
theory was in serious doubt.
Afterwards, Reyzin and Schapire found that Breiman
had not controlled model complexity well in the experiments. They repeated Brieman’s experiments using decision
stumps with two leaves. The results showed that arc-gv was
with larger minimum margin, but worse margin distribution.
Reyzin and Schapire declared that margin distribution was
also important in this problem. But how to evaluate the margin distribution and how the margin distribution affect the
generalization error was not clearly explained.
Later works like , , attempted to prove tighter
generalization bound and explain the inﬂuence of margin
and margin distribution on the generalization error. However,
the certain relationship of the margin yf (x) of AdaBoost
itself with the iteration number T is still not clearly from
the works above. That is these works cannot directly explain
the resistance to overﬁtting of AdaBoost when the iteration
number T increases even after the training error reaches 0.
In next section, we will introduce our AdaBoost+SVM
model, which can give the certain relationship of iteration
number T and the SVM margin to explain the resistance to
overﬁtting of AdaBoost directly.
III. FEATURE LEARNING VIEWPOINT
In this section, we propose our AdaBoost+SVM model to
explain the resistance to overﬁtting of AdaBoost from the
feature learning viewpoint and explain the rationality of doing
Freund and Schapire have proved that the training error
of AdaBoost decreases exponentially fast constantly during
the learning process. There is a theorem as follows:
Theorem 3: The training error of AdaBoost will always
reach 0 since the iterations increase.
This theorem comes from the following equation:
I (F (xi) ̸= yi) ≤exp
where γt = 0.5 −ϵt, T denotes the number of iteration.
Since ϵt is always smaller than 0.5, 0 < γt ≤0.5.
Let ∀γt, 0 < γ ≤γt,
I (F (xi) ̸= yi) ≤exp
Then from Eq.(16) we can easily know that the error of
AdaBoost will be reduced at an exponential rate and always
reach 0 when iterations are enough. Based on this theorem,
we will propose our model to view AdaBoost from the feature
learning point next.
A. ADABOOST+HARD MARGIN SVM
The boosting part
αtht (x) in ﬁnal classiﬁer of AdaBoost
given in equal (10) can be rewritten as
f (x) = [α1, α2, · · · , αT ]
In Eq.(17), we can regard the right vector
z(x) = [h1 (x) , h2 (x) , · · · , hT (x)]T
as a feature of sample x that learning from AdaBoost. Then
α = [α1, α2, · · · , αT ] is the weight vector of this feature
provided by AdaBoost algorithm. In other words, we can
regard the process x →z(x) as a Rn →RT spatial mapping.
From this view, α can be viewed as a hyperplane in the feature
space RT and divide the features into two categories.
However, α may not be the best hyperplane for classifying
the features. According to Theorem 2, the training error of
AdaBoost will always reach 0. And the training error of
AdaBoost reaches 0 means the feature z(x) ∈RT can be
linearly separated into two categories. Another fact is that
SVM algorithm can provide the separating hyperplane with
largest margin in linearly separable problem, which means an
excellent solution . Based on this, using SVM algorithm
to calculate the hyperplane in the feature space RT to replace
α should be a better choice. This is the theoretical basis of our
model. The algorithm will be described next.
Firstly, given the training sample S, get the feature function z(x) by AdaBoost algorithm according to Eq.(18). Then,
according to the following objective function of SVM:
s.t. yi(βz(xi) + b) −1 ≥0,
i = 1, 2, · · · , n,
we can calculate the optimal weight vector β and bias b. Last,
the ﬁnal classiﬁer can be learned by
F(x) = sign(f (x)) = sign(
βtht(x) + b).
We illustrate the feature learning view of AdaBoost
in Fig. 1.
B. ADABOOST+SOFT MAGIN SVM
Although the training error of AdaBoost will always reach
0 with the iterations growing, in practical situation, the training error may usually not reach 0 beacause of the ﬁxed
iterations or the large-scale data. In this situation, the features
VOLUME 7, 2019
F. Wang et al.: Feature Learning Viewpoint of Adaboost and a New Algorithm
FIGURE 1. The feature learning view of AdaBoost.
Algorithm 2 AdaBoost+SVM Algorithm
Training sample S;
Base learning algorithm L;
Number of base learners T.
1. Using AdaBoost to get the feature function z(x)
according to Eq.(18);
2. Using SVM classiﬁer to calculate the new coefﬁcient
β and the bias b according to Eq.(21);
3. Combining the obtained β, b and the feature function
to complete the ﬁnal classiﬁer according to Eq. (20).
The ﬁnal classiﬁer F(x).
from z(x) ∈RT maybe cannot be linearly separated so that
the hard margin SVM is not suitable. To solve this problem,
we use soft margin SVM to replace the hard margin SVM, i.e.
add an additional margin violation ξi to Eq.(19). The objective
function is :
s.t. yi(βz(xi) + b) ≥1 −ξi
i = 1, 2, · · · , n,
where C represents the tolerance of the margin violation ξi.
Then we can use this way to solve our model whether the
training error of AdaBoost reachs 0 or not.
The algorithm of AdaBoost+SVM is described in
Algorithm 2.
C. THE EXPLANATION TO THE RESISTANCE TO
OVERFITTING BY OUR MODEL
Overﬁtting is a common problem in many classiﬁcation
situations. However, the AdaBoost algorithm can resist
overﬁtting. Understanding the mystery is a very fascinating fundamental theoretical problem. Our AdaBoost+SVM
model also has the superiority of explaining this phenomenon. We utilize the following theorem to analyze this
property from feature learning viewpoint.
Theorem 4: SVM is a linear classiﬁer in the feature space.
As the dimensions of features increases, the margin of SVM
will not be smaller.
Proof: The objective function of SVM is:
yi(βxi + b) −1 ≥0,
i = 1, 2, · · · , n.
The optimal solution β∗and b∗can be calculated. Then,
the corresponding hard margin separation hyperplane is
β∗x + b∗= 0.
If x increases to (x, xt), then the corresponding β becomes
to (β, βt). The new optimal solution β∗
new and b∗
new can be
obtained. Then, the corresponding hard margin separation
hyperplane is
If βt = 0, the margin will stay the same. If βt ̸= 0,
the margin will become larger. In a word, the margin does
not become smaller when the dimensions of features are
increased. According to , the bigger margin is, the higher
the predictive conﬁdence is. Therefore, as the number of
feature increases, the classiﬁcation performance will not
Based on this theorem, we regard the obtained results of
base classiﬁers by the AdaBoost algorithm as features of
SVM. As the dimensions of features increases, the performance of SVM classiﬁer would be improved, which can
easily explain the advantage of AdaBoost that can resist over-
ﬁtting. Therefore, our AdaBoost+SVM model have illustrated the mysteries of resistance to overﬁtting from the
feature learning viewpoint.
It should be noticed that as the number of T increased,
we cannot directly obtain the conclusion that the θ in Eq.(13)
also increases. But in our model, the increasing of T must
be helpful for better performance, which can explain the
resistance to overﬁtting of AdaBoost directly and clearly.
IV. EXPERIMENT
In this section, we conduct experiments on four binary benchmark datasets to demonstrate the efﬁciency and effectiveness
VOLUME 7, 2019
F. Wang et al.: Feature Learning Viewpoint of Adaboost and a New Algorithm
FIGURE 2. Classification accuracy (%) and time costs vs the percent of
labeled samples on fourclass dataset.
of the proposed method. Then, we have a detail analysis about
the experimental results.
A. DATASETS
We utilize the following 4 binary datasets to evaluate the
performance of our model.
1) fourclass: This dataset totally has 862 samples and 2
dimensions.
2) ionosphere: This dataset is one of the UCI dataset with
351 samples and 34 features.
3) chess: This dataset is also belongs to UCI dataset with
3196 samples and 36 features.
4) monk1: This dataset is also one of the UCI dataset with
432 samples and 6 features.
The detail descriptions of all datasets are also listed
in Table 1.
TABLE 1. Dataset description.
FIGURE 3. Classification accuracy (%) and time costs vs the percent of
labeled samples on ionosphere dataset.
B. COMPARISON METHODS
To demonstrate the effectiveness of the proposed approaches,
we compare it with the classical AdaBoost algorithm. For all
the methods, we run 5 times and evaluate the classiﬁcation
results with the average classiﬁcation accuracies and time
costs. All the experiments are implemented in MATLAB
R2017a, and run on a windows 10 machine with a 3.6GHz
Intel i7-7700 CPU and 32GB RAM.
C. THE EFFECT ON THE NUMBER OF WEAK LEARNER
In most AdaBoost algorithm, the number of weak learner is
set empirically. Now, we conduct experiments on the above
four binary datasets to observe the effect on the number of
weak learner. To exclude the contingency and make full use
of the data, we adopt the 10-fold cross validation way to
select 90% in each class labeled samples as the training data
to construct the classiﬁer, the rest 10% samples as the testing
data to evaluate the performance of this classiﬁer.
The number of weak learner varies from 100 to 1000
and the corresponding classiﬁcation accuracy (%) and time
cost (s) on the four datasets are recorded in Table 2,
Table 3, Table 4 and Table 5. From Table 2 to Table 5,
we have the following observation. Firstly, on the fourclass
VOLUME 7, 2019
F. Wang et al.: Feature Learning Viewpoint of Adaboost and a New Algorithm
FIGURE 4. Classification accuracy (%) and time costs vs the percent of
labeled samples on chess dataset.
TABLE 2. Classification accuracy (%) and time cost (s) vs the number of
weak learner on fourclass dataset.
and chess datasets, the classiﬁcation accuracies obtained by
AdaBoost+SVM are a little higher than AdaBoost. On the
monk1 dataset, the classiﬁcation accuracies calculated by the
two models are exactly equal. On the ionosphere dataset,
the classiﬁcation accuracies obtained by AdaBoost+SVM
are a little lower than AdaBoost. However, the gap between
the results of AdaBoost and AdaBoost+SVM is not very
large. Then, we can see that time costs obtained by AdaBoost
and AdaBoost+SVM have little gap. The reason is that
FIGURE 5. Classification accuracy (%) and time costs vs the percent of
labeled samples on monk1 dataset.
TABLE 3. Classification accuracy (%) and time cost (s) vs the number of
weak learner on ionosphere dataset.
SVM can quickly handle classiﬁcation problems. Therefore,
we can conclude that our AdaBoost+SVM is very close to
the AdaBoost, which can be regarded as a new explanation to
the AdaBoost algorithm.
D. THE EFFECT ON THE NUMBER OF TRAINING DATA
Next, we conduct experiments on the above datasets to
observe the effect on the number of training data. For all the
datasets, we ﬁx the number of weak learner to 200. We vary
VOLUME 7, 2019
F. Wang et al.: Feature Learning Viewpoint of Adaboost and a New Algorithm
TABLE 4. Classification accuracy (%) and time cost (s) vs the number of
weak learner on chess dataset.
TABLE 5. Classification accuracy (%) and time cost (s) vs the number of
weak learner on monk1 dataset.
the percent of labeled samples in each class from 10% to 90%
as the training data, the remaining samples as testing data.
The results are shown in Fig. 2, Fig. 3, Fig. 4 and Fig. 5.
As a general trend, the classiﬁcation accuracy and time
costs increase with the number of training samples increasing
on all the datasets. However, more training data brings more
time to train this classiﬁer. Then, in terms of classiﬁcation
accuracies and time costs on all the datasets, the results of
AdaBoost and AdaBoost+SVM are still close, which veriﬁes
the performances of the two is comparable again.
V. DISCUSSION
The resistance to overﬁtting of AdaBoost has attracted many
scholars trying to explain it in the past. There were a great
deal of excellent and instructive works on this topic. Under
the guidance of these works, people can propose various
methods to enhance the performance of boosting classiﬁer.
In this paper, we try to illustrate the resistance to overﬁtting
of AdaBoost from the feature learning viewpoint which is
different from the past works. To explain the rationality of
our theory, we ﬁrst demonstrate that the feature generated
by AdaBoost can be linearly segmented. In general, SVM
is a simple, well-known and effective method to solve the
linear classiﬁcation task. That means from the feature learning viewpoint, choose SVM as classiﬁer is intuitive and
reasonable. The experiment results show that our method has
the similar performance to the original AdaBoost algorithm.
That is in line with our expectations. Firstly, the weight α
generated by AdaBoost comes from mathematical derivation
which means α is a relatively reasonable parameter. Secondly,
SVM is a good choice in this feature segmentation task,
but it just provides a new weight compared with original
AdaBoost. The characters of AdaBoost are not changed. So it
is foreseeable that our algorithm has the similar characters
with original AdaBoost. Thirdly, SVM can be one choice of
classifer in our feature learning viewpoint, but it may be not
the only choice. In other words, we can also choose different
kernel functions on the SVM or other classiﬁer according to
the speciﬁc task and various data to improve the performance
of the algorithm. We will also concentrate on these work in
the future. We hope this work may provide a new way of
considering the research on AdaBoost algorithm, rather than
a signiﬁcant improvement in performance.
VI. CONCLUSION
In this paper, we have presented an AdaBoost+SVM model
from the feature learning viewpoint to explain the success
of AdaBoost that can resist the overﬁtting problem. Instead
of directly weighted combination the base classiﬁers calculated by AdaBoost, we regard them as the new features
and input them to the SVM classiﬁer. The iterations increasing means that the dimensions of features are increasing,
so that the performance of SVM would be improved, which
can explain the resistance to overﬁtting of AdaBoost model
in a simple way. The results on four binary datasets show
that AdaBoost+SVM can produce the comparable results to
the AdaBoost algorithm, which illustrates the rationality to
understand the AdaBoost algorithm from the feature learning
viewpoint.