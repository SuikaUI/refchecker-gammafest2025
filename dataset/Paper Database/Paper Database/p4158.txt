Joint Unsupervised Learning of Deep Representations and Image Clusters
Jianwei Yang, Devi Parikh, Dhruv Batra
Virginia Tech
{jw2yang, parikh, dbatra}@vt.edu
In this paper, we propose a recurrent framework for
Joint Unsupervised LEarning (JULE) of deep representations and image clusters. In our framework, successive
operations in a clustering algorithm are expressed as steps
in a recurrent process, stacked on top of representations
output by a Convolutional Neural Network (CNN). During training, image clusters and representations are updated jointly: image clustering is conducted in the forward pass, while representation learning in the backward
pass. Our key idea behind this framework is that good representations are beneﬁcial to image clustering and clustering results provide supervisory signals to representation learning. By integrating two processes into a single
model with a uniﬁed weighted triplet loss and optimizing
it end-to-end, we can obtain not only more powerful representations, but also more precise image clusters.
Extensive experiments show that our method outperforms the
state-of-the-art on image clustering across a variety of image datasets. Moreover, the learned representations generalize well when transferred to other tasks. The source
code can be downloaded from 
jwyang/joint-unsupervised-learning.
1. Introduction
We are witnessing an explosion in visual content. Signi-
ﬁcant recent advances in machine learning and computer
vision, especially via deep neural networks, have relied
on supervised learning and availability of copious annotated data.
However, manually labelling data is a timeconsuming, laborious, and often expensive process. In order
to make better use of available unlabeled images, clustering
and/or unsupervised learning is a promising direction.
In this work, we aim to address image clustering and representation learning on unlabeled images in a uniﬁed framework. It is a natural idea to leverage cluster ids of images as
supervisory signals to learn representations and in turn the
representations would be beneﬁcial to image clustering. At
a high-level view, given a collection of ns unlabeled images
(a) Initial stage
(b) Middle stage
(c) Final stage
Figure 1: Clustering outputs for MNIST test set at different stages of the proposed method. We conduct PCA on
the image representations and then choose the ﬁrst three dimensions for visualization. Different colors correspond to
different clusters. Samples are grouped together gradually
and more discriminative representations are obtained.
I = {I1, ..., Ins}, the global objective function for learning
image representations and clusters can be written as:
where L(·) is a loss function, y denotes the cluster ids for all
images, and θ denotes the parameters for representations. If
we hold one in {y, θ} to be ﬁxed, the optimization can be
decomposed into two alternating steps:
Intuitively, (2a) can be cast as a conventional clustering problem based on ﬁxed representations, while (2b) is
a standard supervised representation learning process.
In this paper, we propose an approach that alternates
between the two steps – updating the cluster ids given
the current representation parameters and updating the representation parameters given the current clustering result.
Speciﬁcally, we cluster images using agglomerative
clustering and represent images via activations of a
Convolutional Neural Network (CNN).
The reason to choose agglomerative clustering is threefold: 1) it begins with an over-clustering, which is more
reliable in the beginning when a good representation has
not yet been learned. Intuitively, clustering with representations from a CNN initialized with random weights are not
 
reliable, but nearest neighbors and over-clusterings are often acceptable; 2) These over-clusterings can be merged as
better representations are learned; 3) Agglomerative clustering is a recurrent process and can naturally be interpreted
in a recurrent framework.
Our ﬁnal algorithm is farily intuitive.
We start with
an intial over-clustering, update CNN parameters (2b) using image cluster labels as supervisory signals, then merge
clusters (2a) and iterate until we reach a stopping criterion.
An outcome of the proposed framework is illustrated in
Fig. 1. Initially, there are 1,762 clusters for MNIST test
set (10k samples), and the representations (image intensities) are not that discriminative. After several iterations, we
obtain 17 clusters and more discriminative representations.
Finally, we obtain 10 clusters which are well-separated
by the learned representations and interestingly correspond
primarily to the groundtruth category labels in the dataset,
even though the representation is learnt in an unsupervised
manner. To summarize, the major contributions of our work
1 We propose a simple but effective end-to-end learning
framework to jointly learn deep representations and
image clusters from an unlabeled image set;
2 We formulate the joint learning in a recurrent framework, where merging operations of agglomerative
clustering are expressed as a forward pass, and representation learning of CNN as a backward pass;
3 We derive a single loss function to guide agglomerative clustering and deep representation learning, which
makes optimization over the two tasks seamless;
4 Our experimental results show that the proposed
framework outperforms previous methods on image
clustering and learns deep representations that can be
transferred to other tasks and datasets.
2. Related Work
Clustering Clustering algorithms can be broadly categorized into hierarchical and partitional approaches . Agglomerative clustering is a hierarchical clustering algorithm
that begins with many small clusters, and then merges
clusters gradually . As for partitional clustering methods, the most well-known is K-means , which
minimizes the sum of square errors between data points
and their nearest cluster centers. Related ideas form the
basis of a number of methods, such as expectation maximization (EM) , spectral clustering , and
non-negative matrix factorization (NMF) based clustering
 .
Deep Representation Learning Many works use raw image intensity or hand-crafted features 
combined with conventional clustering methods. Recently,
representations learned using deep neural networks have
presented signiﬁcant improvements over hand-designed
features on many computer vision tasks, such as image classiﬁcation , object detection ,
etc. However, these approaches rely on supervised learning with large amounts of labeled data to learn rich representations. A number of works have focused on learning representations from unlabled image data. One class
of approaches cater to reconstruction tasks, such as autoencoders , deep belief networks (DBN)
 , etc. Another group of techniques learn discriminative
representations after fabricating supervisory signals for images, and then ﬁnetune them supervisedly for downstream
applications . Unlike our approach, the fabricated supervisory signal in these previous works is not updated during representation learning.
Combination A number of works have explored combining
image clustering with representation learning. In , the
authors proposed to learn a non-linear embedding of the undirected afﬁnity graph using stacked autoencoder, and then
ran K-means in the embedding space to obtain clusters. In
 , a deep semi-NMF model was used to factorize the input into multiple stacking factors which are initialized and
updated layer by layer. Using the representations on the
top layer, K-means was implemented to get the ﬁnal results. Unlike our work, they do not jointly optimize for the
representation learning and clustering.
To connect image clustering and representation learning
more closely, conducted image clustering and codebook learning iteratively. However, they learned codebook
over SIFT feature , and did not learn deep representations. Instead of using hand-crafted features, Chen used
DBN to learn representations, and then conducted a nonparametric maximum margin clustering upon the outputs
of DBN. Afterwards, they ﬁne-tuned the top layer of DBN
based on clustering results. A more recent work on jointly
optimizing two tasks is found in , where the authors
trained a task-speciﬁc deep architecture for clustering. The
deep architecture is composed of sparse coding modules
which can be jointly trained through back propagation from
a cluster-oriented loss. However, they used sparse coding
to extract representations for images, while we use a CNN.
Instead of ﬁxing the number of clusters to be the number of
categories and predicted labels based on softmax outputs,
we predict the labels using agglomerative clustering based
on the learned representations. In our experiments we show
that our approach outperforms .
3. Approach
3.1. Notation
We denote an image set with ns images by I
{I1, ..., Ins}.
The cluster labels for this image set are
y = {y1, ..., yns}. θ are the CNN parameters, based on
which we obtain deep representations X = {x1, ..., xns}
Clustering
Clustering
Clustering
Figure 2: Proposed recurrent framework for unsupervised
learning of deep representations and image clusters.
from I. Given the predicted image cluster labels, we organize them into nc clusters C = {C1, ..., Cnc}, where
Ci = {xk|yk = i, ∀k ∈1, ..., ns}.
are the Ks
nearest neighbours of xi, and N Kc
is the set of Kc nearest
neighbour clusters of Ci. For convenience, we sort clusters
in descending order of afﬁnity with Ci so that the
nearest neighbour argmaxC∈Ct A(Ci, C) is the ﬁrst entry
Ci . Here, A is a function to measure the afﬁnity (or
similarity) between two clusters. We add a superscript t to
{θ, X, y, C} to refer to their states at timestep t. We use Y
to denote the sequence {y1, ..., yT } with T timesteps.
3.2. Agglomerative Clustering
As background, we ﬁrst brieﬂy describe conventional agglomerative clustering . The core idea in agglomerative clustering is to merge two clusters at each step until
some stopping conditions. Mathematically, it tries to ﬁnd
two clusters Ca and Cb by
{Ca, Cb} =
Ci,Cj∈C,i̸=j
There are many methods to compute the afﬁnity between
two clusters . More details can be found
in . We now describe how the afﬁnity is measured by A
in our approach.
3.3. Afﬁnity Measure
First, we build a directed graph G =< V, E >, where V
is the set of vertices corresponding to deep representations
X for I, and E is the set of edges connecting vertices. We
deﬁne an afﬁnity matrix W ∈Rns×ns corresponding to the
edge set. The weight from vertex xi to xj is deﬁned by
W (i, j) =
exp(−||xi−xj||2
if xj ∈N Ks
where σ2 =
||xi −xj||2
way to build up a directed graph can be found in many previous works such as . Here, a and Ks are two predeﬁned parameters (their values are listed in Table 2). After
constructing a directed graph for samples, we then adopt the
graph degree linkage in to measure the afﬁnity between
cluster Ci and Cj, denoted by A(Ci, Cj).
3.4. A Recurrent Framework
Our key insight is that agglomerative clustering can be
interpreted as a recurrent process in the sense that it merges
clusters over multiple timesteps. Based on this insight, we
propose a recurrent framework to combine the image clustering and representation learning processes.
As shown in Fig. 2, at the timestep t, images I are ﬁrst
fed into the CNN to get representations Xt and then used in
conjunction with previous hidden state ht−1 to predict current hidden state ht, i.e, the image cluster labels at timestep
t. In our context, the output at timestep t is yt = ht. Hence,
at timestep t
Xt = fr(I|θt)
ht = fm(Xt, ht−1)
yt = fo(ht) = ht
where fr is a function to extract deep representations Xt
for input I using the CNN parameterized by θt, and fm is a
merging process for generating ht based on Xt and ht−1.
In a typical Recurrent Neural Network, one would unroll all timesteps at each training iteration. In our case, that
would involve performing agglomerative clustering until we
obtain the desired number of clusters, and then update the
CNN parameters by back-propagation.
In this work, we introduce a partial unrolling strategy,
i.e., we split the overall T timesteps into multiple periods,
and unroll one period at a time. The intuitive reason we
unroll partially is that the representation of the CNN at the
beginning is not reliable. We need to update CNN parameters to obtain more discriminative representations for the
following merging processes. In each period, we merge a
number of clusters and update CNN parameters for a ﬁxed
number of iterations at the end of the period. An extreme
case would be one timestep per period, but it involves updating the CNN parameters too frequently and is thus timeconsuming. Therefore, the number of timesteps per period
(and thus the number of clusters merged per period) is determined by a parameter in our approach. We elaborate on
this more in Sec. 3.6.
3.5. Objective Function
In our recurrent framework, we accumulate the losses
from all timesteps, which is formulated as
L({y1, ..., yT }, {θ1, ..., θT }|I) =
Lt(yt, θt|yt−1, I) (6)
Here, y0 takes each image as a cluster. At timestep t, we
ﬁnd two clusters to merge given yt−1. In conventional agglomerative clustering, the two clusters are determined by
ﬁnding the maximal afﬁnity over all pairs of clusters. In
this paper, we introduce a criterion that considers not only
the afﬁnity between two clusters but also the local structure surrounding the clusters. Assume from yt−1 to yt, we
merged a cluster Ct
i and its nearest neighbour. Then the loss
at timestep t is a combination of negative afﬁnities, that is,
Lt(yt, θt|yt−1, I) = −A(Ct
i ) −A(Ct
where λ weighs (7a) and (7b). Note that yt, yt−1 and θt
are not explicitly presented at the right side, but they determine the loss via the image cluster labels and afﬁnities
among clusters. On the right side of the above equation,
there are two terms: 1) (7a) measures the afﬁnity between
cluster Ci and its nearest neighbour, which follows conventional agglomerative clustering; 2) (7b) measures the difference between afﬁnity of Ci to its nearest neighbour cluster
and afﬁnities of Ci to its other neighbour clusters. This term
takes the local structure into account. See Sec. 3.5.1 for
detailed explanation.
simultaneously
{y1, ..., yT } and {θ1, ..., θT } that minimize the overall loss
in Eq. (6). As aforementioned, we optimize iteratively in a
recurrent process. We divide T timesteps into P partially
unrolled periods. In each period, we ﬁx θ and search optimal y in the forward pass, and then in the backward pass
we derive optimal θ given the optimal y. Details will be
explained in the following sections.
Forward Pass
In forward pass of the p-th (p ∈{1, ..., P}) partially unrolled period, we update the cluster labels with θ ﬁxed to
θp, and the overall loss in period p is
Lp(Yp|θp, I) =
Lt(yt|θp, yt−1, I)
where Yp is the sequence of image labels in period p, and
p] is the corresponding timesteps in period p. For optimization, we follow a greedy search similar to conventional agglomerative clustering. Starting from the time step
p, it ﬁnds one cluster and its nearest neighbour to merge so
that Lt is minimized over all possible cluster pairs.
In Fig. 3, we present a toy example to explain the reason
why we employ the term (7b). As shown, it is often the
case that the clusters are densely populated in some regions while sparse in some other regions. In conventional
Figure 3: A toy illustration of (a) conventional agglomerative clustering strategy and (b) the proposed one. For simpli-
ﬁcation, we use a single circle to represent a cluster/sample.
In conventional agglomerative clustering, node b and its
nearest neighbour are chosen to merge because they are
closest to each other; while node e is chosen in our proposed strategy considering the local structure.
agglomerative clustering, it will choose two clusters with
largest afﬁnity (or smallest loss) at each time no mater
where the clusters are located. In this speciﬁc case, it will
choose cluster Cb and its nearest neighbour to merge. In
contrast, as shown in Fig. 3(b), our algorithm by adding
(7b) will ﬁnd cluster Ce, because it is not only close to it
nearest neighbour, but also relatively far away from its other
neighbours, i.e., the local structure is considered around one
cluster. Another merit of introducing (7b) is that it will allow us to write the loss in terms of triplets as explained next.
Backward Pass
In forward pass of the p-th partially unrolled period, we
have merged a number of clusters.
Let the sequence of
optimal image cluster labels be given by Yp
and clusters merged in forward pass are denoted by
Ct∗ ]}, t ∈{ts
p, ..., te
p}. In the backward pass, we
aim to derive the optimal θ to minimize the losses generated
in forward pass. Because the clustering in current period is
conditioned on the clustering results of all previous periods,
we accumulate the losses of all p periods, i.e.,
∗, ..., Yp
Minimizing (9) w.r.t θ leads to representation learning
on I supervised by {Y1
∗, ..., Yp
on (7a) and (7b), the loss in Eq. 9 is reformulated to
Ct∗ ) −A(Ct
where λ′ = (1 + 1/λ). (10) is a loss deﬁned on clusters
of points, which needs the entire dataset to estimate, making it difﬁcult to use batch-based optimization. However,
Algorithm 1 Joint Optimization on y and θ
I: = collection of image data;
c: = target number of clusters;
y∗, θ∗: = ﬁnal image labels and CNN parameters;
1: t ←0; p ←0
2: Initialize θ and y
Update yt to yt+1 by merging two clusters
Update θp to θp+1 by training CNN
p ←(p + 1)
10: until Cluster number reaches n∗
11: y∗←yt; θ∗←θp
we show that this loss can be approximated by a samplebased loss, enabling us to compute unbiased estimators for
the gradients using batch-statistics.
The intuition behind reformulation of the loss is that
agglomerative clustering starts with each datapoint as a
cluster, and clusters at a higher level in the hierarchy are
formed by merging lower level clusters.
Thus, afﬁnities
between clusters can be expressed in terms of afﬁnities
between datapoints. We show in the supplement that the
loss in (10) can be approximately reformulated as
(γA(xi, xj) −A(xi, xk))
where γ is a weight whose value depends on λ′ and how
clusters are merged during the forward pass. xi and xj
are from the same cluster, while xk is from the neighbouring clusters, and their cluster labels are merely determined
by the ﬁnal clustering result y
∗. To further simplify the
optimization, we instead search xk in at most Kc neighbour samples of xi from other clusters in a training batch.
Hence, the batch-wise optimization can be performed using
conventional stochastic gradient descent method. Note that
such triplet losses have appeared in other works .
Because it is associated with a weight, we call (35) the
weighted triplet loss.
3.6. Optimization
Given an image dataset with ns samples, we assume the
number of desired clusters n∗
c is given to us as is standard in
clustering. Then we can build up a recurrent process with
T = ns −n∗
c timesteps, starting by regarding each sample
as a cluster. However, such initialization makes the optimization time-consuming, especially when datasets contain
a large number of samples. To address this problem, we
can ﬁrst run a fast clustering algorithm to get the initial
clusters. Here, we adopt the initialization algorithm proposed in for fair comparison with their experiment results. Note that other kind of initializations can also be used,
e.g. K-means. Based on the algorithm in , we obtain
a number of clusters which contain a few samples for each
(average is about 4 in our experiments). Given these initial
clusters, our optimization algorithm learns deep representations and clusters. The algorithm is outlined in Alg. 1.
In each partially unrolled period, we perform forward and
backward passes to update y and θ, respectively. Speciﬁcally, in the forward pass, we merge two clusters at each
timestep. In the backward pass, we run about 20 epochs
to update θ, and the afﬁnity matrix W is also updated based
on the new representation. The duration of the p-th period
is np = ceil(η × ns
c) timesteps, where ns
c is the number
of clusters at the beginning of current period, and η is a
parameter called unrolling rate to control the number of
timesteps. The less η is, the more frequently we update θ.
4. Experiments
4.1. Image Clustering
We compare our approach with 12 clustering algorithms,
including K-means , NJW spectral clustering (SC-
NJW) , self-tuning spectral clustering (SC-ST) ,
large-scale spectral clustering (SC-LS) , agglomerative
clustering with average linkage (AC-Link) , Zeta function based agglomerative clustering (AC-Zell) , graph
degree linkage-based agglomerative clustering (AC-GDL)
 , agglomerative clustering via path integral (AC-PIC)
 , normalized cuts (N-Cuts) , locality preserving
non-negative matrix factorization (NMF-LP) , NMF with
deep model (NMF-D) , task-speciﬁc clustering with
deep model (TSC-D) .
For evaluation, we use a commonly used metric: normalized mutual information (NMI) . It ranges in .
Larger value indicates more precise clustering results.
We evaluate the clustering performance on two handwritten digit image datasets (MNIST and USPS1), two
multi-view object image datasets (COIL20 and COIL100
 ), and four face image datasets (UMist , FRGCv2.02, CMU-PIE , Youtube-Face (YTF)) .
number of samples and categories, and image size are listed in Table 1. MNIST consists of training set (60,000) and
testing set (10,000). To compare with different approaches,
we experiment on the full set (MNIST-full) and testing set
(MNIST-test), separately. For face image datasets such as
1 
2 
Table 1: Datasets used in our experiments.
#Categories
Image Size
Table 2: Hyper-parameters in our approach.
Hyper-parameter
0.9 or 0.2
UMist, CMU-PIE, we use the images provided as is without
any changes. For FRGC-v2.0 and YTF datasets, we ﬁrst
crop faces and then resize them to a constant size. In FRGCv2.0 dataset, we randomly choose 20 subjects. As for YTF
dataset, we choose the ﬁrst 41 subjects which are sorted by
their names in alphabet order.
Experimental Setup
All the hyper-parameters and their values for our approach
are listed in Table 2. In our experiments, Ks is set to 20, the
same value to . a and λ are simply set to 1.0. We search
the values of Kc and γ for best performance on MNIST-test
set. The unrolling rate η for ﬁrst four datasets is 0.9; and
0.2 for face datasets. The target cluster number n∗
c is set to
be the number of categories in each dataset.
We use Caffe to implement our approach.
stacked multiple combinations of convolutional layer, batch
normalization layer, ReLU layer and pooling layer. For all
the convolutional layers, the number of channels is 50, and
ﬁlter size is 5×5 with stride = 1 and padding = 0. For pooling layer, its kernel size is 2 and stride is 2. To deal with
varying image sizes across datasets, the number of stacked
convolutional layers for each dataset is chosen so that the
size of the output feature map is about 10×10. On the top of
all CNNs, we append an inner product (ip) layer whose dimension is 160. ip layer is followed by a L2-normalization
layer before being fed to the weighted triplet loss layer or
used for clustering. For each partially unrolled period, the
base learning rate is set to 0.01, momentum 0.9, and weight
decay 5 × 10−5. We use the inverse learning rate decay
policy, with Gamma=0.0001 and Power=0.75. Stochastic
gradient descent (SGD) is adopted for optimization.
Quantitative Comparison
We report NMI for different methods on various datasets.
Results are averaged from 3 runs. We report the results by
re-running the code released by original papers. For those
that did not release the code, the corresponding results are
borrowed from the papers. We ﬁnd the results we obtain
are somewhat different from the one reported in original
papers. We suspect that these differences may be caused
by the different experimental settings or the released code
is changed from the one used in the original paper. For all
test algorithms, we conduct L2-normalization on the image
intensities since it empirically improves the clustering performance. We report our own results in two cases: 1) the
straight-forward clustering results obtained when the recurrent process ﬁnish, denoted by OURS-SF; 2) the clustering
results obtained by re-running clustering algorithm after obtaining the ﬁnal representation, denoted by OURS-RC. The
quantitative results are shown in Table 3. In the table cells,
the value before ’/’ is obtained by re-running code while the
value after ’/’ is that reported in previous papers.
As we can see from Table 3, both OURS-SF and OURS-
RC outperform previous methods on all datasets with noticeable margin. Interestingly, we achieved perfect results
(NMI = 1) on COIL20 and CMU-PIE datasets, which means
that all samples in the same category are clustered into the
same group. The agglomerative clustering algorithms, such
as AC-Zell, AC-GDL and AC-PIC perform better than other
algorithms generally. However, on MNIST-full test, they
all perform poorly. The possible reason is that MNISTfull has 70k samples, and these methods cannot cope with
such large-scale dataset when using image intensity as representation.
However, this problem is addressed by our
learned representation. We show that we achieved analogous performance on MNIST-full to MNIST-test set. In most
cases, we can ﬁnd OURS-RC performs better on datasets
that have room for improvement. We believe the reason is
that OURS-RC uses the ﬁnal learned representation over the
entire clustering process, while OURS-SF starts with image
intensity, which indicates that the learned representation is
more discriminative than image intensity. 3
Generalization Across Clustering Algorithms
We now evaluate if the representations learned by our joint
agglomerative clustering and representation learning approach generalize to other clustering techniques. We re-run
all the clustering algorithms without any changes of parameters, but using our learned deep representations as fea-
3We experimented with hand-crafted features such as HOG, LBP, spatial pyramid on a subset of the datasets with some of the better clustering
algorithms from Table 3, and found that they performed worse.
Table 3: Quantitative clustering performance (NMI) for different algorithms using image intensities as input.
MNIST-test
MNIST-full
K-means 
SC-NJW 
0.860/0.889
0.872/0.854
0.409/0.690
0.528/0.755
SC-ST 
0.673/0.895
0.706/0.858
0.342/0.726
0.445/0.756
N-Cuts 
0.768/0.884
0.861/0.823
0.382/0.675
0.386/0.753
AC-Link 
AC-Zell 
0.954/0.911
0.963/0.913
0.774/0.799
0.810/0.768
AC-GDL 
0.945/0.937
0.954/0.929
0.854/0.824
0.864/0.844
AC-PIC 
NMF-LP 
NMF-D 
0.983/0.910
TSC-D 
Table 4: Quantitative clustering performance (NMI) for different algorithms using our learned representations as inputs.
MNIST-test
MNIST-full
K-means 
SC-NJW 
SC-ST 
N-Cuts 
AC-Link 
AC-Zell 
AC-GDL 
AC-PIC 
NMF-LP 
tures. The results are shown in Table 4. It can be seen that
all clustering algorithms obtain more precise image clusters
by using our learned representation. Some algorithms like
K-means, AC-Link that performed very poorly with raw intensities perform much better with our learned representations, and the variance in performance across all clustering
algorithms is much lower. These results clearly demonstrate
that our learned representation is not over-ﬁtting to a single
clustering algorithm, but generalizes well across various algorithms. Interestingly, using our learned representation,
some of the clustering algorithms perform even better than
AC-GDL we build on in our approach.
4.2. Transferring Learned Representation
Cross-Dataset Clustering
Table 5: NMI performance across COIL20 and COIL100.
COIL20 →COIL100
COIL100 →COIL20
Table 6: NMI performance across MNIST-test and USPS.
MNIST-test →USPS
USPS →MNIST-test
In this section, we study whether our learned representations generalize across datasets. We train a CNN based
on our approach on one dataset, and then cluster images
from another (but related) dataset using the image features extracted via the CNN. Speciﬁcally, we experiment on
two dataset pairs: 1) multi-view object datasets (COIL20
and COIL100); 2) hand-written digit datasets (USPS and
MNIST-test). We use the representation learned from one
dataset to represent another dataset, followed by agglomerative clustering.
Note that because the image sizes or
channels are different across datasets, we resize the input
images and/or expand the channels before feeding them to
CNN. The experimental results are shown in Table 5 and 6.
We use the representations from top ip layer and also the
convolutional or pooling layers (top-1, top-2) close to top
layer for image clustering. In two tables, compared with
Table 7: Face veriﬁcation results on LFW.
Supervised
directly using raw image from the data layer, the clustering performance based on learned representations from all
layers improve, which indicates that the learned representations can be transferred across these datasets. As perhaps
expected, the performance on target datasets is worse compared to learning on the target dataset directly. For COIL20
and COIL100, a possible reason is that they have different
image categories. As for MNIST and USPS, the performance beats OURS-SF, but worse than OURS-RC. We ﬁnd
transferring representation learned on MNIST-test to USPS
gets close performance to OURS-RC learned on USPS.
Face Veriﬁcation
We now evaluate the performance of our approach by applying it to face veriﬁcation. In particular, the representation is learned on Youtube-Face dataset and evaluated on
LFW dataset under the restricted protocol. For training, we randomly choose about 10k, 20k, 30k, 50k, 100k
samples from YTF dataset. All these subsets have 1446 categories. We implement our approach to train CNN model
and cluster images on the training set. Then, we remove the
L2-normalization layer and append a softmax layer to ﬁnetune our unsupervised CNN model based on the predicted
image cluster labels. Using the same training samples and
CNN architecture, we also train a CNN model with a softmax loss supervised by the groundtruth labels of the training
set. According to the evaluation protocol in , we run 10fold cross-validation. The cosine similarity is used to compute the similarity between samples. In each of 10 crossvalidations, nine folds are used to ﬁnd the optimal threshold,
and the remaining one fold is used for evaluation. The average accuracy is reported in Table. 7. As shown, though
no groundtruth labels are used for representation learning
in our approach, we obtain analogous performance to the
supervised learning approach. Our approach even (slightly)
beats the supervised learning method in one case.
4.3. Image Classiﬁcation
Recently, unsupervised representation learning methods
are starting to achieve promising results for a variety of recognition tasks . We are interested in knowing whether the proposed method can also learn useful representation for image classiﬁcation. We experiment with
CIFAR-10 . We follow the pipeline in , and base
our experiments on their publicly available code. In this
pipeline, codebook with 1600 codes is build upon 6 × 6
Table 8: Image classiﬁcation accuracy on CIFAR-10.
K-means 
50k (full set)
ZCA-whitened image patches, and then used to code the
training and testing samples by extracting 1,600-d feature
from each of 4 image quadrants. Afterwards, a linear SVM
 is applied for image classiﬁcation on 6,400-d feature. In
our approach, the only difference is that we learn a new representation from 6 × 6 patches, and then use these new representations to build the codebook with 1,600 codes. The
CNN architecture we use contains two convolutional layers, each of which is combined with a ReLu and a pooling
layer, followed by an inner product layer. Both convolutional layers have 50 3 × 3 ﬁlters with pad = 1. The kernel
size of pooling layer is 2, and the stride is 2. To save on
training time, 40k randomly extracted patches are extracted
from 50k training set and used in all the experiments.
Classiﬁcation accuracies on test set with different settings are shown in Table 8. We vary the number of training samples and evaluate the performance for representations from different layers. As we can see, the combination
of representations from the ﬁrst and second convolutional
layer achieve the best performance. We also use the representation output by inner product layer to learn the codebook. However, it performs poorly. A possible reason is
that it discards spatial information of image patches, which
may be important for learning a codebook. When using
400k randomly extracted patches to learn the codebook, 
achieved 77.9%. However, it is still lower than what we
achieved. This performance also beats several other methods listed in .
5. Conclusion
In this paper, we have proposed an approach to jointly
learn deep representations and image clusters. In our approach, we combined agglomerative clustering with CNNs
and formulate them as a recurrent process. We used a partially unrolling strategy to divide the timesteps into multiple periods. In each period, we merged clusters step by
step during the forward pass and learned representation in
the backward pass, which are guided by a single weighted
triplet-loss function. The extensive experiments on image
clustering, deep representation transfer learning and image classiﬁcation demonstrate that our approach can obtain
more precise image clusters and discriminative representations that generalize well across many datasets and tasks.
6. Acknowledgements
This work was supported in part by the Paul G. Allen Family Foundation, Google, and Institute for Critical
Technology and Applied Science (ICTAS) at Virginia Tech
through awards to D. P.; and by a National Science Foundation CAREER award, an Army Research Ofﬁce YIP award,
an Ofﬁce of Naval Research grant, an AWS in Education
Research Grant, and GPU support by NVIDIA to D. B. The
views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the ofﬁcial policies or endorsements, either expressed or
implied, of the U.S. Government or any sponsor.