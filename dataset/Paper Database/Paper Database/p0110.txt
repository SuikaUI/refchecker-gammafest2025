Western University
Western University
Scholarship@Western
Scholarship@Western
Centre for Human Capital and Productivity.
CHCP Working Papers
Economics Working Papers Archive
2003-5 Does Matching Overcome Lalonde's Critique of
2003-5 Does Matching Overcome Lalonde's Critique of
Nonexperimental Estimators?
Nonexperimental Estimators?
Jeffrey Smith
Petra Todd
Follow this and additional works at: 
Part of the Economics Commons
Citation of this paper:
Citation of this paper:
Smith, Jeffrey, Petra Todd. "Does Matching Overcome Lalonde's Critique of Nonexperimental
Estimators?." CIBC Centre for Human Capital and Productivity. CIBC Working Papers, 2003-5. London, ON:
Department of Economics, University of Western Ontario .
Does Matching Overcome Lalonde's
Critique of Nonexperimental
Estimators?
Jeffrey Smith and Petra Todd
Working Paper # 2003-5 June 2003
CIBC Working Paper Series
Department of Economics
Social Science Centre
The University of Western Ontario
London, Ontario, N6A 5C2
This working paper is available as a downloadable pdf file on our website
 
Does Matching Overcome Lalonde’s Critique
of Nonexperimental Estimators?1
Jeﬀrey Smith
University of Maryland
Petra Todd
University of Pennsylvania2
First Draft: November 2000; Current Draft: June 13, 2003
1We thank Robert Lalonde for providing us with the data from his 1986 study.
We thank Rajeev
Dehejia for providing us with information helpful in reconstructing the samples used in the Dehejia and
Wahba studies. We thank seminar participants at Boston College, the CEA meetings, CILN,
the Department of Family and Community Services of Australia, Econometric Society (European meetings),
the GAAC conference on the Evaluation of Active Labor Market Policies, IFS, IRP, IZA, Kentucky, Laval,
Mathematica, McGill, Pittsburg, PSI, Princeton, St.
Gallen, SEA meetings, Stockholm, UCSB, UNC,
Wisconsin, WRNET, and Yale for their helpful comments. We thank Sascha Becker, Dan Black, Per-Anders
Edin, Markus Fr¨olich, John Ham, Robert LaLonde, Michael Lechner, Thomas Lemieux, Miana Plesca, Mike
Veall and two anonymous referees for useful comments. Jingjing Hsee and Miana Plesca provided excellent
research assistance. We are grateful to James Heckman for his encouragement and for ﬁnancial resources
to support Jingjing Hsee. Smith’s participation in this project was supported by the Social Science and
Humanities Research Council of Canada and the CIBC Chair in Human Capital and Productivity at the
University of Western Ontario and Todd’s by the U.S. National Science Foundation (SBR-9730688).
2Smith and Todd are both aﬃliated with the National Bureau of Economic Research (NBER) and the
IZA. Smith’s email address is . Todd’s email address is .
This paper applies cross-sectional and longitudinal propensity score matching estimators to data
from the National Supported Work (NSW) Demonstration that have been previously analyzed by
LaLonde and Dehejia and Wahba . We ﬁnd that estimates of the impact of NSW
based on propensity score matching are highly sensitive to both the set of variables included in the
scores and the particular analysis sample used in the estimation. Among the estimators we study,
the diﬀerence-in-diﬀerences matching estimator performs the best. We attribute its performance
to the fact that it eliminates potential sources of temporally-invariant bias present in the NSW
data, such as geographic mismatch between participants and non-participants and the use of a
dependent variable measured in diﬀerent ways for the two groups. Our analysis demontrates that
while propensity score matching is a potentially useful econometric tool, it does not represent a
general solution to the evaluation problem.
Introduction
There is a long-standing debate in the literature over whether social programs can be reliably evaluated without a randomized experiment. Randomization has a key advantage over nonexperimental
methods in generating a control group that has the same distributions of both observed and unobserved characteristics as the treatment group. At the same time, social experimentation also has
some drawbacks, such as high cost, the potential to distort the operation of an ongoing program,
the common problem of program sites refusing to participate in the experiment and the problem of
randomized-out controls seeking alternative forms of treatment.1 In contrast, evaluation methods
that use nonexperimental data tend to be less costly and less intrusive.
Also, for some questions
of interest, they are the only alternative.2
The major obstacle in implementing a nonexperimental evaluation strategy is choosing among
the wide variety of estimation methods available in the literature. This choice is important given
the accumulated evidence that impact estimates are often highly sensitive to the estimator chosen.
A literature has arisen, starting with LaLonde , that evaluates the performance of nonexperimental estimators using experimental data as a benchmark. Much of this literature implicitly
frames the question as one of searching for “the” nonexperimental estimator that will always solve
the selection bias problem inherent in nonexperimental evaluations.
Two recent contributions to
this literature by Dehejia and Wahba (DW) have drawn attention to a class of estimators called propensity score matching estimators. They apply these matching estimators to the
same experimental data from the National Supported Work (NSW) Demonstration, and the same
1On these points, see, e.g., Burtless and Orr , Heckman , Burtless , Heckman and Smith ,
Heckman, LaLonde and Smith and Heckman, Hohmann, Khoo and Smith .
2For example, Eberwein, Ham and Lalonde analyze the eﬀects of a job training program on employment
probabilities and on the lengths of employment spells. Experimental data do not solve the selection problem that
arises when comparing spells for program participants and nonparticipants at points in time after leaving the program.
Solving this selection problem requires application of nonexperimental evaluation methods.
nonexperimental data from the Current Population Survey (CPS) and the Panel Study of Income
Dynamics (PSID), analyzed by LaLonde and ﬁnd very low biases. Their ﬁndings have made
propensity score matching the estimator de jour in the evaluation literature.
Dehejia and Wahba’s ﬁnding of low bias from applying propensity score matching
to LaLonde’s data is surprising in light of the lessons learned from the analyses of Heckman,
Ichimura and Todd and Heckman, Ichimura, Smith and Todd (henceforth HIT
and HIST) using the experimental data from the U.S. National Job Training Partnership Act
(JTPA) Study.
They conclude that in order for matching estimators to have low bias, it is
important that the data include a rich set of variables related to program participation and labor
market outcomes, that the nonexperimental comparison group be drawn from the same local labor
markets as the participants, and that the dependent variable (typically earnings) be measured in
the same way for participants and non-participants.
All three of these conditions fail to hold in
the NSW data analyzed by LaLonde and Dehejia and Wahba .
In this paper, we analyze these data once again, applying both cross-sectional and longitudinal
variants of propensity score matching.
We ﬁnd that the low bias estimates obtained by DW
 using various cross-sectional matching estimators are highly sensitive to their choice
of a particular subsample of LaLonde’s data for their analysis. We also ﬁnd that changing
the set of variables used to estimate the propensity scores strongly aﬀects the estimated bias in
LaLonde’s original sample.
In contrast, we ﬁnd that diﬀerence-in-diﬀerences (DID) matching
estimators exhibit better performance than the cross-sectional estimators. This is consistent with
the evidence from the JTPA data in HIT and HIST on the importance of avoiding
geographic mismatch and of measuring the dependent variable in the same way in the treatment
and comparison groups.
Both these sources of bias are likely to be relatively stable over time,
and so should diﬀerence out.
More generally, our ﬁndings make it clear that propensity score
matching does not represent a “magic bullet” that solves the selection problem in every context.
The implicit search for such an estimator in the literature cannot succeed.
Instead, the optimal
nonexperimental evaluation strategy in a given context depends critically on the available data and
on the institutions governing selection into the program.
The plan of the paper is as follows. Section 2 reviews some key papers in the previous literature
on the choice among alternative non-experimental estimators. Section 3.1 lays out the evaluation
problem and Section 3.2 brieﬂy describes commonly used non-experimental estimators. Section 3.3
describes the cross-sectional and diﬀerence-in-diﬀerences matching estimators that we focus on in
our study. Sections 3.4 and 3.5 brieﬂy address the issues of choice-based sampling and the bias that
arises from incomplete matching, respectively. Section 3.6 explains how we use the experimental
data to benchmark the performance of non-experimental estimators. Sections 4 describes the NSW
Section 5 describes our analysis samples from the NSW data and the two comparison
groups. Section 6 presents our estimated propensity scores and Section 7 discusses the “balancing
tests” used in some recent studies to aid in selecting a propensity score speciﬁcation.
Sections 8
and 9 give the bias estimates obtained using matching and regression-based estimators, respectively.
Section 10 displays evidence on the use of speciﬁcation tests applied to our cross-sectional matching
estimators and Section 11 concludes.
Previous Research
Several previous papers use data from the National Supported Work Demonstration experiment to
study the performance of econometric estimators. Lalonde was the ﬁrst and the data we
use come from his study. He arranged the NSW data into two samples: one of AFDC women and
one of disadvantaged men. The comparison group subsamples were constructed from two national
survey datasets: the CPS and the PSID. Lalonde applies a number of standard evaluation
estimators, including simple regression adjustment, diﬀerence-in-diﬀerences, and the two-step version of the bivariate normal selection model in Heckman . His ﬁndings show that alternative
estimators produce very diﬀerent estimates, most of which deviate substantially from the experimental benchmark impacts. This is not necessarily surprising, given that the diﬀerent estimators
depend on diﬀerent assumptions about the nature of the outcome and program participation processes. Unless there is no selection problem, at most one set of assumptions will be satisﬁed in the
data. Using a limited set of speciﬁcation tests, Lalonde concludes that no good way exists
to sort among the competing estimators and, hence, that nonexperimental methods do not provide
an eﬀective means of evaluating programs.
His paper played an important role in the late 1980’s
movement towards using experiments to evaluate social programs .
Fraker and Maynard perform a similar analysis that focuses more on
comparison group selection than LaLonde and reach similar conclusions.
Heckman and Hotz respond to the LaLonde study by applying a broader range of
speciﬁcation tests to guide the choice among nonexperimental estimators.3 The primary test they
consider is based on pre-program data, so its validity depends on the assumption that the outcome
and participation processes are similar in pre-program and post-program time periods. Heckman
and Hotz ﬁnd that the tests they apply to the NSW data exclude the estimators that would
3Heckman and Hotz make use of somewhat diﬀerent data from the NSW experiment than LaLonde does.
Their two samples consist of female AFDC recipients, as in LaLonde, and young high school dropouts, most but
not all of whom are men.
They do not make use of the ex-convict and ex-addict samples.
In addition, they use
grouped earnings data from Social Security earnings records for both the NSW samples and the comparison groups,
while LaLonde uses individual level Social Security earnings records for the CPS comparison group and survey-based
earnings measures for the NSW sample and for the PSID comparison group.
Because their administrative data do
not suﬀer from attrition problems, the sample of AFDC women used in Heckman and Hotz is substantially
larger than that used in LaLonde .
imply a substantially diﬀerent qualitative conclusion (impact sign and statistical signﬁcance) than
the experiment.4
In the more recent evaluation literature, researchers have focused on matching estimators, which
were not considered by Lalonde or Heckman and Hotz .
Unlike some of the early
studies evaluating the Comprehensive Employment and Training Act (JTPA’s predecessor) surveyed in Barnow , which used variants of matching, the recent literature focuses on matching
on the probability of participating in the program. This technique, introduced in Rosenbaum and
Rubin , is called propensity score matching. Traditional propensity score matching methods
pair each program participant with a single nonparticipant, where pairs are chosen based on the
degree of similarity in the estimated probabilities of participating in the program (the propensity
scores). The mean impact of the program is estimated by the mean diﬀerence in the outcomes of
the matched pairs.
HIT and HIST extend traditional pairwise matching methods in several ways.
First, they describe kernel and local linear matching estimators that use multiple nonparticipants
in constructing the estimated counterfactual outcome. The main advantage of these estimators
vis-a-vis pairwise matching is a reduction in the asymptotic mean squared error.
Second, HIT
 and HIST propose modiﬁed versions of matching estimators that can be implemented
when longitudinal or repeated cross-section data are available. These estimators take care of timeinvariant diﬀerences in outcomes between participants and nonparticipants that cross-sectional
matching fails to eliminate.
HIT and HIST evaluate the performance of both the traditional pairwise matching
estimators and cross-sectional and longitudinal versions of their kernel and local linear matching
4These tests have also been applied in an evaluation context by, among others, Ashenfelter , Bassi ,
LaLonde , Friedlander and Robins , Regn´er and Raaum and Torp .
estimators using experimental data from the U.S. National JTPA Study combined with comparison
group samples drawn from three sources.
They show that data quality is a crucial ingredient to
any reliable estimation strategy. Speciﬁcally, the estimators examined are only found to perform
well in replicating the results of the experiment when they are applied to comparison group data
satisfying the following criteria: (i) the same data sources (i.e., the same surveys or the same type
of administrative data or both) are used for participants and nonparticipants, so that earnings and
other characteristics are measured in an analogous way, (ii) participants and nonparticipants reside
in the same local labor markets, and (iii) the data contain a rich set of variables that aﬀect both
program participation and labor market outcomes.
If the comparison group data fail to satisfy
these criteria, the performance of the estimators diminishes greatly.
Based on this evidence, HIT
 and HIST hypothesize that data quality probably accounts for much of the poor
performance of the estimators in Lalonde’s study, where participant and nonparticipant
samples were located in diﬀerent local labor markets, the data were collected using a combination
of diﬀerent survey instruments and administrative data sources and the data contain only very
limited information on observable characteristics.
More recently, DW use the NSW data to evaluate the performance of propensity
score matching methods, including pairwise matching and caliper matching (see Section 3.3 for detailed descriptions). They ﬁnd that these simple matching estimators succeed in closely replicating
the experimental NSW results, even through the comparison group data do not satisfy any of the
criteria found to be important in HIT and HIST . These papers are now widely cited
in the empirical literature as showing that propensity score matching solves the selection problem.
In this paper, we use the same NSW data employed by DW to evaluate the performance of both traditional, pairwise matching methods and of the newer methods developed in
HIT and HIST . We ﬁnd that a major diﬀerence between the DW 
studies and the LaLonde study is that DW exclude about 40 percent of the observations
used in Lalonde’s study in order to incorporate one additional variable into their propensity
score model.
As we show below, this restriction makes a tremendous diﬀerence to their results,
as it has the eﬀect of eliminating many of the higher earners from the sample.
Eliminating participants with high pre-program earnings mutes the pre-program “dip” and thereby makes the
selection problem easier to solve. In fact, almost any conventional evaluation estimator applied to
the smaller DW samples exhibits lower bias than when applied to the full LaLonde sample.
When we apply cross-sectional matching estimators to either the full LaLonde sample or an
alternative subsample of persons randomly assigned early in the experiment, we ﬁnd large biases.
Similarly, changing to an alternative propensity score speciﬁcation also increases the estimated bias.
Consistent with the likely sources of bias in the NSW data, we ﬁnd that diﬀerence-in-diﬀerences
matching estimators developed in HIT and HIST perform better than cross-sectional
matching estimators for both comparison groups.
Methodology
The Evaluation Problem
Assessing the impact of any intervention requires making an inference about the outcomes that
would have been observed for program participants had they not participated. Denote by Y1 the
outcome conditional on participation and by Y0 the outcome conditional on non-participation, so
that the impact of participating in the program is
∆= Y1 −Y0.
For each person, only Y1 or Y0 is observed, so ∆is not observed for anyone.
This missing data
problem lies at the heart of the evaluation problem.
Let D = 1 for the group of individuals who applied and got accepted into the program for whom
Y1 is observed. Let D = 0 for persons who do not enter the program for whom Y0 is observed. Let
X denote a vector of observed individual characteristics used as conditioning variables. The most
common evaluation parameter of interest is the mean impact of treatment on the treated,5
TT = E(∆|X, D = 1) = E(Y1 −Y0|X, D = 1) = E(Y1|X, D = 1) −E(Y0|X, D = 1),
which estimates the average impact of the program among those participating in it.
parameter on which LaLonde and DW focus.6 When Y represents earnings, a
comparison of the mean impact of treatment on the treated with the average per-participant cost
of the program indicates whether or not the program’s beneﬁts outweigh its costs, which is a key
question of interest in many evaluations.
Most experiments are designed to provide evidence on the treatment-on-the-treated parameter.
Data on program participants identiﬁes the mean outcome in the treated state, E(Y1|X, D = 1), and
the randomized-out control group provides a direct estimate of E(Y0|X, D = 1). In nonexperimental
(or observational) studies, no direct estimate of this counterfactual mean is available. Instead, the
econometrically adjusted outcomes of the nonparticipants proxy for the missing counterfactual.
Selection bias, or evaluation bias, consists of the diﬀerence between the adjusted outcomes of the
nonparticipants and the desired counterfactual mean.
In the next section, we discuss common
approaches for estimating the missing counterfactual mean.
We apply these approaches to the
5Following the literature, we use “treatment” and “participation” interchangeably throughout.
6However, many other parameters may be of interest in an evaluation. See, e.g., Eberwein, Ham and LaLonde
 , Heckman, Smith and Clements , Heckman, LaLonde and Smith , Heckman , Heckman,
Tobias and Vytlacil and Heckman and Vytlacil for discussions of other parameters of interest.
NSW data in Section 9.
Three Commonly-Used Nonexperimental Estimators
Nonexperimental estimators use two types of data to impute counterfactual outcomes for program
participants: data on participants prior to entering the program and data on nonparticipants.
Three common evaluation estimators are the before-after, cross-section and diﬀerence-in-diﬀerences
estimators. We next describe the estimators and their assumptions.
Assume that outcome measures Y1it and Y0it, where i denotes the individual and t the time
period, can be represented by
ϕ1(Xit) + U1it
ϕ0(Xit) + U0it,
where U1it and U0it are distributed independently across persons and satisfy E(U1it) = 0 and
E(U0it) = 0. The observed outcome is Yit = DiY1it + (1 −Di)Y0it , which can be written as
Yit = ϕ0(Xit) + Diα∗(Xit) + U0it,
where α∗(Xit) = ϕ1(Xit)−ϕ0(Xit)+U1it−U0it is the treatment impact. This is a random coeﬃcient
model because the impact of treatment varies across persons even conditional on Xit. Assuming
that U0it = U1it = Uit, so that the unobservable is the same in the treated and untreated states,
and assuming that ϕ1(Xit) −ϕ0(Xit) is constant with respect to Xit, yields the ﬁxed coeﬃcient or
“common eﬀect” version of the model often used in empirical work.
Before-After Estimators
A before-after estimator uses pre-program data to impute counterfactual outcomes for program participants. To simplify notation, assume that the treatment impact
α∗is constant across individuals. Let t′ and t denote time periods before and after the program
start date. The before-after estimator of the program impact is the least squares solution (ˆαBA) to
Yit −Yit′ = ϕ(Xit) −ϕ(Xit′) + α∗+ Uit −Uit′.
For ˆαBA to be a consistent estimator, we require that E(Uit −Uit′) = 0 and E((Uit −Uit′)(ϕ(Xit)−
ϕ(Xit′))) = 0. A special case where this assumption would be satisﬁed occurs when Uit = fi + vit,
where fi depends on i but does not vary over time and vit is a random error term (i.e., Uit satisﬁes
a ﬁxed eﬀect assumption).
A drawback of a before-after estimation strategy is that identiﬁcation of α∗breaks down in the
presence of time-speciﬁc intercepts.7 Before-after estimates can also be sensitive to the choice of
base time period due to “Ashenfelter’s dip”, the commonly observed pattern that the mean earnings
of program participants decline during the period just prior to participation.
See the discussions
in Ashenfelter , Heckman and Smith and Heckman LaLonde and Smith .
Cross-section Estimators
A cross-section estimator uses data on D = 0 persons in a single
time period to impute the outcomes for D = 1 persons in the same time period. Deﬁne ˆαCS as the
ordinary least squares solution to α∗in
Yit = ϕ(Xit) + Diα∗+ Uit.
Bias for α∗arises if E(UitDi) ̸= 0 or E(Uitϕ(Xit)) ̸= 0.
Diﬀerence-in-Diﬀerences Estimators
A diﬀerence-in-diﬀerences (DID) estimator measures
the impact of the program by the diﬀerence between participants and nonparticipants in the beforeafter diﬀerence in outcomes. It uses both pre- and post-program data (t and t′ data) on D = 1 and
7Suppose ϕ(Xit) = Xitβ + γt, where γt is a time-speciﬁc intercept common across individuals. Such a common
time eﬀect may arise, for example, from life-cycle wage growth or from the business cycle.
In this example, α∗is
confounded with γt −γt′.
D = 0 observations. The diﬀerence-in-diﬀerences estimator ˆαD corresponds to the least squares
solution for α∗in
Yit −Yit′ = ϕ(Xit) −ϕ(Xit′) + Diα∗+ {Uit −Uit′}.
This estimator addresses one shortcoming of the before-after estimator in that it allows for timespeciﬁc intercepts that are common across groups.8 The estimator requires that E(Uit −Uit′) = 0,
E((Uit −Uit′)Di) = 0 and E((Uit −Uit′){ϕ(Xit) −ϕ(Xit′)}) = 0.
Lalonde implements
both the standard estimator just described and an “unrestricted” version that includes Yit′ as a
right-hand-side variable. The latter estimator relaxes the implicit restriction in the standard DID
estimator that the coeﬃcient associated with lagged Yit′ equals 1.
Matching Methods
Traditional matching estimators pair each program participant with an observably similar nonparticipant and interpret the diﬀerence in their outcomes as the eﬀect of the program . Matching estimators are justiﬁed by the assumption that outcomes
are independent of program participation conditional on a set of observable characteristics. That
is, matching assumes that there exists a set of observable conditioning variables Z (which may
be a subset or a superset of X) for which the non-participation outcome Y0 is independent of
participation status D conditional on Z,9
Y0 ⊥⊥D |Z .
8To see this, suppose that Yit = γt + Diα∗+ Uit and that Yit′ = γt′ + Uit′. Then Yit −Yit′ = (γt −γt′) + Diα∗+
{Uit −Uit′}, where the diﬀerence in the time-speciﬁc intercepts, (γt −γt′), becomes the intercept in the diﬀerence
In contrast to the before-after estimator, in this case (γt −γt′) and α∗are separately identiﬁed, because
the D = 0 observations, which are not used in the before-after estimator, identify (γt −γt′).
9In the terminology of Rosenbaum and Rubin treatment assignment is “strictly ignorable” given Z.
It is also assumed that for all Z there is a positive probability of either participating (D = 1) or
not participating (D = 0), i.e.
0 < Pr(D = 1|Z) < 1.
This assumption implies that a match can be found for all D = 1 persons. If assumptions (6)
and (7) are satisﬁed, then, after conditioning on Z, the Y0 distribution observed for the matched
non-participant group can be substituted for the missing Y0 distribution for participants.
Assumption (6) is overly strong if the parameter of interest is the mean impact of treatment on
the treated (TT), in which case conditional mean independence suﬃces:
E(Y0|Z, D = 1) = E(Y0|Z, D = 0) = E(Y0|Z).
Furthermore, when TT is the parameter of interest, the condition 0 < Pr(D = 1|Z) is also not
required, because that condition only guarantees the possibility of a participant analogue for each
non-participant. The TT parameter requires only the possibility of a non-participant analogue for
each participant. For completeness, the required condition is
Pr(D = 1|Z) < 1.
Under these assumptions – either (6) and (7) or (8) and (9) – the mean impact of treatment on
the treated can be written as
E(Y1 −Y0|D = 1)
E(Y1|D = 1) −EZ|D=1{EY (Y0|D = 1, Z)}
E(Y1|D = 1) −EZ|D=1{EY (Y0|D = 0, Z)},
where the ﬁrst term can be estimated from the treatment group and the second term from the
mean outcomes of the matched (on Z) comparison group.
In a social experiment, (6) and (7) are satisﬁed by virtue of random assignment of treatment.
For nonexperimental data, there may or may not exist a set of observed conditioning variables for
which the conditions hold. A ﬁnding of HIT and HIST in their application of
matching methods to the JTPA data and of DW in their application to the NSW data
is that (9) was not satisﬁed, meaning that for a fraction of program participants no match could
be found. If there are regions where the support of Z does not overlap for the D = 1 and D = 0
groups, then matching is only justiﬁed when performed over the common support region.10 The
estimated treatment eﬀect must then be redeﬁned as the treatment impact for program participants
whose propensity scores lie within the common support region.
Reducing the Dimensionality of the Conditioning Problem
Matching may be diﬃcult to implement when the set of conditioning variables Z is large.11 Rosenbaum and Rubin prove a result that is useful in reducing the dimension of the conditioning
problem in implementing matching methods. They show that for random variables Y and Z and
a discrete random variable D
E(D|Y, Pr(D = 1|Z)) = E(E(D|Y, Z)|Y, Pr(D = 1|Z)),
so that E(D| Y, Z) = E(D|Z) = Pr(D = 1|Z) implies E(D|Y, Pr(D = 1|Z)) = E(D| Pr(D = 1|Z)).
This implies that when Y0 outcomes are independent of program participation conditional on Z, they
are also independent of participation conditional on the propensity score, Pr(D = 1|Z). Provided
that the conditional participation probability can be estimated using a parametric method, such
as a logit or probit model, or semi-parametrically using a method that converges faster than the
10One advantage of experiments noted by Heckman , as well as HIT and HIST , is that they
guarantee that the treated and untreated individuals have the same support.
This allows estimation of the mean
impact of the treatment over the entire support.
11If Z is discrete, small (or empty) cell problems may arise. If Z is continuous and the conditional mean E(Y1|D =
0, Z) is estimated nonparametrically, then convergence rates will be slow due to the “curse of dimensionality” problem.
nonparametric rate, the dimensionality of the matching problem is reduced by matching on the
univariate propensity score.
If the propensity score must be estimated non-parametrically, then
the curse of dimensionality reappears in the estimation of the propensity score.
This potential
for reducing the dimensionality of the problem has led much of the recent evaluation literature on
matching to focus on propensity score matching methods.12
Propensity score matching combines groups with diﬀerent values of Z but the same values of
Pr(D = 1|Z). To see why this works, consider two groups, one with Z = Z1 and the other with Z =
Z2, but where Pr(D = 1|Z = Z1) = Pr(D = 1|Z = Z2).
Combining these groups in the matching
works because they will have the same relative proportions in the D = 0 and D = 1 populations
precisely because they have the same probabilty of participation. As a result, any diﬀerence in E(Y0)
between the two groups diﬀerences out when calculating E(Y1|D = 1, P(Z))−E(Y0|D = 0, P(Z)).13
Matching Estimators
For notational simplicity, let P = Pr(D = 1|Z). A typical matching estimator takes the form
[Y1i −ˆE(Y0i|Di = 1, Pi)]
ˆE(Y0i|Di = 1, Pi) =
W(i, j)Y0j,
and where I1 denotes the set of program participants, I0 the set of non-participants, SP the region
of common support (see below for ways of constructing this set), and n1 the number of persons in
the set I1 ∩SP . The match for each participant i ∈I1 ∩SP is constructed as a weighted average
12HIT , Hahn and Angrist and Hahn consider whether it is better in terms of eﬃciency to
match on P(X) or on X directly.
For the TT parameter, neither is necessarily more eﬃcient than the other. If the
treatment eﬀect is constant, then it is more eﬃcient to condition on the propensity score.
13See, e.g., Zhao for a discussion of dimension reduction methods other than propensity score matching,
with an application to the NSW data.
over the outcomes of non-participants, where the weights W(i, j) depend on the distance between
Pi and Pj.
Deﬁne a neighborhood C(Pi) for each i in the participant sample. Neighbors for i are nonparticipants
j ∈I0 for whom Pj ∈C(Pi). The persons matched to i are those people in set Ai
where Ai = {j ∈I0 | Pj ∈C(Pi)}. Alternative matching estimators (discussed below) diﬀer in how
the neighborhood is deﬁned and in how the weights W(i, j) are constructed.
Nearest Neighbor matching
Traditional, pairwise matching, also called single nearest-neighbor
matching without replacement, sets
C(Pi) = min
∥Pi −Pj∥, j ∈I0.
That is, the non-participant with the value of Pj that is closest to Pi is selected as the match and
Ai is a singleton set. This estimator is often used in practice due to its ease of implementation.
Traditional applications of this estimator typically did not to impose any common support condition
and matched without replacement, so that each D = 0 observation could serve as the match for
at most one D = 1 observation.
In our empirical work we implement this method with both a
single nearest neighbor and with the ten nearest neighbors. Each nearest neighbor receives equal
weight in constructing the counterfactual mean when using multiple nearest neighbors. The latter
form of the estimator trades reduced variance (resulting from using more information to construct
the counterfactual for each participant) for increased bias (resulting from using, on average, poorer
matches). We also match with replacement, which allows a given non-participant to get matched
to more than one participant.
Matching with replacement also involves a tradeoﬀbetween bias
and variance. Allowing replacement increases the average quality of the matches (assuming some
re-use occurs), but reduces the number of distinct non-participant observations used to construct
the counterfactual mean, thereby increasing the variance of the estimator.
DW show very
clearly that matching without replacement in contexts such as the NSW data, where there are
many participants with high values of Pi and few non-participants with such values, results in
many bad matches, in the sense that many participants get matched to non-participants with very
diﬀerent propensity scores.
More generally, nearest neighbor matching without replacement has
the additional defect that the estimate depends on the order in which the observations get matched.
Caliper matching
Caliper matching is a variant of nearest neighbor
matching that attempts to avoid “bad” matches (those for which Pj is far from Pi) by imposing
a tolerance on the maximum distance ∥Pi −Pj∥allowed. That is, a match for person i is selected
only if ∥Pi −Pj∥< ε, j
I0, where ε is a pre-speciﬁed tolerance. For caliper matching, the
neighborhood is C(Pi) = {Pj | ∥Pi −Pj∥< ε}.
Treated persons for whom no matches can be
found within the caliper are excluded from the analysis. Thus, caliper matching is one way of
imposing a common support condition. A drawback of caliper matching is that it is diﬃcult to
know a priori what choice for the tolerance level is reasonable.
DW employ a variant
of caliper matching called “radius matching.”
In their variant, the counterfactual consists of the
mean outcome of all the comparison group members within the caliper, rather than just the nearest
neighbor.14
Stratiﬁcation or Interval Matching
In this variant of matching, the common support of P is
partitioned into a set of intervals. Within each interval, a separate impact is calculated by taking
the mean diﬀerence in outcomes between the D = 1 and D = 0 observations within the interval.
14In addition, if there are no comparison group members within the caliper, they use the single nearest neighbor
outside the caliper as the match rather than dropping the corresponding participant observation from the analysis.
A weighted average of the interval impact estimates, using the fraction of the D = 1 population in
each interval for the weights, provides an overall impact estimate. DW implement interval
matching using intervals that are selected such that the mean values of the estimated Pi’s and Pj’s
are not statistically diﬀerent within each interval.
Kernel and Local Linear matching
Recently developed nonparametric matching estimators
construct a match for each program participant using a kernel weighted average over multiple
persons in the comparison group. Consider, for example, the kernel matching estimator described
in HIT and HIST , given by
where G(·) is a kernel function and an is a bandwidth parameter. In terms of equation (10), the
weighting function, W(i, j), equals
. The neighborhood C(Pi) depends on the speciﬁc
kernel function chosen for the analysis. For example, for a kernel function that takes on non-zero
values only on the interval (-1,1), the neighborhood is C(Pi) =
standard conditions on the bandwidth and kernel ,
is a consistent estimator of
E(Y0|D = 1, Pi).15
In this paper, we implement a generalized version of kernel matching, called local linear matching.
Research by Fan demonstrates several advantages of local linear estimation over
more standard kernel estimation methods.16 The local linear weighting function is given by
Gik(Pk −Pi)2 −[Gij(Pj −Pi)][ P
Gik(Pk −Pi)]
Gij(Pk −Pi)2 −
Gik(Pk −Pi)
15We assume that G(·) has a mean of zero and integrates to one and that an →0 as n →∞and nan →∞. In
estimation, we use the quartic kernel function, G(s) = 15
16(s2 −1)2 for |s| ≤1, else G(s) = 0.
16These advantages include a faster rate of convergence near boundary points and greater robustness to diﬀerent
data design densities. See Fan .
Kernel matching can be thought of as a weighted regression of Y0j on an intercept with weights
given by the kernel weights, W(i.j), that vary with the point of evaluation.
The weights depend
on the distance between each comparison group observation and the participant observation for
which the counterfactual is being constructed.
The estimated intercept provides the estimate of
the counterfactual mean. Local linear matching diﬀers from kernel matching in that it includes
in addition to the intercept a linear term in Pi.
Inclusion of the linear term is helpful whenever
comparison group observations are distributed asymmetrically around the participant observations,
as would be the case at a boundary point of P or at any point where there are gaps in the distribution
Trimming to Determine the Support Region
To implement the matching estimator given
by equation (10), the region of common support SP needs to be determined. By deﬁnition, the
region of common support includes only those values of P that have positive density within both
the D = 1 and D = 0 distributions. The common support region can be estimated by
ˆSP = {P : ˆf(P|D = 1) > 0 and ˆf(P|D = 0) > 0},
where ˆf(P|D = d), d ∈{0, 1} are nonparametric density estimators given by ˆf(P|D = d) =
.18 To ensure that the densities are strictly greater than zero, we require that the
densities be strictly positive and exceed zero by a threshold amount determined by a “trimming
level” q. After excluding any P points for which the estimated density is exactly zero, we exclude
an additional q percent of the remaining P points for which the estimated density is positive but
17See Fan and Gijbels for detailed discussion of the distinction between standard kernel regression and local
linear regression methods and Fr¨olich for a Monte Carlo analysis of alternative matching methods.
18In implementation, we select a ﬁxed, global bandwidth parameter using Silverman’s rule-of-thumb method.
very low. The set of eligible matches are therefore given by
ˆSq = {P ∈I1 ∩ˆSP : ˆf(P|D = 1) > cq and ˆf(P|D = 0) > cq},
where cq is the density cut-oﬀtrimming level.19
HIST and HIT also implement a variant of local linear matching which they call
“regression-adjusted matching.” In this variant, the residual from a regression of Y0j on a vector of
exogenous covariates replaces Y0j as the dependent variable in the matching. Regression adjustment
can, in principal, be applied in combination with any of the other matching estimators; we apply
it in combination with the local linear estimator in Section 8 below.
Diﬀerence-in-diﬀerences matching
The estimators described above assume that after conditioning on a set of observable characteristics, mean outcomes are conditionally mean independent
of program participation. However, there may be systematic diﬀerences between participant and
nonparticipant outcomes even after conditioning on observables. Such diﬀerences may arise, for example, (i) because of selection into the program based on unmeasured characteristics, (ii) because
of diﬀerences in earnings levels among the labor markets in which the participants and nonparticipants reside, or (iii) because earnings outcomes for participants and nonparticipants are measured
in diﬀerent ways (as when data are collected using diﬀerent survey instruments). Such diﬀerences
violate the identiﬁcation conditions required for matching.
A diﬀerence-in-diﬀerences (DID) matching strategy, as deﬁned in HIT and HIST ,
allows for temporally invariant diﬀerences in outcomes between participants and nonparticipants.
19The qth quantile, cq, is determined by solving for
{1( ˆf(P|D = 1) < cq) + 1( ˆf(P|D = 0) < cq)} ≤q,
where J is the number of observed values of P that lie in I1 ∩ˆSP . Matches are constructed only for the program
participants whose propensity scores lie in ˆSq. In our empirical work, we set the trimming level at two percent.
This type of estimator is analogous to the standard DID regression estimator deﬁned in Section
3.2, but it does not impose the linear functional form restriction in estimating the conditional
expectation of the outcome variable and it reweights the observations according to the weighting
functions used by the matching estimators. The DID propensity score matching estimator requires
E(Y0t −Y0t′|P, D = 1) = E(Y0t −Y0t′|P, D = 0),
where t and t′ are time periods after and before the program start date, respectively. This estimator
also requires the support condition given in (7) or (9), which must hold in both periods t and t′
(a non-trivial assumption given the attrition present in many panel data sets). The diﬀerence-indiﬀerences matching estimator is given by
(Y1ti −Y0t′i) −
W(i, j)(Y0tj −Y0t′j)
where the weights depend on the particular cross-sectional matching estimator employed in the
before and after periods. If repeated cross-section data are available, instead of longitudinal data,
the estimator can be implemented as
W(i, j)Y0tj
W(i, j)Y0t′j
where I1t, I1t′, I0t, I0t′ denote the treatment and comparison group datasets in each time period.
We implement the panel data version of the estimator in the empirical work reported below and
ﬁnd it to be more robust than the cross-sectional matching estimators.20
20When using repeated cross section data, the identity of future participants and non-participants may not be
known in the pre-program period.
A variant of the diﬀerence-in-diﬀerences matching estimator presented here for
that context appears in Blundell and Costa-Dias .
Choice-Based Sampled Data
The samples used in evaluating the impacts of programs are often choice-based, with program
participants oversampled relative to their frequency in the population of persons eligible for the
program. Under choice-based sampling, weights are required to consistently estimate the probabilities of program participation.21 When the weights are unknown, Heckman and Todd 
show that with a slight modiﬁcation, matching methods can still be applied, because the odds ratio
estimated using the incorrect weights (i.e., ignoring the fact that samples are choice-based) is a
scalar multiple of the true odds ratio, which is itself a monotonic transformation of the propensity
scores. Therefore, matching can proceed on the (misweighted) estimate of the odds ratio (or of the
log odds ratio). In our empirical work, the data are choice-base sampled and the sampling weights
are unknown, so we match on the odds ratio, P/(1 −P).22
When Does Bias Arise in Matching?
The success of a matching estimator clearly depends on the availability of observable data to
construct the conditioning set Z, such that (6) and (7), or (8) and (9), are satisﬁed. Suppose only
a subset Z0 ⊂Z of the variables required for matching is observed. The propensity score matching
estimator based on Z0 then converges to
M = EP(Z0)|D=1 (E(Y1|P(Z0), D = 1) −E(Y0|P(Z0), D = 0)) .
The bias for the parameter of interest, E(Y1 −Y0|D = 1), is
biasM = E(Y0|D = 1) −EP(Z0)|D=1{E(Y0|P(Z0), D = 0)}.
21See, e.g., Manski and Lerman for a discussion of weighting for logistic regressions.
22With single nearest neighbor matching, it does not matter whether matching is performed on the odds ratio or
on the propensity scores (estimated using the wrong weights), because the ranking of the observations is the same
and the same neighbors will be selected. Thus, failure to account for choice-based sampling should not aﬀect the
nearest-neighbor point estimates in the DW studies. However, for methods that take account of the
absolute distance between observations, such as kernel matching or local linear matching, it does matter.
HIST show that what variables are included in the propensity score matters in practice
for the estimated bias.
They ﬁnd that the lowest bias values arise when Z includes a rich set of
variables that aﬀect both program participation and labor market outcomes. They obtain higher
bias values using cruder sets of Z variables. Similar ﬁndings regarding the sensitivity of matching
estimates to the set of matching variables appear in Lechner and in Section 8 of this paper.
Using Data on Randomized-out Controls and Nonparticipants to Estimate
Evaluation Bias
With only nonexperimental data, it is impossible to disentangle the treatment eﬀect from the
evaluation bias associated with any particular estimator.
However, data on a randomized-out
control group makes it possible to separate out the bias. First, subject to some caveats discussed
in Heckman and Smith and Heckman, LaLonde and Smith , randomization ensures
that the control group is identical to the treatment group in terms of the pattern of self-selection.
Second, the randomized-out control group does not participate in the program, so the impact of
the program on them is known to be zero.
Thus, a nonexperimental estimator applied to the
control group data combined with nonexperimental comparison group data should, if consistent,
produce an estimated impact equal to zero. Deviations from zero are properly interpretable as
evaluation bias.23 Therefore, the performance of alternative nonexperimental estimators can be
evaluated by applying the estimators to data from the randomized-out control group and from the
nonexperimental comparison group and then checking whether the resulting estimates yield are
statistically distinguishable from zero.
23A diﬀerent way of isolating evaluation bias would be to compare the program impact estimated experimentally
(using the treatment and randomized-out control samples) to that estimated nonexperimentally (using the treatment
and comparison group samples). This approach is taken in Lalonde and in DW . The procedure
we use, which compares the randomized-out controls to nonparticipants, is equivalent and a more direct way of
estimating the bias. It is also more eﬃcient in our application as the control group is larger than the treatment
group. The latter approach is also taken in HIT and HIST .
The National Supported Work Demonstration
The National Supported Work (NSW) Demonstration24 was a transitional, subsidized work experience program that operated for four years at ﬁfteen locations throughout the United States.
served four target groups: female long-term AFDC recipients, ex-drug addicts, ex-oﬀenders, and
young school dropouts.
The program ﬁrst provided trainees with work in a sheltered training
environment and then assisted them in ﬁnding regular jobs.
About 10,000 persons experienced
12-18 months of employment through the program, which cost around $13,850 per person in 1997
To participate in NSW, potential participants had to satisfy a set of eligibility criteria that were
intended to identify individuals with signiﬁcant barriers to employment. The main criteria were:
(1) the person must have been currently unemployed (deﬁned as having worked no more than 40
hours in the four weeks preceeding the time of selection into the program), and (2) the person must
have spent no more than three months on one regular job of at least 20 hours per week during
the preceding six months. As a result of these criteria as well as of self-selection into the program,
persons who participated in NSW diﬀer in many ways from the general U.S. population.
From April 1975 to August 197725 the NSW program in 10 locations operated as a randomized
experiment with some program applicants being randomly assigned to a control group that was
not allowed to participate in the program.26 The experimental sample includes 6,616 treatment
and control observations for which data were gathered through a retrospective baseline interview
and four follow-up interviews. These interviews cover the two years prior to random assignment
24See Hollister, Kemper and Maynard for a detailed description of the NSW demonstration and Couch
 for long-term experimental impact estimates.
25Our sample does not include persons randomly assigned in all of these months due to the sample restrictions
imposed by LaLonde .
26Then ten locations where random assignment took place are Atlanta, Chicago, Hartford, Jersey City, Newark,
New York, Oakland, Philadelphia, San Francisco, and Fond du Lac and Winnebago counties in Wisconsin.
and up to 36 months thereafter. The data provide information on demographic characteristics,
employment history, job search, mobility, household income, housing and drug use.27 As noted in
Heckman, LaLonde and Smith , the NSW is an ideal experiment in the sense that, unlike
many other social experiments, almost everyone in the experimental treatment group participates
in the program and no one in the experimental control group receives a similar treatment from
other sources (though a small fraction receive much less intensive employment and training services
under the CETA program).
In this study, we consider three experimental samples and two non-experimental comparison groups.
All of the samples are based on the male samples from LaLonde .28
The experimental
sample includes male respondents in the NSW’s ex-addict, ex-oﬀender and high school dropout
target groups who had valid pre- and post-program earnings data.
The ﬁrst experimental sample is the same as that employed by LaLonde .
The sample
consists of 297 treatment group observations and 425 control group observations.
Descriptive
statistics for the LaLonde experimental sample appear in the ﬁrst column of Table 1.
statistics show that solid majorities of male NSW participants were minorities (mostly black), high
school dropouts and unmarried.
As was its aim, the NSW program served a highly economically
disadvantaged population.
The earnings variables for the NSW samples are all based on self-reported earnings measures
from surveys.29 Following LaLonde , all of the earnings variables (for all of the samples) are
27In addition, persons in the AFDC target group were also asked about children in school and welfare participation
and non-AFDC target groups were asked about illegal activities.
28We do not examine LaLonde’s sample of AFDC women as it is no longer available due to data storage
29As noted in Section 2, grouped social security earnings data are also available for the NSW experimental sample,
expressed in 1982 dollars. The variable denoted “Real Earnings in 1974” consists of real earnings
in months 13 to 24 prior to the month of random assignment. For persons randomly assigned early
in the experiment, these months largely overlap with calendar year 1974. For persons randomly
assigned later in the experiment, these months largely overlap with 1975.
This is the variable
denoted “Re74” in DW . The variable “Zero Earnings in 1974” is an indicator variable
equal to one when the “Real Earnings in 1974” variable equals zero.30
The Real Earnings in 1975
variable corresponds to earnings in calendar year 1975; the indicator variable for Zero Earnings in
1975 is coded to one if Real Earnings in 1975 equal zero. Mean earnings in the male NSW sample
prior to random assignment were quite low. They also fall from 1974 to 1975, another example of
the common pattern denoted “Ashenfelter’s dip” in the literature . In order to include
two years of pre-program earnings in their model of program participation, DW omit the (approximately) 40 percent of Lalonde’s original sample for which that information is missing.31
While DW provide general descriptions of the sample selection criteria they used to
generate their analysis samples, we required the exact criteria to replicate their results and to examine alternative propensity scores using their sample.32
Table 2 illustrates the sample inclusion
and were employed by Heckman and Hotz in their analysis.
We do not use them here in order to maintain
comparability with LaLonde and DW .
30This is the variable denoted “U74” in DW ; note that it corresponds to non-employment rather than
unemployment.
31The inclusion of the additional variable was motivated by ﬁndings in the earlier literature. Heckman and Smith
 show that variables based on labor force status in the months leading up to the participation decision perform
better at predicting program participation in the National JTPA Study data than do annual or quarterly earnings.
See also related discussions in Ashenfelter , Ashenfelter and Card , Card and Sullivan and Angrist
 on this point.
32See the bottom of the ﬁrst column of page 1054 of DW for their descriptions.
criteria that we found (partly through trial and error) which correctly account for all but one observation in their sample.33 The table is a cross-tabulation of LaLonde’s sample with month
of random assignment as rows and zero earnings in months 13 to 24 as columns.
Corresponding
to the rows and columns of Table 2, their rule has two parts. First, include everyone randomly
assigned in January through April of 1976. This group corresponds to the eight shaded cells in the
bottom four rows of Table 2.
Second, of those who were randomly assigned after April of 1976,
only include persons with zero earnings in months 13 to 24 before random assignment. This group
corresponds to the six shaded cells at the top of the left column of Table 2.
Left out of the sample are those members of LaLonde’s sample who were randomly assigned after April 1976
and had positive earnings in months 13 to 24 before random assignment.
This rule corresponds
fairly closely to the verbal statement in DW . We do not believe that the second rule is
appropriate. DW state that they want to use “earnings in 1974” as an additional control variable.
However, as already noted, earnings in months 13 to 24 before random assignment either do not
overlap calendar year 1974 or do so only for a few months for those included under the second part
of the rule.
The second column of Table 1 displays the descriptive statistics for the DW sample.
most dimensions, the DW sample is similar to the full LaLonde sample. One key diﬀerence results
from the second part of the rule, which diﬀerentially includes persons with zero earnings in parts
of 1974 and 1975. As a result, mean earnings in both years are lower for the DW sample than for
the larger Lalonde sample.
The other key diﬀerence is in the experimental impact estimate. At
33Dehejia provided us with both their version of the LaLonde sample and a version of the DW sample in separate
Both ﬁles are available on Dehejia’s web page at 
However, neither ﬁle
includes identiﬁcation numbers, so there is no simple way to link them to determine the exact sample restrictions
By trying diﬀerent combinations of sample inclusion criteria, we determined the rules for generating the
subsample. One control group observation is included by the rules stated here but excluded from their sample. Our
estimates below using the “DW” sample do not include this extra observation.
$1794 it is more than twice as large as that for the Lalonde sample.
The third experimental sample we examine is not used in either Lalonde or DW .
It is a proper subset of the DW sample that excludes persons randomized after April of 1976. We
examine this sample because we ﬁnd their decision to include persons randomized after April of
1976 only if they had zero earnings in months 13 to 24 problematic. Our “Early RA” sample
consists of persons randomly assigned during January through April of 1976; put diﬀerently, this
sample consists of the observations in the bottom four rows of Table 2. This sample includes 108
treatment group members and 142 control group members.
Descriptive statistics for this sample
appear in the third column of Table 1.
Ashenfelter’s dip is stronger for this sample (a drop of
about $1200 rather than one of about $700) than for the DW sample, as is to be expected given
that it drops the large contingent of persons with zero earnings in months 13 to 24 prior to random
assignment.
The $2748 experimental impact for the Early RA sample is the largest among the
three experimental samples.
The comparison group samples we use are the same ones used by LaLonde and DW
 . Both are based on representative national samples drawn from throughout the United
States. This implies that the vast majority of comparison group members, even those with observed
characteristics similar to the experimental sample members, are drawn from diﬀerent local labor
In addition, earnings are measured diﬀerently in both comparison group samples than
they are in the NSW data.
The ﬁrst comparison group sample is based on Westat’s matched Current Population Survey –
Social Security Administration ﬁle. This ﬁle contains male respondents from the March 1976 Current Population Survey (CPS) with matched Social Security earnings data.
The sample excludes
persons with nominal own incomes greater than $20,000 and nominal family incomes greater than
$30,000 in 1975. Men over age 55 are also excluded. Descriptive statistics for the CPS comparison
group appear in the fourth column of Table 1.
Examination of the descriptive statistics reveals
that the CPS comparison group is much older, much better educated (70 percent completed high
school), much less likely to be black or Hispanic and much more likely to be married than any of
the NSW experimental samples.
The earnings measures for the CPS sample are individual-level administrative annual earnings
totals from the U.S. Social Security system. The CPS comparison group sample had, on average,
much higher earnings than the NSW experimental sample in every year. There is a slight
dip in the mean earnings of the CPS comparison group from 1974 to 1975; this dip is consistent
with the imposition of maximum individual and family income criteria in 1975 for inclusion in the
sample along with some level of mean-reversion in earnings .
The very substantial diﬀerences between this comparison group and the
NSW experimental group pose a tough problem for any non-experimental estimator to solve.
The second comparison group sample is drawn from the Panel Study of Income Dynamics
(PSID). It consists of all male household heads from the PSID who were continuously present in
the sample from 1975 to 1978, who were less than 55 years old and who did not classify themselves
as retired in 1975.34
Descriptive statistics for the PSID comparison group sample appear in the
ﬁfth column of Table 1.
The PSID comparison group strongly resembles the CPS comparison
group in its observable characteristics.
Mean earnings levels in the PSID sample are higher than
those in the CPS sample and the fraction with zero earnings in 1974 and 1975 lower, most likely due
to the maximum income criteria imposed in selecting the CPS sample.
The over-representation
34Following DW , we drop the three persons from LaLonde’s sample who are missing data on education.
of blacks in the PSID comparison group sample relative to the U.S. population appears to result
from the use of both the representative sample component of the PSID and the component based
on the Survey of Economic Opportunity sample, which consists of low income urban residents in
the North and low income rural residents in the South.35
LaLonde also considers four other comparison groups consisting of various subsets of
the CPS and PSID comparison groups just described.
As deﬁned in the notes to his Table 3,
these subsamples condition on various combinations of employment, labor force status and income
in 1975 or early 1976. We do not examine these subsamples here for two main reasons.
taking these subsamples and then applying matching essentially represents doing “matching” in
two stages - ﬁrst crudely based on a small number of characteristics and then more carefully using
the propensity score.36 As discussed in Heckman, LaLonde and Smith , such estimators (like
estimators consisting of crude matching followed by some other non-experimental estimator) do
not always have clear economic or econometric justiﬁcations. One case where they do is when the
ﬁrst stage amounts to imposing the program’s eligibility rules, thereby dropping from the sample
individuals whose probability of participation is known to equal zero; unfortunately, the CPS and
PSID data sets lack the information required to apply the NSW eligibility rules with any degree of
precision.
Another case where such sample restrictions would have some justiﬁcation consists of
excluding non-participants in local labor markets with no participants from the analysis.
this with the CPS or PSID data in the NSW context would leave only tiny comparison samples.
Second, Table 3 of DW shows that, in the context of propensity score matching, the ﬁrst
round of crude matching performed by LaLonde has little eﬀect on the resulting estimates.
35See the detailed information about the PSID at 
36Even the full CPS comparison group sample we use has this feature due to the conditioning on individual and
family income in 1975 performed by Westat in creating the sample.
The propensity score matching estimator for the full sample assigns little or no weight to those
sample members who get excluded by the crude matching used to create the subsamples.37
Propensity Scores
We present matching estimates based on two alternative speciﬁcations of the propensity score,
Pr(D = 1|Z). The ﬁrst speciﬁcation is that employed in DW ; the second speciﬁcation
is based on LaLonde . Although Lalonde does not consider matching estimators, he estimates
a probability of participation in the course of implementing the classical selection estimator of
Heckman . In both cases, we use the logit model to estimate the scores.
The estimated coeﬃcients and associated estimated standard errors for the propensity scores
based on the DW speciﬁcation appear in Table 3.38
We estimate six sets of scores, one for
each pair of experimental and comparison group samples. In each case, the dependent variable is an
indicator for being in the experimental sample. We follow DW in including slightly diﬀerent sets of
higher order and interaction terms in the speciﬁcations for the CPS and PSID comparison groups.
These terms were selected using their propensity score speciﬁcation selection algorithm, discussed in
the next section. Our estimated scores for the DW speciﬁcation with the DW sample diﬀer slightly
from theirs for two reasons.
First, for eﬃciency reasons we use both the experimental treatment
and experimental control group in estimating the scores, whereas DW appear to use
only the treatment group.39
Second, although DW did not include a constant term in the
37Because the NSW operated in only ten locations and served only a small number of individuals in total, the
probability that even one of our comparison group members participated in NSW is very low.
Hence, the problem
of “contamination bias” (the nonexperimental analogue of substitution bias) deﬁned by Heckman and Robb 
does not arise in our analysis.
38DW use slightly diﬀerent speciﬁcations for both the CPS and PSID comparison groups.
Compare the
notes to Tables 2 and 3 in DW with the notes to Table 3 in DW .
39We experimented a bit with generating estimates based on scores estimated using just the treatment group, just
the control group and both the treatment and control groups.
The samples are small enough that this choice can
move the resulting impact estimates around by two or three hundred dollars.
logistic model, we do.
Most of the coeﬃcient estimates for the DW model are in the expected direction given the
diﬀerences observed in Table 1.
For example, high school dropouts are more likely to participate
in NSW, as are blacks and hispanics, while marriage has a strong negative eﬀect on the probability
of participation.
In the CPS sample, participation probabilities decrease with earnings in both
“1974” and 1975. In the PSID sample, the relationship is quadratic. The estimated probability of
participation is also non-linear in age and education in both samples, with a maximum at around
23.4 years of age for the DW experimental sample and the PSID comparison group. The qualitative,
and also the quantitative, pattern of the coeﬃcients is extremely similar across experimental samples
with the same comparison group.
There are, though, a few diﬀerences across comparison groups
for the same experimental sample, perhaps because of the somewhat diﬀerent speciﬁcations.
With the CPS comparison group, the correlations between scores estimated on diﬀerent experimental samples are around 0.93.
With the PSID, they are a bit higher at around 0.97.
ﬁgure suggests that estimating the score on a particular experimental sample matters much. Using
the prediction rate metric as one tool to assess the quality of the propensity scores shows that the
speciﬁcation does a good job of separating out the participants and the non-participants.40
use the fraction of the combined sample that consists of experimentals as the cutoﬀfor predicting someone to be a participant.
For the DW scores applied to the DW sample, 94.1 percent of
the CPS comparison group members are correctly predicted to be non-participants and 94.6 percent of the experimental sample is correctly predicted to participate.
For the DW scores applied
to the LaLonde and early RA samples, the corresponding correct prediction rates are (95.6,85.3)
and (91.2,94.8).
The prediction rates are similar, but a bit lower in some cases, with the PSID
40This metric is discussed in Heckman and Smith and HIST . For caveats, see Heckman and Navarro-
Lazano .
comparison group.
Figure 1 presents histograms of the estimated log-odds ratios for the DW propensity score
model applied to each of the three experimental samples with each of the two comparison groups.
These ﬁgures allow a graphical assessment of the extent of any support problems in the NSW
The ﬁgures make readily apparent that the distributions of scores among the experimental
samples diﬀer strongly from those of both of the comparison groups.
For every combination of
experimental sample and comparison group, the density for the comparison group lies well to the
left of that of the experimentals. This indicates that many comparison group members have very
low predicted probabilities of participation in the NSW program. This ﬁnding comports with the
strong diﬀerences in observable characteristics reported in Table 1. However, the support problem
here is not as strong as in the JTPA data examined in HIST , where there were large
intervals of P with no comparison group observations at all.
For the two comparison groups
employed here, even at high probabilities, such as those above 0.9, there are at least a handful of
comparison group observations.
Table 4 presents the coeﬃcient estimates from the participation model in LaLonde .41 The
41We ran into two small diﬃculties in replicating LaLonde’s scores that we resolved as follows.
Lalonde indicates that he includes a dummy variable for residence in an SMSA in his model.
Given that everyone
in the NSW experimental sample lives in an SMSA, not living in an SMSA is a perfect predictor of not being in the
NSW demonstration. Thus, this variable should not be included in the model. We dealt with this in two ways. In
one case, we just dropped this variable from the speciﬁcation.
In the other, we set the participation probability to
zero for everyone not in an SMSA and then estimated the model on those who remained.
The scores produced in
these two ways had a correlation of 0.9734 in the combined LaLonde experimental sample and CPS comparison
group sample and a correlation of 0.9730 in the combined sample with the PSID. The estimates presented in Table 4
are for the speciﬁcation that sets the probability to zero for all CPS and PSID comparison group members not living
in an SMSA.
The second issue concerns missing values of the variables for the number of children.
There are missing values
for observations in the experimental sample and in the CPS comparison group, but not in the PSID sample.
result of the asymmetry between the two comparison groups in this regard, we adopt separate strategies in the two
In estimating the LaLonde propensity score model with the CPS comparison group, we set missing values of
the number of children to zero and include an indicator variable set to one for observations with a missing value and
zero otherwise.
In the PSID case, we impute missing values of the number of children variable in the experimental
data by running a regression of number of children on a set of exogenous covariates (including interactions of age and
age squared with race and ethnicity).
patterns are quite similar to those for the DW scores.
The participation probability is quadratic
in age, with a maximum at 25.3 years for the LaLonde sample with the CPS comparison group and
a maximum at 20.2 years for the LaLonde sample with the PSID comparison group. As expected
given the diﬀerences seen in Table 1, being a high school dropout, being black and being Hispanic
have strong and statistically signiﬁcant positive eﬀects on participation. In contrast, being married
and being employed in March of 1976 have strong and statistically signiﬁcant negative eﬀects on
participation.42
Finally, number of children has a strong negative eﬀect on the participation
probability, particularly in the CPS sample.
Like the DW scores, the LaLonde scores estimated on diﬀerent experimental samples are highly
correlated; in every case the correlation exceeds 0.97. The prediction rates are similar as well. For
the LaLonde scores with the LaLonde experimental sample and the CPS comparison group, 95.4
percent of the participants are correctly predicted along with 94.7 percent of the comparison group.
With the PSID, the corresponding values are 95.0 and 92.8 percent.
Similar percentages hold for
the other experimental samples, but with slightly higher prediction rates for the participants and
slightly lower ones for the non-participants. The correlations between the LaLonde scores and the
DW scores are between 0.77 and 0.83 for the CPS comparison group and between 0.88 and 0.93 for
the PSID comparison group; it is not clear why the correlation is higher in the PSID case.
both samples, but particularly with the CPS, it is clear that the LaLonde scores diﬀer meaningfully
from the DW scores.
Finally, Figure 1 shows that the LaLonde scores for all three experimental
samples, like the DW scores, are spread out over the full range between zero and one, but the
density is quite thin among non-participants at the higher scores.
42The latter variable represents an attempt to capture one aspect of the NSW eligibility rules.
Nonetheless, it is
somewhat problematic, given that some members of the NSW sample are randomly assigned in January and February
of 1976, and therefore some treatment group members could be employed as part of the program by March of 1976.
Given the sign and magnitude of the estimated coeﬃcient, this concern appears to be a minor one.
Variable Selection and the Balancing Test
Equation (8) above displays the conditional mean independence assumption required for application of propensity score matching. The outcome variable must be conditionally mean independent
of treatment conditional on the propensity score, P(Z). Implementing matching requires choosing
a set of variables Z that plausibly satisfy this condition. This set should include all of the key
factors aﬀecting both program participation and outcomes – that is, all the variables that aﬀect
both D and Y0. No mechanical algorithm exists that automatically chooses sets of variables Z that
satisﬁes the identiﬁcation conditions. Moreover, the set Z that satisﬁes the matching conditions
is not necessarily the most inclusive one.
Augmenting a set of variables that satisﬁes the identi-
ﬁcation conditions for matching could lead to a violation of those conditions.
Adding additional
conditioning variables may also exacerbate a common support problem.
Finally, it should always
be kept in mind that any given data set may contain no combination of variables Z that satisfy the
conditional indendence assumption. In the latter case, matching is not an appropriate estimator.
To guide in the selection of Z, there is some accumulated empirical evidence on how bias
estimates of matching estimators depend on the choice of Z in particular applications. For example,
HIT , HIST , Heckman and Smith and Lechner show that which variables
are included in the estimation of the propensity score can make a substantial diﬀerence to the
performance of the estimator. These papers ﬁnd, in general, larger biases with cruder conditioning
sets. Theory also provides a guide to variables likely to aﬀect both participation and outcomes in
particular contexts.
Rosenbaum and Rubin present a theorem (see their Theorem 2) that does not aid in
choosing which variables to include in Z, but which can help in determining which interactions and
higher order terms to include for a given set of included Z variables. The theorem states that
Z⊥⊥D| Pr(D = 1|Z),
or equivalently
E(D|Z, Pr(D = 1|Z)) = E(D| Pr(D = 1|Z)).
The basic intuition is that after conditioning on Pr(D = 1|Z), additional conditioning on Z should
not provide new information about D.
Thus, if after conditioning on the estimated values of
P(D = 1|Z) there is still dependence on Z, this suggests misspeciﬁcation in the model used to
estimate Pr(D = 1|Z). Note that the theorem holds for any Z, including sets Z that do not satisfy
the conditional independence condition required to justify matching. As such, the theorem is not
informative about what set of variables to include in Z.
This theorem motivates a speciﬁcation test for Pr(D = 1|Z).
The general idea is to test
whether or not there are diﬀerences in Z between the D = 1 and D = 0 groups after conditioning
The test has been implemented in the literature in a number of ways.
 , Lechner and Eichler and Lechner use a variant of a measure suggested in
Rosenbaum and Rubin that is based on standardized diﬀerences between the treatment
and matched comparison group samples in terms of means of each variable in Z, squares of each
variable in Z and ﬁrst-order interaction terms between each pair of variables in Z. An alternative
approach used in DW divides the observations into strata based on the estimated
propensity scores. These strata are chosen so that there is not a statistically signiﬁcant diﬀerence
in the mean of the estimated propensity scores between the experimental and comparison group
observations within each strata, though how the initial strata are chosen and how they are reﬁned
if statistically signiﬁcant diﬀerences are found is not made precise.
The problem of choosing
the strata in implementing the balancing test is analogous to the problem of choosing the strata in
implementing the interval matching estimator, described earlier. Then, within each stratum, t-tests
are used to test for mean diﬀerences in each Z variable between the experimental and comparison
group observations.
When signiﬁcant diﬀerences are found for particular variables, higher order
and interaction terms in those variables are added to the logistic model and the testing procedure
is repeated, until such diﬀerences no longer emerge.
As described earlier, we use two diﬀerent model speciﬁcations to estimate propensity scores in
this paper. The speciﬁcation based on DW was selected using the balancing test strategy
described above. The speciﬁcation based on Lalonde was selected on the basis of how well
the model predicted program participation. We retain Lalonde’s original speciﬁcation in
this paper when we implement the matching estimators to allow for easy comparison of his results
with our results, which are based on diﬀerent estimators and diﬀerent sample inclusion criteria.
Matching Estimates
We now present our estimates of the bias obtained when we apply matching to the experimental
NSW data and the two diﬀerent nonexperimental comparison groups. Our estimation strategy differs somewhat from that of Lalonde and DW in that we obtain direct estimates
of the bias by applying matching to the randomized-out control group and the nonexperimental
comparison groups, whereas the other papers obtain the bias indirectly by applying matching to
the treatment and comparison groups and comparing the resulting experimental and the nonexperimental impact estimates. Second, in constrast to DW , we match on the log-odds
ratio rather than on the propensity score itself, so that our estimates are robust to choice-based
Finally, we impose the common support condition using the trimming method described above,
which diﬀers from the method used by DW that discards comparison group observations with estimated propensity scores that lie below the minimum or above the maximum of the
estimated scores in the experimental sample.43
The main advantage of this approach is ease of
implementation.
While somewhat more diﬃcult to implement, our approach has two substantive
advantages. First, we do not throw out good matches that lie just below the minimum estimated
score in the D = 1 sample (or just above the estimated maximum).
Second, we allow for gaps
in the empirical common support that lie between the extreme values of the estimated propensity
scores in the experimental sample.
This is important because the nonparametric regression estimators of the counterfactual mean outcomes are unreliable when evaluated at P points where the
estimated density is close to zero. In practice, our method of imposing the support condition is
somewhat more stringent than that of DW, as we drop ﬁve to ten percent of the D = 1 sample
due to the common support condition, in addition to dropping a fraction of the comparison group
samples similar to that dropped by DW.
Cross-Sectional Matching Estimates
Estimates of the bias associated with cross-sectional matching on the propensity score appear in
Tables 5A and 5B. These are estimates of the bias expression given previously in equation (13).
We ﬁrst consider Table 5A, which shows the estimates for the CPS comparison group.
outcome variable throughout both Tables 5A and 5B is earnings in calendar year 1978, where
January 1978 is at least ﬁve months after random assignment for all of the controls.
column of Table 5A gives the simple mean diﬀerence in 1978 earnings between each experimental
43See the ﬁrst column of page 1058 in DW .
control group and the CPS comparison group. The remaining columns present estimates of the bias
associated with diﬀerent matching estimators. The ﬁrst six rows of the table refer to estimates using
the DW propensity score speciﬁcation, while the ﬁnal two rows refer to the LaLonde propensity
score speciﬁcation.
Each pair of rows presents bias estimates for one experimental sample along
with the percentage of the experimental impact estimate for that sample that the bias estimate
represents.
These percentages are useful for comparisons of diﬀerent estimators within each row,
but are not useful for comparisons across rows given the large diﬀerences in experimental impact
estimates among the three experimental samples. We present bootstrap standard errors based on
100 replications below each estimate; the standard errors for the percentage impacts assume the
experimental impact is constant.
The second through the ﬁfth columns in Tables 5A and 5B give various estimates based on
nearest neighbor matching, deﬁned above in Section 3.3.
The second and third columns present
estimates from matching using the one and ten nearest neighbors, respectively, without imposing
the common support condition.
The fourth and ﬁfth columns present estimates using the same
methods but imposing the common support condition.
Five important patterns characterize the
nearest neighbor estimates for the CPS comparison group.
First, using the DW experimental
sample and DW propensity score model, we replicate the low biases that were reported in DW
 .
Second, when the DW propensity score model is applied to the Lalonde sample or
to the Early RA sample, the bias estimates are substantially higher.
Indeed, the bias estimates
for the DW scores as applied to the Early RA sample are among the largest in the table.
the imposition of the common support condition has little eﬀect on the estimates for the LaLonde
and DW samples, but does result in a substantial reduction in bias for the Early RA sample.
Fourth, increasing the number of nearest neighbors reduces bias in the relatively small Early RA
sample, but does little to change the bias estimates for the other two experimental samples. Fifth,
when the LaLonde propensity score model is applied to the LaLonde sample, it does quite poorly
in terms of bias, though not as poorly as the DW scores in the Early RA sample.
results obtained by DW using simple nearest neighbor matching with their propensity
scores are highly sensitive to changes in the sample composition. Moreover, adopting a reasonable
alternative propensity score speciﬁcation strongly increases the estimated bias in the full LaLonde
The remaining four columns present estimates obtained using local linear matching methods.
The sixth and seventh columns report estimates obtained using regular local linear matching with
two diﬀerent bandwidths. Increasing the bandwidth will, in general, increase the bias and reduce
the variance associated with the estimator by putting a heavier weight on the information provided by more distant observations in constructing the counterfactual for each D = 1 observation.
Interestingly, in Table 5A, both the variance and the overall average bias usually decrease when
we increase the bandwidth. The ﬁnal two columns present estimates obtained using regressionadjusted local linear matching, again with two diﬀerent bandwidths.
The notes to Table 5A list
the variables used to do the regression adjustment.
The lessons from the local linear matching estimates are largely the same as those from the
nearest neighbor estimates.
The DW scores do well in their sample, but have much larger biases
in the LaLonde sample and in the Early RA sample. The LaLonde scores have large biases in his
Once again, the results in DW are sensitive to changes in the sample and,
in the full LaLonde sample, an alternative propensity score speciﬁcation yields much larger biases.
The one additional ﬁnding is that, consistent with HIT , the matching estimates do not show
much sensitivity, at least in terms of the qualitative conclusion they provide, to changing the ﬁxed
bandwidth from 1.0 to 4.0 and the local linear matching results do not change much when we use
regression adjusted matches.
Table 5B presents estimates analogous to those in Table 5A but constructed using the PSID
comparison group.
The unadjusted mean diﬀerences shown in the ﬁrst column are substantially
larger here than with the CPS comparison group, presumably due to the sample restrictions imposed
in constructing the CPS sample but not in the PSID sample. Thus, at some level, matching faces a
tougher challenge with this comparison group. In practice, despite the larger raw mean diﬀerences,
the bias estimates in Table 5B are comparable to those in Table 5A.
Overall, the performance of the cross-sectional matching estimators is a bit worse than that
found in HIT and HIST .
These estimators reduce the bias substantially relative to
an unadjusted comparison of means, but the bias that remains after matching is typically somewhat
larger than the corresponding experimental impact estimate. For the DW scores applied to the
DW sample, we ﬁnd that the matching estimators perform extremely well. However, as discussed
above, the DW sample is somewhat peculiar in only including persons randomized after April
of 1976 who had zero earnings in months 13 to 24 prior to randomization. Because we ﬁnd it
diﬃcult to motivate this type of sample inclusion criteria, we do not believe that the evidence that
matching performs well on this particular sample can be generalized. Clearly, the performance of
the matching estimators is much less impressive when applied to samples other than that analyzed
in DW .
Diﬀerence-in-Diﬀerences Matching Estimates
Tables 6A and 6B present diﬀerence-in-diﬀerences matching estimates for the CPS and PSID comparison groups, respectively. These estimators have not previously been applied to the NSW data.
As described in Section 3.3, diﬀerence-in-diﬀerences matching diﬀers from cross-sectional matching
in that it removes any time-invariant diﬀerences between the D = 1 and D = 0 groups conditional
This is accomplished in our context by subtracting a cross-sectional matching estimate
of the pre-random-assignment bias from a cross-sectional matching estimate of the post-random assignment bias. In constructing the diﬀerence-in-diﬀerences matching estimates presented in Tables
6A and 6B, we use the same matching methods used in Tables 5A and 5B.
Consider Table 6A and the CPS comparison group ﬁrst.
Four major patterns emerge.
all of the diﬀerence-in-diﬀerences matching estimators perform well with the DW scores applied to
the DW sample.
This ﬁnding mirrors that for the cross-sectional matching estimators.
the bias associated with the diﬀerence-in-diﬀerences matching estimators is lower in most cases
for the DW scores and the Early RA sample and in all cases with the LaLonde scores applied to
the LaLonde sample.
As a result, the biases associated with diﬀerence-in-diﬀerences propensity
score matching are of the same order of magnitude as the impact (or smaller) for all of the samples
and scores in Table 6A.
Third, as in Table 5A for the cross-sectional matching estimators, the
particular estimator selected, the imposition of the common support condition and the choice of
bandwidth all have no consistent eﬀect on the estimated bias. Finally, and most importantly, when
either the score model or the sample is changed, the estimated bias increases substantially, though
less than in the case of the cross-sectional matching estimators considered in Tables 5A and 5B.
Once again, the bias estimates are not robust to perturbations in the sample or in the propensity
score model, mirroring the ﬁndings for the cross-sectional matching estimators.
The estimates for the PSID comparison group, presented in Table 6B, reveal even stronger
patterns. While the biases for the DW sample with the DW scores get a bit larger with diﬀerencing,
the biases for the other three combinations of scores and samples presented in the table all get
substantially smaller. Especially dramatic are the changes for the Early RA sample with the DW
scores and for the LaLonde sample with the LaLonde scores, where the biases often fall from several
thousand dollars to only a few hundred.
As was the case with the CPS comparison group, the
biases show no consistent pattern in response to the choice of matching procedure, the imposition
of the common support condition or the selection of the bandwidth.
While the cross-sectional matching estimates presented in Tables 5A and 5B reveal the extreme
sensitivity of the results in DW , the estimates in Tables 6A and 6B show fairly stable
performance for the diﬀerence-in-diﬀerences matching estimators.
These results diﬀer from the
ﬁndings in HIT and HIST in the sense that for most demographic groups in the
JTPA data, the biases associated with diﬀerence-in-diﬀerences matching are quite similar to those
associated with cross-sectional matching. The diﬀerence between the ﬁndings here and those from
the JTPA data is consistent with the view that the diﬀerencing is eliminating time-invariant bias
in the NSW data due to geographic mismatch and/or diﬀerent ways of measuring earnings in the
experimental control and non-experimental comparison groups, which were not sources of bias with
the JTPA data.
The very limited set of conditioning variables Z available in the NSW data,
compared to the rich set of conditioning variables available in the JTPA data, may also help to
explain the much larger diﬀerence between the cross-sectional and diﬀerence-in-diﬀerences matching
estimates of the bias obtained using the NSW data.
Regression-Based Estimates
We next present bias estimates obtained using a number of standard, regression-based impact
estimators for each of the three experimental samples and both comparison groups.
answers to two questions.
First, how well do these estimators perform in the diﬀerent samples?
We have argued that the DW sample may implicitly present a less diﬃcult selection problem
than the original LaLonde sample due to its inclusion of persons randomly assigned late in the
experiment only if they had zero earnings in months 13 to 24 prior to random assignment. Second,
is it the matching estimator or just selection of the right conditioning variables that accounts for
the low bias estimates when cross-sectional propensity score matching estimators are applied to
the DW sample with the DW scores?
Both matching and standard regression adjustment seek
to correct for selection on observable characteristics, Y0.
Diﬀerences between the two are that
matching, unlike regression, does not assume a linear (in the parameters) functional form and does
not require E(U|X, D) = 0.
Tables 7A and 7B give the bias estimates for the CPS and PSID comparison group samples,
respectively. In each table, each pair of rows contains the estimates of the bias and of the bias as
a percentage of the impact for one of the three experimental samples.
The ﬁrst column presents
the simple mean diﬀerence in earnings in 1978.
The next four columns present bias estimates for
cross-sectional regression speciﬁcations based on equation (4) in Section 2. The models containing
varying sets of conditioning variables, including the variables from the LaLonde propensity scores,
the DW propensity scores, the DW scores without the “Real Earnings in 1974” variable and a richer
speciﬁcation that includes additional interaction terms found to be signiﬁcant in an investigation
of alternative propensity score models. An exact variable list for each speciﬁcation appears in the
table notes. The last four columns of Tables 7A and 7B show bias estimates from the diﬀerence-indiﬀerences estimators and unrestricted diﬀerence-in-diﬀerences estimators examined in Table 5 of
LaLonde . The diﬀerence between the two pairs of estimators is that in the ﬁrst pair, based
on equation (5), the dependent variable is the diﬀerence between earnings in 1978 and earnings in
1975, while in the second pair, the dependent variable is earnings in 1978 and earnings in 1975 is
included as a right-hand-side variable.
The latter formulation relaxes the restriction implicit in
the former that the coeﬃcient on 1975 earnings equals one.44
The estimates in Tables 7A and 7B gives clear answers to both questions raised.
the bias estimates from the LaLonde and Early RA samples reveals that for the standard regression
estimators and the unrestricted diﬀerence-in-diﬀerence estimators, the bias is smallest in the DW
sample in every case but one. This strongly suggests that the sub-sampling strategy employed by
DW results in a sample with a selection problem that is less diﬃcult to solve.45
exceptions to this rule are the two standard diﬀerence-in-diﬀerences estimators.
Having selected
into the sample persons who may have transitorily, rather than permanently, low earnings, it is
perhaps not surprising that diﬀerencing does relatively poorly in the DW sample.
This pattern
is also consistent with the fact that diﬀerence-in-diﬀerences matching tends to increase the bias
(a bit for the CPS comparison group and a bit more for the PSID comparison group) relative to
cross-sectional matching for the DW sample, but not for the LaLonde and Early RA samples.46
In regard to the second question, the results diﬀer between the CPS and PSID comparison
In the CPS sample, the bias estimate from a regression of earnings in 1978 on an NSW
indicator (equal to one for the control group members and zero otherwise) and the covariates from
the DW propensity score model is -$34 (2% of the experimental impact).
Thus, for the CPS
comparison group, the key to the low bias estimates found in DW is picking the right
44We also estimated the bias for the before-after estimator, described in Section 3.2, associated with each experimental sample.
In each case, the bias was on the order of several thousand dollars.
We do not present estimates
from the Heckman two-step estimator of the bivariate normal selection model examined by LaLonde 
as this estimator is not robust to choice-based sampling.
45This ﬁnding is implicit in Table 2 of DW . Compare the estimated coeﬃcients (not biases) for LaLonde’s
sample to those for their sample both with and without including the “Real Earnings in 1974” variable among the
covariates for the CPS-1 and PSID-1 comparison groups.
46It is also of interest to note that the estimated biases for the regression-adjustment and unrestricted diﬀerencein-diﬀerences models are almost always lower with the CPS comparison group than with the PSID comparison
This indicates the value of the additional sample restrictions imposed on the CPS comparison group when
the estimator employed is simple regression adjustment.
subsample and the right covariates, not matching.
In contrast, in the PSID, matching makes a
big diﬀerence. The bias estimate from nearest neighbor matching with ten nearest neighbors (and
imposing the common support condition) is -$85, compared to a bias estimate from a regression
using the same variables of $1285. For the PSID, the linearity restriction implicit in the regression
has some bite.
Speciﬁcation Tests
As discussed in Section 2, Heckman and Hotz found that when they applied two types of
speciﬁcation tests to the NSW data that they were able to rule out those estimators that implied a
diﬀerent qualitative conclusion than the experimental impact estimates. In this section, we apply
one of the speciﬁcation tests that they use to the cross-sectional matching estimators presented
in Tables 5A and 5B.
The test we apply is the pre-program alignment test, in which each candidate estimator is applied to outcome data from a period prior to the program (i.e., to random
assignment).
Note that this test actually tests the joint null that the outcome and participation
processes are the same in the pre-program and post-program periods and that the estimator being
tested successfully corrects for selection bias.47
We implement the test by applying the matching estimators to earnings in 1975, keeping the
same propensity scores. If the estimated bias is statistically diﬀerent from zero in the pre-program
period, then we reject the corresponding estimator.
Because we lack reliable earnings data for
two pre-program periods, we are unable to apply the test to the diﬀerence-in-diﬀerences matching
estimators in Tables 6A and 6B. 48
47See Heckman and Hotz for a more detailed discussion of the test and Heckman, LaLonde and Smith 
for a discussion of caveats regarding its use. Ham, Li, and Reagan apply pre-program speciﬁcation tests in
the context of controlling for selectivity in estimating the returns to migration and ﬁnd them very useful.
48Recall that we are not using the grouped data on SSA earnings that Heckman and Hotz use in their paper,
and which allow them to apply the pre-program test to longitudinal estimators where it requires multiple periods of
Tables 8A and 8B present the pre-program estimates for the CPS and PSID comparison groups,
respectively. Consider ﬁrst Table 8A. The pre-program test rejects every estimator for the Early
RA sample with the DW scores, which is good, as the biases are all quite high for this sample in
Table 5A. It also rejects all but one of the estimators for the LaLonde sample with the LaLonde
scores (though two are rejected only at the 10 percent level), which is of course desirable given the
large bias values. The test does not reject any of the very low bias estimators for the DW sample
with the DW scores. In the case of the LaLonde sample with the DW scores, where the biases are
of moderate size, the ﬁrst two of the eight estimators in Table 5A are rejected.
Overall, the preprogram test applied to the CPS comparison group does a good job of eliminating the estimators
with the highest estimated biases in the post-program period and not rejecting the estimators with
low or moderate estimated biases.
Similar patterns are observed in Table 8B for the PSID comparison group.
The pre-program
test solidly rejects all of the matching estimators as applied to the Early RA sample with the DW
scores and to the LaLonde sample with the LaLonde scores.
All of these estimators have very
large estimated biases in the post-program period. The test does not reject any of the matching
estimators for the DW scores applied to the DW sample, which have low estimated biases in the
post-program period.
Finally, the test results for the DW scores applied to the LaLonde sample
are again a mixed bag, though in this case the four estimators eliminated by the pre-program
test are the four with the highest estimated biases in the post-program period.
Overall, for both
comparison group samples, our results conﬁrm the eﬀectiveness of the pre-program test at calling
attention to estimators likely to lead to highly biased estimates. Thus, we reach for cross-sectional
matching estimators a similar conclusion to that reached by Heckman and Hotz in regard
pre-program data.
to the standard regression-based estimators they examined.
Summary and Conclusions
Our analysis of the data from the National Supported Work Demonstration yields three main conclusions. First, our evidence leads us to question recent claims in the literature by DW 
and others regarding the general eﬀectiveness of matching estimators relative to more traditional
econometric methods. While we are able to replicate the low bias estimates reported in the DW
 studies, we conclude that their evidence is not generalizable. When we apply the same
methods to other reasonable samples from the NSW data, the low bias results disappear. When we
construct estimates using a modestly diﬀerent propensity score speciﬁcation and the full LaLonde
sample, we obtain much larger biases than with the DW propensity score speciﬁcation. The sample
inclusion rules employed by DW in creating their sample simplify the selection problem
by diﬀerentially including individuals with zero earnings in the pre-program period.
Indeed, in
some cases even very simple regression-adjustment estimators have low bias values when applied
to the DW sample. Thus, their evidence clearly cannot be construed as showing the superiority of
matching over more traditional econometric estimators. More generally, we argue that their study,
like much of the earlier literature in this area, implicitly poses the wrong question.
The question
is not which estimator is the best estimator always and everywhere.
Estimators diﬀer in their
identifying assumptions, and the assumptions underlying a given estimator will sometimes hold in
the data and sometimes fail to hold.
Instead of engaging in a hopeless search for a magic bullet
estimator, the goal of theoretical and empirical investigation should be to develop a mapping from
the characteristics of the data and institutions available in particular evaluation contexts to the
optimal nonexperimental estimators for those contexts. In some contexts, particularly those with
high quality data rich in variables related to participation and outcomes, matching may be the best
choice. In other cases, such as the NSW data, our results show that matching makes a poor choice.
Second, we ﬁnd that the diﬀerence-in-diﬀerences matching estimators introduced in HIT 
and HIST perform substantially better than the corresponding cross-sectional matching
estimators.
This ﬁnding is consistent with the elimination of time-invariant biases between the
NSW sample and the comparison group sample due to geographic mismatch and diﬀerences in the
measurement of the dependent variable.
Matching methods do not perform well in eliminating
these sources of bias, a task for which they were not designed. The positive ﬁndings regarding
diﬀerence-in-diﬀerences matching again highlight the importance of choosing a nonexperimental
method consistent with the features of the data and institutions present in a given context. In the
NSW context, the data are weak in covariates, fail to place comparison group members in the same
local labor markets as participants and rely on diﬀerent measures of earnings for participants and
non-participants.
These features strongly suggest that matching should work poorly in the NSW
data and that diﬀerences-in-diﬀerences matching should work better, which is precisely what we
ﬁnd. As this example shows, knowledge regarding which estimators work given the characteristics
of the available data and the institutional context has begun to accumulate in the literature and
should be used in designing and evaluating evaluation research.
Third, we ﬁnd that while the choice between cross-sectional matching and diﬀerence-in-diﬀerences
matching makes a big diﬀerence to the estimated biases, the details of the matching procedure in
general do not. Thus, the choice between nearest neighbor and local linear matching, or the choice
of bandwidth for local-linear matching (within reasonable limits), do not have strong or consistent
eﬀects on the estimated biases.
This ﬁnding comports with the ﬁndings in a number of other
empirical studies. The imposition of the common support condition represents the one (partial)
exception in our context, as it aﬀects the estimated biases in some cases but not others.