Medi-Care AI: Predicting Medications From Billing
Codes via Robust Recurrent Neural Networks
Deyin Liua, Lin Wub,c,∗, Xue Lid
aZhengzhou University, China
bKey Laboratory of Knowledge Engineering with Big Data (Hefei University of Technology),
Ministry of Education
cSchool of Computer Science and Information Engineering, Hefei University of Technology,
Hefei 230000, China
dDalian Neusoft University of Information, China
In this paper, we present an eﬀective deep prediction framework based on robust recurrent neural networks (RNNs) to predict the likely therapeutic classes
of medications a patient is taking, given a sequence of diagnostic billing codes
in their record. Accurately capturing the list of medications currently taken by
a given patient is extremely challenging due to undeﬁned errors and omissions.
We present a general robust framework that explicitly models the possible contamination through overtime decay mechanism on the input billing codes and
noise injection into the recurrent hidden states, respectively.
By doing this,
billing codes are reformulated into its temporal patterns with decay rates on
each medical variable, and the hidden states of RNNs are regularised by random noises which serve as dropout to improved RNNs robustness towards data
variability in terms of missing values and multiple errors. The proposed method
is extensively evaluated on real health care data to demonstrate its eﬀectiveness
in suggesting medication orders from contaminated values.
Billing codes, Robust recurrent neural networks, Health care data,
Medication prediction
∗Corresponding author
Email addresses: (Deyin Liu), (Lin
Wu ), (Xue Li)
 
January 29, 2020
 
1. Introduction
There has been growing interest in exploiting the large amounts of data
existed in electronic medical records for both clinical events and secondary research. While leveraging large historical data in electronic health records (EHR)
holds great promise, its potential is weakened by multiple errors and omissions
in those records. Some studies show that over 50% of electronic medication
lists contain omissions , and even 25% of all medications taken by patents are
not recorded. To ensure the corrections of medication lists, great eﬀorts have
been dedicated to improve the communications between patients and providers
 , however, manually maintaining these lists would be extremely human-labor
intensive. Thus, it demands a generic yet robust predictive model that is able
to suggest medication consultation to the patients next visit in the context of
medication documentation contaminations.
Recently, Recurrent Neural Networks (RNNs), such as Long Short-Term
Memory (LSTM) , and Gated Recurrent Unit (GRU) have been explored
for modeling diseases and patient diagnosis in health care modality .
For instance, a temporal model based on RNNs, namely Doctor AI, is developed
to predict future physician diagnosis and medication orders. This intelligent
system demonstrates that historical EHR data can be leveraged to forecast the
patient status at the next visit and present medication to a physician would
like to refer at the moment. However, little eﬀorts are put into systematically
modelling the EHR with missing values since it is diﬃcult to capture the
missing patterns in medical billing codes. Simple solutions such as omitting
the missing data and to perform analysis only on the observed data, or ﬁlling
in the missing values through smoothing/interpolation , spectral analysis
 , and multiple imputations oﬀer plausible ways to the
missing values in data series. However, these solutions often result in suboptimal
analysis and poor predictions because the imputations are disparate from the
prediction models and missing patterns are not properly described .
A recent ﬁnding demonstrates that missing values in time series data are
usually informative missing, that is, the missing values and patterns are related
to the target labels in supervised learning tasks. For example, Che et al. show
that the missing rates in time series health care data are usually highly correlated with the labels of interests such as mortality and ICD-9 diagnoses. Hence,
it demands an appropriate strategy to describe the decaying on diagnostic measurements over time. Moreover, the diagnostic billing codes are characterized
of more than missing values in patient records, whereas in most cases they are
combined with multiple errors and omissions. Thus, we use the terminology
noise to generally refer to all potential incorrectness of medication lists.
1.1. Our Approach
Inspired by the noise-based regularizer of RNNs, a.k.a dropout , we
impose a multiplicative noise into the hidden states to ensure the robustness of
recurrence and also preserve the underlying RNN in the context of noise injection. Hence, in this paper we develop a robust RNN model, an eﬀective new way
to deal with incomplete billing codes in medical domain whilst being capable
of predicting the future medication orders given the missing codes in sequence.
The key idea is to not only model the input codes by explicitly encoding the
missing patterns over time, but also inject random noise into the transition
function of recurrence. Intuitively, the explicit noise injection into the hidden
states of RNNs can serve as regulariser to drop the observation diﬀerence that
will be potentially added into the hidden states. Thus, the RNNs are trained to
ﬁt its parameters to maximize the corresponding marginal likelihood of observations in the context of high variability. The proposed model is experimentally
evaluated on real EHR datasets to demonstrate its eﬀectiveness in identifying
missing actual information in relation to therapeutic classes.
1.2. Contributions
The contributions of this paper can be summarized as follows.
• We present a robust RNN based medication prediction framework to eﬀectively cope with sequential billing codes that are contaminated by missing
values and multiple errors.
• The proposed approach is designed to predict the complete set of medications a patient is actively taking at a given moment from a sequence of
diagnostic billing coeds in the context of non-trivial billing record noise.
This is, to our best knowledge, the ﬁrst eﬀort to explicitly model both the
medication care data and delving the RNNs into the medical domain.
• Insightful analysis to our approach are provided in this paper. Extensive
experiments on health care datasets are conducted to demonstrate the superiority of our method over state-of-the-art by achieving the performance
gain on AUC by 13% and 7% on the Physio-net challenge dataset and
MIMIC-III , respectively.
The rest of this paper is organized as follows. Section 2 reviews some related works. We detail the proposed predictive model in Section 4 with some
background described in Section 3 in advance. Section 5 reports extensive experiments over the real-valued medical datasets, and the paper is concluded in
Section 6.
2. Related Work
2.1. Modeling medical event sequences
Common approaches to modeling medical event sequences include continuoustime Markov chain based models and their extension using Baysian networks
 as well as intensity function methodologies such as Hawkes processes . It
is known that continuous-time Markov chain methods are computationally expensive because modeling multi-labelled point processes would expand rapidly
their state-space. On the other hand, Hawkes processes with intensity functions
depend linearly with respect to the past observations, while they are limited in
capturing temporal dynamics. Moreover, there is no study on these models to
deal with missing values or incorrect data. In this paper, we address these challenges by designing a robust recurrent neural network which has shown to be
eﬀective in learning complex yet potentially missing data in sequential patterns
regarding health-care systems.
2.2. Deep learning models for EHR
It has witnessed some attempts to apply neural network models a.k.a deep
learning methods to study EHR since deep learning models are capable of learning complex data patterns. The earlier work is the use of an LSTM model that
produced reasonable accuracy (micro-AUC 0.86) in a 128-dim multi-label prediction of diagnoses from regularly sampled, continuously real-valued physiologic
variables in an Intensive Care Unit (ICU) setting . One successful framework
is Doctor AI which is a predictive temporal model using RNNs to predict the
diagnosis and medication codes for a subsequent visit of patients. They used a
GRU model in a multi-label context to predict the medications, billing codes,
and time of the next patient visit from a sequence of that same information for
previous visits. It can achieve an improvement over a single-hidden-layer MLP
(reach a recall@30 of 70.5 by a 20 margin). This is a successful showcase of
using the strength of recurrence,i.e., to predict the next element in a sequence.
However, aforementioned deep learning paradigms are not able to eﬀectively
cope with EHR with errors and omissions.
Prior eﬀorts have been dedicated into modeling missing data in sequences
with RNNs in clinical time series . A very recent work yet contemporary with our work, namely GRU-Decay , used a GRU model with imputation
on missing data by a decay term to predict the mortality/ICD-9 diagnosis categories from medication orders and billing codes. Our method contrasts with
GRU-Decay in the way of managing the RNN to tackle the missing values.
Instead of using the same decay mechanism on both input sequence and the
hidden state as the GRU-Decay performed, we propose to dealing with the raw
inputs and hidden states in diﬀerent strategies wherein the input billing codes
are multiplied by decay rate on each variable (the same as GRU-Decay ), and
the hidden states are injected into noises in the multiplicative form.
3. Background
3.1. Medical billing codes
In our experiments, codes are from the International Classiﬁcation of Disease, Ninth Revision (ICD-9).
The ICD-9 hierarchy consists of 21 chapters
roughly corresponding to a single organ system or pathologic class. Leaf-level
codes in the tree represent single diseases or disease subtypes. For each time
a patient has billable contact with the health-care system through which the
time stamped billing codes are associated with the patient record, indicating
the medical conditions that are related to the reasoning for the visit. However,
these billing codes are more often unreliable or incomplete, and thus making
the electronic medical records unable to track the set of medications that the
patient is actively taking. The code range and descriptions are shown in Table
3.2. Recurrent neural networks
An recurrent neural network (RNN) considers a sequence of observations,
X1:T = (x1, . . . , xT ), and to handle the sequential time-series the RNN introduces the hidden state ht at time step t, as a parametric function fW (ht−1, xt−1)
of the previous state ht−1 and the previous observation xt−1. The parameter
W is shared across all steps which would greatly reduce the total number of
parameters we need to learn. The function fW is the transition function of the
RNN, which deﬁnes a recurrence relation for the hidden states and renders ht
a function of all the past observations x1:t−1.
The particular form of fW determines the variants of RNN including Long-
Short Term Memory (LSTM) and Gated Recurrent Units (GRU) . In this
paper, we will study GRU which has shown very similar performance to LSTM
but employs a simper architecture. First, we would reiterate the mathematical
Code range
Description
Infectious and parasitic diseases
Endocrine, nutritional and metabolic diseases, immunity disorders
Blood diseases and blood-forming organs
Mental disorders
Nervous system diseases
Sense system diseases
Circulatory system diseases
Respiratory system diseases
Digestive system diseases
Genitourinary system diseases
Complications of pregnancy, childbirth, and the puerperium
Skin and subcutaneous tissue
Musculoskeletal system and connective tissue
Congenital anomalies
Conditions originating in the perinatal period
Symptoms, signs and ill-deﬁned conditions
Injury and poisoning
Table 1: The top level classes for ICD-9 chapters.
formulation of GRU as follows
zt = σ(Wzxt + Uzht−1), rt = σ(Wrxt + Urht−1),
ˆht = tanh(Whxt + Uh(rt ⊙ht−1)),
ht = (1 −zt) ⊙ht−1 + zt ⊙ˆht,
where ⊙is an element-wise multiplication. zt is an update gate that determines
the degree to which the unit updates its activation. rt is a reset gate, and σ
is the sigmoid function. The candidate activation ˆht is computed similarly to
that of traditional recurrent unit. When rt is close to zero, the reset gate make
the unit act as reading the ﬁrst symbol of an input sequence and forgets the
previously computed state.
4. Robust Recurrent Neural Networks for Medication Predictions
In this section, we develop a new framework for clinical medication predictions in the context of missing information and multiple errors.
formulate the prediction problem setting, and then detail the architecture with
explicit noise injection into the recurrent hidden states. Finally, we present the
training procedure on the proposed model.
4.1. Problem setting
For each patient, the temporal observations are represented by multivariate
time series with D variables of length T as X1:T ∈RT ×D, where xt ∈RD denotes
the t-th observations, namely measurements of all variables and xd
the d-th variable of xt.
In the medication records, the variables correspond
to multiple medication codes, such as the codes 493 (asthma) and 428 (heat
failure) from ICD-9.
For each time stamp, we may extract high-level codes
for prediction purpose and denote it by yt. Generic Product Identiﬁer (GPI)
medication codes are extracted from the medication orders. This is because
the input ICD-9 codes are represented sequentially while the medications are
represented as a list that changes over time. Also, many of the GPI medication
codes are very granular, for example, the pulmonary tuberculosis (ICD-9 code
011) can be divided into 70 subcategories (011.01, 011.01,...,011.95, 011.96).
In this paper, we are interested in learning an eﬀective vector representation
for each patient from his billing codes over time with multiple missing values
at each time stamp t = 1, . . . , T, and predicting diagnosis and medication categories in the next visit yT +1. We investigate the use of RNN to learn such
billing code representations, treating the hidden layers as the representation for
the patient status and use them for the prediction tasks. To account for the
situation of missing/incorrect values in EHR, we propose robust RNN architecture, which eﬀectively models the missing patterns from time series onwards
through the temporal decay mechanism and injects noises into the
hidden states of RNN at each time step.
4.2. Robust RNNs with noise injection
To eﬀectively learn representations from missing or incorrect values in billing
codes, we propose to incorporate diﬀerent strategies in regards to the input
billing codes and the hidden states, respectively.
For the missing values in
billing codes of EHR, we employ the decay mechanism which has been designed
for modeling the inﬂuence of missing values in health care domain . This
is based on the property that the values of missing variables tend to be close
to some default value if its last measurement is observed a long time ago. This
property should be considered as critical for disease diagnosis and treatment.
Also, the inﬂuence of the input dimensions will fade away over time if some
dimension is found missing for a while. On the other hand, the hidden states
of RNNs should be injected with random noises which is more advantageous by
preventing the dimensions of hidden states from co-adapting and it can force
the individual units to capture useful features .
Speciﬁcally, we inject a decay rate into each variable of the billing code series.
In this way, the decay rate diﬀers from variable to variable and indicative to
unknown possible missing patterns. To this end, the vector of a decay rate is
formulated as
γt = exp{−max(0, Wγδt + bγ)},
where Wγ and bγ are trainable parameters jointly with the LSTM. exp{·} is the
exponential negative rectiﬁer to keep each decay rate monotonically decreasing
ranged between 0 and 1. δd
t is the time interval for each variable d since its last
observation, which can be deﬁned as
st −st−1 + δd
t−1, t > 1
In Eq.(3), st denotes the time stamp when the t-th observation is obtained and
we assume that the ﬁrst observation is made at time t = 0 (s1 = 0). Hence,
for a missing variable code, we adopt the decay vector γt to decay it overtime
but towards an empirical mean instead of using its last observation. And the
decaying measurement billing code vector can be formulated by applying the
decay scheme into:
t′ + (1 −γd
t′ is the last observation of the d-th variable (t′ < t) and ˆxd is the
empirical mean of the d-th variable. We remark that when the input billing
code is decaying, the parameter Wγx should be constrained to be diagonal so
as to ensure the decay rates of variables are not aﬀecting each other.
To augment the RNN’s capability of coping with multiple errors in sequential
EHR billing codes, we explicitly redeﬁne the hidden states by injecting noises.
This strategy is able to eﬀectively ﬁt the parameters of RNN by maximizing the
likelihood of data observations because the next predicted output from RNN is
determined as p(xt|x1:t−1) = p(xt|ht) 1. Thus, we deﬁne the GRU with noise
as follows
ϵ1:T ∼{0, (1 −δ)}d;
ht = fW (xt−1, ht−1, ϵt) = (1 −zt) ⊙ht−1 ⊙ϵt + zt ⊙ˆht ⊙ϵt.
1The likelihood p(xt|ht) can be in the form of the exponential family.
In Eq.(5), the noise component ϵ1:T is an independent drawn from a scaled
Bernoulli (1-δ) random variable. In this paper, it is used to create the dropout
noise via the element-wise product (·) of each time hidden state ht1. In other
words, dropout noise corresponds to setting h to 0 with probability δ, and
to h/(1 −δ) else.
Intuitively, this multiplicative form of noise injection can
induce the robustness of RNN to how future data may be diﬀerent from the
observations. Also, this can be regarded as a regularization on RNN to normalize
the hidden states, which is similar to noise-based regularizer for neural networks,
namely dropout . This explicit regularization is equivalent to ﬁtting the
RNN loss to maximize the likelihood of the data observations, while being with
a penalty function of its parameters. This type of regularization that involves
noise variables can help the RNNs learn long-term dependencies in sequential
data even in the context of high variability because dropout-based regularization
can only drop diﬀerences that are added to network’s hidden state at each timestep. And thus this dropout scheme allows up to use per-step sampling while
still being able to capture the long-term dependencies .
4.3. The architecture of the prediction model
As shown in Fig.1, the proposed robust neural network architecture receives
input at each time stamp t corresponding to patient visits in sequences. The
billing codes xt are in the form of multi-label categories. The input sequential
billing codes are modelled with the decay of missing values, and then fed into
the stacked multiple layers of GRU to project the inputs into lower dimensional
space, and also learn the status of the patients at each time stamp as real-valued
vectors. For predicting the diagnosis codes and the medication codes at each
time stamp t, a softmax layer is stacked on top of the GRU, using the hidden
state ht as the input, that is, yt+1 = softmax(WT
codeht +bcode). Thus, the objective of our model is to learn the weights W[z,r,h,code,γ], U[z,r,h], b[z,r,h,code,γ].
In particular, the values of all W and U are initialized to orthogonal matrices using singular value decomposition of matrices from the normal distribution
 . All values of b are initialized to be zeros. Therefore, for each patient we
Figure 1: The overview of our framework with robust RNN to solve the problem of
forecasting the medication codes assigned to a patient for his next visit. (a) A conventional RNN model. (b) The proposed model. The input sequential data in regards to
a patient (X = {x1, . . . , xt}) are embedded with decay mechanism (γx) to model the
potential missing pattern, and the stacked recurrent layers with multiplicative noise
regularization (ϵt) learn the status of the patient at each time stamp. Given the learnt
status (ht), the framework is to generate the codes observed in the next time stamp.
employ the cross entropy as the loss function for the code prediction, which is
L(W, U, b) =
(¨yt+1 log(yt+1) + (1 −¨yt+1) log(1 −yt+1)) ,
where ¨y is the ground truth medication category.
5. Experiments
In this section, we demonstrate the performance of our model on two realworld health-care datasets, and compare it to several strong machine learning
and deep learning competitors in the classiﬁcation tasks.
5.1. Data preparation and experimental setting
We conduct experiments on two health-care datasets: Physio-net challenge
dataset and MIMIC-III .
Data: Input billing codes in sequence x1:T , initial hidden state h0, noise
distribution ϕ(· : 1, σ).
Result: Set of learned parameters of GRU:
W[z,r,h,code,γ], U[z,r,h], b[z,r,h,code,γ].
Initialize the set of parameters ;
while stopping criterion not met do
for t from 1 to T do
Sample noise from ϵt ∼ϕ(ϵt : 1, σ) ;
Compute the decayed inputs xd
t′ + (1 −γd
Compute state ht = (1 −zt) ⊙ht−1 ⊙ϵt + zt ⊙ˆht ⊙ϵt ;
Compute loss as in Eq. (6) ;
Update the network parameters ;
Algorithm 1: The proposed robust RNN framework for medication prediction from billing codes.
• Physio-net challenge 2012 dataset (Physio-Net): This PhysioNet Challenge dataset is a publicly available collection of multivariate clinical
time series from 8,000 intensive care unit (ICU) records. Each record is
a multivariate time series of roughly 48 hours and contains 33 variables
such as albumin, heart-rate, glucose etc. We use the training subset A in
our experiments since ground truth outcomes are only publicly available
on this subset. We conduct the prediction of 4 tasks on this dataset: inhospital mortality, length-of-stay less than 3 days, had a cardia condition
or not, and whether the patient was recovering from surgery. This can be
treated as a multi-task classiﬁcation problem.
• MIMIC-III: This is a publicly available dataset collect at Beth Israel Deaconess Medical Center from 2001 to 2012 . It contains over 58,000
hospital admission records, and we extract 99 time series features from
19,714 admission records for 4 modalities which are very useful for mon-
itoring ICU patients . These modalities include input-events (ﬂuids to
patients, e.g., insulin), output-events (ﬂuids out of the patient, e.g., urine),
lab-events (lab test results, e.g., pH values), and prescription-events (active drugs prescribed by doctors, e.g., aspirin). We use the ﬁst 48 hours
data after admission from each time series, and conduct the predictive
ICD-9 code task: predict ICD-9 diagnostic categories (e.g., respiratory
system diagnosis) for each admission, which can be treated as a multilabel problem.
For the training on all the models, we use 85% of the patients as the training set, and 15% as the testing set. All the RNN models are trained with 50
epoches i.e., 50 iterations over the entire training data, and then evaluate the
performance against the testing set. To avoid over-ﬁtting, we apply the dropout
between the GRU layer and the ﬁnal prediction layer, and also between the multiple stacked GRU layers. The dropout rate is 0.3 and the norm-2 regularization
is applied into the weight matrix of Wcode. The dimensionality of the hidden
states h of the GRU is set to be 2048 to ensure the expressive power.
train the models using truncated back-propagation through time with average
stochastic gradient descent . To avoid the problem of exploding gradients,
we clip the gradients to a maximum norm of 0.25.
5.2. Evaluation metrics
For the evaluation on the task in a multi-label context, the performance of
all methods is evaluated against two metrics: the micro-averaged area under
the ROC curve (AUC) and the top-k recall. The measure of AUC treats each
instance with equal weight, regardless of the nature of the positive labels for
that instance , which would not give a score advantage to instances with very
prevalent or very rare labels. The micro-averaged AUC considers each of the
multiple label predictions as either true or false, and then computes the binary
AUC if they all belong to the same 2-class problem, Thus, the micro-average
AUC Aµ can be deﬁned as
Aµ = |(x, x′, l, l′) : f(x, l) ⩾f(x′, l′), (x, l) ∈S, (x′, l′) ∈¯S|
where S = {x, l} : l ∈Y is the set of [instance, label] pairs with a positive label,
and Y = {yd : yd = 1, . . . , D} is the set of positive labels for the input x.
The top-k recall mimics the behavior of doctors examining diﬀerential diagnosis which suggest the doctor is listing most probable diagnoses and treat the
patients accordingly to identity the patients status. The top-k recall is deﬁned
top −k recall = #TP in the top k predictions
where #TP denotes the number of true positives. Thus, a machine with high
top-k recall translates to a doctor with eﬀective diagnostic skills. In this end,
it turns out to make top-k recall a suitable measure for the performance of
prediction models on medications.
5.3. Baselines
We consider baselines in two categories: (1) RNN based methods: Doctor-
AI , GRU-Decay , LSTM-ICU ; MiME ; SRL-RNN ; (2) Non-RNN
based methods: Logistic Regression (LR), Support Vector Machine (SVM), and
Random Forest (RF).
• Doctor-AI : Doctor AI is a temporal model using RNN to assess the
history of patients to make multi-label predictions on physician diagnosis
and the next medication order list.
• GRU-Decay : To tackle the missing values in EHR data, GRU-Decay
is based on Gated Recurrent Units and exploits the missing patterns for
eﬀective imputation and improves the prediction performance.
• LSTM-ICU : It is a study to empirically evaluate the ability of LSTMs
to recognize patterns in multivariate time series of clinical measurements.
They consider multi-label classiﬁcation of diagnoses by training a model
to classify 128 diagnoses given frequently but irregularly sampled clinical
measurements.
• MiME A Multilevel Medical Embedding (MiME) approach to learn the
multilevel embedding of EHR data that only relys on this inherent EHR
structure without the need for external labels.
• SRL-RNN A Supervised Reinforcement Learning with Recurrent Neural Network (SRL-RNN), which fuses them into a synergistic learning
framework.
• Logistic Regression (LR): Logistic regression is a common method to
predict the codes in the next visit xt using the past xt−1.
 , we use the data from L time lags before and aggregate the data
xt−1 +xt−2+, +xt−L for some duration L to create the feature for prediction on xt.
• Support Vector Machine (SVM): A multi-label SVM is trained to obtain
multiple classiﬁers to each diagnostic code and each medication category.
• Random Forest (RF): The random forest is not easily constructed to work
on sequences, and we represented the input data as bag-of-code vector
b ∈RD. As RF cannot be operated on large-size dataset, we break down
it into an ensemble of ten independent forests while each one trained on
one tenth of the training data, and their averaged score is used for test
prediction.
5.4. Results and discussions
Prediction performance. In the ﬁrst experiment, we evaluate all methods on
Physio-Net and MIMIC-III datasets. Table 2 shows the prediction performance
of al the models on the multi-task predictions on real datasets: all 4 tasks on
Physio-Net and 20 ICD-9 code tasks on the MIMIC-III. The proposed method
achieves the best AUC score across all tasks on both the datasets. We notice
that all RNN models perform better than non-RNN methods because the deep
Physio-Net
Doctor-AI 
GRU-Decay 
LSTM-ICU 
SRL-RNN 
Logistic Regression
Random Forest
Logistic Regression-mean
Random Forest-mean
Table 2: Comparison results of AUC on the real-valued datasets for multi-task predictions.
recurrent layers help these models capture the temporal relationship that is
useful in solving prediction tasks. Moreover, explicitly modelling the missing
values in both the input signals and the hidden states, such as GRU-Decay and
our method, can further improve the prediction results due to the capability of
ﬁtting the parameters robust to noisy time-series data.
Table 3 compares the results of the proposed method with diﬀerent algorithms in three settings: predicting only the diagnosis codes (Dx), predicting
only the medication codes (Rx), and jointly predicting both Dx and Rx codes.
The experimental results show that the proposed method is able to outperform
the baseline algorithms by a noticeable margin. The results also conﬁrm that
RNN based approaches achieve superior performance to non-RNN methods.
This is mainly because RNNs are able to learn succinct feature representations
of patients by accumulating the relevant information from their history visits
and the current set of codes, which outperform the hand-crafted features of
Non-RNN baselines. Moreover, in the case of missing values and incorrectness
in billing codes, our method achieves the best results on all measures in the
merit of explicit modelling on billing code variables and robust improvement on
recurrence.
To further examine the capability of our method in a real-world medical care
Dx Recall @k
Rx Recall @k
[Dx, Rx] Recall @k
Doctor-AI 
GRU-Decay 
LSTM-ICU 
SRL-RNN 
Logistic Regression
Random Forest
Logistic Regression-mean
Random Forest-mean
Table 3: Comparison results of accuracy in forecasting future medical activities on the
MIMIC-III dataset.
setting where patients may have varying lengths of their medical records, we conduct an experiment to study the aﬀect of billing code history duration on the
prediction performance. To this end, we select 5,800 patients from MIMIC-III
who had more than 100 visits. We consider the RNN based deep models to predict the diagnosis codes at visit at diﬀerent times and calculate the mean values
of recall@10 across the selected patient samples. Fig.2 shows the experimental
results of diﬀerent RNN based models. It can be observed that all methods
are increasing their performance on prediction as they see longer patient visit
records, and certainly our approach achieved the best prediction performance
amongst all RNN-based models. This is mainly because the recurrence is wellsuited to time-series and the prediction is more faithful given longer sequence
inputs. Also, it is inferred that those patients with high visit count are more
likely caught in severely ill, and therefore their future is easier to predict.
Top-10 Recall (%)
Number of visits
Prediction performance
Doctoer-AI
Figure 2: The prediction performance with respect to the duration of patient medical
More discussions. As the spread σ controls the noise level and determines the
amount of regularisation into RNN, we discuss on the property of diﬀerent noise
distributions, i.e., Gaussian and Bernoulli, and the impact on the training of
RNN. The experimental results are reported in Table 4. It can be found that
what really matters with diﬀerent distributions is the variance σ which determines the degree of regularisation into the RNN. And the RNN regularisation
is not very sensitive to diﬀerent types of distribution, for example, on both
the health-care datasets the AUC values with Gaussian distribution are very
similar to Bernoulli while for each speciﬁc distribution the spread σ aﬀects the
performance.
To further examine the capability of our model in predicting medications
in missing billings, we study a case on a patient with Parkinson’s disease in
which his/her record has at least ﬁve years of data consisting of only codes for
Parkinson’s disease whereas the data contains medications for high cholesterol,
hypertension without explicit labels referring to Parkinson’s disease. In fact, the
medication entities listed as true labels are not suggested for paralysis agitans
(Parkinson’s disease), while the patient was surely taking them even though
not documented into the ICD-9 sequence. As shown in Fig.3, in the case of
missing medication items, the model is still able to predict reasonable medica-
Table 4: The study on diﬀerent noise distributions. The micro-averaged AUC values
are reported on two datasets.
Distribution
Physio-Net
Figure 3: The medication predictions for a patient with only one ICD-9 code. Each
vertical bar represents the prediction for a single medication class and the height
indicates the conﬁdence of the prediction. See the texts for details.
tions for a patient with Parkinson’s disease, such as Dopaminergic agents and
Antiepileptics, which are primary treatment for the disease. The top prediction
probabilities and missing true labels on each treatment regarding a patient are
reported in Table 5. Thus, our model is useful to identifying missing medications in the clinical scenario, such as reconciling information in a large scale
from a range of electronic and human sources to establish the ground truth of
medications that are taken on a particular day.
Top predictions
Dopaminergic agents
Antiepileptics
Other analgesics and antipyretics
Antidepressants
True labels
Lipid modifying agents, plain
Ace inhibitors, plain
Other cardiac preparations
Antiadrenergic agents, peripherally acting
Anabolic steroids
Table 5: A case study: Top prediction and true labels for a patient with Parkinson’s
6. Conclusions and Future Work
In this paper, we present an eﬀective approach to medicare system, which is
a RNN-based deep learning model that can learn robust patient representation
from a large amount of longitudinal patient billing code records and predict
future medication lists. We demonstrate the eﬀectiveness of our method which
achieved improved recall accuracy values in the real medical practice with observed missing values or incorrect records. In the future work, we would strive
to improve the performance the recurrent networks by including additional input data, such as laboratory test results, demographics, and perhaps vital signs
related to rare diseases. One interesting direction is to ﬁgure out a pathway
to convert the medication data into reliably-ordered sequences, so as to fully
exploit the strength of recurrent networks for medication prediction.