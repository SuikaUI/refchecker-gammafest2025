TECHNICAL WORKING PAPER SERIES
TWO-SAMPLE INSTRUMENTAL VARIABLES ESTIMATORS
Atsushi Inoue
Gary Solon
Technical Working Paper 311
 
NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
The views expressed herein are those of the author(s) and do not necessarily reflect the views of the National
Bureau of Economic Research.
©2005 by Atsushi Inoue and Gary Solon. All rights reserved. Short sections of text, not to exceed two
paragraphs, may be quoted without explicit permission provided that full credit, including © notice, is given
to the source.
Two-Sample Instrumental Variables Estimators
Atsushi Inoue and Gary Solon
NBER Technical Working Paper No. 311
JEL No. C3
Following an influential article by Angrist and Krueger on two-sample instrumental variables
(TSIV) estimation, numerous empirical researchers have applied a computationally convenient twosample two-stage least squares (TS2SLS) variant of Angrist and Krueger's estimator. In the twosample context, unlike the single-sample situation, the IV and 2SLS estimators are numerically
distinct. Our comparison of the properties of the two estimators demonstrates that the commonly
used TS2SLS estimator is more asymptotically efficient than the TSIV estimator and also is more
robust to a practically relevant type of sample stratification.
Atsushi Inoue
Department of Agricultural and Resource Economics
North Carolina State University
Raleigh, NC 27607-8109
 
Gary Solon
Department of Economics
University of Michigan
Ann Arbor, MI 48109-1220
 
Two-Sample Instrumental Variables Estimators
I. Introduction
A familiar problem in econometric research is consistent estimation of the coeﬃcient vector in the
linear regression model
y = Wθ + ε
where y and ε are n×1 vectors and W is an n×k matrix of regressors, some of which are endogenous,
i.e., contemporaneously correlated with the error term ε. As is well known, the ordinary least squares
estimator of θ is inconsistent, but consistent estimation is still possible if there exists an n×q (q ≥k)
matrix Z of valid instrumental variables. For example, in the case of exact identiﬁcation with q = k,
the conventional instrumental variables (IV) estimator is
ˆθIV = (Z′W)−1Z′y.
With exact identiﬁcation, this estimator is identical to the two-stage least squares (2SLS) estimator
ˆθ2SLS = ( ˆW ′ ˆW)−1 ˆW ′y
where ˆW = Z(Z′Z)−1Z′W.
If, in addition, ε is i.i.d. normal, this estimator is asymptotically
eﬃcient among “limited information” estimators.
An inﬂuential article by Angrist and Krueger has pointed out that, under certain conditions, consistent instrumental variables estimation is still possible even when only y and Z (but
not W) are observed in one sample and only W and Z (but not y) are observed in a second distinct
sample. In that case, the same moment conditions that lead to the conventional IV estimator in
equation (2) motivate the “two-sample instrumental variables” (TSIV) estimator
ˆθTSIV = (Z′
2W2/n2)−1 , nearly all have
used the “two-sample two-stage least squares” (TS2SLS) estimator
ˆθTS2SLS = ( ˆW ′
1 ˆW1)−1 ˆW ′
where ˆW1 = Z1(Z′
2W2. These researchers may not have been aware that the equivalence of
IV and 2SLS estimation in a single sample does not carry over to the two-sample case. Instead, it
is easy to show that, in the exactly identiﬁed case,
ˆθTS2SLS = (Z′
2W2/n2)−1C(Z′
2Z2/n2)(Z′
1Z1/n1)−1 can be viewed as a sort of correction factor for diﬀerences
between the two samples in their covariance matrices for Z. Under Angrist and Krueger’s assumptions, those diﬀerences would disappear asymptotically. As a result, the correction matrix C would
converge in probability to the identity matrix, and the TSIV and TS2SLS estimators would have
the same probability limit. In ﬁnite samples, however, the TSIV estimator originally proposed by
Angrist and Krueger and the TS2SLS estimator typically used by practitioners are numerically
distinct estimators.1
1In a subsequent paper on split-sample IV estimation as a method for avoiding ﬁnite-sample bias when the instruments are only weakly correlated with the endogenous regressors, Angrist and Krueger noted the distinction
between TS2SLS and TSIV and conjectured (incorrectly) that the two estimators have the same asymptotic distribution. In another related literature, on “generated regressors,” ﬁrst-stage estimation is performed to create a
proxy for an unobserved regressor in the second-stage equation, rather than to treat the endogeneity of the regressor.
Murphy and Topel discussed the instance in which the ﬁrst-stage estimation is based on a diﬀerent sample
than the second-stage estimation.
The obvious question then becomes: Which estimator should be preferred? Our paper addresses
this question while going beyond the simple example described above to consider overidentiﬁed as
well as exactly identiﬁed models, heteroskedastic errors, and stratiﬁed samples. It turns out that
the two-sample two-stage least squares approach commonly used by practitioners not only is computationally convenient, but also has theoretical advantages. Its implicit correction for diﬀerences
between the two samples in the distribution of Z yields a gain in asymptotic eﬃciency and also
maintains consistency in the presence of a practically relevant form of stratiﬁed sampling.
The outline of this paper is as follows. Section II compares the asymptotic eﬃciency of the TSIV
and TS2SLS estimators in a basic model. Section III considers departures from the basic model,
such as heteroskedasticity and stratiﬁcation. Section IV summarizes and concludes the paper.
II. Asymptotic Eﬃciency
In this section, we will compare two-sample IV estimators in a general single-equation framework:
β′x1i + γ′z(1)
1i + ε1i = θ′w1i + ε1i,
Πz1i + η1i,
Πz2i + η2i,
where x1i and x2i are p-dimensional random vectors, z1i = [z(1)′
1i ]′ and z2i are q(= q(1) + q(2))dimensional random vectors, w1i is a k(= p + q(1))-dimensional random vector, and Π is a p × q
matrix of parameters.
For eﬃciency comparison, it is useful to characterize these estimators as generalized method
of moments (GMM) estimators. First the TSIV estimator is a GMM estimator based on moment
conditions
z1i(y1i −z(1)′
1i γ) −z2ix′
Next the TS2SLS estimator is a GMM estimator based on
E[z1i(y1i −z′
1iΠ′β −z(1)′
E[z2i ⊗(x2i −Πz2i)]
When Π is deﬁned to be the coeﬃcient on zi in the population linear projection of xi on zi in the
second sample, (12) always holds by deﬁnition of linear projections.
Finally we consider the two-sample limited-information maximum likelihood (TSLIML) estimator for eﬃciency comparison. Let σ11 = E[(ε1i + β′η1i)2] and Σ22 = E(η2iη′
2i). When [εi η′
1i]′ and η2i
are normally distributed the log of the likelihood function can be written as
2ln(2π) −n1
2 ln(σ11) −n2
(y1i −β′Πz1i −γ′z(1)
(x2i −Πz2i)′Σ−1
22 (x2i −Πz2i).
The TSLIML estimator is asymptotically equivalent to a GMM estimator based on the population
ﬁrst-order conditions for the TSLIML estimator:
E[Πz1i(y1i −β′Πz1i −γ′z(1)
i (y1i −β′Πz1i −γ′z(1)
E(z1i ⊗βu1i/σ11 + z2i ⊗Σ−1
11 −1/σ11)
22 η2i) ⊗(Σ−1
22 η2i) −|Σ22|tr(Σ−1
where D2 is a p2 × p(p + 1)/2 matrix such that vec(Σ22) = D2vech(Σ22).
To derive the asymptotic distributions of these estimators we assume the following conditions.
Assumption 1.
(a) {[y1i, z′
i=1 and {[x2i, z′
i=1 are iid random vectors with ﬁnite fourth moments and are
independent.
(b) E(ε1i|z1i) = 0, E(η1i|z1i) = 0 and E(η2i|z2i) = 0.
1i|z1i) = σ11 and E(η2iη′
2i|z2i) = Σ22 where u1i = ε1i + β′η1i, σ11 > 0 and Σ22 is positive
(d) Third moments of [ε1i η′
1i] and those of η2i are all zero conditional on z1i and z2i, respectively.
(e) For the TSIV estimator
E(z1iz(1)′
and for the TS2SLS and TSLIML estimators rank[E(z1iw′
1i)] = dim(θ).
(f) E(z1iz′
1i) and E(z2iz′
2i) are nonsingular.
(g) E(z1ix′
1i) = E(z2ix′
2i) = E(zix′
i) and E(z1iz′
1i) = E(z2iz′
2i) = E(ziz′
(h) limn1,n2→∞n1/n2 = κ for some κ > 0.
Assumption (c) rules out conditional heteroskedasticity, which will be considered in
Section III. Assumption (d) is used to simplify the derivation of the asymptotic covariance matrix
of the TSLIML estimator. Assumption (g) provides a basis for combining two samples.2
Proposition 1. Under Assumption 1, ˆθTSIV , ˆθTS2SLS and ˆθTSLIML are √n1-consistent3 and asymptotically normally distributed with asymptotic covariance matrices ΣTSIV , ΣTS2SLS and ΣTSLIML,
respectively, where
 σ11 + κβ′Σ22β
i) + Cov(z1iz′
1iΠβ) + κCov(z2iz′
−1 E(ziw′
i)[(σ11 + κβ′Σ22β)E(ziz′
i)]−1E(ziw′
and Cov(ziz′
iΠβ) = E(ziz′
iΠββ′Π′ziz′
i) −E(ziz′
iΠβ)E ,
i) + Cov(ziz′
β′Σ22βE(ziz′
i) + Cov(z2iz′
2. Since Cov(z1iz′
1iΠβ)+κCov(z2iz′
2iΠβ) is positive semideﬁnite, it follows that ΣTSIV −ΣTS2SLS is
positive semideﬁnite. Thus, the TS2SLS estimator is more eﬃcient than the TSIV estimator. The
asymptotic eﬃciency gain comes from the implicit correction of the TS2SLS estimator for diﬀerences
between the ﬁnite-sample distributions of z1i and z2i. 3. Proposition 1 shows that the TS2SLS and
TSLIML estimators are asymptotically equivalent. It follows that, when the disturbance terms
i]′ are jointly normally distributed, the TS2SLS estimator is asymptotically eﬃcient within the
2One can show that the TS2SLS estimator requires a weaker condition E(z1ix′
1i) = cE(z2ix′
2i) and E(z1iz′
2i) for some c. Because c does not have to be unity, the TS2SLS estimator is more robust than the TSIV
estimator.
3Following Angrist and Krueger , we scale the estimator by √n1.
class of “limited information” estimators.4
As an illustration, let us consider the case in which one endogenous variable is the only explanatory variable and we have one instrument, i.e., p = q = q(2) = 1. In this case one can show
√n1(ˆβTSIV −β) −√n1(ˆβTS2SLS −β) = n−1/2
1i −√κn−1/2
(1/n2) Pn2
i=1 z2ix2i
Πβ + op(1).
The ﬁrst term on the RHS will be asymptotically independent of √n1(ˆβTS2SLS −β) and have a
positive variance even asymptotically and it follows from Proposition 1 that its variance is given by
1iΠβ) + κV ar(z2
[E(zixi)]2
III. Robustness to Departures from the Basic Model
In this section, we consider departures from the basic model, namely, conditional heteroskedasticity
and stratiﬁcation. In doing so, we assume that the inverse of a consistent estimator of the variance
covariance matrix of moment conditions is used as an optimal weighting matrix to achieve eﬃciency
among GMM estimators given the moment conditions. We will call the resulting GMM estimator
based on (10) the TSIV estimator and denote it by ˜θTSIV , and the resulting GMM estimator based
on (11) and (12) will be called the TS2SLS estimator and be denoted by ˜θTS2SLS.
4In Monte Carlo experiments, we have veriﬁed that these asymptotic results accurately characterize the ﬁnitesample behavior of the TSIV, TS2SLS, and TSLIML estimators. The exception is that, when the instruments are
very weakly correlated with the endogenous regressor, all three estimators appear to be biased towards zero. This
corroborates an analytical result of Angrist and Krueger concerning TS2SLS. The ﬁnding that TSLIML is
subject to a similar ﬁnite-sample bias is interesting. Apparently, the well-known tendency for LIML to be less biased
than 2SLS when the instruments are weak in the single-sample setting 
does not carry over to the two-sample setting.
First, consider the case of conditional heteroskedasticity. We replace Assumption (c) by
1i) > 0 and E(z2iz′
2i) is positive deﬁnite.
Then we obtain the following:
Proposition 2. Under Assumption 1 with Assumption (c) replaced by Assumption (c’), ˜θTSIV and
˜θTS2SLS are consistent and asymptotically normally distributed with asymptotic covariance matrices
TSIV and Σhetero
TS2SLS, respectively, where
i) + κE(β′ηiη′
i) + Cov(z1iz′
1iΠβ) + κCov(z2iz′
i−1 E(ziw′
i) + κE(β′ηiη′
i−1 E(ziw′
Remark. As in Proposition 1, the GMM estimator based on (11) and (12) is asymptotically more
eﬃcient than the GMM estimator based on (10).
Next consider a practically relevant type of stratiﬁed sampling. Suppose that either or both of
the two samples use sampling rates that vary with some of the instrumental variables. For example,
household surveys commonly use diﬀerent sampling rates by race or location, which may be among
the regressors in z(1)
1i in equation (7). The National Longitudinal Surveys have oversampled African-
Americans, the Health and Retirement Study has oversampled residents of Florida, and the Current
Population Survey has oversampled in less populous states.
When analyzing stratiﬁcation, it is useful to deﬁne two binary selection variables:
if the ﬁrst sample includes the ith observation
if the second sample includes the ith observation
By construction, s1i + s2i = 1 for each i. Note that the TSIV estimator and the TS2SLS estimator
can be viewed as GMM estimators based on population moment conditions
E(s1i)zi(yi −z′
E(s2i)zix′
E[s1izi(yi −z′
1iγ) −κs2izix′
E[(s1i −κs2i)vech(ziz′
respectively.
Assumption 2.
(a) [s1i, s2i, xi, yi, z′
i]′ is an iid random vector with ﬁnite fourth moments.
(b) E(εi|s1i, zi) = 0 and E(ηi|s1i, s2i, zi) = 0.
i |s1i, zi) = σ11 and E(ηiη′
i|s2i, zi) = Σ22 where ui = εi + β′ηi, σ11 > 0 and Σ22 is positive
(d) For the TSIV estimator
" E(s2izix′
E(s1iziz(1)′
and for the TS2SLS and TSLIML estimators rank[E(s1iziw′
i)] = dim(θ).
(e) E(s1iziz′
i) and E(s2iziz′
i) are nonsingular.
(f) E(s1is2i) = 0.
(g) s1i and s2i are independent of xi.
(h) E((s1i/p1)zix′
i) = κE((s2i/p2)zix′
i) and E((s1i/p1)ziz′
i) = κE((s2i/p2)ziz′
i) for some constant
κ where p1 = P(s1i = 1) and p2 = P(s2i = 1).
E[s1izi(yi −z′
E(s1iziεi) + E[s1iziη′
E[s2izi ⊗(xi −Πzi)]
E(s2izi ⊗ηi)
this type of stratiﬁcation does not aﬀect the validity of the moment conditions for the TS2SLS
estimator. In contrast, when the two samples diﬀer in their stratiﬁcation schemes, the population
moment function for the TSIV estimator
E(s1iziεi) + E
is likely to be nonzero. As a result, the TSIV estimator will not be consistent in general.
It is possible, however, to modify the TSIV estimator so that it is robust to stratiﬁcation. Deﬁne
a robust modiﬁcation of the TSIV estimator, ˜θRTSIV = [˜β′
RTSIV ]′, by a GMM estimator based
on sample moment functions
E[s1izi(yi −z′
1iγ) −κs2izix′
E[(s1i −κs2i)vech(ziz′
Proposition 3. Under Assumption 2, ˜θTS2SLS and ˜θRTSIV are consistent and asymptotically normally
distributed with asymptotic covariance matrices ΣTS2SLS and ΣRTSIV , respectively, where
ΣTS2SLS = (σ11 + κβ′Σ22β)[E(s1iwiz′
i)E(s1iziz′
i)−1E(s1iziw′
and ΣRTSIV is the upper-left k × k submatrix of (G′
RTSIV V −1
RTSIV GRTSIV )−1,
GRTSIV = −
E(s2ivech(ziz′
 (σ11 + κβ′Σ22β)E(s1iziz′
i) + E[(s1i + κ2s2i)ziz′
iΠ′ββ′Πziz′
E[(s1i + κ2s2i)ziz′
iΠ′βvech(ziz′
E[(s1i + κ2s2i)vech(ziz′
E[(s1i + κ2s2i)vech(ziz′
i)vech(ziz′
Moreover, ΣTS2SLS ≤ΣRTSIV .5
Remarks. The TS2SLS estimator is more eﬃcient than the robust version of the TSIV estimator.
Proposition 3 does not rule out cases in which Σstrat
TS2SLS = Σstrat
RTSIV . Even in those cases, the TS2SLS
estimator may be still preferable for the following reason. Note that the number of overidentifying
restrictions for the robust TSIV estimator is q2 +q(q +1)/2−p−1 whereas the one for the TS2SLS
estimator is q2−p. In typical applications, q > 1 and thus q2+q(q+1)/2−p−1 is greater than q2−p.
Because using too many moment conditions often results in poor ﬁnite-sample performance of the
GMM estimator , the TS2SLS estimator
may be preferable in small samples.
IV. Summary
Following Angrist and Krueger’s inﬂuential work on two-sample instrumental variables
(TSIV) estimation, many applied researchers have used a two-sample two-stage least squares (TS2SLS)
variant of Angrist and Krueger’s estimator. In the two-sample context, unlike the single-sample
setting, the IV and 2SLS estimators are numerically distinct. Under the conditions in which both
estimators are consistent, we have shown that the commonly used TS2SLS approach is more asymptotically eﬃcient because it implicitly corrects for diﬀerences in the empirical distributions of
the instrumental variables between the two samples. That correction also protects the TS2SLS
5Following the convention, Σstrat
T S2SLS ≤Σstrat
RT SIV if and only if Σstrat
RT SIV −Σstrat
T S2SLS is positive semideﬁnite.
estimator from an inconsistency that aﬄicts the TSIV estimator when the two samples diﬀer in
their stratiﬁcation schemes.
Appendix: Proofs
In Propositions 1, 2 and 3, the consistency and asymptotic normality of the GMM estimators follow
from the standard arguments. Thus, we will focus on the derivation of asymptotic variances in the
following proofs.
Proof of Proposition 1.
Let GTSIV and VTSIV denote the Jacobian and covariance matrix,
respectively, of the moment condition (10). Under Assumptions 1(c)(g)(h), we have
(σ11 + κβ′Σ22β) E(ziz′
i) + Cov(z1iz′
1iΠβ) + κCov(z2iz′
from which (18) follows.
Let GTS2SLS and VTS2SLS denote the Jacobian and covariance matrix, respectively, of the moment
conditions (11) and (12). Because the Jacobian and covariance matrices of the moment functions
are given by
respectively, the asymptotic covariance matrix of the TS2SLS estimator is the k × k upper-left
submatrix of the inverse of
TS2SLSV −1
TS2SLSGTS2SLS =
i))−1E(ziw′
Because the k × k upper-left submatrix of (G′
TS2SLSV −1
TS2SLSGTS2SLS)−1 is the inverse of
i))−1E(ziw′
i) −E(wiz′
i))−1E and
= σ11 + κβ′Σ22β
by Theorem 0.7.4 of Horn and Johnson , (19) follows.
Under the assumptions, one can show that the asymptotic distribution of ˆθTSLIML and the one
of the TSLIML estimator for [σ11 vech(Σ22)′]′ are independent. Thus, we can focus on the moment
conditions (13), (14) and (15). Under the stated assumptions, the Jacobian GTSLIML and covariance
matrix VTSLIML of these moment conditions are the same and are given by
i) ⊗β′/σ11
i) ⊗β′/σ11
i)Π′ ⊗β/σ11
1i) ⊗β/σ11
i) ⊗β′/σ11
i)Π′ ⊗β′/σ11
i) ⊗β′/σ11
i)Π′ ⊗β′/σ11
22 )−1β = κβ′Σ22β −(κβ′Σ22β)2 / (σ11 + κβ′Σ22β) ,
one can show that the upper-left k × k submatrix of (G′
TSLIMLV −1
TSLIMLGTSLIML)−1 can be written
Proof of Proposition 2. Note that
A−1 −A−1BA−1 + A−1B(A + B)−1BA−1 = B−1A(A + B)−1BA−1 = (A + B)−1,
where A = E(u2
i) and B = E(β′ηiη′
i). The remainders of the proofs of (21) and (22) are
analogous to those of (18) and (19), respectively, and thus they are omitted.
Proof of Proposition 3. The proof of (29) is analogous to the one of (19) and is omitted.
By Theorem 13 of Amemiya , the upper-left q × q submatrix matrix and the
upper-right q × (q(1 + 1)/2) submatrix of V −1
RTSIV can be written as the inverse of
(σ11 + κβ′Σ22β)E(s1iziz′
i) + E[(s1i + κ2s2i)ziz′
iΠ′ββ′Πziz′
−E[(s1i + κ2s2i)ziz′
iΠ′βvech(ziz′
i)′]{E[(s1i + κ2s2i)ziz′
iΠ′ββ′Πziz′
×E[(s1i + κ2s2i)vech(ziz′
(σ11 + κβ′Σ22β)E(s1iziz′
where the equality follows because the residuals from regressing β′Πziz′
i on vech(zizi)′ are numerically zero by the projection argument. The other submatrices of V −1
RAK can be obtained by using
the same theorem. After some matrix algebra, one can show that
RTSIV V −1
RTSIV GRTSIV )−1 =
i)[E(s1iziz′
i)β −E[(s1i + κ2s2i)ziz′
iΠ′βvech(zizi)′]
× {E[(s1i + κ2s2i)vech(zizi)vech(zizi)′]}−1E .