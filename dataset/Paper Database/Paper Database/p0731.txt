Reducing Quantity Hallucinations in Abstractive Summarization
Zheng Zhao
Shay B. Cohen
Bonnie Webber
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh, EH8 9AB
 , {scohen,bonnie}@inf.ed.ac.uk
It is well-known that abstractive summaries
are subject to hallucination—including material that is not supported by the original text.
While summaries can be made hallucinationfree by limiting them to general phrases, such
summaries would fail to be very informative.
Alternatively, one can try to avoid hallucinations by verifying that any speciﬁc entities in
the summary appear in the original text in a
similar context. This is the approach taken by
our system, HERMAN. The system learns to
recognize and verify quantity entities (dates,
numbers, sums of money, etc.)
in a beamworth of abstractive summaries produced by
state-of-the-art models, in order to up-rank
those summaries whose quantity terms are supported by the original text. Experimental results demonstrate that the ROUGE scores of
such up-ranked summaries have a higher Precision than summaries that have not been upranked, without a comparable loss in Recall,
resulting in higher F1.
Preliminary human
evaluation of up-ranked vs.
original summaries shows people’s preference for the former.
Introduction
Automatic summarization is the task of compressing a lengthy text to a more concise version that
preserves the information of the original text. Common approaches are either extractive, selecting and
assembling salient words, phrases and sentences
from the source text to form the summary , or abstractive, generating the summary
from scratch, containing novel words and phrases
that are paraphrased from important parts of the
original text . The latter is more challenging as it involves human-like capabilities, e.g.,
paraphrasing, generalizing, inferring and including
Article: . .. the volcano was still spewing ash on Sunday,
hampering rescue operations. More than a dozen people
were killed when it erupted in 2014 ...rescue teams are
still scouring the area, looking for more victims who may
have been killed or badly burned . . .
Summary: Rescue teams in Indonesia are searching for
more than 20 people missing after the Mount Sinabung
volcano erupted on Saturday, killing at least 11 people
and injuring at least 20 others.
Article: The scale of the criminal operation has been detailed by the three sources, who say they were . . . a victim
of the fraud shown the call centre script has conﬁrmed it
matched the one read out to her when she was conned out
of 5,000 . . .
Summary: Three whistleblowers have told the BBC that
they were involved in a scam that conned hundreds of
TalkTalk customers out of more than 100,000.
Article: The government and the doctors’ union have
agreed to continue negotiating until Wednesday. The talks,
hosted by conciliation service Acas . . .
Summary: Talks aimed at averting the imposition of a
new junior doctors’ contract in England have been extended for a second day.
Examples of system generated abstractive
summaries with hallucinated quantities. Phrases in the
articles highlighted in cyan have been used by the summarization system to generate summaries. Phrases in
the summaries highlighted in green are correct with respect to the article, whereas red highlighting indicates
hallucinations. Note that the ﬁrst article describes both
a new eruption and a previous one in 2014. It was in the
previous eruption that more than a dozen people were
killed, hence a hallucination of at least 11 people killed
and at least 20 injured in the new eruption.
real-world knowledge .
Abstractive summarization has attracted increasing attention recently, thanks to the availability of large-scale datasets and advances on neural architectures
 . Although modern abstractive summarization systems
generate relatively ﬂuent summaries, recent work
has called attention to the problem they have with
 
factual inconsistency .
That is, they produce summaries that contain hallucinated facts that are not supported by the source
text. A recent study has shown that up to 30%
of summaries generated by abstractive summarization systems contain hallucinated facts . Such high levels of factual hallucination
raise serious concern about the usefulness of abstractive summarization, especially if one believes
that summaries (whether extractive or abstractive)
should contain a mixture of general and speciﬁc
information .
This paper explores reducing the frequency
of one type of hallucinated fact in abstractive
summaries—hallucinated quantities. We focus on
quantities not only because they are important for
factual consistency, but also because, unless they
are wildly inaccurate, a reader might not notice that
they are hallucinated. Moreover, unlike people’s
names (which are also frequently hallucinated),
quantity entities are rarely referred to anaphorically,
avoiding the need to resolve anaphoric expressions,
making them an excellent testbed for the study of
hallucination. The quantities we address can be
broadly categorized into seven types: dates, times,
percentages, monetary values, measurements, ordinals, and cardinal numbers. Table 1 shows some
examples of hallucinated quantities introduced by
abstractive summarization models.
We present HERMAN1, a system that learns to
recognize quantities in a summary and verify their
factual consistency with the source text. Our system can be easily coupled with any abstractive
summarization models that produce a beam-worth
of candidate summaries. After verifying consistency, we use a re-ranking approach that up-rank
those summaries whose quantities are supported by
the source text, similar to the method proposed by
Falke et al. . Training data is automatically
generated in a weakly supervised manner from a
summarization dataset containing both original and
synthetic data. The synthetic data is created by selecting quantity entities from the summary and replacing them with randomly selected entities from
the source text that are the same type. We perform experiments on the XSum dataset which favors an abstractive modeling approach. Results based on automatic evaluation using ROUGE demonstrate that
1Name inspired by the fact-checker Herman Brooks from
the 1980s American sitcom “Herman’s Head.”
up-ranked summaries have higher ROUGE Precision than original summaries produced by three
different summarization systems. While ROUGE
Recall of these up-ranked summaries is lower, overall ROUGE F1 is higher for up-ranked summaries,
showing that it is not simply a like-for-like tradeoff of Recall for Precision. A preliminary human
evaluation study shows that subjects prefer the upranked summaries to the original summaries.
Related Work
Recent studies have suggested that abstractive summarization systems are prone to generate summaries with hallucinated facts that cannot be supported by the source document. Cao et al. reported that almost 30% of the outputs of a state-ofthe-art system contain factual inconsistencies. An
evaluation of summaries produced by recent stateof-the-art models via crowdsourcing suggested that
25% of the summaries have factual errors . The work also showed that ROUGE
scores do not correlate with factual correctness,
emphasizing that ROUGE based evaluation alone
is not enough for summarization task. In addition, Kryscinski et al. pointed out that
current evaluation protocols correlate weakly with
human judgements and do not take factual correctness into account. Maynez et al. conducted
a large scale human evaluation on the generated
summaries of various abstractive summarization
systems and found substantial amounts of hallucinated content in those summaries. They also
concluded that summarization models initialized
with pre-trained parameters perform best on not
only ROUGE, but also human judgements of faithfulness/factuality.
Another line of research focused on evaluating factual consistency of summarization systems.
Kryscinski et al. proposed a weaklysupervised, model-based approach for evaluating
factual consistency between source documents and
generated summaries. They ﬁrst generate training data by applying a series of transformations
to randomly selected individual sentences from
source documents (which they call claims) and
assign them a binary label based on the type of
the transformation. Then they train a fact-checking
model to classify the label of the claim and extract
spans in both the source document and the generated summary explaining the model’s decision.
Goodrich et al. introduced a model-based
The crash happened at Evanton at about 17:20 on Saturday. The ﬁre service and the air
ambulance was sent to the scene. The occupants of all three vehicles were injured, but
the extent of their injuries was not known, police said. A spokesman added: “Inquiries
are ongoing into this matter and no further witnesses are sought at this time” ...
Table 2: An example of a VERIFIED summary with its labels from our dataset. Cyan text highlights the support
in the source document for the quantity token highlighted green in the summary.
metric for estimating the factual accuracy of generated text. Factual accuracy is deﬁned as Precision
between claims made in the source document and
the generated summary, where claims are represented as subject-relation-object triplets. They also
released a new dataset for training relation classi-
ﬁers and end-to-end fact extraction models based
on Wikipedia and Wikidata.
Several studies have focused on tackling the
problem of factual inconsistencies between inputs
and outputs of summarization models by exploring different model architectures and methods for
training and inference. Cao et al. attempted
to solve the problem by encoding extracted facts
as additional inputs to the system. The fact descriptions are obtained by leveraging Open Information Extraction along with
parsed dependency trees of the input text. Zhang
et al. developed a framework to evaluate the
factual correctness of generated summaries by employing an information extraction module to check
facts against the source document, and proposed
a training strategy that optimizes the model using
reinforcement learning with factual correctness as
a reward policy. Falke et al. proposed a
re-ranking approach to improve factual consistency
of summarization models. Their approach used
natural language inference models to score candidate summaries obtained in beam search by averaging the entailment
probability between all sentence pairs of source
document and summary. The summary with the
highest score is up-ranked and used as ﬁnal output of the summarization system. After evaluating
their approach using summaries generated by summarization systems trained on the CNN-DailyMail
corpus , they concluded that
out-of-the-box NLI models transfer poorly to the
task of evaluating factual correctness, limiting the
effectiveness of re-ranking.
Methodology
Let X be the article and S be the corresponding summary where both are sequences of tokens, x1 · · · xa and s1 · · · sn, respectively. Given a
(X, S) pair, our aim is to generate a tag sequence Y
with the same length as S (i.e., n) and a summarylevel label z ∈{VERIFIED, UNVERIFIED}, indicating whether the summary S can be veriﬁed
using X. The generated tag sequence y1 · · · yn
contains token-level labels where yj ∈{B-V, B-U,
I-U, I-V, O} indicating whether the token is Veriﬁed, Unveriﬁed, or Other. We adopt the BIO format for labels since
entities may span multiple tokens. To aid the recognition of quantity based entities, we also obtain
a sequence of binary labels M = (m1, . . . , mn)
for the summary indicating the location of these
Our approach consists of two steps. First, we
create a synthetic, weakly-supervised dataset D =
{(X(i), S(i), M(i), Y (i), z(i)) | i ∈{1 . . . N}}
consisting of N input-output pairs, where X, S,
and M are the input, Y and z are the output. At
training time, a veriﬁcation model learns to recognize and verify quantities in the summary. At
test time, the same veriﬁcation model is applied
to the summaries identiﬁed in a beam search for
candidate summaries carried out by the summarization systems, which results in each of them being
given a veriﬁcation score. We provide a detailed
description in the rest of this section.
Dataset Generation
The dataset used to train the veriﬁcation model
comprises the dataset used to train the summarization system, augmented with negative examples
and additional labels. As we focus on quantities,
Hidden States
Attention Layer
Source Article
Hidden States
Figure 1: Architecture of HERMAN. Note that the binary classiﬁer for predicting whether a summary is veriﬁed (z
labels) is omitted here. It simply takes the context vectors of the summary tokens and run through a MLP classiﬁer.
we apply the spaCy NER tagger to identify all such entities in both
the article and summary. A gold summary in the
original summarization dataset receives a z label
VERIFIED. To generate versions of this summary
with z label UNVERIFIED, we replace quantity
entities in the summary with randomly selected entities from the article that are the same type. For
example, a date entity can only be replaced by
another date entity from the article. We ensure
the UNVERIFIED summary is different from its
VERIFIED counterpart. If an article only contains the one quantity entity which appears in the
VERIFIED summary, i.e. no replacement can be
found to get the UNVERIFIED version, we discard both examples for our dataset to maintain a
balanced dataset.
In addition to the binary summary-level label z,
we also generate two sequences of labels Y and
M. Quantity entities recognized by spaCy NER in
VERIFIED summaries are labeled V, and replaced
ones in the UNVERIFIED summaries are labelled
U. Tokens with O labels are unlikely to directly
affect whether a quantity based entity has been hallucinated, whereas tokens with V and U labels indicate they are important and could potentially affect
the factual accuracy of the summary. With BIO format adopted, these labels become B-V, B-U, I-V,
I-U, and O. For the sequence of binary labels M:
otherwise .
Table 2 illustrates an example of VERIFIED summary with its labels and corresponding article.
Veriﬁcation Model
The overall architecture for our veriﬁcation model
HERMAN is illustrated in Figure 1. The article
encoder provides hidden representations for every
input token which are then fed to a decoder with
attention to obtain the context vector. The context
vectors from every token in the summary are then
fed into a Conditional Random Fields (CRF) layer
 to generate the tag sequence
Y . The same context vectors are fed into a binary
classiﬁer to obtain the binary label z.
BiLSTM Article Encoder
For input article X
where X = {x1, . . . , xa} and xi denotes the ith
token in X, a contextualized token-level encoding
hi is obtained via a BiLSTM encoder :
−→h i = LSTMf(xi, −→h i−1),
←−h i = LSTMb(xi, ←−h i+1),
hi = [−→h i; ←−h i],
where −→h i and ←−h i are hidden states of forward and
backward LSTMs at time step i, and ; denotes the
concatenation operation.
BiLSTM-CRF Decoder with Attention
decoder generates sequence of labels Y as well as a
binary label z. As the length of labels to be decoded
is ﬁxed, the setup is similar to BiLSTM-CRF used
in the sequence tagging task .
The difference is that the decoder takes additional
input hi which is article encoding and incorporates
attention mechanism . The
BiLSTM with attention component ﬁrst encodes
the summary, token by token, to produce an intermediate representation. We also obtain a sequence of binary labels M = {m1, . . . , mn} for
the summary using spaCy NER to recognize tokens
that make up quantity entities. Then the intermediate representation, along with the binary label
sequence, is fed to the CRF layer to predict the Y
label. The intermediate representation is also fed
to an MLP classiﬁer to obtain the binary label z.
Training and Inference
Given the training set with labelled sequence
{X(i), S(i), M(i), Y (i), z(i) | i ∈{1 . . . N}}, we
maximize the conditional log likelihood for the
local veriﬁcation objective:
¯w = argmax
log p(Y (i) | X(i), S(i), M(i), w),
where w denotes the model’s parameters including the weights of the LSTMs and the transition weights of the CRF. The loss function for
Y labels is the negative log-likelihood based on
Y (i) = {y1, . . . , yn}:
log p(yj),
where yj ∈Y (i). For global veriﬁcation which is
predicting z label, the loss function is the binary
cross entropy:
z(i) log p(z(i))
+ (1 −z(i)) log(1 −p(z(i))).
The ﬁnal objective which combines both local
and global veriﬁcation is deﬁned as the following:
L = αLY + (1 −α)Lz,
where α ∈ is a hyperparameter indicating
weight balance between LY and Lz. At test time,
inference for a summary S is obtained by applying
Viterbi algorithm at the CRF layer to ﬁnd the most
probable sequence ˆY :
ˆY = argmax
P(Y | X, S, M, ¯w).
Re-ranking to Avoid Hallucination
We adopt a re-ranking approach in order to reduce
the frequency of hallucinated quantities in the output of abstractive summarization. This is similar
to the approach taken by Falke et al. with
the difference being that their system’s inputs are
sentence level whereas ours are document-level.
Assume an abstractive summarization system can
produce a list of k candidate summaries S1, . . . , Sk
for a given document X using beam search, we
leverage predictions of HERMAN to give each summary a veriﬁcation score. Our scoring approach has
two variants: HERMAN-GLOBAL, and HERMAN-
LOCAL. HERMAN-GLOBAL uses the raw output
of global veriﬁcation label z which has a real value
between . HERMAN-LOCAL uses the average
probabilities of B-V, B-U, I-V, and I-U labels
where entries of B-U and I-U are counted negatively. Out of the k candidate summaries, the summary with the highest veriﬁcation score is selected
as the ﬁnal generated summary for the summarization system.
We use the XSum dataset which was developed
for abstractive document summarization . The XSum dataset consists of BBC
articles, with a single-sentence summary of each.
This summary is a professionally written introductory sentence, typically written by the author of the
article, which is separated from the article, with
the remaining text taken to be the document. This
one-sentence summary, different from a headline
whose purpose is to attract readers to read the article, draws on information distributed in various
parts of the document and displays multiple levels
of abstraction including paraphrasing, fusion, synthesis, and inference. The dataset contains 204,045
instances for training, 11,332 instances for validation, and 11,334 instances for testing. Overall, 55%
of the instances contain at least one quantity. The
distribution of quantity entities is shown in Table 3.
It is clear that the different types of quantities are
distributed unevenly: While almost 30% of summaries contain at least one date entity, only 1%
contain at least one quantity entity. Due to the way
in which the summary was created for a document,
the summary often contains phrases that do not
appear in the document itself. In fact, fewer than
16% of the summaries in the test set have quantity tokens that also appear in their corresponding
Table 3: The distribution of quantity entities in the XSum dataset. Note that the percentages sum to more than
55%, as a summary can contain more than one type of quantity entity. For more details regarding the types of
entities, please refer to the ofﬁcial spaCy webpage2.
documents.
In order to obtain the dataset used to train HER-
MAN, we follow procedures described in Section 3.1. We apply same pre-processing steps noted
by Narayan et al. . We also truncate the
input document to 400 tokens and limit the length
of the summary to 90 tokens. The dataset size for
training, validation, and test are 190,370, 10,594,
and 10,592, respectively. As noted in Section 3.1,
the dataset we use is smaller than the XSum dataset
because we discard instances which cannot be perturbed to obtain an UNVERIFIED summary.
Experiments
For all experiments, we set the hidden dimensions
to 256, the word embeddings to 100, and the vocabulary size to 50k. The word embeddings are
initialized using pre-trained GloVe vectors (6B tokens, uncased). We also
experimented using a pre-trained, base-uncased
BERT for word embedding
initialization. Our training used the Adam optimizer with a learning rate
of 0.001. We also use gradient clipping with a maximum gradient norm of 5 and we do not use any
kind of regularization. We use loss on the validation set to perform early stopping. We set α to 0.66,
suggesting local veriﬁcation is more important than
global veriﬁcation. Our model was trained on a single GeForce GTX 1080 Ti GPU with a batch size
of 32. We use PyTorch for
our model implementation. For CRF, we used the
AllenNLP library with constrained decoding for the BIO scheme. To evaluate
our veriﬁcation model, we need outputs from abstractive summarization systems. We obtain those
from three selected systems: TCONVS2S , BERTSUM ,
and BART using pre-trained
checkpoints provided by the authors.
2 
named-entities
Results of HERMAN on the test set using
GloVe word embedding.
Results of HERMAN on the test set using
BERT word embedding.
Automatic Evaluation
We ﬁrst present results
in Table 4 from our veriﬁcation model using GloVe
on the test set. On the binary classiﬁcation task
of determining whether a summary is VERIFIED
or UNVERIFIED, the model achieved accuracy of
80.12 and F1 of 80.94. The results using BERT
are displayed in Table 5. The model attained accuracy of 80.23 and F1 of 81.6. While no signiﬁcant
difference can be observed in performance, using
BERT does triple the needed training time, so does
not seem justiﬁed.
The standard automatic evaluation metric for
summarization is ROUGE. We report the Precision, Recall and F1 scores of ROUGE-1/2/L, which
respectively measure the word-overlap, bigramoverlap, and longest common sequence between
system and reference summaries. Using HERMAN,
we obtain veriﬁcation scores for the full beam of
candidate summaries produced by the summarization systems. We re-rank candidate summaries
using the veriﬁcation score as described in Section 3.4 and evaluate the up-ranked summaries.
In addition to HERMAN-GLOBAL and HERMAN-
Baseline-shortest
Baseline-max-overlap
HERMAN-LOCAL
HERMAN-GLOBAL
Baseline-shortest
Baseline-max-overlap
HERMAN-LOCAL
HERMAN-GLOBAL
Baseline-shortest
Baseline-max-overlap
HERMAN-LOCAL
HERMAN-GLOBAL
Table 6: Automatic evaluation on the XSum test set. Each of the three horizontal sections reports scores for one
of the three abstractive summarization systems: BART, BERTSUM and TCONVS2S. For each system, we present
ROUGE scores for the two baseline models, the one original model, and the two variants of our HERMAN model.
Baseline-shortest refers to the model that selects the shortest summary. Baseline-max-overlap refers to the model
that selects the summary which overlaps the most with the source document in terms of quantity entities . avg-Q
denotes the average number of quantity entities per summary.
LOCAL, we also introduce two baseline re-ranking
approaches: the ﬁrst selects the shortest summary
from the beam, and the second selects the summary with maximum quantity entity overlap with
the source document. The results on the XSum
dataset are shown in Table 6. While selecting the
shortest summary is a very strong baseline, outperforming all other systems in ROUGE-1/2/L Precision, we can still see that HERMAN-GLOBAL has
the best performance in ROUGE-1/2/L Precision
and F1 despite that baseline. After re-ranking by
HERMAN-GLOBAL, 17.27% originally ranked top
summaries produced by BART stayed at the top
rank. While BERTSUM had nearly the same, only
9.05% of the summaries produced by TCONVS2S
stayed top-ranked, so if re-ranking leads to improvements, it would be even more helpful in the
case of TCONVS2S.
The ﬁrst thing to note is that the up-ranked summaries have a lower ROUGE Recall than other
models. This is common with any model that ﬁlters output, since it can exclude items that might
otherwise contribute to Recall. ROUGE-1/2/L Precision increases after re-ranking as the veriﬁcation
model ensures summaries with more veriﬁed content will be ranked higher in the beam. More veriﬁed content also means more tokens appearing
in the document and reference summary. Overall,
ROUGE-1/2/L F1 score for up-ranked summaries
exceeds that of original summaries. To analyze
the effect of our systems on quantity entities, we
also compute average number of quantity entities
per summary for each system. The baseline that
selects the summary with maximum quantity entity
overlap with the source document, not surprisingly,
has very high averages and achieved the highest
number for BART. HERMAN-GLOBAL achieves
highest average for BERTSUM and TCONVS2S.
In BART, it follows the baseline closely at second
place. Together with its ROUGE performance, this
indicates that our model not only encourages the
inclusion of quantity entities in the summary, but
also includes them correctly.
To further analyze how our approach affects the
distribution of different types of quantity entities,
we also computed test set statistics for both original summaries produced by the summarization
systems and up-ranked summaries produced by
HERMAN-GLOBAL. The results are provided in
Table 7. Overall, counting all quantity types, we
can see that BART encourages the inclusion of
quantities the most, for both original and up-ranked
summaries, while TCONVS2S has the fewest summaries with quantity entities. However, the number
of up-ranked summaries that contain at least one
quantity increases the most for TCONVS2S, a 26%
increase compared with the original summaries.
This agrees with our prior point that as TCONVS2S
has the fewest summaries that remained top after
re-ranking, our approach should be most helpful
for TCONVS2S. Looking at individual quantity
types, the number of summaries containing date or
time quantities increases across-the-board through
re-ranking. For BERTSUM and TCONVS2S, re-
Table 7: The statistics of different types of quantity entities on test set summaries for all three abstractive summarization systems: BART, BERTSUM and TCONVS2S. For each system, we provide the number of original
summaries and up-ranked summaries that contain at least one instance of the given type of quantity entity. Upranked summaries are produced by HERMAN-GLOBAL. % diff denotes the percentage difference between the
number of up-ranked summaries and the number of original summaries for a given quantity type.
ranking generally increases the number of summaries that contain a speciﬁc quantity type, with
the exception of percent in BERTSUM and quantity
in TCONVS2S where they decreased slightly. We
suspect the reason to be that these types are underrepresented in the dataset: Thus, there is insufﬁcient data for the model to learn from. On the other
hand, re-ranking in BART leads to more decreases
of the number of summaries that contain a speciﬁc
quantity type. The reason could be that BART already has the highest number of summaries that
contain a speciﬁc quantity type before re-ranking,
and quantity types with a decrease after re-ranking
are generally underrepresented types like percent
and quantity. Representative types like date and
cardinal are still increased through re-ranking.
Human Evaluation
Falke et al. have argued convincingly that ROUGE is inadequate as
a measure of hallucination and factual correctness.
As such, we have begun to carry out human evaluation. We noted in Section 4 that the XSum reference summary may not be an accurate representation of the source article, in that less than 16%
of the test set reference summaries have quantity
tokens that also appear in their corresponding articles. As a result, our human evaluation presented
subjects with a text consisting of both the reference
summary and the source article, to give subjects a
full sense of its contents.
Subjects assessed 40 trials, each consisting of
a text followed by two candidate summaries—the
original summary produced by the summarization
model and the up-ranked summary selected by
HERMAN-GLOBAL. These two summaries also
satisﬁed the condition of being very similar except
for one quantity entity. The trials comprised 37 randomly selected text-summary pairs that satisﬁed
the additional condition, plus three simple catch
trials in which one of the candidate summaries
has obvious hallucinated quantities that are never
present in the source article, to check whether subjects were paying attention and following the instructions. The order of the trials was randomized
for each subject.
In presenting each trial, quantities in the summaries and those with the same type in the text
were highlighted to make them easy to ﬁnd. Subjects were asked to choose the one summary whose
highlighted quantity entity is more faithful to the
source article. Subjects were also told not to select
a summary based on any other factors such as its
ﬂuency (i.e., Does the summary sound like wellformed English?). After subjects make a choice of
summary, they are also asked whether they think
both candidate summaries were equally faithful or
equally unfaithful. We will show shortly how subjects can prefer one summary over the other, even
while considering both to be faithful (or both to be
unfaithful) to the original text. This preliminary experiment was carried out on the Qualtrics platform,
with three volunteer subjects. Each subject took
between 35 and 45 minutes to ﬁnish.
While our results are still preliminary, they provide some evidence that subjects consider the upranked summaries to be more faithful. Speciﬁcally,
of the 19 trials (other than the three catch trials)
where all three subjects agreed on which summary
was more faithful, in 12 trials, it was the re-ranked
summary (as in Table 8, Article 49), while in only 7
was it the original summary (as in Table 8, Article
4). In all of these cases, the authors agreed with the
subjects. Note that no information can be gleaned
from those trials in which two of three subjects
agreed, since in half of them (9), they agreed on
Article 49:Interest rates for savers have fallen to new record
lows, after hundreds of cuts in recent months and more than
1,000 in the past year . . . In research carried out for the BBC,
the rate-checking ﬁrm Savings Champion recorded 1,440
savings rate cuts last year and more than 230 so far . . .
Article 4: A man has been charged with causing the death
of a three-year-old girl by dangerous driving in a crash involving eight vehicles. Thomas Hunter, 58, of Mansﬁeld
Road, Mansﬁeld, was arrested after the crash on the A34 at
Hinksey Hill, Oxford, on 25 August . . .
Original Summary:
More than 1,500 savings rate cuts
have been made by banks in the past year and more than 230
so far this year, the BBC has learned.
Original Summary: A man has been charged with causing
the death of a three-year-old girl by dangerous driving after
a crash in which seven people were injured.
Up-ranked Summary: More than 1,000 savings rate cuts
have been made by banks in the past year and more than 230
so far this year.
Up-ranked Summary: A man has been charged with causing the death of a six-year-old girl by dangerous driving after
a crash in which seven people were injured.
Article 83: Millions of people face a rise in their insurance
bills this week-end, as a result of an increase in Insurance
Premium Tax (IPT). From Sunday, IPT will increase from
6% to 9.5%, a rise that was announced by Chancellor George
Osborne in his Summer Budget ...
Article 24: Shares in Paddy Power Betfair fell more than
5% despite the bookmaker reporting rising revenues and
underlying proﬁts . . . But after the costs of last year’s merger
between Paddy Power and Betfair were taken into account
the company reported a loss of 5.7m . . .
Original Summary: Car insurance premiums (IPT) will
increase by 9% from Sunday, the AA has said.
Original Summary: Shares in bookmaker Paddy Power
Betfair fell 6% after the company reported a loss for the ﬁnal
three months of last year.
Up-ranked Summary: Car insurance premiums (IPT) will
increase by 9.5% from Sunday , the AA has announced.
Up-ranked Summary: Shares in bookmaker Paddy Power
Betfair fell 7% after the company reported a loss for the ﬁnal
three months of 2016.
Table 8: Example trials selected from our human evaluation. Quantity entities have been highlighted the same way
we did for human evaluation. With article 49 and 83 (containing cardinal and percentage quantities), all subjects
agree that the up-ranked summary is more faithful, while with article 4 and 24 (containing date and percentage
quantities), all agree that the original summary is more faithful.
the re-ranked summary, and in the other half, they
agreed on the original (9).
Finally, the reader may recall that we asked subjects after they selected a summary, whether they
considered one summary to be more faithful than
the other, or whether both summaries were equally
faithful (or equally unfaithful). In 21 trials, at least
two subjects indicated that both summaries were
equally unfaithful, even if they indicated that they
felt one summary was more faithful than the other.
Often, it was because its quantity entities were
closer to those in the text. For example, Table 8,
Article 24 shows that subjects felt the original summary was more faithful since its quantity term (6%)
was closer to the 5% that was in the original text,
while Table 8, Article 83 shows them to feel that
“by 9.5%” is closer to the original text than “by
9%”, even though the quantity in the original text
is “to 9.5%”. In over half these trials (13/21), at
least two subjects felt that the up-ranked summaries
were more faithful.
Conclusions
In this paper, we addressed the problem of hallucinated quantities in summaries generated by abstractive summarization systems. We introduced
HERMAN, a novel approach to recognize and verify quantities in these summaries. Experimental
results demonstrate that up-ranked summaries have
a higher ROUGE Precision and F1 than original
summaries produced by a summarization system,
indicating our approach reduces hallucinated quantities while still encourage the inclusion of quantity
entities. Through human evaluation, we showed
that summaries up-ranked by our proposed model
are felt to be more faithful than the summaries directly generated by a summarization system.
We also discovered that simple re-ranking strategies, such as the selection of the shortest summary
from the beam search, can yield strong performance, if one doesn’t care whether a summary communicates speciﬁc quantities. We also found that
our approach was limited by its use of the XSum
dataset, where factual information in the summary
sometimes cannot be veriﬁed using the article due
to the fact that the summary is simply the ﬁrst sentence of the original article. In the future, we would
like to explore the option of incorporating the veriﬁcation model into training and inference to improve
factual correctness of generated summaries.
Acknowledgments
We would like to thank the anonymous reviewers,
Ronald A. Cardenas, and Shashi Narayan for their
helpful feedback. We also would like to thank
Ronald A. Cardenas, Arlene Casey, Christian Hardmeier, and Javad Hosseini for participating in our
human evaluation.