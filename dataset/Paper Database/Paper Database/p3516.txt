Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 232–237,
Lisbon, Portugal, 17-21 September 2015. c⃝2015 Association for Computational Linguistics.
Non-lexical neural architecture for ﬁne-grained POS Tagging
Matthieu Labeau, Kevin L¨oser, Alexandre Allauzen
Universit´e Paris-Sud and LIMSI-CNRS,
Rue John von Neumann
91403 Orsay cedex
 
In this paper we explore a POS tagging application of neural architectures that can
infer word representations from the raw
character stream.
It relies on two modelling stages that are jointly learnt:
convolutional network that infers a word
representation directly from the character
stream, followed by a prediction stage.
Models are evaluated on a POS and morphological tagging task for German. Experimental results show that the convolutional network can infer meaningful word
representations, while for the prediction
stage, a well designed and structured strategy allows the model to outperform stateof-the-art results, without any feature engineering.
Introduction
Most modern statistical models for natural language processing (NLP) applications are strongly
or fully lexicalized, for instance part-of-speech
(POS) and named entity taggers, as well as language models, and parsers. In these models, the
observed word form is considered as the elementary unit, while its morphological properties remain neglected. As a result, the vocabulary observed on training data heavily restricts the generalization power of lexicalized models.
Designing subword-level systems is appealing
for several reasons. First, words sharing morphological properties often share grammatical function and meaning, and leveraging that information
can yield improved word representations.
Second, a subword-level analysis can address the outof-vocabulary issue i.e the fact that word-level
models fail to meaningfully process unseen word
forms. This allows a better processing of morphologically rich languages in which there is a combinatorial explosion of word forms, most of which
are not observed during training. Finally, using
subword units could allow processing of noisy text
such as user-generated content on the Web, where
abbreviations, slang usage and spelling mistakes
cause the number of word types to explode.
This work investigates models that do not rely
on a ﬁxed vocabulary to make a linguistic prediction. Our main focus in this paper is POS tagging, yet the proposed approach could be applied
to a wide variety of language processing tasks.
Our main contribution is to show that neural networks can successfully learn unlexicalized models that infer a useful word representation from
the character stream. This approach achieves state
of-the-art performance on a German POS tagging
task. This task is difﬁcult because German is a
morphologically rich language1, as reﬂected by
the large number of morphological tags (255) in
our study, yielding a grand total of more than
600 POS+MORPH tags.
An aggravating factor
is that these morphological categories are overtly
marked by a handful of highly ambiguous inﬂection marks (sufﬁxes). We therefore believe that
this case study is well suited to assess both the representation and prediction power of our models.
The architecture we explore in section 2 differs
from previous work that only consider the character level. Following ,
it consists in two stages that are jointly learnt. The
lower stage is a convolutional network that infers
a word embedding from a character string of arbitrary size, while the higher network infers the
POS tags based on this word embedding sequence.
For the latter, we investigate different architectures of increasing complexities: from a feedforward and context-free inference to a bi-recurrent
network that predicts the global sequence. Experimental results (section 4) show that the proposed
approach can achieve state of the art performance
1Besides inﬂected forms, German is characterized by a
possibly inﬁnite and evolving set of compound nouns.
and that the choice of architecture for the prediction part of the model has a signiﬁcant impact.
Network Architectures
The different architectures we propose act in two
stages to infer, for a sentence s = {w1, . . . , w|s|},
a sequence of tags {t1, . . . , t|s|}. Each tag belongs
to the tagset T . The ﬁrst stage is designed to represent each word locally, and focuses on capturing
the meaningful morphological information. In the
second stage, we investigate different ways to predict the tag sequence that differ in how the global
information is used.
From character to word level
To obtain word embeddings, the usual approach
introduced by relies on a
ﬁxed vocabulary W and each word w ∈W is
mapped to a vector of nf real valued features by
a look-up matrix W ∈R|W|∗nf . To avoid the use
of a ﬁxed vocabulary, we propose to derive a word
representation from a sequence of character embedding: if C denotes the ﬁnite set of characters,
each character is mapped on a vector of nc features
gathered in the look-up matrix C.
To infer a word embedding , we use a convolution layer , build as in .
As illustrated in ﬁgure 1, a word w is a character
sequence {c1, .., c|w|} represented by their embeddings {Cc1, .., Cc|w|}, where Cci denotes the row
in C associated to the character ci. A convolution ﬁlter W conv ∈Rnf × Rdc∗nc is applied over
a sliding window of dc characters, producing local
features :
xn = W conv(Ccn−dc+1 : .. : Ccn)T + bconv,
where xn is a vector of size nf obtained for each
position n in the word2. The i-th element of the
embedding of w is the maximum over the i-th elements of the feature vectors :
[f]i = tanh( max
1≤n≤|s|[xn]i)
Using a maximum after a sliding convolution window ensures that the embedding combines local
features from the whole word, and selects the more
2Two padding character tokens are used to deal with border effects. The ﬁrst is added at the beginning and the second
at the end of the word, as many times as it is necessary to obtain the same number of windows than the length of the word.
Their embeddings are added to C.
Wconv × (.)T + bconv
Figure 1: Architecture of the layer for characterlevel encoding of words.
useful ones. The parameters of the layer are the
matrices C and W conv and the bias bconv.
From words to prediction
To predict the tag sequence associated to a sentence s, we ﬁrst use a feedforward architecture,
with a single hidden layer. To compute the probability of tagging the n-th word in the sentence with
tag ti, we use a window of dw word embeddings3
centered around the word wn:
xn = fn−dw−1
: ... : fn+ dw−1
followed by a hidden and output layers:
sn = W o tanh(W hxn + bh) + bo.
The parameters of the hidden an output layers
are respectively W h, bh and W o, bo.
We also experiment with a a bidirectional recurrent layer, as described in . The forward and backward passes allow
each prediction to be conditioned on the complete
past and future contexts, instead of merely a neighboring window. As illustrated in ﬁgure 2, the forward hidden state, at position n, will be computed
using the previous forward hidden state and the
word embedding in position n:
hn = tanh(
hn−1 + bh)
3Similarly, we use special word tokens for padding.
Figure 2: Bidirectional recurrent architecture for
tag prediction. The upper part is used in the case
of structured inference.
W hh are the transition matrices of
the forward part of the layer, and bh is the bias.
The backward hidden states are computed similarly, and the hidden states of each direction are
concatenated to pass through an output layer:
sn = W o ,
also used in . Let consider each possible tag sequence {t1, . . . , t|s|} as a
possible path over a sequence of hidden states. We
can add a transition matrix W trans and then compute the score of a sequence as follows:
1 , {w}|s|
tn−1,tn + [sn]tn
The Viterbi algorithm offers an exact solution to infer the path that gives the maximum score. It is worth noticing that both these
strategies can be applied to the feedforward and
bidirectional recurrent networks. For both strategies, the whole network can estimate conditional
log-likelihood of a tag sequence given a sentence
s and the set of parameters θ. This criterion can
then be optimized using a stochastic gradient ascent with the back-propagation algorithm.
Related Work
The choice to consider words from the character level has recently been more and more explored.
While its raw application to language
modeling did not achieve clear improvement over
the word-based models , this
approach shown impressive results for text generation .
However, for this line of work, the main issue is
to learn long range dependencies at the character
level since the word level is not considered by the
More recently, the character level was considered as more interpretable and convenient
way to explore and understand recurrent networks . In , the authors build a text understanding model that does not require any knowledge
and uses hierarchical feature extraction. Here the
character level allows the model to ignore the definition a priori of a vocabulary and let the model
build its own representation of a sentence or a document, directly from the character level. To some
extent, our work can be considered as an extension
of their work, tailored for POS tagging.
 applies a very similar model to the POS tagging of Portuguese and
English. also descends lower
than the word level, using a dictionary of morphemes and recursive neural networks to model
the structure of the words. Similarly, this allows
a better representation of rare and complex words,
evaluated on a word similarity task.
Experiments and Results
Experiments are carried out on the Part-of-Speech
and Morphological tagging tasks using the German corpus TIGER Treebank .
To the best of our knowledge, the best results on
this task were published in ,
who applied a high-order CRF that includes an intensive feature engineering to ﬁve different languages. German was highlighted as having ’the
most ambiguous morphology’. The corpus, de-
Architecture
Feedforward
4.22 ± 0.05
5.89 ± 0.07
13.97 ± 0.14
17.46 ± 0.14
3.90 ± 0.05
5.33 ± 0.09
12.22 ± 0.13
15.34 ± 0.13
3.31 ± 0.07
4.22 ± 0.07
13.50 ± 0.16
16.23 ± 0.13
2.92 ± 0.02
3.82 ± 0.04
11.65 ± 0.11
14.43 ± 0.19
2.59 ± 0.05
3.34 ± 0.09
11.89 ± 0.14
14.63 ± 0.22
2.22 ± 0.03∗
2.86 ± 0.03∗
9.11 ± 0.14
11.29 ± 0.06
6.03 ± 0.06
8.05 ± 0.05
17.83 ± 0.11
21.33 ± 0.26
3.89 ± 0.06
5.26 ± 0.05
11.88 ± 0.05
17.78 ± 0.12
4.46 ± 0.08
5.84 ± 0.19
16.61 ± 0.18
19.39 ± 0.12
2.74 ± 0.07
3.59 ± 0.07
10.09 ± 0.09
12.88 ± 0.28
3.63 ± 0.06
4.63 ± 0.04
14.83 ± 0.11
17.54 ± 0.13
2.21 ± 0.04∗
2.86 ± 0.05∗
8.63 ± 0.21∗
10.97 ± 0.19∗
Table 1: Comparison of the feedforward and bidirectional recurrent architectures for predictions, with
different settings. The non-lexical encoding is convolutional. CRF refers to state-of-the-art system of
 . Simple and Struct. respectively denote the position-by-position and structured
prediction. ∗indicates our best conﬁguration.
scribed in details in , contains
a training set of 40472 sentences, a development
and a test set of both 5000 sentences. We consider
the two tagging tasks, with ﬁrst a coarse tagset (54
tags), and then a morpho-syntactical rich tagset
(619 items observed on the the training set).
Experimental settings
All the models are implemented4 with the Theano
library . For optimization,
we use Adagrad , with a learning rate of 0.1. The other hyperparameters are:
the window sizes, dc and dw, respectively set to
5 and 9, the dimension of character embeddings,
word embeddings and of the hidden layer, nc, nf
and nh, that are respectively of 100, 200 and 2005.
The models were trained on 7 epochs. Parameter initialization and corpus ordering are random,
and the results presented are the average and standard deviation of the POS Tagging error rate over
4Implementation is available at 
com/MatthieuLabeau/NonlexNN
5For both the learning rate and the embedding sizes, results does not differ in a signiﬁcant way in a large range of hyperparameters, and their impact resides more in convergence
speed and computation time
The ﬁrst experiment aims to evaluate the efﬁciency
of a convolutional encoding with the basic feedforward architecture for prediction. We compare
a completely non-lexicalized model which relies
only on a character-level encoding with a lexicalized model where we use conventional word embeddings stored with a ﬁxed vocabulary6.
Results are reported in Table 1 along with with the
state-of-the-art results published in and . For instance in the former, by choosing the
most probable tag position-by-position, the error
rate on the development set of the TIGER dataset
6Every word that appears in the training set.
is 32.7 for the simple POS Tagging task.
We further analyze the results by looking at
the error rates respectively on known and unknown words7.
From table 2, we observe that
the number of unknown words wrongly labeled
is divided by 3 for POS and almost divided by
2 for POS+Morph tagging, showing the ability
of character-level encoding to generalize to new
words. Moreover, a strictly non-lexical encoding
makes slightly more mistakes on words already
seen, whereas the model that concatenates both
embeddings will make less mistakes for both unknown and known words.
This shows that information from the context
and from the morphology are complementary,
which is conjectured in by
using a morphological analyzer in complement of
higher-order CRF.
Table 2: Error counts for known/unknown words
in the test set, with a structured feedforward prediction model for the tagging task.
In the second set of experiments, we evaluate
the convolutional encoding with a bidirectional recurrent network for prediction. Results are presented in the second half of Table 1. Surprisingly,
this architecture performs poorly with simple inference, but clearly improves when predicting a
structured output using the Viterbi algorithm, both
for training and testing. Moreover, a non-lexical
model trained to infer a tag sequence with the
Viterbi algorithm achieves results that are close to
the state-of-the-art, thus validating our approach.
We consider that this improvement comes from the
synergy between using a global training objective
with a global hidden representation, complexifying the model but allowing a more efﬁcient solution. Finally, the model that uses the combination
of both the character and word-level embeddings
yields the best results. It is interesting to notice
that the predictive architecture has no inﬂuence on
the results of the simple task when the prediction is
7Unknown words refer to words present in the development or test sets, but not in the training set.
structured, but improves them on the difﬁcult task.
This also shows that the contribution of word embeddings to our model corresponds to a difference
of 1.5 to 2 points in performance.
Conclusion
In this paper, we explored new models that can infer meaningful word representations from the raw
character stream, allowing the model to exploit the
morphological properties of words without using
any handcrafted features or external tools. These
models can therefore efﬁciently process words that
were unseen in the training data. The evaluation
was carried out on a POS and morphological tagging task for German. We described different architectures that act in two stages: the ﬁrst stage is a
convolutional network that infers a word representation directly from the character stream, while the
second stage performs the prediction. For the prediction stage, we investigated different solutions
showing that a bidirectional recurrent network can
outperform state-of-the-art results when using a
structured inference algorithm.
Our results showed that character-level encoding can address the unknown words problem for
morphologically complex languages.
In the future, we plan to extend these models to other tasks
such as syntactic parsing and machine translation.
Moreover, we will also investigate other architectures to infer word embeddings from the character
level. For instance, preliminary experiments show
that bidirectional recurrent network can achieve
very competitive and promising results.
Acknowledgments
We would like to thank the anonymous reviewers for their helpful comments and suggestions.
This work has been partly funded by the European Unions Horizon 2020 research and innovation programme under grant agreement No.
645452 (QT21).