Recurrent Network Models for Human Dynamics
Katerina Fragkiadaki
Sergey Levine
Panna Felsen
Jitendra Malik
University of California, Berkeley
Berkeley, CA
{katef,svlevine@eecs,panna@eecs,malik@eecs}.berkeley.edu
We propose the Encoder-Recurrent-Decoder (ERD)
model for recognition and prediction of human body pose in
videos and motion capture. The ERD model is a recurrent
neural network that incorporates nonlinear encoder and decoder networks before and after recurrent layers. We test
instantiations of ERD architectures in the tasks of motion
capture (mocap) generation, body pose labeling and body
pose forecasting in videos. Our model handles mocap training data across multiple subjects and activity domains, and
synthesizes novel motions while avoiding drifting for long
periods of time. For human pose labeling, ERD outperforms a per frame body part detector by resolving left-right
body part confusions. For video pose forecasting, ERD predicts body joint displacements across a temporal horizon of
400ms and outperforms a Ô¨Årst order motion model based on
optical Ô¨Çow. ERDs extend previous Long Short Term Memory (LSTM) models in the literature to jointly learn representations and their dynamics. Our experiments show such
representation learning is crucial for both labeling and prediction in space-time. We Ô¨Ånd this is a distinguishing feature between the spatio-temporal visual domain in comparison to 1D text, speech or handwriting, where straightforward hard coded representations have shown excellent results when directly combined with recurrent units .
1. Introduction
Humans have a remarkable ability to make accurate
short-term predictions about the world around them conditioned on prior events . Predicting the movements
of other humans is an important facet of these predictions.
Although the number of possible movements is enormous,
conditioning on visual history can reduce the range of probable outcomes to a manageable degree of variation. For example, a walking pedestrian will most likely continue walking, and will probably not begin dancing spontaneously.
Short term predictions of human kinematics allows people to adjust their behavior, plan their actions, and properly
direct their attention when interacting with others. Similarly, for Computer Vision algorithms, predicting human
motion is important for timely human-computer interaction , obstacle avoidance , and people tracking .
While simpler physical phenomena, such as the motion of
inanimate objects, can be predicted using known physical
laws, there is no simple equation that governs the conscious
movements of a person. Predicting the motion of humans
instead calls for a statistical approach that can model the
range of variation of future behavior, and presents a tremendous challenge for machine learning algorithms.
We address this challenge by introducing Encoder-
Recurrent-Decoder (ERD) networks, a type of Recurrent
Neural Network (RNN) model that combines representation learning with learning temporal dynamics. We
apply this model to generation, labeling, and forecasting of
human kinematics. We consider two data domains: motion capture (‚Äúmocap‚Äù) and video sequences. For mocap,
conditioning on a mocap sequence so far, we learn a distribution over mocap feature vectors in the subsequent frame.
At test time, by supplying mocap samples as input back to
the model, long sequences are synthesized. For video, conditioning on a person bounding box sequence, we predict
the body joint locations in the current frame or, for the task
of body pose forecasting, at a speciÔ¨Åc point in the future.
In the mocap case, the input and output domains coincide
(3D body joint angles). In the video case, the input and
output domains differ (raw video pixels versus body joint
locations).
RNNs are network models that process sequential data
using recurrent connections between their neural activations
at consecutive time steps. They have been successfully applied in the language domain for text and handwriting generation , image captioning , action recognition . Ranzato et al. applies RNNs for visual prediction by quantizing the visual signal into a vocabulary of
visual words, and predicts a distribution over those words
in the next frame, given the visual word sequence observed
at a particular pixel location.
We advocate a visual predictive model that is ‚ÄúLa-
 
grangian‚Äù in nature : it predicts future outcomes conditioning on an object tracklet rather than on a tube Ô¨Åxated at a
particular pixel location, as (the ‚ÄúEulerian‚Äù approach).
Such object-centric conditioning exploits more relevant visual history of the object for prediction. In contrast, a visual
tube Ô¨Åxated at a particular pixel location encounters dramatically different content under camera or object motion.
In the ERD, the encoder transforms the input data to a
representation where learning of dynamics is easy. The decoder transcribes the output of the recurrent layers to the
desired visual form. For mocap generation, the encoder and
decoder are multilayer fully connected networks. For video
pose labeling and prediction, the encoder is a Convolutional
Neural Network (CNN) initialized by a CNN per frame
body part detector and decoder is a fully connected network. ERDs simultaneously learn both the representation
most suitable for recognition or prediction (input to the recurrent layer), as well as its dynamics, represented in the
recurrent weights, by jointly training encoding, decoding
and recurrent subnetworks. We found such joint Ô¨Ånetuning
crucial for empirical performance.
We test ERDs in kinematic tracking and forecasting in
the H3.6M video pose dataset of Ionescu et al. . It is
currently the largest video pose dataset publicly available.
It contains a diverse range of activities performed by professional actors and recorded with a Vicon motion capture system. We show that ERDs effectively learn human dynamics
in video and motion capture. In motion generation, ERDs
synthesize mocap data across multiple activities and subjects. We demonstrate the importance of the nonlinear encoder and decoder in ERDs by comparing to previous multilayer LSTM models . We show that such models do not
produce realistic motion beyond very short horizons. For
video pose labeling, ERDs outperforms a per frame body
part CNN detector, particularly in the case of left-right confusions. For future pose forecasting, ERDs forecast joint
positions 400ms in the future, outperforming Ô¨Årst order motion modeling with optical Ô¨Çow constancy assumptions.
Our experiments show that the proposed ERD models
can simultaneously model multiple activity domains, implicitly detect the right activity scenario, and adapt their
output labels or predictions accordingly. The action transitioning is transparent, in contrast to previous switching
dynamical systems or switching HMMs for activity
2. Related work
Motion generation
Generation of naturalistic human motion using probabilistic models trained on motion capture
data has previous been addressed in the context of computer
graphics and machine learning. Prior work has tackled synthesis of stylized human motion using bilinear spatiotemporal basis models , Hidden Markov Models , linear
dynamical systems , and Gaussian process latent variable models , as well as multilinear variants thereof
 . Unlike methods based on Gaussian processes, we
use a parametric representation and a simple, scalable supervised training method that makes it practical to train on
large datasets.
Dynamical models based on Restricted Boltzmann Machines (RBMs) have been proposed for synthesis and in-
Ô¨Ålling of motion data .
While such approaches have the advantage of learning probabilistic models, this also results in a substantially more complex training
algorithm and, when multilayer models are used, requires
sampling for approximate inference. In contrast, our RNNbased models can be trained with a simple stochastic gradient descent method, and can be evaluated very efÔ¨Åciently at
test time with simple feedforward operations.
Video pose labeling and forecasting
Temporal context
has been exploited in kinematic tracking using dynamic
programming over multiple per frame body pose hypotheses , where unary potentials encore detectors‚Äô conÔ¨Ådence and pairwise potentials encode temporal smoothness.
Optical Ô¨Çow has been used in to adjust the temporal smoothness penalty across consecutive frames. Optical
Ô¨Çow can only estimate the motion of body joints that do
not move too fast and do not get occluded or dis-occluded.
Moreover, the temporal coupling is again pairwise, not long
range. ERDs keep track of body parts as they become occluded and disoccluded by aggregating information in time
across multiple frames, rather than the last frame.
Parametric temporal Ô¨Ålters such as Kalman Ô¨Åltering ,
HMMs or Gaussian processes for activity speciÔ¨Åc dynamics generally use simple, linear dynamics models for prediction. Such simple dynamics are only valid
within very short temporal horizons, making it difÔ¨Åcult to
incorporate long range temporal information.
dynamic systems or HMMs detect activity transitioning explicitly. In contrast, in ERD, action transitioning is transparent to the engineer, and also more effective.
Moreover, HMM capacity increases linearly with increasing numbers of hidden states, but its parameter count increases quadratically. This makes it difÔ¨Åcult to scale such
models to large and diverse datasets. ERDs scale better than
previous parametric methods in capturing human dynamics.
RNNs use distributed representations: each world ‚Äústate‚Äù is
represented with the ensemble of hidden activations in the
recurrent layer, rather than a single one. Thus, adding a neural unit quadratically increases the number parameters yet
doubles the representation power - assuming binary units.
Standard temporal smoothers or Ô¨Ålters are disconnected
from the pose detector and operate on its output, such as
the space of body joint locations. This representation discards context information that may be present in the original
Encoder-¬≠‚ÄêRecurrent-¬≠‚Äê
forecas7ng
ùê∏ùëõùëêùëúùëëùëíùëüùë°‚àí1
ùëÖùëíùëêùë¢ùëüùëüùëíùëõùë°ùë°‚àí1
ùê∑ùëíùëêùëúùëëùëíùëüùë°‚àí1
ùëÖùëíùëêùë¢ùëüùëüùëíùëõùë°ùë°
Figure 1. ERDs for human dynamics in video and motion capture. Given a mocap sequence till time t, the ERD for mocap generation
predicts the mocap vector at time instance t + 1. Given a person tracklet till time t, ERD for video forecasting predicts body joint heat
maps of the next frame t + 1. ERD for video labeling predicts heat maps of the current frame instead.
video. In contrast, ERDs learn the representation suitable
for temporal reasoning and can take advantage of visual appearance and context.
3. ERDs for recurrent kinematic tracking and
forecasting
Figure 1 illustrates ERD models for recurrent kinematic
tracking and forecasting. At each time step t, vector xt of
a sequence x = (x1, ¬∑ ¬∑ ¬∑ , xT ) passes through the encoder,
the recurrent layers, and the decoder network, producing the
output yt. In general, we are interested in estimating some
function f(x) of the input x at the current time step, or at
some time in the future. For example, in the case of motion
capture, we are interested in estimating the mocap vector
at the next frame. Since both the input and output consists
of mocap vectors, f is the identity transformation, and the
desired output at step t is f(xt+1). In case of video pose
labeling and forecasting, f(x) denotes body joint locations
corresponding to the image in the current bounding box x.
At step t, we are interested in estimating either f(xt) in
the case of labeling, or f(xt+H) in the case of forecasting,
where H is the forecast horizon.
The units in each recurrent layer implement the Long
Short Term Memory functions , where writing, resetting, and reading a value from each recurrent hidden unit
is explicitly controlled via gating units, as described by
Graves . Although LSTMs have four times more parameters than regular RNNs, they facilitate long term storage of
task-relevant data. In Computer Vision, LSTMs have been
used so far for image captioning and action classiÔ¨Åcation in videos .
ERD architecture extends prior work on LSTMs by augmenting the model with encoder and decoder networks.
Omitting the encoder and decoder networks and instead using linear mappings between the input, recurrent state, and
output caused underÔ¨Åtting on all three of our tasks. This
can be explained by the complexity of the mocap and video
input in comparison to the words or pen stroke 2D locations
considered in prior work . For example, word embeddings were not crucial for RNNs to do well in text generation or machine translation, and the standard one hot encoding vocabulary representation also showed excellent results
3.1. Generating Motion Capture
Our goal is to predict the mocap vector in the next frame,
given a mocap sequence so far. Since the output yt has the
same format as the input xt+1, if we can predict xt+1, we
can ‚Äúplay‚Äù the motion forward in time to generate a novel
mocap sequence by feeding the output at the preceding time
step as the input to the current one.
Each mocap vector consists of a set of 3D body joint angles in a kinematic tree representation. We represent the
orientation of each joint by an exponential map in the coordinate frame of its parent, corresponding to 3 degrees of
freedom per joint. The global position of the body in the
x-y plane and the global orientation about the vertical z axis
are predicted relative to the previous frame, since each clip
has an arbitrary global position. This is similar to the approach taken in previous work . We standardize our
input by mean subtraction and division by the standard deviation along each dimension.
We consider both deterministic and probabilistic predictions. In the deterministic case, the decoder‚Äôs output yt is
a single mocap vector. In this case, we train our model by
minimizing the Euclidean loss between target and predicted
body joint angles. In the probabilistic case, yt parametrizes
a Gaussian Mixture Model (GMM) over mocap vectors in
the next frame. We then minimize the GMM negative loglikelihood during training:
logPr(xt+1|yt)
We use Ô¨Åve mixture components and diagonal covariances.
The variances are outputs of exponential layers to ensure
positivity, and the mixture component probabilities are outputs of a softmax layer, similar to .
During training,
we pad the variances in each iteration by a Ô¨Åxed amount
to ensure they do not collapse around the mixture means.
Weights are initialized randomly. We experimented with
initializing the encoder and decoder networks of the mocap
ERD from the (Ô¨Årst two layers of) encoder and (last two
layers of) decoder of a) a ten layer autoencoder trained for
dimensionality reduction of mocap vectors , b) a ‚Äúskip‚Äù
autoencoder trained to reconstruct the mocap vector in few
frames in the future given the current one. In both cases,
we did not observe improvement over random weight initialization. We train our ERD model with stochastic gradient descent and backpropagation through time with
momentum and gradient clipping at 25, using the publicly
available Caffe package and the LSTM layer implementation from .
We regularize our mocap ERD with denoising: we provide mocap vectors corrupted with zero mean Gaussian
noise and have the model predict the correct, uncorrupted mocap vector in the next frame. We found it valuable to progressively increase the noise standard deviation,
learning from non-corrupted examples Ô¨Årst.
This corresponds to a type of curriculum learning. At test time, we
run the model forward by feeding the predictions as input
to the model in the following time step. Without denoising, this kind of forward unrolling suffers from accumulation of small prediction mistakes at each frame, and the
model quickly falls into unnatural regions of the state space.
Denoising ensures that corrupted mocap data are shown to
the network during training so that it learns to correct small
amounts of drift and stay close to the manifold of natural
3.2. Labeling and forecasting video pose
In the previous section, we described how the ERD
model can be used to synthesize naturalistic human motion by training on motion capture datasets. In this section,
we extend this model to identify human poses directly from
pixels in a video. We consider a pose labeling task and a
pose forecasting task. In the labeling task, given a bounding
box sequence depicting a person, we want to estimate body
joint locations for the current frame, given the sequence so
far. In the forecasting task, we want to estimate body joint
locations for a speciÔ¨Åc future time instance instead.
We represent K body joint locations as a set of K N √óN
heat maps over the person‚Äôs bounding box, that represent
likelihood for each joint to appear in each of the N 2 grid
locations, similar to . Predicting heat maps naturally incorporates uncertainty over body joint locations, as opposed
to predicting body joint pixel coordinates.
Figure 1right illustrates our ERD architecture for video
pose labeling and forecasting.
The encoder is a Ô¨Åve
layer convolutional network with architecture similar to
Krizhevsky et al. . Our decoder is a two layer network
with fully connected layers interleaved with rectiÔ¨Åed linear
unit layers. The output of the decoder is body joint heat
maps over the person bounding box in the current frame for
the labeling task, or body joint heat maps at a speciÔ¨Åed future time instance for the forecasting task.
We train both our pose labeler and forecaster ERDs under a Euclidean loss between estimated and target heat
maps. We initialize the weights of the encoder from a six
layer convolutional network trained for per frame body part
detection, in which the Ô¨Ånal CONV6 layer corresponds to
the body joint heat maps.
Empirically, we found it valuable to input to the recurrent layer not the per frame estimated heat maps (CONV6),
but rather the preceding CONV5 feature maps. These feature maps capture rich appearance information, rather than
merely body joint likelihood. Rich appearance information
assists the network in discriminating between different actions and pose dynamics without explicit switching across
activity domains, as previous switching dynamical linear
systems or HMMs .
We use two networks on different image scales for our
per frame pose detector and ERD: one where the output
layer resolution is 6√ó6 and one that works on double image
size and has output resolution of 12√ó12. The heat maps of
the coarser scale are upsampled and added to the Ô¨Åner scale
to provide the Ô¨Ånal combined 12√ó12 heat maps. Multiple
scales have shown to be beneÔ¨Åcial for static pose estimation
in .
4. Experiments
We test our method on the H3.6M dataset of Ionescu
et al. , which is currently the largest video pose dataset.
It consists of 15 activity scenarios, performed by seven
different professional actors and recorded from four static
For each activity scenario, subject, and camera viewpoint, there are two video sequences, each between 3000 and 5000 frames. Each activity scenario features rich gestures, pose variations and interesting subactions performed by the actors. For example, the walking activity includes holding hands, carrying a heavy load, putting
hands in the pockets, looking around etc. The activities are
recorded using a Vicon motion capture system that tracks
markers on actors‚Äô body joints and provides high quality
3D body joint locations. 2D body joints locations are obtained by projecting the 3D positions onto the image plane
using the known camera calibration and viewpoint. For all
our experiments, we treat subject 5 as the test subject and
all others as our training subjects.
Motion capture generation
We compare our ERD mocap generator with a) an LSTM recurrent neural network
with linear encoder and decoders that has 3 LSTM layers
of 1000 units each (architecture found through experimentation to work well), b) Conditional Restricted Boltzmann
Machines (CRBMs) of Taylor et al. , c) Gaussian Process Dynamic Model (GPDM) of Wang et al. , and d)
a nearest neighbor N-gram model (NGRAM). For CRBM
and GPDM, we used the code made publicly available by
the authors. For the nearest neighbor N-gram model, we
used a frame window of length N = 6 and Euclidean distance on 3D angles between the conditioning preÔ¨Åx and our
training set, and copy past the subsequent frames of the best
matching training subsequence. We applied denoising during training to regularize both the ERD and the LSTM-3LR.
For all models, the mocap frame sequences were subsampled by two. ERD, LSTM-3LR and CRBM are trained on
multiple activity scenarios (Walking, Eating and Smoking).
GPDM is trained on Walking activity only, because its cubic complexity prohibits its training on a large number of
sequences. Our comparison focuses on motion forecasting
(prediction) and synthesis, conditioning on motion preÔ¨Åxes
of our test subject. Mocap in-Ô¨Ålling and denoising are nontrivial with our current model but developing this functionality is an interesting avenue for future work.
We show qualitative motion synthesis results in Figure
2 and quantitative motion prediction errors in Table 1. In
Figure 2, the conditioning motion preÔ¨Åx from our test subject is shown in green and the generated motion is shown in
blue. In Table 1, we show Euclidean norm between the synthesized motion and ground-truth motion for our test subject
for different temporal horizons past the conditioning motion
preÔ¨Åx, the largest being 560msecs, averaged across 8 different preÔ¨Åxes. The stochasticity of human motion prevents
a metric evaluation for longer temporal horizons, thus all
comparisons in previous literature are qualitative. LSTM-
3LR dominates the short-term motion generation, yet soon
converges to the mean pose, as shown in Figure 2. CRBM
also provides smooth short term motion completions, yet
quickly drifts to unrealistic motions. ERD provides slightly
Ground-¬≠‚Äêtruth
LSTM-¬≠‚Äê3LR
Figure 2. Motion synthesis. LSTM-3LR and CRBMs provide smooth short-term motion completions (for up to 600msecs),
mimicking well novel styles of motion, (e.g., here, walking
with upright back).
However, ERD generates realistic motion for longer periods of time while LSTM-3LR soon converges to the mean pose and CRBM diverges to implausible motion.
NGRAM has a non-smooth transition from conditioning
to generation.
Per frame mocap vectors predicted by GPDM
 look plausible, but their temporal evolution is far from
realistic.
You can watch the corresponding video results at
 
less smooth completions, yet can generate realistic motion
for long periods of time. For ERD, the smallest error was always produced by the most probable GMM sample, which
was similar to the output of an ERD trained under a standard
Euclidean loss. N-gram model exhibits a sudden change
of style during transitioning from the conditioning preÔ¨Åx to
the Ô¨Årst generated frame, and cannot generate anything outside of the training set. Due to low-dimensional embedding,
GPDM cannot adequately handle the breadth of styles in the
training data, and produces unrealistic temporal evolution.
The quantitative and qualitative motion generation results of ERD and LSTM-3LR suggest an interesting tradeoff between smoothness of motion completion (interesting
motion extrapolations) and stable long-term motion generation. Generating short-term motion that mimics the style of
the test subject is possible with LSTM-3LR, yet, since the
network has not encountered similar examples during training, it is unable to correctly generate motion for longer periods of time. In contrast, ERD gears the generated motion
towards similarly moving training examples. ERD though
cannot really extrapolate, but rather interpolate among the
training subjects. It does provides much smoother motion
completions than the N-gram baseline. Both setups are interesting and useful in different applications, and in between
architectures potentially lie somewhere in between the two
ends of that spectrum. Finally, it is surprising that LSTM-
3LR outperforms CRBMs given its simplicity during training and testing, not requiring inference over latent variables.
CRBM 0.68
GPDM 1.76
Table 1. Motion prediction error during 80, 160, 240, 320, 400,
480 and 560 msecs past the conditioning preÔ¨Åx for our test subject
during Walking activity. Quantitative evaluation for longer temporal horizons is not possible due to stochasticity of human motion.
Figure 3. Pretraining. Initialization of the CNN encoder with the
weights of a body pose detector leads to a much better solution
than random weight initialization. For motion generation, we did
not observe this performance gap between pertaining and random
initialization, potentially due to much shallower encoder and low
dimensionality of the mocap data.
Video pose labeling
Given a person bounding box sequence, we want to label 2D pixel locations of the person‚Äôs
body joint locations. Both occluded and non-occluded body
joints are required to be detected correctly: the occluder‚Äôs
appearance often times contains useful information regarding the location of an occluded body joint . Further, for
transcribing 2D to 3D pose, all body joints are required .
We compare our ERD video labeler against two baselines: a per frame CNN pose detector (PF) used as the encoder part of our ERD model, and a dynamic programming
approach over multiple body pose hypotheses per frame
(VITERBI) similar in spirit to . For our VITERBI
baseline, we consider for each body joint in each frame
all possible grid locations and encode temporal smoothness
as the negative exponential of the Euclidean distance between the locations of the same body joint across consecutive frames. The intuition behind VITERBI is that temporal
smoothness will help rule out isolated, bad pose estimates,
by promoting ones that have lower per frame scores, yet are
more temporally coherent.
We evaluate our model and baselines by recording
the highest scoring pixel location for each frame and
body joint.
We compute the percentage of detected
joints within a tolerance radius of a circle centered at
the ground-truth body joint locations, for various tolerance thresholds.
We normalize the tolerance radii with
the distance between left hip and right shoulder.
is the standard evaluation metric for static image pose
labeling . We show pose labeling performance curves
in Figure 4.
For a video comparison between ERD
and the per frame CNN detector, please see the video at
 
discriminatively learning to integrate temporal information for body joint tracking, instead of employing generic
motion smoothness priors. ERD‚Äôs performance boost stems
from correcting left and right confusions of the per frame
part detector, as Figure 5 qualitatively illustrates. Left and
right confusion is a major challenge for per frame part detectors, to the extent that certain works measure their performance in image centric coordinates, rather than object centric . Last, VITERBI is marginally better than the
per frame CNN detector. While motion coherence proved
important when combined with shallow and inaccurate per
frame body pose detectors , it does not improve much
upon stronger multilayer CNNs.
Figure 3 compares ERD training and test losses during
Ô¨Ånetuning the encoder from (the Ô¨Årst Ô¨Åve layers of) our per
frame CNN pose detector, versus training the encoder from
scratch (random weights). CNN encoder‚Äôs initialization is
crucial to reach a good solution.
We further compare our video labeler in a subset of 200
video sequences of around 50 frames each from the Flic-
Motion dataset of that we annotated densely in time
with person bounding boxes. We used 170 video sequences
for training and 30 for testing. We show performance curves
for the upper body joints in Figure 7. VITERBI has simi-
Figure 4. Video pose labeling in H3.6M. Quantitative comparison of a per frame CNN body part detector of (PF), dynamic programming for temporal coherence of the body pose sequence in the spirit of (VITERBI), and ERD video pose labeler. ERD outperforms
the per frame detector as well as the dynamic programming baseline. Oracle curve shows the performance upper-bound imposed by our
grid resolution of 12x12.
Figure 5. Left-right disambiguation. ERD corrects left-right confusions of the per frame CNN detector by aggregating appearance
features (CONV5) across long temporal horizons.
lar performance as in H3.6M, marginally exceeding the per
frame CNN detector. However ERD does much worse since
the training set is too small to learn effectively. Finetuning from the model learnt from H3.6M did not help since
H3.6M concerns full body motion while FlicMotion captures upper body only. We did not change the architecture
in comparison to the ERD used in H3.6M. It is probable that
a smaller recurrent layer and decoder would improve performance preventing overÔ¨Åtting. Large training sets such
as in H3.6M allow high capacity discriminative temporal
smoothers as our video labelled ERD to outperform generic
motion smoothness priors for human dynamics.
Figure 7. Video pose labeling in FlicMotion. ERD does not succeed in learning effectively from the small set of 170 videos of
about 50 frames each.
Large training sets, such as those provided in H3.6M, are necessary for ERD video labeler to outperform generic motion smoothness priors.
Video pose forecasting
We predict 2D body joint locations 400ms ahead of the current frame. Figure 6 shows
pose forecasting performance curves for our ERD model,
a model that assumes zero object and camera motion
(NoMotion-NM), and a model that assumes constant optical
Ô¨Çow within the prediction horizon (OF). ERD carries out
more accurate predictions than the zero order and Ô¨Årst order motion baselines, as also shown qualitatively in Figure
8. Optical Ô¨Çow based motion models cannot make reasonable predictions for occluded body joints, since their frame
to frame displacements are not observed. Further, standard
motion models suffer from separation of the observation
model (part detector) and temporal aggregation, which ERD
combines into a single network.
Discussion
Currently, the mocap ERD performs better on
periodic activities (walking, smoking etc) in comparison to
non periodic ones (sitting etc.). Interesting directions for future research is predicting 3D angle differences from frame
to frame as opposed to angles directly. Such transformation
prediction may generalize better to new subjects, focusing
more on motion rather than appearance of the skeleton. We
are also investigating using large frame volumes as input
to our video prediction ERDs with spatio-temporal convolutions in CONV1 as opposed to a single frame LSTM, in
order to exploit short temporal horizon more effectively.
Figure 6. Video pose forecasting. Quantitative comparison between the ERD model, a zero motion (NM), and constant velocity (OF)
models. ERD outperforms the baselines for the lower body limbs, which are frequently occluded and thus their per frame motion is not
frequently observed using optical Ô¨Çow.
right ankle
right shoulder
right ankle
left shoulder
Figure 8. Video pose forecasting 400ms in the future. Left: the
prediction of the body part detector 400ms before superimosed on
the frame to predict pose for (zero motion model). MiddleLeft:
Predictions of the ERD. The body joints have been moved towards their correct location. MiddleRight: The current and 400ms
ahead frame superimposed. Right: Ground-truth body joint location (discretized in a N √ó N heat map grid). In all cases we show
the highest scoring heat map grid location.
5. Conclusion
We have presented end-to-end discriminatively trained
encoder-recurrent-decoder models for modeling human
kinematics in videos and motion capture. ERDs learn the
representation for recurrent prediction or labeling, as well
as its dynamics, by jointly training encoder recurrent and
decoder networks. Such expressive models of human dynamics come at a cost of increased need for training examples. In future work, we plan to explore semi-supervised
models in this direction, as well learning human dynamics
in multi-person interaction scenarios.
Acknowledgements
We would like to thank Jeff Donahue and Philipp Kr¬®ahenb¬®uhl for useful discussions. We
gratefully acknowledge NVIDIA corporation for the donation of K40 GPUs for this research. This research was
funded by ONR MURI N000014-10-1-0933.