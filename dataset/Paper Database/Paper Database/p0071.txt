IEICE TRANS. INF. & SYST., VOL.E94–D, NO.10 OCTOBER 2011
Special Section on Information-Based Induction Sciences and Machine Learning
Adaptive Online Prediction Using Weighted Windows
Shin-ichi YOSHIDA†, Nonmember, Kohei HATANO††a), Eiji TAKIMOTO††, Members,
and Masayuki TAKEDA††, Nonmember
We propose online prediction algorithms for data streams
whose characteristics might change over time. Our algorithms are applications of online learning with experts. In particular, our algorithms combine
base predictors over sliding windows with diﬀerent length as experts. As a
result, our algorithms are guaranteed to be competitive with the base predictor with the best ﬁxed-length sliding window in hindsight.
key words: machine learning, data stream, online learning, sliding window
Introduction
Data stream arises in many applications. For example, developments of distributed censor devices enable us to collect
data which are generated constantly over time. Also, there
are more and more huge data available and such huge data
can be viewed as data stream as well if we want to deal
with them by “one-pass” scan. Researches on data stream
have become popular in various areas in computer science
such as databases, algorithms , data mining and machine
learning .
There are two notable properties of data stream. The
ﬁrst property is that nature of the data stream might change
over time. The underlying distribution which generates the
data might change gradually over time or change suddenly
at some trial (concept drifts). So, prediction algorithms for
data stream need to adapt concept drifts. The second property is that whole the data stream is too huge to keep since
the new data comes endlessly. Therefore, prediction algorithms also need to choose some partial data.
A natural approach to deal with data stream is to use a
sliding window. The sliding window keeps only recent data.
As a new instance comes in, then the oldest instance in the
window is discarded from the window and the new one is
added to the window. Then prediction algorithms use only
the data of the sliding window to make predictions on future
For time-changing data streams, it is reasonable to assume that recent data is more informative than older data.
So, sliding window approaches seem to work well for prediction tasks on data streams. However, it is not trivial to determine the size of sliding window in advance. If the size is
Manuscript received January 7, 2011.
Manuscript revised May 4, 2011.
†The author is with NTT West, Osaka-shi, 540–8511 Japan.
††The authors are with Department of Informatics, Kyushu University, Fukuoka-shi, 819–0395 Japan.
a) E-mail: 
DOI: 10.1587/transinf.E94.D.1917
too large, accuracy of prediction might become worse when
the nature of the data stream changes, since older data affects the prediction. On the other hand, if the size is too
small, accuracy of prediction might become worse as well
when the data is rather stationary.
There are some researches to make the size of the
sliding window adaptive – . These proposed methods
heavily depend on the choice of parameters, e.g., a threshold to determine when to discard data in the window. For
example, given a single temporal outlier, ADWIN discards the all data in the window even if the data stream is
rather stationary, which might get the accuracy worse.
In this paper, we take an alternative approach. Instead
of choosing a ﬁxed-sized window or changing the size of the
window adaptively, we combine the predictions using multiple windows with diﬀerent sizes. More precisely, we employ the approach of online learning with experts – .
We consider M “sub-windows” which contains the k newest
elements of the window (k = 1, . . . , M).
We assume a ﬁxed predictor, called the base predictor,
which works with a sliding window so that it makes predictions using only the data in the window. Since we have
M sliding windows of size 1 through M, we get M predictors: the k-th predictor is the base predictor running with
the sliding window of size k. Using these M predictors as
experts and applying Weighted Average Algorithm by Kivinen and Warmuth , we obtain an online prediction algorithm, called the Weighted Window (WW, for short). The
WW is guaranteed to perform almost as well as the best expert, i.e., the base predictor with the best ﬁxed size window.
More precisely, we show that the WW has O(ln M) regret,
where the regret of a prediction algorithm is deﬁned as the
cumulative loss of the algorithm minus that of the predictor
with the best ﬁxed size window.
Furthermore, we apply the method of Hazan and Seshadhri to make the prediction algorithm more adaptive.
In particular, by combining multiple copies of WWs over
diﬀerent intervals, we obtain the second algorithm called the
Weighted Windows with follow the leading Hisotry (WWH,
for short). The WWH is guaranteed to perform almost as
well as the best experts for all intervals in the data stream.
More precisely, we show that for any interval I in the data
stream, the regret of WWH measured for I is bounded above
by O(ln M ln T + ln2 T), where T is the length of the data
Note that our contribution is not to develop new tech-
Copyright c⃝2011 The Institute of Electronics, Information and Communication Engineers
IEICE TRANS. INF. & SYST., VOL.E94–D, NO.10 OCTOBER 2011
niques for online learning with experts, but to apply online
learning with experts framework to sequence prediction using sliding windows in order to make it robust.
In our experiments over artiﬁcial and read time-series
data, WW and WWH outperform other previous methods
and compete fairly with predictions with the best ﬁxed window.
Preliminaries
For a ﬁxed integer N, let X be the domain of interest. A
member x in X is called an instance. For integers r and
s(r ≤s), we denote by [r, s] the set consisting of sequential
integers r, . . . , s. In particular, we write [s] for short if r = 1.
Online Prediction with Experts
We consider the following protocol of online prediction. At
each trial t = 1, . . . , T,
1. the adversary gives an instance xt to the learner,
2. the learner guesses a prediction ˆyt ∈ for xt,
3. the adversary gives the true value yt ∈ to the
learner, and
4. the learner incurs loss ℓ(yt, ˆyt).
Here the function ℓ: × →R is called the loss
function. In particular, in the setting of online learning with
experts ( , ), the learner can use predictions of experts.
More precisely, the learner is given M experts in advance.
At each trial t, each expert is given an instance xt and returns
its prediction ˆyt,i ∈ .
The goal of the learner is to predict as well as the best
expert in hindsight. More detailed goals are the following:
ℓ(yt, ˆyt) −min
ℓ(yt, ˆyt,i).
• Adaptive regret 
I=[r,s]⊂[T]
ℓ(yt, ˆyt) −min
ℓ(yt, ˆyt,i)
A loss function ℓis called α-exp concave if the function
e−αℓ(ˆy,y) is concave w.r.t. ˆy. It is known that some natural loss
functions such as square loss, log loss, relative entropy and
Hellinger loss are α-exp concave for some α (see, e.g., ).
Let us discuss the diﬀerence between regret and adaptive regret. Regret measures the diﬀerence between cumulative losses of the algorithm and the best expert for all T trials. However, low regret does not necessarily imply a good
performance over time-changing data. This is because, for
data changing its tendency over time, the best expert for all
T trials might not adapt changes in data and predicts badly
for some intervals. On the other hand, If the adaptive regret
is bounded, then the regret w.r.t. the best expert for any interval is bounded as well. So, minimizing the adaptive regret
is more challenging goal, especially for time-changing data
Sliding Window
A sliding window is popular in the task of prediction of data
streams which might change over time. A sliding window of
size k keeps k newest instances. More formally, the sliding
window W of size k at trial t is a sequence of instances of
j=1{(x j, yj)}
if t −1 ≤k
j=t−k{(x j, yj)}
if t −1 > k .
We assume a base prediction algorithm associated with a
sliding window. The algorithm uses the examples in the
sliding window to make predictions.
In general, behaviors of the prediction algorithm using
a sliding window depend on the size of the window. When
the size of the sliding window is large, predictions using the
window tends to be insensitive to outliers. So, the predictions are robust with respect to temporal noises. However,
if the tendency of the data stream changes, the predictions
tend to become worse, since older data in the sliding window
aﬀects so that the prediction algorithm to adapt the change
more slowly. On the other hand, when the size of the sliding window is small, the predictions are more sensitive to
changes in the data stream. Thus the prediction algorithm
can adapt the change quickly. But, its disadvantage is that
the predictions become sensitive to temporal noises as well.
Therefore, in order to predict adaptively w.r.t. data streams,
we need to determine the size of the sliding window appropriately.
Given a base predictor, our goal is to predict as well as
the base predictor using the best ﬁxed-sized sliding window. Speciﬁcally, we aim to construct online prediction algorithms whose regret or adaptive regret w.r.t. the base predictor with the best ﬁxed sliding window.
Algorithms
In this section, we propose two algorithms, which are modiﬁcations of existing algorithms having regret and adaptive
regret bounds, respectively.
Weighted Window
The ﬁrst algorithm, which we call Weighted Window (WW),
the special case of Weighted Average Algorithm with
base predictors with sliding windows as experts. More precisely, WW has a sliding window of size M, and the sliding
window induces M sub-windows. Each sub-window, which
is denoted as W[i] (i = 1, . . . , M), has the at most i newest
We regard the base predictor with each subwindow W[i] as an expert. That is, each expert predicts using the base predictor and the data in the sub-window W[i].
YOSHIDA et al.: ADAPTIVE ONLINE PREDICTION USING WEIGHTED WINDOWS
Algorithm 1 WW(Weighted Window)
1. w1 = ( 1
M , . . . , 1
2. For t = 1, . . . , T
a. The sliding window W contains at most M newest examples
before trial t.
j=1{(x j, y j)}
if t −1 ≤M,
j=t−M{(x j, y j)}
if t −1 > M.
Each sub-window W[i] contains at most i newest examples (i =
1, . . . , M).
b. Receive an instance xt.
c. Each expert Ei predicts ˆyt,i using the sub-window W[i] (1 ≤
d. Predict ˆyt =
i=1 wt,iˆyt,i.
e. Receive the true outcome yt.
f. Update the weight vector.
e−αℓ(yt,ˆyt,i)
i=1 e−αℓ(yt,ˆyt,i) .
Finally, Weighted Average Algorithm combines the experts’
predictions by computing the weighted average. The details
of WW is given in Algorithm 1.
An advantage of WW is that it predicts adaptively w.r.t.
changes of tendency in the data stream. For example, when
the tendency of the data changes drastically, experts corresponding to small sub-windows would have larger weights
and older data no longer aﬀects the predictions of WW.
Similarly, if the tendency of the data does not change, experts corresponding to large sub-windows would have larger
weights and predictions of WW would become resistant to
temporal outliers in the data stream.
The regret bound of WW is directly follows from that
of Weighted Average Algorithm .
Theorem 1 (Kivinen & Warmuth ). Suppose that the
loss function ℓis α-exp concave. Then the regret of WW is
at most (1/α) ln M.
By Theorem 1, WW is guaranteed to perform almost as
well as predictions with best ﬁxed window of size less than
Weighted Window with Follow the Leading History
The second algorithm, Weighted Window with follow the
leading History (WWH), is a modiﬁcation of Follow the
Leading History (FLH) with many copies of WWs as
experts. Speciﬁcally, WWH diﬀers from FLH in that experts
of FLH use all the past instances given to them while those
of WWH use instances in their sliding windows only. This
change makes, as we will show later, practical improvement
in changing environments.
At each trial i, WWH generates a copy of WW denoted
as WWi as an expert. Each WWi has a lifetime lifetimei
and it is only active in lifetimei trials.
Each WWi runs
WW through the data given in the lifetime. Each WW has
a sliding window of size M and M sub-windows as subexperts. At each trial, WWH combines the prediction of
experts which are active at the trial.
More precisely, an expert WWi, which is generated at
trial i, is active at trial t if i + liftimei ≥t. The lifetimei
of expert WWi is given as follows: If i is represented as
i = r2k, where r is some odd number and k is an integer,
we ﬁx lifetimei = 2k+2 + 1. Note that r and k are unique
for each i. For example, if i is odd, then k = 0 and r = i.
Similarly, if i is even, there also exist unique k such that
k ≥1 and odd number r satisfying i = r2k. Let At be the
set of indices of active experts at trial t. Then the following
lemma holds . Note that the lemma holds not only for
FLH but also for WWH.
Lemma 1 (Hazan & Seshadhri ).
1. For any s ≤t, [s, (s + t)/2] ∩At  ∅.
2. For any t, |At| = O(log T).
3. For any t, At+1 \ At = {t + 1}.
The description of WWH is given in Algorithm 2. At
each trial WWH combines the predictions of active experts
by computing the weighted average. Then WWH generates
a new expert. Finally, WWH removes the experts whose
lifetimes are zero, and normalize the weights of active experts.
We show a regret bound of WWH. First, we use the lemma
for FLH .
Lemma 2 ( ). Suppose that for an interval I = [r, s]
WWr is active. Then, regret of WWH w.r.t. WWr for the
interval I is 2
α(ln r + ln |I|).
By Lemma 2 and Theorem 1, we have the following
Lemma 3. Suppose that, during the interval I = [r, s], WWr
is active. Then, regret of WWH w.r.t. any sub-window for the
interval I is at most 2
α(ln r + ln |I| + ln M).
Then we analyze the regret of WWH w.r.t. any interval
I = [r, s] and any sub-window.
Lemma 4. For any interval I = [r, s], the regret of WWH
w.r.t. I is O( 1
α(ln M + ln s) ln |I|).
Proof. By Lemma 1, for the trial s and the interval I = [r, s],
there exists i ∈As such that (i) i ∈[r, r+s
2 ], and (ii) the expert
WWi is generated at trial i and is active at trial s. Therefore,
by Lemma 3, the regret of WWH for any sub-window and
the interval I is at most 2
α(ln i + ln |I| + ln M).
Similarly, for the interval [r, i], there exists i′ such that
i′ ∈[r, r+i
2 ], we can evaluate the regret of WWH for any
sub-window and the interval [i′, i]. Note that, by this argument, the interval for which the regret is evaluated becomes
at most a half of the original interval. So, there are at most
log2 |I| intervals to consider. Thus the regret of WWH for
any sub-window and the interval I = [r, s] is at most
IEICE TRANS. INF. & SYST., VOL.E94–D, NO.10 OCTOBER 2011
Algorithm 2 WWH (Weighted Window with follow the
leading History)
1. Let A1={1} and w1,1 = 1. Generate the expert WW1 having WW as
its prediction algorithm.
2. For t = 1, . . . , T
a. Receive an instance xt.
b. For each i ∈At, the expert WWi predicts ˆyt,i
c. Predict ˆyt =
i∈At wt,iˆyt,i.
d. Receive the true outcome yt.
e. Update:
wt,ie−αℓ(yt,ˆyt,i)
j∈At wt, je−αℓ(yt,ˆyt,j)
f. Add the new expert WWt:
if i = t + 1,
t+1 ) ˆwt+1,i
if i  t + 1.
g. Let At+1 be the set of indices of active experts at trial t +1. For
each i ∈At+1, let
j∈At+1 ¯wt+1, j
α(ln s + ln |I| + ln M) · log2 |I|
α(ln M + ln s) ln |I|
Finally, we prove the adaptive regret bound of WWH.
Theorem 2. The adaptive regret of WWH w.r.t. the best
ﬁxed-sized window is O(ln M ln T + ln2 T).
Proof. By Lemma 4, the regret of WWH for any interval I =
[r, s] and the best sub-window is O
α(ln M + ln s) ln |I|
Since, s, |I| ≤T, we complete the proof.
Experiments
We evaluate our proposed algorithms and other previous
methods over synthetic and real time series data.
The data we deal with has the following form: S =
{(x1, y1), (x2, y2), . . . , (xt, yt)}(1 ≤t ≤T), where each xt = t,
and yt ∈ (1 ≤t ≤T). At each trial t, each prediction algorithm is supposed to predict ˆyt ∈ , given
xt = t. The loss function we consider here is square loss,
i.e., ℓ(y, ˆy) = (y−ˆy)2. It can be shown that square loss ℓ(y, ˆy)
is α-exp concave for α ≤1/2 when y, ˆy ∈ . Since
larger α implies smaller regret (as stated in Theorem 1), we
ﬁx α = 1/2 when we use WW in our experiments.
We assume that the base prediction algorithm associated with each sub-window performs least square regression.
Then, given M examples (x1, y1), . . . , (xM, yM) ⊂
(R × )M, the prediction is ˆy = ax + b, where
i=1(xi −¯x)(yi −¯y)
i=1(xi −¯x)2
b = ¯y −a¯x.
Here, ¯x, ¯y are denoted as the averages of xi and yi (i =
1, . . . , M), respectively. For WW, a naive implementation
takes O(M2) time to make a prediction. But, it is possible to
reduce the computation down to O(M).
The algorithms we evaluate are WW, WWH, FLH ,
ADWIN (ADaptive WINdowing) , KAARch (Kernel Aggregating Algorithm for Regression with Changing dependencies) , and the best ﬁxed-sized sub-window. Note
that all algorithms use least square regression as the base
prediction algorithm.
For ADWIN, we set δ = 0.9. For KAARCh, we set
Y2c2T(T −1)/2s(T) as suggested in .
Experiments for Artiﬁcial Data
We use the following artiﬁcial data sets: Each data set consists of a sequence of 1000 examples.
Radical : a sequence where the values radically changes at
t = 500 (in Fig. 1).
Gradual : a sequence where the values gradually change at
t = 300, . . . , 700 (in Fig. 2).
Temporal : a sequence where an outlier appears after each
200 steps (in Fig. 3).
Random : a sequence where a trend changes at random
trials and the degree of the gradient changes as much
as randomly determined (in Fig. 4).
For each artiﬁcial data, we further add random noises at
each trial, where each random noise is generated i.i.d. from
N(0, 0.05).
The cumulative loss of each algorithm for each data is
shown in Figs. 1, 2, 3, 4, respectively. We set the size of the
window M = 10 and α = 1/2.
For all artiﬁcial data sets except from Gradual data,
WW and WWH perform better than other algorithms. Further, cumulative losses of WW, WWH and the best window are close. For Gradual data, ADWIN performs best
among all algorithms other than the best window.
Curiously, even though WWH has a stronger theoretical guarantee (i.e., the adaptive regret), its performance is slightly
worse than that of WW. Other algorithms sometimes perform well but sometimes not. In particular, FLH seems not
to adapt changes of the data well at later trials. ADWIN
shows a good performance for data with gradual changes
such as Gradual data, but behaves badly when temporal
noises appear in the data such as Temporal data. We omit
the plots of KAARCh since its performance is much worse
than others.
Experiments on Real Data
As a real data, we use Nikkei 225, a stock market index for
the Tokyo Stock Exchange. The data consists of 6311 daily
average stocks (the closing price) of 225 Japanese representative companies, ranging from 1984/1/4 to 2009/8/27 (in
YOSHIDA et al.: ADAPTIVE ONLINE PREDICTION USING WEIGHTED WINDOWS
Radical data (left) and the cumulative losses of the algorithms (right).
Gradual data (left) and the cumulative losses of the algorithms (right).
Temporal data (left) and the cumulative losses of the algorithms (right).
IEICE TRANS. INF. & SYST., VOL.E94–D, NO.10 OCTOBER 2011
Random data (left) and the cumulative losses of the algorithms (right).
Nikkei 225 data (left) and the cumulative losses of the algorithms (right).
Fig. 5). We set M = 50 for WW and WWH.
For Nikkei 225 data, again, WW and WWH perform
the best among other algorithms except from the best window (We omit the plot of KAARCh since the performance is
much worse). Similar to the results for artiﬁcial data, WWH
performs slightly worse than WW.
Conclusion
In this paper, we propose algorithms for predicting of data
streams, based on techniques of online learning with experts. The ﬁrst algorithm, WW, combines slide windows
of diﬀerent sizes, and we show it predicts almost as well as
the best sub-window. The second algorithm, WWH, further
combines WWs over diﬀerent intervals, having the adaptive
regret small.
Acknowledgements
We thank anonymous reviewers for providing many useful
comments and suggestions to improve initial manuscript.
This research was partially supported by MEXT Grand-in-
Aid for Young Scientists (B) 21700171.