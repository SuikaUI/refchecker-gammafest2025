Extracting and Composing Robust Features with
Denoising Autoencoders
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, Pierre-Antoine Manzagol
Dept. IRO, Universit´e de Montr´eal
C.P. 6128, Montreal, Qc, H3C 3J7, Canada
 
Technical Report 1316, February 2008
Previous work has shown that the diﬃculties in learning deep generative or discriminative models can be overcome by an initial unsupervised
learning step that maps inputs to useful intermediate representations. We
introduce and motivate a new training principle for unsupervised learning
of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can
be used to train autoencoders, and these denoising autoencoders can be
stacked to initialize deep architectures. The algorithm can be motivated
from a manifold learning and information theoretic perspective or from a
generative model perspective. Comparative experiments clearly show the
surprising advantage of corrupting the input of autoencoders on a pattern
classiﬁcation benchmark suite.
Introduction
Recent theoretical studies indicate that deep architectures may be needed to eﬃciently model complex distributions
and achieve better generalization performance on challenging recognition tasks.
The belief that additional levels of functional composition will yield increased
representational and modeling power is not new . However, in practice, learning in deep
architectures has proven to be diﬃcult. One needs only to ponder the diﬃcult problem of inference in deep directed graphical models, due to “explaining
away”. Also looking back at the history of multi-layer neural networks, their
diﬃcult optimization has long prevented
reaping the expected beneﬁts of going beyond one or two hidden layers. However this situation has recently changed with the successful approach of for training Deep Belief Networks and stacked autoencoders.
One key ingredient to this success appears to be the use of an unsupervised
training criterion to perform a layer-by-layer initialization: each layer is at ﬁrst
trained to produce a higher level (hidden) representation of the observed patterns, based on the representation it receives as input from the layer below, by
optimizing a local unsupervised criterion. Each level produces a representation
of the input pattern that is more abstract than the previous level’s, because it
is obtained by composing more operations. This initialization yields a starting
point, from which a global ﬁne-tuning of the model’s parameters is then performed using another training criterion appropriate for the task at hand. This
technique has been shown empirically to avoid getting stuck in the kind of poor
solutions one typically reaches with random initializations. While unsupervised
learning of a mapping that produces “good” intermediate representations of
the input pattern seems to be key, little is understood regarding what constitutes “good” representations for initializing deep architectures, or what explicit
criteria may guide learning such representations. We know of only a few algorithms that seem to work well for this purpose: Restricted Boltzmann Machines
(RBMs) trained with contrastive divergence on one hand, and various types of
autoencoders on the other.
The present research begins with the question of what explicit criteria a good
intermediate representation should satisfy. Obviously, it should at a minimum
retain a certain amount of “information” about its input, while at the same time
being constrained to a given form (e.g. a real-valued vector of a given size in the
case of an autoencoder). A supplemental criterion that has been proposed for
such models is sparsity of the representation distribution of its observed input. For high dimensional redundant
input (such as images) at least, such structures are likely to depend on evidence
gathered from a combination of many input dimensions. They should thus be
recoverable from partial observation only.
A hallmark of this is our human
ability to recognize partially occluded or corrupted images. Further evidence is
our ability to form a high level concept associated to multiple modalities (such
as image and sound) and recall it even when some of the modalities are missing.
To validate our hypothesis and assess its usefulness as one of the guiding
principles in learning deep architectures, we propose a modiﬁcation to the autoencoder framework to explicitly integrate robustness to partially destroyed
inputs. Section 2 describes the algorithm in details. Section 3 discusses links
with other approaches in the literature. Section 4 is devoted to a closer inspection of the model from diﬀerent theoretical standpoints. In section 5 we verify
empirically if the algorithm leads to a diﬀerence in performance.
concludes the study.
Description of the Algorithm
Notation and Setup
Let X and Y be two random variables with joint probability density p(X, Y ),
with marginal distributions p(X) and p(Y ).
Throughout the text, we will
use the following notation: Expectation: EEp(X)[f(X)] =
p(x)f(x)dx.
Entropy: IH(X) = IH(p) = EEp(X)[−log p(X)]. Conditional entropy: IH(X|Y ) =
EEp(X,Y )[−log p(X|Y )]. Kullback-Leibler divergence: IDKL(p∥q) = EEp(X)[log p(X)
Cross-entropy: IH(p∥q) = EEp(X)[−log q(X)] = IH(p) + IDKL(p∥q). Mutual information: I(X; Y ) = IH(X) −IH(X|Y ).
Sigmoid: s(x) =
1+e−x and s(x) =
(s(x1), . . . , s(xd))T . Bernoulli distribution with mean µ: Bµ(x). and by extension Bµ(x) = (Bµ1(x1), . . . , Bµd(xd)).
The setup we consider is the typical supervised learning setup with a training
set of n (input, target) pairs Dn = {(x(1), t(1)) . . . , (x(n), t(n))}, that we suppose
to be an i.i.d. sample from an unknown distribution q(X, T) with corresponding
marginals q(X) and q(T).
The Basic Autoencoder
We begin by recalling the traditional autoencoder model such as the one used
in to build deep networks. An autoencoder takes an input
vector x ∈ d, and ﬁrst maps it to a hidden representation y ∈ d′
through a deterministic mapping y = fθ(x) = s(Wx + b), parameterized by
θ = {W, b}. W is a d′ × d weight matrix and b is a bias vector. The resulting
latent representation y is then mapped back to a “reconstructed” vector z ∈
 d in input space z = gθ′(y) = s(W′y + b′) with θ′ = {W′, b′}. The weight
matrix W′ of the reverse mapping may optionally be constrained by W′ = WT ,
in which case the autoencoder is said to have tied weights. Each training x(i) is
thus mapped to a corresponding y(i) and a reconstruction z(i). The parameters
of this model are optimized to minimize the average reconstruction error:
x(i), z(i)
x(i), gθ′(fθ(x(i)))
where L is a loss function such as the traditional squared error L(x, z) = ∥x−z∥2.
An alternative loss, suggested by the interpretation of x and z as either bit
vectors or vectors of bit probabilities (Bernoullis) is the reconstruction crossentropy:
LIH(x, z)=
[xk log zk+(1 −xk) log(1 −zk)]
Figure 1: An example x is corrupted to ˜x. The autoencoder then maps it to y
and attempts to reconstruct x.
Note that if x is a binary vector, LIH(x, z) is a negative log-likelihood for the
example x, given the Bernoulli parameters z. Equation 1 with L = LIH can be
θ⋆, θ′⋆= arg min
EEq0(X) [LIH (X, gθ′(fθ(X)))]
where q0(X) denotes the empirical distribution associated to our n training
inputs. This optimization will typically be carried out by stochastic gradient
The Denoising Autoencoder
To test our hypothesis and enforce robustness to partially destroyed inputs we
modify the basic autoencoder we just described. We will now train it to reconstruct a clean “repaired” input from a corrupted, partially destroyed one. This
is done by ﬁrst corrupting the initial input x to get a partially destroyed version ˜x by means of a stochastic mapping ˜x ∼qD(˜x|x). In our experiments, we
considered the following corrupting process, parameterized by the desired proportion ν of “destruction”: for each input x, a ﬁxed number νd of components
are chosen at random, and their value is forced to 0, while the others are left
untouched. The procedure can be viewed as replacing a component considered
missing by a default value, which is a common technique. A motivation for
zeroing the destroyed components is that it simulates the removal of these components from the input. For images on a white (0) background, this corresponds
to “salt noise”. Note that alternative corrupting noises could be considered1.
The corrupted input ˜x is then mapped, as with the basic autoencoder, to a
hidden representation y = fθ(˜x) = s(W˜x + b) from which we reconstruct a
z = gθ′(y) = s(W′y + b′) (see ﬁgure 1 for a schematic representation of the
process). As before the parameters are trained to minimize the average reconstruction error LIH(x, z) = IH(Bx∥Bz) over a training set, i.e. to have z as close
as possible to the uncorrupted input x. But the key diﬀerence is that z is now
a deterministic function of ˜x rather than x and thus the result of a stochastic
mapping of x.
Let us deﬁne the joint distribution
X, Y ) = q0(X)qD( e
X|X)δfθ( e
1the approach we describe and our analysis is not speciﬁc to a particular kind of corrupting
where δu(v) puts mass 0 when u ̸= v. Thus Y is a deterministic function of
X. q0(X, e
X, Y ) is parameterized by θ. The objective function minimized by
stochastic gradient descent becomes:
X, gθ′ , with the representation of the k-th layer used as
input for the (k + 1)-th, and the (k + 1)-th layer trained after the k-th has been
trained. After a few layers have been trained, the parameters are used as initialization for a network optimized with respect to a supervised training criterion.
This greedy layer-wise procedure has been shown to yield signiﬁcantly better
local minima than random initialization of deep networks ,
achieving better generalization on a number of tasks .
The procedure to train a deep network using the denoising autoencoder is
similar. The only diﬀerence is how each layer is trained, i.e., to minimize the
criterion in eq. 5 instead of eq. 3. Note that the corruption process qD is only
used during training, but not for propagating representations from the raw input
to higher-level representations. Note also that when layer k is trained, it receives
as input the uncorrupted output of the previous layers.
Relationship to Other Approaches
Our training procedure for the denoising autoencoder involves learning to recover a clean input from a corrupted version, a task known as denoising. The
problem of image denoising, in particular, has been extensively studied in the
image processing community and many recent developments rely on machine
learning approaches ; Elad and Aharon ;
Hammond and Simoncelli ). A particular form of gated autoencoders has
also been used for denoising in Memisevic . Denoising using autoencoders
was actually introduced much earlier , as
an alternative to Hopﬁeld models . Our objective however is
fundamentally diﬀerent from that of developing a competitive image denoising
algorithm. We investigate explicit robustness to corrupting noise only as a criterion to guide the learning of suitable intermediate representations, with the
goal to build a better general purpose learning algorithm. Thus our corruption+denoising procedure is applied not only on the input, but also recursively
to intermediate representations. It is not speciﬁc to images and does not use
prior knowledge of image topology.
Whereas the proposed approach does not rely on prior knowledge, it bears
resemblance to the well known technique of augmenting the training data with
stochastically “transformed” patterns. But again we do not rely on prior knowledge. Moreover we only use the corrupted patterns to optimize an unsupervised
criterion, as an initialization step.
There are also similarities with the work of on robust
coding over noisy channels. In their framework, a linear encoder is to encode
a clean input for optimal transmission over a noisy channel to a decoder that
reconstructs the input. This work was later extended to robustness to noise in
the input, in a proposal for a model of retinal coding .
Though some of the inspiration behind our work comes from neural coding
and computation, our goal is not to account for experimental data of neuronal
activity as in . Also, the non-linearity of our denoising
autoencoder is crucial for its use in initializing a deep neural network.
It may be objected that, if our goal is to handle missing values correctly,
we could have more naturally deﬁned a proper latent variable generative model,
and infer the posterior over the latent (hidden) representation in the presence
of missing inputs. But this usually requires a costly marginalization2 which has
to be carried out for each new example. By contrast, our approach tries to learn
a fast and robust deterministic mapping fθ from examples of already corrupted
inputs. The burden is on learning such a constrained mapping during training,
rather than on unconstrained inference at use time. We expect this may force
the model to capture implicit invariances in the data, and result in interesting
features. Also note that in section 4.4 we will see how our learning algorithm
for the denoising autoencoder can be viewed as a form of variational inference
in a particular generative model.
Analysis of the Denoising Autoencoder
The above intuitive motivation for the denoising autoencoder was given with the
perspective of discovering robust representations. In the following, which can
be skipped without hurting the remainder of the paper, we try to gain insight
by considering several alternative perspectives on the algorithm.
Manifold Learning Perspective
The process of mapping a corrupted example to an uncorrupted one can be
visualized in Figure 2, with a low-dimensional manifold near which the data
concentrate. We learn a stochastic operator p(X| e
X) that maps an e
X to an X,
X) = Bgθ′(fθ( e
X))(X). The corrupted examples will be much more likely to
2as in the case of RBMs, where it is exponential in the number of missing values
gθ′(fθ(˜x))
Figure 2: Illustration of what the denoising autoencoder is trying to learn. Suppose training data (crosses) concentrate near a low-dimensional manifold. A corrupted example (circle) is obtained by applying a corruption process qD( e
(left side). Corrupted examples (circles) are typically outside and farther from
the manifold, hence the model learns with p(X| e
X) to map points to more likely
points (right side).
Mapping from more corrupted examples requires bigger
jumps (longer dashed arrows).
be outside and farther from the manifold than the uncorrupted ones. Hence the
stochastic operator p(X| e
X) learns a map that tends to go from lower probability
X to high probability points X, generally on or near the manifold. Note
that when e
X is farther from the manifold, p(X| e
X) should learn to make bigger
steps, to reach the manifold. At the limit we see that the operator should map
even far away points to a small volume near the manifold.
The denoising autoencoder can thus be seen as a way to deﬁne and learn a
manifold. The intermediate representation Y = f(X) can be interpreted as a
coordinate system for points on the manifold (this is most clear if we force the
dimension of Y to be smaller than the dimension of X). More generally, one can
think of Y = f(X) as a representation of X which is well suited to capture the
main variations in the data, i.e., on the manifold. When additional criteria (such
as sparsity) are introduced in the learning model, one can no longer directly view
Y = f(X) as an explicit low-dimensional coordinate system for points on the
manifold, but it retains the property of capturing the main factors of variation
in the data.
The Stochastic Operator Perspective
The denoising autoencoder can be seen as corresponding to a semi-parametric
model from which we can sample. Let us augment the set of modeled random
variables to include the corrupted example e
X in addition to the corresponding
uncorrupted example X, and let us perform maximum likelihood training on
a model of their joint. We consider here the simpler case where X is discrete,
but the approach can be generalized. We deﬁne a joint distribution p(X, e
X) from the stochastic operator p(X| e
X), with marginal p( e
X) = q0( e
set by construction.
We now have an empirical distribution q0 and a model p on (X, e
Performing maximum likelihood on them or minimizing IDKL(q0(X, e
is a reasonable training objective, again yielding the denoising criterion in eq. 5.
As an additional motivation for minimizing IDKL(q0(X, e
that as we minimize it (i.e., IDKL(q0(X, e
X)) →0), the marginals of p
approach those of q0, hence in particular
p(X) →q0(X),
i.e., training in this way corresponds to a semi-parametric model p(X) which
approaches the empirical distribution q0(X). By applying the marginalization
deﬁnition for p(X), we see what the corresponding model is
X = ˜x)qD(˜x|xi)
where xi is one of the n training examples. Note that only the parameters of
X) are optimized in this model. Note also that sampling from the model is
easy. We have thus see that the denoising autoencoder learns a semi-parametric
model which can be sampled from, based on the stochastic operator p(X| e
What would happen if we were to apply this operator repeatedly? That
would deﬁne a chain pk(X) where p0(X = x) = q0( e
X = x), p1(X) = p(X)
and pk(X) = P
X = x)pk−1(x).
If the operator was ergodic (which
is plausible, following the above argumentation), its ﬁxed point would deﬁne
yet another distribution π(X) = limk→∞pk(X), of which the semi-parametric
p(X) would be a ﬁrst-order approximation. The advantage of this formulation
is that π(X) is purely parametric: it does not explicitly depend on the empirical
distribution q0.
Bottom-up Filtering, Information Theoretic Perspective
In this section we adopt a bottom-up ﬁltering viewpoint, and an informationtheoretic perspective. Let X, e
X, Y be random variables representing respectively
an input sample, a corrupted version of it, and the corresponding hidden representation, i.e. X ∼q(X) (the true generating process for X), e
X), with the associated joint q(X, e
X, Y ). Notice that it is deﬁned with
the same dependency structure as q0(X, e
X, Y ), and the same conditionals e
The role of the greedy layer initialization is to learn a non-linear ﬁlter that
yields a good representation of its input for the next layer.
A good representation should retain a suﬃcient amount of “information” about its input,
while we might at the same time encourage its marginal distribution to display certain desirable properties. From this high level perspective, the ﬁlter
we learn, which has a ﬁxed parameterized form with parameters θ, should be
optimized to realize a tradeoﬀbetween yielding a somewhat easier to model
marginal distribution and retaining as much information as possible about the
input. This can be formalized as optimizing an objective function of the form
arg maxθ {I(X; Y ) + λJ (Y )}, where J is a functional that induces some preference over the marginal q(Y ), and hyper-parameter λ controls the tradeoﬀ. J
could for example encourage IDKL closeness to some reference prior distribution,
or be some measure of sparsity or independence. In the present study, however, we shall suppose that Y is only constrained by its dimensionality. This
corresponds to the case where J (Y ) is constant for all distributions (i.e. no
preference).
Let us thus examine only the ﬁrst term. We have I(X; Y ) = IH(X)−IH(X|Y ).
If we suppose the unknown input distribution q(X) ﬁxed, IH(X) is a constant.
Maximizing mutual information then amounts to maximizing −IH(X|Y ), i.e.
EEq(X,Y )[log q(X|Y )]
θ,p⋆EEq(X,Y )[log p⋆(X|Y )]
where we optimize over all possible distributions p⋆. It is easy to show that the
maximum is obtained for p⋆(X|Y ) = q(X|Y ). If instead we constrain p⋆(X|Y )
to a given parametric form p(X|Y ) parameterized by θ′, we get a lower bound:
θ,θ′ EEq(X,Y )[log p(X|Y )]
where we have an equality if ∃θ′ q(X|Y ) = p(X|Y ).
In the case of an autoencoder with binary vector variable X, we can view the
top-down reconstruction as representing a p(X|Y ) = Bgθ′(Y )(X). Optimizing
for the lower bound leads to:
θ,θ′ EEq(X,Y )[log Bgθ′(Y )(X)]
θ,θ′ EEq(X, e
X)[log Bgθ′(fθ( e
θ,θ′ EEq(X, e
X)[LIH(X, gθ′(fθ( e
where in the second line we use the fact that Y = fθ( e
X) deterministically. This
is the criterion we use for training the autoencoder (eq. 5), but replacing the
true generating process q by the empirical q0.
This shows that minimizing the expected reconstruction error amounts to
maximizing a lower bound on IH(X|Y ) and at the same time on the mutual
information between input X and the hidden representation Y . Note that this
reasoning holds equally for the basic autoencoder, but with the denoising autoencoder, we maximize the mutual information between X and Y even as Y
is a function of corrupted input.
Top-down, Generative Model Perspective
In this section we try to recover the training criterion for our denoising autoencoder (eq. 5) from a generative model perspective. Speciﬁcally we show that
training the denoising autoencoder as described in section 2.3 is equivalent to
maximizing a variational bound on a particular generative model.
Consider the generative model p(X, e
X, Y ) = p(Y )p(X|Y )p( e
X|X) where
p(X|Y ) = Bs(W′Y +b′) and p( e
X|X) = qD( e
X|X). p(Y ) is a uniform prior over
Y . This deﬁnes a generative model with parameter set θ′ = {W′, b′}. We will
use the previously deﬁned q0(X, e
X, Y ) = q0(X)qD( e
X|X)δfθ( e
X)(Y ) (equation 4)
as an auxiliary model in the context of a variational approximation of the loglikelihood of p( e
X). Note that we abuse notation to make it lighter, and use
the same letters X, e
X and Y for diﬀerent sets of random variables representing
the same quantity under diﬀerent distributions: p or q0. Keep in mind that
whereas we had the dependency structure X →e
X →Y for q or q0, we have
Since p contains a corruption operation at the last generative stage, we
propose to ﬁt p( e
X) to corrupted training samples. Performing maximum likelihood ﬁtting for samples drawn from q0( e
X) corresponds to minimizing the
cross-entropy, or maximizing
θ′ {−IH(q0( e
θ′ {EEq0( e
X)[log p( e
Let q⋆(X, Y | e
X) be a conditional density, the quantity L(q⋆, e
X) = EEq⋆(X,Y | e
log p(X, e
q⋆(X,Y | e
is a lower bound on log p( e
X) since the following can be shown to be true for
X) = L(q⋆, e
X) + IDKL(q⋆(X, Y | e
X)∥p(X, Y | e
Also it is easy to verify that the bound is tight when q⋆(X, Y | e
X) = p(X, Y | e
where the IDKL becomes 0. We can thus write log p( e
X) = maxq⋆L(q⋆, e
consequently rewrite equation 7 as
θ′ {EEq0( e
θ′,q⋆{EEq0( e
X)[L(q⋆, e
where we moved the maximization outside of the expectation because an unconstrained q⋆(X, Y | e
X) can in principle perfectly model the conditional distribution
needed to maximize L(q⋆, e
X) for any e
X. Now if we replace the maximization
over an unconstrained q⋆by the maximization over the parameters θ of our q0
(appearing in fθ that maps an x to a y), we get a lower bound on H
θ′,θ {EEq0( e
X)[L(q0, e
Maximizing this lower bound, we ﬁnd
X)[L(q0, e
log p(X, e
q0(X, Y | e
log p(X, e
IH[q0(X, Y | e
log p(X, e
Note that θ only occurs in Y = fθ( e
X), and θ′ only occurs in p(X|Y ). The last
line is therefore obtained because q0(X| e
X|X)q0(X) (none of which
depends on (θ, θ′)), and q0(Y | e
X) is deterministic, i.e., its entropy is constant,
irrespective of (θ, θ′). Hence the entropy of q0(X, Y | e
X) = q0(Y | e
does not vary with (θ, θ′). Finally, following from above, we obtain our training
criterion (eq. 5):
X)[L(q0, e
X,Y )[log[p(Y )p(X|Y )p( e
X,Y )[log p(X|Y )]
X)[log p(X|Y = fθ( e
X, gθ′(fθ( e
where the third line is obtained because (θ, θ′) have no inﬂuence on EEq0(X, e
X,Y )[log p(Y )]
because we chose p(Y ) uniform, i.e. constant, nor on EEq0(X, e
X)[log p( e
X|X)], and
the last line is obtained by inspection of the deﬁnition of LIH in eq. 2, when
p(X|Y = fθ( e
X)) is a Bgθ′ 3. It contains diﬀerent variations of the MNIST digit classiﬁcation problem, with added factors of
variation such as rotation (rot), addition of a background composed of random
pixels (bg-rand) or made from patches extracted from a set of images (bg-img), or
combinations of these factors (rot-bg-img). These variations render the problems
 
Table 1: Comparison of stacked denoising autoencoders (SdA-3) with
other models.
Test error rate on all considered classiﬁcation problems is reported together with
a 95% conﬁdence interval. Best performer is in bold, as well as those for which
conﬁdence intervals overlap. SdA-3 appears to achieve performance superior or
equivalent to the best other model on all problems except bg-rand. For SdA-3,
we also indicate the fraction ν of destroyed input components, as chosen by
proper model selection. Note that SAA-3 is equivalent to SdA-3 with ν = 0%.
2.80±0.14 (10%)
11.11±0.28
15.42±0.32
14.69±0.31
10.30±0.27
10.30±0.27
10.29±0.27 (10%)
14.58±0.31
16.62±0.33
11.28±0.28
10.38±0.27 (40%)
22.61±0.37
24.01±0.37
16.15±0.32
23.00±0.37
16.31±0.32
16.68±0.33 (25%)
rot-bg-img
55.18±0.44
56.41±0.43
52.21±0.44
51.93±0.44
47.39±0.44
44.49±0.44 (25%)
1.99±0.12 (10%)
24.04±0.37
24.05±0.37
23.69±0.37
24.05±0.37
22.50±0.37
21.59±0.36 (25%)
19.13±0.34
19.82±0.35
19.92±0.35
18.41±0.34
18.63±0.34
19.06±0.34 (10%)
particularly challenging for current generic learning algorithms. Each problem
is divided into a training, validation, and test set . A subset of the original MNIST problem is also included with the
same example set sizes (problem basic). The benchmark also contains additional
binary classiﬁcation problems: discriminating between convex and non-convex
shapes (convex), and between wide and long rectangles (rect, rect-img).
Neural networks with 3 hidden layers initialized by stacking denoising autoencoders (SdA-3), and ﬁne tuned on the classiﬁcation tasks, were evaluated
on all the problems in this benchmark. Model selection was conducted following a similar procedure as Larochelle et al. . Several values of hyper
parameters (destruction fraction ν, layer sizes, number of unsupervised training
epochs) were tried, combined with early stopping in the ﬁne tuning phase. For
each task, the best model was selected based on its classiﬁcation performance
on the validation set.
Table 1 reports the resulting classiﬁcation error on the test set for the new
model (SdA-3), together with the performance reported in Larochelle et al.
 4 for SVMs with Gaussian and polynomial kernels, 1 and 3 hidden layers
deep belief network (DBN-1 and DBN-3) and a 3 hidden layer deep network
initialized by stacking basic autoencoders (SAA-3). Note that SAA-3 is equivalent to a SdA-3 with ν = 0% destruction. As can be seen in the table, the
corruption+denoising training works remarkably well as an initialization step,
and in most cases yields signiﬁcantly better classiﬁcation performance than basic autoencoder stacking with no noise. On all but one task the SdA-3 algorithm
performs on par or better than the best other algorithms, including deep belief
4Except that rot and rot-bg-img, as reported on the website from which they are available,
have been regenerated since Larochelle et al.
 , to ﬁx a problem in the initial data
generation process. We used the updated data and corresponding benchmark results given on
this website.
Figure 3: Filters obtained after training the ﬁrst denoising autoencoder.
(a-c) show some of the ﬁlters obtained after training a denoising autoencoder
on MNIST samples, with increasing destruction levels ν. The ﬁlters at the same
position in the three images are related only by the fact that the autoencoders
were started from the same random initialization point.
(d) and (e) zoom in on the ﬁlters obtained for two of the neurons, again for
increasing destruction levels.
As can be seen, with no noise, many ﬁlters remain similarly uninteresting (undistinctive almost uniform grey patches). As we increase the noise level, denoising
training forces the ﬁlters to diﬀerentiate more, and capture more distinctive
features. Higher noise levels tend to induce less local ﬁlters, as expected. One
can distinguish diﬀerent kinds of ﬁlters, from local blob detectors, to stroke
detectors, and some full character detectors at the higher noise levels.
(a) No destroyed inputs
(b) 25% destruction
(c) 50% destruction
(d) Neuron A (0%, 10%, 20%, 50% destruction)
(e) Neuron B (0%, 10%, 20%, 50% destruction)
nets. This suggests that our proposed procedure was indeed able to produce
more useful feature detectors.
Next, we wanted to understand qualitatively the eﬀect of the corruption+denoising
training. To this end we display the ﬁlters obtained after initial training of the
ﬁrst denoising autoencoder on MNIST digits. Figure 3 shows a few of these
ﬁlters as little image patches, for diﬀerent noise levels. Each patch corresponds
to a row of the learnt weight matrix W, i.e. the incoming weights of one of the
hidden layer neurons. The beneﬁcial eﬀect of the denoising training can clearly
be seen. Without the denoising procedure, many ﬁlters appear to have learnt
no interesting feature. They look like the ﬁlters obtained after random initialization. But when increasing the level of destructive corruption, an increasing
number of ﬁlters resemble sensible feature detectors.
As we move to higher
noise levels, we observe a phenomenon that we expected: ﬁlters become less
local, they appear sensitive to larger structures spread out across more input
dimensions.
Conclusion and Future Work
We have introduced a very simple training principle for autoencoders, based on
the objective of undoing a corruption process. This is motivated by the goal of
learning representations of the input that are robust to small irrelevant changes
in input. Several perspectives also help to motivate it from a manifold learning
perspective and from the perspective of a generative model.
This principle can be used to train and stack autoencoders to initialize a
deep neural network.
A series of image classiﬁcation experiments were performed to evaluate this new training principle. The empirical results support
the following conclusions: unsupervised initialization of layers with an explicit
denoising criterion helps to capture interesting structure in the input distribution. This in turn leads to intermediate representations much better suited for
subsequent learning tasks such as supervised classiﬁcation. The experimental
results with Deep Belief Networks (whose layers are initialized as RBMs) suggest that RBMs may also encapsulate a form of robustness in the representations
they learn, possibly because of their stochastic nature, which introduces noise
in the representation during training. Future work inspired by this observation
should investigate other types of corruption process, not only of the input but
of the representation itself as well.